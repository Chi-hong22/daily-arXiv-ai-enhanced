# 功能规则: Daily arXiv Spider

本文档详细说明了 `daily_arxiv_spider` 功能的设计思路、实现细节和工作流程。

## 1. 功能概述

### 1.1. 功能名称

Daily arXiv Spider (每日 arXiv 爬虫)

### 1.2. 核心目标

该功能的核心目标是每日自动从 arXiv.org 上抓取指定分类下的最新论文ID。这些ID随后可用于获取论文详细信息（如摘要、作者等），并进行后续的AI处理（如总结、分类）。

## 2. 设计与实现

### 2.1. 技术选型

- **爬虫框架**: [Scrapy](https://scrapy.org/) - 一个强大且灵活的Python爬虫框架。
- **目标网站**: [arXiv.org](https://arxiv.org/) - 著名的预印本论文托管网站。

### 2.2. 核心组件

- **`ArxivSpider`**: 位于 `daily_arxiv/daily_arxiv/spiders/arxiv.py` 的Scrapy爬虫。

### 2.3. 实现细节

#### a. 动态配置分类

爬虫的起始目标URL不是硬编码的，而是通过环境变量 `CATEGORIES` 动态生成。

- **环境变量**: `CATEGORIES`
- **格式**: 一个或多个arXiv分类，以逗号分隔 (例如: `"cs.CV,cs.AI,cs.LG"`)。
- **默认值**: 如果未设置该环境变量，则默认为 `"cs.CV"` (计算机视觉)。

爬虫会为每个分类生成一个起始URL，格式为 `https://arxiv.org/list/{category}/new`。

#### b. 论文ID提取逻辑

爬虫的 `parse` 方法负责从列表页面提取论文ID。其逻辑如下：

1.  首先，它会识别出页面上所有论文的锚点（`item`）。
2.  然后，它遍历所有论文条目（`<dl>`）。
3.  它会跳过当天最新发布的几篇论文（具体逻辑是 `int(paper.css("a[name^='item']::attr(name)").get().split("item")[-1]) >= anchors[-1]`），这部分逻辑可能需要根据需求进行复查或优化。
4.  对于符合条件的论文，它会提取其唯一的arXiv ID（例如 `2401.12345`）。
5.  最终，爬虫产出（`yield`）一个包含 `id` 字段的字典。

## 3. 工作流程

1.  **触发**: 运行爬虫 (例如，通过 `scrapy crawl arxiv`)。
2.  **读取配置**: 爬虫启动时，读取 `CATEGORIES` 环境变量。
3.  **发送请求**: 向配置的每个分类的 `new` 列表页面发送HTTP请求。
4.  **解析页面**: 收到响应后，调用 `parse` 方法。
5.  **提取ID**: 根据上述逻辑提取论文ID。
6.  **输出**: 将提取到的论文ID作为数据项输出。这些数据可以被后续的 Scrapy Pipeline 或其他脚本捕获和处理。

## 4. 后续步骤与依赖

此爬虫只负责第一步：**获取ID**。

完整的应用链路还应包括：
- **详情获取**: 一个使用这些ID通过arXiv API（或再次爬取）获取论文标题、摘要等详细信息的模块。
- **AI处理**: 一个利用 `langchain` 和 `langchain-openai` 将论文信息发送给大语言模型进行处理的模块。
- **结果存储/展示**: 将AI处理后的结果（如摘要）保存到数据库或生成Markdown报告。

这些后续步骤在此功能的范围之外，但构成了整个项目的核心价值。 