<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文评估了500M和2.2B参数的SmolVLM2变体在BLV用户可访问性描述质量上的表现，引入了两个新的评估框架，并测试了不同提示策略和移动设备部署性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成视频描述，但其高资源需求限制了实际应用，特别是对盲人和低视力用户的实用性。

Method: 使用SmolVLM2的500M和2.2B参数变体，在两个数据集上评估，引入多上下文BLV框架和导航辅助框架，测试四种提示策略，并在智能手机上评估FP32和INT8精度变体。

Result: 评估了不同模型规模对可访问性描述质量的影响，以及移动设备上的实际性能约束。

Conclusion: 研究表明较小模型在资源受限环境下仍能提供有效的可访问性描述，为BLV用户的实际应用提供了可行性方案。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: LUA是一种轻量级模块，直接在生成器的潜在代码上执行超分辨率，在最终VAE解码步骤之前完成，避免了传统图像超分辨率的延迟和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型难以扩展到超出训练分辨率，直接高分辨率采样成本高且慢，而传统图像超分辨率在解码后操作会引入伪影和额外延迟。

Method: 使用轻量级模块LUA在潜在空间执行超分辨率，采用共享Swin风格主干和尺度特定像素重组头，支持2倍和4倍放大，无需修改基础模型或添加额外扩散阶段。

Result: LUA在感知质量上与像素空间SR相当，但解码和上采样时间降低近3倍（1024px生成仅增加0.42秒，而像素空间SR需要1.87秒），且在不同VAE的潜在空间中表现出强泛化能力。

Conclusion: LUA提供了实用且高效的路径，在现代扩散管道中实现可扩展的高保真图像合成，几乎匹配原生高分辨率生成的保真度。

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [3] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 提出Self-Consistency Sampling (SCS)方法，解决多模态大语言模型在强化学习中因错误推理链但猜对选项而获得相同奖励的问题，通过视觉扰动和轨迹重采样获得一致性分数来优化策略更新。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试中，基于结果奖励的强化学习方法存在一个被忽视的问题：即使推理链错误但猜对选项的轨迹也会获得与真正正确推理相同的奖励，这影响了模型的学习效果。

Method: 提出SCS方法：(i)引入小的视觉扰动，(ii)对初始轨迹进行重复截断和重采样，通过结果轨迹的一致性产生可微分的一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 基于Qwen2.5-VL-7B-Instruct模型，在RLOO、GRPO和REINFORCE++系列方法中集成SCS，在六个多模态基准测试上准确率提升最高达7.7个百分点，计算开销可忽略。在Qwen2.5-VL-3B-Instruct和InternVL3-8B模型上也获得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一个简单通用的解决方案，能有效纠正奖励分配不准确的问题。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文提出了两个新的参数化多臂老虎机算法家族，通过离线数据学习近最优算法，在满足特定凹性条件时获得更强的性能保证，同时保证在良好实例上的最优臂识别和在恶劣实例上的最坏情况保证。


<details>
  <summary>Details</summary>
Motivation: 改进多臂老虎机问题用于在不确定性下分配努力，如投资新技术、临床试验和超参数选择。现有算法存在悲观的最坏情况保证，本文旨在通过参数化算法家族和数据依赖分析获得更强的性能保证。

Method: 提出两个参数化算法家族：第一个包含先前最优随机算法，在臂奖励曲线满足额外凹性性质时可获得更强的k依赖保证；第二个保证在良好实例上的最优臂识别和在恶劣实例上的最坏情况保证。使用离线数据学习每个家族中的近最优算法。

Result: 通过统计学习视角，实现了更强的数据依赖保证，无需验证假设是否满足。在满足特定凹性条件时，第一个算法家族可获得最优的k依赖保证。

Conclusion: 参数化算法家族和统计学习视角为改进多臂老虎机问题提供了更强大的数据依赖保证，在满足额外性质时显著提升了性能，同时保持了在恶劣情况下的鲁棒性。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种验证模拟环境中发现的自动驾驶故障场景在真实世界中可重现性的方法，通过定义时间序列传感器数据与Scenic场景程序的匹配关系，开发了高效的查询算法来识别真实数据集中匹配的故障场景。


<details>
  <summary>Details</summary>
Motivation: 解决仿真测试中发现的自动驾驶故障场景是否能在真实世界中重现的问题，克服由于模拟传感器数据与真实数据差异导致的sim-to-real差距，确保故障场景的有效性验证。

Method: 引入形式化定义来描述标记时间序列传感器数据如何匹配抽象场景（使用Scenic概率编程语言表示），并提出了查询算法来在给定场景程序和标记数据集的情况下识别匹配的数据子集。

Result: 实验表明，该算法在查询场景时比最先进的商业视觉大语言模型更准确，速度提高了几个数量级，并且能够随着查询时间序列数据长度的增加而扩展。

Conclusion: 提出的方法能够有效验证模拟故障场景在真实世界中的可重现性，为自动驾驶系统的安全测试提供了可靠的工具，显著提升了故障场景验证的效率和准确性。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [6] [Robot Crash Course: Learning Soft and Stylized Falling](https://arxiv.org/abs/2511.10635)
*Pascal Strauch,David Müller,Sammy Christen,Agon Serifi,Ruben Grandia,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: 本文提出了一种机器人无关的奖励函数，通过强化学习实现双足机器人的受控软着陆，在跌倒时平衡期望姿态达成与冲击最小化，并保护关键部件。


<details>
  <summary>Details</summary>
Motivation: 尽管双足机器人的稳健运动研究取得进展，但在现实世界中仍有跌倒风险。大多数研究专注于防止跌倒，而本文关注跌倒现象本身，旨在减少机器人物理损伤并让用户控制机器人最终姿态。

Method: 提出机器人无关的奖励函数，在强化学习中平衡期望姿态达成、冲击最小化和关键部件保护；引入基于仿真的初始姿态和最终姿态采样策略，使策略对广泛初始跌倒条件具有鲁棒性，并能在推理时指定任意未见过的最终姿态。

Result: 通过仿真和真实世界实验证明，即使是双足机器人也能执行受控的软着陆。

Conclusion: 本文展示了双足机器人可以实现受控软着陆，通过平衡姿态控制和冲击最小化来减少跌倒时的物理损伤。

Abstract: Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.

</details>
