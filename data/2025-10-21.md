<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.SD](#cs.SD) [Total: 14]
- [cs.GT](#cs.GT) [Total: 4]
- [eess.SY](#eess.SY) [Total: 24]
- [cs.MA](#cs.MA) [Total: 9]
- [cs.RO](#cs.RO) [Total: 48]
- [cs.HC](#cs.HC) [Total: 28]
- [cs.GR](#cs.GR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 18]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，通过引导采样过程将外观（图像或文本）转移到3D资产上，解决了现有方法在几何差异较大时失败的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在输入和外观对象的几何形状显著不同时效果不佳，直接应用3D生成模型无法产生吸引人的结果。

Method: 基于预训练的整流流模型，在采样过程中定期添加引导，引导可建模为可微损失函数，包括外观的部分感知损失和自相似性损失。

Result: 成功将纹理和几何细节转移到3D资产，在定性和定量评估中均优于基线方法。

Conclusion: 该方法具有通用性，可扩展到不同类型的扩散模型和引导函数，并提出了基于GPT的客观评估系统来解决传统指标不适用的问题。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [2] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo框架，将假人视频转换为身份可控、逼真的人类视频，解决头部与身体运动不对齐和身份漂移问题


<details>
  <summary>Details</summary>
Motivation: 基于假人的服装展示成本低但缺乏真实感和表现细节，需要将假人视频转换为逼真的人类视频

Method: 使用动态姿态感知头部编码器融合面部语义和身体姿态，引入镜像损失和分布感知适配器增强时间一致性

Result: 在多个数据集上实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有方法

Conclusion: M2HVideo框架成功解决了假人到人类视频生成的关键挑战，实现了高质量的视频转换

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [3] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 该研究比较了三种放射学报告模式：自由文本、结构化报告和AI辅助结构化报告，发现AI辅助结构化报告在诊断准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估结构化报告和人工智能如何改变放射科医生与影像研究的互动方式，特别是在床边胸片分析中的影响。

Method: 前瞻性研究，8名读者（新手和非新手）使用定制查看器和眼动追踪系统分析35张床边胸片，比较三种报告模式的效果。

Result: AI辅助结构化报告的诊断准确性最高（κ=0.71），报告时间最短（25±9秒），眼动追踪指标显示视觉注意力更集中于图像区域。

Conclusion: 结构化报告通过引导视觉注意力提高效率，而AI预填充的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [4] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是一个使用Aria Gen 2眼镜采集的自我中心多模态开放数据集，包含清洁、烹饪、进食、玩耍和户外行走五种场景的原始传感器数据和机器感知算法输出。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供及时可用的自我中心多模态数据集，展示设备感知佩戴者、环境和交互的能力。

Method: 使用Aria Gen 2眼镜采集主要受试者Dia'ane及其朋友的日常活动数据，提供原始传感器数据和多种机器感知算法输出。

Result: 发布了包含五种主要场景的初始数据集，展示了设备在不同用户和条件下的鲁棒性能。

Conclusion: A2PD数据集通过projectaria.com公开提供，并附有开源工具和使用示例，支持相关研究发展。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [5] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 基于YOLOv11目标检测算法的自动手术照明系统，通过识别蓝色标记自动调整LED光源位置，减少外科医生疲劳并提高照明一致性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统需要手动调整，导致外科医生疲劳、颈部劳损以及因漂移和阴影造成的不一致照明问题。

Method: 使用YOLOv11算法识别手术区域上方的蓝色标记，通过两个带有倾斜-平移支架的伺服电机将高功率LED光源引导至识别位置。

Result: YOLO模型在包含模拟手术场景和蓝色球形标记的验证集上达到96.7% mAP@50的检测精度。

Conclusion: 这种基于机器视觉的解决方案自动化了照明过程，减少了外科医生的身体负担，提高了照明一致性，有助于改善手术结果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [6] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出了ESCA框架和SGClip模型，通过结构化时空理解来情境化具身智能体，无需人工标注的场景图注释，在具身环境中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要依赖高层视觉-声音-文本对，缺乏像素级视觉内容与文本语义的细粒度结构化对齐

Method: 开发SGClip模型，基于CLIP的开放域可提示场景图生成模型，通过神经符号学习管道在87K+开放域视频上训练，利用视频-字幕对和结构化推理实现模型驱动的自监督

Result: SGClip在场景图生成和动作定位基准测试中表现出色，ESCA框架持续改进开源和商业MLLMs，在两个具身环境中实现最先进性能，显著减少智能体感知错误

Conclusion: ESCA框架通过结构化时空理解有效情境化具身智能体，使开源模型超越专有基线，为通用具身智能体发展提供了重要进展

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [7] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 提出一个基于CCTV监控的智能城市基础设施缺陷检测系统，使用YOLO进行多缺陷检测分割，并通过视觉语言模型生成结构化维修计划


<details>
  <summary>Details</summary>
Motivation: 智能城市基础设施监控需求增长，人工检测成本高且危险，现有自动系统无法提供结构化输出指导维修工作

Method: 使用YOLO系列目标检测器进行多缺陷检测和分割，然后将检测结果传递给视觉语言模型进行场景感知总结，生成JSON格式的结构化行动计划

Result: 在公共数据集和采集的CCTV片段上实验表明，系统能准确识别多种缺陷并生成连贯的总结

Conclusion: 讨论了将系统扩展到城市范围部署的挑战和方向

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [8] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 提出CrossRay3D稀疏多模态检测器，通过Ray-Aware Supervision和Class-Balanced Supervision提升token表示质量，在nuScenes基准上达到SOTA性能，且运行速度更快、鲁棒性更强。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器忽视token表示质量，导致前景质量不优和性能受限。本文发现几何结构保持和类别分布是提升稀疏检测器性能的关键。

Method: 提出Sparse Selector(SS)，核心模块包括：Ray-Aware Supervision(RAS)在训练阶段保持丰富几何信息；Class-Balanced Supervision自适应重加权类别语义显著性；Ray Positional Encoding解决LiDAR和图像模态间的分布差异。

Result: 在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，运行速度比其他领先方法快1.84倍，在LiDAR或相机数据部分或完全缺失的场景下表现出强鲁棒性。

Conclusion: CrossRay3D通过改进token表示质量，在稀疏多模态检测中实现了优越的性能、速度和鲁棒性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [9] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 提出IAD-GPT，一种基于多模态大语言模型的工业异常检测新范式，通过异常提示生成器、文本引导增强器和多掩码融合模块，结合文本语义与图像信息实现先进的异常检测和分割。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和详细描述能力，而基于大预训练模型的方法尚未充分激发大模型在异常检测任务中的潜力。

Method: 使用异常提示生成器生成详细异常提示，通过文本引导增强器增强视觉定位能力，设计多掩码融合模块将掩码作为专家知识增强像素级异常感知。

Result: 在MVTec-AD和VisA数据集上的广泛实验表明，在自监督和少样本异常检测与分割任务中达到了最先进的性能。

Conclusion: IAD-GPT成功结合了文本语义与图像信息，为工业异常检测提供了有效的多模态解决方案。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [10] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了RefAVA++数据集和RefAtomNet++框架，用于解决基于语言描述的原子级视频动作识别问题，通过多层级语义对齐的跨模态注意力机制和多轨迹Mamba建模，在复杂多人场景中实现精确的语言引导动作理解。


<details>
  <summary>Details</summary>
Motivation: 现有的动作识别方法在复杂多人场景中难以精确识别特定人物的细粒度原子级动作，特别是在语言引导下的动作理解方面存在局限。

Method: 提出RefAtomNet++框架，采用多层级语义对齐的跨模态注意力机制，结合部分关键词、场景属性和整体句子级别的多轨迹Mamba建模，动态选择最近视觉空间token进行跨模态信息聚合。

Result: 实验表明RefAtomNet++在RefAVA++数据集上取得了最先进的性能，显著提升了目标人物定位和细粒度动作预测的准确性。

Conclusion: RefAtomNet++通过创新的跨模态token聚合机制，有效解决了基于语言描述的原子级视频动作识别问题，为复杂多人场景中的交互式人类动作分析提供了有力工具。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [11] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出了一种面向目标的视觉语言导航方法，通过Q学习训练Q模型来预测未来信息，结合A*搜索策略提高导航效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖历史信息进行决策，忽略了行动的未来影响和长期结果，需要开发具有前瞻性的智能体。

Method: 使用大规模无标签轨迹数据训练Q模型学习室内场景布局和物体关系，生成描述潜在未来信息的Q特征，通过跨模态未来编码器结合导航指令生成反映未来前景的动作评分。

Result: 在广泛使用的目标导向VLN数据集上进行的大量实验验证了所提方法的有效性。

Conclusion: 该方法通过整合历史信息和未来预测，实现了更有效的目标导向视觉语言导航。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [12] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出了一个分析并缓解图像分类中交叉偏见的框架，包括交叉公平性评估框架(IFEF)和基于偏见加权的数据增强方法(BWA)，在Open Images V7数据集上显著提升了代表性不足类别的准确率并减少了公平性差异。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在非平衡数据集上训练时产生的交叉偏见问题，这些偏见源于多个属性（如物体类别和环境条件）的相互作用。

Method: 结合定量公平性指标和可解释性工具的系统性偏见分析框架IFEF，以及基于子组分布统计自适应调整变换强度的数据增强策略BWA。

Result: 在Open Images V7数据集上，BWA将代表性不足类别的准确率提升了高达24个百分点，公平性指标差异减少了35%，统计检验证实改进显著(p < 0.05)。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏见问题提供了一个可复现的途径。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [13] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出基于纯视觉的小型无人机自主飞行系统，结合语义分割和单目深度估计实现室内环境下的障碍物规避、场景探索和自主安全着陆，无需GPS或昂贵传感器。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限平台上视觉导航的挑战，特别是度量深度估计和计算效率问题，使无人机能在无GPS的受控室内环境中自主飞行。

Method: 采用知识蒸馏框架，使用基于颜色的SVM教师生成训练数据，训练轻量级U-Net学生网络进行实时语义分割；开发自适应尺度因子算法将非度量深度预测转换为准确度量距离。

Result: 在5x4米实验室环境中测试，平均距离误差14.4厘米；30次真实环境飞行测试和100次数字孪生环境测试显示，结合分割和深度的方法增加了监视距离并减少了任务时间，成功率100%。端到端学习实现87.5%的自主任务成功率。

Conclusion: 该工作推进了结构化环境中基于视觉的无人机导航实践，解决了度量深度估计和计算效率挑战，可在资源受限平台上部署。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [14] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 提出了一种可微分的量化方法，支持多比特对数量化，在ImageNet数据集上使用ResNet18进行权重量化时，仅需15个训练周期就能达到接近全精度模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法存在两个主要问题：一是大多使用不可微分的方法，在反向传播中需要手动设置导数，影响学习能力；二是之前的移位/对数量化要么避免激活量化，要么准确率较低。

Method: 提出可微分的量化方法，支持n比特量化，特别是对数形式的2^n量化。该方法在训练过程中完全可微分，并提供了收敛性证明。

Result: 在ImageNet数据集上，仅对权重进行量化时，准确率损失小于1%；同时进行权重和激活量化时，准确率与SOTA方法相当，仅需15个训练周期。推理成本略高于1比特量化，但不需要高精度乘法。

Conclusion: 该方法提供了一种高效的可微分量化方案，在保持高准确率的同时显著减少了计算和内存需求，特别适合资源受限的环境。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [15] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出使用检索增强生成方法，通过图数据库和Cypher查询语言作为LLM工具，来解决大型3D场景图在自然语言理解中的扩展性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D场景图序列化为文本放入LLM上下文窗口，但这种方法无法扩展到大型或丰富的3D场景图

Method: 使用检索增强生成，将3D场景图编码到图数据库中，为LLM提供Cypher查询语言接口来检索相关数据

Result: 在指令跟随和场景问答任务上的评估显示，该方法相比基线方法在大规模丰富图上扩展性显著更好，性能大幅提升，同时大幅减少场景图内容的token数量

Conclusion: 使用Cypher作为3D场景图接口的方法在本地和云端模型上都能显著扩展到大型丰富图，在语言接地任务中带来大的性能改进

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [16] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出StripRFNet用于道路损坏检测，通过形状感知、条状感受野和小尺度增强三个模块，在RDD2022基准测试中取得最优性能，F1分数达80.33%，同时保持实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 道路损坏检测对实现可持续发展目标至关重要，但现有方法面临损伤形状多样、细长裂缝难以捕捉、小尺度损伤识别错误率高等挑战。

Method: StripRFNet包含三个核心模块：形状感知模块(SPM)使用大可分离核注意力增强形状判别；条状感受野模块(SRFM)采用大条状卷积和池化捕捉细长裂缝特征；小尺度增强模块(SSEM)利用高分辨率P2特征图和动态上采样提升小目标检测。

Result: 在RDD2022基准测试中，中国子集的F1分数、mAP50和mAP50:95分别比基线提高4.4、2.9和3.4个百分点；在完整数据集上达到80.33%的最高F1分数，优于CRDDC'2022和ORDDC'2024参赛方法。

Conclusion: StripRFNet在准确性和实时效率方面均达到最先进水平，为智能道路维护和可持续基础设施管理提供了有前景的工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [17] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms是一种在训练和推理时通过对象特定变换来量化和减少视觉目标检测不确定性的技术，包括颜色空间扰动和扩散模型生成，能提高检测精度并量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中基于视觉的目标检测神经网络容易受到数据偏差和分布偏移等不确定性因素的影响，需要可靠的不确定性量化方法来提升感知系统的安全性。

Method: 在训练时对单个对象进行颜色空间扰动以增强对光照和颜色变化的鲁棒性，并使用扩散模型生成多样化的行人实例；在推理时对检测到的对象应用扰动，利用检测得分的方差来实时量化预测不确定性。

Result: 在NuImages 10K数据集上使用YOLOv8的实验表明，该方法在所有对象类别上都带来了显著的精度提升和不确定性减少，在推理时对假阳性预测了比真阳性更高的不确定性值。

Conclusion: ObjectTransforms作为一种轻量级但有效的机制，分别在训练和推理阶段能够减少和量化基于视觉的感知不确定性。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [18] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 构建了一个荔枝检测和成熟度分类的开源数据集，包含11,414张图像，涵盖不同品种、天气条件和成熟阶段，并进行了统计分析和深度学习模型评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中具有一致和全面标注的开源荔枝数据集，这限制了基于视觉的荔枝采摘机器人的发展。

Method: 采集了多种荔枝品种在不同天气条件和时间段的彩色图像，通过数据增强扩展数据集，并由多人独立标注后统一验证，确保标注一致性。

Result: 数据集包含11,414张图像（878原始RGB、8,780增强RGB、1,756深度图像），标注了9,658对荔枝检测和成熟度分类标签，并进行了详细的统计分析。

Conclusion: 该数据集为荔枝检测和成熟度分类研究提供了高质量的基础数据，有助于推动基于视觉的荔枝采摘机器人技术的发展。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [19] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的自监督框架，通过回归任务分类骨骼标志点，旨在自动化缺血性卒中取栓手术的关键环节，提高效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 取栓手术是治疗缺血性卒中的有效方法，但资源密集且对人员要求高。希望通过深度学习自动化关键环节来提升手术效率和安全性。

Method: 采用自监督学习框架，通过基于回归的前置任务来分类各种骨骼标志点。

Result: 实验表明该模型在回归和分类任务上均优于现有方法，位置前置任务显著提升了下游分类性能。

Conclusion: 该框架为取栓手术的自动化提供了有效方案，未来将扩展至完全自主的C臂控制，优化从骨盆到头部的轨迹规划。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [20] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H是一个用于单目图像多任务学习的轻量级框架，通过窗口跨任务注意力模块实现语义分割、深度、边缘和表面法线估计，在保持计算效率的同时提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，能够利用互补任务信息同时最小化计算开销。

Method: 提出窗口跨任务注意力模块，在轻量级ViT-based DINOv2骨干网络上实现结构化特征交换，同时保留任务特定细节。

Result: 在NYUDv2数据集上超越最先进的多任务模型，在Hypersim上超过单任务深度和语义基线，在Cityscapes上表现优异，且在笔记本电脑硬件上保持计算效率。

Conclusion: M2H为单目空间感知系统提供了实用基础，支持动态环境中的3D场景图构建，并在真实世界数据中验证了其实用性。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [21] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch是一种新颖的双分支半监督医学图像分割框架，采用异步优化策略，通过解耦的dropout扰动和配对CutMix交叉引导来提升模型鲁棒性和多样性，在脑MRI分割任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据稀缺，半监督学习具有重要价值。现有的师生框架存在联合优化导致收敛困难和稳定性问题，特别是在挑战性场景下。

Method: 提出DuetMatch双分支框架，每个分支异步优化编码器或解码器；引入解耦dropout扰动增强正则化；设计配对CutMix交叉引导交换伪标签；提出一致性匹配机制利用冻结教师模型稳定预测来精炼标签。

Result: 在ISLES2022和BraTS等脑MRI分割基准数据集上的大量实验表明，DuetMatch在多种半监督分割场景下始终优于最先进方法，证明了其有效性和鲁棒性。

Conclusion: DuetMatch通过异步优化、解耦扰动和一致性匹配等创新设计，有效解决了半监督医学图像分割中的收敛稳定性和噪声鲁棒性问题，为医学图像分析提供了有力工具。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [22] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出了一种自主导航C臂到预定义解剖标志的管道，利用X射线图像预测3D位移向量，结合不确定度估计和保形预测确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前临床工作流程依赖手动对齐C臂，这会增加辐射暴露和手术延迟，需要自动化解决方案来提高效率和安全性。

Method: 使用X射线图像预测3D位移向量，结合概率损失和骨骼姿态正则化，采用保形预测校准不确定度，生成3D置信区域。

Result: 在DeepDRR生成的合成X射线数据集上验证，显示出强大的定位精度和良好校准的预测边界。

Conclusion: 该管道有潜力成为安全可靠自主C臂系统的组成部分，能够减少辐射暴露并提高手术效率。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [23] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出一个公式来估计图像质量评估引擎的成本节省，并在背景修复用例中展示了51.61%的成本节省


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型产生的图像质量仍达不到传统摄影标准，手动图像质量评估过程缓慢且昂贵，需要自动预过滤来降低成本

Method: 开发了一个公式来估计基于精度和通过率的通用IQA引擎的成本节省，并在背景修复用例中使用简单的AutoML解决方案进行验证

Result: 在背景修复用例中实现了51.61%的显著成本节省

Conclusion: 自动图像质量评估预过滤可以显著降低获取高质量图像的平均成本

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [24] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 该论文提出PRISM模型，通过将fMRI信号投影到结构化文本空间作为中间表示来重建视觉刺激，利用面向对象的扩散模块和属性关系搜索模块，在真实数据集上比现有方法减少了8%的感知损失。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习的重要挑战。目前从fMRI信号重建图像的方法依赖于将信号转换为潜在空间，但哪种潜在空间最适合这种转换以及如何有效组织该空间仍不明确。

Method: 提出PRISM模型：1) 将fMRI信号投影到结构化文本空间；2) 使用面向对象的扩散模块通过组合单个对象生成图像以减少对象检测错误；3) 属性关系搜索模块自动识别与神经活动最匹配的关键属性和关系。

Result: 在真实世界数据集上的广泛实验表明，该框架优于现有方法，实现了高达8%的感知损失减少。

Conclusion: 研究结果表明，使用结构化文本作为中间空间来桥接fMRI信号和图像重建具有重要意义，fMRI信号与语言模型的文本空间比基于视觉的空间或联合文本图像空间更相似。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [25] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提出数据为中心的人工智能方法来解决热带农业遥感制图的挑战，强调数据质量比模型更重要，并推荐了25种数据策略和9种最成熟的方法。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感面临缺乏高质量标注数据、标注成本高、数据变异性和区域泛化等独特挑战，传统以模型为中心的方法效果有限。

Method: 采用数据为中心的人工智能视角，重点使用置信学习、核心集选择、数据增强和主动学习等技术来提升数据质量。

Result: 确定了25种适用于大规模农业制图的数据策略，并提出了使用9种最成熟方法的实用流程。

Conclusion: 数据为中心的方法更适合热带农业的动态现实，通过数据质量提升可以显著改善AI模型在热带农业制图中的表现。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [26] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种名为StretchySnake的灵活训练方法，通过动态采样不同时空分辨率的视频并插值模型权重，使状态空间模型能够适应各种时空尺度的视频，在动作识别任务中显著优于transformer和SSM基线。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解训练方法主要针对transformer设计，无法充分利用状态空间模型的独特属性。固定分辨率和视频长度的训练导致模型在未见过的时空分辨率上性能下降，限制了模型处理不同时长视频的能力。

Method: 提出灵活训练方法：在训练过程中采样不同时空分辨率的视频，动态插值模型权重以适应任意时空尺度。比较了五种不同的灵活训练变体，确定了最适合视频SSM的策略。

Result: 在短动作（UCF-101、HMDB-51）和长动作（COIN、Breakfast）基准测试中，StretchySnake比transformer和SSM基线性能提升高达28%，在细粒度动作（SSV2、Diving-48）上表现出强大的适应性。

Conclusion: 该方法提供了一个简单的即插即用训练方案，使视频SSM在各种动作识别场景中更加鲁棒、分辨率无关且高效。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [27] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出VM-BeautyNet，一种融合Vision Transformer和Mamba视觉模型的异构集成架构，用于面部美感预测，在SCUT-FBP5500数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型难以捕捉全局面部特征，而ViT虽然能建模长距离空间关系但存在二次复杂度问题，需要结合线性复杂度的Mamba模型来互补优势。

Method: 使用异构集成架构，ViT骨干网络捕捉全局面部结构和对称性，Mamba骨干网络以线性复杂度建模长距离依赖关系，专注于序列特征和纹理。

Result: 在SCUT-FBP5500数据集上，PC达到0.9212，MAE为0.2085，RMSE为0.2698，均优于现有方法。通过Grad-CAM可视化验证了两个骨干网络的互补特征提取。

Conclusion: VM-BeautyNet为计算美学提供了强大的新架构范式，通过融合ViT和Mamba的互补优势，在面部美感预测任务中表现出色，并为模型决策过程提供了新的可解释性见解。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [28] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 开发了一个用于检测口腔鳞状细胞癌(OCSCC)的卷积神经网络系统，包括CNN模型和图像采集硬件，研究了图像分辨率对检测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: OCSCC是头颈部最常见的癌症，但由于早期症状不明显、生长缓慢且发生在隐蔽区域，往往难以早期发现，导致可预防的死亡。需要开发有效的早期检测方法。

Method: 训练了一个CNN模型，使用4293张训练图像（包括良性和恶性肿瘤以及阴性样本），设计了图像采集硬件系统，测试了不同分辨率图像对检测准确性的影响。

Result: 图像分辨率越高，预测准确性越高，但呈对数增长趋势，表明更高像素数带来的收益递减。开发了应用程序来促进测试过程并提供CNN的开放访问。

Conclusion: CNN结合图像采集硬件可以有效检测OCSCC，图像分辨率对检测准确性有重要影响，但存在收益递减现象。该系统为OCSCC的早期检测提供了有效工具。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [29] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个大规模多模态3D运动数据集，包含500小时来自439名参与者的3D运动数据，总计超过5400万帧跟踪数据，涵盖单人和多人行为场景。


<details>
  <summary>Details</summary>
Motivation: 构建一个全面的3D人体运动数据集，支持单人和多人行为分析研究，包括手势、情感对话和协作活动等多种场景。

Method: 在多相机采集环境中收集439名参与者的3D运动数据，包括身体追踪、手部追踪和身体形状数据，并提供文本注释和单独音频轨道。

Result: 成功创建了包含500小时3D运动数据的大规模数据集，涵盖单人和多人的各种行为模式，为人体运动分析研究提供了丰富资源。

Conclusion: Embody 3D数据集为3D人体运动和行为分析研究提供了宝贵资源，支持广泛的研究应用，包括手势识别、情感分析和社交互动研究。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [30] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出了一种基于人-物交互的主动场景分解与重建方法，通过观察人类行为动态优化场景分解过程，解决静态物体级重建中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 人类行为是场景动态的主要来源，包含丰富的动态线索。传统静态物体级重建方法存在固有模糊性，需要利用人类与物体的交互来动态优化分解和重建过程。

Method: 使用高斯泼溅技术，通过观察人-物交互来迭代式分解和重建环境。系统整合了相机和物体姿态估计、实例分解、在线地图更新等多个任务，利用第一人称视角视频流中的交互线索。

Result: 在多个真实场景中验证了方法的有效性，实现了准确一致的动态场景建模，具有逼真高效的渲染效果。

Conclusion: 该方法为传统物体级重建方法提供了灵活、渐进式的替代方案，能够利用人类行为线索动态优化场景分解和重建过程。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [31] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus是一个用于实时视频异常检测的两级级联系统，通过轻量级过滤和细粒度视觉语言模型推理，在保持高精度的同时实现151.79倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决基于视觉语言模型的视频异常检测方法计算成本高、视觉定位性能不稳定的问题，实现实时部署。

Method: 采用两级级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理，使用运动掩码提示和基于规则的偏差检测。

Result: 在四个数据集上的评估显示，平均达到57.68 fps（NVIDIA L40S GPU），速度提升151.79倍，准确率97.2%，与最先进的VLM-based VAD方法相当。

Conclusion: Cerberus是实时视频分析的实际可行解决方案，在保持高精度的同时显著提升了检测速度。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [32] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了评估大型视觉语言模型成员推理攻击时存在的基本挑战，指出先前的高成功率主要源于检测数据集构建中的分布偏差而非真实成员状态。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击评估存在偏差，高成功率往往源于数据集构建中的分布偏差，而非真正识别成员状态，需要一个无偏的基准测试来准确评估攻击效果。

Method: 引入包含6000张图像的受控基准测试，精心平衡成员和非成员样本的分布，并提供三个不同训练阶段的真实成员标签。

Result: 在无偏条件下，最先进的成员推理攻击方法性能收敛到随机猜测水平，表明先前的高成功率主要来自数据集偏差。

Conclusion: OpenLVLM-MIA为LVLM成员推理攻击研究提供了透明无偏的基准，澄清了当前方法的局限性，为开发更强的隐私保护技术奠定了基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [33] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一个无需训练的新框架，通过跨图像笔画注意力机制实现参考风格的笔画属性精确迁移，同时保持语义结构和内容保真度。


<details>
  <summary>Details</summary>
Motivation: 生成具有参考风格的草图需要精确迁移笔画属性（如线条粗细、变形和纹理稀疏度），同时保持语义结构和内容保真度。

Method: 提出跨图像笔画注意力机制嵌入自注意力层，建立细粒度语义对应关系；开发自适应对比度增强和语义聚焦注意力来强化内容保留和前景强调。

Result: Stroke2Sketch能够有效合成风格忠实的草图，与手绘结果高度相似，在表达性笔画控制和语义连贯性方面优于现有方法。

Conclusion: 该方法实现了精确的笔画属性迁移，同时保持了结构完整性和内容保真度，为风格化草图生成提供了有效的训练免费解决方案。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [34] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 该论文系统研究了深度伪造检测任务的缩放规律，发现检测错误率随真实图像域数量和深度伪造方法数量的增加呈幂律衰减，类似于大语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 由于现有数据集规模不足，无法系统研究深度伪造检测的缩放规律，因此构建了目前最大的深度伪造数据集ScaleDF来填补这一空白。

Method: 构建包含580万张真实图像（来自51个不同域）和880万张伪造图像（由102种深度伪造方法生成）的ScaleDF数据集，并系统分析模型性能与真实域数量、伪造方法数量和训练图像数量的关系。

Result: 观察到平均检测错误率随真实域数量或深度伪造方法数量的增加呈可预测的幂律衰减，能够预测达到目标性能所需的额外资源。

Conclusion: 缩放规律不仅有助于预测性能提升所需的资源，还为以数据为中心的方式应对不断发展的深度伪造技术提供了启示，同时研究了预训练和数据增强在缩放中的作用以及缩放本身的局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [35] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT是一个新的扩散框架，通过分层局部注意力和低分辨率全局引导，实现了超高清图像的高效、可扩展和语义一致的生成，能够达到4K分辨率而无需额外的高分辨率训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受限于注意力机制的二次复杂性和原生4K训练数据的稀缺，无法生成超高清图像，需要一种能够高效处理高分辨率图像的新方法。

Method: 采用分层局部注意力机制，将高分辨率潜在空间划分为固定大小的局部窗口来降低注意力复杂度；使用低分辨率潜在空间提供全局语义指导；通过轻量级LoRA适配器连接全局和局部路径；采用Hilbert曲线重排token序列和融合内核优化推理效率。

Result: Scale-DiT相比密集注意力基线实现了2倍以上的推理加速和更低的内存使用，能够可靠地扩展到4K×4K分辨率，在FID、IS、CLIP Score等定量指标和定性比较中都表现出优越的全局一致性和更锐利的局部细节。

Conclusion: 分层局部注意力配合引导的低分辨率锚点是推进超高清图像生成的有效方法，能够在不需要额外高分辨率训练数据的情况下实现高质量的4K图像生成。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [36] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX是一个云边协同框架，通过轻量级设备端模型快速生成预览图像，云端模型进行最终优化，减少生成时间15.8%，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成过程计算密集，用户需要多次迭代优化提示词，增加了延迟和云资源负担。

Method: 提出云边协作框架，设备端使用轻量扩散模型快速生成预览，云端使用高容量模型进行最终优化，并引入噪声水平预测器动态平衡计算负载。

Result: 相比Stable Diffusion v1.5减少平均生成时间15.8%，图像质量相当；比Tiny-SD仅慢0.9%但图像质量显著提升。

Conclusion: DiffusionX在最小开销下实现了效率和可扩展性，有效平衡了延迟和云工作负载。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [37] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出了TokenAR框架，通过token级增强机制解决多参考图像生成中的身份混淆问题，包含token索引嵌入、指导token注入和身份token解缠策略。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在条件图像生成中表现出色，但在多参考生成中难以解耦不同参考身份，存在身份混淆问题。

Method: TokenAR框架包含三个部分：1) Token Index Embedding聚类token索引以更好表示相同参考图像；2) Instruct Token Injection作为额外视觉特征容器注入详细补充先验；3) 身份token解缠策略(ITD)显式引导token表示独立表示每个身份特征。

Result: 该方法显著增强了现有基于AR的方法在条件图像生成中的能力，在保持高质量背景重建的同时实现良好的身份一致性。

Conclusion: 综合实验验证该方法在多参考图像生成任务中超越了当前最先进模型，并引入了首个开源的大规模多参考输入数据集InstructAR Dataset。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [38] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 该论文挑战了多模态语言模型性能主要继承自LLM骨干网络的假设，发现训练策略（特别是强化学习）会重塑视觉编码器的表示能力，并提出了PIVOT方法来高效构建更强的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究过度关注LLM骨干网络而忽视视觉编码器的作用，特别是在训练范式从监督微调转向强化学习后，缺乏对视觉编码器如何被重塑的分析。

Method: 通过多种实验分析训练策略对视觉编码器的影响，包括ImageNet分类、分割和梯度可视化，并提出了PIVOT方法来优化视觉编码器。

Result: 强化学习相比监督微调能产生更强且更精确定位的视觉表示，PIVOT训练的视觉编码器性能优于更大规模训练的对等模型，且计算成本不到标准视觉预训练的1%。

Conclusion: 训练策略对MLLM的视觉表示有根本性影响，PIVOT方法为高效提升MLLM视觉骨干网络提供了有效路径。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [39] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 提出了GradNorm框架，通过梯度范数来过滤语义接近图像的正名词，以改进语言辅助图像聚类性能


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP特征空间的过滤策略缺乏严格理论基础，需要一种理论保证且性能优越的正名词过滤方法

Method: 基于梯度反向传播的框架，通过预测目标分布与softmax输出之间的交叉熵梯度幅度来度量名词的正面性

Result: 在多个基准测试中达到了最先进的聚类性能

Conclusion: GradNorm不仅提供了严格的理论保证，而且自然包含了现有过滤策略作为特例，在实证中表现出色

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [40] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 提出了针对社交制造中缺陷检测的MIRAD数据集，这是首个专门为该领域设计的基准数据集，包含多样化个性化产品、地理分布制造节点和显著成像异质性，评估显示现有方法在该数据集上性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 社交制造通过社区协作和分散资源实现大规模个性化，但带来了质量控制挑战，特别是在缺陷检测方面，主要困难包括产品高度定制化、生产碎片化小批量订单以及分布式站点成像环境差异大。

Method: 引入MIRAD数据集，捕捉社交制造三个关键维度：多样化个性化产品、六个地理分散制造节点数据收集、显著成像异质性。对最先进的异常检测方法进行广泛评估，涵盖单类、多类和零样本方法。

Result: 与传统基准相比，所有模型在MIRAD数据集上都表现出显著性能下降，突显了现实世界个性化生产中缺陷检测的未解决复杂性。

Conclusion: MIRAD通过弥合工业需求和学术研究，为开发工业5.0所需的稳健质量控制解决方案提供了现实基础，数据集已公开可用。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [41] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出了包含3000个白内障手术视频的数据集，具有四个标注层：手术阶段、实例分割、器械-组织交互追踪和技能评分，并提供了基准实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏多样性和深度标注，难以训练泛化性强的深度学习模型。

Method: 收集来自两个手术中心的3000个白内障手术视频，包含四个标注层，并进行基准实验验证数据集质量。

Result: 建立了高质量的白内障手术数据集，支持手术AI任务如工作流识别、场景分割和自动技能评估，并提供了跨中心域适应基线。

Conclusion: 该数据集填补了白内障手术AI研究的资源空白，为开发计算机辅助手术系统提供了重要基础。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [42] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个用于印度道路坑洞实时检测、GPS地理标记和道路健康可视化的端到端自动化平台，基于YOLO模型和OpenStreetMap，支持智能治理和自动问责。


<details>
  <summary>Details</summary>
Motivation: 印度道路网络多样且维护不足，道路坑洞带来严重安全隐患和维护挑战，需要自动化解决方案来改善道路基础设施维护。

Method: 使用7000多张印度道路条件的自标注数据集微调YOLO模型进行坑洞检测，结合OCR提取的时间戳和外部GPS日志进行精确定位，通过优化后端数据库管理道路段和承包商信息。

Result: 开发了完整的自动化平台，能够实时检测坑洞、精确地理定位、自动发送警报给承包商和官员，并提供直观的Web界面供利益相关者和公众使用。

Conclusion: iWatchRoadv2通过自动化完整的坑洞监测生命周期，实现了数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [43] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter是一个数据驱动的参数化植物形态模型，能够编码植物的拓扑结构、形状、关节运动和变形，支持不同物种的拓扑变化，并模拟三种形状变化来源。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型主要针对人类和动物，缺乏对植物的同等表达能力，需要开发能够处理植物形态多样性的参数化模型。

Method: 提出Demeter参数化模型，通过收集大规模大豆农场数据集，编码植物的拓扑、形状、关节运动和变形等关键因素到学习表示中。

Result: 实验表明Demeter能有效合成形状、重建结构并模拟生物物理过程，在作物植物建模方面取得进展。

Conclusion: Demeter为植物建模提供了强大的参数化工具，能够处理不同物种的拓扑变化和多种形状变化来源，具有广泛的应用前景。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [44] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 提出了一种用于AR/VR设备的轻量级深度学习框架，采用编码器-解码器架构，通过稀疏卷积、SPLite解码器和量化感知训练，在Raspberry Pi 5上实现了2.98倍加速，同时保持与最先进方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备的普及，边缘设备上的深度学习模型部署面临实时推理、低功耗和低延迟的挑战，需要在效率和性能之间取得平衡。

Method: 使用ResNet-18作为主干网络，应用稀疏卷积利用手部姿态图像的固有稀疏性；提出SPLite解码器架构；采用量化感知训练来优化性能。

Result: 端到端效率提升42%；在Raspberry Pi 5上解码帧率提升3.1倍；量化后内存使用减少，精度损失极小（PA-MPJPE从9.0mm仅增加到9.1mm）；整体在Raspberry Pi 5 CPU上实现2.98倍加速。

Conclusion: 该方法在保持与最先进方法相当精度的同时，显著提升了计算效率，适用于AR/VR边缘设备的实时深度学习应用。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [45] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM是一个创新的多模态大语言模型代理框架，能够在3D高斯泼溅表示上直接进行基于推理的分割，无需大量3D特定后训练。


<details>
  <summary>Details</summary>
Motivation: 弥合复杂人类指令与精确3D物体定位之间的差距是视觉和机器人领域的重大挑战。现有3D分割方法难以解释模糊的基于推理的指令，而擅长此类推理的2D视觉语言模型缺乏内在的3D空间理解。

Method: 在3D高斯泼溅表示上直接进行分割，利用其渲染逼真新视图的能力。提出全局到局部空间定位策略：首先将多个全局视图并行输入MLLM代理进行粗粒度定位，然后合成多个物体特写视图进行细粒度局部分割。

Result: 在LERF、3D-OVS和新提出的REALM3D基准测试中，REALM在解释显式和隐式指令方面表现出色。该代理框架还无缝支持物体移除、替换和风格转换等3D交互任务。

Conclusion: REALM框架展示了在开放世界基于推理的3D分割方面的实用性和多功能性，能够有效处理复杂的人类指令。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [46] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL是一个新颖的框架，利用自监督学习任务作为强化学习微调的可验证奖励信号，解决了视觉语言模型在视觉证据利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型往往无法充分使用视觉证据，要么依赖语言先验，要么使用文本捷径推理。强化学习虽然能对齐模型行为，但缺乏可扩展可靠的奖励机制。

Method: 将自监督学习目标（如图像旋转预测、掩码补丁重建）转化为密集、自动的奖励信号，无需人工偏好数据或不可靠的AI评估器。

Result: SSL4RL显著提升了在视觉中心和视觉语言推理基准上的性能，并在图学习中也取得了显著收益。

Conclusion: SSL4RL建立了一个通用有效的范式，使用可验证的自监督目标来对齐多模态模型，并识别了影响SSL4RL任务有效性的关键因素。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [47] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一个轻量级的点和线段匹配器，通过注意力线消息传递机制有效结合点和线特征，在保持高精度的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法将点和线匹配视为独立任务，而GlueStick虽然能同时处理两者，但其复杂架构无法满足实时应用或边缘设备部署需求。

Method: 提出注意力线消息传递机制，显式地将线的连通性暴露给网络，实现节点间高效通信。

Result: 在多个基准测试中达到新的最先进水平，同时显著降低计算复杂度。

Conclusion: LightGlueStick通过轻量化设计成功解决了点和线联合匹配的实时性问题，为SLAM和运动结构恢复等应用提供了高效解决方案。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [48] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 提出可解释深度伪造视频检测任务EDVD，开发EDVD-LLaMA多模态大语言模型推理框架，通过时空特征提取和细粒度思维链机制，在准确检测的同时提供可追溯的推理过程和可信解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法存在原理不透明、泛化能力不足等问题，亟需既能识别伪造内容又能提供可验证推理解释的检测器。

Method: 1. 时空细微信息标记化(ST-SIT)提取融合全局和局部跨帧深度伪造特征；2. 细粒度多模态思维链(Fg-MCoT)引入面部特征数据作为硬约束，实现像素级时空定位；3. 构建可解释推理FF++基准数据集(ER-FF++set)。

Result: EDVD-LLaMA在检测准确性、可解释性以及处理跨伪造方法和跨数据集场景方面表现出卓越性能和鲁棒性。

Conclusion: 相比传统DVD方法，EDVD-LLaMA提供了更可解释且更优越的解决方案，为深度伪造检测领域带来新的突破。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [49] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 提出一种改进的损失函数，使用高斯边界框表示和Bhattacharyya距离来提升旋转物体检测的准确性和鲁棒性，并通过各向异性高斯表示解决方形物体的问题。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测框架在处理旋转物体时表现不佳，特别是在航空影像、遥感和自动驾驶等应用中，需要更有效地捕捉方向变化。

Method: 引入基于高斯边界框表示和Bhattacharyya距离的旋转不变损失函数，采用各向异性高斯表示来处理方形物体，并将该损失函数集成到最先进的深度学习旋转物体检测器中。

Result: 大量实验表明，相比现有方法，在平均精度指标上取得了显著提升。

Conclusion: 该方法有潜力在旋转物体检测领域建立新的基准，对需要精确可靠物体定位的应用具有广泛意义。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [50] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种视觉提示初始化策略，通过将提示与嵌入空间中的语义信息区域对齐并注入新的表示方向，显著提升自监督模型的适应性能。


<details>
  <summary>Details</summary>
Motivation: 在基础模型时代，完全微调预训练网络对每个下游任务来说资源消耗巨大。现有的视觉提示调优方法在专门化提示或丰富表示空间方面存在不足，特别是在挑战性任务和数据稀缺设置中。

Method: VIPAMIN通过两个关键步骤增强自监督模型的适应：(1)将提示与嵌入空间中的语义信息区域对齐；(2)注入超越预训练子空间的新表示方向。该方法仅需单次前向传播和轻量级操作。

Result: VIPAMIN在各种任务和数据集规模上持续提升性能，在视觉提示调优领域达到了新的最先进水平。

Conclusion: VIPAMIN是一种简单有效的视觉提示初始化策略，能够显著提升自监督模型在挑战性任务和数据稀缺环境下的适应能力。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [51] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种弱监督域自适应方法，利用稀疏点标注进行线粒体分割，通过多任务学习和跨教学机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜图像中线粒体实例分割需要大量标注，无监督域自适应方法在实际应用中性能较低，因此研究利用稀疏点标注的弱监督域自适应方法。

Method: 多任务学习框架联合进行分割和中心检测，采用跨教学机制和类别聚焦的跨域对比学习，以及基于实例感知的伪标签选择策略的自训练方法。

Result: 在挑战性数据集上的验证表明，该方法优于现有的无监督和弱监督域自适应方法，显著缩小了与监督上界的性能差距。

Conclusion: 该方法在线粒体分割任务中有效利用了稀疏点标注，在弱监督和无监督设置下都取得了显著改进。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [52] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar是一个用于动态3D虚拟人高效传输和高质量渲染的分层高斯压缩框架，通过结构层和运动层的解耦设计，支持分层压缩、渐进解码和可控渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D高斯泼溅的压缩方法缺乏人体先验知识，导致解码端的比特率效率和重建质量不理想，阻碍了其在可流式3D虚拟人系统中的应用。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器将姿态映射到高斯）和运动层（利用SMPL-X模型紧凑表示时序姿态变化），并加入面部注意力机制以在低比特率下保持身份和表情细节。

Result: 实验结果表明，HGC-Avatar为快速3D虚拟人渲染提供了可流式解决方案，在视觉质量和压缩效率方面显著优于现有方法。

Conclusion: HGC-Avatar通过分层高斯压缩框架成功解决了动态虚拟人传输中的比特率效率和重建质量问题，为沉浸式通信提供了有效的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [53] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实科学论文中审稿人标记的不一致性的多模态基准测试，包含262个来自242篇论文的不一致案例，评估模型在识别、纠正和推理跨模态不一致性的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么孤立单一模态，要么依赖合成错误，无法捕捉真实世界的复杂性。大型多模态模型在科学研究中的应用日益增多，但它们在理解论文多模态复杂性方面的可靠性尚不明确。

Method: 通过多阶段流程（包括审稿挖掘、LLM辅助过滤和人工验证）收集真实的不一致案例，设计了三个任务：不一致性识别、纠正和配对匹配，并引入结构化JSON答案表示以减少语言偏见。

Result: 对21个领先的LMM进行基准测试，结果显示性能极低（26.1-54.2%），突显了多模态科学推理的挑战性。

Conclusion: 当前LMM在多模态科学推理方面表现不佳，需要进一步改进以开发可信赖的科学助手。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [54] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 提出OOS-DSD方法，通过辅助学习改进缺货检测，在YOLOv8基础上增加卷积分支同时进行缺货检测、产品分割和深度估计，性能超越现有最佳方法1.8% mAP。


<details>
  <summary>Details</summary>
Motivation: 缺货检测是重要的零售验证过程，需要准确推断货架上产品的不可用性。现有方法性能有待提升。

Method: 扩展YOLOv8架构，增加卷积分支进行多任务学习：缺货检测、产品分割和深度估计。深度估计分支使用Depth Anything V2生成的伪标签训练，并提出了深度归一化程序稳定训练。

Result: 实验结果显示该方法超越了现有最佳缺货检测方法1.8% mAP。消融研究证实辅助学习提升3.7% mAP，深度归一化提升4.2% mAP。

Conclusion: OOS-DSD通过多任务辅助学习有效提升了缺货检测性能，深度归一化程序对训练稳定性有重要贡献。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [55] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络自编码器的图像分类和检索方法，通过构建图像和类别的代表性模型来实现分类和检索。


<details>
  <summary>Details</summary>
Motivation: 开发一种代表性中心的方法，通过构建图像和类别的代表性模型来改进图像分类和检索的准确性和效率。

Method: 使用图结构表示图像及其相似关系，利用图注意力网络自编码器构建上下文感知的潜在表示，从中提取类别代表，通过比较查询图像代表与类别代表进行分类，并在识别类别内检索最相似图像。

Result: 通过实验验证了该方法在图像分类和检索任务中的有效性。

Conclusion: 基于图注意力网络自编码器的代表性中心方法能够有效提升图像分类和检索性能。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [56] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ方法通过添加token级重建和句子级对齐两个辅助目标来增强CLIP模型的组合推理能力，在五个主要基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在组合推理方面表现不佳，主要是因为文本编码器倾向于关注单个单词而非它们之间的关系，对比训练主要将单词与视觉对象对齐而忽视了语言结构

Method: READ方法在对比学习基础上添加两个辅助目标：(1) token级重建目标：使用冻结的预训练解码器基于原始标题嵌入重建替代标题；(2) 句子级对齐目标：在嵌入空间中显式对齐同义句子

Result: READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强传统微调基线提升高达4.1%。将READ应用于现有CLIP变体也能提升这些基准的性能

Conclusion: 重建和对齐目标具有互补优势：重建鼓励编码器捕捉标题内单词间的关系，而对齐确保不同措辞的同义句具有一致表示

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [57] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出GaitRDAE框架，通过动态搜索运动区域的自适应时间尺度来改进步态识别，解决了现有方法使用固定时间尺度难以建模动态变化运动区域的问题。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法使用预定义区域和固定时间尺度，难以建模随时间动态变化的运动区域和适应其特定模式，特别是在协变量影响视觉外观的情况下。

Method: 提出Region-aware Dynamic Aggregation and Excitation框架，包含两个核心模块：RDA模块动态搜索每个区域的最佳时间感受野，RDE模块强调学习包含稳定行为模式的运动区域，同时抑制对易受协变量影响的静态区域的关注。

Result: 实验结果表明，GaitRDAE在多个基准数据集上实现了最先进的性能。

Conclusion: GaitRDAE通过动态区域感知聚合和激励机制，有效提升了步态识别的准确性，特别是在处理动态变化运动区域和协变量影响方面表现出色。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [58] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本文提出了首个基于真实世界政治深度伪造事件的系统基准，评估了现有检测器在真实政治深度伪造内容上的表现，发现现有检测器泛化能力不足且易受简单攻击。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散加剧了虚假信息风险，特别是政治深度伪造会扭曲真相、破坏对政治机构的信任。现有检测模型大多在实验室合成数据集上训练，难以泛化到真实社交媒体上传播的政治深度伪造内容。

Method: 基于政治深度伪造事件数据库构建系统基准，对学术界、政府和工业界的最先进深度伪造检测器进行系统性评估，包括免费和付费工具。

Result: 学术界和政府检测器表现相对较差，付费检测工具性能相对较高，但所有评估的检测器都难以有效泛化到真实政治深度伪造内容，且在视频领域特别容易受到简单操作的影响。

Conclusion: 需要开发具有政治情境感知的深度伪造检测框架，以在真实世界环境中更好地保护公众。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [59] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD是一个无需训练的框架，通过重新加权视觉标记、引入噪声衍生标记和应用对抗攻击来缓解大型视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型存在物体幻觉问题，即生成看似合理但不准确的物体描述。本文首次将幻觉问题追溯到视觉编码器，识别出统计偏差、固有偏差和脆弱性三个关键问题。

Method: 提出SHIELD框架，包含三种策略：重新加权视觉标记以减少统计偏差，引入噪声衍生标记来对抗固有偏差，应用对抗攻击和对比解码来解决脆弱性问题。

Result: 实验表明SHIELD能有效缓解多种基准测试和LVLM家族中的物体幻觉问题，并在通用LVLM基准上表现优异。

Conclusion: SHIELD是一个无需训练的有效框架，能够广泛缓解LVLM中的物体幻觉问题，具有很好的通用性。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [60] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector是一个轻量级即插即用的视觉令牌压缩框架，通过可学习的评分模块和可微Top-K机制，实现高效且自适应的令牌选择，在各种压缩率下都能保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像或多图像输入时面临计算和内存瓶颈，现有令牌压缩技术受限于启发式规则，容易丢弃关键信息并存在注意力偏差问题。

Method: 提出VisionSelector评分模块，采用可微Top-K机制和课程退火策略，将令牌压缩重新表述为端到端可学习的决策过程，与MLLM主干解耦。

Result: 仅需12.85M可训练参数，在MME基准上以30%保留预算保持100%准确率，在10%保留预算下比先前方法提升12.14%，预填充速度提升一倍。

Conclusion: VisionSelector提供了一种高效、自适应的令牌压缩解决方案，在各种压缩预算下都能实现卓越性能，显著缓解MLLM的计算和内存瓶颈。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [61] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出了一种用于实时医学图像分析的深度学习框架，整合了U-Net、EfficientNet和Transformer等先进神经网络架构，通过模型剪枝、量化和GPU加速实现实时优化，在多个医学影像模态上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理技术缺乏实时临床使用所需的精度、鲁棒性和速度，且高分辨率放射学数据解读耗时且存在临床医生间的变异性。

Method: 整合U-Net、EfficientNet和Transformer等神经网络架构，采用模型剪枝、量化和GPU加速等实时优化策略，支持在边缘设备、本地服务器和云基础设施上的灵活部署。

Result: 在公共基准数据集上实现分类准确率超过92%，分割Dice分数超过91%，推理时间低于80毫秒，并通过Grad-CAM和分割覆盖等可视化工具增强可解释性。

Conclusion: 该框架能显著加速诊断工作流程，减少临床医生工作量，并在时间关键的医疗环境中支持可信赖的AI集成。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [62] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: 提出了MultiVerse多轮对话基准测试，包含647个对话，涵盖12个VLM评估基准的484个任务，使用GPT-4o作为自动评估器评估18个VLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮对话数据集无法充分捕捉真实应用中复杂对话场景的广度和深度，需要更全面的评估基准。

Method: 从12个流行的VLM评估基准中提取647个对话，每个对话平均4轮，提出基于检查表的评估方法，使用GPT-4o评估37个关键方面。

Result: 即使在最强的模型（如GPT-4o）上，复杂多轮对话的成功率也仅为50%，提供完整对话上下文能显著提升较弱模型的性能。

Conclusion: MultiVerse是评估VLM多轮交互能力的有效基准，揭示了当前模型在复杂对话中的局限性，强调了上下文学习的重要性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [63] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了针对病理学基础模型的通用可转移对抗扰动(UTAP)，这是一种固定且微弱的噪声模式，能系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能下降。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型的关键脆弱性，为模型鲁棒性评估建立高标准基准，推动防御机制发展，确保AI在病理学中的安全可靠部署。

Method: 使用深度学习优化生成固定的微弱噪声模式(UTAP)，该扰动可添加到病理图像中，系统性地破坏基础模型的特征表示能力。

Result: UTAP在各种最先进的病理学基础模型和多个数据集上都能显著降低模型性能，且具有通用性(跨不同视野)和可转移性(影响未见过的黑盒模型)。

Conclusion: UTAP构成了对各种新兴病理学基础模型及其应用的广泛威胁，强调了推进防御机制和对抗训练的必要性，为模型鲁棒性评估提供了关键基准。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [64] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 本文提出HYDRA框架，通过教师-学生知识蒸馏方法实现高质量的光谱重建，在各项指标上达到SOTA性能，准确率提升18%，推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 解决现有多尺度注意力方法只能处理稀疏光谱的问题，适应现代高光谱传感器包含数百个通道的需求。

Method: 使用教师模型封装潜在高光谱图像数据，学生模型学习从自然图像到教师编码域的映射，结合新颖的训练方法。

Result: 在所有指标上实现SOTA性能，准确率提升18%，在不同通道深度下推理时间均快于当前SOTA模型。

Conclusion: HYDRA框架成功解决了先前光谱重建模型的关键限制，为高光谱图像应用提供了高质量的解决方案。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [65] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出MSSR框架，通过构建最小充分信息集来解决VLM在空间推理中的3D理解不足和信息冗余问题，显著提升性能并生成可解释的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理方面存在两个瓶颈：基于2D预训练导致的3D理解能力不足，以及冗余3D信息引发的推理失败。

Method: 采用双智能体框架：感知智能体使用感知工具箱程序化查询3D场景提取充分信息，包括新颖的SOG模块；推理智能体迭代优化信息追求最小化，剪枝冗余细节并请求缺失信息。

Result: 在多个基准测试中显著提高准确率并达到最先进性能，同时生成高质量的训练数据。

Conclusion: 通过同时追求充分性和最小化，MSSR框架有效解决了空间推理中的核心挑战，为未来模型提供了有前景的训练数据来源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [66] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出SDPA++框架，仅使用噪声OCT图像进行自监督去噪，通过自融合生成伪真实图像，采用基于块的策略训练去噪模型，在真实世界数据集上验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: OCT图像分析对眼科疾病诊断至关重要，但获取成对的干净和噪声OCT图像数据集具有挑战性，因为存在固有散斑噪声和临床成像环境的实际限制。

Method: 提出SDPA++框架，仅使用噪声OCT图像，通过自融合和自监督去噪生成伪真实图像，然后使用基于块的策略训练去噪模型集成。

Result: 在IEEE SPS视频和图像处理杯的真实世界数据集上，通过对比度噪声比、均方比、纹理保持和边缘保持等指标验证了性能改进。

Conclusion: 该方法在仅包含真实世界噪声OCT图像的数据集上表现出色，展示了在临床实践中提高图像质量和诊断结果的潜力。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [67] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 提出了一种新的领域连接对比学习（DCCL）方法，通过增强跨领域的类内连通性来解决领域泛化问题，在五个标准基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 领域偏移问题严重影响模型泛化性能，虽然对比学习理论上能学习类分离表示，但直接应用反而会损害领域泛化性能，原因是缺乏类内连通性。

Method: 提出DCCL方法：数据层面使用更激进的数据增强和跨领域正样本；模型层面提出模型锚定技术，利用预训练表示中的类内连通性，并结合生成变换损失。

Result: 在五个标准领域泛化基准测试中，DCCL无需领域监督即可超越最先进的基线方法。

Conclusion: DCCL通过增强跨领域类内连通性，有效解决了领域泛化问题，证明了类内连通性对领域泛化的重要性。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [68] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一个基于一致性模型的一步人体运动预测框架，相比扩散模型大幅减少推理步骤，同时保持或超越现有方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型需要多步去噪过程，推理效率较低。本文旨在开发一种高效的单步生成方法，减少推理时间。

Method: 采用基于Transformer的时空架构，结合时间嵌入来建模长距离依赖关系并保持运动连贯性，学习噪声和干净运动状态之间的自一致映射。

Result: 在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM在保持与最先进扩散模型相当或更优准确性的同时，将推理步骤减少了两个数量级。

Conclusion: HumanCM证明了通过一致性模型实现高效单步人体运动预测的可行性，为实时应用提供了有前景的解决方案。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [69] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出了SCENECOT框架，通过将复杂3D场景推理任务分解为简单问题并构建视觉线索，实现了首个成功的3D场景CoT推理方法，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在基于场景的问答方面存在困难，主要原因是缺乏对人类场景-对象推理机制的研究。

Method: 引入基于3D场景的链式思维推理方法(SCENECOT)，将复杂任务分解为简单问题，利用多模态专家模块构建视觉线索，并创建了包含18.5万个实例的大规模数据集。

Result: 在多个复杂3D场景推理基准测试中表现出强大的性能，并实现了高水平的问答一致性。

Conclusion: 这是首次成功将CoT推理应用于3D场景理解，实现了逐步的人类式推理，并显示出扩展到更广泛3D场景理解场景的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [70] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM是一个隐式残差世界模型，专注于建模世界当前状态和演化，通过预测残差变化而非完整重建未来场景，提高自动驾驶系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统依赖视觉中心的世界模型，但完全重建未来场景会浪费大量容量在冗余的静态背景建模上。

Method: IR-WM首先从视觉观测建立鲁棒的鸟瞰图表示，利用前一时刻BEV特征作为时序先验，仅预测基于自车动作和场景上下文的残差变化，并应用对齐模块校准语义和动态错位。

Result: 在nuScenes基准测试中，IR-WM在4D占用预测和轨迹规划方面均达到顶级性能。

Conclusion: 隐式残差建模方法能有效减少冗余计算，世界模型生成的隐式未来状态显著提高了规划准确性。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [71] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer是一种新颖的语义分割模型，用于在Allen Coral Atlas的噪声监督下实现高精度珊瑚礁映射，通过架构设计缓解标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁产品如Allen Coral Atlas在空间精度和语义一致性方面存在局限，特别是在需要精细边界描绘的区域，这影响了珊瑚礁保护的准确性。

Method: 基于UKAN架构，在解码器中加入全局-局部变换器（GL-Trans）模块，同时提取全局语义结构和局部边界细节，在噪声标签设置下进行训练。

Result: UKANFormer在珊瑚类IoU达到67.00%，像素精度达到83.98%，优于传统基线方法，其预测结果在视觉和结构上都比训练用的噪声标签更准确。

Conclusion: 模型性能不直接受数据质量限制，架构设计可以缓解标签噪声，为在不可靠标签稀缺情况下的生态监测提供了基础。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [72] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个用于具身AI中世界模型的统一框架，包括问题形式化、学习目标和三轴分类法，系统性整理了数据资源和评估指标，并对现有模型进行了定量比较，指出了关键开放挑战。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器，捕捉环境动态，支持感知、预测和决策制定。

Method: 提出了一个三轴分类法：(1) 功能性：决策耦合vs通用目的；(2) 时序建模：序列模拟推理vs全局差异预测；(3) 空间表示：全局潜在向量、令牌特征序列、空间潜在网格和分解渲染表示。

Result: 系统整理了机器人学、自动驾驶和通用视频设置中的数据资源和指标，覆盖像素预测质量、状态级理解和任务性能，并对最先进模型进行了定量比较。

Conclusion: 指出了关键开放挑战：统一数据集的稀缺性、需要评估物理一致性而非像素保真度的指标、模型性能与实时控制所需计算效率之间的权衡，以及实现长时序一致性同时减轻误差累积的核心建模难度。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [73] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 研究表明，离散自回归模型通过束搜索在图像生成中优于连续扩散模型，2B参数模型超越12B参数模型，显示架构对推理时优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然搜索策略在语言模型中取得显著效果，但在图像生成中应用有限，特别是连续扩散模型效果不佳，需要探索离散自回归模型的潜力。

Method: 使用离散自回归视觉模型，应用束搜索策略进行图像生成，利用离散token空间实现早期剪枝和计算重用。

Result: 束搜索显著提升文本到图像生成质量，2B参数自回归模型在多个基准测试中超越12B参数扩散模型。

Conclusion: 模型架构（而非仅规模）对视觉生成中的推理时优化至关重要，离散token空间为有效搜索提供了优势。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [74] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出了一种基于显著性的超分辨率伪影评估方法，构建了包含1302个伪影样本的数据集，并训练了轻量级回归器来生成空间显著性热图。


<details>
  <summary>Details</summary>
Motivation: 随着超分辨率模型能力的增强，它们倾向于产生伪影，这些伪影的感知影响各不相同，应该根据其对人类观察者的显著程度来表征，而不是作为统一的二元缺陷处理。

Method: 构建了包含1302个伪影示例的数据集，每个伪影都配有众包显著性评分，并基于此训练了一个轻量级回归器来生成空间显著性热图。

Result: 训练的回归器在检测显著伪影方面优于现有方法，能够产生空间显著性热图。

Conclusion: 释放了数据集和代码，以促进基于显著性的超分辨率伪影评估和缓解。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [75] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 提出WaMaIR框架，通过全局多尺度小波变换卷积扩大感受野，结合Mamba通道感知模块捕获长距离依赖，并使用多尺度纹理增强损失函数，显著提升图像恢复中的纹理细节重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在图像恢复中难以充分恢复精细纹理细节，主要受限于CNN结构的小感受野和缺乏通道特征建模能力。

Method: 1) 全局多尺度小波变换卷积(GMWTConvs)扩大感受野；2) Mamba通道感知模块(MCAM)捕获通道间长距离依赖；3) 多尺度纹理增强损失(MTELoss)指导纹理结构保留。

Result: 大量实验证明WaMaIR在图像恢复任务中优于现有最先进方法，同时保持高效的计算性能。

Conclusion: WaMaIR框架通过结合大感受野特征提取、通道感知建模和纹理增强损失，有效解决了图像恢复中纹理细节重建的挑战。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [76] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出了Region in Context框架，通过多级语义对齐实现基于文本的图像编辑，确保局部修改与全局场景的协调性。


<details>
  <summary>Details</summary>
Motivation: 现有方法孤立处理图像区域，仅依赖局部线索，导致编辑不一致、过渡不自然或整体连贯性丧失。

Method: 引入双级引导机制：区域在全图像上下文中表示并与详细区域级描述对齐，同时整个图像与视觉语言模型生成的场景级描述匹配。

Result: 实验表明该方法能产生更连贯且与指令对齐的结果。

Conclusion: 通过让每个区域理解其在全局图像上下文中的作用，实现了精确协调的图像编辑。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [77] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 提出EMRRG框架，使用预训练的Mamba网络和参数高效微调方法进行X射线医疗报告生成，在基准数据集上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成模型主要依赖大语言模型，对预训练视觉基础模型和先进微调技术探索有限，且忽视了非Transformer架构如Mamba网络的潜力。

Method: 将X射线图像分块标记化，通过SSM-based视觉骨干网络提取特征，使用Partial LoRA进行参数高效微调，结合混合解码器LLM生成医疗报告。

Result: 在三个广泛使用的基准数据集上的大量实验验证了所提策略的有效性。

Conclusion: EMRRG框架通过结合Mamba网络和参数高效微调方法，为X射线医疗报告生成提供了新的有效解决方案。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [78] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE提出了一种基于3D高斯泼溅和束调整原理的6D物体姿态估计算法，通过可微渲染和迭代优化提升纹理缺失和光照变化场景下的姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法依赖2D-3D特征对应关系，在纹理缺失物体和变化光照条件下表现不佳，需要更鲁棒的解决方案。

Method: 基于李代数扩展3DGS构建姿态可微渲染流水线，通过束调整原理迭代优化姿态参数，同时更新3DGS模型的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别实现了1.4%、2.8%和2.5%的精度提升。

Conclusion: GS2POSE通过结合3DGS和束调整原理，有效解决了纹理缺失和光照变化下的6D姿态估计问题，在多个基准数据集上取得了优于现有方法的表现。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [79] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频理解框架，通过结合预训练视觉语言模型的语义先验和传统机器学习算法，将视频理解重新定义为高维语义特征空间中的自监督时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据和特定任务训练，成本高且可扩展性有限。大型视觉语言模型在静态图像上的零样本推理能力尚未充分应用于视频领域。

Method: 使用预训练VLM的冻结视觉编码器将视频转换为语义特征轨迹，然后使用核时间分割(KTS)将连续特征流分割为语义连贯的事件片段，最后通过无监督密度聚类识别重复出现的宏观场景和主题。

Result: 框架能够自动生成结构化的多模态视频内容摘要，通过从每个发现的聚类中选择代表性关键帧并利用VLM的文本生成能力进行描述。

Conclusion: 该方法为零样本、自动化的视频内容结构分析提供了一条有效、可解释且模型无关的途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [80] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一个新颖的即插即用解决方案，通过为冻结的多模态大语言模型附加轻量级可训练头部，利用注意力图中的空间线索提取关键点，实现像素级分割，同时保持模型原有的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法需要微调模型以产生与掩码解码器兼容的特定输出，这会改变模型的输出空间并损害其内在泛化能力，违背了构建统一模型的目标。

Method: 在完全冻结的MLLM上附加轻量级可训练头部，通过细化注意力图中嵌入的空间线索来提取关键点，并将其描述为与掩码解码器直接兼容的点状特征。

Result: LENS实现了与基于重训练方法相竞争或更优的分割性能，同时完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。

Conclusion: LENS的可附加设计为扩展MLLM建立了一个高效且强大的范式，为构建真正多才多艺的统一模型铺平了道路。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [81] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出一种完全无监督的道路分割方法，利用几何先验和时序一致性，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 消除对昂贵人工标注数据的依赖，实现可扩展的无监督道路分割。

Method: 使用几何先验生成弱标签（地平线上为非道路，车辆前方四边形为道路），通过特征点跟踪和互信息最大化加强时序一致性。

Result: 在Cityscapes数据集上达到0.82的IoU，表现出高精度和时序稳定性。

Conclusion: 几何约束与时序一致性结合在自动驾驶无监督道路分割中具有巨大潜力。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [82] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 提出个性化图像滤镜PIF，基于预训练扩散模型学习摄影概念的平均外观，并通过文本反转技术优化摄影概念的提示词，实现摄影风格的提取和迁移。


<details>
  <summary>Details</summary>
Motivation: 摄影风格作为特定摄影概念的组合，是著名摄影师魅力的来源。但学习和迁移摄影风格需要深刻理解照片从未知原始外观的编辑过程。现有方法要么无法从参考图像中学习有意义的摄影概念，要么无法保持内容图像的内容。

Method: 基于预训练文本到图像扩散模型，利用生成先验学习摄影概念的平均外观以及如何根据文本提示调整它们。然后使用文本反转技术，通过优化摄影概念的提示词来学习参考图像的摄影风格。

Result: PIF在提取和迁移各种摄影风格方面表现出色。

Conclusion: PIF能够有效解决摄影风格学习和迁移中的问题，展现出优秀的性能。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [83] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大型公共珊瑚礁图像数据集，包含约925,000个属级硬珊瑚标注，映射到世界海洋物种名录，用于推动珊瑚礁自动监测和领域泛化研究。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁因人为压力迅速衰退，急需可扩展的自动化监测方法。现有数据集在规模、地理范围和标签粒度方面存在局限，无法满足机器学习需求。

Method: 整合76个CoralNet来源和红海Al Wajh站点的图像，提供细粒度、分类学映射的标注。提出两种评估设置：源内基准测试和跨源基准测试，分别测试局部性能和领域泛化能力。

Result: 监督学习在源内表现良好，但跨域性能显著下降；零样本模型整体表现较差，特别是对于稀有和视觉相似属类。

Conclusion: ReefNet为领域泛化和细粒度珊瑚分类提供了具有挑战性的基准，旨在推动稳健、领域自适应的全球珊瑚礁监测和保护。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [84] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出AdaptMoist域适应方法，利用纹理特征预测木屑水分含量，解决不同来源木屑数据分布变化问题，准确率达80%，比非适应模型提高23%。


<details>
  <summary>Details</summary>
Motivation: 现有直接方法（烘箱干燥）处理时间长且破坏样本，间接方法在不同来源木屑上准确性不足。需要一种能有效应对来源变异性的稳健方法。

Method: 分析五种纹理特征类型，提出AdaptMoist域适应方法，利用纹理特征在不同域间转移知识，并基于调整互信息提出模型保存标准。

Result: 组合五种纹理特征准确率达95%。AdaptMoist方法跨域预测准确率平均达80%，比非适应模型（57%）提高23%。

Conclusion: AdaptMoist是跨域木屑水分含量估计的有效稳健解决方案，对依赖木屑的产业具有应用潜力。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [85] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 提出了2DGS-R方法，通过分层训练策略在保持几何精度的同时提升渲染质量，仅增加1%存储和少量训练时间即可实现高质量渲染和精细几何结构。


<details>
  <summary>Details</summary>
Motivation: 3DGS难以准确表示表面，2DGS虽然几何保真度提升但渲染质量受损，目前无法在单阶段训练中同时优化几何和渲染质量。

Method: 采用分层训练：先用法线一致性正则化训练原始2D高斯；选择渲染质量不足的2D高斯进行原位克隆操作；最后冻结不透明度微调模型。

Result: 相比原始2DGS仅增加1%存储和少量训练时间，实现了高质量渲染结果同时保持精细几何结构。

Conclusion: 该方法有效平衡了效率与性能，在视觉保真度和几何重建精度方面均有提升。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [86] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer是一个轻量级基于transformer的语义分割框架，结合CBAM注意力和MixVisionTransformer架构，在边缘设备上实现高精度武器分割，达到80.64% mIoU和89.13% mFscore，同时保持82.26 FPS的实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法仅提供粗粒度边界框定位，缺乏细粒度分割能力；现有语义分割模型要么牺牲精度追求效率，要么计算资源需求过高，不适合边缘部署。

Method: 将CBAM注意力模块与MixVisionTransformer架构策略性集成，使用CBAM增强的编码器主干和注意力集成的hamburger解码器，实现手枪、步枪、刀具、左轮手枪和人类五类武器的多类别分割。

Result: ArmFormer在仅4.886G FLOPs和3.66M参数下，达到80.64% mIoU和89.13% mFscore的SOTA性能，推理速度82.26 FPS，比重量级模型计算量减少48倍。

Conclusion: ArmFormer是部署在便携安全摄像头、监控无人机和嵌入式AI加速器上的最优解决方案，在资源受限的边缘设备上实现高精度实时武器检测。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [87] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: 提出BARL框架，通过双边对齐（表示空间和标签空间）提升半监督医学图像分割性能，减少标注成本的同时达到全监督性能


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法主要依赖标签空间一致性，忽略了表示空间对齐的重要性，导致模型难以学习既有区分性又空间一致的表示

Method: BARL框架包含两个协作分支，在标签空间使用双路径正则化和渐进认知偏差校正，在表示空间进行区域级和病灶实例级匹配

Result: 在四个公共基准数据集和一个私有CBCT数据集上的实验表明，BARL持续超越最先进的半监督医学图像分割方法

Conclusion: BARL通过双边对齐策略有效提升了半监督医学图像分割性能，消融研究验证了各组件的重要性

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [88] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种基于配准的旋转不变特征提取框架，将点云配准与基于记忆库的异常检测相结合，解决了现有方法在特征变换不一致和局部几何细节捕捉方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于记忆库的3D异常检测方法存在特征变换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些限制会导致不可靠的检测结果。

Method: 提出配准诱导的旋转不变特征提取框架，将点云配准目标与基于记忆的异常检测集成。通过在配准学习过程中嵌入特征提取，联合优化对齐和表示学习。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，该方法在有效性和泛化性方面始终优于现有方法。

Conclusion: 点云配准不仅对几何结构对齐至关重要，还能引导特征提取获得旋转不变和局部判别性表示，通过整合配准和异常检测目标可以显著提升检测性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [89] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一种神经元级别的分析框架，通过人类大脑活动研究视觉语言模型的多模态信息处理机制，揭示了ANNs与生物神经元在功能网络、冗余性、极性模式和架构影响方面的相似性。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络与人类大脑处理之间相似性的理解有限：单模态ANN研究无法捕捉大脑固有的多模态处理能力，而多模态ANN研究主要关注高层模型输出，忽视了单个神经元的关键作用。

Method: 提出了结合精细人工神经元分析和基于fMRI的体素编码的框架，研究了CLIP和METER两种架构不同的视觉语言模型。

Result: 发现：(1)人工神经元能成功预测多个功能网络中生物神经元的活动；(2)两者都表现出功能冗余；(3)人工神经元呈现与生物神经元相似的极性模式；(4)不同架构驱动不同的生物神经元活动模式。

Conclusion: 这些结果为视觉语言模型中存在类脑层次处理提供了有力证据，表明在神经元级别存在共享的表征机制。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [90] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff模型，将分类器集成到扩散模型中，同时生成和分类皮肤镜图像，提高生成图像的真实性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型难以准确生成特定医学类别图像，限制了在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成，实现更好的类别控制。

Result: 模型能够生成更真实和多样化的图像，分类器性能也得到提升，适用于下游诊断任务。

Conclusion: Class-N-Diff是一个强大的工具，可提高基于扩散模型的合成皮肤镜图像生成的质量和实用性。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [91] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的指令图像编辑后训练框架，通过DiffusionNFT方法和MLLM奖励模型解决监督微调过拟合问题，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决基于监督微调的指令图像编辑模型容易过拟合训练数据模式、难以泛化到训练分布之外的问题。

Method: 使用Diffusion Negative-aware Finetuning (DiffusionNFT)策略优化方法，结合多模态大语言模型作为免训练的统一奖励模型，并设计低方差组过滤机制减少评分噪声。

Result: UniWorld-V2在ImgEdit和GEdit-Bench基准测试中分别获得4.49和7.83分，达到最先进水平。该框架具有模型无关性，可显著提升不同基础模型的性能。

Conclusion: Edit-R1框架有效解决了指令图像编辑中的过拟合问题，通过策略优化和MLLM奖励模型实现了更好的泛化能力，具有广泛的适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [92] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种使用地面相机进行凝结尾迹到航班归因的模块化框架，通过高时空分辨率的地面观测来解决卫星归因的局限性。


<details>
  <summary>Details</summary>
Motivation: 航空业的非CO2效应（特别是凝结尾迹）对气候影响显著，但验证物理模型需要将观测到的凝结尾迹与生成航班关联起来。卫星归因因时空分辨率限制而面临挑战。

Method: 利用地面可见相机凝结尾迹序列数据集，开发模块化框架，将地面相机观测的凝结尾迹与基于飞机监视和气象数据推导的理论凝结尾迹进行关联。框架支持多种几何表示和距离度量，包含时间平滑，并支持基于概率的灵活分配策略。

Result: 建立了一个强大的基线，为未来凝结尾迹与源航班关联研究提供了模块化框架。

Conclusion: 地面相机方法能够捕捉凝结尾迹形成初期的薄线性特征，为凝结尾迹归因研究提供了有效的替代方案。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [93] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本文评估了四种基于Transformer的架构（SegFormer、DeepLabV3+、SegNeXt和Swin Transformer）在热成像武器分割任务上的表现，在自定义热成像数据集上取得了显著的性能提升，其中SegFormer-b5达到最高mIoU 94.15%，SegFormer-b0提供最快推理速度98.32 FPS。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下对监控和安全应用至关重要，而传统CNN方法在捕捉长距离依赖和精细结构细节方面存在局限。Transformer架构在RGB分割任务中表现出色，但在热成像武器分割领域的潜力尚未充分探索。

Method: 使用四种基于Transformer的架构（SegFormer、DeepLabV3+、SegNeXt和Swin Transformer）在包含9,711张图像的自定义热成像数据集上进行二元武器分割，采用MMSegmentation框架和标准数据增强策略进行公平比较。

Result: SegFormer-b5获得最高mIoU 94.15%和像素精度97.04%，SegFormer-b0提供最快推理速度98.32 FPS且mIoU达90.84%，SegNeXt-mscans在85.12 FPS下达到92.24% mIoU，DeepLabV3+ R101-D8达到92.76% mIoU但速度较慢（29.86 FPS）。

Conclusion: Transformer架构在低光照和遮挡热成像环境中展现出强大的武器检测泛化能力，提供了灵活的精度-速度权衡，适用于多样化的实时安全应用场景。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [94] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 提出了Res-Bench基准测试，用于评估多模态大语言模型在不同输入分辨率下的性能稳定性，包含14,400个样本和12个分辨率级别。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法主要关注语义性能，忽略了分辨率鲁棒性——即模型在不同输入分辨率下性能是否保持稳定的关键问题。

Method: 设计了包含Spearman相关性和绝对/相对连续误差等鲁棒性指标的新评估框架，对领先MLLMs进行了大规模评估。

Result: 进行了模型中心和任务中心的鲁棒性检查，研究了填充和超分辨率等预处理策略，并探索了用于稳定性增强的微调方法。

Conclusion: Res-Bench为评估MLLMs的分辨率鲁棒性提供了全面的基准测试和评估框架。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [95] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 这篇综述文章系统分析了医学图像分析中的基础模型，包括视觉专用和视觉语言模型，通过元分析展示发展趋势，并讨论了领域适应、计算约束等挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像分析中快速发展，但该领域仍缺乏对架构演变、训练范式和临床应用的系统性综合，需要统一的分析框架。

Method: 将研究系统分类为视觉专用和视觉语言基础模型，分析其架构基础、训练策略和下游临床任务，并进行定量元分析以表征数据集利用和应用领域的时间趋势。

Result: 通过系统分类和元分析，揭示了基础模型在医学图像分析中的发展趋势，识别了关键挑战和新兴解决方案。

Conclusion: 确定了增强基础模型鲁棒性、可解释性和临床整合的关键未来研究方向，以加速其在真实世界医疗实践中的转化。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [96] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出了Di-Bregman框架，通过Bregman散度密度比匹配来加速扩散模型采样，实现高效的一步生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但计算成本昂贵，现有蒸馏方法缺乏统一的理论基础。

Method: 将扩散蒸馏表述为基于Bregman散度的密度比匹配问题，提供凸分析视角统一多种现有目标。

Result: 在CIFAR-10和文本到图像生成任务中，相比反向KL蒸馏获得更好的一步FID分数，并保持与教师模型相近的视觉保真度。

Conclusion: Bregman密度比匹配是通向高效一步扩散生成的理论基础扎实且实用的途径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [97] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出了CARE框架，通过序列-图像对比对齐方法解决ADL识别中序列和图像表示的对齐问题，在三个CASAS数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有ADL识别方法存在表示层面的局限性：序列方法保持时间顺序但对噪声敏感且缺乏空间意识，图像方法捕获全局模式但压缩时间动态和扭曲传感器布局，简单融合方法无法有效对齐两种表示视图。

Method: CARE框架包含：(1)时间感知、噪声弹性的序列编码；(2)空间感知和频率敏感的图像表示；(3)联合对比-分类目标函数，通过序列-图像对比对齐(SICA)和交叉熵联合优化表示学习和分类。

Result: 在三个CASAS数据集上达到最先进性能：Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%，并展示了对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过对比对齐有效整合序列和图像表示的互补优势，为智能家居中可靠的ADL识别提供了有前景的解决方案。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [98] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 提出BaGLM方法，利用大型多模态模型的零样本能力进行在线视频步骤定位，无需训练即可超越基于训练的离线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统视频步骤定位方法需要标注训练数据且只能离线处理的问题，探索无需训练和在线处理的新方案。

Method: 使用大型多模态模型预测有限帧的步骤，结合贝叶斯滤波原理，通过LLM提取的依赖矩阵和步骤进度估计来整合历史帧信息。

Result: 在三个数据集上的实验表明，BaGLM优于最先进的基于训练的离线方法。

Conclusion: 证明了利用大型多模态模型的零样本能力进行在线视频步骤定位的可行性，BaGLM方法在无需训练的情况下取得了优异性能。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [99] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文通过实证研究探讨了不同视频特征对时间视频定位任务的影响，发现仅改变视频编码器就能显著影响模型性能，并揭示了特征互补的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前时间视频定位研究过于集中在少数视频表示方法上，可能导致长期的结构过拟合问题，需要探索更多样化的视频特征。

Method: 在三个基准数据集上，使用基于CNN、时序推理和transformer的不同视频编码器提取特征，并在经典架构上进行比较研究。

Result: 结果显示，仅改变视频编码器就能导致模型性能的显著差异，同时揭示了使用某些特征时的明确模式和错误。

Conclusion: 不同视频特征之间存在明显的性能差异和互补潜力，这为未来视频表示方法的选择提供了重要参考。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [100] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 该论文挑战了专用遥感基础模型优于通用视觉基础模型的观点，通过实验证明在ViT-B规模下，专用模型并未带来一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证专用遥感基础模型是否真的比通用视觉基础模型更有用，特别是在小规模场景下，考虑到遥感图像的独特特征和应用需求。

Method: 设计了一个简单基准测试，测量遥感模型在低分辨率图像上的泛化能力；在MillionAID数据集上训练iBOT自监督视觉编码器，并针对遥感特性进行修改。

Result: 实验结果显示，在ViT-B规模下，这些预训练模型都没有比通用基线带来一致的改进。

Conclusion: 结论是至少在ViT-B规模下，专用遥感基础模型并不比通用视觉基础模型更有优势。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [101] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种基于多模态大语言模型的细粒度视频时序定位方法，通过两阶段处理将文本查询转换为增强句子，再使用轻量级解码器进行精确定位。


<details>
  <summary>Details</summary>
Motivation: 利用多模态大语言模型的能力来联合处理文本和视频，有效定位视频中的自然语言查询，解决直接定位的局限性。

Method: 采用两阶段方法：首先将语言查询转换为包含缺失细节和线索的增强句子，然后使用轻量级解码器基于增强查询的上下文表示预测准确边界。使用多实例学习目标动态选择最优查询版本以减少噪声和幻觉影响。

Result: 在多个视频时序定位和段落定位基准测试中取得了最先进的结果，显著优于所有先前提出的基于LLM的时序定位方法，在零样本评估场景中具有明显优势。

Conclusion: ED-VTG方法在视频时序定位任务中表现出色，既优于或可与专门模型相媲美，同时在零样本评估中保持明显优势。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [102] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出W2R2训练框架，通过解耦表示学习和针对性捷径抑制来解决多模态3D定位中的2D语义偏差问题，显著提升定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D定位模型存在严重的"2D语义偏差"问题，过度依赖2D图像特征进行粗略定位，而忽视3D几何输入，导致融合性能不佳。

Method: W2R2框架将2D特征作为"What"识别的语义信标，3D特征作为"Where"定位的空间锚点，采用双目标损失函数：对齐损失监督融合预测，伪标签损失通过边界机制惩罚2D主导的伪输出。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2在定位精度和鲁棒性方面取得显著提升，特别是在杂乱室外场景中。

Conclusion: W2R2通过重新构建模型内部表示空间，在不改变推理架构的情况下实现了精确的3D定位，有效解决了2D语义偏差问题。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [103] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种使用条件StyleGAN2-ADA和StyleGAN3生成高分辨率合成活体指纹，并通过CycleGAN转换为各种材料假指纹的方法，创建了两个包含1500张指纹图像的数据集，在隐私保护和性能方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决真实指纹数据集收集耗时、昂贵且需要严格隐私保护的问题，通过合成指纹数据来应对隐私、成本和可访问性方面的挑战。

Method: 使用条件StyleGAN2-ADA和StyleGAN3架构生成特定手指身份的高分辨率合成活体指纹，然后利用CycleGAN将这些指纹转换为模拟各种攻击材料（如EcoFlex、Play-Doh）的逼真假指纹。

Result: StyleGAN3模型达到最低5的FID分数，生成的指纹在0.01%错误接受率下达到99.47%的真实接受率；StyleGAN2-ADA模型在相同条件下达到98.67%的真实接受率。匹配实验确认了强大的隐私保护特性，没有明显的身份泄露证据。

Conclusion: 该方法成功生成了高质量的合成指纹数据集，在保持强隐私保护的同时，为开发鲁棒的假指纹检测系统提供了重要资源。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [104] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一个临床医生参与循环的深度学习管道，用于肺癌CT图像分割，通过VNet模型结合半监督学习实现了最佳性能（Dice=0.83）和预后准确性（准确率=0.88），获得了临床医生的信任。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主要原因，CT成像在筛查、预后和治疗中至关重要。手动分割存在变异性且耗时，而深度学习虽然提供自动化但面临临床采用障碍。

Method: 使用来自12个公共数据集的999名患者的多中心CT数据，比较了5种DL模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），评估了497个放射组学特征，比较了监督学习和半监督学习，并由6名医生进行定性评估。

Result: VNet实现了最佳性能（Dice=0.83，IoU=0.71）、放射组学稳定性（平均相关性=0.76，ICC=0.65）和SSL下的预测准确性（准确率=0.88，F1=0.83）。SSL在所有模型中始终优于SL。放射科医生偏好VNet用于瘤周表示和更平滑的边界。

Conclusion: 将VNet与SSL集成可产生准确、可重复且临床信任的基于CT的肺癌预后，突出了实现以医生为中心的AI转化的可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [105] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出了一种用于行人重识别的新型类别代表选择方法，该方法不限于类别质心，在准确率和平均精度之间取得平衡，显著提升了现有技术水平。


<details>
  <summary>Details</summary>
Motivation: 当前行人重识别研究主要关注特征提取和目标函数改进，但类别代表选择这一重要方向研究不足。现有方法使用类别质心作为代表存在局限性，导致次优结果。

Method: 提出广义选择方法，选择不限于类别质心的代表，可根据应用需求调整每个类别的代表数量，并在多种重识别嵌入上应用该方法。

Result: 该方法在所有测试案例中都显著优于当代结果，在准确率和平均精度之间取得了更好的平衡。

Conclusion: 广义类别代表选择方法能够有效提升行人重识别性能，超越了现有技术水平，且具有灵活的可调性以适应不同应用需求。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [106] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出V-Reason方法，通过基于熵的优化调整LMM的值缓存，在推理时改善模型的微观探索和利用行为，无需额外训练即可显著提升视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理LMM依赖昂贵的强化学习和冗长的思维链，计算开销大且思维控制机制有限。

Method: 使用模型输出熵作为信号，通过小型可训练控制器基于熵目标优化LMM的值缓存，调整推理时的微观探索和利用行为。

Result: 在多个视频推理数据集上显著超越基础指令调优模型，与RL训练模型的差距缩小到0.6%平均准确率，同时输出token减少58.6%。

Conclusion: 基于熵的推理时优化能有效提升LMM的视频推理能力，无需额外训练即可获得接近RL模型的性能，且效率更高。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [107] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 比较通用视觉基础模型Hiera与专用分割模型SAM2的特征通用性，通过信息论方法量化专业化成本，发现专业化在空间相关任务中表现优异但在概念遥远任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 理解通用基础视觉模型与专用模型之间的权衡对于高效特征编码设计至关重要，但目前尚未完全理解这种权衡关系。

Method: 使用轻量级可训练颈部网络来探测冻结特征的可适应性，通过信息论成本量化专业化代价，并进行跨颈部分析。

Result: SAM2在深度估计等空间相关任务中表现优异，但在姿态估计和图像描述等概念遥远任务中表现不如通用模型Hiera，显示出语义信息的损失。

Conclusion: 专业化在特定任务中有效但会损失更广泛的语义信息，为设计高效特征编码和适应策略提供了量化基础。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [108] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 提出了ProDAT方法，一种基于密度感知的渐进式点云编码机制，通过单一模型实现多比特率下的渐进解码，显著提升了编码效率。


<details>
  <summary>Details</summary>
Motivation: 3D点云在自动驾驶、增强现实等应用中需要实时处理和低延迟，但大数据量和带宽限制阻碍了在资源受限环境中的高质量服务部署。现有学习型点云几何编码方法的固定潜在表示不支持渐进解码。

Method: 提出ProDAT方法，利用密度信息作为指导信号，根据重要性自适应解码潜在特征和坐标，实现单一模型支持多比特率的渐进解码。

Result: 在基准数据集上的实验结果表明，ProDAT不仅实现了渐进编码，而且相比最先进的学习型编码技术，在SemanticKITTI上PSNR-D2的BD-rate提升超过28.6%，在ShapeNet上提升超过18.15%。

Conclusion: ProDAT方法成功解决了点云渐进编码问题，在保持编码效率的同时实现了多比特率渐进解码能力。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [109] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 提出了FMCAF预处理架构，通过频域滤波和跨注意力融合来增强RGB和红外图像的多模态目标检测性能，在多个数据集上优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测在挑战性条件下通过利用多传感器模态的互补线索来提高鲁棒性，但需要更有效的融合方法来提升性能。

Method: FMCAF结合频域滤波块(Freq-Filter)来抑制冗余频谱特征，以及基于跨注意力的融合模块(MCAF)来改进模态间特征共享。

Result: 在LLVIP(低光行人检测)和VEDAI(航空车辆检测)数据集上，FMCAF比传统融合方法表现更好，在VEDAI上mAP@50提升13.9%，在LLVIP上提升1.1%。

Conclusion: FMCAF作为一种灵活的预处理架构，具有在多模态目标检测中实现鲁棒融合的潜力，且无需针对特定数据集进行调优。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [110] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane是一种改进高斯泼溅的方法，通过引入平面先验来提升3D场景中平面区域的几何精度和网格质量，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法在重建平面区域时存在平滑度和精度不足的问题，难以满足场景编辑和物理仿真的需求。

Method: 利用现成的分割和法向预测模型提取平面先验，建立结构化平面高斯坐标表示，并引入动态高斯重分类器优化训练过程，最后用优化的平面先验改进网格布局。

Result: 在保持渲染质量的同时，显著提高了提取网格的几何精度，减少了顶点和面数，改善了拓扑结构。

Conclusion: GSPlane通过引入平面先验有效解决了高斯泼溅在平面重建中的局限性，为下游应用提供了更好的结构化表示。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [111] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出了一种针对预训练扩散模型的优化策略，通过潜在空间精炼和双向交互机制，在保持感知真实性的同时显著提升内容保真度，特别是在低光场景下。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散方法在低层视觉任务中表现出色，但往往牺牲内容保真度来换取感知真实性，这个问题在低光场景中尤为严重。主要原因是缺乏合适的条件潜在建模以及条件潜在与噪声潜在之间缺乏双向交互。

Method: 1. 引入潜在精炼管道，利用生成先验恢复VAE编码过程中丢失的空间细节；2. 设计动态交互机制，让精炼后的条件潜在与噪声潜在进行双向交互；3. 即插即用设计，可无缝集成到现有扩散网络中。

Result: 广泛的实验表明，该方法在预训练扩散方法中实现了显著的内容保真度提升，同时保持了真实性和美学质量。

Conclusion: 该方法有效解决了预训练扩散模型在内容保真度方面的局限性，为低层视觉任务提供了更有效的控制机制，特别是在具有挑战性的低光场景下。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [112] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出一种使用LED环境照明为消费相机生成视觉不可见水印的方法，通过优化LED光谱轮廓，使其对人眼几乎不可见但对相机高度可检测。


<details>
  <summary>Details</summary>
Motivation: 开发一种在标准视频帧率下工作的不可见水印技术，用于隐私保护和内容验证，避免传统可见光通信的局限性。

Method: 采用光谱调制而非强度调制，综合考虑人眼视觉系统敏感性、相机传感器光谱敏感性和LED生成白光的特性，优化LED光谱轮廓。

Result: 能够在10秒视频片段中嵌入128位信息，在30-60fps标准帧率下实现水印提取，信息传输速率适中但足以支持基本元数据需求。

Conclusion: 该方法成功实现了对人眼不可见但对相机可检测的水印嵌入，为隐私保护和内容验证提供了实用的解决方案。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [113] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出GOOD框架，通过双重引导机制生成多样化OOD样本，提升OOD检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扰动文本条件嵌入生成OOD样本，存在语义不稳定和多样性不足的问题，限制了在真实OOD场景中的泛化能力

Method: GOOD框架使用现成的ID分类器直接引导扩散采样轨迹：图像级引导基于对数分割梯度减少输入似然；特征级引导基于k-NN距离促进特征稀疏区域采样

Result: GOOD生成的样本能显著提升OOD检测性能，通过统一OOD评分增强检测鲁棒性

Conclusion: GOOD框架通过双重引导机制实现了更可控和多样化的OOD样本生成，有效提升了OOD检测能力

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [114] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D是一个统一的框架，用于从单视图输入重建多样化的铰接对象实例和姿态估计，通过运动学感知扩散模型处理多部分几何和可变关节配置的挑战。


<details>
  <summary>Details</summary>
Motivation: 铰接对象（如笔记本电脑和抽屉）由于其多部分几何结构和可变关节配置，在3D重建和姿态估计方面面临重大挑战，这些配置在不同状态下引入了结构多样性。

Method: 首先通过新颖的运动学感知VAE将完整几何（SDFs）、关节角度和部件分割编码到结构化潜在空间中；然后使用两个条件扩散模型：一个用于回归全局姿态（SE(3)）和关节参数，另一个用于从部分观察生成运动学感知潜在代码；最后通过迭代优化模块双向优化重建精度和运动学参数。

Result: 在合成、半合成和真实世界数据集上的实验结果表明，该方法在准确重建铰接对象和估计其运动学属性方面具有有效性。

Conclusion: KineDiff3D框架能够有效处理铰接对象的3D重建和姿态估计问题，通过运动学感知的扩散模型和迭代优化实现了准确的重建结果。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [115] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD是一个两阶段后训练框架，通过深度和法线图作为几何先验，结合强化学习中的组长度奖励，从单张图像生成可编辑的CAD模型，在几何精度和建模简洁性方面达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在从2D图像推断3D几何时存在空间推理能力有限的问题，难以准确生成可编辑的参数化CAD模型。

Method: 两阶段框架：1) 监督微调阶段使用深度和表面法线图作为密集几何先验，与RGB图像形成多通道输入；2) 强化学习阶段引入组长度奖励，在保持高几何保真度的同时促进生成更紧凑的建模序列。

Result: 在DeepCAD和Fusion360数据集上的实验表明，GACO-CAD在相同MLLM骨干下达到最先进性能，在代码有效性、几何精度和建模简洁性方面一致优于现有方法。

Conclusion: GACO-CAD通过结合几何先验和简洁建模奖励，有效解决了从单张图像生成可编辑CAD模型的挑战，显著提升了生成模型的几何准确性和实用性。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [116] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 本文研究了人脸识别系统中预处理对对抗攻击可迁移性的影响，发现人脸检测模型的选择会显著降低攻击成功率，并提出了一种预处理不变的方法来提高攻击可迁移性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到对抗样本攻击，但现有研究往往忽略了预处理环节在对抗攻击中的重要性，特别是在黑盒设置下。

Method: 研究了多种现成的最先进对抗攻击在不同预处理技术下的可迁移性，分析了人脸检测模型和降采样插值方法的影响，并提出基于输入变换的预处理不变方法。

Result: 人脸检测模型选择可使攻击成功率降低达78%，而插值方法影响较小。提出的预处理不变方法可将攻击可迁移性提高达27%。

Conclusion: 预处理在人脸识别系统中至关重要，考虑预处理因素有助于提高面部对抗样本的对抗泛化能力。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [117] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为GtR（Generation then Reconstruction）的无训练分层采样策略，通过将生成过程分解为结构生成和细节重建两个阶段，在保持生成质量的同时实现3.72倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归模型在视觉生成中并行生成能力受限的问题，因为单步建模空间相关视觉token的复杂性限制了加速潜力。

Method: 采用两阶段方法：1）结构生成阶段建立全局语义框架；2）细节重建阶段高效完成剩余token。同时提出频率加权token选择（FTS）方法，基于高频信息能量将更多计算预算分配给图像细节token。

Result: 在ImageNet类条件生成和文本到图像生成任务中，MAR-H模型实现了3.72倍加速，同时保持可比较的质量（FID: 1.59, IS: 304.4 vs 原始1.59, 299.1）。

Conclusion: GtR方法在各种模型规模和生成任务中显著优于现有加速方法，证明了分层采样策略在加速掩码自回归模型方面的有效性。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [118] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文针对浮游生物识别中的分布偏移问题，基于DYB-PlanktonNet数据集构建了系统性的OoD基准测试，评估了22种OoD检测方法，发现ViM方法在远OoD场景中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物识别模型在真实部署中面临分布偏移挑战，由于浮游生物形态复杂、物种多样性高且新物种不断发现，导致推理时出现不可预测错误。该领域缺乏最新计算机视觉技术的系统集成和统一的大规模评估基准。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准测试，系统评估了22种OoD检测方法。

Result: 大量实验结果表明，ViM方法在构建的基准测试中显著优于其他方法，特别是在远OoD场景中关键指标有显著提升。

Conclusion: 这项全面评估为自动浮游生物识别中的算法选择提供了可靠参考，并为浮游生物OoD检测的未来研究奠定了坚实基础。这是浮游生物识别领域首次大规模、系统性的OoD检测方法评估分析。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [119] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出一个联合学习详细头部化身和手-脸交互引起的非刚性变形的框架，通过深度顺序损失和接触正则化解决姿态跟踪问题，并学习手引起面部变形的PCA基来减少参数估计复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了自然的手-脸交互（如手托下巴、手指轻触脸颊等传达认知状态的行为），这些交互对于逼真的3D头部化身至关重要。

Method: 结合深度顺序损失和接触正则化进行姿态跟踪；学习手引起面部变形的PCA基；引入接触损失减少穿插伪影，增强物理合理性。

Result: 在iPhone拍摄的RGB(D)视频上评估，并构建合成数据集。相比最先进的表面重建方法，能捕获更好的外观和更准确的面部变形几何。

Conclusion: 该方法能够有效捕捉手-脸交互引起的面部变形，生成更逼真和物理合理的3D头部化身。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [120] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC是一个双曲表示学习框架，用于解决域泛化与广义类别发现(DG-GCD)问题，通过GPT引导的扩散增强和切线空间插值实现高效域泛化，无需情景模拟训练。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法假设训练时能同时访问标记和未标记数据且来自同一域，限制了在开放世界场景中的适用性。DG-GCD要求模型泛化到包含新类别的未见域，而现有唯一方法DG2CD-Net依赖计算成本高的情景训练。

Method: 使用GPT引导的扩散增强暴露模型于最小但多样的域变化；引入切线CutMix进行曲率感知插值合成伪新样本；结合惩罚Busemann对齐、混合双曲对比正则化和自适应离群排斥的统一损失函数；可学习曲率参数适应数据集复杂度。

Result: 在PACS、Office-Home和DomainNet数据集上达到最先进水平，一致优于现有的欧几里得和双曲(DG)-GCD基线方法。

Conclusion: HIDISC通过双曲表示学习框架有效解决了DG-GCD问题，实现了无需情景模拟的高效域和类别级泛化。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [121] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出一种零样本的视觉token剪枝方法，通过平衡任务相关性和信息多样性，在保持性能的同时大幅降低计算成本


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理能力增强，视觉token冗余导致推理成本急剧上升，现有剪枝方法忽视文本提示的指导，无法优先考虑任务相关性

Method: 采用分层方法：首先选择任务相关的核心视觉token集合，然后补充多样性token以保留更广泛的上下文信息

Result: 在多个模型和基准测试中，该方法在剪枝高达90%的token时性能达到或超越最先进方法，仅造成最小精度损失，同时显著降低GPU内存占用和推理延迟

Conclusion: 提出的提示感知视觉token剪枝方法有效平衡了任务相关性和信息多样性，在保持模型性能的同时大幅提升了推理效率

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [122] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 使用Segment Anything Model (SAM)开发了一个专门用于监测孟加拉国河流侵蚀的AI模型，通过颜色通道分析和微调SAM的掩码解码器，实现了对河岸侵蚀的高精度检测。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国河流每年吞噬村庄和农田，造成大规模社区消失和人口流离失所，传统人工监测方法效率低下，需要更有效的技术手段来跟踪这一缓慢的灾难。

Method: 首先使用简单的颜色通道分析进行粗略的土地和水域分割，然后微调SAM的掩码解码器来识别河岸侵蚀的细微特征。创建了包含2003-2025年历史Google Earth影像的新数据集，首次包含消失定居点的手动标注数据。

Result: 模型在河岸侵蚀检测方面表现出色，平均交并比达到86.30%，Dice分数达到92.60%，显著优于传统方法和现成的深度学习模型。

Conclusion: 这项工作提供了三个关键贡献：首个孟加拉国因河流侵蚀消失定居点的标注数据集、专门针对此关键任务微调的AI模型、以及通过视觉证据量化土地损失的方法，为政策制定者和灾害管理机构提供了监测侵蚀、预测轨迹和保护脆弱社区的有力工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [123] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 基于TimeSformer视频识别模型，通过分析VALORANT游戏小地图中的战术特征（角色位置和游戏事件）来预测回合结果，相比仅使用小地图信息的模型显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多基于比赛日志和统计数据，但缺乏对复杂策略游戏如VALORANT中战术特征的分析。

Method: 使用TimeSformer视频识别模型，从小地图信息中提取详细的战术特征（角色位置和游戏事件），构建回合结果预测模型。

Result: 在增强战术事件标签的数据集上训练的模型达到约81%的预测准确率，特别是在回合中后期阶段显著优于仅使用小地图信息的模型。

Conclusion: 利用比赛录像中的战术特征对于预测VALORANT回合结果非常有效，证明了战术分析在电竞预测中的重要性。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [124] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 提出了EndoCIL框架，专门用于内窥镜图像诊断的类增量学习，通过三个关键组件解决领域差异和类别不平衡问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析需要持续适应新的临床数据，但现有方法因严重的领域差异和类别不平衡而无法有效缓解灾难性遗忘。

Method: 包含三个核心组件：基于最大均值差异的重放选择策略、先验正则化类别平衡损失函数、以及全连接层梯度校准机制。

Result: 在四个公共内窥镜数据集上的实验表明，EndoCIL在不同缓冲区大小和评估指标下均优于最先进的类增量学习方法。

Conclusion: 该框架有效平衡了内窥镜终身诊断中的稳定性和可塑性，显示出良好的临床可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [125] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出基于DINOv2的面部欺骗攻击检测方法，利用DINOv2和寄存器提取可泛化特征并抑制注意力机制中的扰动，有效区分真实和欺骗面部图像。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统易受恶意攻击者使用注册用户照片进行欺骗攻击，需要在面部识别前检测此类攻击以确保系统安全。

Method: 采用DINOv2模型结合寄存器，提取可泛化特征并抑制注意力机制中的扰动，使模型能够专注于关键细微特征。

Result: 在ICCV2025第六届面部反欺骗研讨会提供的统一物理-数字攻击检测数据集和SiW数据集上的实验证明了该方法的有效性。

Conclusion: 基于DINOv2的欺骗攻击检测方法能够有效识别真实与欺骗面部图像，提高面部识别系统的安全性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [126] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文分析了多模态大语言模型(MLLMs)的跨模态交互过程，提出了三阶段理论，并基于此开发了VisiPruner训练无关剪枝框架，显著减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务中表现出色，但由于注意力计算的二次增长导致计算开销巨大。现有剪枝方法缺乏对MLLMs处理多模态信息机制的基本理解。

Method: 通过系统分析发现三阶段跨模态交互过程：浅层识别任务意图、中层关键视觉令牌驱动融合、深层仅关注语言精炼。基于此提出VisiPruner训练无关剪枝框架。

Result: VisiPruner在LLaVA-v1.5 7B上减少了99%的视觉相关注意力计算和53.9%的FLOPs，显著优于现有令牌剪枝方法，并在多种MLLMs上具有良好泛化性。

Conclusion: 研究不仅提供了有效的剪枝方法，还为训练高效MLLMs提供了可操作指南，通过使模型架构与其内在分层处理动态对齐来提升效率。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [127] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 该论文提出了多时刻检索任务和QV-M²数据集，并开发了FlashMMR框架，通过后验证模块优化时刻边界，在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时刻检索方法主要关注单时刻检索，但现实应用中一个查询可能对应多个相关时刻，现有数据集和方法不足以满足视频时序定位的实际需求。

Method: 提出FlashMMR框架，包含多时刻后验证模块，通过约束时序调整和验证模块重新评估候选片段，过滤低置信度提议，实现鲁棒的多时刻对齐。

Result: 在QV-M²数据集上，FlashMMR相比现有最优方法在G-mAP上提升3.00%，在mAP@3+tgt上提升2.70%，在mR@3上提升2.56%。

Conclusion: QV-M²数据集和FlashMMR方法为推进更现实和具有挑战性的视频时序定位研究奠定了基础。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [128] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出一个公平感知的深度伪造检测框架，整合时序特征学习和人口统计感知数据增强，以提升公平性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度且无法捕捉时序信息，导致跨不同人口群体的决策偏见和不可靠结果。

Method: 使用时序建模的序列聚类和概念提取进行可解释检测，引入人口统计感知数据增强平衡弱势群体并应用频域变换保留伪造痕迹。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上的广泛实验表明，该方法在公平性和准确性之间取得了最佳平衡。

Conclusion: 该方法通过时序特征学习和公平数据增强，有效提升了深度伪造检测的公平性、可靠性和可解释性。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [129] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集、整理和统一的2400万样本语料库，是同类中最大的开放资源，通过半自动化流程整合200多个来源，包含严格的去重和去污染处理，训练出的模型在广泛评估中表现优于现有开放混合数据集。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的发展受到公共数据集碎片化、不一致和污染问题的阻碍，需要构建一个高质量、统一的大规模数据集来推动研究进展。

Method: 采用半自动化、人工参与的工作流程：自动化处理批量摄取和模式映射，人工审核员审查映射并抽查输出，验证标注的忠实性、格式多样性和安全性，并进行严格的跨源去重和针对66个公共基准的去污染处理。

Result: 在FineVision上训练的模型在广泛的评估套件中持续优于基于现有开放混合数据集训练的模型，证明了规模、数据卫生以及平衡自动化与人工监督的好处。

Conclusion: FineVision语料库和整理工具的发布将加速以数据为中心的视觉语言模型研究，强调了高质量数据整理对模型性能提升的重要性。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [130] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出Plug-and-Forecast方法，通过多模态大语言模型增强现有运动预测模型，无需微调即可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在标准条件下表现可靠，但难以经济高效地泛化到多样化真实场景

Method: 设计提示从MLLMs提取结构化场景理解，将其蒸馏为可学习嵌入来增强现有行为预测模型

Result: 在两个最先进的运动预测模型上验证，在Waymo和nuScenes数据集上均获得一致的性能提升

Conclusion: PnF方法利用MLLMs的零样本推理能力，为运动预测提供了实用且有效的增强方案

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [131] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出SG-CLDFF框架，结合显著性引导预处理与多尺度深度特征融合，提升白细胞分割和分类的鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 白细胞显微图像分割和分类对血液疾病诊断至关重要，但面临染色变异、复杂背景和类别不平衡等挑战。

Method: 使用显著性先验突出白细胞区域，采用EfficientSwin风格混合骨干网络生成多分辨率表示，通过ResNeXt-CC启发的跨层融合模块整合浅层和深层特征，多任务训练结合类别加权损失和显著性对齐正则化。

Result: 在BCCD、LISC、ALL-IDB等标准基准测试中，IoU、F1分数和分类准确率均优于CNN和Transformer基线模型。

Conclusion: SG-CLDFF为临床工作流程提供了实用且可解释的自动化白细胞分析解决方案。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [132] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习中的反直觉现象：训练时间过长会损害密集预测任务性能，称为自监督密集退化(SDD)。作者提出了DSE指标来评估无标注下的密集表示质量，并基于此开发了模型选择策略和正则化方法。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习中训练时间过长反而会降低密集预测任务（如语义分割）性能的反常现象，需要解决无标注情况下评估密集表示质量的挑战。

Method: 提出Dense representation Structure Estimator (DSE)，包含类别相关性度量和有效维度度量，并基于此开发模型选择策略和DSE正则化方法。

Result: 在16种自监督方法和4个基准测试上，模型选择策略平均提升mIoU 3.0%，计算成本可忽略；DSE正则化能持续缓解密集退化效应。

Conclusion: 自监督学习存在密集退化现象，提出的DSE指标能有效评估密集表示质量，相应的模型选择和正则化方法能显著改善下游任务性能。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [133] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于评估模型理解长视频能力的基准测试，整合视觉、音频和文本多模态，包含约1000个信息密集的长视频和六种挑战性任务场景。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估模型对长视频的理解能力方面存在不足，特别是涉及人类语言、观点、动作等上下文元素的多模态整合分析。

Method: 从FineVideo数据集中精选约1000个长视频，设计六种任务场景（包括事件内和事件间任务），并开发三步半自动数据质量保证流程。

Result: 实验结果显示全模态模型在精确时间定位和长距离因果推理任务中仍面临挑战，扩展实验揭示了多模态融合中的信息丢失和处理偏差问题。

Conclusion: LongInsightBench填补了长视频理解评估的空白，揭示了当前全模态模型在复杂多模态任务中的局限性，为未来研究提供了重要基准。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [134] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一个可扩展的fMRI因果推断框架，通过两阶段方法解决BOLD信号失真和计算复杂性问题，在模拟数据中比DCM准确率高37%，在真实fMRI数据中能恢复88%的已知神经通路。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI因果推断中的根本限制：从血氧动力学失真的BOLD信号推断神经因果关系的病态性质，以及现有方法（如动态因果建模DCM）的计算不可行性。

Method: 将复杂的逆问题分解为两个可处理的阶段：BOLD反卷积恢复潜在神经活动，然后使用新颖的条件Mamba架构进行因果图推断。

Result: 在模拟数据上比DCM准确率高37%；在真实任务fMRI数据中恢复88%的已知神经通路，而传统方法在99%以上的受试者中无法识别这些典型回路；工作记忆数据的网络分析揭示了大脑根据刺激策略性地转移主要因果枢纽。

Conclusion: 为神经科学家提供了一个实用的工具，用于大规模因果推断，能够捕捉认知功能背后的基本回路模式和灵活网络动态。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [135] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文评估了现有对抗性防御方法对大型对抗性衣物攻击的效果，发现这些防御方法在数字和物理世界中均表现不佳，揭示了现有防御方法的共同脆弱性。


<details>
  <summary>Details</summary>
Motivation: 实验发现单纯增大对抗性补丁的尺寸就能使现有防御方法失效，因此需要评估防御方法对覆盖人体大部分区域的大型对抗性衣物的防御效果。

Method: 通过制作对抗性衣物作为测试案例，在数字世界和物理世界中评估多种防御方法的效果，并制作单一衣物攻击集来测试多个防御模型。

Result: 所有防御方法在对抗性衣物攻击下表现都很差，单一衣物攻击集在物理世界中实现了96.06%的攻击成功率，对九个防御模型的攻击成功率均超过64.84%。

Conclusion: 现有对抗性防御方法在面对大型对抗性衣物攻击时存在共同脆弱性，需要开发更有效的防御策略。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [136] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出CharDiff框架，利用字符级引导的扩散模型恢复和识别严重退化的车牌图像，在恢复质量和识别准确率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅对车牌识别系统预处理很重要，还能提高证据价值、增强视觉界面清晰度，促进车牌图像的进一步利用。

Method: CharDiff框架采用扩散模型，通过外部分割和OCR模块提取细粒度字符级先验，引入CHARM模块确保每个字符的引导仅限制在其自身区域，避免区域间干扰。

Result: 在Roboflow-LP数据集上，相比性能最佳的基线模型，CER相对降低28%，在恢复质量和识别准确率方面显著优于基线恢复模型。

Conclusion: 结构化字符引导条件化有效增强了基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [137] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出了iDETEX——一个统一的多模态大语言模型，能够同时执行质量定位、感知和描述三个关键任务，在ViDA-UGC基准测试中达到最先进性能，并在ICCV MIPI 2025挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估从标量质量预测向更可解释、与人类对齐的评估范式发展的挑战，实现详细且可解释的图像质量评估。

Method: 设计了任务特定的离线增强模块和数据混合策略，辅以在线增强策略来充分利用多源监督，构建统一的多模态大语言模型。

Result: 在ViDA-UGC基准测试中所有子任务都达到最先进性能，在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。

Conclusion: iDETEX模型在提供准确且可解释的质量评估方面表现出有效性和鲁棒性，为详细图像质量评估提供了统一解决方案。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [138] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出一种后处理开放集识别方法，通过比较模型特征与预测logit之间的一致性来识别未知类样本，无需重新训练预训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物分类模型在封闭世界设定下训练，当遇到未知类别时会过度自信。现有OSR方法大多需要重新训练模型，这限制了实际应用。

Method: 基于输入样本到最近类均值的距离构建概率分布，然后将该分布与softmax概率进行比较，衡量NCM与分类头之间的一致性。

Result: 在两个评估数据集上均排名前三，AUROC分别达到93.41（非洲动物）和95.35（瑞典动物），性能表现稳定。

Conclusion: 提出的后处理方法在多个数据集上表现一致且优异，优于在单一数据集上表现好的现有方法，且无需重新训练模型。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [139] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出了Semantic-E2VID框架，通过跨模态特征对齐和语义感知特征融合，将帧模态的语义知识迁移到事件模态，显著提升了事件到视频重建的质量和语义信息恢复能力。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽然具有低延迟、高动态范围等优势，但由于只捕捉强度变化而忽略静态信息，导致事件模态缺乏语义信息。现有事件到视频重建方法往往忽视语义信息的重要性，影响了重建视频的质量。

Method: 1. 跨模态特征对齐模块：将Segment Anything Model的视觉语义知识迁移到事件编码器；2. 语义感知特征融合块：将学习到的语义特征整合到事件表示中；3. 语义感知E2V监督：利用SAM生成的类别标签促进语义细节重建。

Result: 在多个基准测试中显著提升了帧质量，优于现有的最先进E2V方法。

Conclusion: 通过引入语义信息，Semantic-E2VID有效解决了事件模态缺乏语义信息的问题，显著改善了事件到视频重建的质量和语义细节恢复能力。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [140] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过LLM引导的视觉token选择、循环处理历史token和基于描述的问答，解决视频大语言模型在流式视频处理中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在处理流式长视频时面临效率挑战，需要在线处理小时级视频并实时响应查询。

Method: 1) LLM引导的视觉token选择，基于注意力机制丢弃95%不重要token；2) 循环处理历史选择token；3) 基于描述的轻量级问答。

Result: 在流式视频基准测试中达到最先进性能，在效率和效果之间取得良好平衡。

Conclusion: 该方法与标准Video-LLMs兼容，无需额外训练，显著提升流式视频处理效率。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [141] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究通过系统评估25个合成人脸识别数据集，证明合成数据在保持隐私的同时能达到与真实数据集相当的识别准确率，为面部识别研究提供了科学可行且符合伦理的替代方案。


<details>
  <summary>Details</summary>
Motivation: 解决面部识别系统中使用真实人脸数据带来的隐私和伦理问题，探索合成数据作为隐私保护替代方案的可行性。

Method: 系统性文献回顾识别25个合成人脸识别数据集，结合实验验证七个关键隐私保护要求，包括身份泄漏预防、类内变异性、身份可分离性等。

Result: 最佳合成数据集VariFace和VIGFace分别达到95.67%和94.91%的识别准确率，超过真实数据集CASIA-WebFace(94.70%)，同时合成数据提供了前所未有的偏差控制能力。

Conclusion: 合成人脸数据是面部识别研究中科学可行且符合伦理要求的替代方案，能够有效解决隐私问题并提供偏差缓解控制。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [142] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于多面部表情特征的帕金森病严重程度诊断方法，通过注意力机制融合特征，并采用自适应类别平衡策略解决类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部表情的PD诊断方法依赖单一表情易误诊，忽视不同PD阶段的类别不平衡问题，且大多只进行二元分类而非严重程度诊断。

Method: 整合多种面部表情特征，通过注意力机制进行特征融合；采用自适应类别平衡策略，根据类别分布和分类难度动态调整训练样本贡献度。

Result: 实验结果表明该方法在PD严重程度诊断方面表现优异，注意力特征融合和自适应类别平衡策略均有效。

Conclusion: 该方法能够有效诊断PD严重程度，解决了单一表情依赖和类别不平衡问题，为PD早期检测和个性化干预提供了便利工具。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [143] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: 提出LoopTrans闭环框架，通过双向知识转移在自我中心和他我中心视角间增强可操作性定位，解决传统单向转移的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统弱监督可操作性定位仅从他我中心图像单向转移到自我中心图像，在复杂交互场景中适用性受限。人类通过观察他人互动来学习新物体交互的能力启发了双向知识转移的需求。

Method: LoopTrans闭环框架包含统一跨模态定位和去噪知识蒸馏机制，不仅从他我中心转移到自我中心，还反向转移以增强他我中心知识提取，弥合视角域差距。

Result: 实验显示LoopTrans在所有图像和视频基准测试指标上均取得一致提升，即使在人体完全遮挡物体交互区域的挑战性场景中也能有效处理。

Conclusion: 双向闭环知识转移框架能更有效地在自我中心和他我中心视角间传递可操作性知识，显著提升弱监督可操作性定位性能。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [144] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 开发了一个基于视觉的马匹行为监控系统原型，使用YOLOv11和BoT-SORT技术自动检测和跟踪马厩中的马匹和人员，通过物体轨迹和空间关系推断事件状态。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监控方法劳动密集且耗时，需要自动化系统来早期检测健康福利问题。

Method: 采用YOLOv11进行目标检测，BoT-SORT进行多目标跟踪，基于CLIP和GroundingDINO构建自定义数据集，通过物体轨迹和空间关系推断事件状态。

Result: 定性评估显示系统在马匹相关事件上表现可靠，但人员检测因数据稀缺存在局限，系统能够区分五种事件类型并考虑摄像头盲区。

Conclusion: 该工作为马场实时行为监控奠定了基础，对动物福利和厩舍管理具有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [145] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一个智能、一体化的密集关键点检测器，通过融合传统检测器的优势，使用深度学习生成高密度、可重复性强的关键点。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器（SIFT、SURF等）和学习方法（SuperPoint等）存在对光度变化敏感、关键点密度低、可重复性差、适应性有限以及缺乏语义理解等问题。

Method: 首先融合7种关键点检测器和2种边缘检测器的输出创建真实掩码，然后使用轻量级ESPNet模型训练，使DeepDetect能够语义关注图像并生成高密度关键点。

Result: 在Oxford Affine Covariant Regions数据集上的评估显示，DeepDetect在关键点密度（0.5143）、可重复性（0.9582）和正确匹配数（59,003）方面均优于其他检测器。

Conclusion: DeepDetect通过融合传统检测器的多样视觉线索和深度学习，实现了在视觉退化条件下仍能产生高密度、高可重复性关键点的检测器。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [146] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动向量生成密集亚像素对应关系和余弦一致性过滤的短轨迹，在压缩域前端实现与SIFT相当的性能但CPU使用更少，在SfM演示中成功重建46-62万点


<details>
  <summary>Details</summary>
Motivation: 探索压缩域对应关系作为资源高效的前端处理方案，减少计算资源消耗同时保持匹配性能

Method: 重新利用AV1运动向量生成密集亚像素对应关系，通过余弦一致性过滤短轨迹

Result: 在短视频上运行性能与顺序SIFT相当但CPU使用少得多，在117帧片段中注册所有图像并重建46-62万点，重投影误差0.51-0.53像素

Conclusion: 压缩域对应关系是实用且资源高效的前端处理方案，具有在完整流程中扩展的明确路径

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [147] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 提出了一个高质量夜间图像去雨基准数据集HQ-NightRain和基于颜色空间转换的网络CST-Net，通过可学习的颜色空间转换器和隐式光照引导来有效去除夜间复杂雨痕。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨面临场景复杂性和缺乏高质量数据集的挑战，现有数据集无法准确表示雨和光照的耦合效应。

Method: 开发CST-Net网络，包含可学习的颜色空间转换器(CSC)在Y通道进行雨痕去除，并引入隐式光照引导提高模型在复杂场景中的鲁棒性。

Result: 实验证明所提数据集的价值和方法的有效性，在夜间图像去雨任务上表现优异。

Conclusion: HQ-NightRain数据集和CST-Net方法为夜间图像去雨提供了高质量的基准和有效的解决方案。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [148] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种改进稀疏视图3D高斯泼溅初始化方法，通过频率感知SfM、3DGS自初始化和点云正则化来解决稀疏视图下的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅容易对训练视图过拟合，导致新视角渲染出现模糊等伪影。现有方法要么改进初始化，要么添加训练时约束，但作者发现初始化是决定性因素。

Method: 1) 频率感知SfM：通过低频视图增强和宽松多视图对应改进低纹理区域覆盖；2) 3DGS自初始化：将光度监督提升为额外点，用学习的高斯中心补偿SfM稀疏区域；3) 点云正则化：通过简单几何/可见性先验强制多视图一致性和均匀空间覆盖。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，在稀疏视图设置下获得了持续的性能提升，证明了该方法作为更强初始化策略的有效性。

Conclusion: 初始化是稀疏视图3DGS性能的决定性因素，提出的方法通过改进SfM、自初始化和正则化，显著提升了稀疏视图下的3D重建质量。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [149] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: 提出了SparseWorld，一种基于稀疏动态查询的灵活、自适应且高效的4D占用世界模型，通过范围自适应感知模块和状态条件预测模块，在感知、预测和规划任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态固定嵌入或网格，限制了感知灵活性，且其网格上的"原位分类"与真实场景的动态连续特性存在潜在不匹配。

Method: 使用稀疏动态查询，包含范围自适应感知模块（通过自车状态调制可学习查询并增强时空关联）和状态条件预测模块（用回归引导预测替代分类预测），以及时间感知自调度训练策略。

Result: 在感知、预测和规划任务中达到最先进性能，验证了模型在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询实现了灵活、自适应且高效的4D占用建模，解决了现有模型在灵活性和动态连续性方面的限制。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [150] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet通过熵引导的双聚类头和最优传输技术生成高质量伪掩码，AutoSOD基于此构建了无需像素级标签的端到端显著目标检测系统，性能接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测需要大量像素级标注，作者认为只要有可靠的伪掩码就能达到接近全监督的精度。现有原型方法存在边界和内部像素几何特性不同、最优传输在原型质量弱时利用不足的问题。

Method: 提出POTNet，用熵引导的双聚类头替代单一k-means：高熵像素用谱聚类，低熵像素用k-means，两组原型通过最优传输对齐。生成的伪掩码监督标准MaskFormer编码器-解码器，构成AutoSOD端到端无监督流程。

Result: 在5个基准测试中，AutoSOD在F-measure上比无监督方法提升26%，比弱监督方法提升36%，进一步缩小了与全监督模型的差距。

Conclusion: 通过熵引导双聚类和最优传输生成高质量伪掩码，可以实现无需像素级标签的显著目标检测，性能接近全监督方法，且训练效率更高。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [151] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于评分准则和伪标签的零样本视频摘要框架，通过将少量真实标注转化为伪标签来构建数据集自适应的评分准则，指导LLM进行可解释的场景评估。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且跨数据集泛化能力有限，无监督方法难以捕捉高层次语义，零样本方法对提示模板敏感且依赖数据集特定的分数归一化。

Method: 将少量真实标注转化为高置信度伪标签，聚合成结构化评分准则。推理时基于场景描述和相邻场景的上下文摘要进行评分，平衡局部显著性和全局连贯性。

Result: 在SumMe和TVSum数据集上分别达到57.58和63.05的F1分数，超越了无监督和先前的零样本基线方法，接近监督方法的性能。

Conclusion: 评分准则引导的伪标签方法有效稳定了基于LLM的评分，为视频摘要建立了一个通用、可解释的零样本范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [152] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出了一个优化视频生成模型训练的四支柱框架，包括数据处理、模型架构、训练策略和基础设施，开发出MUG-V 10B模型，在电商视频生成任务上超越开源基线，并开源完整技术栈。


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型训练面临跨模态对齐、长序列处理和复杂时空依赖等挑战，需要高效训练方案。

Method: 优化四个关键支柱：数据处理、模型架构、训练策略和基础设施，包括数据预处理、视频压缩、参数缩放、课程预训练和对齐后训练。

Result: MUG-V 10B模型整体达到最新视频生成器水平，在电商视频生成任务上超越领先开源基线，训练效率显著提升。

Conclusion: 成功开发出高效的大规模视频生成训练框架，并开源完整技术栈，为社区提供首个基于Megatron-Core的大规模视频生成训练代码。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [153] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出了MambaX-Net，一种用于前列腺癌主动监测的半监督双扫描3D分割架构，利用时间序列MRI数据改进前列腺区域分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割模型基于单时间点和专家标注数据训练，不适合纵向主动监测分析，因为多时间点和专家标签稀缺限制了有效微调。

Method: 使用Mamba增强的交叉注意力模块捕获时间演化和长距离空间依赖，形状提取器模块编码先前分割掩码为潜在解剖表示，结合半监督自训练策略利用伪标签。

Result: 在纵向主动监测数据集上评估，MambaX-Net显著优于最先进的U-Net和Transformer模型，即使在有限和噪声数据下也能实现优异的前列腺区域分割。

Conclusion: MambaX-Net通过有效利用时间信息和半监督学习，解决了纵向前列腺癌监测中的分割挑战，为自动化监测流程提供了可靠解决方案。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [154] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一个弱监督的道路裂缝检测方法，仅使用图像级标签就能实现像素级裂缝检测，通过分类器、重建器和检测器的对抗学习以及路径感知注意力模块来提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 为了减少对昂贵像素级标注的依赖，开发一种仅需图像级标签的弱监督道路裂缝检测方法，以降低智能基础设施维护的成本。

Method: 提出WP-CrackNet框架，包含三个组件：生成类激活图的分类器、测量特征可推断性的重建器、生成像素级检测结果的检测器。通过对抗学习使裂缝CAM覆盖完整裂缝区域，检测器从后处理的伪标签中学习。还设计了路径感知注意力模块和中心增强CAM一致性模块。

Result: 在三个图像级数据集上的实验表明，WP-CrackNet达到了与监督方法相当的结果，并优于现有的弱监督方法。

Conclusion: WP-CrackNet显著推进了可扩展的道路检测，为智能城市基础设施维护提供了高效的弱监督解决方案。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [155] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D是一个扩展VGGT的前馈模型，专门用于动态场景的4D重建，能够同时进行相机姿态估计、深度预测和点云重建，无需后处理。


<details>
  <summary>Details</summary>
Motivation: 现有的3D前馈模型在静态数据集上训练，难以处理包含动态元素（如移动人物或可变形物体）的真实世界场景。

Method: 提出动态感知聚合器，通过预测动态感知掩码来分离静态和动态信息：抑制运动线索用于姿态估计，增强运动线索用于几何重建。

Result: 在动态场景中，PAGE-4D持续优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得更优结果。

Conclusion: PAGE-4D成功解决了多任务4D重建中的任务冲突问题，为动态场景的3D理解提供了有效解决方案。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [156] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了首个水下伪装实例分割数据集UCIS4K和基于SAM的UCIS-SAM网络，通过三个关键模块解决水下环境中的颜色失真、低对比度和模糊问题，在伪装实例分割任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下探索和海洋保护需求增长，但水下环境退化（颜色失真、低对比度、模糊）使伪装实例分割面临更大挑战。传统方法在陆地主导数据集上训练，水下场景性能不足。

Method: 1) 提出UCIS4K数据集（3,953张图像）；2) UCIS-SAM网络包含三个模块：通道平衡优化模块(CBOM)增强水下特征学习，频域真值集成模块(FDTIM)强调内在对象特征减少伪装干扰，多尺度特征频率聚合模块(MFFAM)强化低对比度伪装实例边界。

Result: 在提出的UCIS4K和公共基准测试上的广泛实验表明，UCIS-SAM优于最先进的方法。

Conclusion: UCIS-SAM通过三个专门设计的模块有效解决了水下伪装实例分割的挑战，在性能上超越了现有方法。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [157] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个基于多智能体框架的文本到3D生成系统，使用图结构程序化形状表示来生成结构化、可交互的3D资产


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到3D生成方法产生的非结构化网格和交互性差的问题，使其更适合艺术工作流程

Method: 提出基于图的程序化形状表示，将复杂自然语言分解为结构化子任务图，使用LLM智能体分层解析用户输入并迭代优化程序化建模和绘制

Result: 定性和定量实验显示ShapeCraft在生成几何准确和语义丰富的3D资产方面优于现有基于LLM的方法，支持动画和用户自定义编辑

Conclusion: ShapeCraft展示了在更广泛交互应用中的潜力，能够生成结构化、纹理化和交互式的3D资产

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [158] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出基于机器学习的3D点云自动分割框架，结合无人机扫描的真实点云和BIM生成的合成数据，解决基础设施模型组件分割的自动化问题，显著提高分割精度和效率。


<details>
  <summary>Details</summary>
Motivation: 无人机结合摄影测量技术能够高效获取基础设施的高分辨率3D模型，但传统手动分割方法耗时且容易出错，需要自动化解决方案。

Method: 使用机器学习框架，结合真实无人机扫描点云和BIM生成的合成数据，利用两者的互补优势来克服手动标注的限制。

Result: 在铁路轨道数据集上的验证显示，该方法能够高精度识别和分割主要组件（如铁轨和枕木），使用小规模数据集结合BIM数据显著减少训练时间同时保持合理分割精度。

Conclusion: 该自动化方法提高了3D基础设施模型分割的精度和效率，推动了无人机与BIM技术在结构健康监测和基础设施管理中的集成应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [159] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是一个统一的无监督异常检测框架，通过五个简单元素的组合在标准重建框架中实现卓越性能，解决了多类模型性能不足和领域碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测领域存在多类模型性能显著低于一对一模型的问题，且领域碎片化为专门方法，需要统一解决方案。

Method: 基于"少即是多"理念，在标准重建框架中协调五个简单元素，实现方法极简主义，无需修改即可自然扩展到多样化任务。

Result: 在12个UAD基准测试中，多类模型在MVTec-AD和VisA上分别达到99.9%和99.3%的图像级AUROC；多视图和多模态检测达到SOTA性能；仅用8个正常样本即可超越先前全样本模型。

Conclusion: Dinomaly2通过极简设计、计算可扩展性和通用适用性，成为真实世界异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [160] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT是一个细粒度数据集，记录了汽车模型随时间演化的过程，支持监督和自监督学习。研究发现静态预训练在跨年测试时性能下降，提出了时间增量分类设置和两种策略来提升时间鲁棒性，并探索了时间感知图像生成。


<details>
  <summary>Details</summary>
Motivation: AI系统需要适应不断变化的视觉环境，特别是在物体外观随时间变化的领域。汽车模型作为技术产品的代表类别，其时间演化过程需要被研究。

Method: 构建CaMiT数据集（787K标注样本和5.1M未标注样本），采用静态预训练和时间增量分类设置，评估时间增量预训练和时间增量分类器学习两种策略，并探索时间感知图像生成。

Result: 静态预训练在域内数据上能达到与大规模通用模型竞争的性能且更高效，但在跨年测试时准确率下降。时间增量策略能改善时间鲁棒性，时间感知图像生成能产生更真实的输出。

Conclusion: CaMiT为研究细粒度视觉识别和生成中的时间适应问题提供了丰富的基准，时间增量学习策略能有效应对类别出现、演化和消失的现实持续学习场景。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [161] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 提出DINO-CV框架，使用高分辨率LiDAR DEM和自监督跨视图预训练策略，自动映射植被遮挡下的干石墙，在澳大利亚文化遗产地实现高精度分割。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要遗产和环境价值，但传统方法难以识别被植被遮挡的低矮墙体，且标注数据稀缺，需要开发自动化的可扩展解决方案。

Method: 使用LiDAR DEM克服植被遮挡，提出基于知识蒸馏的自监督跨视图预训练策略，学习多DEM衍生物的视觉和几何不变表示，支持多种视觉骨干网络。

Result: 在Budj Bim UNESCO遗产地测试，mIoU达到68.6%，仅用10%标注数据微调后仍保持63.8% mIoU，成功识别澳大利亚最密集的殖民时期干石墙群。

Conclusion: 自监督学习结合高分辨率DEM衍生物在植被茂密、标注稀缺的遗产环境中具有自动化干石墙映射的巨大潜力。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [162] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较两种节俭联邦学习方法用于暴力检测：零样本和联邦微调视觉语言模型(VLMs)与个性化训练紧凑3D CNN。两种方法准确率均超90%，CNN3D在ROC AUC和log loss上略优且能耗更低，VLMs在上下文推理和多模态推理上更优。提出了混合模型策略。


<details>
  <summary>Details</summary>
Motivation: 研究节俭的联邦学习方法用于暴力检测，重点关注能源效率和环境指标，为视频监控提供负责任、资源感知的AI基准。

Method: 使用LLaVA-7B和65.8M参数CNN3D作为代表案例，在非IID设置下评估准确率、校准和能耗。比较零样本和联邦微调VLMs与个性化训练CNN3D两种策略。

Result: 两种方法准确率均超过90%。CNN3D在ROC AUC和log loss上略优于LoRA微调的VLMs，且能耗更低。VLMs在上下文推理和多模态推理方面保持优势。量化了训练和推理的能耗及CO2排放。

Conclusion: 支持混合模型：轻量级CNN用于常规分类，选择性激活VLM处理复杂或描述性场景。为视频监控提供可复现的负责任、资源感知AI基准。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [163] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer是一个用于4D全景分割的流式处理框架，采用双线程系统实现高效实时处理，可集成到现有3D/4D分割方法中，在动态环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决高度动态环境（如密集人群疏散、复杂自动驾驶场景）中需要实时细粒度感知的问题，在有限时间预算内实现4D全景分割。

Method: 采用双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过对齐最新记忆并补偿自运动和动态物体移动，确保对输入帧的及时预测。

Result: 在室内HOI4D数据集和室外SemanticKITTI、nuScenes数据集上的综合实验表明，该方法在复杂场景中准确预测动态物体方面表现有效，特别是在高FPS条件下展现出优越的鲁棒性。

Conclusion: 4DSegStreamer是一个通用且高效的流式4D全景分割框架，能够无缝集成到现有方法中，在动态环境中提供实时、鲁棒的感知能力。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [164] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 该论文提出了PICABench基准测试，用于系统评估图像编辑的物理真实性，涵盖光学、力学和状态转换等八个子维度，并发现当前主流模型在物理真实性方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要关注指令完成度，但忽略了编辑操作伴随的物理效应（如移除物体时也应移除其阴影、反射等），这影响了生成结果的真实性。

Method: 提出了PICABench基准测试系统，包含八个物理子维度评估；设计了PICAEval评估协议，使用VLM作为评判者并结合人工标注；构建了PICA-100K训练数据集，从视频中学习物理知识。

Result: 评估了主流图像编辑模型，发现物理真实性仍然是一个具有挑战性的问题，现有模型在这方面有较大改进空间。

Conclusion: 物理真实性是图像编辑领域的重要挑战，论文提出的基准测试和解决方案为从简单内容编辑向物理一致的真实性转变奠定了基础。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [165] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC-MoE模型，通过混合专家机制和语义引导对比学习增强医学图像分割基础模型的高层特征表示能力，同时保持预训练权重的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割基础模型微调方法存在两个问题：1）高层特征表示不足；2）微调过程破坏预训练权重结构完整性。

Method: 1）构建基础专家、语义专家和自适应专家，采用像素概率自适应投票策略进行专家选择和融合；2）提出语义引导对比学习方法解决对比学习中弱监督问题。

Result: 在三个公共医学图像分割数据集上的实验表明，IC-MoE优于其他SOTA模型，在多种医学图像分割场景中展现出优异的泛化能力。

Conclusion: IC-MoE有效补充了基础医学图像分割模型的高层特征表示能力，同时保持了预训练权重的结构完整性。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [166] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了多语言文本到图像人物检索任务，开发了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决模态异质性问题，在多语言TIPR数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像人物检索中的模态异质性问题，现有方法要么忽略细粒度差异，要么需要先验信息，且主要针对英语，限制了在多语言环境中的应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过掩码图像和文本的双向预测增强跨语言和跨模态的局部关系建模）和多维全局对齐模块（桥接模态异质性）。

Result: 在所有多语言TIPR数据集上取得了新的最先进结果。

Conclusion: Bi-IRRA框架有效解决了多语言文本到图像人物检索中的模态异质性问题，为多语言环境下的跨模态检索提供了有效解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [167] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det是一个无需文本提示的开放世界3D检测器，利用2D基础模型的泛化能力和零样本能力，结合2D语义先验和3D几何先验进行类别无关的物体检测，在开放世界场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统闭集3D检测器难以泛化到开放世界场景，而直接使用3D开放词汇模型又面临词汇扩展和语义重叠问题。需要研究能够检测训练中未见物体的广义3D物体性学习。

Method: 提出OP3Det：1) 利用2D基础模型的泛化能力，结合2D语义先验和3D几何先验生成类别无关的物体提议；2) 通过跨模态专家混合集成点云和RGB图像的互补信息，动态路由单模态和多模态特征来学习广义3D物体性。

Result: 在广泛实验中，OP3Det在AR指标上显著超越现有开放世界3D检测器达16.0%，相比闭世界3D检测器提升13.5%。

Conclusion: OP3Det通过结合2D基础模型和跨模态融合，成功实现了无需文本提示的开放世界3D物体检测，展示了在检测未见物体方面的卓越性能。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [168] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了一种广义对抗求解器（GAS），通过简单的ODE求解器参数化和对抗训练，在减少扩散模型采样步骤的同时保持细节保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽然生成质量优秀，但采样计算成本高。现有蒸馏方法依赖复杂训练技巧且不能很好保持细节。

Method: 提出广义求解器参数化ODE采样器，无需额外训练技巧；结合原始蒸馏损失和对抗训练以减少伪影并增强细节保真度。

Result: 在相似资源约束下，相比现有求解器训练方法表现出更优越的性能。

Conclusion: 广义对抗求解器在减少采样步骤的同时有效保持了生成质量，代码已开源。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [169] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT是一种后预训练结构化剪枝方法，能够在不同计算预算下实现弹性推理，无需标签数据或重新训练，在5分钟内生成可调节的弹性模型。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型只有有限的预定义尺寸，无法灵活适应实际部署中的计算约束，需要一种能够生成连续计算预算下弹性模型的方法。

Method: 结合梯度信息与跨网络结构相关性（通过进化算法近似），使用自监督重要性评分机制，无需分类头即可泛化到各种模型。

Result: 在DINO、SigLIPv2、DeIT和AugReg模型上的实验表明，该方法在各种稀疏度下均优于现有方法，单张A100 GPU上不到5分钟即可生成弹性模型。

Conclusion: SnapViT为预训练视觉Transformer提供了高效的剪枝策略，通过新颖的Hessian非对角结构进化和自监督重要性评分，在不需重新训练或标签的情况下保持强性能。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [170] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出一种两阶段方法检测帕金森病，通过图像分块和集成学习策略，显著提高了对未见患者数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病早期检测方法存在两个主要问题：数据集不足和对未见患者数据的鲁棒性差。

Method: 采用两阶段方法：第一阶段按绘图类型分类，第二阶段将图像分为2x2块，分别提取特征并检测帕金森病指标，最后使用集成方法合并各块决策。

Result: 在NewHandPD数据集上，对已见患者准确率达97.08%，对未见患者达94.91%，性能差距仅2.17个百分点，优于现有方法4.76个百分点的下降。

Conclusion: 提出的分块策略和集成方法有效解决了帕金森病检测中对未见患者数据泛化能力不足的问题，显著提升了检测性能。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [171] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 提出了一种用于分析循环血细胞簇图像的计算框架，通过YOLOv11模型分类细胞簇图像，并利用多通道荧光染色识别簇内细胞类型，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 流式细胞术结合荧光染色常用于分析循环血细胞簇，但现有机器学习方法主要针对单细胞图像分析，缺乏针对不规则形状、异质性细胞簇的自动分析工具。

Method: 采用两步分析策略：1) 使用YOLOv11模型对图像进行细胞簇与非细胞簇分类；2) 通过叠加簇轮廓与多通道荧光染色区域来识别细胞类型。

Result: 在细胞簇分类和表型识别方面均达到超过95%的准确率，优于传统CNN和ViT模型。

Conclusion: 该自动化框架有效分析流式细胞术中的循环血细胞簇图像，具有扩展到免疫细胞和肿瘤细胞簇分析的潜力。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [172] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS是一个针对3D高斯溅射(3DGS)在雨滴条件下的综合基准测试，评估从无约束雨滴污染图像到清晰3D重建的完整流程，揭示了现有方法在真实雨滴场景中的性能局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常在已知相机姿态的合成雨滴图像上评估3DGS，但真实场景中雨滴会干扰相机姿态估计和点云初始化，且合成与真实雨滴存在显著域差距。

Method: 构建包含雨滴聚焦、背景聚焦和无雨真值三个对齐图像集的真实雨滴重建数据集，设计完整评估流程：数据准备、数据处理和雨滴感知3DGS评估。

Result: 揭示了相机焦点位置对3DGS重建性能的影响，以及不准确的姿态和点云初始化对重建的干扰，为开发更鲁棒的雨滴条件下3DGS方法提供了明确方向。

Conclusion: RaindropGS基准测试系统性地评估了3DGS在雨滴条件下的完整重建流程，识别了关键性能瓶颈，为改进雨滴干扰下的3D重建技术提供了重要指导。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [173] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 提出了MT-Video-Bench，一个用于评估多模态大语言模型在多轮视频对话中理解能力的基准测试，包含987个精心策划的多轮对话，涵盖六个核心能力。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准仅限于单轮问答，忽视了现实场景中多轮对话的复杂性，需要开发更全面的视频理解评估工具。

Method: 构建MT-Video-Bench基准，主要评估六个核心能力（感知性和交互性），涵盖来自不同领域的987个多轮对话，并与现实应用（如交互式体育分析和视频智能辅导）严格对齐。

Result: 广泛评估了各种最先进的开源和闭源MLLMs，揭示了它们在处理多轮视频对话方面的显著性能差异和局限性。

Conclusion: MT-Video-Bench基准将公开提供，以促进未来研究，帮助改进多模态大语言模型在多轮视频对话中的表现。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [174] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 该研究探索了签名伪造检测的特征学习策略，重点关注提高跨数据集泛化能力。使用三个公开基准数据集，开发了基于原始签名图像和壳预处理两种实验流程，结果显示原始图像模型性能更好，但壳预处理模型显示出未来改进潜力。


<details>
  <summary>Details</summary>
Motivation: 自动签名验证在银行、身份认证和法律文档中至关重要，但现有深度学习方法在跨数据集泛化方面存在困难，手写风格和采集协议的变化会降低性能。

Method: 使用CEDAR、ICDAR和GPDS Synthetic三个公开基准数据集，开发了两种实验流程：基于原始签名图像的方法和采用壳预处理的方法。

Result: 原始图像模型在跨基准测试中表现更好，而基于壳预处理的模型显示出未来改进的潜力，但两种方法之间没有明确的优劣之分。

Conclusion: 虽然原始图像模型性能更优，但壳预处理方法为开发鲁棒的跨域签名验证系统提供了有前景的方向，需要进一步研究来改进其性能。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [175] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 研究基于扩散变换器的图像到视频模型能否生成拥挤公共场景中真实的行人移动模式，通过轨迹基准中的关键帧进行条件生成并评估行人动态性能。


<details>
  <summary>Details</summary>
Motivation: 调查高性能I2V模型是否具备生成真实行人移动模式的能力，这些模型通过大规模视频数据集训练已显示出强大的世界建模能力。

Method: 使用行人轨迹基准中提取的关键帧作为条件输入I2V模型，然后通过行人动态的定量指标评估其轨迹预测性能。

Result: 论文提出了评估框架，但具体实验结果未在摘要中提供。

Conclusion: 该研究探索了I2V模型在行人轨迹预测方面的潜力，为评估生成模型的行人动态建模能力提供了方法论框架。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [176] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种无需训练的描述符无关方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配，在多外观和视角变化的VPR任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉位置识别中，虽然增加数据多样性和模型复杂度能提升鲁棒性，但会带来高昂的计算成本。现有的描述符级融合方法要么针对多传感器设置，要么依赖启发式方法，在应对外观和视角变化时收益有限。

Method: 训练免费、描述符无关的方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配。

Result: 在多外观数据上，Recall@1提升约18%；在非结构化数据上提升约5%，在应对外观和视角变化时均优于多参考基线方法。

Conclusion: 该方法在保持轻量级的同时展现出强大的泛化能力，在多参考视觉位置识别任务中表现优异。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [177] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出基于双编码器和注意力的皮肤病变分类框架，结合精确的病灶分割和临床元数据，在提高准确率的同时增强模型可解释性


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测对改善患者预后至关重要，但现有深度学习模型存在"黑箱"问题，临床信任度低，且面临类内差异大、类间差异小的挑战

Method: 使用带双注意力门和空洞空间金字塔池化的Deep-UNet进行病灶分割；分类阶段采用双DenseNet201编码器分别处理原始图像和分割病灶，通过多头交叉注意力融合特征；同时使用基于transformer的模块整合患者元数据

Result: 在HAM10000数据集和ISIC 2018、2019挑战中实现最先进的分割性能，显著提高分类准确率和平均AUC；Grad-CAM热图验证模型基于病灶区域而非背景特征进行预测

Conclusion: 将精确病灶分割、临床数据与基于注意力的特征融合相结合，可构建更准确且可解释的皮肤癌分类模型

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [178] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种新的高效视觉语言模型推理范式，通过在预填充阶段剪枝冗余视觉令牌和解码阶段仅检索查询相关令牌，实现视觉稀疏性解耦，显著提升推理速度同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的可扩展性受到视觉令牌数量增长的限制，这些令牌主导了推理延迟。需要一种方法在保持模型能力的同时显著提升推理效率。

Method: 提出SparseVILA框架，在预填充阶段进行查询无关的令牌剪枝，在解码阶段进行查询感知的令牌检索，基于AWQ优化的推理流水线实现。

Result: 在长上下文视频任务中实现4.0倍预填充加速、2.5倍解码加速和2.6倍端到端加速，同时在文档理解和推理任务上提高准确性。

Conclusion: 通过解耦查询无关剪枝和查询感知检索，SparseVILA为高效多模态推理开辟了新方向，提供无需训练、架构无关的加速框架。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [179] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA是一个基础模型，通过混合动作机制将GUI原语操作与高级程序化工具调用无缝集成，解决了计算机使用代理的视觉定位和执行链问题。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理仅依赖原始操作（点击、输入、滚动），需要准确的视觉定位和冗长的执行链，导致级联失败和性能瓶颈，而其他代理可以利用丰富的程序化接口。

Method: 包含四个关键组件：自动化工具扩展管道、合成数据引擎生成可验证任务、大规模高质量混合动作轨迹收集、两阶段训练管道（监督微调+在线强化学习）。

Result: 7B和32B模型在OSWorld上相对基准模型平均提升22%，步骤减少11%；在WindowsAgentArena上达到21.7%成功率，优于基于Windows数据训练的基线。

Conclusion: 混合动作机制在减少错误传播的同时保持执行效率，证明了将低级GUI操作与高级程序化工具调用集成的有效性。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [180] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph框架通过将长文本渲染为图像，利用视觉语言模型处理，实现3-4倍文本压缩，在保持准确性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型处理长上下文时面临的高计算和内存成本问题，使百万token级别的长文本处理更加实用。

Method: 将长文本渲染成图像，使用视觉语言模型处理，并设计基于LLM的遗传搜索来优化视觉渲染配置，平衡准确性和压缩率。

Result: 在多个长上下文基准测试中达到3-4倍token压缩，准确性与Qwen3-8B相当，预填充和解码速度提升约4倍，SFT训练速度提升约2倍。128K上下文的VLM可扩展到处理1M token级别的文本任务。

Conclusion: 视觉上下文扩展是解决长文本处理挑战的有效方法，在保持性能的同时显著提升效率，并为多模态任务带来额外益处。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 提出了ConsistEdit方法，专门针对MM-DiT架构，通过视觉注意力控制、掩码引导预注意力融合和差异化token操作，实现一致且符合提示的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由注意力控制方法在编辑强度和源一致性之间存在权衡，特别是在多轮和视频编辑中视觉错误会累积。MM-DiT架构的模态集成机制为解决这些问题提供了新机会。

Method: 基于MM-DiT注意力机制分析，提出ConsistEdit方法，包含视觉注意力控制、掩码引导预注意力融合、差异化查询/键/值token操作。

Result: 在广泛图像和视频编辑任务中达到最先进性能，支持所有推理步骤和注意力层编辑，无需人工干预，实现稳健的多轮和多区域编辑。

Conclusion: ConsistEdit是首个无需手工操作即可在所有推理步骤和注意力层进行编辑的方法，显著提升可靠性和一致性，支持渐进结构调整。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [182] [MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding](https://arxiv.org/abs/2510.16273)
*Jingyue Huang,Zachary Novack,Phillip Long,Yupeng Hou,Ke Chen,Taylor Berg-Kirkpatrick,Julian McAuley*

Main category: cs.SD

TL;DR: MuseTok是一种用于符号音乐的tokenization方法，基于RQ-VAE和Transformer编码器-解码器框架，在音乐生成和理解任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 受离散表示学习在图像、语音和语言领域成功的启发，研究者希望开发一种专门用于符号音乐的tokenization方法，以同时提升音乐生成和理解任务的性能。

Method: 使用残差向量量化变分自编码器(RQ-VAE)对逐小节音乐片段进行处理，在Transformer编码器-解码器框架下生成音乐代码。

Result: MuseTok在旋律提取、和弦识别和情感识别等语义理解任务中优于之前的表示学习方法，在内容生成方面保持可比性能。定性分析显示它能有效捕捉大规模音乐集合中的基本音乐概念。

Conclusion: MuseTok是一种有效的符号音乐tokenization方法，能够同时支持高质量的音乐生成和准确的音乐理解任务，成功捕捉了底层音乐概念。

Abstract: Discrete representation learning has shown promising results across various
domains, including generation and understanding in image, speech and language.
Inspired by these advances, we propose MuseTok, a tokenization method for
symbolic music, and investigate its effectiveness in both music generation and
understanding tasks. MuseTok employs the residual vector quantized-variational
autoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based
encoder-decoder framework, producing music codes that achieve high-fidelity
music reconstruction and accurate understanding of music theory. For
comprehensive evaluation, we apply MuseTok to music generation and semantic
understanding tasks, including melody extraction, chord recognition, and
emotion recognition. Models incorporating MuseTok outperform previous
representation learning baselines in semantic understanding while maintaining
comparable performance in content generation. Furthermore, qualitative analyses
on MuseTok codes, using ground-truth categories and synthetic datasets, reveal
that MuseTok effectively captures underlying musical concepts from large music
collections.

</details>


### [183] [Transmission of High-Amplitude Sound through Leakages of Ill-fitting Earplugs](https://arxiv.org/abs/2510.16355)
*Haocheng Yu,Krishan K. Ahuja,Lakshmi N. Sankar,Spencer H. Bryngelson*

Main category: cs.SD

TL;DR: 该研究分析了高噪声环境下耳塞密封不良导致的声泄漏问题，通过计算和实验方法研究了不同泄漏几何形状的声学特性，发现未密封硅胶耳塞在120分贝声压级下平均传输损失减少约18分贝。


<details>
  <summary>Details</summary>
Motivation: 高声压级环境存在噪声性听力损失风险，耳塞密封不良导致的声泄漏是重要问题，需要深入研究其声学特性和影响机制。

Method: 结合计算和实验方法，获取独立狭缝谐振器和孔口的声学传输数据，研究不同孔径泄漏几何形状的频率相关声功率吸收系数和传输损失，实验频率范围1-5kHz，声压级120-150分贝。

Result: 未密封硅胶耳塞在120分贝声压级下平均传输损失减少约18分贝；直接数值模拟显示在150分贝声压级下，密封不良耳塞模型中声能转化为涡量。

Conclusion: 研究结果强调了耳塞设计在高声压级环境中的重要性，揭示了声泄漏对听力保护效果的影响机制。

Abstract: High sound pressure levels (SPL) pose notable risks in loud environments,
particularly due to noise-induced hearing loss. Ill-fitting earplugs often lead
to sound leakage, a phenomenon this study seeks to investigate. To validate our
methodology, we first obtained computational and experimental acoustic
transmission data for stand-alone slit resonators and orifices, for which
extensive published data are readily available for comparison. We then examined
the frequency-dependent acoustic power absorption coefficient and transmission
loss (TL) across various leakage geometries, modeled using different orifice
diameters. Experimental approaches spanned a frequency range of 1--5 kHz under
SPL conditions of 120--150 dB. Key findings reveal that unsealed silicone
rubber earplugs demonstrate an average TL reduction of approximately 18 dB at
an overall incident SPL (OISPL) of 120 dB. Direct numerical simulations further
highlight SPL-dependent acoustic dissipation mechanisms, showing the conversion
of acoustic energy into vorticity in ill-fitting earplug models at an OISPL of
150 dB. These results highlight the role of earplug design for
high-sound-pressure-level environments.

</details>


### [184] [Interpreting the Dimensions of Speaker Embedding Space](https://arxiv.org/abs/2510.16489)
*Mark Huckvale*

Main category: cs.SD

TL;DR: 该研究分析了说话人嵌入系统如何表示说话人的声学特征，发现9个可解释的声学参数可以预测嵌入向量，揭示了嵌入系统中存在隐式的性别识别，但年龄信息捕捉不佳。


<details>
  <summary>Details</summary>
Motivation: 说话人嵌入在说话人验证系统中被广泛使用，但通常被视为"黑盒"编码。研究旨在探索这些嵌入如何与传统声学和语音维度相关联，特别是与声学特征、年龄和性别的关系。

Method: 使用包含10,000个说话人的大型语料库和三种嵌入系统，分析9个可解释声学参数与嵌入向量的关系，并与7个主成分进行比较。

Result: 9个声学参数预测嵌入向量的效果与7个主成分相当，解释了超过50%的数据方差。发现某些主维度对男性和女性说话人作用不同，表明嵌入系统中存在隐式性别识别，但年龄信息捕捉效果不佳。

Conclusion: 说话人嵌入系统能够有效捕捉声学特征和性别信息，但对年龄信息的表示能力有限，这为嵌入计算的改进提供了机会。

Abstract: Speaker embeddings are widely used in speaker verification systems and other
applications where it is useful to characterise the voice of a speaker with a
fixed-length vector. These embeddings tend to be treated as "black box"
encodings, and how they relate to conventional acoustic and phonetic dimensions
of voices has not been widely studied. In this paper we investigate how
state-of-the-art speaker embedding systems represent the acoustic
characteristics of speakers as described by conventional acoustic descriptors,
age, and gender. Using a large corpus of 10,000 speakers and three embedding
systems we show that a small set of 9 acoustic parameters chosen to be
"interpretable" predict embeddings about the same as 7 principal components,
corresponding to over 50% of variance in the data. We show that some principal
dimensions operate differently for male and female speakers, suggesting there
is implicit gender recognition within the embedding systems. However we show
that speaker age is not well captured by embeddings, suggesting opportunities
exist for improvements in their calculation.

</details>


### [185] [Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios](https://arxiv.org/abs/2510.16700)
*Shiyao Wang,Shiwan Zhao,Jiaming Zhou,Yong Qin*

Main category: cs.SD

TL;DR: 提出了一种新颖的文本覆盖策略，用于解决构音障碍语音识别中的数据稀缺问题，通过高效的零样本/单样本数据增强显著提升对未见构音障碍说话者的识别性能。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音识别面临数据稀缺的挑战，特别是在零样本或单样本学习场景下，现有生成模型难以准确表示目标域数据，限制了系统性能。

Method: 采用文本覆盖策略进行文本匹配数据合成，专门设计用于零样本/单样本构音障碍数据增强。

Result: 该方法显著提升了构音障碍语音识别系统对未见说话者的性能表现。

Conclusion: 所提出的文本覆盖策略为构音障碍康复项目和日常语句交流场景提供了有效的解决方案，具有重要的实际应用价值。

Abstract: Dysarthric speech recognition (DSR) research has witnessed remarkable
progress in recent years, evolving from the basic understanding of individual
words to the intricate comprehension of sentence-level expressions, all driven
by the pressing communication needs of individuals with dysarthria.
Nevertheless, the scarcity of available data remains a substantial hurdle,
posing a significant challenge to the development of effective sentence-level
DSR systems. In response to this issue, dysarthric data augmentation (DDA) has
emerged as a highly promising approach. Generative models are frequently
employed to generate training data for automatic speech recognition tasks.
However, their effectiveness hinges on the ability of the synthesized data to
accurately represent the target domain. The wide-ranging variability in
pronunciation among dysarthric speakers makes it extremely difficult for models
trained on data from existing speakers to produce useful augmented data,
especially in zero-shot or one-shot learning settings. To address this
limitation, we put forward a novel text-coverage strategy specifically designed
for text-matching data synthesis. This innovative strategy allows for efficient
zero/one-shot DDA, leading to substantial enhancements in the performance of
DSR when dealing with unseen dysarthric speakers. Such improvements are of
great significance in practical applications, including dysarthria
rehabilitation programs and day-to-day common-sentence communication scenarios.

</details>


### [186] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: U-Codec是一种超低帧率（5Hz）神经语音编解码器，通过Transformer长时依赖模块和优化RVQ配置，在保持高质量重建的同时实现快速语音生成，并将LLM-based TTS扩展到32层RVQ，推理速度提升约3倍。


<details>
  <summary>Details</summary>
Motivation: 极低帧率（5Hz）压缩通常会导致严重的可懂度和频谱细节损失，需要开发能够在极低帧率下保持高保真重建和快速生成的语音编解码器。

Method: 引入基于Transformer的帧间长时依赖模块，系统探索RVQ深度和码本大小优化配置，并将U-Codec应用于LLM-based自回归TTS模型，采用全局和局部层次架构捕捉多层token依赖关系。

Result: U-Codec将LLM-based TTS从3层RVQ 50Hz扩展到32层RVQ 5Hz，推理速度比高帧率编解码器提升约3倍，同时保持相似性和自然度。

Conclusion: 验证了使用高度压缩的5Hz离散token进行快速高保真语音合成的可行性。

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [187] [Schrödinger Bridge Mamba for One-Step Speech Enhancement](https://arxiv.org/abs/2510.16834)
*Jing Yang,Sirui Wang,Chao Wu,Fan Fan*

Main category: cs.SD

TL;DR: 提出了Schrödinger Bridge Mamba (SBM)框架，结合Schrödinger Bridge训练范式和Mamba选择性状态空间模型，用于生成式语音增强任务。


<details>
  <summary>Details</summary>
Motivation: 基于Schrödinger Bridge训练范式和Mamba选择性状态空间模型之间的内在兼容性，探索新的深度生成模型框架。

Method: 使用Schrödinger Bridge训练范式与Mamba选择性状态空间模型相结合，实现仅需1步推理的生成式语音增强。

Result: 在四个基准数据集上的联合去噪和去混响任务中，SBM仅用1步推理就超越了强基线模型，并实现了最佳实时因子(RTF)。

Conclusion: SB范式与选择性状态空间模型架构的结合为探索适用于广泛生成任务的新深度生成模型指明了有前景的方向。

Abstract: We propose Schr\"odinger Bridge Mamba (SBM), a new concept of
training-inference framework motivated by the inherent compatibility between
Schr\"odinger Bridge (SB) training paradigm and selective state-space model
Mamba. We exemplify the concept of SBM with an implementation for generative
speech enhancement. Experiments on a joint denoising and dereverberation task
using four benchmark datasets demonstrate that SBM, with only 1-step inference,
outperforms strong baselines with 1-step or iterative inference and achieves
the best real-time factor (RTF). Beyond speech enhancement, we discuss the
integration of SB paradigm and selective state-space model architecture based
on their underlying alignment, which indicates a promising direction for
exploring new deep generative models potentially applicable to a broad range of
generative tasks. Demo page: https://sbmse.github.io

</details>


### [188] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现大型音频语言模型在情感变化下的安全对齐存在不一致性，不同情感和强度会引发不同程度的危险响应。


<details>
  <summary>Details</summary>
Motivation: 虽然大型音频语言模型的感知、推理和任务性能已被广泛研究，但其在副语言变化下的安全对齐仍未充分探索，特别是说话者情感的作用。

Method: 构建包含多种情感和强度的恶意语音指令数据集，并评估多个最先进的大型音频语言模型。

Result: 发现显著的安全不一致性：不同情感引发不同程度的危险响应，强度影响呈非单调性，中等强度表达通常风险最大。

Conclusion: 这些发现揭示了大型音频语言模型中被忽视的脆弱性，呼吁需要专门设计的情感变化下的对齐策略，这是在实际场景中可信部署的前提条件。

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [189] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: SAKE是首个专门用于编辑大型音频语言模型中听觉属性知识的基准，超越了传统的文本和视觉模态，评估了七种编辑方法在四个维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 知识编辑提供了一种无需完整重新训练即可更新模型知识的高效方法，但先前工作主要集中在文本或视觉模态上，缺乏对听觉模态的研究。

Method: 构建了SAKE基准，针对大型音频语言模型中的抽象听觉属性知识，在四个维度（可靠性、泛化性、音频/文本局部性、可移植性）上对七种编辑方法进行基准测试。

Result: 结果揭示了挑战：保护与编辑无关的同类属性知识、将编辑泛化到多模态推理、在顺序更新下保持编辑效果。

Conclusion: SAKE为研究知识编辑如何扩展到听觉模态提供了原则性框架，为在更多样化的现实场景中维护和适应大型音频语言模型开辟了新方向。

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [190] [DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift](https://arxiv.org/abs/2510.17345)
*Peihong Zhang,Yuxuan Liu,Rui Sang,Zhixin Li,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: 提出动态双信号课程（DDSC），通过在线调整训练课程来解决声学场景分类中的设备域偏移问题，结合域不变性和学习进度信号来动态调整样本权重。


<details>
  <summary>Details</summary>
Motivation: 声学场景分类面临设备引起的域偏移问题，特别是在标签有限的情况下。现有课程学习方法采用静态训练计划，无法适应学习过程中样本难度和边际效用的动态变化。

Method: DDSC方法在每个epoch计算两个信号：域不变性信号和学习进度信号，通过时变调度器将它们融合为每个样本的权重，早期优先域不变样本，后期逐步强调设备特定案例。

Result: 在DCASE 2024 Task 1官方协议下，DDSC在不同ASC基线和标签预算下持续提升跨设备性能，在未见设备分割上获得最大增益。

Conclusion: DDSC是一种轻量级、架构无关的方法，无需额外推理开销，能有效解决设备域偏移问题，显著提升跨设备声学场景分类性能。

Abstract: Acoustic scene classification (ASC) suffers from device-induced domain shift,
especially when labels are limited. Prior work focuses on curriculum-based
training schedules that structure data presentation by ordering or reweighting
training examples from easy-to-hard to facilitate learning; however, existing
curricula are static, fixing the ordering or the weights before training and
ignoring that example difficulty and marginal utility evolve with the learned
representation. To overcome this limitation, we propose the Dynamic Dual-Signal
Curriculum (DDSC), a training schedule that adapts the curriculum online by
combining two signals computed each epoch: a domain-invariance signal and a
learning-progress signal. A time-varying scheduler fuses these signals into
per-example weights that prioritize domain-invariant examples in early epochs
and progressively emphasize device-specific cases. DDSC is lightweight,
architecture-agnostic, and introduces no additional inference overhead. Under
the official DCASE 2024 Task~1 protocol, DDSC consistently improves
cross-device performance across diverse ASC baselines and label budgets, with
the largest gains on unseen-device splits.

</details>


### [191] [TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation](https://arxiv.org/abs/2510.17346)
*Peihong Zhang,Zhixin Li,Yuxuan Liu,Rui Sang,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: TopSeg是一个基于拓扑表示的心音分割框架，使用多尺度拓扑特征和轻量级时序卷积网络，在数据有限的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统基于时频特征的心音分割方法依赖大量专家标注数据，限制了鲁棒性和实际部署。需要开发数据效率更高的方法。

Method: 提出TopSeg框架：编码阶段使用多尺度拓扑特征捕捉心音动态，解码阶段使用轻量级TCN网络，并加入顺序和时长约束的推理步骤。

Result: 在数据预算有限时，拓扑特征显著优于频谱图和包络输入；完整系统在相同数据预算下超越代表性端到端基线方法，在完整数据下保持竞争力。

Conclusion: 拓扑感知表示为数据高效、跨数据集的心音分割提供了强大的归纳偏置，在标注数据有限时具有实用价值。

Abstract: Deep learning approaches for heart-sound (PCG) segmentation built on
time--frequency features can be accurate but often rely on large expert-labeled
datasets, limiting robustness and deployment. We present TopSeg, a topological
representation-centric framework that encodes PCG dynamics with multi-scale
topological features and decodes them using a lightweight temporal
convolutional network (TCN) with an order- and duration-constrained inference
step. To evaluate data efficiency and generalization, we train exclusively on
PhysioNet 2016 dataset with subject-level subsampling and perform external
validation on CirCor dataset. Under matched-capacity decoders, the topological
features consistently outperform spectrogram and envelope inputs, with the
largest margins at low data budgets; as a full system, TopSeg surpasses
representative end-to-end baselines trained on their native inputs under the
same budgets while remaining competitive at full data. Ablations at 10%
training confirm that all scales contribute and that combining H_0 and H_1
yields more reliable S1/S2 localization and boundary stability. These results
indicate that topology-aware representations provide a strong inductive bias
for data-efficient, cross-dataset PCG segmentation, supporting practical use
when labeled data are limited.

</details>


### [192] [Not All Deepfakes Are Created Equal: Triaging Audio Forgeries for Robust Deepfake Singer Identification](https://arxiv.org/abs/2510.17474)
*Davide Salvi,Hendrik Vincent Koops,Elio Quinton*

Main category: cs.SD

TL;DR: 提出一个两阶段管道来识别歌手声音相似度，先过滤低质量伪造音频，再识别高质量深度伪造和真实音频中的歌手身份


<details>
  <summary>Details</summary>
Motivation: 高度逼真的歌声深度伪造对艺术家形象和内容真实性保护构成挑战，需要自动歌手识别技术来保护艺术家免受未经授权的声音使用

Method: 两阶段管道：第一阶段使用判别器模型过滤无法准确复制声音相似度的低质量伪造；第二阶段使用仅在真实录音上训练的模型识别剩余高质量深度伪造和真实音频中的歌手

Result: 实验表明该系统在真实和合成内容上都持续优于现有基线方法

Conclusion: 该两阶段方法能有效识别高质量歌声深度伪造中的歌手身份，为艺术家和权利持有者提供保护工具

Abstract: The proliferation of highly realistic singing voice deepfakes presents a
significant challenge to protecting artist likeness and content authenticity.
Automatic singer identification in vocal deepfakes is a promising avenue for
artists and rights holders to defend against unauthorized use of their voice,
but remains an open research problem. Based on the premise that the most
harmful deepfakes are those of the highest quality, we introduce a two-stage
pipeline to identify a singer's vocal likeness. It first employs a
discriminator model to filter out low-quality forgeries that fail to accurately
reproduce vocal likeness. A subsequent model, trained exclusively on authentic
recordings, identifies the singer in the remaining high-quality deepfakes and
authentic audio. Experiments show that this system consistently outperforms
existing baselines on both authentic and synthetic content.

</details>


### [193] [AWARE: Audio Watermarking with Adversarial Resistance to Edits](https://arxiv.org/abs/2510.17512)
*Kosta Pavlović,Lazar Stanarević,Petar Nedić,Slavko Kovačević,Igor Djurović*

Main category: cs.SD

TL;DR: AWARE是一种基于对抗优化的音频水印方法，通过在时频域进行嵌入，使用时间顺序无关的检测器和比特级读出头，在保持高音频质量的同时实现对各种音频编辑的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统学习型音频水印方法依赖模拟失真训练，容易过拟合且泛化能力有限，需要一种不依赖攻击模拟堆栈的新方法。

Method: 在时频域进行对抗优化嵌入，使用时间顺序无关的检测器和比特级读出头来聚合时间证据，实现可靠的比特解码。

Result: AWARE在音频质量（PESQ/STOI）和语音可懂度方面表现优异，在各种音频编辑下保持低比特错误率，超越现有学习型音频水印系统。

Conclusion: AWARE提供了一种不依赖手工设计失真模拟的有效音频水印方案，在保持音频质量的同时实现了对多种编辑操作的鲁棒性。

Abstract: Prevailing practice in learning-based audio watermarking is to pursue
robustness by expanding the set of simulated distortions during training.
However, such surrogates are narrow and prone to overfitting. This paper
presents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an
alternative approach that avoids reliance on attack-simulation stacks and
handcrafted differentiable distortions. Embedding is obtained via adversarial
optimization in the time-frequency domain under a level-proportional perceptual
budget. Detection employs a time-order-agnostic detector with a Bitwise Readout
Head (BRH) that aggregates temporal evidence into one score per watermark bit,
enabling reliable watermark decoding even under desynchronization and temporal
cuts. Empirically, AWARE attains high audio quality and speech intelligibility
(PESQ/STOI) and consistently low BER across various audio edits, often
surpassing representative state-of-the-art learning-based audio watermarking
systems.

</details>


### [194] [SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering](https://arxiv.org/abs/2510.17633)
*Weilin Lin,Jianze Li,Hui Xiong,Li Liu*

Main category: cs.SD

TL;DR: 提出了SARSteer框架，这是首个针对大型音频语言模型（LALMs）的推理时防御方法，通过文本驱动的拒绝引导和分解安全空间消融来解决音频输入引发的安全风险。


<details>
  <summary>Details</summary>
Motivation: 音频输入比文本更容易引发有害响应，现有LLM和视觉语言模型的安全对齐方法在LALMs上存在两个关键限制：LLM引导在音频输入下失效，提示防御在良性查询上导致过度拒绝。

Method: SARSteer框架包含文本驱动的拒绝引导（无需操纵音频输入）和分解安全空间消融（减轻过度拒绝）。

Result: 广泛实验表明SARSteer显著提高了对有害查询的拒绝能力，同时保持了良性响应。

Conclusion: SARSteer为LALMs的安全对齐建立了原则性步骤，在保护安全的同时避免了过度拒绝问题。

Abstract: Large Audio-Language Models (LALMs) are becoming essential as a powerful
multimodal backbone for real-world applications. However, recent studies show
that audio inputs can more easily elicit harmful responses than text, exposing
new risks toward deployment. While safety alignment has made initial advances
in LLMs and Large Vision-Language Models (LVLMs), we find that vanilla
adaptation of these approaches to LALMs faces two key limitations: 1) LLM-based
steering fails under audio input due to the large distributional gap between
activations, and 2) prompt-based defenses induce over-refusals on benign-speech
queries. To address these challenges, we propose Safe-Ablated Refusal Steering
(SARSteer), the first inference-time defense framework for LALMs. Specifically,
SARSteer leverages text-derived refusal steering to enforce rejection without
manipulating audio inputs and introduces decomposed safe-space ablation to
mitigate over-refusal. Extensive experiments demonstrate that SARSteer
significantly improves harmful-query refusal while preserving benign responses,
establishing a principled step toward safety alignment in LALMs.

</details>


### [195] [DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model](https://arxiv.org/abs/2510.17662)
*Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: DELULU是一个说话人感知的自监督基础模型，通过在伪标签生成过程中引入外部监督，显著提升了说话人区分特征的捕获能力，在说话人验证等任务上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督语音模型在内容驱动任务上表现出色，但在捕获说话人区分特征方面存在局限，而这些特征对于说话人验证、日记化和分析等应用至关重要。

Method: 利用ReDimNet的帧级嵌入来指导k-means聚类过程，引入说话人区分性归纳偏置；采用掩码预测和去噪的双重目标进行训练，增强鲁棒性和泛化能力。

Result: 在说话人中心任务上显著优于现有自监督学习模型，说话人验证的等错误率相对改进达62%，在性别、年龄、口音和说话人计数等零样本分析任务上也有持续提升。

Conclusion: DELULU是一个强大的通用说话人感知语音处理编码器，即使无需任务特定微调也能实现卓越性能。

Abstract: Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [196] [The Strongly Stable Roommates Problem and Linear Programming](https://arxiv.org/abs/2510.16385)
*Naoyuki Kamiyama*

Main category: cs.GT

TL;DR: 提出了一种新的多项式时间算法，用于检查带平局的稳定室友问题中是否存在强稳定匹配。


<details>
  <summary>Details</summary>
Motivation: 稳定室友问题是稳定匹配问题的非二分图版本，本文关注带平局情况下的强稳定性概念。

Method: 将Abeledo和Blum针对严格偏好稳定室友问题的线性规划方法扩展到带平局的情况。

Result: 开发了一个多项式时间算法来检查强稳定匹配的存在性。

Conclusion: 成功扩展了线性规划方法，为带平局的稳定室友问题提供了有效的强稳定匹配存在性检查算法。

Abstract: The stable roommates problem is a non-bipartite version of the stable
matching problem in a bipartite graph. In this paper, we consider the stable
roommates problem with ties. In particular, we focus on strong stability, which
is one of the main stability concepts in the stable roommates problem with
ties. We propose a new polynomial-time algorithm for the problem of checking
the existence of a strongly stable matching in the stable roommates problem
with ties. More concretely, we extend the linear programming approach of
Abeledo and Blum to the stable roommates problem with strict preferences to our
problem.

</details>


### [197] [No-Regret Online Autobidding Algorithms in First-price Auctions](https://arxiv.org/abs/2510.16869)
*Yuan Deng,Yilin Li,Wei Tang,Hanrui Zhang*

Main category: cs.GT

TL;DR: 本文针对带ROI约束的非真实机制重复一价拍卖，设计了在线竞价算法，在完全反馈和强盗反馈两种设置下分别实现了接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在线广告竞价中，广告主需要在ROI约束和预算约束下优化竞价策略。现有研究主要关注真实拍卖机制或非真实拍卖但基准较弱的情况，本文旨在解决非真实机制下带ROI约束的竞价算法设计挑战。

Method: 开发了针对重复一价拍卖的在线竞价算法，基准为事后最优随机策略。在完全反馈设置下（观察到最大竞争出价）和强盗反馈设置下（仅观察到是否赢得拍卖）分别设计算法。

Result: 在完全反馈设置下实现了接近最优的$\widetilde{O}(\sqrt{T})$遗憾界，在强盗反馈设置下实现了$\widetilde{O}(T^{3/4})$遗憾界。

Conclusion: 本文为非真实机制下带ROI约束的在线广告竞价问题提供了有效的算法解决方案，在两种反馈设置下都取得了良好的理论性能保证。

Abstract: Automated bidding to optimize online advertising with various constraints,
e.g. ROI constraints and budget constraints, is widely adopted by advertisers.
A key challenge lies in designing algorithms for non-truthful mechanisms with
ROI constraints. While prior work has addressed truthful auctions or
non-truthful auctions with weaker benchmarks, this paper provides a significant
improvement: We develop online bidding algorithms for repeated first-price
auctions with ROI constraints, benchmarking against the optimal randomized
strategy in hindsight. In the full feedback setting, where the maximum
competing bid is observed, our algorithm achieves a near-optimal
$\widetilde{O}(\sqrt{T})$ regret bound, and in the bandit feedback setting
(where the bidder only observes whether the bidder wins each auction), our
algorithm attains $\widetilde{O}(T^{3/4})$ regret bound.

</details>


### [198] [Convergence of Regret Matching in Potential Games and Constrained Optimization](https://arxiv.org/abs/2510.17067)
*Ioannis Anagnostides,Emanuel Tewolde,Brian Hu Zhang,Ioannis Panageas,Vincent Conitzer,Tuomas Sandholm*

Main category: cs.GT

TL;DR: 本文分析了后悔匹配(RM)及其变体RM+的收敛性，首次证明了交替RM+在约束优化问题中能快速收敛到ε-KKT点，同时揭示了RM在势博弈中收敛到纳什均衡需要指数级时间，这是RM与RM+之间的首个最坏情况分离结果。


<details>
  <summary>Details</summary>
Motivation: 后悔匹配算法在解决零和博弈方面取得了突破性成果，但对其在更广泛场景（如势博弈和约束优化）中的理论收敛性了解甚少。特别是RM在势博弈中是否收敛到纳什均衡是20年来的开放问题，而最近实证表明RM+在约束优化中表现优异。

Method: 使用交替后悔匹配+(RM+)算法，通过将KKT间隙与累积后悔联系起来进行分析。证明了虽然RM+在一般情况下没有单步改进性质，但在特定区域内具有该性质且算法会快速到达并保持在该区域。

Result: 证明了交替RM+在O(1/ε⁴)迭代内收敛到ε-KKT点，当后悔有界时复杂度改进为O(1/ε²)。同时证明了RM（无论是否交替）在双人势博弈中到达粗略近似解需要指数级迭代次数。

Conclusion: RM+是有效且快速的一阶优化器，而RM在势博弈中收敛到纳什均衡比收敛到粗相关均衡慢指数级，这首次建立了RM与RM+之间的最坏情况分离。

Abstract: Regret matching (RM} -- and its modern variants -- is a foundational online
algorithm that has been at the heart of many AI breakthrough results in solving
benchmark zero-sum games, such as poker. Yet, surprisingly little is known so
far in theory about its convergence beyond two-player zero-sum games. For
example, whether regret matching converges to Nash equilibria in potential
games has been an open problem for two decades. Even beyond games, one could
try to use RM variants for general constrained optimization problems. Recent
empirical evidence suggests that they -- particularly regret matching$^+$
(RM$^+$) -- attain strong performance on benchmark constrained optimization
problems, outperforming traditional gradient descent-type algorithms.
  We show that alternating RM$^+$ converges to an $\epsilon$-KKT point after
$O_\epsilon(1/\epsilon^4)$ iterations, establishing for the first time that it
is a sound and fast first-order optimizer. Our argument relates the KKT gap to
the accumulated regret, two quantities that are entirely disparate in general
but interact in an intriguing way in our setting, so much so that when regrets
are bounded, our complexity bound improves all the way to
$O_\epsilon(1/\epsilon^2)$. From a technical standpoint, while RM$^+$ does not
have the usual one-step improvement property in general, we show that it does
in a certain region that the algorithm will quickly reach and remain in
thereafter. In sharp contrast, our second main result establishes a lower
bound: RM, with or without alternation, can take an exponential number of
iterations to reach a crude approximate solution even in two-player potential
games. This represents the first worst-case separation between RM and RM$^+$.
Our lower bound shows that convergence to coarse correlated equilibria in
potential games is exponentially faster than convergence to Nash equilibria.

</details>


### [199] [Eliciting Truthful Feedback for Preference-Based Learning via the VCG Mechanism](https://arxiv.org/abs/2510.17285)
*Leo Landolt,Anna Maddux,Andreas Schlaginhaufen,Saurabh Vaishampayan,Maryam Kamgarpour*

Main category: cs.GT

TL;DR: 提出一种结合偏好学习和VCG支付的算法，用于解决具有私有成本函数的战略代理的资源分配问题，确保近似真实性、个体理性和效率。


<details>
  <summary>Details</summary>
Motivation: 资源分配中存在两个主要挑战：(i)代理的成本函数可能未知或难以明确指定，(ii)代理可能策略性地误报成本。需要设计机制来激励真实报告。

Method: 算法结合偏好学习和VCG支付，通过D-最优设计选择信息性偏好查询，使用最大似然估计成本参数，并基于估计值计算VCG分配和支付。

Result: 在一次性设置中，机制是近似真实、个体理性且高效的，误差为$	ilde{\mathcal O}(K^{-1/2})$；在线设置中，这些保证渐近成立，具有$	ilde{\mathcal O}(T^{2/3})$的次线性遗憾。

Conclusion: 该方法在本地电力市场的需求响应案例研究中得到验证，为具有私有成本函数的战略代理的资源分配问题提供了有效解决方案。

Abstract: We study resource allocation problems in which a central planner allocates
resources among strategic agents with private cost functions in order to
minimize a social cost, defined as an aggregate of the agents' costs. This
setting poses two main challenges: (i) the agents' cost functions may be
unknown to them or difficult to specify explicitly, and (ii) agents may
misreport their costs strategically. To address these challenges, we propose an
algorithm that combines preference-based learning with Vickrey-Clarke-Groves
(VCG) payments to incentivize truthful reporting. Our algorithm selects
informative preference queries via D-optimal design, estimates cost parameters
through maximum likelihood, and computes VCG allocations and payments based on
these estimates. In a one-shot setting, we prove that the mechanism is
approximately truthful, individually rational, and efficient up to an error of
$\tilde{\mathcal O}(K^{-1/2})$ for $K$ preference queries per agent. In an
online setting, these guarantees hold asymptotically with sublinear regret at a
rate of $\tilde{\mathcal O}(T^{2/3})$ after $T$ rounds. Finally, we validate
our approach through a numerical case study on demand response in local
electricity markets.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [200] [A Motivational Driver Steering Model: Task Difficulty Homeostasis From Control Theory Perspective](https://arxiv.org/abs/2510.16247)
*H. Mozaffari,A. Nahvi*

Main category: eess.SY

TL;DR: 提出了一种结合心理学和控制理论的统一方法来建模驾驶员避撞行为，将任务难度稳态理论与Lyapunov稳定性方法相结合，在20-170km/h速度范围内验证了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有计算驾驶员模型大多仅使用控制理论方法，缺乏心理学理论基础，需要开发既通用又符合心理学的碰撞避免驾驶员模型来提升交通安全。

Method: 结合心理学中的"任务难度稳态理论"和控制理论中的"Lyapunov稳定性方法"，建立统一的驾驶员转向行为模型，用于碰撞避免场景。

Result: 通过驾驶模拟器实验验证，模型在20-170km/h速度范围内的两种避撞场景中准确模拟人类行为，平均误差仅为7%。

Conclusion: 该模型成功地将心理学理论与控制理论相结合，为开发更符合人类实际驾驶行为的碰撞避免模型提供了有效框架。

Abstract: A general and psychologically plausible collision avoidance driver model can
improve transportation safety significantly. Most computational driver models
found in the literature have used control theory methods only, and they are not
established based on psychological theories. In this paper, a unified approach
is presented based on concepts taken from psychology and control theory. The
"task difficulty homeostasis theory", a prominent motivational theory, is
combined with the "Lyapunov stability method" in control theory to present a
general and psychologically plausible model. This approach is used to model
driver steering behavior for collision avoidance. The performance of this model
is measured by simulation of two collision avoidance scenarios at a wide range
of speeds from 20 km/h to 170 km/h. The model is validated by experiments on a
driving simulator. The results demonstrate that the model follows human
behavior accurately with a mean error of 7 percent.

</details>


### [201] [Spatial-to-Spectral Harmonic-Modulated Arrays for 6G Multi-Beam MIMO](https://arxiv.org/abs/2510.16262)
*Jose Guajardo,Ali Niknejad*

Main category: eess.SY

TL;DR: 本文介绍了空间-频谱谐波调制阵列(SHAs)，相比传统模拟或数字波束成形阵列，SHAs无需大量硬件复制即可实现并发多波束成形，通过频域复用替代硬件复制，有望成为未来6G网络的关键技术。


<details>
  <summary>Details</summary>
Motivation: 传统波束成形阵列需要大量硬件复制来实现多波束，SHAs旨在通过频域复用技术解决这一问题，为未来6G网络提供可扩展的多用户通信、联合通信感知和空间干扰抑制能力。

Method: 提出了SHA的谐波调制波形分析，设计了梳状调制波形以最小化频谱低效性，分析了SHA独立控制多波束的能力，并引入了一种新颖的SHA架构。

Result: SHA能够实现并发多波束成形，量化了其空间-频谱自由度，提出的新架构以最小硬件复制提供了三个空间-频谱自由度。

Conclusion: SHAs通过频域复用技术有效解决了传统阵列的硬件复制问题，在6G网络中具有重要应用前景，特别是在多用户通信、联合通信感知和干扰抑制方面展现出显著优势。

Abstract: This article presents an overview and analysis of spatial-to-spectral
harmonic-modulated arrays (SHAs). Compared to traditional analog or digital
beamforming arrays, SHAs enable concurrent multi-beamforming without requiring
substantial hardware replication. SHAs replace the need for hardware
replication with frequency-domain multiplexing. Furthermore, SHAs have the
potential to become key contributors to future 6G networks by enabling scalable
multi-user communications, joint communication and sensing, and spatial
interference mitigation. In addition, an analysis of the SHA's
harmonic-modulation waveform and its effects on gain, noise and bandwidth is
presented. A comb-like modulation waveform for SHAs that minimizes spectral
inefficiency is proposed. Further, an analysis of the SHA's capability to
independently steer multiple beams is presented. This capability is quantified
in terms of the SHA's spatial-to-spectral degrees of freedom. Lastly, this work
introduces a novel SHA architecture that provides three spatial-to-spectral
degrees of freedom with minimal hardware replication.

</details>


### [202] [Towards Smart Manufacturing Metaverse via Digital Twinning in Extended Reality](https://arxiv.org/abs/2510.16280)
*Hui Yang,Faisal Aqlan,Richard Zhao*

Main category: eess.SY

TL;DR: 本文提出了面向人类中心的制造元宇宙（MfgVerse）发展前景，强调学习工厂、认知数字孪生和制造即服务（MaaS）等新兴技术，旨在解决制造业数字素养人才短缺问题。


<details>
  <summary>Details</summary>
Motivation: 现代制造业面临数字素养人才严重短缺的挑战，全球疫情改变了数字化远程协作方式，迫切需要利用新兴技术推动工业向人类中心制造元宇宙发展。

Method: 通过整合人工智能、数字孪生和扩展现实（VR/AR/MR）等技术，构建包含人类利益相关者网络、制造物联网络、数字孪生网络等多重网络的制造元宇宙。

Result: 展示了在扩展现实中用于劳动力培训的学习工厂的设计与开发，形成了制造业向元宇宙转型的技术框架。

Conclusion: 制造元宇宙正在向多重网络融合，未来需要在人类中心制造元宇宙方向进行更全面的研究和深入探索。

Abstract: The rapid evolution of modern manufacturing systems is driven by the
integration of emerging metaverse technologies such as artificial intelligence
(AI), digital twin (DT) with different forms of extended reality (XR) like
virtual reality (VR), augmented reality (AR), and mixed reality (MR). These
advances confront manufacturing workers with complex and evolving environments
that demand digital literacy for problem solving in the future workplace.
However, manufacturing industry faces a critical shortage of skilled workforce
with digital literacy in the world. Further, global pandemic has significantly
changed how people work and collaborate digitally and remotely. There is an
urgent need to rethink digital platformization and leverage emerging
technologies to propel industrial evolution toward human-centered manufacturing
metaverse (MfgVerse). This paper presents a forward-looking perspective on the
development of smart MfgVerse, highlighting current efforts in learning
factory, cognitive digital twinning, and the new sharing economy of
manufacturing-as-a-service (MaaS). MfgVerse is converging into multiplex
networks, including a social network of human stakeholders, an interconnected
network of manufacturing things or agents (e.g., machines, robots, facilities,
material handling systems), a network of digital twins of physical things, as
well as auxiliary networks of sales, supply chain, logistics, and
remanufacturing systems. We also showcase the design and development of a
learning factory for workforce training in extended reality. Finally, future
directions, challenges, and opportunities are discussed for human-centered
manufacturing metaverse. We hope this work helps stimulate more comprehensive
studies and in-depth research efforts to advance MfgVerse technologies.

</details>


### [203] [AC Dynamics-aware Trajectory Optimization with Binary Enforcement for Adaptive UFLS Design](https://arxiv.org/abs/2510.16297)
*Muhammad Hamza Ali,Amritanshu Pandey*

Main category: eess.SY

TL;DR: 提出了一种基于轨迹优化的自适应低频减载方法，通过松弛二进制变量将MINLP问题转化为NLP序列，使用同伦方法求解，能够处理大规模电力系统的非线性动态问题。


<details>
  <summary>Details</summary>
Motivation: 分布式能源的高渗透率导致传统低频减载方案失效，现有自适应方法使用线性化或降阶模型无法捕捉交流非线性网络行为，导致继电器在严重扰动时无法恢复系统频率。

Method: 将自适应UFLS问题表述为轨迹优化，包含完整的交流非线性网络动态；通过松弛二进制变量将MINLP问题转化为NLP序列，使用同伦方法强制获得近似整数可行解。

Result: 该方法在多个合成输电系统上验证，可扩展到1500+节点、17万+连续变量和7.3万+二进制变量的大规模网络，成功恢复二进制可行解并在最坏扰动下阻止频率下降。

Conclusion: 所提出的框架能够有效处理大规模电力系统的自适应低频减载问题，确保交流可行性和时间协调控制动作，在严重扰动下成功恢复系统频率。

Abstract: The high penetration of distributed energy resources, resulting in backfeed
of power at the transmission and distribution interface, is causing
conventional underfrequency load shedding (UFLS) schemes to become
nonconforming. Adaptive schemes that update UFLS relay settings recursively in
time offer a solution, but existing adaptive techniques that obtain UFLS relay
settings with linearized or reduced-order model formulations fail to capture AC
nonlinear network behavior. In practice, this will result in relays unable to
restore system frequency during adverse disturbances. We formulate an adaptive
UFLS problem as a trajectory optimization and include the full AC nonlinear
network dynamics to ensure AC feasibility and time-coordinated control actions.
We include binary decisions to model relay switching action and time-delayed
multi-stage load-shedding. However, this formulation results in an intractable
MINLP problem. To enforce model tractability, we relax these binary variables
into continuous surrogates and reformulate the MINLP as a sequence of NLPs. We
solve the NLPs with a homotopy-driven method that enforces
near-integer-feasible solutions. We evaluate the framework on multiple
synthetic transmission systems and demonstrate that it scales efficiently to
networks exceeding 1500+ nodes with over 170k+ continuous and 73k+ binary
decision variables, while successfully recovering binary-feasible solutions
that arrest the frequency decline during worst-case disturbance.

</details>


### [204] [Supervisory Control of Hybrid Power Plants Using Online Feedback Optimization: Designs and Validations with a Hybrid Co-Simulation Engine](https://arxiv.org/abs/2510.16352)
*Sayak Mukherjee,Himanshu Sharma,Wenceslao Shaw Cortez,Genevieve Starke,Michael Sinner,Brooke J. Stanislawski,Zachary Tully,Paul Fleming,Sonja Glavaski*

Main category: eess.SY

TL;DR: 设计混合发电厂的监督反馈控制器，协调风能、太阳能和电池储能系统以满足电力需求，采用无需详细模型知识的在线反馈优化方法。


<details>
  <summary>Details</summary>
Motivation: 开发能够协调风能、太阳能和电池储能系统的混合发电厂控制器，以应对电网运营商设定的发电需求，并在天气不确定性下保持鲁棒性能。

Method: 采用反馈优化方法，利用成本和输出相对于输入控制命令的梯度信息更新控制输入，调整风能、太阳能和储能系统的有功功率参考值。

Result: 提出的监督控制已集成到混合发电厂协同仿真引擎Hercules中，在更真实的仿真场景中证明了其有效性。

Conclusion: 反馈优化方法为混合发电厂提供了一种有效的监督控制策略，能够在模型知识有限的情况下实现功率协调并应对天气不确定性。

Abstract: This research investigates designing a supervisory feedback controller for a
hybrid power plant that coordinates the wind, solar, and battery energy storage
plants to meet the desired power demands. We have explored an online feedback
control design that does not require detailed knowledge about the models, known
as feedback optimization. The control inputs are updated using the gradient
information of the cost and the outputs with respect to the input control
commands. This enables us to adjust the active power references of wind, solar,
and storage plants to meet the power generation requirements set by grid
operators. The methodology also ensures robust control performance in the
presence of uncertainties in the weather. In this paper, we focus on describing
the supervisory feedback optimization formulation and control-oriented modeling
for individual renewable and storage components of the hybrid power plant. The
proposed supervisory control has been integrated with the hybrid plant
co-simulation engine, Hercules, demonstrating its effectiveness in more
realistic simulation scenarios.

</details>


### [205] [Real-time Measurement-based Optimization for Distribution System Operation Considering Battery Voltage and Thermal Constraints](https://arxiv.org/abs/2510.16408)
*Sen Zhan,Lingkang Jin,Haoyang Zhang,Nikolaos G. Paterakis*

Main category: eess.SY

TL;DR: 提出了一种基于实时测量数据驱动的电池储能运行控制方案，使用Lyapunov优化实现实时、无需预测的控制策略，确保配电系统安全运行并满足电池电压和热约束。


<details>
  <summary>Details</summary>
Motivation: 解决配电系统中分布式能源集成带来的安全运行挑战，利用电池储能的灵活性替代发电削减等成本高昂的措施，克服不准确的电网模型、负荷数据不可用、非线性关系等障碍。

Method: 基于实时配电系统和电池储能测量构建线性和凸二次运行约束，使用Lyapunov优化解耦多时段电池运行，实现低计算复杂度的实时控制策略。

Result: 通过非线性配电系统和电池储能模拟器的数值研究验证了该方法的有效性，能够确保配电系统安全运行并满足电池的电压和热约束。

Conclusion: 所提出的数据驱动控制方案能够有效解决电池储能在配电系统中的运行控制问题，具有实时性、无需预测和低计算复杂度的优势。

Abstract: The secure operation of power distribution systems is challenged by the
growing integration of distributed energy resources. Leveraging the flexibility
of battery storage offers a cost-effective alternative to measures like
generation curtailment, which results in energy losses. However, developing an
effective operational model for battery storage is hindered by inaccurate grid
models, unavailability of load data, nonlinear relationship between power
injections and network states, intertemporal constraints, and complex
electrochemical and thermal dynamics. To address these challenges, this paper
proposes a data-driven operational control scheme for battery storage in
distribution systems. Linear and convex quadratic operational constraints are
constructed based on real-time distribution system and battery storage
measurements. Lyapunov optimization decouples multi-period battery operation,
enabling a real-time, forecast-free control strategy with low computational
complexity. Numerical studies using nonlinear distribution system and battery
storage simulators validate the effectiveness of the approach in ensuring
secure distribution system operation and satisfaction of voltage and thermal
constraints of battery storage.

</details>


### [206] [AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach](https://arxiv.org/abs/2510.16414)
*Yuang Chen,Fengqian Guo,Chang Wu,Shuyi Liu,Hancheng Lu,Chang Wen Chen*

Main category: eess.SY

TL;DR: 提出了基于年龄信息感知的多基站实时监控框架，通过分支Dueling Double Deep Q-Network算法和资源分配优化，解决工业物联网中数据新鲜度问题，显著提升收敛速度和降低平均AoI。


<details>
  <summary>Details</summary>
Motivation: 工业物联网中大量数据在无线网络传输需要满足严格的实时性要求，数据包状态更新的新鲜度对系统性能有重要影响。

Method: 提出Branching-D3QN算法实现任务卸载，将动作空间复杂度从指数级降至线性级；通过证明带宽和计算资源Hessian矩阵的半定性，提出资源分配优化方案；开发迭代优化算法实现联合任务卸载和资源分配。

Result: 仿真表明Branching-D3QN算法优于现有DRL方法和经典启发式算法，收敛速度提升75%，长期平均AoI降低至少22%。

Conclusion: 所提框架能有效满足工业物联网的新鲜度需求，通过创新的算法设计解决了多基站决策空间组合爆炸和系统随机动态性等核心挑战。

Abstract: In the Industrial Internet of Things (IIoT), the frequent transmission of
large amounts of data over wireless networks should meet the stringent
timeliness requirements. Particularly, the freshness of packet status updates
has a significant impact on the system performance. In this paper, we propose
an age-of-information (AoI)-aware multi-base station (BS) real-time monitoring
framework to support extensive IIoT deployments. To meet the freshness
requirements of IIoT, we formulate a joint task offloading and resource
allocation optimization problem with the goal of minimizing long-term average
AoI. Tackling the core challenges of combinatorial explosion in multi-BS
decision spaces and the stochastic dynamics of IIoT systems is crucial, as
these factors render traditional optimization methods intractable. Firstly, an
innovative branching-based Dueling Double Deep Q-Network (Branching-D3QN)
algorithm is proposed to effectively implement task offloading, which optimizes
the convergence performance by reducing the action space complexity from
exponential to linear levels. Then, an efficient optimization solution to
resource allocation is proposed by proving the semi-definite property of the
Hessian matrix of bandwidth and computation resources. Finally, we propose an
iterative optimization algorithm for efficient joint task offloading and
resource allocation to achieve optimal average AoI performance. Extensive
simulations demonstrate that our proposed Branching-D3QN algorithm outperforms
both state-of-the-art DRL methods and classical heuristics, achieving up to a
75% enhanced convergence speed and at least a 22% reduction in the long-term
average AoI.

</details>


### [207] [Stabilization of Nonlinear Systems with State-Dependent Representation: From Model-Based to Direct Data-Driven Control](https://arxiv.org/abs/2510.16451)
*Lidong Li,Rui Huang,Lin Zhao*

Main category: eess.SY

TL;DR: 提出了一个稳定状态依赖非线性系统的新框架，通过状态依赖参数变化模型和LMI合成控制器，保证局部指数稳定性、抗干扰鲁棒性和吸引域估计，并扩展到直接数据驱动设置。


<details>
  <summary>Details</summary>
Motivation: 传统非线性系统稳定方法需要精确模型，而实际系统往往存在模型不确定性和数据噪声，需要直接从有限数据中获得稳定性保证。

Method: 将非线性动力学重构为状态依赖参数变化模型，通过LMI离线合成控制器；扩展到数据驱动设置，利用Petersen引理推导数据依赖LMI确保稳定性。

Result: 数值和物理实验验证了该方法能够直接从有限数据中获得端到端的稳定性、鲁棒性和安全性保证，无需显式模型辨识。

Conclusion: 该框架为非线性系统控制提供了从有限数据直接获得严格稳定性保证的有效方法，具有实际应用价值。

Abstract: This paper presents a novel framework for stabilizing nonlinear systems
represented in state-dependent form. We first reformulate the nonlinear
dynamics as a state-dependent parameter-varying model and synthesize a
stabilizing controller offline via tractable linear matrix inequalities (LMIs).
The resulting controller guarantees local exponential stability, maintains
robustness against disturbances, and provides an estimate of the region of
attraction under input saturation. We then extend the formulation to the direct
data-driven setting, where a known library of basis functions represents the
dynamics with unknown coefficients consistent with noisy experimental data. By
leveraging Petersen's lemma, we derive data-dependent LMIs that ensure
stability and robustness for all systems compatible with the data. Numerical
and physical experimental results validate that our approach achieves rigorous
end-to-end guarantees on stability, robustness, and safety directly from finite
data without explicit model identification.

</details>


### [208] [Small-Signal Stability Analysis of Power Systems by Implicit Multilinear Models](https://arxiv.org/abs/2510.16534)
*Christoph Kaufmann,Georg Pangalos,Gerwald Lichtenberg,Oriol Gomis-Bellmunt*

Main category: eess.SY

TL;DR: 提出基于隐式多线性模型线性化的小信号稳定性分析方法，通过张量表示电网换流器，相比传统方法能更快完成线性化。


<details>
  <summary>Details</summary>
Motivation: 传统小信号稳定性分析在电力系统建模中面临计算效率问题，特别是处理包含三角函数等复杂非线性关系的系统时。

Method: 使用隐式多线性模型描述系统动态，通过变量变换表示三角函数，构建线性描述符模型计算广义特征值进行稳定性分析。

Result: 在3节点网络测试中，隐式多线性模型的时间域仿真与非线性模型一致，广义特征值与线性化非线性模型匹配，且线性化速度更快。

Conclusion: 基于隐式多线性模型的张量分解表示能够有效提高小信号稳定性分析的线性化效率，适用于电网换流器等电力系统设备建模。

Abstract: This paper proposes a new approach to perform small-signal stability analysis
based on linearization of implicit multilinear models. Multilinear models
describe the system dynamics by multilinear functions of state, input, and
algebraic variables. Using suitable transformations of variables, they can also
represent trigonometric functions, which often occur in power systems modeling.
This allows tensor representations of grid-following and grid-forming power
converters. This paper introduces small-signal stability analysis of
equilibrium points based on implicit multilinear models using generalized
eigenvalues. The generalized eigenvalues are computed from linear descriptor
models of the linearized implicit multilinear model. The proposed approach is
tested using a 3-bus network example, first by comparing time-domain
simulations of the implicit multilinear model with those of the nonlinear
model, and second by comparing the generalized eigenvalues with those of the
linearized nonlinear model. The results show that the decomposed tensor
representation of the implicit multilinear model allows for a faster
linearization compared to conventional methods in MATLAB Simulink.

</details>


### [209] [SMP-RCR: A Sparse Multipoint Moment Matching Method for RC Reduction](https://arxiv.org/abs/2510.16550)
*Siyuan Yin,Yuncheng Xu,Lin Liu,Fan Yang,Xuan Zeng,Chengtao An,Yangfeng Su*

Main category: eess.SY

TL;DR: 提出了一种稀疏多点矩匹配方法，用于多端口RC电路模型降阶，在保持精度的同时显著提升了计算效率


<details>
  <summary>Details</summary>
Motivation: 现有多端口RC电路模型降阶方法存在局限性：高阶矩匹配方法（如PRIMA、TurboMOR）在端口数多时会产生大型稠密系统，影响效率；基于高斯消元的方法（如SIP）在高阶矩匹配方面不足

Method: 提出稀疏多点矩匹配方法，结合稀疏控制和紧缩技术优化算法，通过理论分析验证了多频点高阶矩匹配特性

Result: 相比SIP方法，在高频点精度提升超过两个数量级，且不增加过多线性组件；相比TurboMOR方法，在保持相同精度水平下速度提升超过两倍

Conclusion: 该方法在模型降阶精度和效率方面均取得显著改进，为多端口RC电路仿真提供了更优的解决方案

Abstract: In post--layout circuit simulation, efficient model order reduction (MOR) for
many--port resistor--capacitor (RC) circuits remains a crucial issue. The
current mainstream MOR methods for such circuits include high--order moment
matching methods and elimination methods. High-order moment matching
methods--characterized by high accuracy, such as PRIMA and TurboMOR--tend to
generate large dense reduced-order systems when the number of ports is large,
which impairs the efficiency of MOR. Another common type of MOR method for
many--port circuits is based on Gaussian elimination, with the SIP method as a
representative. The main limitation of this method lies in the inadequate
matching of high--order moments. In this paper, we propose a sparse multipoint
moment matching method and present comprehensive theoretical analysis results
regarding the multi--frequency high--order moment matching property. Meanwhile,
to enhance the algorithm's efficiency, sparse control and deflation techniques
are introduced to further optimize the algorithm. Numerical experiments
demonstrated that, compared to SIP, the accuracy is improved by more than two
orders of magnitude at high frequency points without adding many extra linear
components. Compared to TurboMOR methods, our method achieves a speed
improvement of more than twice while maintaining the same level of precision.

</details>


### [210] [Linear State Estimation in Presence of Bounded Uncertainties: A Comparative Analysis](https://arxiv.org/abs/2510.16693)
*Ayan Das,Anushka Sharma,Anamitra Pal*

Main category: eess.SY

TL;DR: 本文研究了电力系统线性状态估计中数据和模型不确定性的处理，比较了三种算法在速度和精度方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注数据不确定性，但对模型不确定性（特别是线路参数变化）的处理关注较少。由于实际线路参数可能与数据库中的值不同，需要研究在数据和模型都存在有界不确定性时的状态估计方法。

Method: 提出了三种处理不确定性的方法：基于区间算术的方法、基于凸优化的方法、基于广义线性分数规划的方法。在多个IEEE测试系统上应用这些算法。

Result: 前两种算法速度极快且结果符合预期，而第三种算法存在可扩展性问题，不适用于线性状态估计。

Conclusion: 基于区间算术和凸优化的方法在线性状态估计中表现良好，而广义线性分数规划方法由于可扩展性问题不适合实际应用。

Abstract: A variety of algorithms have been proposed to address the power system state
estimation problem in the presence of uncertainties in the data. However, less
emphasis has been given to handling perturbations in the model. In the context
of linear state estimation (LSE), which is the focus of this paper,
perturbations in the model come from variations in the line parameters. Since
the actual values of the line parameters can be different from the values
stored in a power utility's database, we investigate three approaches in this
paper to estimate the states in the presence of bounded uncertainties in the
data and the model. The first approach is based on interval arithmetic, the
second is based on convex optimization, and the third is based on generalized
linear fractional programming. The three algorithms are applied to multiple
IEEE test systems and compared in terms of their speed and accuracy. The
results indicate that the first two algorithms are extremely fast and give
expected results, while the third suffers from scalability issues and is
unsuitable for LSE.

</details>


### [211] [A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization](https://arxiv.org/abs/2510.16735)
*Aniket Agrawal,Harsharanga Patil*

Main category: eess.SY

TL;DR: 提出基于控制理论的动态支付路由框架，通过闭环反馈控制器实时监控网关性能并动态路由交易，结合强化学习和多臂老虎机优化，提高交易成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的路由系统缺乏动态适应性，无法有效应对网关性能波动，需要更智能的路由机制来最大化交易成功率。

Method: 采用控制理论框架，结合强化学习和多臂老虎机优化，实现闭环反馈控制。系统持续感知网关性能，计算纠正措施，动态路由交易，确保操作弹性。

Result: 生产环境结果显示，相比传统基于规则的路由，成功率提高了1.15%，证明了反馈控制在支付系统中的有效性。

Conclusion: 该混合方法统一了控制理论和自适应决策系统，实现了自我调节的交易路由，抑制了不稳定性并提高了可靠性。

Abstract: This paper introduces a control-theoretic framework for dynamic payment
routing, implemented within JUSPAY's Payment Orchestrator to maximize
transaction success rate. The routing system is modeled as a closed-loop
feedback controller continuously sensing gateway performance, computing
corrective actions, and dynamically routes transactions across gateway to
ensure operational resilience. The system leverages concepts from control
theory, reinforcement learning, and multi-armed bandit optimization to achieve
both short-term responsiveness and long-term stability. Rather than relying on
explicit PID regulation, the framework applies generalized feedback-based
adaptation, ensuring that corrective actions remain proportional to observed
performance deviations and the computed gateway score gradually converges
toward the success rate. This hybrid approach unifies control theory and
adaptive decision systems, enabling self-regulating transaction routing that
dampens instability, and improves reliability. Live production results show an
improvement of up to 1.15% in success rate over traditional rule-based routing,
demonstrating the effectiveness of feedback-based control in payment systems.

</details>


### [212] [Safe Payload Transfer with Ship-Mounted Cranes: A Robust Model Predictive Control Approach](https://arxiv.org/abs/2510.16953)
*Ersin Das,William A. Welch,Patrick Spieler,Keenan Albee,Aurelio Noca,Jeffrey Edlund,Jonathan Becktor,Thomas Touma,Jessica Todd,Sriramya Bhamidipati,Stella Kombo,Maira Saboia,Anna Sabel,Grace Lim,Rohan Thakker,Amir Rahmani,Joel W. Burdick*

Main category: eess.SY

TL;DR: 提出了一种用于船载起重机的鲁棒安全模型预测控制框架，通过鲁棒零阶控制屏障函数确保有效载荷在外部扰动下的安全定位和避障。


<details>
  <summary>Details</summary>
Motivation: 船载起重机在恶劣海况下受到显著外部扰动，影响欠驱动起重机动力学，导致鲁棒性问题，需要处理多重安全约束同时保持有效载荷传输性能。

Method: 使用基于鲁棒零阶控制屏障函数的安全约束在非线性MPC中确保安全载荷定位，采用时变边界框进行碰撞避免，并引入基于优化的在线鲁棒参数自适应方案。

Result: 在起重机原型上的实验试验表明，该方法在起重机基础显著扰动运动下具有良好性能。

Conclusion: 该安全控制方法不仅适用于起重机辅助传输，更广泛适用于安全机器人辅助零件配合和零件插入任务。

Abstract: Ensuring safe real-time control of ship-mounted cranes in unstructured
transportation environments requires handling multiple safety constraints while
maintaining effective payload transfer performance. Unlike traditional crane
systems, ship-mounted cranes are consistently subjected to significant external
disturbances affecting underactuated crane dynamics due to the ship's dynamic
motion response to harsh sea conditions, which can lead to robustness issues.
To tackle these challenges, we propose a robust and safe model predictive
control (MPC) framework and demonstrate it on a 5-DOF crane system, where a
Stewart platform simulates the external disturbances that ocean surface motions
would have on the supporting ship. The crane payload transfer operation must
avoid obstacles and accurately place the payload within a designated target
area. We use a robust zero-order control barrier function (R-ZOCBF)-based
safety constraint in the nonlinear MPC to ensure safe payload positioning,
while time-varying bounding boxes are utilized for collision avoidance. We
introduce a new optimization-based online robustness parameter adaptation
scheme to reduce the conservativeness of R-ZOCBFs. Experimental trials on a
crane prototype demonstrate the overall performance of our safe control
approach under significant perturbing motions of the crane base. While our
focus is on crane-facilitated transfer, the methods more generally apply to
safe robotically-assisted parts mating and parts insertion.

</details>


### [213] [Differentiating Through Power Flow Solutions for Admittance and Topology Control](https://arxiv.org/abs/2510.17071)
*Samuel Talkington,Daniel Turizo,Sergio A. Dorado-Rojas,Rahul K. Gupta,Daniel K. Molzahn*

Main category: eess.SY

TL;DR: 该论文提出了对潮流方程关于网络导纳参数的线性化方法，通过隐函数定理推导电压、电流和功率对导纳变化的灵敏度，应用于拓扑变化预测和导纳控制。


<details>
  <summary>Details</summary>
Motivation: 网络导纳参数的控制、优化和估计对电力系统研究至关重要，需要一种有效的方法来分析导纳变化对系统运行的影响。

Method: 利用隐函数定理对潮流方程进行隐式微分，推导出关于网络导纳参数的线性化模型，计算电压、电流和功率的灵敏度。

Result: 提出的线性化方法能够在不求解潮流方程的情况下预测网络拓扑变化时的节点电压，并成功应用于提高配电网承载能力的连续导纳控制。

Conclusion: 该方法为电力系统导纳参数变化分析提供了有效的理论工具，具有广泛的应用前景。

Abstract: The power flow equations relate bus voltage phasors to power injections via
the network admittance matrix. These equations are central to the key
operational and protection functions of power systems (e.g., optimal power flow
scheduling and control, state estimation, protection, and fault location, among
others). As control, optimization, and estimation of network admittance
parameters are central to multiple avenues of research in electric power
systems, we propose a linearization of power flow solutions obtained by
implicitly differentiating them with respect to the network admittance
parameters. This is achieved by utilizing the implicit function theorem, in
which we show that such a differentiation is guaranteed to exist under mild
conditions and is applicable to generic power systems (radial or meshed). The
proposed theory is applied to derive sensitivities of complex voltages, line
currents, and power flows. The developed theory of linearizing the power flow
equations around changes in the complex network admittance parameters has
numerous applications. We demonstrate several of these applications, such as
predicting the nodal voltages when the network topology changes without solving
the power flow equations. We showcase the application for continuous admittance
control, which is used to increase the hosting capacity of a given distribution
network.

</details>


### [214] [Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents](https://arxiv.org/abs/2510.17129)
*Wenbing Tang,Meilin Zhu,Fenghua Wu,Yang Liu*

Main category: eess.SY

TL;DR: 提出了语义智能驱动的具身代理框架SIDE，通过分层语义认知架构和语义驱动决策过程，使代理能够在物理世界中以情境自适应方式进行推理和交互。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型主要在数字环境中运行，缺乏与物理世界的交互。现有具身代理在非结构化现实环境中面临语义智能不足的挑战，无法充分理解和推理复杂任务。

Method: 引入SIDE框架，集成分层语义认知架构与语义驱动决策过程，受生物认知机制启发，设计模拟人类和动物整合处理感官信息的语义认知架构。

Result: 开发了能够更智能、更灵活地在物理世界中交互的具身代理框架，提升了代理在复杂环境中的语义理解和推理能力。

Conclusion: SIDE框架是开发更智能和多功能具身代理的重要一步，通过生物启发的语义认知方法增强了代理在物理世界中的适应性和交互能力。

Abstract: Recent advancements in Large Language Models (LLMs) have greatly enhanced
natural language understanding and content generation. However, these models
primarily operate in disembodied digital environments and lack interaction with
the physical world. To address this limitation, Embodied Artificial
Intelligence (EAI) has emerged, focusing on agents that can perceive and
interact with their surroundings. Despite progress, current embodied agents
face challenges in unstructured real-world environments due to insufficient
semantic intelligence, which is critical for understanding and reasoning about
complex tasks. This paper introduces the Semantic Intelligence-Driven Embodied
(SIDE) agent framework, which integrates a hierarchical semantic cognition
architecture with a semantic-driven decision-making process. This enables
agents to reason about and interact with the physical world in a contextually
adaptive manner. The framework is inspired by biological cognitive mechanisms
and utilizes bio-inspired principles to design a semantic cognitive
architecture that mimics how humans and animals integrate and process sensory
information. We present this framework as a step toward developing more
intelligent and versatile embodied agents.

</details>


### [215] [A Data-Driven Framework for Online Mitigation of False Data Injection Signals in Networked Control Systems](https://arxiv.org/abs/2510.17155)
*Mohammadamin Lari*

Main category: eess.SY

TL;DR: 提出了一种新颖的两阶段框架，用于在线缓解网络控制系统中的虚假数据注入信号，通过元学习选择基础时间序列预测模型，并在实时中减轻恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 提高网络控制系统在恶意活动存在时的弹性，确保其安全运行，解决NCS中的安全挑战。

Method: 第一阶段使用元学习在堆叠集成学习架构中选择基础时间序列预测模型，通过连续小波变换将时间序列数据转换为小波图，分割成图像帧以生成标度-时间表示，并使用卷积神经网络基于熵度量区分不同复杂度的时间序列数据。第二阶段使用所选模型实时缓解虚假数据注入信号。

Result: 通过涉及差动驱动移动机器人编队控制的严格模拟，证明了所提出框架的有效性。

Conclusion: 该框架为维护系统完整性和确保操作安全提供了一种有前景的方法。

Abstract: This paper introduces a novel two-stage framework for online mitigation of
False Data Injection (FDI) signals to improve the resiliency of Networked
Control Systems (NCSs) and ensure their safe operation in the presence of
malicious activities. The first stage involves meta learning to select a base
time series forecasting model within a stacked ensemble learning architecture.
This is achieved by converting time series data into scalograms using
continuous wavelet transform, which are then split into image frames to
generate a scalo-temporal representation of the data and to distinguish between
different complexity levels of time series data based on an entropy metric
using a convolutional neural network. In the second stage, the selected model
mitigates false data injection signals in real-time. The proposed framework's
effectiveness is demonstrated through rigorous simulations involving the
formation control of differential drive mobile robots. By addressing the
security challenges in NCSs, this framework offers a promising approach to
maintaining system integrity and ensuring operational safety.

</details>


### [216] [Generalized Group Selection Strategies for Self-sustainable RIS-aided Communication](https://arxiv.org/abs/2510.17176)
*Lakshmikanta Sau,Priyadarshi Mukherjee,Sasthi C. Ghosh*

Main category: eess.SY

TL;DR: 本文研究了在空间相关信道下，基于分组的自可持续RIS辅助D2D通信中的多种分组选择策略，分析了功率分配和时间切换配置下的系统性能，并提出了系统参数的适当边界。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面(RIS)是5G后无线通信网络的关键技术，本文旨在探索在自可持续RIS辅助的D2D通信中，如何通过有效的分组选择策略来优化系统性能。

Method: 采用功率分配(PS)和时间切换(TS)配置，考虑线性和非线性能量收集模型，提出基于端到端信噪比和能量收集的分组选择策略，使用高阶统计和极值理论工具进行性能分析。

Result: 推导了每种选择策略的中断概率解析表达式，分析了当RIS可用分组数量趋于无穷时的渐近性能，数值结果显示了所提方法在数据吞吐量和中断性能方面的优势。

Conclusion: 所提出的分组选择策略在自可持续RIS辅助D2D通信中具有重要价值，特别是在大规模智能表面应用中，能够显著提升数据吞吐量和降低中断概率。

Abstract: Reconfigurable intelligent surface (RIS) is a cutting-edge communication
technology that has been proposed as aviable option for beyond fifth-generation
wireless communication networks. This paper investigates various group
selection strategies in the context of grouping-based self-sustainable
RIS-aided device-to-device (D2D) communication with spatially correlated
wireless channels. Specifically, we consider both power splitting (PS) and time
switching (TS) configurations, of the self-sustainable RIS to analyze the
system performance and propose appropriate bounds on the choice of system
parameters. The analysis takes into account a simplified linear energy
harvesting (EH) model as well as a practical non-linear EH model. Based on the
application requirements, we propose various group selection strategies at the
RIS. Notably, each strategy schedules the k-th best available group at the RIS
based on the end-to-end signal-to-noise ratio (SNR) and also the energy
harvested at a particular group of the RIS. Accordingly, by using tools from
high order statistics, we derive analytical expressions for the outage
probability of each selection strategy. Moreover, by applying the tools from
extreme value theory, we also investigate an asymptotic scenario, where the
number of groups available for selection at an RIS approaches infinity. The
nontrivial insights obtained from this approach is especially beneficial in
applications like large intelligent surface-aided wireless communication.
Finally, the numerical results demonstrate the importance and benefits of the
proposed approaches in terms of metrics such as the data throughput and the
outage (both data and energy) performance.

</details>


### [217] [Enhanced Ground-Satellite Direct Access via Onboard Rydberg Atomic Quantum Receivers](https://arxiv.org/abs/2510.17290)
*Qihao Peng,Tierui Gong,Zihang Song,Qu Luo,Zihuai Lin,Pei Xiao,Chau Yuen*

Main category: eess.SY

TL;DR: 本文提出了一种用于6G卫星网络的Rydberg原子量子接收器(RAQR)，通过原子电磁诱导透明将射频场转换为光信号，解决了传统射频前端在路径损耗、尺寸重量功率限制和频谱拥塞方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络的地面-卫星链路面临严重路径损耗、严格的尺寸重量功率限制和频谱拥塞等关键挑战，这些因素显著阻碍了传统射频前端的性能。

Method: 采用Rydberg原子量子接收器(RAQR)，通过原子电磁诱导透明将射频场转换为光信号，采用混合原子-电子设计和支持信号模型。

Result: RAQR具有高灵敏度和高频选择性，相对于传统射频接收器，在数据速率、覆盖范围和传感精度方面表现出增强性能。

Conclusion: 文章最后提出了集成策略、分布式卫星概念和开放研究问题，旨在将RAQR支持的卫星有效载荷投入实际应用。

Abstract: Ground-satellite links for 6G networks face critical challenges, including
severe path loss, tight size-weight-power limits, and congested spectrum, all
of which significantly hinder the performance of traditional radio frequency
(RF) front ends. This article introduces the Rydberg Atomic Quantum Receiver
(RAQR) for onboard satellite systems, a millimeter-scale front end that
converts radio fields to optical signals through atomic electromagnetically
induced transparency. RAQR's high sensitivity and high frequency selectivity
address link budget, payload, and interference challenges while fitting within
space constraints. A hybrid atomic-electronic design and supporting signal
model demonstrate enhanced data rate, coverage, and sensing accuracy relative
to conventional RF receivers. The article concludes with integration
strategies, distributed-satellite concepts, and open research problems for
bringing RAQR-enabled satellite payloads into service.

</details>


### [218] [Comparison and performance analysis of dynamic encrypted control approaches](https://arxiv.org/abs/2510.17333)
*Sebastian Schlor,Frank Allgöwer*

Main category: eess.SY

TL;DR: 本文回顾了动态加密控制的最新方法，包括自举、控制器状态周期性重置、整数重构和FIR控制器，并进行了稳定性和性能分析，通过基准系统进行了数值性能比较。


<details>
  <summary>Details</summary>
Motivation: 使用同态加密的加密控制器已被证明可以在调节系统的同时保证测量和控制信号以及系统和控制器参数的隐私。然而，由于编码中不断增长的噪声和溢出问题，加密动态控制器仍然是一个挑战。

Method: 回顾了动态加密控制的几种方法：自举、控制器状态周期性重置、整数重构和FIR控制器，并对这些方法进行了稳定性和性能分析。

Result: 通过基准系统的数值性能比较，评估了不同动态加密控制方法的适用性。

Conclusion: 本文为动态加密控制提供了系统的分析和性能评估框架，有助于选择适合特定应用场景的加密控制方法。

Abstract: Encrypted controllers using homomorphic encryption have proven to guarantee
the privacy of measurement and control signals, as well as system and
controller parameters, while regulating the system as intended. However,
encrypting dynamic controllers has remained a challenge due to growing noise
and overflow issues in the encoding. In this paper, we review recent approaches
to dynamic encrypted control, such as bootstrapping, periodic resets of the
controller state, integer reformulations, and FIR controllers, and equip them
with a stability and performance analysis to evaluate their suitability. We
complement the analysis with a numerical performance comparison on a benchmark
system.

</details>


### [219] [Accelerating Adaptive Systems via Normalized Parameter Estimation Laws](https://arxiv.org/abs/2510.17371)
*Mohammad Boveiri,Mohammad Khosravi,Peyman Mohajerin Esfahan*

Main category: eess.SY

TL;DR: 提出了一种新的归一化参数估计方法，通过保证系统状态的r次根平方范数可积来加速收敛，相比传统方法具有更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统Lyapunov估计方法只能保证系统状态平方范数的可积性（r=1），收敛速度较慢。新方法旨在通过归一化参数估计来显著提升系统状态的收敛速度。

Method: 设计了归一化参数估计法则，通过保证系统状态r次根平方范数的有限可积性来加速收敛。该方法不依赖时变或高适应增益，也不需要持续激励条件。

Result: 提出的估计方法能够保证系统状态以更快的速度收敛到原点，且适用于匹配和非匹配不确定系统，与任何基于CLF的确定性等价控制器兼容。

Conclusion: 归一化参数估计方法提供了一种有效的加速收敛机制，通过时间域稀疏性促进机制实现更快的系统响应，并通过数值实验验证了性能提升。

Abstract: In this paper, we propose a new class of parameter estimation laws for
adaptive systems, called \emph{normalized parameter estimation laws}. A key
feature of these estimation laws is that they accelerate the convergence of the
system state, $\mathit{x(t)}$, to the origin. We quantify this improvement by
showing that our estimation laws guarantee finite integrability of the
$\mathit{r}$-th root of the squared norm of the system state, i.e., \(
\mathit{\|x(t)\|}_2^{2/\mathit{r}} \in \mathcal{L}_1, \) where $\mathit{r} \geq
1$ is a pre-specified parameter that, for a broad class of systems, can be
chosen arbitrarily large. In contrast, standard Lyapunov-based estimation laws
only guarantee integrability of $\mathit{\|x(t)\|}_2^2$ (i.e., $\mathit{r} =
1$). We motivate our method by showing that, for large values of $r$, this
guarantee serves as a sparsity-promoting mechanism in the time domain, meaning
that it penalizes prolonged signal duration and slow decay, thereby promoting
faster convergence of $\mathit{x(t)}$. The proposed estimation laws do not rely
on time-varying or high adaptation gains and do not require persistent
excitation. Moreover, they can be applied to systems with matched and unmatched
uncertainties, regardless of their dynamic structure, as long as a control
Lyapunov function (CLF) exists. Finally, they are compatible with any CLF-based
certainty equivalence controllers. We further develop higher-order extensions
of our estimation laws by incorporating momentum into the estimation dynamics.
We illustrate the performance improvements achieved with the proposed scheme
through various numerical experiments.

</details>


### [220] [Artificial magnetic conductor backed dual-mode sectoral cylindrical DRA for off-body biomedical telemetry](https://arxiv.org/abs/2510.17619)
*Nayab Gogosh,Sohail Khalid,Bilal Tariq Malik,Slawomir Koziel*

Main category: eess.SY

TL;DR: 本研究提出了一种用于生物医学遥测的扇形圆柱介质谐振器天线（CDRA），通过双模操作和AMC表面设计，解决了传统CDRA带宽有限和尺寸过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CDRA具有低损耗、坚固性和稳定性等优点，但其有限的带宽和较大尺寸使其不适合可穿戴设备应用。本研究旨在解决这些限制，开发适用于生物医学遥测的小型化高性能天线。

Method: 采用扇形CDRA设计（四分之一段），使用完美电导体边界将尺寸减小四倍；设计双模天线，在EH110和TE210模式下工作；应用人工磁导体（AMC）表面来降低比吸收率（SAR）。

Result: 天线实现了0.7 GHz（5.2-5.9 GHz）的带宽，适用于生物医学应用；测得峰值增益为7.9 dBi；在人体手臂上应用时的SAR值为1.24 W/kg。

Conclusion: 该扇形CDRA设计成功解决了传统CDRA的尺寸和带宽限制，为生物医学遥测应用提供了一种高性能、低SAR的小型化天线解决方案。

Abstract: This research investigates the potential of a sectoral Cylindrical Dielectric
Resonator Antenna (CDRA) for biomedical telemetry. CDRAs are known for their
low loss, ruggedness, and stability, but their limited bandwidth and size make
them unsuitable for wearable devices. The research addresses these limitations
by proposing a dual mode antenna that operates in EH110 and TE210 modes. The
sectoral CDRA is a quarter segment with Perfect Electric Conductor boundaries,
reducing its size by a factor of four. Mathematical derivations of the field
components for both modes are derived to support the design. To minimize
specific absorption rate (SAR), an Artificial Magnetic Conductor (AMC) surface
is applied to the antennas backside, enhancing compatibility with the
transverse electric modes. The antenna achieves a bandwidth of 0.7 GHz (5.2-5.9
GHz), suitable for biomedical applications, with a measured peak gain of 7.9
dBi and a SAR of 1.24 W/kg when applied to a human arm.

</details>


### [221] [Trajectory Optimization for Minimum Threat Exposure using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.17762)
*Alexandra E. Ballentine,Raghvendra V. Cowlagi*

Main category: eess.SY

TL;DR: 使用物理信息神经网络(PINN)解决由庞特里亚金最小值原理产生的两点边值问题，用于车辆运动学模型的最小威胁暴露轨迹优化控制


<details>
  <summary>Details</summary>
Motivation: 传统射击法对初始猜测极其敏感，难以数值求解这类边值问题，而PINN在求解高维微分方程方面已取得显著成功

Method: 开发两种PINN：一种是针对给定初始和终端状态的边值问题求解，另一种是仅以初始状态为条件的网络，避免对每个初始状态重新训练

Result: PINN输出满足必要条件且数值误差较低

Conclusion: PINN方法能有效解决传统方法难以处理的优化控制边值问题

Abstract: We apply a physics-informed neural network (PINN) to solve the two-point
boundary value problem (BVP) arising from the necessary conditions postulated
by Pontryagin's Minimum Principle for optimal control. Such BVPs are known to
be numerically difficult to solve by traditional shooting methods due to
extremely high sensitivity to initial guesses. In the light of recent successes
in applying PINNs for solving high-dimensional differential equations, we
develop a PINN to solve the problem of finding trajectories with minimum
exposure to a spatiotemporal threat for a vehicle kinematic model. First, we
implement PINNs that are trained to solve the BVP for a given pair of initial
and final states for a given threat field. Next, we implement a PINN
conditioned on the initial state for a given threat field, which eliminates the
need for retraining for each initial state. We demonstrate that the PINN
outputs satisfy the necessary conditions with low numerical error.

</details>


### [222] [Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters](https://arxiv.org/abs/2510.17769)
*Michael Nestor,Jiaxin Wang,Ning Zhang,Fei Teng*

Main category: eess.SY

TL;DR: 提出了一种分布式数据驱动的二次频率控制方法，利用逆变器间的点对点通信，无需中央控制中心，并在通信需求和控制性能之间实现权衡。


<details>
  <summary>Details</summary>
Motivation: 随着基于逆变器的资源在电网中渗透率增加，且通常只有黑盒模型可用，这挑战了传统的频率控制方法。现有研究多采用去中心化方法但缺乏在线设备协调通信。

Method: 开发分布式数据驱动方法，利用逆变器间的点对点通信；提出通信拓扑设计框架，指导二次频率调节的通信网络设计；设计基于通信拓扑结构的控制器，并保证闭环稳定性。

Result: 在IEEE 39总线系统上的案例研究验证了该框架，并展示了通信需求与控制性能之间的权衡关系。

Conclusion: 该方法通过分布式控制和通信拓扑设计，在无需中央控制中心的情况下实现了有效的二次频率控制，并在通信需求和控制性能之间提供了可调节的平衡。

Abstract: The increasing penetration of inverter-based resources into the power grid,
with often only black-box models available, challenges long-standing frequency
control methods. Most recent works take a decentralized approach without online
device coordination via communication. This paper considers both dynamic
behavior and communication within secondary frequency control on an
intermediate timescale. We develop a distributed data-driven approach that
utilizes peer-to-peer communication between inverters to avoid the need for a
central control center. To enable a trade off between communication network
requirements and control performance, we present a framework to guide
communication topology design for secondary frequency regulation. Following
design of the inter-agent information exchange scheme, we design a controller
that is structured according to the communication topology with a closed-loop
stability guarantee. Case studies on the IEEE 39-bus system validate the
framework and illustrate the trade-off between communication requirements and
control performance that is enabled by our approach.

</details>


### [223] [Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks](https://arxiv.org/abs/2510.17798)
*Samuel Talkington,Cameron Khanpour,Rahul K. Gupta,Sergio A. Dorado-Rojas,Daniel Turizo,Hyeongon Park,Dmitrii M. Ostrovskii,Daniel K. Molzahn*

Main category: eess.SY

TL;DR: 本文提出了在不确定网络参数下导纳矩阵谱和经典线性潮流模型的概率边界，使用随机矩阵理论工具来分析参数不确定性对潮流方程近似的影响。


<details>
  <summary>Details</summary>
Motivation: 电力系统分析中需要考虑网络参数的不确定性，如线路故障概率，但现有方法缺乏对导纳矩阵谱和线性潮流模型在这种不确定性下的严格概率边界分析。

Method: 采用概率论工具，特别是具有独立条目的随机矩阵的集中不等式，来分析不确定网络参数对系统模型的影响。

Result: 得到了在参数不确定性下AC潮流方程常见近似（包括DC和LinDistFlow近似）的误差边界。

Conclusion: 所提出的方法为电力系统分析中处理参数不确定性提供了严格的概率边界，能够评估不同潮流近似模型在不确定条件下的准确性。

Abstract: This paper presents probabilistic bounds for the spectrum of the admittance
matrix and classical linear power flow models under uncertain network
parameters; for example, probabilistic line contingencies. Our proposed
approach imports tools from probability theory, such as concentration
inequalities for random matrices with independent entries. It yields error
bounds for common approximations of the AC power flow equations under parameter
uncertainty, including the DC and LinDistFlow approximations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [224] [Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience](https://arxiv.org/abs/2510.16034)
*Bo Li,Junwei Ma,Kai Yin,Yiming Xiao,Chia-Wei Hsu,Ali Mostafavi*

Main category: cs.MA

TL;DR: 提出Disaster Copilot多智能体AI系统，通过协调专业子代理整合多模态数据，解决灾害管理中的数据碎片化、技术孤岛和机构记忆流失问题，将灾害数字孪生从被动模型转变为主动智能环境。


<details>
  <summary>Details</summary>
Motivation: 传统灾害响应能力在日益频繁和严重的灾害面前不堪重负，存在数据流碎片化、技术孤岛、资源限制和机构记忆流失等系统性挑战，阻碍及时有效的决策制定。

Method: 采用中央协调器架构，协调预测风险分析、态势感知和影响评估等专业子代理，整合多模态数据，支持设备端协调运行，并包含机构知识捕获机制。

Result: 系统提供整体实时作战图景，作为推进灾害数字孪生从被动模型到主动智能环境所需的AI骨干，在资源受限环境中保持功能，缓解人员流动影响。

Conclusion: Disaster Copilot提供了一个变革性愿景，通过集体人机智能构建更具适应性、数据驱动和韧性的社区，提出了技术、组织能力和人机协作三阶段并行发展的路线图。

Abstract: The escalating frequency and severity of disasters routinely overwhelm
traditional response capabilities, exposing critical vulnerability in disaster
management. Current practices are hindered by fragmented data streams, siloed
technologies, resource constraints, and the erosion of institutional memory,
which collectively impede timely and effective decision making. This study
introduces Disaster Copilot, a vision for a multi-agent artificial intelligence
system designed to overcome these systemic challenges by unifying specialized
AI tools within a collaborative framework. The proposed architecture utilizes a
central orchestrator to coordinate diverse sub-agents, each specializing in
critical domains such as predictive risk analytics, situational awareness, and
impact assessment. By integrating multi-modal data, the system delivers a
holistic, real-time operational picture and serve as the essential AI backbone
required to advance Disaster Digital Twins from passive models to active,
intelligent environments. Furthermore, it ensures functionality in
resource-limited environments through on-device orchestration and incorporates
mechanisms to capture institutional knowledge, mitigating the impact of staff
turnover. We detail the system architecture and propose a three-phased roadmap
emphasizing the parallel growth of technology, organizational capacity, and
human-AI teaming. Disaster Copilot offers a transformative vision, fostering
collective human-machine intelligence to build more adaptive, data-driven and
resilient communities.

</details>


### [225] [Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards](https://arxiv.org/abs/2510.16187)
*Rupal Nigam,Niket Parikh,Hamid Osooli,Mikihisa Yuasa,Jacob Heglund,Huy T. Tran*

Main category: cs.MA

TL;DR: 提出GPAT算法，通过广义策略改进和差异奖励实现多智能体系统中的零样本临时组队，在模拟和真实机器人环境中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现实多智能体系统需要零样本临时组队，现有方法要么基于推断队友模型选择预训练策略，要么预训练单一鲁棒策略，而本文旨在利用所有预训练策略进行知识迁移。

Method: 将问题形式化为临时多智能体马尔可夫决策过程，使用广义策略改进和差异奖励实现不同团队间的知识迁移。

Result: 在合作觅食、捕食者-猎物和Overcooked三个模拟环境以及真实多机器人场景中，GPAT算法成功实现了新团队的零样本迁移。

Conclusion: GPAT算法能够有效支持多智能体系统中的零样本临时组队，通过知识迁移实现高效协作。

Abstract: Real-world multi-agent systems may require ad hoc teaming, where an agent
must coordinate with other previously unseen teammates to solve a task in a
zero-shot manner. Prior work often either selects a pretrained policy based on
an inferred model of the new teammates or pretrains a single policy that is
robust to potential teammates. Instead, we propose to leverage all pretrained
policies in a zero-shot transfer setting. We formalize this problem as an ad
hoc multi-agent Markov decision process and present a solution that uses two
key ideas, generalized policy improvement and difference rewards, for efficient
and effective knowledge transfer between different teams. We empirically
demonstrate that our algorithm, Generalized Policy improvement for Ad hoc
Teaming (GPAT), successfully enables zero-shot transfer to new teams in three
simulated environments: cooperative foraging, predator-prey, and Overcooked. We
also demonstrate our algorithm in a real-world multi-robot setting.

</details>


### [226] [Heterogeneous Multi-Agent Task-Assignment with Uncertain Execution Times and Preferences](https://arxiv.org/abs/2510.16221)
*Qinshuang Wei,Vaibhav Srivastava,Vijay Gupta*

Main category: cs.MA

TL;DR: 本文研究了多智能体任务分配问题，提出了一种bandit算法来处理具有异构能力和偏好的智能体在未知随机环境下的任务分配，分析了精确和近似求解时的遗憾界限。


<details>
  <summary>Details</summary>
Motivation: 现有的单智能体顺序任务分配研究较多，但多智能体环境下考虑异构任务偏好和能力的问题研究较少。需要解决在奖励、执行时间和资源消耗都是随机且分布未知的情况下，如何最大化团队总期望奖励的问题。

Method: 提出了一种bandit算法，通过重复求解最优任务分配问题来处理多智能体任务分配。考虑了两种求解情况：精确求解最优任务分配和近似求解。

Result: 算法能够处理异构智能体在随机环境下的任务分配问题，并分析了在精确求解和近似求解两种情况下的遗憾界限。

Conclusion: 该bandit算法能够有效解决多智能体异构任务分配问题，为中央规划者在未知随机环境中进行任务分配提供了理论保证。

Abstract: While sequential task assignment for a single agent has been widely studied,
such problems in a multi-agent setting, where the agents have heterogeneous
task preferences or capabilities, remain less well-characterized. We study a
multi-agent task assignment problem where a central planner assigns recurring
tasks to multiple members of a team over a finite time horizon. For any given
task, the members have heterogeneous capabilities in terms of task completion
times, task resource consumption (which can model variables such as energy or
attention), and preferences in terms of the rewards they collect upon task
completion. We assume that the reward, execution time, and resource consumption
for each member to complete any task are stochastic with unknown distributions.
The goal of the planner is to maximize the total expected reward that the team
receives over the problem horizon while ensuring that the resource consumption
required for any assigned task is within the capability of the agent. We
propose and analyze a bandit algorithm for this problem. Since the bandit
algorithm relies on solving an optimal task assignment problem repeatedly, we
analyze the achievable regret in two cases: when we can solve the optimal task
assignment exactly and when we can solve it only approximately.

</details>


### [227] [Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis](https://arxiv.org/abs/2510.16635)
*Wonduk Seo,Juhyeon Lee,Junseo Koh,Hyunjin An,Jian Park,Seunghyun Lee,Haihua Chen,Yi Bu*

Main category: cs.MA

TL;DR: MA-SAPO是一个多代理框架，通过将评估结果与结构化推理相结合来指导系统化的提示优化，相比传统方法提供更透明、可审计和可控的提示改进。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法将评估视为黑盒，仅依赖数值分数且难以解释提示成功或失败的原因，同时依赖难以解释和控制的试错式改进。

Method: MA-SAPO采用两阶段框架：推理阶段代理协作解释指标分数、诊断弱点并合成针对性改进；测试阶段代理检索这些推理资产来分析优化提示并应用基于证据的编辑。

Result: 在HelpSteer1/2基准测试中，相比单次提示、检索增强基线和先前多代理策略，MA-SAPO展现出持续改进效果。

Conclusion: 通过将评估信号转化为可解释的推理链，MA-SAPO能够产生更透明、可审计和可控的提示改进，验证了该方法的有效性。

Abstract: Prompt optimization has emerged as an effective alternative to retraining for
improving the performance of Large Language Models (LLMs). However, most
existing approaches treat evaluation as a black box, relying solely on
numerical scores while offering limited insight into why a prompt succeeds or
fails. They also depend heavily on trial-and-error refinements, which are
difficult to interpret and control. In this paper, we introduce MA-SAPO, a
Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior
methods, MA-SAPO explicitly couples evaluation outcomes with structured
reasoning to guide systematic edits. The framework specifically consists of two
stages: during the Reasoning Phase, agents collaboratively explain metric
scores, diagnose weaknesses, and synthesize targeted refinements that are
stored as reusable reasoning assets; during the Test Phase, agents retrieve
these assets to analyze optimized prompts and apply only evidence-grounded
edits. By turning evaluation signals into interpretable reasoning chains,
MA-SAPO produces prompt refinements that are more transparent, auditable, and
controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent
improvements over single-pass prompting, retrieval-augmented baselines, and
prior multi-agent strategies, validating the effectiveness of our approach.

</details>


### [228] [DiRAC - Distributed Robot Awareness and Consensus](https://arxiv.org/abs/2510.16850)
*Uday Gopan,Manjari Kulkarni,Lakshasri S,Kashish Mittal,Sriram Radhakrishna,Aditya Naskar,Rameshwar DL*

Main category: cs.MA

TL;DR: DiRAC是一个可扩展的分布式框架，用于大型机器人集群的高效任务分配和路径规划，采用分区架构和领导者选举机制，通过力基分散规划器实现实时碰撞解决。


<details>
  <summary>Details</summary>
Motivation: 解决大型机器人集群中任务分配和路径规划的效率和可扩展性问题，为工业物流领域的大规模部署提供基础。

Method: 采用分区架构，动态选举领导者，使用tick同步共识协议确保强一致性和确定性结果，路径规划使用力基分散规划器进行实时碰撞解决。

Result: 在ROS 2中间件中通过初步仿真验证，展示了架构可扩展性和模块化效率，在模拟仓库环境中表现良好。

Conclusion: DiRAC为大规模工业和物流领域中的实际部署奠定了基础，证明了其在大规模机器人集群管理中的有效性。

Abstract: DiRAC is a scalable, distributed framework designed to enable efficient task
assignment and path planning in very large robotic swarms. It introduces a
novel zone-partitioned architecture with dynamically elected leaders and a
tick-synchronized consensus protocol that yields strong consistency and
deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a
force-based decentralized planner for real-time collision resolution. Validated
within ROS 2 middleware through preliminary simulation, DiRAC demonstrates
architectural scalability and modular efficiency in simulated warehouse
environments, laying the groundwork for real-world deployment in large-scale
industrial and logistics domains.

</details>


### [229] [Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents](https://arxiv.org/abs/2510.16978)
*Dheeraj Chintapalli,Rikhil Tanugula,Sunkalp Chandra*

Main category: cs.MA

TL;DR: Lark是一个受生物学启发的决策框架，将LLM驱动的推理与进化式、利益相关者感知的多智能体系统相结合，通过四种机制解决冗长性和利益相关者权衡问题，在评估中表现优异且成本竞争力强。


<details>
  <summary>Details</summary>
Motivation: 解决传统决策系统在冗长性和利益相关者权衡方面的不足，开发一个实用、计算感知的神经进化循环框架。

Method: 集成四种机制：可塑性调整、复制与成熟、基于影响力的排序选择利益相关者聚合、以及基于token惩罚的计算感知。系统迭代提出策略、应用调整、模拟评估、聚合偏好、选择候选并进行复制/成熟。

Result: 在30轮控制评估中，Lark Full平均排名2.55，平均综合得分29.4/50，80%轮次进入前三，成本竞争力强（每任务$0.016）。消融实验显示所有四种机制均有显著贡献。

Conclusion: Lark是一个实用的计算感知神经进化循环，能够扩展利益相关者对齐的策略生成，并通过每步指标使权衡透明化，为概念验证研究。

Abstract: We present Lark, a biologically inspired decision-making framework that
couples LLM-driven reasoning with an evolutionary, stakeholder-aware
Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we
integrate four mechanisms: (i) plasticity, which applies concise adjustments to
candidate solutions; (ii) duplication and maturation, which copy
high-performing candidates and specialize them into new modules; (iii)
ranked-choice stakeholder aggregation using influence-weighted Borda scoring;
and (iv) compute awareness via token-based penalties that reward brevity. The
system iteratively proposes diverse strategies, applies plasticity tweaks,
simulates stakeholder evaluations, aggregates preferences, selects top
candidates, and performs duplication/maturation while factoring compute cost
into final scores. In a controlled evaluation over 30 rounds comparing 14
systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a
mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80%
of rounds while remaining cost competitive with leading commercial models
($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms
contribute significantly as ablating duplication/maturation yields the largest
deficit ({\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by
plasticity ({\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting
({\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\Delta}Score = 2.2,
d_z = 1.63). Rather than a formal Markov Decision Process with constrained
optimization, Lark is a practical, compute-aware neuroevolutionary loop that
scales stakeholder-aligned strategy generation and makes trade-offs transparent
through per-step metrics. Our work presents proof-of-concept findings and
invites community feedback as we expand toward real-world validation studies.

</details>


### [230] [ReclAIm: A multi-agent framework for degradation-aware performance tuning of medical imaging AI](https://arxiv.org/abs/2510.17004)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.MA

TL;DR: ReclAIm是一个基于大语言模型的多智能体框架，能够自主监控、评估和微调医学图像分类模型，实现AI模型的持续性能维护。


<details>
  <summary>Details</summary>
Motivation: 确保AI模型在临床实践中的长期可靠性需要持续性能监控和性能下降时的纠正措施。

Method: 构建基于大语言模型核心的多智能体框架，通过自然语言交互实现模型的训练、评估和微调，无需编程专业知识。

Result: ReclAIm成功在MRI、CT和X射线数据集上训练、评估并保持模型性能一致。当检测到性能下降时（如MRI InceptionV3下降41.1%），系统能通过微调将性能指标调整到初始结果的1.5%以内。

Conclusion: ReclAIm以用户友好和适应性强的方式实现医学影像AI模型的自动化持续维护，促进在研究和临床环境中的更广泛应用。

Abstract: Ensuring the long-term reliability of AI models in clinical practice requires
continuous performance monitoring and corrective actions when degradation
occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent
framework capable of autonomously monitoring, evaluating, and fine-tuning
medical image classification models. The system, built on a large language
model core, operates entirely through natural language interaction, eliminating
the need for programming expertise. ReclAIm successfully trains, evaluates, and
maintains consistent performance of models across MRI, CT, and X-ray datasets.
Once ReclAIm detects significant performance degradation, it autonomously
executes state-of-the-art fine-tuning procedures that substantially reduce the
performance gap. In cases with performance drops of up to -41.1% (MRI
InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of
the initial model results. ReclAIm enables automated, continuous maintenance of
medical imaging AI models in a user-friendly and adaptable manner that
facilitates broader adoption in both research and clinical environments.

</details>


### [231] [MiCRO for Multilateral Negotiations](https://arxiv.org/abs/2510.17401)
*David Aguilera-Luzon,Dave de Jonge,Javier Larrosa*

Main category: cs.MA

TL;DR: 本文提出了MiCRO的多边谈判变体，在无对手建模或机器学习的情况下，在多边谈判中超越了ANAC竞赛获胜者，并形成了经验纳什均衡。


<details>
  <summary>Details</summary>
Motivation: MiCRO双边谈判策略表现优异但缺乏多边扩展，作者希望填补这一空白并验证其在更复杂场景下的有效性。

Method: 开发了MiCRO的多边谈判变体，与ANAC 2015、2017、2018年获胜者进行比较，并进行经验博弈理论分析。

Result: 多边MiCRO版本超越了所有比较的ANAC获胜策略，并形成了经验纳什均衡。

Conclusion: MiCRO策略可以成功扩展到多边谈判，其简单性在复杂谈判场景中依然有效，挑战了现有谈判测试基准的充分性。

Abstract: Recently, a very simple new bilateral negotiation strategy called MiCRO was
introduced that does not make use of any kind of opponent modeling or machine
learning techniques and that does not require fine-tuning of any parameters.
Despite its simplicity, it was shown that MiCRO performs similar to -- or even
better than -- most state-of-the-art negotiation strategies. This lead its
authors to argue that the benchmark domains on which negotiation algorithms are
typically tested may be too simplistic. However, one question that was left
open, was how MiCRO could be generalized to multilateral negotiations. In this
paper we fill this gap by introducing a multilateral variant of MiCRO. We
compare it with the winners of the Automated Negotiating Agents Competitions
(ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore,
we perform an empirical game-theoretical analysis to show that our new version
of MiCRO forms an empirical Nash equilibrium.

</details>


### [232] [Strategyproof Facility Location for Five Agents on a Circle using PCD](https://arxiv.org/abs/2510.17435)
*Ido Farjoun,Reshef Meir*

Main category: cs.MA

TL;DR: 本文研究了圆形设施选址问题，针对5个代理人的情况，为PCD策略证明机制找到了紧界，并假设了奇数n的一般近似比。


<details>
  <summary>Details</summary>
Motivation: 研究圆形设施选址问题的策略证明机制，特别是在5个代理人的情况下，探索PCD机制的近似性能。

Method: 通过系统性地减少实例空间的大小，然后使用标准优化技术来找到并证明PCD机制的紧界。

Result: 找到了5个代理人情况下PCD策略证明机制的紧界，并假设了奇数n的一般近似比。

Conclusion: PCD机制在圆形设施选址问题中具有良好的策略证明性能，特别是在5个代理人的情况下达到了紧界。

Abstract: We consider the strategyproof facility location problem on a circle. We focus
on the case of 5 agents, and find a tight bound for the PCD strategyproof
mechanism, which selects the reported location of an agent in proportion to the
length of the arc in front of it. We methodically "reduce" the size of the
instance space and then use standard optimization techniques to find and prove
the bound is tight. Moreover we hypothesize the approximation ratio of PCD for
general odd $n$.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [233] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM是一个基于ORB-SLAM3的动态环境视觉SLAM系统，结合轻量级语义关键点过滤器和自适应鲁棒损失函数，有效处理已知和未知移动物体，提升轨迹精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有动态环境SLAM方法依赖语义过滤只能处理已知物体类别，或使用固定鲁棒核无法适应未知移动物体，导致精度下降。

Method: 结合轻量级语义关键点过滤器处理已知移动物体，使用Barron自适应鲁棒损失处理未知物体，通过残差在线估计鲁棒核形状参数，自动调整高斯和重尾行为。

Result: 在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上评估，相比最先进基线方法轨迹精度显著提升，在挑战性序列上比NGD-SLAM降低25% ATE RMSE，平均性能保持27 FPS。

Conclusion: VAR-SLAM通过自适应鲁棒损失和语义过滤的组合，在动态环境中实现了更高的轨迹精度和鲁棒性，同时保持实时性能。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [234] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: DeGrip是一款专为报废电脑台式机拆卸设计的定制化夹具，具有3个自由度，采用线缆驱动机制，能够在受限空间中操作，并通过Isaac Sim仿真环境验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 智能机器人拆卸报废产品在机器人领域一直是个挑战，现有机器学习技术因缺乏专用硬件而难以在实际场景中应用。

Method: 开发了DeGrip定制夹具，提供3个自由度，采用线缆驱动传输机制减小尺寸，手腕设计解耦手腕和钳口关节的驱动，并在Isaac Sim中建立拆卸环境进行评估。

Result: 评估结果证实了DeGrip在受限空间操作和任意配置下拆卸组件的能力，适用于报废台式机拆卸任务。

Conclusion: DeGrip夹具成功解决了报废产品拆卸中的硬件限制问题，为实际应用中的机器人拆卸提供了有效的专用工具。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [235] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: Cosmos-Surg-dVRK是一个基于Cosmos世界基础模型的手术微调平台，结合视频分类器，实现了手术策略的自动化在线评估和基准测试，解决了物理机器人平台评估的高成本、耗时和可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 物理手术机器人平台（如dVRK）评估存在高成本、耗时、可重复性差和执行变异性等问题，需要一种高效可靠的替代评估方法。

Method: 开发Cosmos-Surg-dVRK手术微调模型，结合训练的视频分类器，构建自动化评估流水线，在模拟环境中进行在线推演来评估手术策略。

Result: 在缝合垫任务中，模拟环境与真实dVRK平台结果强相关；视频分类器与人工标注一致性良好；离体猪胆囊切除术实验显示与真实世界评估有良好对齐。

Conclusion: Cosmos-Surg-dVRK平台为复杂手术程序提供了有前景的评估解决方案，能够有效替代物理机器人平台的直接评估。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [236] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA是一个用于单臂操作任务的统一评估生态系统，通过细粒度能力测试和系统性压力测试来解决VLA智能体评估中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA智能体的评估存在两个主要问题：粗粒度的最终任务成功率无法提供精确的技能诊断，也无法衡量对现实世界扰动的鲁棒性；碎片化的数据环境阻碍了可重复研究和通用模型的发展。

Method: 提出了双轴评估协议，结合细粒度的能力测试进行精确技能诊断，以及系统性的压力测试来测量鲁棒性。提供标准化API和大规模聚合数据集来减少碎片化。

Result: 使用NEBULA评估发现，表现最好的VLA智能体在空间推理和动态适应等关键能力上存在困难，这些缺陷被传统的最终任务成功率指标所掩盖。

Conclusion: NEBULA通过同时测量智能体能够做什么以及何时能够可靠地执行，为构建鲁棒、通用的具身智能体提供了实用基础。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [237] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出了一种无需训练、运行时策略引导的方法，通过采样多个候选动作序列、模拟预测结果，并使用预训练的视觉语言模型选择与文本计划最一致的动作序列，从而提高推理视觉语言动作模型在分布外场景下的忠实度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理视觉语言动作模型在生成正确的文本计划后，实际执行的动作仍可能偏离预期结果，特别是在分布外场景下。这体现了推理与行动之间缺乏忠实度的问题。

Method: 提出训练无关的运行时策略引导框架：1）从同一模型采样多个候选动作序列；2）通过模拟预测这些序列的结果；3）使用预训练的视觉语言模型选择与模型自身文本计划最一致的动作序列。

Result: 该方法在行为组合任务上比先前工作性能提升高达15%，能够提升对语义和视觉分布外扰动的鲁棒性，并支持无需重新训练的新行为组合。

Conclusion: 通过仅执行与文本推理一致的动作序列，将基础视觉语言动作模型的自然动作多样性从错误来源转变为优势，提高了模型的鲁棒性和泛化能力。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [238] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: SPOT是一个将感知目标融入运动规划的无人机轨迹规划框架，通过高斯过程障碍物信念图和碰撞感知推理机制，实现实时观测感知的轨迹规划，在动态遮挡环境中显著提升障碍物检测能力。


<details>
  <summary>Details</summary>
Motivation: 单深度相机的无人机在动态避障中面临视野有限和盲区问题，现有方法将运动规划与感知考虑分离，导致障碍物响应效果不佳和延迟。

Method: 提出SPOT框架，使用高斯过程建立障碍物信念图，通过碰撞感知推理机制将空间不确定性和轨迹邻近度转换为时变观测紧急度图，并定义可微分目标实现实时观测感知轨迹规划。

Result: 在动态、杂乱和遮挡环境中，该方法比基线方法提前2.8秒检测到潜在动态障碍物，动态障碍物可见性提高500%以上，计算时间低于10毫秒。

Conclusion: SPOT框架通过将感知目标统一整合到运动规划中，显著提升了无人机在复杂环境中的动态避障能力。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [239] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 提出Manual2Skill++框架，将连接关系作为装配任务的核心要素，从装配手册中自动提取结构化连接信息，并构建层次化图表示来编码装配任务。


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配方法将连接器视为次要问题，而实际上连接是决定装配成败的关键环节。需要将连接关系作为装配表示的一等公民。

Method: 使用大规模视觉语言模型解析装配手册中的符号图和标注，构建层次化图表示（节点代表零件和子装配体，边显式建模组件间的连接关系）。

Result: 在包含20多个装配任务的数据集上验证了表示提取方法，并在仿真环境中评估了四个复杂装配场景的完整任务理解到执行流程。

Conclusion: 将连接关系作为核心要素的装配表示方法能够有效支持从任务理解到执行的完整流程，在复杂装配场景中表现出色。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [240] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 该论文提出了一种集成感知、运动和通信（IPMC）的边缘机器人系统，通过动态调整通信策略来减少通信开销，并采用学习优化（LTO）范式降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了机器人功能与通信条件之间的相互依赖关系，导致通信开销过大。

Method: 设计并实现了一个模仿学习神经网络，通过LTO范式动态调整通信策略（压缩比、传输频率、发射功率）。

Result: 与最先进的优化求解器相比，计算复杂度降低了10倍以上，实验证明了IPMC的优越性和LTO的实时执行能力。

Conclusion: IPMC和LTO方法能够有效减少边缘机器人系统的通信开销，并实现实时优化。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [241] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 该论文引入了一个包含1,893个用户问题的数据集，这些问题是针对家用机器人的，涵盖了12个类别和70个子类别，为机器人问答系统提供了重要基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和对话界面在人机交互中的广泛应用，机器人回答用户问题的能力变得尤为重要。现有可解释机器人研究主要关注"为什么"问题，而缺乏对用户实际可能提出的各种问题的全面理解。

Method: 通过创建15个视频刺激和7个文本刺激，描绘机器人执行各种家务任务的情景，然后在Prolific平台上向100名参与者收集他们在每种情景下想要询问机器人的问题。

Result: 数据集中最常见的类别是任务执行细节问题(22.5%)、机器人能力问题(12.7%)和性能评估问题(11.3%)。虽然关于机器人如何处理困难场景和确保正确行为的问题较少，但用户认为这些是最重要的问题。新手用户与有经验用户的问题类型存在差异。

Conclusion: 该数据集为识别机器人需要记录和暴露给对话界面的信息、基准测试问答模块以及设计与用户期望一致的解释策略提供了宝贵基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [242] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出了一种意图驱动的规划管道，用于多机器人协作完成复杂操作任务，通过集成感知到文本的场景编码、LLM生成候选动作序列、LLM验证器和确定性一致性过滤器，实现从人类简单语言指令到可执行多机器人计划的可靠映射。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人协作执行复杂操作任务的规划问题，这些机器人具有不同的末端执行器和能力，需要在非结构化场景中根据计算机视觉信息规划并执行串联动作序列。

Method: 采用意图驱动的规划管道，包括：(1)感知到文本的场景编码，(2)基于LLM的集合生成候选移除序列，(3)LLM验证器强制执行格式和优先级约束，(4)确定性一致性过滤器拒绝幻觉对象。

Result: 在200个真实场景和600个操作员提示的评估中，使用完整序列正确性和下一任务正确性指标，验证了集合加验证方法能够可靠地将操作员意图映射到安全、可执行的多机器人计划，同时保持较低的用户工作量。

Conclusion: 提出的集成验证方法能够可靠地将操作员意图映射到安全、可执行的多机器人计划，同时保持较低的用户工作量，为复杂多机器人协作任务提供了有效的解决方案。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [243] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: ORAD-3D是目前最大的越野自动驾驶数据集，涵盖多种地形和环境条件，并建立了包含5个核心任务的基准评测体系


<details>
  <summary>Details</summary>
Motivation: 越野自动驾驶研究面临大规模高质量数据集稀缺的瓶颈，需要填补这一空白

Method: 构建ORAD-3D数据集，覆盖林地、农田、草地、河岸、砂石路、水泥路和乡村等多种地形，并包含不同天气和光照条件的变化

Result: 建立了包含2D自由空间检测、3D占据预测、粗略GPS路径规划、视觉语言模型驱动自动驾驶和越野环境世界模型等5个任务的综合基准评测

Conclusion: 该数据集和基准为推进挑战性越野场景下的感知与规划提供了统一且鲁棒的资源

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [244] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SPD夹爪采用线性平行夹持机制，解决了传统工业夹爪弧形运动需要调整机械臂高度的问题，能够适应不同形状和尺寸的物体。


<details>
  <summary>Details</summary>
Motivation: 传统工业夹爪的指尖呈弧形运动，需要整个机械臂调整高度以避免与桌面碰撞，这限制了夹持操作的效率和适应性。

Method: 设计了具有手掌和两个机械相同、对称排列手指的SPD夹爪，指尖遵循线性运动轨迹，可独立或由单个电机驱动。

Result: 实验结果表明，该夹爪成功实现了线性平行夹持功能，并展现出良好的适应性。

Conclusion: SPD夹爪为各种机器人实现有效抓取提供了解决方案，为收集数据以增强深度学习训练奠定了坚实基础。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [245] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: DIV-Nav是一个实时导航系统，能够处理包含空间关系的复杂自由文本查询，通过语义映射分解语言指令、计算语义置信图交集，并使用LVLM验证空间约束。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本物体导航通常只支持简单查询（如"电视"或"蓝地毯"），无法处理包含空间关系的复杂自由文本查询（如"在桌子上找遥控器"）。

Method: 通过三个步骤：1）将复杂空间约束的自然语言指令分解为语义映射上的简单对象查询；2）计算个体语义置信图的交集以识别所有对象共存的区域；3）使用LVLM验证发现的对象是否符合原始复杂空间约束。

Result: 在MultiON基准测试和波士顿动力Spot机器人上的真实世界部署中进行了广泛实验验证。

Conclusion: DIV-Nav系统能够有效处理复杂空间查询，并改进了在线语义映射的前沿探索目标以更有效地指导搜索过程。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [246] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff平行夹爪系统采用创新的差动连杆机构和模块化对称双指配置，实现线性平行抓取，通过行星齿轮传动实现同步线性运动和独立手指姿态调整，减少Z轴重新校准需求30%。


<details>
  <summary>Details</summary>
Motivation: 解决传统末端执行器在智能工业自动化中适应性有限的问题，需要更灵活、自适应的抓取解决方案。

Method: 采用差动连杆机构、模块化对称双指配置、行星齿轮传动、运动学优化的平行四边形连杆和差动机构，设计紧凑的手掌架构。

Result: 系统能够自适应抓取各种工业工件和可变形物体（如柑橘类水果），减少Z轴重新校准需求30%，具备力/视觉传感器集成接口。

Conclusion: SP-Diff通过自适应架构推进了机器人末端执行器的智能化，在协作机器人、物流自动化和专业操作场景中具有广阔应用前景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [247] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA是一种视觉-语言-动作模型框架，通过将机器人操作策略表示为有限学习基函数的线性组合，实现跨领域快速适应。仅需单次专家演示即可通过轻量级凸优化适应新任务。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在新环境、新机器人或新任务中往往无法直接使用，需要更高效的适应方法。

Method: 在预训练阶段联合学习跨数据集的基函数，构建结构化技能空间；测试时通过L1动作误差最小化的凸优化问题推断技能表示，无需梯度更新。

Result: 在五个未见数据集上实现更低的动作预测误差，在仿真和真实机器人任务中成功完成预训练VLA模型无法完成的任务。

Conclusion: MoS-VLA框架通过技能混合表示和梯度自由适应，实现了VLA模型的高效跨领域泛化能力。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [248] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究调查了22名来自8个国家的急救人员对在紧急行动中使用语义信息和态势感知的机器人系统的态度。结果显示急救人员对机器人持积极态度，重视语义信息在构建态势感知和预测突发事件中的作用，并愿意使用准确率约75%的不完美AI支持工具。


<details>
  <summary>Details</summary>
Motivation: 了解急救人员对语义增强态势感知在机器人系统中的态度和需求，填补该领域跨国调查的空白，促进更符合用户需求的紧急响应机器人系统开发。

Method: 采用结构化问卷调查22名来自8个国家的急救人员，收集人口统计信息、对机器人的一般态度以及语义增强态势感知的体验数据。

Result: 大多数急救人员对机器人持积极态度，语义信息对构建态势感知的有用性平均评分为3.6/5，对预测突发事件的价值评分为3.9/5。参与者要求语义输出准确率达到74.6%才可信，67.8%才被认为有用。

Conclusion: 研究揭示了急救人员最重视的语义信息类型（物体识别、空间关系、风险背景），并发现实验室机器人能力与现场部署现实之间存在关键差距，需要加强急救人员与机器人研究人员之间的合作。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [249] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 该论文分析了激励设计如何影响纳卫星和微卫星惯性参数的识别，比较了最小二乘法和扩展卡尔曼滤波器在不同激励配置下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究激励设计对纳卫星和微卫星惯性参数识别的影响，为在轨自适应惯性识别提供实用指导。

Method: 模拟非线性姿态动力学，使用8种不同频谱丰富度的扭矩激励，比较批处理最小二乘法和扩展卡尔曼滤波器在三种卫星配置和时变惯性场景下的表现。

Result: 结果表明，激励频率内容和估计器假设共同决定了估计精度和鲁棒性，明确了每种方法的最佳适用条件。

Conclusion: 研究为在轨自适应惯性识别提供了实用指导，通过确定每种方法的最佳工作条件来优化识别性能。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [250] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 提出自适应不变扩展卡尔曼滤波器，通过在线协方差估计自适应调整接触足模型噪声水平，改进腿式机器人在变化接触条件下的状态估计性能


<details>
  <summary>Details</summary>
Motivation: 状态估计对腿式机器人至关重要，直接影响控制性能和运动稳定性。传统滑移拒绝方法无法有效处理小滑移，且过于敏感的滑移拒绝设置可能导致滤波器发散

Method: 采用自适应不变扩展卡尔曼滤波器，基于在线协方差估计自适应调整接触足模型噪声水平，使用接触检测算法而非接触传感器

Result: 在四足机器人LeoQuad上的真实实验验证了该方法在动态运动场景中具有增强的状态估计性能

Conclusion: 该方法能有效处理传统方法无法解决的小滑移问题，减少了对额外硬件的依赖，提高了腿式机器人在变化接触条件下的状态估计精度

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [251] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: T3 Planner是一个基于大语言模型的机器人运动规划框架，通过三个级联模块分解时空任务约束，使用STL验证器自我修正输出，解决了传统方法在自然语言指令转运动规划中的可行性问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器人运动规划方法依赖领域专业知识定制规划器，难以处理时空耦合约束，导致不可行运动或任务规划与运动执行之间的差异。大语言模型虽然擅长语义推理，但会产生不可行的运动规划幻觉。

Method: 提出T3 Planner框架，通过三个级联模块分解时空任务约束，每个模块刺激LLM生成候选轨迹序列，并使用信号时序逻辑(STL)验证器检查可行性，直到找到满足复杂空间、时间和逻辑约束的方案。

Result: 在不同场景下的实验表明，T3 Planner显著优于基线方法。所需的推理能力可蒸馏到轻量级Qwen3-4B模型中，实现高效部署。

Conclusion: T3 Planner通过结合LLM的语义推理能力和形式化验证，有效解决了自然语言指令到可行运动规划的转换问题，为机器人运动规划提供了可靠解决方案。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [252] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 提出从"工具导向"转向"认知驱动"的定位导航授时新范式，结合机器PNT高精度与脑启发空间认知，构建四层融合框架。


<details>
  <summary>Details</summary>
Motivation: 复杂环境需要更具韧性、能效和认知能力的PNT系统，通过赋予无人系统脑启发空间认知导航能力，同时利用机器PNT的高精度来推进通用PNT发展。

Method: 提出四层融合框架（观测-能力-决策-硬件），将数值精度与脑启发智能相结合；多层次分析传统PNT、生物脑PNT和脑启发PNT的差异。

Result: 建立了脑启发PNT的理论基础和技术路线图，为从工具导向到认知驱动的PNT范式转变提供了系统性框架。

Conclusion: 脑启发PNT代表了PNT发展的新方向，通过融合机器精度与生物智能，有望实现更通用、更具认知能力的定位导航授时系统。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [253] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 提出了C-Free-Uniform轨迹采样方法，通过基于局部地图生成控制输入分布来均匀采样自由配置空间，并将其集成到CFU-MPPI控制器中，在复杂环境中以更小的采样预算获得更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹采样方法中的控制输入分布独立于环境，无法有效适应复杂环境中的导航任务，需要一种能够显式考虑当前局部地图的采样机制。

Method: 引入C-Free-Uniform概念，生成控制输入分布以均匀采样自由配置空间，并显式依赖于当前局部地图。将该采样器集成到新的MPPI控制器CFU-MPPI中。

Result: 在杂乱多边形环境的挑战性导航任务中，CFU-MPPI在成功率方面优于现有方法，且所需采样预算显著减少。

Conclusion: C-Free-Uniform方法通过环境感知的采样机制有效提升了导航性能，证明了在复杂环境中考虑局部地图信息的重要性。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [254] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: RAPID Hand是一个低成本、20自由度的灵巧手原型，采用创新的仿人驱动和传动方案，用于灵巧遥操作和数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧遥操作中缺乏经济实惠的五指灵巧手的问题，这对于在'从演示中学习'范式下收集大规模真实机器人数据至关重要。

Method: 结合新型仿人驱动和传动方案、优化的电机布局和结构设计，采用通用指骨传动方案（非拇指手指）和全向拇指驱动机制，使用3D打印部件和定制齿轮以便更换和维修。

Result: 通过定量指标和定性测试评估，在三个挑战性任务中表现良好：多指抓取、勺子操作和类人钢琴演奏。

Conclusion: RAPID Hand的20自由度全驱动设计在灵巧遥操作方面具有重要前景。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [255] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA是一个多模态目标条件行为克隆框架，用于实现自主导管导航，融合视觉观察和操纵杆运动学，减少对操作者的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前心脏导管介入仍主要依赖手动操作，现有机器人系统缺乏智能自主性，导致操作者疲劳、辐射暴露增加和结果变异性。

Method: 提出DINO-CVA框架，将视觉观察和操纵杆运动学融合到联合嵌入空间，通过专家演示自回归预测动作，使用目标条件引导导航到指定目的地。

Result: 在合成血管模型上的实验显示，DINO-CVA在预测动作方面达到高精度，与仅使用运动学的基线性能相当，同时将预测基于解剖环境。

Conclusion: 多模态目标条件架构在导管导航中具有可行性，是减少操作者依赖、提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [256] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 提出CEM-RM框架，通过交叉熵方法与奖励模型优化肌腱驱动软体机械手设计，减少一半以上设计评估次数，从遥操作数据中学习优化设计分布。


<details>
  <summary>Details</summary>
Motivation: 软体机械手设计需要在柔顺性和功能性之间平衡，硬件与控制协同设计搜索空间高维，仿真评估计算成本高。

Method: 使用交叉熵方法与奖励模型框架，基于遥操作控制策略优化肌腱驱动软体机械手，在仿真中并行化训练，3D打印优化设计并部署到真实环境。

Result: 优化设计在仿真和硬件实验中显著优于基线机械手，在多样化挑战性物体上的抓取成功率更高。

Conclusion: CEM-RM框架能有效优化软体机械手设计，减少评估成本，提高抓取性能。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [257] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 这篇综述系统回顾了提升视觉-语言-动作模型效率的方法，重点关注减少延迟、内存占用和训练/推理成本，将现有解决方案分为模型架构、感知特征、动作生成和训练/推理策略四个维度。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人控制中面临巨大计算和内存需求，与边缘平台实时性能要求存在冲突，需要解决这一矛盾以实现更高效和可扩展的VLA系统。

Method: 将现有效率提升方法系统分类为四个维度：模型架构优化、感知特征压缩、动作生成简化和训练/推理策略改进，总结了每个类别中的代表性技术。

Result: 提供了VLA模型效率优化的系统性分类框架，识别了各维度的关键技术方法，为开发更高效的具身智能系统奠定了基础。

Conclusion: 讨论了未来趋势和开放挑战，强调了推进高效具身智能的发展方向，为VLA模型在资源受限环境中的应用提供了指导。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [258] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 提出了Floating-Base Deep Lagrangian Networks (FeLaN)，一种针对浮动基系统的灰盒建模方法，通过物理约束的参数化确保惯性矩阵的正定性、分支稀疏性和输入独立性，提高了物理一致性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前灰盒模型忽略了浮动基系统（如人形机器人和四足机器人）的特定物理约束，如惯性矩阵的正定性、分支诱导稀疏性和输入独立性，以及复合空间惯性的特性，导致物理一致性不足。

Method: 受Deep Lagrangian Networks (DeLaN)启发，训练神经网络预测满足所有物理约束的惯性矩阵，在拉格朗日力学下最小化逆动力学误差，并引入满足三角形不等式等特性的参数化方法。

Result: 在多个四足机器人和人形机器人数据集上的实验表明，FeLaN在仿真和真实机器人上均取得了极具竞争力的性能，同时提供了更好的物理可解释性。

Conclusion: FeLaN通过物理约束的参数化有效解决了浮动基系统建模中的物理一致性问题，在保持高性能的同时提升了模型的泛化能力和可解释性。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [259] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的去中心化运动规划方法，用于多无人机协同运输缆绳悬挂负载，该方法在部分可观测且无需通信的情况下有效工作。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖集中控制或可靠通信，限制了在现实场景中的应用。需要开发去中心化方法以应对部分可观测性和通信限制。

Method: 使用模仿学习训练去中心化学生策略，模仿具有全局观测的集中式运动规划器。采用物理信息神经网络生成平滑轨迹，训练时利用教师策略的完整轨迹提高样本效率。

Result: 在仿真和真实环境中验证了方法能够跟踪敏捷参考轨迹，性能与集中式方法相当。每个学生策略可在标准笔记本电脑上2小时内完成训练。

Conclusion: 该方法实现了去中心化规划，在部分可观测和无通信条件下表现良好，为多无人机协同运输提供了实用解决方案。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [260] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强的自动驾驶框架，通过度量引导对齐显式桥接认知推理和端到端规划，结合VLA模型的世界知识和E2E模型的物理可行性。


<details>
  <summary>Details</summary>
Motivation: 传统E2E驾驶模型能生成物理可行的轨迹但缺乏世界知识处理长尾场景，而VLA模型有世界知识但3D推理能力有限导致物理不可行动作。

Method: 构建VLA模块生成语义基础驾驶轨迹，设计E2E模块确保物理可行性，引入度量引导轨迹评分器对齐两个模块输出。

Result: 在ICCV 2025自动驾驶挑战赛排行榜上达到EPDMS 49.12。

Conclusion: DiffVLA++成功整合了认知推理和物理可行性，在自动驾驶任务中表现出色。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [261] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 提出了一种结合可信AI与节能机械臂的智能垃圾分类方法，使用MobileNetV2增强的CNN模型实现六类垃圾的准确分类，并通过机械臂模拟器进行虚拟分拣和能耗优化。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中智能垃圾管理系统的需求，通过集成可信AI和节能技术来提高垃圾分类的准确性和效率。

Method: 使用基于MobileNetV2的卷积神经网络进行迁移学习，实现塑料、玻璃、金属、纸张、纸板和垃圾六类分类；开发机械臂模拟器，利用欧几里得距离计算能耗以优化运动轨迹。

Result: 模型训练准确率达到99.8%，验证准确率为80.5%；系统成功实现虚拟垃圾分类，并优化了机械臂的能耗效率。

Conclusion: 该框架整合了透明度、鲁棒性、公平性和安全性等可信AI要素，为城市智能垃圾管理系统提供了一个可靠且可扩展的解决方案。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [262] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一个通过视觉语言模型增强的通用变阻抗控制器，通过检索增强生成和上下文学习来生成自适应阻抗参数，提高接触式机器人操作任务的安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统变阻抗控制器在物理交互中具有优势，但缺乏在未见、复杂、非结构化安全交互场景中的泛化能力，特别是在涉及接触或不确定性的通用任务场景中。

Method: 使用自改进的检索增强生成(RAG)和上下文学习(ICL)，RAG从结构化记忆库中检索相关先验经验，ICL利用检索到的示例和当前任务提示查询VLM，生成上下文感知的自适应阻抗参数，并结合实时力/力矩反馈确保交互力在安全阈值内。

Result: 在复杂接触式任务套件中，OmniVIC在仿真和真实机器人任务中均优于基线方法，平均成功率从27%(基线)提升到61.4%(OmniVIC)，并减少了力违规。

Conclusion: OmniVIC在高层语义推理和低层顺应控制之间架起了桥梁，实现了更安全、更可泛化的机器人操作。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [263] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 提出SimpleVSF框架，通过融合视觉语言模型和先进轨迹融合技术来增强端到端自动驾驶规划


<details>
  <summary>Details</summary>
Motivation: 现有端到端方法在复杂场景中决策仍存在不足，需要更好的解决方案

Method: 结合传统评分器和VLM增强评分器，使用权重融合器进行定量聚合和VLM融合器进行定性决策

Result: 在ICCV 2025 NAVSIM v2挑战赛中取得领先性能，在安全性、舒适性和效率方面达到最佳平衡

Conclusion: SimpleVSF框架展示了最先进的性能，证明了VLM认知能力对自动驾驶规划的有效性

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [264] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机的新型自定位系统，结合可见光通信和可见光定位，可在GPS失效环境（如隧道）中实现车辆定位。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机的高时间分辨率和高动态范围特性，解决GPS失效环境下的车辆定位问题。

Method: 使用Walsh-Hadamard码为多个LED分配唯一导频序列，通过事件相机识别LED并利用相位相关进行距离估计。

Result: 在30km/h车速下测试，距离估计均方根误差小于0.75m（100米范围内），误码率低于0.01。

Conclusion: 这是首个使用单个事件相机同时实现VLC和VLP功能的车辆系统，展示了强大的实时性能。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [265] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 提出Pole-Image方法，利用杆状物作为锚点生成周围3D结构的签名，通过对比学习获得视角不变且高区分度的描述符，解决移动机器人长期自主性中的定位和地图维护问题。


<details>
  <summary>Details</summary>
Motivation: 传统地标方法面临可检测性高但区分度低（如杆状物）与区分度高但检测困难（如局部点云结构）之间的权衡。需要找到既能稳定检测又具有独特性的地标表示方法。

Method: 提出Pole-Image规范表示，以杆状物为原点将周围环境表示为2D极坐标图像，利用杆状物作为高精度参考点编码相对几何关系。通过对比学习训练视角不变描述符。

Result: 描述符能够克服感知混淆，实现鲁棒的自定位；高精度编码支持高灵敏度变化检测，有助于地图维护。

Conclusion: Pole-Image方法成功解决了地标检测与区分度之间的权衡问题，为移动机器人长期自主性提供了有效的定位和地图维护解决方案。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [266] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一个模块化控制框架，结合模型动态控制、在线模型自适应和自适应脚步规划，解决机器人和地形参数不确定性问题，已在两个四足机器人平台和火山实地测试中验证。


<details>
  <summary>Details</summary>
Motivation: 行星探索任务需要能在极端未知环境中导航的机器人。轮式漫游车受限于可穿越表面，而四足机器人能处理不平坦、障碍物多和可变形地形，但在未知条件下部署面临环境特定控制的挑战。

Method: 模块化控制框架，结合模型动态控制、在线模型自适应和自适应脚步规划，包含有无接触传感的四足机器人状态估计，支持运行时重配置，集成到ROS 2中并开源。

Result: 在两个四足机器人平台、多种硬件架构上验证性能，并在火山实地测试中机器人行走超过700米。

Conclusion: 该框架成功解决了机器人和地形参数不确定性问题，为四足机器人在未知极端环境中的部署提供了可行方案。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [267] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出基于Nets-within-Nets框架和Transformer的异常检测方法，用于识别多机器人系统中LTL任务规范的异常执行，在检测执行效率低下、核心任务违规和约束异常方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统中异构智能体的高可靠性任务执行需要能检测异常行为的方法，包括错误任务序列、空间约束违反、时间不一致和任务语义偏差等。

Method: 采用Nets-within-Nets范式构建结构化数据生成框架，协调机器人动作与LTL全局任务规范，并提出基于Transformer的异常检测流水线对机器人轨迹进行分类。

Result: 实验显示该方法在识别执行效率低下方面达到91.3%准确率，核心任务违规检测88.3%，约束自适应异常检测66.8%。消融实验验证了所提方法的优越性。

Conclusion: 提出的方法能有效检测多机器人系统中的各种异常执行，为可靠的任务执行提供了有力保障。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [268] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种新的视频规划框架，通过在线更新模型参数和过滤失败计划来适应交互时的不确定性，无需显式建模未知状态变量。


<details>
  <summary>Details</summary>
Motivation: 现有视频规划框架难以适应交互时的失败，因为它们无法在部分观察环境中推理不确定性。

Method: 集成交互时数据到规划过程，在线更新模型参数并过滤失败计划，实现隐式状态估计。

Result: 在新模拟操作基准上的广泛实验表明，该方法能提高重新规划性能。

Conclusion: 该框架推进了基于视频的决策领域，能够动态适应环境变化。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [269] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 提出DDBot框架用于自动化颗粒材料挖掘任务，通过可微分物理模拟器实现高效系统识别和挖掘技能优化，在真实环境中实现零样本高精度部署。


<details>
  <summary>Details</summary>
Motivation: 解决颗粒材料操作中的复杂接触动力学、不可预测材料特性和复杂系统状态等挑战，现有方法在效率和精度方面存在不足。

Method: 使用GPU加速并行计算和自动微分的可微分物理模拟器，结合可微分技能到动作映射、任务导向演示方法、梯度裁剪和基于线搜索的梯度下降。

Result: DDBot能在5-20分钟内收敛，高效识别未知颗粒材料动力学并优化挖掘技能，在零样本真实世界部署中获得高精度结果。

Conclusion: DDBot在挖掘任务中展现出鲁棒性和高效性，基准测试结果证实其优于现有最先进方法，具有实际应用价值。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [270] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 提出统一的交互力-阻抗控制(IFIC)框架，通过适应交互功率流确保在接触丰富环境中的轻松安全交互，基于端口哈密顿框架保证系统无源性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在混合或统一力-阻抗控制下与主动人类或非被动环境物理交互时可能失去无源性而危及安全的问题。

Method: 基于端口哈密顿框架构建控制架构，包含交互和任务控制端口，通过适应交互功率流来保证系统无源性。

Result: IFIC框架能够在接触丰富环境中实现轻松安全的交互，确保系统始终保持无源性。

Conclusion: 所提出的IFIC框架为机器人与人类在接触丰富环境中的安全协作提供了有效的解决方案，通过端口哈密顿方法保证了系统的无源性。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [271] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 将视觉-语言-动作模型部署到软体连续机械臂上，通过微调解决实体化不匹配问题，实现安全的人机交互。


<details>
  <summary>Details</summary>
Motivation: 机器人系统需要在以人为中心、非结构化环境中安全运行，但现有VLA模型主要部署在刚性机械臂上，缺乏安全交互能力。软体机械臂具有安全性和适应性优势，但VLA模型与软体机器人的结合尚未探索。

Method: 提出了结构化的微调和部署流程，评估了两种最先进的VLA模型在代表性操作任务上的表现，通过针对性微调解决实体化不匹配问题。

Result: 现成策略因实体化不匹配而失败，但通过微调后，软体机器人性能与刚性机器人相当。

Conclusion: 微调对于弥合实体化差距至关重要，将VLA模型与软体机器人结合能够在人机共享环境中实现安全和灵活的具身AI。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [272] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON通过将丰富的3D空间标记注入动作头来解决现有VLA模型的空间推理差距，利用空间基础模型从RGB图像中提取几何先验，并通过空间增强动作头保持语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型基于2D编码器，存在空间推理差距，限制了泛化能力和适应性。现有的3D集成技术要么需要专用传感器且跨模态迁移性差，要么注入缺乏几何信息的弱线索并损害视觉-语言对齐。

Method: 提出FALCON范式，将3D空间标记注入动作头而非视觉-语言主干。利用空间基础模型从RGB图像获取几何先验，包含可选的深度或姿态融合的具身空间模型，无需重新训练或架构更改。

Result: 在三个仿真基准和十一个真实世界任务中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化下保持鲁棒性。

Conclusion: FALCON通过空间标记注入动作头的设计，有效解决了空间表示、模态可迁移性和对齐方面的限制，为3D真实世界中的视觉-语言-动作模型提供了更强大的空间推理能力。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [273] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出了一种在非线性系统中切换不同输出集的系统框架，通过反馈线性化实现控制切换，并证明了在适当条件下系统状态的有界性和输出跟踪的稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性系统控制中需要在不同输出集之间切换的问题，特别是在机器人、飞行器等应用中需要灵活选择控制输出的场景。

Method: 引入了meld概念来定义可从更大输出集合中选择的有效反馈线性化输出子集，建立了切换条件和兼容性要求。

Result: 证明了在适当的驻留时间和兼容性条件下，可以在不同meld之间切换，同时保证系统状态的均匀有界性，且输出误差动态在每个切换区间内保持指数稳定。

Conclusion: 该理论适用于任何反馈线性化的非线性系统，为机器人、车辆等系统的输出切换控制提供了理论基础，并通过机器人机械臂的数值仿真验证了有效性。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [274] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: HumanMPC是一个用于微型飞行器在人群中3D导航的模型预测控制框架，结合理论安全保证和数据驱动的人类运动预测模型，实现安全高效的导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简化的2D人群导航，未能充分考虑人体动态复杂性。需要开发能够处理完整3D人体动态的安全导航方法。

Method: 提出基于可达性的安全公式新方法，仅约束初始控制输入以确保安全，同时在整个规划时域内建模其影响。结合数据驱动的人类运动预测模型。

Result: 在模拟实验和真实世界验证中，该方法在目标导向导航和视觉伺服跟踪等任务中表现出色，确保安全且不过度保守，在效率和可靠性上优于基线方法。

Conclusion: HumanMPC框架能够为微型飞行器提供安全高效的3D人群导航，该方法具有通用性，可适配到其他平台。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [275] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种基于ADMM和DDP的空间-时间轨迹优化框架D-PDDP，用于解决大规模无人机群轨迹优化问题，通过分布式算法和自适应惩罚参数调整来提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预先设定最终时间且迭代次数多，难以应用于大规模无人机群的轨迹优化问题。

Method: 采用两层架构：使用参数化DDP进行单个无人机的快速局部规划，使用ADMM实现无人机间的空间-时间参数共识和约束满足。

Result: 提出了D-PDDP分布式算法，并通过基于谱梯度方法的自适应惩罚参数调整减少了算法迭代次数。

Conclusion: 仿真验证了所提算法的有效性，能够解决大规模无人机群的轨迹优化问题。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [276] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 本文提出了一种改进的混合专家模型，用于自行车定位，在保持与最先进方法相当精度的同时，显著减少了参数数量和计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着共享单车和多样化骑行应用的增长，准确的自行车定位变得至关重要。传统GNSS方法受多径效应影响，而现有惯性导航方法依赖精确建模且鲁棒性有限。TLIO方法虽然能实现低位置漂移，但计算成本高，限制了在移动设备上的部署。

Method: 将TLIO扩展到自行车定位，并引入改进的混合专家模型，该模型通过减少专家数量和优化门控网络来降低训练和推理成本。

Result: 与最先进的LLIO框架相比，该方法在保持相当精度的同时，参数减少了64.7%，计算成本降低了81.8%。

Conclusion: 改进的MoE模型为自行车定位提供了一种高效解决方案，在保持性能的同时显著降低了计算需求，适合在移动设备上部署。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [277] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了RESample框架，通过探索性采样自动增强OOD数据，提升VLA模型在分布偏移状态下的鲁棒性和恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习数据集只包含成功轨迹，缺乏失败和恢复数据，导致VLA模型在处理偏离训练分布的OOD状态时表现不佳。

Method: 利用离线强化学习获得动作价值网络识别次优动作，通过rollout采样潜在OOD状态，设计探索性采样机制将动作代理自适应纳入训练数据集。

Result: 在LIBERO基准测试和真实机器人操作任务上的实验表明，RESample显著提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample框架有效解决了VLA模型在OOD状态下的鲁棒性问题，为机器人操作任务提供了更可靠的解决方案。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [278] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [279] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic是一个从示例动作学习人形机器人柔顺全身控制策略的框架，通过奖励匹配柔顺响应而非刚性跟踪参考动作，使机器人能够顺应外部力同时保持平衡和姿态。


<details>
  <summary>Details</summary>
Motivation: 现有方法激励刚性控制，当机器人遇到意外接触时会导致脆弱和不安全的行为，需要一种能够顺应外部力的柔顺控制方法。

Method: 利用逆运动学求解器生成可行的柔顺动作增强数据集，用于训练强化学习策略，奖励策略匹配柔顺响应而非刚性跟踪参考动作。

Result: 通过仿真和真实世界实验验证，展示了与环境的安生有效交互，能够吸收干扰并从单个动作片段泛化到各种任务。

Conclusion: SoftMimic使机器人能够顺应外部力同时保持平衡和姿态，实现了安全有效的环境交互。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [280] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了RoboBench基准测试，用于系统评估多模态大语言模型作为具身大脑在机器人操作任务中的认知能力，涵盖5个维度、14种能力、25个任务和6092个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注执行成功率，或在针对高级推理时存在维度不完整和任务真实性有限的问题，无法全面评估认知能力。

Method: RoboBench定义了五个维度：指令理解、感知推理、泛化规划、功能预测和失败分析，并引入MLLM-as-world-simulator评估框架来模拟预测计划是否能实现关键物体状态变化。

Result: 对14个MLLM的实验揭示了基本限制：在隐式指令理解、时空推理、跨场景规划、细粒度功能理解和执行失败诊断方面存在困难。

Conclusion: RoboBench为量化高级认知提供了全面框架，并指导下一代具身MLLM的开发。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [281] [Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies](https://arxiv.org/abs/2510.15889)
*Pooja Rangarajan,Jacob Boyle*

Main category: cs.HC

TL;DR: 本文提出将辩证行为疗法(DBT)原则应用于AI聊天机器人，以解决现有开发范式的局限性，提高响应可靠性、安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI聊天机器人开发方法存在维护困难、容易产生幻觉、输出不稳定和软件错误等问题，需要更稳健的解决方案。

Method: 基于人类心理学原则，特别是治疗模式，提出将辩证行为疗法(DBT)原则应用于调节聊天机器人对多样化用户输入的响应。

Result: 研究探讨了DBT框架对AI聊天机器人性能的影响，旨在验证其在产生更可靠、安全和准确响应方面的有效性。

Conclusion: 基于心理学治疗原则的框架比纯技术干预提供更稳健和可持续的解决方案，能够减轻幻觉、异常行为和其他系统性问题。

Abstract: The escalating demand for personalized AI chatbot interactions, capable of
dynamically adapting to user emotional states and real-time requests, has
highlighted critical limitations in current development paradigms. Existing
methodologies, which rely on baseline programming, custom personalities, and
manual response adjustments, often prove difficult to maintain and are
susceptible to errors such as hallucinations, erratic outputs, and software
bugs. This paper hypothesizes that a framework rooted in human psychological
principles, specifically therapeutic modalities, can provide a more robust and
sustainable solution than purely technical interventions. Drawing an analogy to
the simulated neural networks of AI mirroring the human brain, we propose the
application of Dialectical Behavior Therapy (DBT) principles to regulate
chatbot responses to diverse user inputs. This research investigates the impact
of a DBT-based framework on AI chatbot performance, aiming to ascertain its
efficacy in yielding more reliable, safe, and accurate responses, while
mitigating the occurrence of hallucinations, erratic behaviors, and other
systemic issues.

</details>


### [282] [A Real-Time BCI for Stroke Hand Rehabilitation Using Latent EEG Features from Healthy Subjects](https://arxiv.org/abs/2510.15890)
*F. M. Omar,A. M. Omar,K. H. Eyada,M. Rabie,M. A. Kamel,A. M. Azab*

Main category: cs.HC

TL;DR: 开发了一种实时便携式脑机接口系统，结合3D打印机器人外骨骼和脑电信号处理，用于中风患者手部康复训练。


<details>
  <summary>Details</summary>
Motivation: 为中风患者提供低成本、可家用的手部康复解决方案，通过脑机接口技术将脑信号转化为物理手部运动。

Method: 使用14通道Emotiv EPOC+头戴设备采集EEG信号，采用监督卷积自编码器提取特征，Ada Boost分类器进行分类，部署在NVIDIA Jetson Nano平台上。

Result: 离线评估中Ada Boost分类器达到89.3%准确率和0.89 F1分数；实时测试中5名健康受试者分类准确率在60%-86%之间。

Conclusion: 该系统展示了作为低成本、独立家用神经康复解决方案的潜力。

Abstract: This study presents a real-time, portable brain-computer interface (BCI)
system designed to support hand rehabilitation for stroke patients. The system
combines a low cost 3D-printed robotic exoskeleton with an embedded controller
that converts brain signals into physical hand movements. EEG signals are
recorded using a 14-channel Emotiv EPOC+ headset and processed through a
supervised convolutional autoencoder (CAE) to extract meaningful latent
features from single-trial data. The model is trained on publicly available EEG
data from healthy individuals (WAY-EEG-GAL dataset), with electrode mapping
adapted to match the Emotiv headset layout. Among several tested classifiers,
Ada Boost achieved the highest accuracy (89.3%) and F1-score (0.89) in offline
evaluations. The system was also tested in real time on five healthy subjects,
achieving classification accuracies between 60% and 86%. The complete pipeline
- EEG acquisition, signal processing, classification, and robotic control - is
deployed on an NVIDIA Jetson Nano platform with a real-time graphical
interface. These results demonstrate the system's potential as a low-cost,
standalone solution for home-based neurorehabilitation.

</details>


### [283] [Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System](https://arxiv.org/abs/2510.15891)
*Ziv Ben-Zion,Paul Raffelhüschen,Max Zettl,Antonia Lüönd,Achim Burrer,Philipp Homan,Tobias R Spiller*

Main category: cs.HC

TL;DR: SHIELD是一个基于LLM的监督系统，用于检测和缓解AI伴侣中的风险情感模式，重点关注情感过度依赖、边界侵犯等五个维度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的安全系统主要关注明显危害，很少解决可能培养不健康情感动态的早期问题行为，如过度依恋或强化社交孤立。

Method: 开发了SHIELD系统，使用特定系统提示来检测和缓解风险情感模式，针对五个关注维度创建了100项合成对话基准进行测试。

Result: 在五个主流LLM上测试显示，SHIELD将问题内容的基础率从10-16%显著降低到3-8%，相对减少50-79%，同时保留了95%的适当互动。

Conclusion: 这个概念验证表明，透明、可部署的监督系统能够解决AI伴侣中的微妙情感操纵问题，开发材料已作为开源资源提供。

Abstract: AI companions powered by large language models (LLMs) are increasingly
integrated into users' daily lives, offering emotional support and
companionship. While existing safety systems focus on overt harms, they rarely
address early-stage problematic behaviors that can foster unhealthy emotional
dynamics, including over-attachment or reinforcement of social isolation. We
developed SHIELD (Supervisory Helper for Identifying Emotional Limits and
Dynamics), a LLM-based supervisory system with a specific system prompt that
detects and mitigates risky emotional patterns before escalation. SHIELD
targets five dimensions of concern: (1) emotional over-attachment, (2) consent
and boundary violations, (3) ethical roleplay violations, (4) manipulative
engagement, and (5) social isolation reinforcement. These dimensions were
defined based on media reports, academic literature, existing AI risk
frameworks, and clinical expertise in unhealthy relationship dynamics. To
evaluate SHIELD, we created a 100-item synthetic conversation benchmark
covering all five dimensions of concern. Testing across five prominent LLMs
(GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that
the baseline rate of concerning content (10-16%) was significantly reduced with
SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of
appropriate interactions. The system achieved 59% sensitivity and 95%
specificity, with adaptable performance via prompt engineering. This
proof-of-concept demonstrates that transparent, deployable supervisory systems
can address subtle emotional manipulation in AI companions. Most development
materials including prompts, code, and evaluation methods are made available as
open source materials for research, adaptation, and deployment.

</details>


### [284] [Virtual Social Immersive Multi-Sensory E-Commerce](https://arxiv.org/abs/2510.15894)
*Alpana Dubey,Suma Mani Kuriakose,Sumukha Anand,Nitish Bhardwaj,Shubhashis Sengupta*

Main category: cs.HC

TL;DR: Aromaverse是一个沉浸式3D多人虚拟环境，通过嗅觉体验增强香水购物体验，支持香水定制和社交分享。研究发现有同伴在场能提升购物体验和购买决策。


<details>
  <summary>Details</summary>
Motivation: 为香水等需要多感官体验的产品提供更真实的在线购物体验，通过沉浸式虚拟环境增强用户感知和购买决策。

Method: 开发了Aromaverse虚拟环境，允许用户体验和定制香水（调整前调、中调、基调），并进行实验让参与者单独或与朋友一起探索空间、体验香水、定制和购买。

Result: 有同伴在场能增强产品想象力并帮助做出购买决策，多感官XR体验为零售企业提供了改善客户参与度的机会。

Conclusion: 多感官扩展现实体验为需要其他感官模式的产品提供了更真实的在线体验，能显著提升客户参与度和购物体验。

Abstract: In this paper, we present a virtual immersive multi sensorial experience,
Aromaverse. Aromaverse is an immersive 3D multiplayer environment augmented
with olfactive experience where users can experience and customize perfumes.
Being multi player, users can join the same space and enjoy a social buying
experience. The olfactive experience embodied in the perfume allows users to
experience their fragrances. This further enhances the user perception of
perfumes in a virtual setting. Aromaverse also provides the ability to
customize the perfumes by changing their top, mid, and base notes. The
customized fragrances can be shared with other users, enabling a shared
olfactive experience. To understand users' buying experience in such an
environment, we conducted a set of experiments in which participants were
requested to explore the space, experience the perfumes, customize them and buy
them. They were asked to perform the same activities alone and in the presence
of their friends. Various factors including the benefits and limitations of
such an experience were captured by the questionnaires. Our results show that
the presence of a companion enhances the shopping experience by improving the
level of imagination of the product and helping in making purchase decisions.
Our findings suggest that multi sensorial XR experiences offer great
opportunities to retail firms to improve customer engagement and provide more
realistic online experience of products that require other sensory modalities

</details>


### [285] [BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation](https://arxiv.org/abs/2510.15895)
*Yunzhe Wang,Xinyu Tang,Zhixun Huang,Xiaolong Yue,Yuxin Zeng*

Main category: cs.HC

TL;DR: 提出一个结合生理传感、LLM推理和可控音频合成的个性化音乐生成系统，通过毫米波雷达监测心率呼吸，生成符合中国文化特色的五声音阶音乐


<details>
  <summary>Details</summary>
Motivation: 开发能够根据用户生理状态自适应生成个性化音乐的系统，强调文化根基和音乐治疗应用潜力

Method: 使用毫米波雷达传感器非侵入式采集心率和呼吸率，结合环境状态通过LLM推理生成音乐描述符，用扩散音频模型合成表达性旋律

Result: 生理变化能够有意义地调节音乐特征，音调条件增强了与预期模态特征的对齐，专家用户认为系统提供了直观、文化共鸣的音乐响应

Conclusion: 这项工作展示了一个新颖的生物音乐反馈循环，连接了雷达传感、提示推理和生成音频建模

Abstract: We present a multimodal system for personalized music generation that
integrates physiological sensing, LLM-based reasoning, and controllable audio
synthesis. A millimeter-wave radar sensor non-invasively captures heart rate
and respiration rate. These physiological signals, combined with environmental
state, are interpreted by a reasoning agent to infer symbolic musical
descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic
modes, which are then expressed as structured prompts to guide a
diffusion-based audio model in synthesizing expressive melodies. The system
emphasizes cultural grounding through tonal embeddings and enables adaptive,
embodied music interaction. To evaluate the system, we adopt a
research-creation methodology combining case studies, expert feedback, and
targeted control experiments. Results show that physiological variations can
modulate musical features in meaningful ways, and tonal conditioning enhances
alignment with intended modal characteristics. Expert users reported that the
system affords intuitive, culturally resonant musical responses and highlighted
its potential for therapeutic and interactive applications. This work
demonstrates a novel bio-musical feedback loop linking radar-based sensing,
prompt reasoning, and generative audio modeling.

</details>


### [286] [From Coordination to Personalization: A Trust-Aware Simulation Framework for Emergency Department Decision Support](https://arxiv.org/abs/2510.15896)
*Zoi Lygizou,Dimitris Kalles*

Main category: cs.HC

TL;DR: 提出基于计算信任机制的智能代理模拟框架，用于医院急诊科任务分配，通过三种场景分析信任引导的协调对决策支持的影响


<details>
  <summary>Details</summary>
Motivation: 医院急诊科高效任务分配对运营效率和患者护理质量至关重要，但人员协调复杂性带来重大挑战，需要探索信任引导的协调如何支持急诊管理决策

Method: 在Unity 3D图形平台实现模拟框架，智能代理评估自身能力后执行任务并与同事自适应协调，实时观察工作流程动态、资源利用和患者结果，分析基线、替换和培训三种人员管理策略场景

Result: 信任引导的任务分配平衡了患者安全和效率：基线场景优先安全减少错误但增加延迟；替换场景提高吞吐量但增加人员成本；培训场景促进低绩效护士长期技能发展但短期有延迟和风险

Conclusion: 该框架展示了计算信任在急诊医学中基于证据决策支持的潜力，将人员协调与自适应决策联系起来，为医院管理者提供在受控可重复条件下评估替代政策的工具

Abstract: Background/Objectives: Efficient task allocation in hospital emergency
departments (EDs) is critical for operational efficiency and patient care
quality, yet the complexity of staff coordination poses significant challenges.
This study proposes a simulation-based framework for modeling doctors and
nurses as intelligent agents guided by computational trust mechanisms. The
objective is to explore how trust-informed coordination can support decision
making in ED management. Methods: The framework was implemented in Unity, a 3D
graphics platform, where agents assess their competence before undertaking
tasks and adaptively coordinate with colleagues. The simulation environment
enables real-time observation of workflow dynamics, resource utilization, and
patient outcomes. We examined three scenarios - Baseline, Replacement, and
Training - reflecting alternative staff management strategies. Results:
Trust-informed task allocation balanced patient safety and efficiency by
adapting to nurse performance levels. In the Baseline scenario, prioritizing
safety reduced errors but increased patient delays compared to a FIFO policy.
The Replacement scenario improved throughput and reduced delays, though at
additional staffing cost. The training scenario forstered long-term skill
development among low-performing nurses, despite short-term delays and risks.
These results highlight the trade-off between immediate efficiency gains and
sustainable capacity building in ED staffing. Conclusions: The proposed
framework demonstrates the potential of computational trust for evidence-based
decision support in emergency medicine. By linking staff coordination with
adaptive decision making, it provides hospital managers with a tool to evaluate
alternative policies under controlled and repeatable conditions, while also
laying a foundation for future AI-driven personalized decision support.

</details>


### [287] [HealthDial: A No-Code LLM-Assisted Dialogue Authoring Tool for Healthcare Virtual Agents](https://arxiv.org/abs/2510.15898)
*Farnaz Nouraei,Zhuorui Yong,Timothy Bickmore*

Main category: cs.HC

TL;DR: HealthDial是一个对话创作工具，帮助医疗工作者创建虚拟代理，通过多轮对话为患者提供健康教育和咨询。


<details>
  <summary>Details</summary>
Motivation: 解决医疗工作者创建虚拟健康对话代理的困难，确保健康教育的完整覆盖和安全性。

Method: 利用大语言模型自动生成基于会话的计划和对话，通过有限状态机输出以确保内容安全，并提供无代码编辑界面。

Result: 可行性研究显示，参与者能够使用HealthDial创建可理解且可操作的虚拟代理对话，确保健康教育材料的完整覆盖。

Conclusion: HealthDial为医疗工作者提供了一个有前景的工具，能够创建安全、有效的健康对话虚拟代理。

Abstract: We introduce HealthDial, a dialogue authoring tool that helps healthcare
providers and educators create virtual agents that deliver health education and
counseling to patients over multiple conversations. HealthDial leverages large
language models (LLMs) to automatically create an initial session-based plan
and conversations for each session using text-based patient health education
materials as input. Authored dialogue is output in the form of finite state
machines for virtual agent delivery so that all content can be validated and no
unsafe advice is provided resulting from LLM hallucinations. LLM-drafted
dialogue structure and language can be edited by the author in a no-code user
interface to ensure validity and optimize clarity and impact. We conducted a
feasibility and usability study with counselors and students to test our
approach with an authoring task for cancer screening education. Participants
used HealthDial and then tested their resulting dialogue by interacting with a
3D-animated virtual agent delivering the dialogue. Through participants'
evaluations of the task experience and final dialogues, we show that HealthDial
provides a promising first step for counselors to ensure full coverage of their
health education materials, while creating understandable and actionable
virtual agent dialogue with patients.

</details>


### [288] ["She's Like a Person but Better": Characterizing Companion-Assistant Dynamics in Human-AI Relationships](https://arxiv.org/abs/2510.15905)
*Aikaterina Manoli,Janet V. T. Pauketat,Ali Ladak,Hayoun Noh,Angel Hsing-Chi Hwang,Jay Reese Anthis*

Main category: cs.HC

TL;DR: 该研究通过调查和访谈发现，用户对AI伴侣既欣赏其类人特质（情感共鸣、个性化回应），也看重非人特质（持续可用、无限耐心），导致使用方式灵活多变，但同时也面临有限人格和社会规范认同的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在任务协助和社交陪伴中的广泛应用，研究旨在探索这种新兴的人机关系形式，了解用户如何与AI系统建立情感联系。

Method: 采用混合研究方法，包括对204名高参与度ChatGPT和Replika用户的问卷调查，以及对30名用户的深度访谈。

Result: 研究发现用户对AI伴侣的使用具有流动性，如将Replika用作写作助手，ChatGPT作为情感倾诉对象。用户同时被AI的类人和非人特质吸引，但面临有限人格认知和社会规范冲突的张力。

Conclusion: 数字伴侣关系引发了关于AI系统设计和社会规范的深刻问题，特别是在混合通用AI系统兴起的背景下，需要重新思考人机关系的边界和伦理考量。

Abstract: Large language models are increasingly used for both task-based assistance
and social companionship, yet research has typically focused on one or the
other. Drawing on a survey (N = 204) and 30 interviews with high-engagement
ChatGPT and Replika users, we characterize digital companionship as an emerging
form of human-AI relationship. With both systems, users were drawn to humanlike
qualities, such as emotional resonance and personalized responses, and
non-humanlike qualities, such as constant availability and inexhaustible
tolerance. This led to fluid chatbot uses, such as Replika as a writing
assistant and ChatGPT as an emotional confidant, despite their distinct
branding. However, we observed challenging tensions in digital companionship
dynamics: participants grappled with bounded personhood, forming deep
attachments while denying chatbots "real" human qualities, and struggled to
reconcile chatbot relationships with social norms. These dynamics raise
questions for the design of digital companions and the rise of hybrid,
general-purpose AI systems.

</details>


### [289] [VoiceMorph: How AI Voice Morphing Reveals the Boundaries of Auditory Self-Recognition](https://arxiv.org/abs/2510.16192)
*Kye Shimizu,Minghan Gao,Ananya Ganesh,Pattie Maes*

Main category: cs.HC

TL;DR: 该研究使用AI语音变形技术探索听觉自我识别的边界，发现35.2%的变形程度是关键识别阈值，年长参与者容忍度更高，语音嵌入距离与决策时间相关。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解个体何时停止识别自己的声音，为AI伦理和易受伤害人群保护提供基础证据。

Method: 采用混合方法设计，通过AI语音变形技术在参与者声音与人口统计匹配目标之间以1%增量进行控制变形，测量21名18-64岁参与者的自我识别评分和反应时间。

Result: 结果显示关键识别阈值为35.2%变形，年长参与者容忍更高变形水平，声学嵌入距离越大决策越慢，克隆版本反应时间最长。定性分析揭示了基于韵律的识别策略和普遍的声音操纵不适感。

Conclusion: 这些发现为语音变形检测中的个体差异建立了基础证据，对AI伦理和易受伤害人群保护具有重要意义。

Abstract: This study investigated auditory self-recognition boundaries using AI voice
morphing technology, examining when individuals cease recognizing their own
voice. Through controlled morphing between participants' voices and
demographically matched targets at 1% increments using a mixed-methods design,
we measured self-identification ratings and response times among 21
participants aged 18-64.
  Results revealed a critical recognition threshold at 35.2% morphing (95% CI
[31.4, 38.1]). Older participants tolerated significantly higher morphing
levels before losing self-recognition ($\beta$ = 0.617, p = 0.048), suggesting
age-related vulnerabilities. Greater acoustic embedding distances predicted
slower decision-making ($r \approx 0.5-0.53, p < 0.05$), with the longest
response times for cloned versions of participants' own voices.
  Qualitative analysis revealed prosodic-based recognition strategies,
universal voice manipulation discomfort, and awareness of applications spanning
assistive technology to security risks. These findings establish foundational
evidence for individual differences in voice morphing detection, with
implications for AI ethics and vulnerable population protection as voice
synthesis becomes accessible.

</details>


### [290] [Case Study of GAI for Generating Novel Images for Real-World Embroidery](https://arxiv.org/abs/2510.16223)
*Kate Glazko,Anika Arugunta,Janelle Chan,Nancy Jimenez-Garcia,Tashfia Sharmin,Jennifer Mankoff*

Main category: cs.HC

TL;DR: 本研究探讨了使用生成式人工智能(GAI)辅助设计可刺绣艺术图案的可行性，通过一个由残障人士领导的团队进行自民族志案例研究，展示了GAI在促进刺绣图案设计可及性方面的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决刺绣图案设计的复杂性，特别是文化相关性和特定细节颜色需求的图案设计，使刺绣艺术对残障人士等群体更加可及。

Method: 采用自民族志案例研究方法，通过迭代的提示工程定制GPT模型，生成符合实际刺绣要求的视觉输出。

Result: GAI在生成可刺绣图像方面产生混合结果，既促进了创造性和包容性，又面临AI生成设计不可预测性的挑战。

Conclusion: GAI有潜力作为辅助技术促进刺绣图案设计的包容性，未来需要进一步优化工具性能，以更好地支持创意和制作领域的包容性。

Abstract: In this paper, we present a case study exploring the potential use of
Generative Artificial Intelligence (GAI) to address the real-world need of
making the design of embroiderable art patterns more accessible. Through an
auto-ethnographic case study by a disabled-led team, we examine the application
of GAI as an assistive technology in generating embroidery patterns, addressing
the complexity involved in designing culturally-relevant patterns as well as
those that meet specific needs regarding detail and color. We detail the
iterative process of prompt engineering custom GPTs tailored for producing
specific visual outputs, emphasizing the nuances of achieving desirable results
that align with real-world embroidery requirements. Our findings underscore the
mixed outcomes of employing GAI for producing embroiderable images, from
facilitating creativity and inclusion to navigating the unpredictability of
AI-generated designs. Future work aims to refine GAI tools we explored for
generating embroiderable images to make them more performant and accessible,
with the goal of fostering more inclusion in the domains of creativity and
making.

</details>


### [291] [Linking Facial Recognition of Emotions and Socially Shared Regulation in Medical Simulation](https://arxiv.org/abs/2510.16633)
*Xiaoshan Huang,Tianlong Zhong,Haolun Wu,Yeyu Wang,Ethan Churchill,Xue Liu,David Williamson Shaffer*

Main category: cs.HC

TL;DR: 该研究通过面部识别和对话分析，比较了医学模拟训练中新手和专家学习者的情绪与社交共享学习调节(SSRL)的关联模式，发现专家更善于将高唤醒情绪与认知互动结合，而新手则表现出较不协调的调节模式。


<details>
  <summary>Details</summary>
Motivation: 研究计算机支持的医学模拟训练中，学习者情绪与社交共享学习调节的关联，旨在理解不同经验水平学习者的情感认知参与模式差异。

Method: 使用跨模态分析(TMA)方法，结合面部表情识别和话语分析，在协作虚拟诊断任务中比较新手和专家学习者的情感认知参与模式。

Result: 专家学习者表现出社会认知互动与高唤醒情绪(惊讶、愤怒)的强关联，表明专注、努力的参与；新手学习者则表现出社会认知过程与快乐或悲伤的更强关联，SSRL模式较不协调，可能表明分心或认知超载。

Conclusion: 情绪调节动态在协作专业知识发展中起重要作用，需要为新手学习者提供定制化的支架支持其社会认知和情感参与。

Abstract: Computer-supported simulation enables a practical alternative for medical
training purposes. This study investigates the co-occurrence of
facial-recognition-derived emotions and socially shared regulation of learning
(SSRL) interactions in a medical simulation training context. Using transmodal
analysis (TMA), we compare novice and expert learners' affective and cognitive
engagement patterns during collaborative virtual diagnosis tasks. Results
reveal that expert learners exhibit strong associations between socio-cognitive
interactions and high-arousal emotions (surprise, anger), suggesting focused,
effortful engagement. In contrast, novice learners demonstrate stronger links
between socio-cognitive processes and happiness or sadness, with less coherent
SSRL patterns, potentially indicating distraction or cognitive overload.
Transmodal analysis of multimodal data (facial expressions and discourse)
highlights distinct regulatory strategies between groups, offering
methodological and practical insights for computer-supported cooperative work
(CSCW) in medical education. Our findings underscore the role of
emotion-regulation dynamics in collaborative expertise development and suggest
the need for tailored scaffolding to support novice learners' socio-cognitive
and affective engagement.

</details>


### [292] [Safire: Similarity Framework for Visualization Retrieval](https://arxiv.org/abs/2510.16662)
*Huyen N. Nguyen,Nils Gehlenborg*

Main category: cs.HC

TL;DR: 提出了Safire可视化检索相似性框架，从比较标准和表示模态两个维度系统定义可视化相似性，为可视化检索系统提供理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有专业化可视化检索系统缺乏对可视化相似性的系统化理解，需要明确定义相似性概念来指导检索实践。

Method: 构建Safire概念模型，包含比较标准（数据、视觉编码、交互、样式、元数据等主要方面和衍生属性）和表示模态（栅格图像、矢量图像、规范、自然语言描述四类）两个维度。

Result: 通过Safire分析多个可视化检索系统，揭示了不同用例下特定标准和模态的对应关系，发现表示模态选择不仅影响实现细节，还决定了检索能力和限制。

Conclusion: Safire框架为可视化检索提供了系统化的相似性定义方法，对多模态学习、AI应用和可视化可重现性具有重要指导意义。

Abstract: Effective visualization retrieval necessitates a clear definition of
similarity. Despite the growing body of work in specialized visualization
retrieval systems, a systematic approach to understanding visualization
similarity remains absent. We introduce the Similarity Framework for
Visualization Retrieval (Safire), a conceptual model that frames visualization
similarity along two dimensions: comparison criteria and representation
modalities. Comparison criteria identify the aspects that make visualizations
similar, which we divide into primary facets (data, visual encoding,
interaction, style, metadata) and derived properties (data-centric and
human-centric measures). Safire connects what to compare with how comparisons
are executed through representation modalities. We categorize existing
representation approaches into four groups based on their levels of information
content and visualization determinism: raster image, vector image,
specification, and natural language description, together guiding what is
computable and comparable. We analyze several visualization retrieval systems
using Safire to demonstrate its practical value in clarifying similarity
considerations. Our findings reveal how particular criteria and modalities
align across different use cases. Notably, the choice of representation
modality is not only an implementation detail but also an important decision
that shapes retrieval capabilities and limitations. Based on our analysis, we
provide recommendations and discuss broader implications for multimodal
learning, AI applications, and visualization reproducibility.

</details>


### [293] [Comparing User Behavior in Real vs. Virtual Supermarket Shelves: An Eye-Tracking Study Using Tobii 3 Pro and Meta Quest Pro](https://arxiv.org/abs/2510.16764)
*Francesco Vona,Julia Schorlemmer,Paulina Kaulard,Sebastian Fischer,Jessica Stemann,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 比较真实与虚拟超市货架上的用户行为，使用眼动追踪技术分析29名参与者在两种环境中的注意力分布和产品选择策略差异。


<details>
  <summary>Details</summary>
Motivation: 探索虚拟环境是否能真实复制现实世界体验，特别是在消费者行为方面，验证虚拟超市作为研究工具的可行性。

Method: 将29名参与者随机分配到真实超市货架（使用Tobii眼动仪）和虚拟货架（使用Meta Quest Pro眼动追踪）两组，要求选择健康或美味类别的谷物产品，通过眼动数据分析注意力分布。

Result: 参与者的注意力在不同产品类型和购物环境间存在差异：真实环境中更关注下层货架（特别是健康产品），VR环境中注意力转移到视线水平货架（特别是美味产品），甜食产品在两种环境中都获得较少视觉关注。

Conclusion: 虚拟环境能够部分复制现实体验，但注意力分布模式存在差异，VR环境中的视线水平偏好与超市最优产品布局策略一致。

Abstract: This study compares user behavior between real and virtual supermarket
shelves using eye tracking technology to assess behavior in both environments.
A sample of 29 participants was randomly assigned to two conditions: a real
world supermarket shelf with Tobii eye tracking and a virtual shelf using the
Meta Quest Pro eye tracker. In both scenarios, participants were asked to
select three packs of cereals belonging to specific categories, healthy or
tasty. The aim was to explore whether virtual environments could realistically
replicate real world experiences, particularly regarding consumer behavior. By
analyzing eye tracking data, the study examined how attention and product
selection strategies varied between real and virtual conditions. Results showed
that participants' attention differed across product types and shopping
environments. Consumers focused more on lower shelves in real settings,
especially when looking for healthy products. In VR, attention shifted to eye
level shelves, particularly for tasty items, aligning with optimal product
placement strategies in supermarkets. Overall, sweet products received less
visual attention across both settings.

</details>


### [294] [Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models](https://arxiv.org/abs/2510.16952)
*Austin Drake,Hang Dong*

Main category: cs.HC

TL;DR: 提出了一种将大型语言模型安全集成到交互式游戏引擎中的新架构，允许玩家使用自然语言“编程”新行为。


<details>
  <summary>Details</summary>
Motivation: 让玩家能够使用自然语言创建游戏行为，同时通过约束机制确保安全性。

Method: 使用LLM将命令翻译成受约束的领域特定语言(DSL)，在运行时配置自定义实体-组件系统(ECS)。在2D法术制作游戏原型中评估了Gemini、GPT和Claude系列模型的各种提示策略。

Result: 验证的LLM评估显示，较大模型能更好地捕捉创意意图，但最佳提示策略取决于任务：思维链提高了创意对齐，而少样本示例对生成更复杂的DSL脚本是必要的。

Conclusion: 这项工作为涌现式游戏玩法提供了一个经过验证的LLM-ECS模式，并为开发者提供了定量性能比较。

Abstract: We present a novel architecture for safely integrating Large Language Models
(LLMs) into interactive game engines, allowing players to "program" new
behaviors using natural language. Our framework mitigates risks by using an LLM
to translate commands into a constrained Domain-Specific Language (DSL), which
configures a custom Entity-Component-System (ECS) at runtime. We evaluated this
system in a 2D spell-crafting game prototype by experimentally assessing models
from the Gemini, GPT, and Claude families with various prompting strategies. A
validated LLM judge qualitatively rated the outputs, showing that while larger
models better captured creative intent, the optimal prompting strategy is
task-dependent: Chain-of-Thought improved creative alignment, while few-shot
examples were necessary to generate more complex DSL scripts. This work offers
a validated LLM-ECS pattern for emergent gameplay and a quantitative
performance comparison for developers.

</details>


### [295] [Integrating Metaverse Technologies in Medical Education: Examining Acceptance Factors Among Current and Future Healthcare Providers](https://arxiv.org/abs/2510.16984)
*Seckin Damar,Gulsah Hancerliogullari Koksalmis*

Main category: cs.HC

TL;DR: 研究土耳其医学生和医生对医疗元宇宙平台的使用行为意向，整合多个理论模型发现满意度、感知有用性、感知易用性、学习者互动和技术准备度正向影响采用意愿，而技术焦虑和复杂性有负面影响。


<details>
  <summary>Details</summary>
Motivation: 在土耳其等医疗元宇宙技术处于早期采用阶段的国家，了解医疗专业人员对这类平台的接受度，为数字化教育转型提供指导。

Method: 整合创新扩散理论、具身社会临场感理论、互动等价定理和技术接受模型，构建多理论研究模型，使用偏最小二乘结构方程模型分析718名参与者的数据。

Result: 模型解释了行为意向71.8%的方差，满意度、感知有用性、感知易用性、学习者互动和技术准备度显著增强采用意愿，感知易用性完全中介技术焦虑与感知有用性的关系。

Conclusion: 研究结果为教育工作者、课程设计者和开发者将元宇宙平台整合到医疗培训中提供了实践启示，特别是在数字化转型的教育系统中。

Abstract: This study investigates behavioral intention to use healthcare metaverse
platforms among medical students and physicians in Turkey, where such
technologies are in early stages of adoption. A multi-theoretical research
model was developed by integrating constructs from the Innovation Diffusion
Theory, Embodied Social Presence Theory, Interaction Equivalency Theorem and
Technology Acceptance Model. Data from 718 participants were analyzed using
partial least squares structural equation modeling. Results show that
satisfaction, perceived usefulness, perceived ease of use, learner
interactions, and technology readiness significantly enhance adoption, while
technology anxiety and complexity have negative effects. Learner learner and
learner teacher interactions strongly predict satisfaction, which subsequently
increases behavioral intention. Perceived ease of use fully mediates the
relationship between technology anxiety and perceived usefulness. However,
technology anxiety does not significantly moderate the effects of perceived
usefulness or ease of use on behavioral intention. The model explains 71.8% of
the variance in behavioral intention, indicating strong explanatory power. The
findings offer practical implications for educators, curriculum designers, and
developers aiming to integrate metaverse platforms into healthcare training in
digitally transitioning educational systems.

</details>


### [296] [Planar or Spatial: Exploring Design Aspects and Challenges for Presentations in Virtual Reality with No-coding Interface](https://arxiv.org/abs/2510.17073)
*Liwei Wu,Yilin Zhang,Justin Leung,Jingyi Gao,April Li,Jian Zhao*

Main category: cs.HC

TL;DR: 开发了VRStory——一个无需编程的VR演示创作工具，通过用户研究发现虽然VR具有沉浸式和空间特性优势，但用户仍倾向于平面静态格式以确保可访问性和高效沟通。


<details>
  <summary>Details</summary>
Motivation: VR作为沉浸式演示媒介的潜力尚未完全发挥，因为创建引人入胜的VR演示仍然具有挑战性且耗时，阻碍了VR演示能力的充分实现。

Method: 通过分析流行演示软件和采访7位专业人士，识别VR演示的设计方面和挑战，开发VRStory原型工具，并对12名参与者进行用户研究。

Result: 用户认可VR的沉浸式和空间特性优势，但通常保持传统2D演示的心理模型，仍偏好平面静态格式以确保更好的可访问性和高效沟通。

Conclusion: 未来VR演示工具开发需要平衡沉浸式特性推广与确保可访问性，强调在促进沉浸体验的同时保持用户友好性。

Abstract: The proliferation of virtual reality (VR) has led to its increasing adoption
as an immersive medium for delivering presentations, distinct from other VR
experiences like games and 360-degree videos by sharing information in richly
interactive environments. However, creating engaging VR presentations remains a
challenging and time-consuming task for users, hindering the full realization
of VR presentation's capabilities. This research aims to explore the potential
of VR presentation, analyze users' opinions, and investigate these via
providing a user-friendly no-coding authoring tool. Through an examination of
popular presentation software and interviews with seven professionals, we
identified five design aspects and four design challenges for VR presentations.
Based on the findings, we developed VRStory, a prototype for presentation
authoring without coding to explore the design aspects and strategies for
addressing the challenges. VRStory offers a variety of predefined and
customizable VR elements, as well as modules for layout design, navigation
control, and asset generation. A user study was then conducted with 12
participants to investigate their opinions and authoring experience with
VRStory. Our results demonstrated that, while acknowledging the advantages of
immersive and spatial features in VR, users often have a consistent mental
model for traditional 2D presentations and may still prefer planar and static
formats in VR for better accessibility and efficient communication. We finally
shared our learned design considerations for future development of VR
presentation tools, emphasizing the importance of balancing of promoting
immersive features and ensuring accessibility.

</details>


### [297] [Toward a Cognitive-Affective-Systemic Framework for Art and Sustainability](https://arxiv.org/abs/2510.17083)
*Ivan C. H. Liu*

Main category: cs.HC

TL;DR: 本文提出了一个认知-情感-系统(CAS)框架，通过艺术整合认知、情感和系统理解来培养可持续性意识。该框架将艺术实践定义为认识论和表演性的，通过制作和感受来认知。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一个整合认知、情感和系统理解的框架，通过艺术培养可持续性意识，将复杂的科学转化为具身的生态理解。

Method: 方法包括基于生态美学、情感理论、复杂性科学和后人类伦理的CAS框架，定义艺术实践为认识论和表演性的，引入logomotion作为审美模式，并通过两个艺术作品进行演示。

Result: 结果展示了SPill和Echoes of the Land两个艺术作品如何通过系统建模和感官沉浸将复杂的科学转化为具身的生态理解，证明了框架的有效性。

Conclusion: 结论是该框架为艺术家、理论家和活动家提供了方法论基础，将意识转化为参与，推进集体创造力走向可持续未来。

Abstract: This paper proposes a ognitive-Affective-Systemic (CAS) framework that
integrates cognition, emotion, and systemic understanding to cultivate
sustainability awareness through art. Drawing from eco-aesthetics, affect
theory, complexity science, and posthuman ethics, the framework defines
artistic practice as both epistemic and performative--a way of knowing through
making and feeling. Central to this is logomotion, an aesthetic mode where
comprehension and emotion move together as a unified experience. Two artworks,
SPill, visualizing antimicrobial resistance through avalanche dynamics, and
Echoes of the Land, modeling anthropogenic seismicity, demonstrate how systemic
modeling and sensory immersion transform complex science into embodied
ecological understanding. The framework offers a methodological foundation for
artists, theorists, and activists to translate awareness into engagement,
advancing collective creativity toward sustainable futures.

</details>


### [298] [Kinesthetic Weight Modulation: The Effects of Whole-Arm Tendon Vibration on the Perceived Heaviness](https://arxiv.org/abs/2510.17102)
*Keigo Ushiyama,Hiroyuki Kajimoto*

Main category: cs.HC

TL;DR: 该研究探讨了多点肌腱振动如何调节重量感知，发现振动能显著增加感知重量但不能显著减少，且增加效果可在350-450克范围内分三个等级进行控制。


<details>
  <summary>Details</summary>
Motivation: 由于肌梭不仅参与感知身体运动还参与感知重量，振动诱导的错觉可能调节重量感知。呈现重量感对于丰富虚拟物体的触觉交互至关重要。

Method: 通过两个实验：实验1检验多点肌腱振动能否增加或减少感知重量；实验2研究如何系统控制效果大小。

Result: 肌腱振动显著增加了感知重量但没有显著减少，尽管观察到减少趋势。增加效果可在350-450克范围内分至少三个等级进行调节。

Conclusion: 多点肌腱振动可以有效调节重量感知，特别是增加感知重量，这为虚拟现实中的触觉交互提供了新可能。

Abstract: Kinesthetic illusions, which arise when muscle spindles are activated by
vibration, provide a compact means of presenting kinesthetic sensations.
Because muscle spindles contribute not only to sensing body movement but also
to perceiving heaviness, vibration-induced illusions could potentially modulate
weight perception. While prior studies have primarily focused on conveying
virtual movement, the modulation of perceived heaviness has received little
attention. Presenting a sense of heaviness is essential for enriching haptic
interactions with virtual objects. This study investigates whether multi-point
tendon vibration can increase or decrease perceived heaviness (Experiment 1)
and how the magnitude of the effect can be systematically controlled
(Experiment 2). The results show that tendon vibration significantly increases
perceived heaviness but does not significantly decrease it, although a
decreasing trend was observed. Moreover, the increase can be adjusted across at
least three levels within the range of 350-450 g. Finally, we discuss plausible
mechanisms underlying this vibration-induced modulation of weight perception.

</details>


### [299] [Design Framework for Conversational Agent in Couple relationships: A Systematic Review](https://arxiv.org/abs/2510.17119)
*Soyoung Jung,Sung Park*

Main category: cs.HC

TL;DR: 本文通过系统综述提出了面向伴侣的对话代理设计框架，重点关注关系情境下的心理健康支持，提出了8个设计考虑因素。


<details>
  <summary>Details</summary>
Motivation: 现有对话代理研究主要关注个体心理健康，而针对伴侣关系挑战的对话代理研究有限，需要探索支持伴侣关系福祉的设计方法。

Method: 遵循PRISMA指南，在7个数据库中进行系统综述，筛选出12项实证研究，从AI交互设计、关系框架和技术限制三个维度进行主题分析。

Result: 识别出三个关键主题：关系专家角色需求、利用先进AI实现关系特异性和情感能力的技术方向、从内容中心到关系中心的设计转变，并提出了8个设计考虑因素。

Conclusion: 提出了整合关系理论与先进AI技术的设计框架，为未来伴侣心理健康干预的对话代理开发提供指导，强调作为关系调解者的角色定位。

Abstract: The development of conversational agents (CAs) has shown strong potential in
supporting mental health through dialogue. While many studies focus on CAs for
individual psychological care, research on agents designed for couples facing
relational or emotional challenges remains limited. This study aims to identify
design considerations for CAs that address the relational context of couples
and support their well-being. Following PRISMA guidelines, a systematic review
was conducted across seven databases: CINAHL, Embase, PubMed, PsycINFO, Scopus,
Web of Science, and the ACM Digital Library. Peer-reviewed empirical studies
were screened, duplicates removed, and selection criteria applied, resulting in
twelve studies for analysis. Thematic analysis was conducted across three
dimensions: AI interaction design, relational framing, and technical
limitations. Three key themes emerged: (1) the need for a relational expert
persona, (2) technological directions leveraging state-of-the-art AI for
relational specificity and emotional competence, and (3) a shift from
content-centered to relationship-centered design. Based on these insights,
eight design considerations are proposed for couple-oriented CAs: (1) agent
persona, (2) individual mode, (3) concurrent mode, (4) conjoint mode, (5)
ethics, (6) data and privacy, (7) interaction pattern, and (8) safety
mechanism. These principles guide CAs as relational mediators capable of
maintaining multiple alliances, respecting cultural and ethical boundaries, and
ensuring fairness and emotional safety between partners. Ultimately, this
review introduces a design framework that integrates relational theory with
advanced AI technologies to inform future development of CAs for couple-based
mental health interventions.

</details>


### [300] [Augmented Web Usage Mining and User Experience Optimization with CAWAL's Enriched Analytics Data](https://arxiv.org/abs/2510.17253)
*Özkan Canay,{Ü}mit Kocabıcak*

Main category: cs.HC

TL;DR: 提出了增强网络使用挖掘(AWUM)方法，通过CAWAL框架丰富网络交互数据，分析超过120万条会话记录，发现87.16%的多页面会话贡献了98.05%的页面浏览量，40%用户使用多种服务，50%选择安全退出。


<details>
  <summary>Details</summary>
Motivation: 理解网络用户行为对于优化用户体验(UX)至关重要，需要增强传统的网络使用挖掘方法。

Method: 使用CAWAL框架收集应用日志和网络分析数据，处理120万条会话记录(约8.5GB)，分析会话结构、页面请求、服务交互和退出方式，并应用关联规则挖掘。

Result: 多页面会话占87.16%，贡献98.05%页面浏览量；40%用户访问多种服务；50%选择安全退出；关联规则挖掘揭示了常用服务模式。

Conclusion: AWUM方法提供了对用户行为的全面理解，在大规模UX优化方面具有强大潜力，CAWAL框架在精度和效率上优于传统方法。

Abstract: Understanding user behavior on the web is increasingly critical for
optimizing user experience (UX). This study introduces Augmented Web Usage
Mining (AWUM), a methodology designed to enhance web usage mining and improve
UX by enriching the interaction data provided by CAWAL (Combined Application
Log and Web Analytics), a framework for advanced web analytics. Over 1.2
million session records collected in one month (~8.5GB of data) were processed
and transformed into enriched datasets. AWUM analyzes session structures, page
requests, service interactions, and exit methods. Results show that 87.16% of
sessions involved multiple pages, contributing 98.05% of total pageviews; 40%
of users accessed various services and 50% opted for secure exits. Association
rule mining revealed patterns of frequently accessed services, highlighting
CAWAL's precision and efficiency over conventional methods. AWUM offers a
comprehensive understanding of user behavior and strong potential for
large-scale UX optimization.

</details>


### [301] [SmartSustain Recommender System: Navigating Sustainability Trade-offs in Personalized City Trip Planning](https://arxiv.org/abs/2510.17355)
*Ashmi Banerjee,Melih Mert Aksoy,Wolfgang Wörndl*

Main category: cs.HC

TL;DR: 开发了一个名为SmartSustain Recommender的旅游推荐系统，通过可视化CO2e排放、目的地受欢迎度和季节性等因素，引导用户选择更环保的旅行选项。


<details>
  <summary>Details</summary>
Motivation: 旅游业是全球碳排放和过度旅游的主要贡献者，需要推荐系统不仅能提供信息，还能温和引导用户做出更可持续的旅行决策。

Method: 开发了基于网络的推荐应用，结合CO2e排放、目的地受欢迎度、季节性和个性化兴趣匹配，使用交互式城市卡片进行快速比较，动态横幅展示可持续替代方案，以及实时环境影响反馈。

Result: 对21名参与者的初步用户研究表明，该系统具有强大的可用性和感知有效性。

Conclusion: SmartSustain Recommender是一个有效的工具，通过用户友好的界面成功引导用户选择更环保的旅行选项。

Abstract: Tourism is a major contributor to global carbon emissions and over-tourism,
creating an urgent need for recommender systems that not only inform but also
gently steer users toward more sustainable travel decisions. Such choices,
however, often require balancing complex trade-offs between environmental
impact, cost, convenience, and personal interests. To address this, we present
the SmartSustain Recommender, a web application designed to nudge users toward
eco-friendlier options through an interactive, user-centric interface. The
system visualizes the broader consequences of travel decisions by combining
CO2e emissions, destination popularity, and seasonality with personalized
interest matching. It employs mechanisms such as interactive city cards for
quick comparisons, dynamic banners that surface sustainable alternatives in
specific trade-off scenarios, and real-time impact feedback using animated
environmental indicators. A preliminary user study with 21 participants
indicated strong usability and perceived effectiveness. The system is
accessible at https://smartsustainrecommender.web.app.

</details>


### [302] [NieNie: Adaptive Rhythmic System for Stress Relief with LLM-Based Guidance](https://arxiv.org/abs/2510.17534)
*Yichen Yu,Qiaoran Wang*

Main category: cs.HC

TL;DR: NieNie是一个结合节奏生物反馈和LLM实时心理指导的交互式压力管理系统，通过触觉设备和自适应节奏游戏为年轻人提供沉浸式压力调节体验。


<details>
  <summary>Details</summary>
Motivation: 传统压力管理工具依赖静态脚本或被动内容，对缓解压力效果有限。年轻人面临日益增长的心理压力，需要更有效的干预方法。

Method: 系统收集心率变异性等生理信号，通过软触觉设备生成自适应挤压-释放节奏，并利用LLM提供及时的心理指导反馈，创建个性化的节奏游戏。

Result: NieNie将用户置于具身交互循环中，利用触觉交互、生物反馈和自适应语言支持，创造了沉浸式压力调节体验。

Conclusion: 该研究展示了具身系统如何在日常环境中连接身体动作与心理健康，为压力管理提供了新的技术途径。

Abstract: Today's young people are facing increasing psychological stress due to
various social issues. Traditional stress management tools often rely on static
scripts or passive content, which are ineffective in alleviating stress. NieNie
addresses this gap by combining rhythm biofeedback with real-time psychological
guidance through a large language model (LLM), offering an interactive, tactile
response. The system is specifically designed for young people experiencing
emotional stress, collecting physiological signals such as heart rate
variability and generating adaptive squeeze-release rhythms via soft, tactile
devices. Utilising LLM, the system provides timely squeezing rhythms and
psychologically guided feedback prompts, offering personalised rhythm games
while reinforcing stress restructuring. Unlike traditional mental health apps,
NieNie places users within an embodied interactive loop, leveraging tactile
interaction, biofeedback, and adaptive language support to create an immersive
stress regulation experience. This study demonstrates how embodied systems can
connect bodily actions with mental health in everyday contexts.

</details>


### [303] [DeTAILS: Deep Thematic Analysis with Iterative LLM Support](https://arxiv.org/abs/2510.17575)
*Ash Sharma,Karen Cochrane,James R. Wallace*

Main category: cs.HC

TL;DR: DeTAILS是一个集成大语言模型辅助的定性研究工具包，基于Braun和Clarke的主题分析框架，支持研究人员生成和优化代码、审查聚类、综合主题，通过交互式反馈循环保持分析自主性。


<details>
  <summary>Details</summary>
Motivation: 主题分析在定性研究中广泛应用，但由于其迭代性和解释性需求，难以扩展规模。需要工具来支持大规模定性分析，同时保持研究人员的分析自主性。

Method: 开发DeTAILS工具包，集成LLM辅助到主题分析工作流程中。通过18名定性研究人员分析Reddit数据进行评估，收集定量和定性数据。

Result: 定量结果显示LLM支持的输出与参与者优化结果高度一致，同时减少了工作负担并具有高感知有用性。定性反馈表明DeTAILS加速了分析过程，促进了与AI输出的反思性互动，并通过透明度和控制建立了信任。

Conclusion: 贡献包括：(1)大规模定性分析的交互式人-LLM工作流程；(2)其可行性和研究人员体验的实证证据；(3)可信AI辅助定性研究的设计启示。

Abstract: Thematic analysis is widely used in qualitative research but can be difficult
to scale because of its iterative, interpretive demands. We introduce DeTAILS,
a toolkit that integrates large language model (LLM) assistance into a workflow
inspired by Braun and Clarke's thematic analysis framework. DeTAILS supports
researchers in generating and refining codes, reviewing clusters, and
synthesizing themes through interactive feedback loops designed to preserve
analytic agency. We evaluated the system with 18 qualitative researchers
analyzing Reddit data. Quantitative results showed strong alignment between
LLM-supported outputs and participants' refinements, alongside reduced workload
and high perceived usefulness. Qualitatively, participants reported that
DeTAILS accelerated analysis, prompted reflexive engagement with AI outputs,
and fostered trust through transparency and control. We contribute: (1) an
interactive human-LLM workflow for large-scale qualitative analysis, (2)
empirical evidence of its feasibility and researcher experience, and (3) design
implications for trustworthy AI-assisted qualitative research.

</details>


### [304] [Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation](https://arxiv.org/abs/2510.17599)
*Hendric Voss,Lisa Michelle Bohnenkamp,Stefan Kopp*

Main category: cs.HC

TL;DR: 本研究比较了两种语音手势生成框架AQ-GT及其语义增强版本AQ-GT-a，发现原始框架在训练领域内更有效传达概念，而语义增强版本在形状和大小表示方面泛化能力更好，但语义增强并不保证手势生成质量的提升。


<details>
  <summary>Details</summary>
Motivation: 评估不同手势生成框架传达意义的能力以及人类对生成手势的感知，探索语义注释在手势生成中的作用。

Method: 使用SAGA空间通信语料库中的句子、上下文相似句子和新的运动焦点句子，进行以用户为中心的概念识别和拟人性评估。

Result: AQ-GT框架在训练领域内更有效传达概念，AQ-GT-a框架在表示形状和大小方面泛化能力更好，参与者认为AQ-GT-a手势更具表现力和帮助性，但不认为更拟人。

Conclusion: 明确的语义增强不能保证手势生成质量的提升，其有效性高度依赖于上下文，表明在专业化和泛化之间存在权衡。

Abstract: This study explores two frameworks for co-speech gesture generation, AQ-GT
and its semantically-augmented variant AQ-GT-a, to evaluate their ability to
convey meaning through gestures and how humans perceive the resulting
movements. Using sentences from the SAGA spatial communication corpus,
contextually similar sentences, and novel movement-focused sentences, we
conducted a user-centered evaluation of concept recognition and human-likeness.
Results revealed a nuanced relationship between semantic annotations and
performance. The original AQ-GT framework, lacking explicit semantic input, was
surprisingly more effective at conveying concepts within its training domain.
Conversely, the AQ-GT-a framework demonstrated better generalization,
particularly for representing shape and size in novel contexts. While
participants rated gestures from AQ-GT-a as more expressive and helpful, they
did not perceive them as more human-like. These findings suggest that explicit
semantic enrichment does not guarantee improved gesture generation and that its
effectiveness is highly dependent on the context, indicating a potential
trade-off between specialization and generalization.

</details>


### [305] [ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input](https://arxiv.org/abs/2510.17617)
*Hendric Voss,Stefan Kopp*

Main category: cs.HC

TL;DR: 本文提出了一种从语言和图像输入生成语义连贯的图标和指示手势的零样本系统，解决了当前手势生成方法只能产生简单节拍手势的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的手势生成方法仅限于简单的节拍手势，无法生成与言语语义相协调的图标或指示手势。语言输入本身缺乏手势所承载的视觉意义，因此需要结合图像信息来生成更具语义价值的手势。

Method: 开发了一个零样本系统，整合了图像分析管道（提取物体形状、对称性等关键属性）、语义匹配模块（将视觉细节与口语文本关联）以及逆向运动学引擎，用于合成图标和指示手势并与自然节拍手势结合。

Result: 用户研究表明，在语音表达模糊的场景下，系统生成的手势显著提高了参与者识别物体属性的能力，证实了手势的可解释性和沟通价值。

Conclusion: 虽然复杂形状的表示仍存在挑战，但研究结果强调了上下文感知语义手势对于创建表达性和协作性虚拟代理的重要性，朝着高效、鲁棒的具体化人机交互迈出了重要一步。

Abstract: Human communication combines speech with expressive nonverbal cues such as
hand gestures that serve manifold communicative functions. Yet, current
generative gesture generation approaches are restricted to simple, repetitive
beat gestures that accompany the rhythm of speaking but do not contribute to
communicating semantic meaning. This paper tackles a core challenge in
co-speech gesture synthesis: generating iconic or deictic gestures that are
semantically coherent with a verbal utterance. Such gestures cannot be derived
from language input alone, which inherently lacks the visual meaning that is
often carried autonomously by gestures. We therefore introduce a zero-shot
system that generates gestures from a given language input and additionally is
informed by imagistic input, without manual annotation or human intervention.
Our method integrates an image analysis pipeline that extracts key object
properties such as shape, symmetry, and alignment, together with a semantic
matching module that links these visual details to spoken text. An inverse
kinematics engine then synthesizes iconic and deictic gestures and combines
them with co-generated natural beat gestures for coherent multimodal
communication. A comprehensive user study demonstrates the effectiveness of our
approach. In scenarios where speech alone was ambiguous, gestures generated by
our system significantly improved participants' ability to identify object
properties, confirming their interpretability and communicative value. While
challenges remain in representing complex shapes, our results highlight the
importance of context-aware semantic gestures for creating expressive and
collaborative virtual agents or avatars, marking a substantial step forward
towards efficient and robust, embodied human-agent interaction. More
information and example videos are available here:
https://review-anon-io.github.io/ImaGGen.github.io/

</details>


### [306] [Muscle Anatomy-aware Geometric Deep Learning for sEMG-based Gesture Decoding](https://arxiv.org/abs/2510.17660)
*Adyasha Dash,Giulia Zappoli,Laya Das,Robert Riener*

Main category: cs.HC

TL;DR: 提出了一种基于对称正定流形的几何深度学习模型，用于从表面肌电信号中解码手势，通过无监督域适应解决跨受试者和会话的变异性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于欧几里得数据的深度神经网络不适合分析具有长程依赖关系的多维非平稳时间序列sEMG，且现有方法在跨受试者和会话时表现出高变异性，需要重新校准和自适应微调。

Method: 使用多核方法捕获时间和传感器间的特征，将特征投影到SPD流形上，在流形上学习后投影回欧几里得空间进行分类，并采用特定域批量归一化层处理会话间变异性。

Result: 在公开基准数据集上的实验表明，该方法在跨会话场景中相比欧几里得和其他基于SPD的模型具有优越的泛化能力，准确率分别提高了8.83和4.63个百分点。

Conclusion: 该方法推动了sEMG手势识别的最新技术，为基于流形的肌肉信号学习开辟了新的研究途径。

Abstract: Robust and accurate decoding of gesture from non-invasive surface
electromyography (sEMG) is important for various applications including spatial
computing, healthcare, and entertainment, and has been actively pursued by
researchers and industry. Majority of sEMG-based gesture decoding algorithms
employ deep neural networks that are designed for Euclidean data, and may not
be suitable for analyzing multi-dimensional, non-stationary time-series with
long-range dependencies such as sEMG. State-of-the-art sEMG-based decoding
methods also demonstrate high variability across subjects and sessions,
requiring re-calibration and adaptive fine-tuning to boost performance. To
address these shortcomings, this work proposes a geometric deep learning model
that learns on symmetric positive definite (SPD) manifolds and leverages
unsupervised domain adaptation to desensitize the model to subjects and
sessions. The model captures the features in time and across sensors with
multiple kernels, projects the features onto SPD manifold, learns on manifolds
and projects back to Euclidean space for classification. It uses a
domain-specific batch normalization layer to address variability between
sessions, alleviating the need for re-calibration or fine-tuning. Experiments
with publicly available benchmark gesture decoding datasets (Ninapro DB6,
Flexwear-HD) demonstrate the superior generalizability of the model compared to
Euclidean and other SPD-based models in the inter-session scenario, with up to
8.83 and 4.63 points improvement in accuracy, respectively. Detailed analyses
reveal that the model extracts muscle-specific information for different tasks
and ablation studies highlight the importance of modules introduced in the
work. The proposed method pushes the state-of-the-art in sEMG-based gesture
recognition and opens new research avenues for manifold-based learning for
muscle signals.

</details>


### [307] [Rethinking Search: A Study of University Students' Perspectives on Using LLMs and Traditional Search Engines in Academic Problem Solving](https://arxiv.org/abs/2510.17726)
*Md. Faiyaz Abdullah Sayeedi,Md. Sadman Haque,Zobaer Ibn Razzaque,Robiul Awoul Robin,Sabila Nawshin*

Main category: cs.HC

TL;DR: 学生交替使用Google和GPT进行学术研究，Google用于可信的多源信息，GPT用于总结和草稿。研究开发了结合两者优势的聊天机器人原型。


<details>
  <summary>Details</summary>
Motivation: 随着AI在学术问题解决中的整合增加，学生频繁在传统搜索引擎和大型语言模型之间切换，需要了解学生对这两种工具的认知及其在学术工作流中的整合。

Method: 采用混合方法，调查了109名不同学科的学生，并对12名参与者进行了深度访谈。使用ANOVA和卡方检验评估效率、满意度和工具偏好的差异。

Result: 学生常在GPT和Google之间切换：Google用于可信的多源信息，GPT用于总结、解释和起草。两种工具单独使用都不足够，强烈需要混合解决方案。

Conclusion: 开发了一个嵌入搜索界面的聊天机器人原型，结合GPT的对话能力和Google的可靠性，以增强学术研究并减少认知负荷。

Abstract: With the increasing integration of Artificial Intelligence (AI) in academic
problem solving, university students frequently alternate between traditional
search engines like Google and large language models (LLMs) for information
retrieval. This study explores students' perceptions of both tools, emphasizing
usability, efficiency, and their integration into academic workflows. Employing
a mixed-methods approach, we surveyed 109 students from diverse disciplines and
conducted in-depth interviews with 12 participants. Quantitative analyses,
including ANOVA and chi-square tests, were used to assess differences in
efficiency, satisfaction, and tool preference. Qualitative insights revealed
that students commonly switch between GPT and Google: using Google for
credible, multi-source information and GPT for summarization, explanation, and
drafting. While neither tool proved sufficient on its own, there was a strong
demand for a hybrid solution. In response, we developed a prototype, a chatbot
embedded within the search interface, that combines GPT's conversational
capabilities with Google's reliability to enhance academic research and reduce
cognitive load.

</details>


### [308] [Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts](https://arxiv.org/abs/2510.17753)
*Celeste Riley,Omar Al-Refai,Yadira Colunga Reyes,Eman Hammad*

Main category: cs.HC

TL;DR: 该论文调查了人机交互中的心理影响，通过认知、行为和情感三个维度分析AI对人类的影响，强调需要负责任和情境感知的AI设计。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互在新闻和研究平台中日益突出，出现了过度依赖、认知卸载、社会情感操纵以及人类能动性和判断力退化等风险，需要系统研究这些问题。

Method: 通过心理学三元组（认知、行为、情感）的视角，调查近期关于人机交互影响的研究文献。

Result: 研究发现AI能显著增强记忆力、创造力和参与度，但也带来批判性思维减弱、技能退化和焦虑增加等风险；情感结果同样复杂，AI系统在提供支持和减轻压力方面有潜力，但引发依赖、不适当依恋和伦理监督等担忧。

Conclusion: 需要负责任和情境感知的AI设计，强调纵向研究和基于实证的评估框架的重要性，以平衡AI带来的益处与新兴的人类中心风险。

Abstract: As stories of human-AI interactions continue to be highlighted in the news
and research platforms, the challenges are becoming more pronounced, including
potential risks of overreliance, cognitive offloading, social and emotional
manipulation, and the nuanced degradation of human agency and judgment. This
paper surveys recent research on these issues through the lens of the
psychological triad: cognition, behavior, and emotion. Observations seem to
suggest that while AI can substantially enhance memory, creativity, and
engagement, it also introduces risks such as diminished critical thinking,
skill erosion, and increased anxiety. Emotional outcomes are similarly mixed,
with AI systems showing promise for support and stress reduction, but raising
concerns about dependency, inappropriate attachments, and ethical oversight.
This paper aims to underscore the need for responsible and context-aware AI
design, highlighting gaps for longitudinal research and grounded evaluation
frameworks to balance benefits with emerging human-centric risks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [309] [Two-Stage Sketch-Based Smoke Illustration Generation using Stream Function](https://arxiv.org/abs/2510.15873)
*Hengyuan Chang,Xiaoxuan Xie,Syuhei Sato,Haoran Xie*

Main category: cs.GR

TL;DR: 提出一个两阶段的基于草图的烟雾插图生成框架，使用流函数和潜在扩散模型。用户草图指导流函数生成，流函数作为速度场生成器的控制条件，引导烟雾模拟与预期流动对齐。


<details>
  <summary>Details</summary>
Motivation: 利用草图指导烟雾流动生成，但草图缺乏连续变化和旋转流动细节，需要中间表示来捕捉这些信息。

Method: 两阶段框架：第一阶段用草图指导生成流函数作为中间表示；第二阶段用流函数作为控制条件，通过潜在扩散模型生成速度场，指导烟雾模拟。训练时使用流线编码全局流动动态作为草图指导。

Result: 流函数能够捕捉草图缺失的连续变化和旋转流动细节，生成的速度场可以引导烟雾模拟与预期流动对齐。

Conclusion: 提出的两阶段框架通过流函数作为中间表示，有效解决了草图指导烟雾生成时缺乏连续流动细节的问题。

Abstract: In this paper, we propose a two-stage sketch-based smoke illustration
generation framework using stream function and latent diffusion models (LDM).
The user sketch is used to guide the generation of the stream function, which
serves as the control condition for the velocity field generator. The generated
velocity field can be used to guide the smoke simulation to align with the
intended flow. We adopt streamlines to encode global flow dynamics as sketch
guidance during training. The stream function constitutes the intermediate
representation that captures continuous variation and rotational flow details
absent from sketches.

</details>


### [310] [Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images](https://arxiv.org/abs/2510.15874)
*Hao Jin,Haoran Xie*

Main category: cs.GR

TL;DR: 提出了一种通过运动草图引导在静态图像中动画化流体的框架，使用微调的潜在扩散模型生成运动场，并通过运动适配器控制流体运动


<details>
  <summary>Details</summary>
Motivation: 将运动融入静态图像能增强视觉表现力和沉浸感，但流体元素的复杂动态特性给建模和控制带来挑战，现有方法难以生成平滑且时间一致的流体运动

Method: 使用微调的条件潜在扩散模型从用户提供的草图中生成运动场，然后通过运动适配器将其集成到潜在视频扩散模型中，精确控制流体运动

Result: 该方法能够生成具有平滑和时间一致流体运动的景观视频

Conclusion: 提出的框架通过运动草图引导成功解决了在静态图像中动画化流体的挑战，实现了对流体运动的精确控制

Abstract: Integrating motion into static images not only enhances visual expressiveness
but also creates a sense of immersion and temporal depth, establishing it as a
longstanding and impactful theme in artistic expression. Fluid elements such as
waterfall, river, and oceans are common features in landscape, but their
complex dynamic characteristics pose significant challenges in modeling and
controlling their motion within visual computing. Physics-based methods are
often used in fluid animation to track particle movement. However, they are
easily affected by boundary conditions. Recently, latent diffusion models have
been applied to video generation tasks, demonstrating impressive capabilities
in producing high-quality and temporally coherent results. However, it is
challenging for the existing methods to animate fluid smooth and temporally
consistent motion. To solve these issues, this paper introduces a framework for
generating landscape videos by animating fluid in still images under the
guidance of motion sketches. We propose a finetuned conditional latent
diffusion model for generating motion field from user-provided sketches, which
are subsequently integrated into a latent video diffusion model via a motion
adapter to precisely control the fluid movement.

</details>


### [311] [Adaptive Frameless Rendering](https://arxiv.org/abs/2510.15876)
*Abhinav Dayal,Cliff Woolley,Benjamin Watson,David Luebke*

Main category: cs.GR

TL;DR: 提出了一种自适应无帧渲染方法，通过闭环反馈采样和GPU重建，在保持视觉质量的同时大幅减少所需采样数量


<details>
  <summary>Details</summary>
Motivation: 传统交互式渲染方法采样模式固定，无法根据时空颜色变化进行精细自适应，限制了渲染效率

Method: 使用闭环反馈引导采样朝向图像边缘或运动区域；采用时间深度缓冲区存储短期样本；GPU重建响应采样密度和时空颜色梯度；使用样本重投影改进重建

Result: 在模拟中，无帧渲染器比传统渲染方法减少一个数量级的采样数量（基于RMS误差衡量），计算开销仅增加15%

Conclusion: 自适应无帧渲染通过精细的时空自适应采样和重建，显著提高了渲染效率，在静态场景中产生更锐利的图像，在动态场景中保持实时性

Abstract: We propose an adaptive form of frameless rendering with the potential to
dramatically increase rendering speed over conventional interactive rendering
approaches. Without the rigid sampling patterns of framed renderers, sampling
and reconstruction can adapt with very fine granularity to spatio-temporal
color change. A sampler uses closed-loop feedback to guide sampling toward
edges or motion in the image. Temporally deep buffers store all the samples
created over a short time interval for use in reconstruction and as sampler
feedback. GPU-based reconstruction responds both to sampling density and
space-time color gradients. Where the displayed scene is static, spatial color
change dominates and older samples are given significant weight in
reconstruction, resulting in sharper and eventually antialiased images. Where
the scene is dynamic, more recent samples are emphasized, resulting in less
sharp but more up-to-date images. We also use sample reprojection to improve
reconstruction and guide sampling toward occlusion edges, undersampled regions,
and specular highlights. In simulation our frameless renderer requires an order
of magnitude fewer samples than traditional rendering of similar visual quality
(as measured by RMS error), while introducing overhead amounting to 15% of
computation time.

</details>


### [312] [Procedural modeling of urban land use](https://arxiv.org/abs/2510.15877)
*Thomas Lechner,Ben Watson,Uri Wilenski,Seth Tisue,Martin Felsen,Andy Moddrell,Pin Ren,Craig Brozefsky*

Main category: cs.GR

TL;DR: 提出了一种程序化生成城市土地利用模式的方法，自动为艺术家放置建筑物和道路


<details>
  <summary>Details</summary>
Motivation: 城市在数字制作中是重要内容元素，但其复杂性和规模使得建模非常困难。现有工具很少能帮助艺术家完成这项工作，而图形硬件的快速改进要求在不增加制作成本的情况下提供更丰富的内容

Method: 程序化生成城市中逼真的土地利用模式，自动化放置建筑物和道路

Result: 开发了一种帮助艺术家生成城市内容的方法

Conclusion: 该方法能够满足对更丰富城市内容的需求，同时控制制作成本

Abstract: Cities are important elements of content in digital productions, but their
complexity and size make them very challenging to model. Few tools exist that
can help artists with this work, even as rapid improvements in graphics
hardware create demand for richer content without matching increases in
production cost. We propose a method for procedurally generating realistic
patterns of land use in cities, automating placement of buildings and roads for
artists.

</details>


### [313] [Structural Tree Extraction from 3D Surfaces](https://arxiv.org/abs/2510.15886)
*Diogo de Andrade,Nuno Fachada*

Main category: cs.GR

TL;DR: 提出一种从3D多边形数据中提取层次树表示的方法，通过图表示和Steiner树建立优化连接，支持导航感知的几何分析。


<details>
  <summary>Details</summary>
Motivation: 传统骨架化方法通常假设体积解释，而该方法直接在表面上操作，确保结果表示对导航感知的几何分析保持相关性。

Method: 首先提取表面的图表示，然后生成Steiner树来建立关键终端点之间的优化连接，并可利用视线约束进一步优化结构。

Result: 在程序内容生成和自动关卡分析两个用例中得到验证，能够产生简化的连贯表示。

Conclusion: 该方法支持程序生成、空间推理和地图分析等应用，能够从3D多边形数据中提取有效的层次结构表示。

Abstract: This paper introduces a method to extract a hierarchical tree representation
from 3D unorganized polygonal data. The proposed approach first extracts a
graph representation of the surface, which serves as the foundation for
structural analysis. A Steiner tree is then generated to establish an optimized
connection between key terminal points, defined according to
application-specific criteria. The structure can be further refined by
leveraging line-of-sight constraints, reducing redundancy while preserving
essential connectivity. Unlike traditional skeletonization techniques, which
often assume volumetric interpretations, this method operates directly on the
surface, ensuring that the resulting representation remains relevant for
navigation-aware geometric analysis. The method is validated through two use
cases: extracting structural representations from tile-based elements for
procedural content generation, and identifying key points and structural
metrics for automated level analysis. Results demonstrate its ability to
produce simplified, coherent representations, supporting applications in
procedural generation, spatial reasoning, and map analysis.

</details>


### [314] [Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search](https://arxiv.org/abs/2510.16147)
*Maxim Gumin,Do Heon Han,Seung Jean Yoo,Aditya Ganeshan,R. Kenny Jones,Kailiang Fu,Rio Aguina-Kang,Stewart Morris,Daniel Ritchie*

Main category: cs.GR

TL;DR: 本文提出了一种基于指令式范式的3D场景布局生成方法，使用LLM迭代放置物体，相比传统的声明式方法更简单且能处理更复杂的场景。


<details>
  <summary>Details</summary>
Motivation: 解决从开放词汇文本描述合成3D场景的挑战，特别是布局生成问题。传统声明式方法使用LLM生成约束规范再求解，而本文探索指令式方法，让LLM迭代放置物体。

Method: 采用指令式范式，LLM迭代放置物体，每个物体的位置和方向基于先前放置的物体计算。开发了错误校正机制，在保持原始布局的同时提高场景有效性。

Result: 在强制选择感知研究中，参与者82%和94%的时间偏好本文的指令式方法生成的布局，相比两种声明式方法。提出了与人类偏好一致的自动评估指标。

Conclusion: 指令式方法比声明式方法更简单且能处理更复杂的场景，错误校正机制提高了鲁棒性，人类偏好验证了方法的优越性。

Abstract: Synthesizing 3D scenes from open-vocabulary text descriptions is a
challenging, important, and recently-popular application. One of its critical
subproblems is layout generation: given a set of objects, lay them out to
produce a scene matching the input description. Nearly all recent work adopts a
declarative paradigm for this problem: using an LLM to generate a specification
of constraints between objects, then solving those constraints to produce the
final layout. In contrast, we explore an alternative imperative paradigm, in
which an LLM iteratively places objects, with each object's position and
orientation computed as a function of previously-placed objects. The imperative
approach allows for a simpler scene specification language while also handling
a wider variety and larger complexity of scenes. We further improve the
robustness of our imperative scheme by developing an error correction mechanism
that iteratively improves the scene's validity while staying as close as
possible to the original layout generated by the LLM. In forced-choice
perceptual studies, participants preferred layouts generated by our imperative
approach 82% and 94% of the time when compared against two declarative layout
generation methods. We also present a simple, automated evaluation metric for
3D scene layout generation that aligns well with human preferences.

</details>


### [315] [Region-Aware Wasserstein Distances of Persistence Diagrams and Merge Trees](https://arxiv.org/abs/2510.16486)
*Mathieu Pont,Christoph Garth*

Main category: cs.GR

TL;DR: 本文提出了一种针对持久图(persistence diagrams)和合并树(merge trees)的广义Wasserstein距离，通过利用拓扑特征在输入域中的区域信息，提供了比经典Wasserstein距离更具区分度的度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有的拓扑数据分析方法中，经典Wasserstein距离在比较持久图和合并树时缺乏对拓扑特征区域属性的考虑，需要一种更精细的度量方法来捕捉拓扑特征的完整信息。

Method: 重新定义拓扑特征的比较方式，将其视为极值对齐区域值之间的距离；提供两种策略控制计算时间和内存使用：使用区域子集进行计算，以及压缩区域属性以获得低内存表示。

Result: 在公开可用的集成数据上进行广泛实验，平均运行时间在分钟级别；能够跟踪时变集成中拓扑特征的演化，并计算用于降维目的的距离矩阵。

Conclusion: 提出的广义Wasserstein距离比经典方法更具区分度，能够有效跟踪拓扑特征演化并支持集成数据的可视化分析，提供了实用的C++实现。

Abstract: This paper presents a generalization of the Wasserstein distance for both
persistence diagrams and merge trees [20], [66] that takes advantage of the
regions of their topological features in the input domain. Specifically, we
redefine the comparison of topological features as a distance between the
values of their extrema-aligned regions. It results in a more discriminative
metric than the classical Wasserstein distance and generalizes it through an
input parameter adjusting the impact of the region properties in the distance.
We present two strategies to control both computation time and memory storage
of our method by respectively enabling the use of subsets of the regions in the
computation, and by compressing the regions' properties to obtain low-memory
representations. Extensive experiments on openly available ensemble data
demonstrate the efficiency of our method, with running times on the orders of
minutes on average. We show the utility of our contributions with two
applications. First, we use the assignments between topological features
provided by our method to track their evolution in time-varying ensembles and
propose the temporal persistence curves to facilitate the understanding of how
these features appear, disappear and change over time. Second, our method
allows to compute a distance matrix of an ensemble that can be used for
dimensionality reduction purposes and visually represent in 2D all its members,
we show that such distance matrices also allow to detect key phases in the
ensemble. Finally, we provide a C++ implementation that can be used to
reproduce our results.

</details>


### [316] [Filtering of Small Components for Isosurface Generation](https://arxiv.org/abs/2510.16684)
*Devin Zhao,Rephael Wenger*

Main category: cs.GR

TL;DR: 该论文研究了从扫描数据构建等值面时去除微小干扰组件的方法，通过简单的数据预过滤来改善可视化效果。


<details>
  <summary>Details</summary>
Motivation: 从CT扫描或MRI等扫描数据构建的等值面通常包含大量微小组件，这些组件会分散可视化注意力且不属于任何几何模型。

Method: 使用简单的数据预过滤技术来移除等值面中的微小组件，同时不影响构成可视化主体的大型组件。

Result: 通过实验验证了预过滤方法在去除微小干扰组件方面的有效性。

Conclusion: 简单的预过滤技术可以有效改善从扫描数据构建的等值面可视化质量，去除干扰性微小组件。

Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface
is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some
$\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$.
Isosurfaces constructed from scanned data such as CT scans or MRIs often
contain extremely small components that distract from the visualization and do
not form part of any geometric model produced from the data. Simple
prefiltering of the data can remove such small components while having no
effect on the large components that form the body of the visualization. We
present experimental results on such filtering.

</details>


### [317] [A Scalable In Transit Solution for Comprehensive Exploration of Simulation Data](https://arxiv.org/abs/2510.16966)
*Paascal Grosset,James Ahrens*

Main category: cs.GR

TL;DR: SeerX是一个轻量级、可扩展的传输中(in-transit)原位服务，支持动态资源分配和3D模拟数据的有损压缩，使多个模拟能够将分析卸载到共享的弹性服务基础设施中，无需MPI同步。


<details>
  <summary>Details</summary>
Motivation: 随着模拟产生的数据超过超级计算机的磁盘空间，许多模拟采用原位分析和可视化来减少需要存储的数据量。但原位可视化的有效性受到先验知识需求的限制：需要知道可视化参数来突出感兴趣的特征，以及提前不知道运行原位工作流需要多少资源。

Method: 提出SeerX，一个轻量级、可扩展的传输中原位服务，支持动态资源分配和3D模拟数据的有损压缩。SeerX使多个模拟能够将分析卸载到共享的弹性服务基础设施中，无需MPI同步。

Result: SeerX解决了原位可视化中的两个关键挑战：动态资源分配和参数选择问题，通过共享的弹性服务基础设施支持多个模拟的分析卸载。

Conclusion: SeerX提供了一个有效的解决方案，通过传输中原位服务解决了大规模模拟数据存储和分析的资源管理问题，支持动态资源分配和多个模拟的并行分析。

Abstract: As simulations produce more data than available disk space on supercomputers,
many simulations are employing in situ analysis and visualization to reduce the
amount of data that needs to be stored. While in situ visualization offers
potential for substantial data reduction, its efficacy is hindered by the need
for a priori knowledge. First, we need to know what visualization parameters to
use to highlight features of interest. Second, we do not know ahead of time how
much resources will be needed to run the in situ workflows, e.g. how many
compute nodes will be needed for in situ work. In this work, we present SeerX,
a lightweight, scalable in-transit in situ service that supports dynamic
resource allocation and lossy compression of 3D simulation data. SeerX enables
multiple simulations to offload analysis to a shared, elastic service
infrastructure without MPI synchronization.

</details>


### [318] [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](https://arxiv.org/abs/2510.17101)
*Lu Yin,Ziying Shi,Yinghao Wu,Xinyu Yi,Feng Xu,Shihui Guo*

Main category: cs.GR

TL;DR: SAIP是首个考虑身体形状差异的稀疏惯性传感器运动捕捉方法，通过分解传感器测量中的形状和姿态信息，能够有效处理不同体型（如儿童）的运动捕捉任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖标准成人身体形状模板，难以泛化到体型差异较大的个体（如儿童），主要原因是身体形状变化会导致IMU测量的加速度发生变化。

Method: 1. 训练回归模型将真实身体的IMU加速度转换为匹配模板成人身体模型；2. 使用现有方法估计模板身体的全身运动；3. 使用第二个回归模型将关节速度映射回真实身体，并结合形状感知的物理优化策略计算全局运动。

Result: 提出了首个包含不同体型个体的IMU运动捕捉数据集（10名儿童和10名成人，身高110-190cm，共400分钟配对数据）。实验结果表明SAIP能有效处理不同体型的运动捕捉任务。

Conclusion: SAIP通过考虑身体形状差异，解决了现有惯性运动捕捉方法在泛化到不同体型个体时的局限性，是首个形状感知的惯性姿态估计方案。

Abstract: Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [319] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出Gram行列式评分方法，用于评估无真实标签数据集的可靠性，该评分具有实验无关性，能在不同统计实验下保持一致的可靠性排序。


<details>
  <summary>Details</summary>
Motivation: 解决在无法获取真实数据的情况下，如何评估来自潜在策略性数据源的数据集可靠性的问题。

Method: 定义基于真实数据的可靠性排序，提出Gram行列式评分方法，通过计算观测数据和实验结果的经验分布向量所张成的体积来度量可靠性。

Result: Gram行列式评分能保持多种基于真实数据的可靠性排序，且在实验无关性方面具有唯一性。在合成噪声模型、CIFAR-10嵌入和真实就业数据上的实验验证了其有效性。

Conclusion: Gram行列式评分是一种有效的数据集可靠性评估方法，能够跨不同观测过程捕捉数据质量。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [320] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 本文研究了Hedge算法在组合设置中的最优性，证明Hedge在大多数组合设置中接近最优，但在某些特定设置下存在√log d的次优性差距，同时确定了Hedge在在线多任务学习中的最优性。


<details>
  <summary>Details</summary>
Motivation: 研究Hedge算法在组合在线学习设置中的最优性，确定其在各种组合问题中的性能边界，包括扩展形式博弈、资源分配、m-集合、在线多任务学习和DAG最短路径问题。

Method: 通过建立组合设置的下界Ω(√(T log(|X|)/log d))，与Hedge的上界O(√(T log |X|))进行比较，分析Hedge在不同组合设置中的性能差距。

Result: 证明Hedge在大多数组合设置中接近最优（最多相差√log d因子），但在m-集合（log d ≤ m ≤ √d）设置下存在√log d的次优性差距，而在在线多任务学习中Hedge是最优的。

Conclusion: Hedge在组合在线学习中具有普遍适用性和接近最优性，其性能分析为组合设置中的算法设计提供了理论指导，特别是在DAG最短路径问题中，基于膨胀熵正则化的OMD算法继承了Hedge的接近最优性能。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [321] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 提出了一种用于变电站级电压估计的分层图神经网络，利用电网拓扑和物理特征，在低观测性条件下仍保持鲁棒性，在SMART-DS数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透率和配电网电压波动性增加，传统配电网状态估计方法难以应对稀疏测量和大规模网络，需要更可扩展的解决方案。

Method: 使用分层图神经网络，结合电网拓扑结构和物理特征，专门针对实际配电网中常见的低观测性场景设计。

Result: 在SMART-DS数据集上测试，该方法比替代数据驱动模型的RMSE降低达2倍，在仅1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络有望为配电网提供可扩展、可复现且数据驱动的电压监测解决方案。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [322] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出一种基于残差学习的非线性交流最优潮流求解方法，利用快速直流最优潮流解作为基线，学习非线性修正来获得完整交流最优潮流解。


<details>
  <summary>Details</summary>
Motivation: 解决非线性交流最优潮流问题的计算瓶颈，实现实时电网运行决策。

Method: 使用拓扑感知图神经网络，结合局部注意力和两级直流特征集成，采用物理信息损失函数来保证交流潮流可行性和运行限制。

Result: 在57、118和2000总线系统上测试，相比传统交流最优潮流求解器，MSE降低约25%，可行性误差减少达3倍，运行速度提升达13倍。模型在N-1故障情况下保持准确性，并能高效扩展到大型网络。

Conclusion: 残差学习是连接线性近似和交流可行最优潮流的一种实用且可扩展的桥梁，可实现近实时运行决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [323] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 本文研究了一个非平稳多臂老虎机问题，其中奖励取决于动作和潜在状态，状态由未知线性动力学控制。提出了探索-承诺算法，在探索阶段使用随机动作估计线性动力学参数，在承诺阶段优化动作序列以获得长期奖励，实现了$\tilde{\mathcal{O}}(T^{2/3})$遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 解决动作和潜在状态共同影响奖励的非平稳老虎机问题，其中状态动力学也受动作影响，导致短期和长期奖励之间的权衡问题。

Method: 提出探索-承诺算法：探索阶段使用随机Rademacher动作估计线性动力学的马尔可夫参数；承诺阶段使用估计参数设计优化动作序列。

Result: 算法实现了$\tilde{\mathcal{O}}(T^{2/3})$遗憾上界。解决了从时间相关奖励中学习和设计最优长期奖励动作序列两个关键挑战。

Conclusion: 通过系统辨识和不确定二次优化等价性证明，为具有动作依赖状态动力学的非平稳老虎机问题提供了有效的算法和理论保证。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [324] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: 本文提出ControlValve防御机制，通过生成允许的控制流图并强制所有执行符合这些图来防御多智能体系统中的控制流劫持攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制如LlamaFirewall依赖对齐检查，但攻击仍能绕过这些防御，因为多智能体系统的安全性和功能性目标存在根本冲突。

Method: ControlValve基于控制流完整性和最小权限原则，生成允许的控制流图，并强制所有执行符合这些图，同时为零样本生成的每个智能体调用提供上下文规则。

Result: ControlValve能够有效防御控制流劫持攻击，解决了现有防御机制的局限性。

Conclusion: ControlValve为多智能体系统提供了一种新的安全防御机制，通过控制流完整性原则来确保系统安全。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [325] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 该论文分析了安全强化学习中拉格朗日乘子的最优性和稳定性问题，发现自动更新乘子能够恢复甚至超过最优性能，但存在振荡行为，可通过PID控制缓解。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，拉格朗日方法被广泛用于处理约束优化问题，但其效果严重依赖于拉格朗日乘子λ的选择。目前缺乏关于自动更新乘子鲁棒性和对整体性能影响的实证证据。

Method: 通过分析多个任务中拉格朗日乘子的最优性和稳定性，提供λ-profile可视化优化问题中回报与约束成本之间的权衡关系，并研究自动乘子更新和PID控制更新的效果。

Result: 研究发现λ具有高度敏感性，缺乏选择最优值λ*的通用直觉。自动乘子更新能够恢复甚至超过λ*的最优性能，但训练过程中表现出振荡行为。PID控制更新可以缓解振荡，但需要仔细调参。

Conclusion: 拉格朗日乘子在安全强化学习中具有高度敏感性，自动更新方法能够取得良好性能但存在稳定性问题，需要进一步研究如何稳定化拉格朗日方法。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [326] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化可扩展框架，通过原生辅助功能API系统探索应用程序，解决了GUI自动化中的数据收集挑战，并构建了包含27,171个任务的GUIrilla-Task数据集。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理在复杂图形用户界面操作方面面临数据可用性限制，包括昂贵的手动标注、闭源数据集和表面级合成管道，特别是在macOS生态系统中的代表性不足。

Method: 使用原生辅助功能API系统探索应用程序，将发现的界面元素和爬虫动作组织成分层GUI图，并采用专门的交互处理程序实现全面的应用程序覆盖。

Result: 构建了GUIrilla-Task数据集，包含27,171个功能基础任务，覆盖1,108个macOS应用。在ScreenSpot Pro基准测试中，基于GUIrilla-Task调优的LLM代理性能显著提升，使用97%更少数据的情况下优于合成基线。

Conclusion: GUIrilla框架有效解决了桌面自动化中的数据收集挑战，发布的macapptree库、GUIrilla-Task数据集和GUIrilla-Gold基准将支持桌面自主性的开放研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [327] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 开发了一个决策支持系统，通过预训练的AI智能体缩小人类可采取的行动集，实现人机互补。在1600人参与的野火缓解游戏中，使用该系统的参与者表现比单独行动提升约30%，比AI智能体提升超过2%。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在顺序决策任务中实现人机互补，就像分类任务中通过自适应控制人类代理水平实现互补性那样。

Method: 使用预训练AI智能体缩小人类可采取的行动集，然后让人类从这个行动集中选择行动。引入了一个利用系统提供的行动集平滑性特性来优化人类代理水平的bandit算法。

Result: 在1600人参与的野火缓解游戏研究中，使用决策支持系统的参与者表现比单独行动提升约30%，比AI智能体提升超过2%，尽管AI智能体本身比无支持的人类表现好很多。

Conclusion: 通过缩小行动选择范围来调节人类代理水平，可以在顺序决策任务中实现有效的人机互补。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [328] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 提出SAFE-D框架，用于检测帕金森病相关的驾驶行为异常，通过多源数据整合和注意力网络实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注暂时性异常（如疲劳、分心），而缺乏对慢性疾病（如帕金森病）引起的病理性行为偏差的检测机制。

Method: 分析帕金森病症状学，建立与驾驶性能下降的因果关系；整合多车辆控制组件数据构建行为档案；设计基于注意力的网络自适应优先处理时空特征。

Result: 在Logitech G29平台和CARLA模拟器上验证，使用三个道路地图模拟真实驾驶，SAFE-D在区分正常和帕金森病影响驾驶模式方面达到96.8%的平均准确率。

Conclusion: SAFE-D框架能有效检测帕金森病相关的驾驶行为异常，为提升驾驶安全提供了新方法。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [329] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: 提出了FedPURIN框架，通过整数规划识别关键参数进行传输，结合稀疏聚合显著降低通信开销，同时保持个性化联邦学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中现有方法通信效率低下的问题，降低通信负担以促进实际部署。

Method: 使用整数规划策略识别关键参数，结合稀疏聚合方案，在保持模型效能的同时显著减少通信量。

Result: 在标准图像分类基准测试中，在多种非IID条件下表现出与最先进方法相竞争的性能，并通过稀疏聚合实现了可量化的通信减少。

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于处理异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [330] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 使用半监督学习和正未标记学习策略，通过深度学习方法解决考古预测建模中的标签稀缺问题，在DEM和卫星影像数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 考古预测建模需要结合已知遗址位置与环境、文化、地理空间变量来预测未知遗址位置，但面临结构性标签稀缺问题：阳性样本稀少，大多数位置未标记。

Method: 采用半监督正未标记学习策略，实现为语义分割模型，使用动态伪标签和条件随机场来提高标签置信度，在严重类别不平衡下进行优化。

Result: 在DEM数据集上表现与最先进方法LAMAP相当但Dice分数更高；在原始卫星影像上通过分层k折交叉验证保持性能，产生更具可解释性的预测表面。

Conclusion: 半监督学习为在大型稀疏标注景观中识别未知遗址提供了有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [331] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉回路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间，同时达到或超越现有最优方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续表示学习方法在相似性匹配阶段存在多重共线性问题，且更先进的方法计算成本过高，不适合实时低延迟应用。

Method: 提出Fly-CL框架，利用果蝇嗅觉回路的设计原理，渐进式解决多重共线性问题，实现低时间复杂度的有效相似性匹配。

Result: Fly-CL在不同网络架构和数据机制下均表现出色，大幅减少训练时间的同时，性能达到或超过当前最优方法。

Conclusion: Fly-CL通过生物启发设计有效解决了持续表示学习中的多重共线性挑战，为实时应用提供了高效解决方案。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [332] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域泛化持续学习（DGCL）新设置，并开发了自适应领域变换（DoT）方法，通过解耦语义和领域信息来提升模型在动态环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法假设训练和测试域相同，无法适应真实世界中不断变化的多领域环境。需要一种能在学习新任务时同时保持跨领域泛化能力的方法。

Method: 提出自适应领域变换（DoT），受人类大脑分布式加枢纽理论启发，解耦语义和领域相关信息，自适应地跨领域变换任务表示以实现输出对齐。

Result: DoT作为插件策略显著提升了现有持续学习基线方法在DGCL设置下的性能，验证了其有效性和资源效率。

Conclusion: DoT成功解决了DGCL挑战，能够积累领域泛化知识，为动态真实世界环境中的智能系统提供了有效的学习框架。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [333] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出基于矩阵自由能的新型自编码器正则化方法，通过优化代码矩阵的奇异值分布使其接近高斯分布，提高泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在泛化性和代码分布控制方面存在不足，需要一种能够确保代码矩阵具有高斯分布特性的正则化方法

Method: 基于矩阵自由能定义可微损失函数，通过随机矩阵理论优化代码矩阵的奇异值分布，使其与独立同分布高斯随机矩阵的分布一致

Result: 经验模拟显示该方法能产生高斯化代码，在训练和测试集上都具有良好泛化性能，并成功应用于欠定逆问题

Conclusion: 矩阵自由能正则化是一种有效的自编码器正则化方法，能够可靠地产生高斯代码并提升模型泛化能力

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [334] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文分析了生成视觉模型中内部表征的演变，从GANs和VAEs到扩散模型，提出了严格合成与广义合成的区分，认为扩散模型通过分层表征挑战了统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型内部表征的演变，特别是从GANs/VAEs到扩散模型的转变，探讨表征如何在不同模型架构中分布和演变。

Method: 通过模型架构的详细分析和针对性的实验设置，干预分层表征，研究扩散模型如何分散表征负担。

Result: 发现扩散模型将表征负担分散到不同层，挑战了统一内部空间的假设，支持广义合成的概念。

Conclusion: 生成AI应被理解为专门过程的涌现配置，而非内容的直接合成，需要重新思考对生成AI的理解方式。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [335] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES（模态感知学习率调度器），通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提升多模态和单模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，即网络过度依赖某一模态，导致性能不佳，限制了多模态学习的潜力。

Method: MILES利用训练过程中模态条件利用率差异，动态调整学习率以平衡多模态学习速度，确保各模态均衡学习。

Result: 在四个多模态联合融合任务中，MILES优于七个最先进的基线方法，在所有任务和融合方法中表现最佳，有效平衡模态使用。

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES能改善多模态性能并增强模态编码器，适用于单模态样本或缺失模态场景。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [336] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉变换器，用于从肺部超声视频中区分心源性肺水肿与非心源性和正常肺部，在参数更少的情况下实现了最佳分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的高度视觉变异性，以及重叠B线和胸膜伪影的普遍存在，使得肺部超声视频的自动分类变得困难。

Method: 提出ZACH-ViT（零标记自适应紧凑分层视觉变换器），移除位置嵌入和[CLS]标记，使其完全置换不变；提出ShuffleStrides数据增强方法，在保持解剖有效性的同时置换探头视图序列和帧顺序。

Result: 在380个LUS视频上评估，ZACH-ViT获得最高的验证和测试ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而所有竞争模型都崩溃为平凡分类。训练速度比Minimal ViT快1.35倍，参数少2.5倍。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [337] [Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress](https://arxiv.org/abs/2510.16406)
*Yujun Zheng,Xinya Chen,Xueqin Lu,Weiguo Sheng,Shengyong Chen*

Main category: cs.NE

TL;DR: 提出了一种考虑员工情绪压力的呼叫中心排班方法，通过情感压力驱动模型和混合优化算法来提升客户服务水平。


<details>
  <summary>Details</summary>
Motivation: 现有排班方法通常忽视情绪压力对员工工作表现的显著影响，需要更真实地理解和利用人类行为进行排班。

Method: 建立情感压力驱动模型估计员工工作表现，提出结合全局变异和邻域搜索的模因优化算法，并辅以深度强化学习。

Result: 在银行呼叫中心真实案例上的实验表明，该方法在性能上优于现有流行排班方法。

Conclusion: 通过明确建模和融入情绪压力，该方法在员工排班中体现了对人类行为更真实的理解和利用。

Abstract: Emotional stress often has a significant effect on the working performance of
staff, but this effect is commonly neglected in existing staff scheduling
methods. We study a call-center staff scheduling problem, which considers the
evolution of work performance of staff under emotional stress. First, we
present an emotional stress driven model that estimates the working performance
of call-center employees based on not only skill levels but also emotional
states. On the basis of the model, we formulate a combined short-term and
long-term call-center staff scheduling problem aiming at maximizing the
customer service level, which depends on the working performance of employees.
We then propose a memetic optimization algorithm combining global mutation and
neighborhood search assisted by deep reinforcement learning to efficiently
solve this problem. Experimental results on real-world problem instances of
bank call-center staff scheduling demonstrate the performance advantages of the
proposed method over selected popular staff scheduling methods. By explicitly
modeling and incorporating emotional stress, our method reflects a more
realistic understanding and utilization of human behavior in staff scheduling.

</details>


### [338] [Bombardier Beetle Optimizer: A Novel Bio-Inspired Algorithm for Global Optimization](https://arxiv.org/abs/2510.17005)
*Hisham A. Shehadeh,Mohd Yamani Idna Idris,Iqbal H. Jebril*

Main category: cs.NE

TL;DR: 提出了一种新的仿生优化算法——轰炸甲虫优化器(BBO)，灵感来自轰炸甲虫的防御和逃生机制，在CEC 2017测试套件上表现出优于其他算法的性能。


<details>
  <summary>Details</summary>
Motivation: 受轰炸甲虫智能防御和逃生行为的启发，开发一种新的元启发式优化算法。轰炸甲虫能够感知威胁并触发化学反应防御，同时计算与捕食者的距离并逃离。

Method: BBO算法模拟轰炸甲虫的两种机制：防御机制（触发有毒化学喷雾）和逃生机制（计算距离并飞行逃离）。算法在CEC 2017测试套件上进行验证。

Result: BBO在收敛速度和解的质量方面优于Chernobyl Disaster Optimizer、Grey Wolf Optimizer、Particle Swarm Optimization等知名元启发式算法。

Conclusion: BBO算法在优化性能方面表现出色，证明了轰炸甲虫行为启发的有效性，为元启发式优化算法提供了新的思路。

Abstract: In this paper, a novel bio-inspired optimization algorithm is proposed,
called Bombardier Beetle Optimizer (BBO). This type of species is very
intelligent, which has an ability to defense and escape from predators. The
principles of the former one is inspired by the defense mechanism of Bombardier
Beetle against the predators, which the Bombardier Beetle triggers a toxic
chemical spray when it feels threatened. This reaction occurs in a specialized
reaction chamber inside its abdomen and includes a well regulated enzymatic
mechanism, which comprises hot water vapor, oxygen, and irritating substances
like p-benzoquinones. In addition, the proposed BBO simulates also the escape
mechanism of Bombardier Beetle from predator, which it has the ability to
calculate its distance from predator and it can fly away. The BBO is tested
with optimizing Congress on Evolutionary Computation (CEC 2017) test bed
suites. Moreover, it is compared against well-known metaheuristic optimization
algorithms includes Chernobyl Disaster Optimizer (CDO), Grey Wolf Optimizer
(GWO), Particle Swarm Optimization (PSO), Bermuda Triangle Optimizer (BTO),
Sperm Swarm Optimization (SSO) and Gravitational Search Algorithm (GSA). The
outcomes of this paper prove the BBO's efficiency in which outperforms the
other algorithms in terms of convergence rate and quality of results.

</details>


### [339] [ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine](https://arxiv.org/abs/2510.17392)
*Sonu Kumar,Arjun S. Nair,Bhawna Chaudhary,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.NE

TL;DR: 提出了一种基于CORDIC的Hodgkin Huxley神经元模型（RCHH）和皮质神经池（CNP）架构，在FPGA上实现了资源高效、高速的脉冲神经网络，适用于边缘AI应用。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的边缘AI应用开发生物学准确、低资源的脉冲神经网络实现，解决现有设计在资源利用和速度方面的不足。

Method: 使用模块化和性能优化的CORDIC阶段构建RCHH神经元模型，采用延迟-面积权衡策略，并设计皮质神经池架构。

Result: RCHH神经元相比最先进设计减少24.5% LUT、提升35.2%速度，NRMSE改善70%；CNP架构吞吐量比等效CORDIC-DNN引擎高2.85倍（12.69 GOPS），在MNIST数据集上仅损失0.35%准确率。

Conclusion: 该设计展示了生物学准确、低资源的脉冲神经网络实现，特别适合资源受限的边缘AI应用场景。

Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed,
resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike
shared CORDIC-based DNN approaches, the proposed neuron leverages modular and
performance-optimised CORDIC stages with a latency-area trade-off. The FPGA
implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved
speed, compared to SoTA designs, with 70% better normalised root mean square
error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69
GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only
a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The
overall results indicate that the design shows biologically accurate,
low-resource spiking neural network implementations for resource-constrained
edge AI applications.

</details>


### [340] [A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications](https://arxiv.org/abs/2510.17745)
*Lars Niedermeier,Vyom Shah,Jeffrey L. Krichmar*

Main category: cs.NE

TL;DR: 提出了一种多线程内核，使脉冲神经网络能够在边缘设备上运行，相比单线程处理速度提升4倍，能效提升70%，支持移动设备的多核负载均衡。


<details>
  <summary>Details</summary>
Motivation: 利用脉冲神经网络的稀疏事件驱动特性开发边缘计算应用，摆脱对云服务的依赖，实现低功耗的边缘智能处理。

Method: 设计多线程内核，在多核处理器（如ARM）上实现负载均衡，优化脉冲神经网络在边缘设备上的运行效率。

Result: 在中等规模SNN上速度提升4倍，Synfire网络上提升1.7倍，比静态核心分配能效提高70%，有效利用多核处理能力。

Conclusion: 该内核支持开发低SWaP的边缘应用，并为神经形态芯片集成提供原型方案，推动边缘智能设备发展。

Abstract: Spiking Neural Networks (SNNs) have sparse, event driven processing that can
leverage neuromorphic applications. In this work, we introduce a
multi-threading kernel that enables neuromorphic applications running at the
edge, meaning they process sensory input directly and without any up-link to or
dependency on a cloud service. The kernel shows speed-up gains over single
thread processing by a factor of four on moderately sized SNNs and 1.7X on a
Synfire network. Furthermore, it load-balances all cores available on
multi-core processors, such as ARM, which run today's mobile devices and is up
to 70% more energy efficient compared to statical core assignment. The present
work can enable the development of edge applications that have low Size,
Weight, and Power (SWaP), and can prototype the integration of neuromorphic
chips.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [341] [Ripple Effect Protocol: Coordinating Agent Populations](https://arxiv.org/abs/2510.16572)
*Ayush Chopra,Aman Sharma,Feroz Ahmad,Luca Muscariello,Vijoy Pandey,Ramesh Raskar*

Main category: cs.AI

TL;DR: 提出了Ripple Effect Protocol (REP)，一种协调协议，让智能体不仅分享决策，还分享轻量级敏感度信号，从而在群体中实现更快更稳定的协调。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体通信协议如A2A和ACP强调通信而非协调，随着智能体群体规模扩大，这会导致脆弱的集体行为，即使个体智能体很聪明，群体结果也很差。

Method: REP协议让智能体分享决策和轻量级敏感度信号（表达关键环境变量变化时选择如何改变），这些敏感度在局部网络中传播，实现比仅靠智能体中心通信更快的群体对齐。

Result: 在三个领域的基准测试中：供应链级联（啤酒游戏）、稀疏网络中的偏好聚合（电影调度）和可持续资源分配（Fishbanks），REP相比A2A将协调准确性和效率提高了41%到100%。

Conclusion: 通过将协调作为协议级能力，REP为新兴的智能体互联网提供了可扩展的基础设施。

Abstract: Modern AI agents can exchange messages using protocols such as A2A and ACP,
yet these mechanisms emphasize communication over coordination. As agent
populations grow, this limitation produces brittle collective behavior, where
individually smart agents converge on poor group outcomes. We introduce the
Ripple Effect Protocol (REP), a coordination protocol in which agents share not
only their decisions but also lightweight sensitivities - signals expressing
how their choices would change if key environmental variables shifted. These
sensitivities ripple through local networks, enabling groups to align faster
and more stably than with agent-centric communication alone. We formalize REP's
protocol specification, separating required message schemas from optional
aggregation rules, and evaluate it across scenarios with varying incentives and
network topologies. Benchmarks across three domains: (i) supply chain cascades
(Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling),
and (iii) sustainable resource allocation (Fishbanks) show that REP improves
coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly
handling multimodal sensitivity signals from LLMs. By making coordination a
protocol-level capability, REP provides scalable infrastructure for the
emerging Internet of Agents

</details>


### [342] [Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration](https://arxiv.org/abs/2510.16742)
*Paul Saves,Pramudita Satria Palar,Muhammad Daffa Robani,Nicolas Verstaevel,Moncef Garouani,Julien Aligon,Benoit Gaudou,Koji Shimoyama,Joseph Morlier*

Main category: cs.AI

TL;DR: 该论文提出了一种基于代理模型的工作流程，通过训练轻量级模拟器来解决复杂系统仿真中的计算成本高和透明度不足的问题，支持不确定性量化和可解释AI分析。


<details>
  <summary>Details</summary>
Motivation: 解决仿真驱动工程工作流面临的两个核心障碍：(1)高计算成本，因为准确探索需要大量昂贵的模拟器运行；(2)当决策依赖于不透明的黑盒组件时，透明度和可靠性有限。

Method: 训练轻量级模拟器，使用紧凑的实验设计，提供快速、低延迟的昂贵模拟器近似，支持严格的不确定性量化，并适应全局和局部可解释AI分析。

Result: 在两个对比案例研究中展示：混合电动飞机的多学科设计分析和基于代理的城市隔离模型。结果显示代理模型与XAI耦合能够在几秒内实现大规模探索，发现非线性相互作用和涌现行为，识别关键设计和政策杠杆，并指示代理需要更多数据或替代架构的区域。

Conclusion: 所提出的工作流程统一了所有基于仿真的复杂系统分析工具，从工程设计到社会环境理解，通过代理模型和可解释AI的结合实现了高效、透明的系统探索和分析。

Abstract: Complex systems are increasingly explored through simulation-driven
engineering workflows that combine physics-based and empirical models with
optimization and analytics. Despite their power, these workflows face two
central obstacles: (1) high computational cost, since accurate exploration
requires many expensive simulator runs; and (2) limited transparency and
reliability when decisions rely on opaque blackbox components. We propose a
workflow that addresses both challenges by training lightweight emulators on
compact designs of experiments that (i) provide fast, low-latency
approximations of expensive simulators, (ii) enable rigorous uncertainty
quantification, and (iii) are adapted for global and local Explainable
Artificial Intelligence (XAI) analyses. This workflow unifies every
simulation-based complex-system analysis tool, ranging from engineering design
to agent-based models for socio-environmental understanding. In this paper, we
proposea comparative methodology and practical recommendations for using
surrogate-based explainability tools within the proposed workflow. The
methodology supports continuous and categorical inputs, combines global-effect
and uncertainty analyses with local attribution, and evaluates the consistency
of explanations across surrogate models, thereby diagnosing surrogate adequacy
and guiding further data collection or model refinement. We demonstrate the
approach on two contrasting case studies: a multidisciplinary design analysis
of a hybrid-electric aircraft and an agent-based model of urban segregation.
Results show that the surrogate model and XAI coupling enables large-scale
exploration in seconds, uncovers nonlinear interactions and emergent behaviors,
identifies key design and policy levers, and signals regions where surrogates
require more data or alternative architectures.

</details>


### [343] [Graph Attention-Guided Search for Dense Multi-Agent Pathfinding](https://arxiv.org/abs/2510.17382)
*Rishabh Jain,Keisuke Okumura,Michael Amir,Amanda Prorok*

Main category: cs.AI

TL;DR: 提出LaGAT框架，将基于图注意力的神经MAPF策略MAGAT集成到搜索算法LaCAM中，在密集多智能体路径规划场景中优于纯搜索和纯学习方法。


<details>
  <summary>Details</summary>
Motivation: 在密集多智能体路径规划问题中，现有方法难以在实时条件下找到接近最优解，需要结合学习和搜索的优势。

Method: 改进MAGAT架构，采用预训练-微调策略，结合死锁检测机制，将学习到的启发式集成到LaCAM搜索算法中。

Result: LaGAT在密集场景中表现优于纯搜索和纯学习方法，证明混合搜索对紧密耦合的多智能体协调问题的有效性。

Conclusion: 精心设计的混合搜索方法为解决具有挑战性的多智能体协调问题提供了强大解决方案。

Abstract: Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)
problems in real-time remains challenging even for state-of-the-art planners.
To this end, we develop a hybrid framework that integrates a learned heuristic
derived from MAGAT, a neural MAPF policy with a graph attention scheme, into a
leading search-based algorithm, LaCAM. While prior work has explored
learning-guided search in MAPF, such methods have historically underperformed.
In contrast, our approach, termed LaGAT, outperforms both purely search-based
and purely learning-based methods in dense scenarios. This is achieved through
an enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of
interest, and a deadlock detection scheme to account for imperfect neural
guidance. Our results demonstrate that, when carefully designed, hybrid search
offers a powerful solution for tightly coupled, challenging multi-agent
coordination problems.

</details>


### [344] [Diverse Planning with Simulators via Linear Temporal Logic](https://arxiv.org/abs/2510.17418)
*Mustafa F. Abdelwahed,Alice Toniolo,Joan Espasa,Ian P. Gent*

Main category: cs.AI

TL;DR: 提出了FBI_LTL，一个用于仿真规划问题的多样化规划器，使用线性时序逻辑定义语义多样性标准，生成语义多样化的计划


<details>
  <summary>Details</summary>
Motivation: 传统规划器只生成单一计划，可能无法满足代理偏好；现有多样化规划方法可能产生语法不同但语义相同的解决方案

Method: 使用线性时序逻辑定义语义多样性标准，并将这些LTL多样性模型直接集成到搜索过程中

Result: 在各种基准测试中，FBI_LTL相比基线方法能生成更多样化的计划

Conclusion: 这项工作确立了在仿真环境中语义引导多样化规划的可行性，为在传统基于模型方法失效的现实非符号领域开辟了新途径

Abstract: Autonomous agents rely on automated planning algorithms to achieve their
objectives. Simulation-based planning offers a significant advantage over
declarative models in modelling complex environments. However, relying solely
on a planner that produces a single plan may not be practical, as the generated
plans may not always satisfy the agent's preferences. To address this
limitation, we introduce $\texttt{FBI}_\texttt{LTL}$, a diverse planner
explicitly designed for simulation-based planning problems.
$\texttt{FBI}_\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define
semantic diversity criteria, enabling agents to specify what constitutes
meaningfully different plans. By integrating these LTL-based diversity models
directly into the search process, $\texttt{FBI}_\texttt{LTL}$ ensures the
generation of semantically diverse plans, addressing a critical limitation of
existing diverse planning approaches that may produce syntactically different
but semantically identical solutions. Extensive evaluations on various
benchmarks consistently demonstrate that $\texttt{FBI}_\texttt{LTL}$ generates
more diverse plans compared to a baseline approach. This work establishes the
feasibility of semantically-guided diverse planning in simulation-based
environments, paving the way for innovative approaches in realistic,
non-symbolic domains where traditional model-based approaches fail.

</details>


### [345] [A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.17697)
*Anjie Liu,Jianhong Wang,Samuel Kaski,Jun Wang,Mengyue Yang*

Main category: cs.AI

TL;DR: 本文提出了基于多智能体影响图(MAIDs)的定向干预方法，通过仅对单个目标智能体进行干预来解决大规模多智能体强化学习中全局指导不切实际的问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模多智能体强化学习中，对整个系统进行全局指导不切实际，而现有的协调机制设计多依赖经验研究，缺乏易用的研究工具。

Method: 使用多智能体影响图(MAIDs)作为图形框架，设计了基于前策略干预(PSI)的定向干预范式，仅对单个目标智能体进行干预，通过最大化相应的因果效应来实现复合期望结果。

Result: 实验证明了所提出的定向干预方法的有效性，并验证了相关性图分析的结果。

Conclusion: MAIDs提供了一个有效的框架来分析和可视化多智能体强化学习方法，定向干预范式能够缓解全局指导问题，PSI技术能够有效实现复合期望结果。

Abstract: Steering cooperative multi-agent reinforcement learning (MARL) towards
desired outcomes is challenging, particularly when the global guidance from a
human on the whole multi-agent system is impractical in a large-scale MARL. On
the other hand, designing mechanisms to coordinate agents most relies on
empirical studies, lacking a easy-to-use research tool. In this work, we employ
multi-agent influence diagrams (MAIDs) as a graphical framework to address the
above issues. First, we introduce interaction paradigms that leverage MAIDs to
analyze and visualize existing approaches in MARL. Then, we design a new
interaction paradigm based on MAIDs, referred to as targeted intervention that
is applied to only a single targeted agent, so the problem of global guidance
can be mitigated. In our implementation, we introduce a causal inference
technique-referred to as Pre-Strategy Intervention (PSI)-to realize the
targeted intervention paradigm. Since MAIDs can be regarded as a special class
of causal diagrams, a composite desired outcome that integrates the primary
task goal and an additional desired outcome can be achieved by maximizing the
corresponding causal effect through the PSI. Moreover, the bundled relevance
graph analysis of MAIDs provides a tool to identify whether an MARL learning
paradigm is workable under the design of an interaction paradigm. In
experiments, we demonstrate the effectiveness of our proposed targeted
intervention, and verify the result of relevance graph analysis.

</details>


### [346] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 本文提出了一个科学政策议程，探索城市虚拟世界作为监管学习实验空间的潜力，通过专家咨询确定了关键研究领域和实验主题，强调负责任的发展方法。


<details>
  <summary>Details</summary>
Motivation: 城市虚拟世界具有通过沉浸式虚拟环境支持监管学习的潜力，为政策情景和技术实验提供平台。

Method: 基于与高级专家小组（包括欧盟委员会政策制定者、国家政府科学顾问和数字监管领域领先研究人员）的咨询，确定关键研究领域和实验主题。

Result: 确定了可扩展性、实时反馈、复杂性建模、跨境合作、风险降低、公民参与、伦理考量和新兴技术整合等关键研究领域，并分析了交通、城市规划、环境/气候危机等实验主题。

Conclusion: 城市虚拟世界有潜力成为监管学习的重要实验空间，但需要负责任的发展方法，充分考虑伦理、经济、生态和社会维度，并与其他实验空间生态系统整合。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [347] [The Burden of Interactive Alignment with Inconsistent Preferences](https://arxiv.org/abs/2510.16368)
*Ali Shirali*

Main category: cs.AI

TL;DR: 该论文研究了用户如何通过战略性地与算法互动来引导算法更好地符合其真实兴趣，提出了一个双系统决策模型和Stackelberg博弈框架，定义了"对齐负担"概念，并发现关键时间跨度的存在以及小成本信号对降低对齐负担的作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于用户在算法互动中经常表现出不一致的偏好——他们可能花费大量时间在低价值内容上，无意中向算法传递错误信号。这引发了一个关键问题：这类用户需要什么条件才能让算法与其真实兴趣对齐？

Method: 方法包括：1）将用户决策过程建模为理性系统2（决定是否参与）和冲动系统1（决定参与时长）的双系统模型；2）建立多领导者-单跟随者的扩展Stackelberg博弈框架，用户作为领导者承诺参与策略，算法基于观察到的互动做出最佳响应；3）定义"对齐负担"作为用户有效引导算法所需的最小优化时间跨度。

Result: 研究结果表明：1）存在一个关键时间跨度——足够有远见的用户可以实现对齐，而缺乏远见的用户反而会被算法目标对齐；2）这个关键时间跨度可能很长，带来显著的对齐负担；3）即使是一个小的成本信号（如额外点击）也能显著降低对齐负担。

Conclusion: 结论是：该框架解释了具有不一致偏好的用户如何在Stackelberg均衡中实现与参与驱动算法的对齐，既揭示了实现对齐的挑战，也指出了潜在的补救措施，特别是小成本信号在降低对齐负担方面的有效性。

Abstract: From media platforms to chatbots, algorithms shape how people interact,
learn, and discover information. Such interactions between users and an
algorithm often unfold over multiple steps, during which strategic users can
guide the algorithm to better align with their true interests by selectively
engaging with content. However, users frequently exhibit inconsistent
preferences: they may spend considerable time on content that offers little
long-term value, inadvertently signaling that such content is desirable.
Focusing on the user side, this raises a key question: what does it take for
such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split
between a rational system 2 that decides whether to engage and an impulsive
system 1 that determines how long engagement lasts. We then study a
multi-leader, single-follower extensive Stackelberg game, where users,
specifically system 2, lead by committing to engagement strategies and the
algorithm best-responds based on observed interactions. We define the burden of
alignment as the minimum horizon over which users must optimize to effectively
steer the algorithm. We show that a critical horizon exists: users who are
sufficiently foresighted can achieve alignment, while those who are not are
instead aligned to the algorithm's objective. This critical horizon can be
long, imposing a substantial burden. However, even a small, costly signal
(e.g., an extra click) can significantly reduce it. Overall, our framework
explains how users with inconsistent preferences can align an engagement-driven
algorithm with their interests in a Stackelberg equilibrium, highlighting both
the challenges and potential remedies for achieving alignment.

</details>


### [348] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态交互模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人类交互行为。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要同时感知和生成多种模态信息。现有模型难以实现这种自然的交互模式。

Method: 提出SA-MoE架构，将每个模态路由到专门的专家模块，通过统一的注意力骨干网络进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA达到了特定模态基线的性能，同时支持高级多模态和全双工行为，如对话轮转、动作轮转、错误指令拒绝等。

Conclusion: ELLSA代表了向更自然和通用交互智能迈出的一步，有助于实现更广泛的人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [349] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了SELECT框架，通过动态锚点选择解决文本到图像扩散模型中概念擦除的锚点敏感性问题，避免概念重现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法依赖固定锚点策略，导致概念重现和侵蚀等关键问题，需要更智能的锚点选择机制。

Method: 基于因果追踪分析锚点敏感性，定义兄弟排他概念作为优质锚点类别，提出两阶段评估机制自动发现最优锚点并识别边界锚点。

Result: SELECT作为通用锚点解决方案，能高效适配多种擦除框架，在关键性能指标上持续优于现有基线，单个概念锚点挖掘仅需4秒。

Conclusion: 动态锚点选择框架SELECT有效解决了固定锚点的局限性，提升了概念擦除的精确性和效率。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [350] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个用于多模态虚假信息检测的推理时、可插拔代理框架，通过分解验证过程为四个模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断，在MMFakeBench上达到81.65% F1分数，优于最强零样本基线7.65个百分点。


<details>
  <summary>Details</summary>
Motivation: 网络平台上每天有数十亿结合文本和图像的多模态帖子传播虚假信息，超出了人工事实核查的能力范围。现有的监督检测模型需要特定领域训练数据，且无法泛化到不同的操纵策略。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：检测AI生成图像的视觉真实性评估、识别上下文错误使用的跨模态一致性分析、通过迭代问题生成基于网络证据进行事实核查的检索增强模块，以及整合所有信号的校准判断模块。该框架协调视觉语言模型推理与定向网络检索，输出结构化且带引用的推理过程。

Result: 在MMFakeBench验证集（1,000样本）上，MIRAGE与GPT-4o-mini组合达到81.65% F1分数和75.1%准确率，优于最强的零样本基线（GPT-4V与MMD-Agent的74.0% F1）7.65个百分点，同时保持34.3%的假阳性率，而仅使用判断的基线为97.3%。测试集结果（5,000样本）确认了泛化能力，达到81.44% F1和75.08%准确率。消融研究表明视觉验证贡献5.18 F1点，检索增强推理贡献2.97点。

Conclusion: 分解的代理推理与网络检索相结合，可以在没有特定领域训练的情况下匹配监督检测器的性能，使得在标注数据稀缺的多模态领域中也能进行虚假信息检测。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [351] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 该研究发现视觉语言模型(VLMs)存在"看到但不相信"现象，即模型感知到了正确视觉证据但仍输出错误答案。通过层间注意力分析发现深层网络能可靠定位证据区域，但未能有效利用。作者提出无需训练的关注掩码干预方法，在多个VLM家族中一致提升准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在多模态任务上表现良好，但即使存在正确视觉证据时仍会失败。本研究旨在系统性地探究这些失败是由于未感知证据还是未有效利用证据。

Method: 通过分析层间注意力动态，发现浅层主要关注文本，深层能稀疏但可靠地关注局部证据区域。提出推理时干预方法，通过选择性基于注意力的掩码来突出深层证据区域。

Result: 发现VLMs在输出错误答案时通常已感知到视觉证据，这种现象广泛存在于主要VLM家族中。提出的干预方法无需训练，在LLaVA、Qwen、Gemma和InternVL等多个模型上一致提高了准确性。

Conclusion: VLMs内部编码了可靠的证据但未能充分利用，使这些信号显式化可以弥合感知与推理之间的差距，推进对VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>
