{"id": "2506.09098", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09098", "abs": "https://arxiv.org/abs/2506.09098", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "comment": "https://youtu.be/AQAgVdrx1DE", "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "AI": {"tldr": "论文提出了一种基于小波去噪的检测变换器（WD-DETR）网络，用于解决事件相机中密集事件表示的噪声问题，提升检测性能并实现实时处理。", "motivation": "密集事件表示中的累积噪声会降低表示质量并增加漏检概率，现有研究对此关注不足。", "method": "提出密集事件表示并设计小波变换去噪方法，将其集成到特征提取主干网络中，随后通过变换器网络进行目标预测，并引入动态重组卷积块（DRCB）以减少推理时间。", "result": "在DSEC、Gen1和1Mpx数据集上，WD-DETR优于现有方法，并在NVIDIA Jetson Orin NX上实现约35 FPS的高帧率。", "conclusion": "WD-DETR有效解决了事件相机中的噪声问题，提升了检测性能，并适用于实时机器人感知。"}}
{"id": "2506.09169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09169", "abs": "https://arxiv.org/abs/2506.09169", "authors": ["Yuemin Mao", "Bardienus P. Duisterhof", "Moonyoung Lee", "Jeffrey Ichnowski"], "title": "Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport", "comment": null, "summary": "Object transport tasks are fundamental in robotic automation, emphasizing the\nimportance of efficient and secure methods for moving objects. Non-prehensile\ntransport can significantly improve transport efficiency, as it enables\nhandling multiple objects simultaneously and accommodating objects unsuitable\nfor parallel-jaw or suction grasps. Existing approaches incorporate constraints\nbased on the Coulomb friction model, which is imprecise during fast motions\nwhere inherent mechanical vibrations occur. Imprecise constraints can cause\ntransported objects to slide or even fall off the tray. To address this\nlimitation, we propose a novel method to learn a friction model using acoustic\nsensing that maps a tray's motion profile to a dynamically conditioned friction\ncoefficient. This learned model enables an optimization-based motion planner to\nadjust the friction constraint at each control step according to the planned\nmotion at that step. In experiments, we generate time-optimized trajectories\nfor a UR5e robot to transport various objects with constraints using both the\nstandard Coulomb friction model and the learned friction model. Results suggest\nthat the learned friction model reduces object displacement by up to 86.0%\ncompared to the baseline, highlighting the effectiveness of acoustic sensing in\nlearning real-world friction constraints.", "AI": {"tldr": "论文提出了一种通过声学传感学习摩擦模型的方法，优化非抓取式物体运输任务，显著减少物体位移。", "motivation": "现有基于库仑摩擦模型的约束在快速运动时不够精确，导致物体滑动或掉落，需要更准确的动态摩擦模型。", "method": "利用声学传感学习动态摩擦系数，结合优化运动规划器调整每一步的摩擦约束。", "result": "实验显示，学习模型比标准库仑模型减少物体位移达86.0%。", "conclusion": "声学传感能有效学习真实摩擦约束，提升非抓取式运输的效率和安全性。"}}
{"id": "2506.09182", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.09182", "abs": "https://arxiv.org/abs/2506.09182", "authors": ["Hang Zhou", "Chengyuan Ma", "Shiyu Shen", "Xiaopeng Li"], "title": "Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method", "comment": "NA", "summary": "With the rapid development of automated vehicles (AVs) in recent years,\ncommercially available AVs are increasingly demonstrating high-level automation\ncapabilities. However, most existing AV safety evaluation methods are primarily\ndesigned for simple maneuvers such as car-following and lane-changing. While\nsuitable for basic tests, these methods are insufficient for assessing\nhigh-level automation functions deployed in more complex environments. First,\nthese methods typically use crash rate as the evaluation metric, whose accuracy\nheavily depends on the quality and completeness of naturalistic driving\nenvironment data used to estimate scenario probabilities. Such data is often\ndifficult and expensive to collect. Second, when applied to diverse scenarios,\nthese methods suffer from the curse of dimensionality, making large-scale\nevaluation computationally intractable. To address these challenges, this paper\nproposes a novel framework for full-scenario AV safety evaluation. A unified\nmodel is first introduced to standardize the representation of diverse driving\nscenarios. This modeling approach constrains the dimension of most scenarios to\na regular highway setting with three lanes and six surrounding background\nvehicles, significantly reducing dimensionality. To further avoid the\nlimitations of probability-based method, we propose a volume-based evaluation\nmethod that quantifies the proportion of risky scenarios within the entire\nscenario space. For car-following scenarios, we prove that the set of safe\nscenarios is convex under specific settings, enabling exact volume computation.\nExperimental results validate the effectiveness of the proposed volume-based\nmethod using both AV behavior models from existing literature and six\nproduction AV models calibrated from field-test trajectory data in the Ultra-AV\ndataset. Code and data will be made publicly available upon acceptance of this\npaper.", "AI": {"tldr": "本文提出了一种新的全场景自动驾驶车辆（AV）安全评估框架，解决了现有方法在复杂环境中评估高自动化功能的不足。", "motivation": "现有AV安全评估方法主要针对简单操作（如跟车和变道），难以适用于复杂环境中的高自动化功能评估，且依赖高质量自然驾驶数据，计算复杂度高。", "method": "提出统一模型标准化驾驶场景表示，将场景维度限制为常规高速公路设置；引入基于体积的评估方法，量化风险场景比例。", "result": "实验验证了基于体积的方法的有效性，使用文献中的AV行为模型和实际测试数据校准的AV模型。", "conclusion": "新框架显著降低了评估维度，避免了概率方法的局限性，为AV安全评估提供了更高效的方法。"}}
{"id": "2506.09204", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09204", "abs": "https://arxiv.org/abs/2506.09204", "authors": ["Xiaotian Chen", "Hongyun Liu", "Seyed Sahand Mohammadi Ziabari"], "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project", "comment": null, "summary": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective\nand have been applied across diverse domains within deep learning. However, as\nDNN models increase in complexity, the demand for reduced computational costs\nand memory overheads has become increasingly urgent. Sparsity has emerged as a\nleading approach in this area. The robustness of sparse Multi-layer Perceptrons\n(MLPs) for supervised feature selection, along with the application of Sparse\nEvolutionary Training (SET), illustrates the feasibility of reducing\ncomputational costs without compromising accuracy. Moreover, it is believed\nthat the SET algorithm can still be improved through a structural optimization\nmethod called motif-based optimization, with potential efficiency gains\nexceeding 40% and a performance decline of under 4%. This research investigates\nwhether the structural optimization of Sparse Evolutionary Training applied to\nMulti-layer Perceptrons (SET-MLP) can enhance performance and to what extent\nthis improvement can be achieved.", "AI": {"tldr": "研究探讨了稀疏多层感知机（SET-MLP）的结构优化是否能提升性能及其可能的改进程度。", "motivation": "随着DNN模型复杂度增加，降低计算成本和内存开销的需求日益迫切，稀疏性成为一种重要方法。", "method": "采用稀疏进化训练（SET）和基于基序的优化方法对稀疏多层感知机进行结构优化。", "result": "潜在效率提升超过40%，性能下降低于4%。", "conclusion": "结构优化可以显著提升稀疏进化训练的性能，同时保持准确性。"}}
{"id": "2506.09217", "categories": ["cs.RO", "cs.CV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.09217", "abs": "https://arxiv.org/abs/2506.09217", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "comment": null, "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "AI": {"tldr": "论文提出了一种新的感知评估指标PCD，用于量化物体可靠检测的最远距离，并结合模型输出的不确定性。同时发布了SensorRainFall数据集，用于评估不同天气条件下的感知性能。", "motivation": "传统感知评估指标是静态的，无法捕捉模型输出置信度的动态变化，尤其是在不同距离和天气条件下。", "method": "提出了PCD指标，结合SensorRainFall数据集（包含晴天和雨天场景的传感器数据），通过统计分析检测置信度方差的变化点。", "result": "PCD能够有效捕捉不同天气条件下感知系统的可靠性差异，而传统静态指标无法做到。", "conclusion": "PCD为感知性能提供了分布感知的评估方法，支持更安全的自动驾驶系统运行；SensorRainFall数据集为评估提供了基准。"}}
{"id": "2506.09599", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.09599", "abs": "https://arxiv.org/abs/2506.09599", "authors": ["Enrique Barba Roque", "Luis Cruz"], "title": "Energy Aware Development of Neuromorphic Implantables: From Metrics to Action", "comment": "ICT45 2025 submission", "summary": "Spiking Neural Networks (SNNs) and neuromorphic computing present a promising\nalternative to traditional Artificial Neural Networks (ANNs) by significantly\nimproving energy efficiency, particularly in edge and implantable devices.\nHowever, assessing the energy performance of SNN models remains a challenge due\nto the lack of standardized and actionable metrics and the difficulty of\nmeasuring energy consumption in experimental neuromorphic hardware. In this\npaper, we conduct a preliminary exploratory study of energy efficiency metrics\nproposed in the SNN benchmarking literature. We classify 13 commonly used\nmetrics based on four key properties: Accessibility, Fidelity, Actionability,\nand Trend-Based analysis. Our findings indicate that while many existing\nmetrics provide useful comparisons between architectures, they often lack\npractical insights for SNN developers. Notably, we identify a gap between\naccessible and high-fidelity metrics, limiting early-stage energy assessment.\nAdditionally, we emphasize the lack of metrics that provide practitioners with\nactionable insights, making it difficult to guide energy-efficient SNN\ndevelopment. To address these challenges, we outline research directions for\nbridging accessibility and fidelity and finding new Actionable metrics for\nimplantable neuromorphic devices, introducing more Trend-Based metrics, metrics\nthat reflect changes in power requirements, battery-aware metrics, and\nimproving energy-performance tradeoff assessments. The results from this paper\npave the way for future research on enhancing energy metrics and their\nActionability for SNNs.", "AI": {"tldr": "本文探讨了评估脉冲神经网络（SNN）能量效率的挑战，分类了13种常用指标，并指出了现有指标的局限性，提出了未来研究方向。", "motivation": "SNN和神经形态计算在边缘和植入设备中具有高能效潜力，但缺乏标准化和可操作的能源评估指标。", "method": "对SNN文献中的能源效率指标进行初步探索性研究，基于四个关键属性（可访问性、保真度、可操作性和趋势分析）分类13种常用指标。", "result": "发现现有指标虽能比较架构，但缺乏对开发者的实际指导；指出可访问性与高保真度指标之间的差距，以及可操作性指标的不足。", "conclusion": "提出未来研究方向，包括提升指标的可访问性与保真度、开发可操作指标、趋势分析指标等，为SNN能源评估提供更实用的工具。"}}
{"id": "2506.09288", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2506.09288", "abs": "https://arxiv.org/abs/2506.09288", "authors": ["Alireza Kaviani", "Alireza Keshavarz", "Masoud Seddighin", "AmirMohammad Shahrezaei"], "title": "Improved Approximate EFX Guarantees for Multigraphs", "comment": null, "summary": "In recent years, a new line of work in fair allocation has focused on EFX\nallocations for \\((p, q)\\)-bounded valuations, where each good is relevant to\nat most \\(p\\) agents, and any pair of agents share at most \\(q\\) relevant\ngoods. For the case \\(p = 2\\) and \\(q = \\infty\\), such instances can be\nequivalently represented as multigraphs whose vertices are the agents and whose\nedges represent goods, each edge incident to exactly the one or two agents for\nwhom the good is relevant. A recent result of \\citet{amanatidis2024pushing}\nshows that for additive $(2,\\infty)$ bounded valuations, a\n\\((\\nicefrac{2}{3})\\)-EFX allocation always exists. In this paper, we improve\nthis bound by proving the existence of a \\((\\nicefrac{1}{\\sqrt{2}})\\)-\\(\\efx\\)\nallocation for additive \\((2,\\infty)\\)-bounded valuations.", "AI": {"tldr": "本文改进了在加性(2,∞)-有界估值下EFX分配的存在性，证明了(1/√2)-EFX分配的存在性。", "motivation": "近年来，公平分配领域的研究集中在EFX分配上，特别是在(p,q)-有界估值下。本文旨在改进现有结果，证明更强的分配存在性。", "method": "通过多图表示代理人和商品的关系，研究加性(2,∞)-有界估值下的EFX分配。", "result": "证明了在加性(2,∞)-有界估值下，存在(1/√2)-EFX分配，改进了之前的(2/3)-EFX结果。", "conclusion": "本文为EFX分配提供了更强的理论保证，扩展了公平分配的研究范围。"}}
{"id": "2506.09070", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09070", "abs": "https://arxiv.org/abs/2506.09070", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "AI": {"tldr": "STREAMINGGS通过算法-架构协同设计，优化3D高斯泼溅（3DGS）的内存效率，显著提升移动设备上的实时渲染性能。", "motivation": "3DGS在移动设备上难以达到90 FPS的实时要求，现有加速器忽视内存效率，导致DRAM流量冗余。", "method": "提出STREAMINGGS，采用完全流式设计，从基于瓦片的渲染转变为基于内存的渲染，实现细粒度流水线并减少DRAM流量。", "result": "设计在移动Ampere GPU上实现了45.7倍加速和62.9倍能耗节省。", "conclusion": "STREAMINGGS通过优化内存效率，显著提升了3DGS在移动设备上的实时性能。"}}
{"id": "2506.09089", "categories": ["cs.HC", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.09089", "abs": "https://arxiv.org/abs/2506.09089", "authors": ["Xia Li"], "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "comment": "in French language", "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "AI": {"tldr": "论文探讨了在大学对外汉语口语教学中，教师如何利用ChatGPT设计基于冲突的交际任务，以提升学习者的口语互动能力，并分析了教师与ChatGPT的互动特点及其影响。", "motivation": "研究旨在探索ChatGPT在对外汉语口语教学任务设计中的应用，以及其对教学过程的潜在影响。", "method": "教师设计基于冲突的交际任务，并利用ChatGPT辅助完成教学计划，分析其互动特点。", "result": "研究发现ChatGPT在任务设计中提供了有效支持，促进了教师与AI的互动，提升了教学效率。", "conclusion": "ChatGPT在对外汉语口语教学中具有潜力，可作为辅助工具优化任务设计，但需进一步研究其长期影响。"}}
{"id": "2506.09335", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09335", "abs": "https://arxiv.org/abs/2506.09335", "authors": ["Moshi Wei", "Sparks Li"], "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds", "comment": "11 pages, 1 figures,", "summary": "The Intelligent System of Emergent Knowledge (ISEK) establishes a\ndecentralized network where human and artificial intelligence agents\ncollaborate as peers, forming a self-organizing cognitive ecosystem. Built on\nWeb3 infrastructure, ISEK combines three fundamental principles: (1) a\ndecentralized multi-agent architecture resistant to censorship, (2) symbiotic\nAI-human collaboration with equal participation rights, and (3) resilient\nself-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a\nsix-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for\ndynamic task allocation, supported by robust fault tolerance and a\nmultidimensional reputation system. Economic incentives are governed by the\nnative $ISEK token, facilitating micropayments, governance participation, and\nreputation tracking, while agent sovereignty is maintained through NFT-based\nidentity management.\n  This synthesis of blockchain technology, artificial intelligence, and\nincentive engineering creates an infrastructure that actively facilitates\nemergent intelligence. ISEK represents a paradigm shift from conventional\nplatforms, enabling the organic development of large-scale, decentralized\ncognitive systems where autonomous agents collectively evolve beyond\ncentralized constraints.", "AI": {"tldr": "ISEK是一个基于Web3的去中心化网络，结合人类与AI代理的协作，通过六阶段工作流程和代币经济激励，实现自组织和自适应。", "motivation": "旨在打破传统平台的集中式限制，促进大规模去中心化认知系统的有机发展。", "method": "采用多代理架构、六阶段工作流程（发布、发现、招募、执行、结算、反馈）、代币经济激励和NFT身份管理。", "result": "构建了一个支持涌现智能的基础设施，实现代理自主协作与进化。", "conclusion": "ISEK代表了从集中式平台向去中心化认知系统的范式转变。"}}
{"id": "2506.09180", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09180", "abs": "https://arxiv.org/abs/2506.09180", "authors": ["Khai Doan", "Wesley Araujo", "Evangelos Kranakis", "Ioannis Lambadaris", "Yannis Viniotis", "Wonjae Shin"], "title": "Optimal Task Offloading with Firm Deadlines for Mobile Edge Computing Systems", "comment": null, "summary": "Under a dramatic increase in mobile data traffic, a promising solution for\nedge computing systems to maintain their local service is the task migration\nthat may be implemented by means of Autonomous mobile agents (AMA). In\ndesigning an optimal scheme for task offloading to AMA, we define a system cost\nas a minimization objective function that comprises two parts. First, an\noffloading cost which can be interpreted as the cost of using computational\nresources from the AMA. Second, a penalty cost due to potential task\nexpiration. To minimize the expected (timeaverage) cost over a given time\nhorizon, we formulate a Dynamic programming (DP). However, the DP Equation\nsuffers from the well-known curse of dimensionality, which makes computations\nintractable, especially for infinite system state space. To reduce the\ncomputational burden, we identify three important properties of the optimal\npolicy and show that it suffices to evaluate the DP Equation on a finite subset\nof the state space only. We then prove that the optimal task offloading\ndecision at a state can be inferred from that at its adjacent states, further\nreducing the computational load. We present simulations to verify the\ntheoretical results and to provide insights into the considered system.", "AI": {"tldr": "论文提出了一种基于自主移动代理（AMA）的任务迁移方案，通过动态规划（DP）优化卸载成本，并利用有限状态空间和相邻状态推断降低计算负担。", "motivation": "移动数据流量激增，边缘计算系统需通过任务迁移维持本地服务，设计最优任务卸载方案以最小化系统成本。", "method": "定义包含卸载成本和任务过期惩罚的系统成本函数，采用动态规划优化，并通过有限状态空间和相邻状态推断降低计算复杂度。", "result": "理论分析表明最优策略具有三个重要性质，仿真验证了方案的有效性。", "conclusion": "提出的方法能显著降低计算负担，同时保持任务卸载的高效性。"}}
{"id": "2506.09189", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09189", "abs": "https://arxiv.org/abs/2506.09189", "authors": ["Esteban Gutiérrez", "Rodrigo Cádiz", "Carlos Sing Long", "Frederic Font", "Xavier Serra"], "title": "Fractional Fourier Sound Synthesis", "comment": "Accepted to the International Computer Music Conference (ICMC) 2025\n  held in Boston, USA. 6 pages and 2 figures", "summary": "This paper explores the innovative application of the Fractional Fourier\nTransform (FrFT) in sound synthesis, highlighting its potential to redefine\ntime-frequency analysis in audio processing. As an extension of the classical\nFourier Transform, the FrFT introduces fractional order parameters, enabling a\ncontinuous interpolation between time and frequency domains and unlocking\nunprecedented flexibility in signal manipulation. Crucially, the FrFT also\nopens the possibility of directly synthesizing sounds in the alpha-domain,\nproviding a unique framework for creating timbral and dynamic characteristics\nunattainable through conventional methods. This work delves into the\nmathematical principles of the FrFT, its historical evolution, and its\ncapabilities for synthesizing complex audio textures. Through experimental\nanalyses, we showcase novel sound design techniques, such as alpha-synthesis\nand alpha-filtering, which leverage the FrFT's time-frequency rotation\nproperties to produce innovative sonic results. The findings affirm the FrFT's\nvalue as a transformative tool for composers, sound designers, and researchers\nseeking to push the boundaries of auditory creativity.", "AI": {"tldr": "本文探讨了分数阶傅里叶变换（FrFT）在声音合成中的创新应用，展示了其在音频处理中重新定义时频分析的潜力。", "motivation": "传统傅里叶变换在时频分析中存在局限性，FrFT通过引入分数阶参数，提供了更灵活的时频域插值方法，为声音合成开辟了新途径。", "method": "研究了FrFT的数学原理和历史演变，并提出了基于FrFT的声音设计技术，如alpha合成和alpha滤波。", "result": "实验分析表明，FrFT能够生成传统方法无法实现的复杂音频纹理和动态特性。", "conclusion": "FrFT为作曲家和声音设计师提供了一种突破听觉创意边界的变革性工具。"}}
{"id": "2506.09052", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.09052", "abs": "https://arxiv.org/abs/2506.09052", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "comment": "7 Pages", "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "AI": {"tldr": "论文提出了一种基于Llama 3的抗体-抗原结合亲和力预测模型（LlamaAffinity），在多项评估指标上优于现有方法，并显著提高了计算效率。", "motivation": "传统实验方法测量抗体亲和力耗时且昂贵，人工智能尤其是大语言模型（LLMs）为抗体设计和亲和力预测提供了新途径。", "method": "利用开源Llama 3框架和Observed Antibody Space（OAS）数据库的抗体序列数据，开发了LlamaAffinity模型。", "result": "模型在准确率（0.9640）、F1分数（0.9643）、精确率（0.9702）、召回率（0.9586）和AUC-ROC（0.9936）上表现优异，且训练时间显著缩短（0.46小时）。", "conclusion": "LlamaAffinity在抗体亲和力预测中表现出色，为AI驱动的抗体设计提供了高效工具。"}}
{"id": "2506.09066", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet提出了一种可重用和可拼接的网络，通过动态拼接两个预训练模型来适应异构设备资源，实现灵活的效率-精度权衡。", "motivation": "解决预训练模型在异构设备上部署的挑战，传统压缩方法无法适应动态资源约束。", "method": "通过CKA计算层间相似度选择拼接点，保留大模型早期层和小模型深层，仅微调拼接层。", "result": "在多个基准测试中实现灵活的效率-精度权衡，显著降低训练成本。", "conclusion": "ReStNet为异构设备部署提供了一种高效灵活的解决方案。"}}
{"id": "2506.09284", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09284", "abs": "https://arxiv.org/abs/2506.09284", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "comment": null, "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "AI": {"tldr": "UAD是一种无监督方法，通过从基础模型中提取知识，无需手动标注即可训练任务条件化的视觉功能模型，并在机器人场景中表现出良好的泛化能力。", "motivation": "现有视觉功能预测方法依赖手动标注或预定义任务集，限制了其在开放任务指令下的应用。", "method": "UAD利用大型视觉模型和视觉语言模型的互补优势，自动标注大规模数据集，并训练轻量级任务条件化解码器。", "result": "UAD在仿真环境中训练，但在真实机器人场景和人类活动中表现出泛化能力，模仿学习策略在少量演示后能泛化到新对象和任务。", "conclusion": "UAD为无监督视觉功能学习提供了有效方法，展示了在开放任务指令下的潜力。"}}
{"id": "2506.09404", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.09404", "abs": "https://arxiv.org/abs/2506.09404", "authors": ["Shengda Gu", "Kai Li", "Junliang Xing", "Yifan Zhang", "Jian Cheng"], "title": "Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization", "comment": null, "summary": "Combinatorial optimization problems are notoriously challenging due to their\ndiscrete structure and exponentially large solution space. Recent advances in\ndeep reinforcement learning (DRL) have enabled the learning heuristics directly\nfrom data. However, DRL methods often suffer from limited exploration and\nsusceptibility to local optima. On the other hand, evolutionary algorithms such\nas Genetic Algorithms (GAs) exhibit strong global exploration capabilities but\nare typically sample inefficient and computationally intensive. In this work,\nwe propose the Evolutionary Augmentation Mechanism (EAM), a general and\nplug-and-play framework that synergizes the learning efficiency of DRL with the\nglobal search power of GAs. EAM operates by generating solutions from a learned\npolicy and refining them through domain-specific genetic operations such as\ncrossover and mutation. These evolved solutions are then selectively reinjected\ninto the policy training loop, thereby enhancing exploration and accelerating\nconvergence. We further provide a theoretical analysis that establishes an\nupper bound on the KL divergence between the evolved solution distribution and\nthe policy distribution, ensuring stable and effective policy updates. EAM is\nmodel-agnostic and can be seamlessly integrated with state-of-the-art DRL\nsolvers such as the Attention Model, POMO, and SymNCO. Extensive results on\nbenchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM\nsignificantly improves both solution quality and training efficiency over\ncompetitive baselines.", "AI": {"tldr": "提出了一种结合深度强化学习（DRL）和遗传算法（GA）的框架EAM，通过生成和优化解决方案来提升探索能力和训练效率。", "motivation": "解决DRL方法探索能力有限和GA样本效率低的问题，结合两者的优势。", "method": "EAM框架通过DRL生成解决方案，利用GA的遗传操作优化，并将优化后的方案重新注入训练循环。", "result": "在多个基准问题上显著提升了解决方案质量和训练效率。", "conclusion": "EAM是一种通用且高效的框架，能够稳定提升DRL和GA的性能。"}}
{"id": "2506.09291", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2506.09291", "abs": "https://arxiv.org/abs/2506.09291", "authors": ["Hedyeh Beyhaghi", "Linda Cai", "Yiding Feng", "Yingkai Li", "S. Matthew Weinberg"], "title": "Competition Complexity in Multi-Item Auctions: Beyond VCG and Regularity", "comment": null, "summary": "We quantify the value of the monopoly's bargaining power in terms of\ncompetition complexity--that is, the number of additional bidders the monopoly\nmust attract in simple auctions to match the expected revenue of the optimal\nmechanisms (c.f., Bulow and Klemperer, 1996, Eden et al., 2017)--within the\nsetting of multi-item auctions. We show that for simple auctions that sell\nitems separately, the competition complexity is $\\Theta(\\frac{n}{\\alpha})$ in\nan environment with $n$ original bidders under the slightly stronger assumption\nof $\\alpha$-strong regularity, in contrast to the standard regularity\nassumption in the literature, which requires $\\Omega(n \\cdot \\ln \\frac{m}{n})$\nadditional bidders (Feldman et al., 2018). This significantly reduces the value\nof learning the distribution to design the optimal mechanisms, especially in\nlarge markets with many items for sale. For simple auctions that sell items as\na grand bundle, we establish a constant competition complexity bound in a\nsingle-bidder environment when the number of items is small or when the value\ndistribution has a monotone hazard rate. Some of our competition complexity\nresults also hold when we compete against the first best benchmark (i.e.,\noptimal social welfare).", "AI": {"tldr": "论文研究了垄断者在多物品拍卖中的议价能力，通过竞争复杂性量化其价值，发现简单拍卖中吸引额外竞标者的数量可以显著减少最优机制设计的复杂性。", "motivation": "探讨垄断者在多物品拍卖中的议价能力，并量化其价值，以简化最优机制设计的复杂性。", "method": "在多物品拍卖环境中，分析简单拍卖（单独销售或捆绑销售）所需的额外竞标者数量，并与最优机制进行比较。", "result": "在强正则性假设下，单独销售物品的简单拍卖仅需Θ(n/α)额外竞标者，而捆绑销售在小物品数量或单调风险率分布下仅需常数额外竞标者。", "conclusion": "研究结果表明，简单拍卖在某些条件下可以显著减少最优机制设计的复杂性，尤其是在大规模市场中。"}}
{"id": "2506.09075", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09075", "abs": "https://arxiv.org/abs/2506.09075", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "AI": {"tldr": "论文提出了一种基于Transformer的简单框架，用于运动插值任务，强调数据建模选择对性能的重要性。", "motivation": "现有运动插值方法依赖复杂模型，本文旨在探索简单模型通过数据优化实现高质量动画的可能性。", "method": "使用单一Transformer编码器，重点优化数据建模（如数据量、姿态表示和速度特征）。", "result": "实验表明，数据优化能提升运动过渡质量，挑战了模型复杂度决定动画质量的假设。", "conclusion": "数据为中心的优化策略可替代复杂模型，为运动插值提供新思路。"}}
{"id": "2506.09153", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09153", "abs": "https://arxiv.org/abs/2506.09153", "authors": ["Tanjil Hasan Sakib", "Samia Jahan Mojumder", "Rajan Das Gupta", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "title": "Real-Time Confidence Detection through Facial Expressions and Hand Gestures", "comment": "Accepted in MECON 2025", "summary": "Real-time face orientation recognition is a cutting-edge technology meant to\ntrack and analyze facial movements in virtual environments such as online\ninterviews, remote meetings, and virtual classrooms. As the demand for virtual\ninteractions grows, it becomes increasingly important to measure participant\nengagement, attention, and overall interaction. This research presents a novel\nsolution that leverages the Media Pipe Face Mesh framework to identify facial\nlandmarks and extract geometric data for calculating Euler angles, which\ndetermine head orientation in real time. The system tracks 3D facial landmarks\nand uses this data to compute head movements with a focus on accuracy and\nresponsiveness. By studying Euler angles, the system can identify a user's head\norientation with an accuracy of 90\\%, even at a distance of up to four feet.\nThis capability offers significant enhancements for monitoring user\ninteraction, allowing for more immersive and interactive virtual ex-periences.\nThe proposed method shows its reliability in evaluating participant\nattentiveness during online assessments and meetings. Its application goes\nbeyond engagement analysis, potentially providing a means for improving the\nquality of virtual communication, fostering better understanding between\nparticipants, and ensuring a higher level of interaction in digital spaces.\nThis study offers a basis for future developments in enhancing virtual user\nexperiences by integrating real-time facial tracking technologies, paving the\nway for more adaptive and interactive web-based platform.", "AI": {"tldr": "论文提出了一种基于Media Pipe Face Mesh框架的实时人脸朝向识别方法，用于虚拟环境中用户互动的监测与分析。", "motivation": "随着虚拟互动的需求增长，实时监测用户参与度和注意力变得至关重要。", "method": "利用Media Pipe Face Mesh识别面部关键点，提取几何数据计算欧拉角，实时确定头部朝向。", "result": "系统在四英尺距离内识别头部朝向的准确率达90%，显著提升了虚拟互动体验。", "conclusion": "该方法为未来虚拟用户体验的改进提供了基础，有望推动更自适应的网络平台发展。"}}
{"id": "2506.09434", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09434", "abs": "https://arxiv.org/abs/2506.09434", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "comment": null, "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "AI": {"tldr": "论文研究了多智能体任务分配问题中异质性团队的优势，通过奖励设计理论证明了异质性的价值，并提出了一种梯度算法HED来优化多智能体强化学习环境。", "motivation": "探讨在什么情况下异质性团队优于同质性团队，并从奖励设计的角度研究这一问题。", "method": "结合理论分析和多智能体强化学习（MARL），提出Heterogeneous Environment Design（HED）算法，优化环境参数以促进异质性。", "result": "理论证明了异质性的优势取决于奖励函数的曲率，HED算法在实验中验证了理论预测。", "conclusion": "研究揭示了行为多样性何时能带来显著优势，为多智能体系统的奖励设计提供了理论支持。"}}
{"id": "2506.09187", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09187", "abs": "https://arxiv.org/abs/2506.09187", "authors": ["Ahmed Aboudonia", "Johannes Estermann", "Keith Moffat", "Manfred Morari", "John Lygeros"], "title": "A Data-driven Predictive Control Architecture for Train Thermal Energy Management", "comment": null, "summary": "We aim to improve the energy efficiency of train climate control\narchitectures, with a focus on a specific class of regional trains operating\nthroughout Switzerland, especially in Zurich and Geneva. Heating, Ventilation,\nand Air Conditioning (HVAC) systems represent the second largest energy\nconsumer in these trains after traction. The current architecture comprises a\nhigh-level rule-based controller and a low-level tracking controller. To\nimprove train energy efficiency, we propose adding a middle data-driven\npredictive control layer aimed at minimizing HVAC energy consumption while\nmaintaining passenger comfort. The scheme incorporates a multistep prediction\nmodel developed using real-world data collected from a limited number of train\ncoaches. To validate the effectiveness of the proposed architecture, we conduct\nmultiple experiments on a separate set of train coaches; our results suggest\nenergy savings between 10% and 35% with respect to the current architecture.", "AI": {"tldr": "提出了一种中间数据驱动的预测控制层，用于优化瑞士区域列车HVAC系统的能源效率，实验显示节能10%-35%。", "motivation": "HVAC系统是列车第二大能源消耗源，现有架构效率不足，需提升能源效率同时保持乘客舒适度。", "method": "在现有高低层控制器间加入数据驱动的预测控制层，利用真实数据开发多步预测模型。", "result": "实验验证节能效果达10%-35%。", "conclusion": "提出的架构显著提升能源效率，具有实际应用潜力。"}}
{"id": "2506.09206", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09206", "abs": "https://arxiv.org/abs/2506.09206", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "comment": null, "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "AI": {"tldr": "论文提出了一种利用游戏引擎合成课堂噪声的方法，并创建了SimClass数据集，包含合成噪声和模拟课堂语音数据，实验表明其接近真实课堂语音。", "motivation": "大规模课堂语音数据的稀缺阻碍了教育领域AI语音模型的发展，现有公开数据集有限且缺乏专用噪声库。", "method": "使用游戏引擎合成课堂噪声，结合公共儿童语音库和YouTube讲座视频生成模拟课堂语音数据。", "result": "SimClass数据集在实验中表现出与真实课堂语音的高度近似性。", "conclusion": "SimClass为开发鲁棒的语音识别和增强模型提供了有价值的资源。"}}
{"id": "2506.09080", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2506.09080", "abs": "https://arxiv.org/abs/2506.09080", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "comment": null, "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "AI": {"tldr": "FinHEAR是一个多智能体框架，专注于结合人类专业知识和自适应风险感知推理，以改进语言模型在金融决策中的表现。", "motivation": "语言模型在金融决策中缺乏对人类行为模式（如信息不对称下的专家依赖、损失厌恶和反馈驱动调整）的捕捉能力。", "method": "FinHEAR通过多智能体框架，结合历史趋势分析、事件解读和专家知识检索，融入行为经济学原理。", "result": "在金融数据集上，FinHEAR在趋势预测和交易任务中表现优于基线模型，准确性和风险调整收益更高。", "conclusion": "FinHEAR通过结合人类专业知识和自适应风险感知，显著提升了语言模型在金融决策中的表现。"}}
{"id": "2506.09067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "论文提出了一种针对生成式医学视觉语言模型（Med-VLMs）的安全防御策略，旨在解决其可能面临的恶意查询问题，同时避免过度防御影响正常性能。", "motivation": "Med-VLMs在生成复杂医学文本信息时存在安全漏洞，可能被用于恶意目的（如保险欺诈），但现有研究对此关注不足。同时，过度防御可能导致模型拒绝正常临床查询。", "method": "提出了一种基于合成临床示例的推理时防御策略，通过多样化的医学影像数据集验证其有效性，并引入混合示例策略以平衡安全性和性能。", "result": "实验表明，该策略能有效防御视觉和文本攻击，且增加示例预算可缓解过度防御问题。混合策略在少样本预算下实现了安全与性能的平衡。", "conclusion": "该防御策略为Med-VLMs的安全性和实用性提供了可行的解决方案，尤其在资源受限的场景下表现优异。"}}
{"id": "2506.09366", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09366", "abs": "https://arxiv.org/abs/2506.09366", "authors": ["Yuxuan Kuang", "Haoran Geng", "Amine Elhafsi", "Tan-Dzung Do", "Pieter Abbeel", "Jitendra Malik", "Marco Pavone", "Yue Wang"], "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending", "comment": null, "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.", "AI": {"tldr": "SkillBlender是一个分层强化学习框架，通过动态混合预训练的任务无关技能，实现多样化的人形机器人操作任务，减少任务特定调优。", "motivation": "解决现有方法在多样化任务中需要大量任务特定调优的问题，提升人形机器人的通用性和扩展性。", "method": "预训练目标条件的任务无关技能，动态混合这些技能完成复杂任务，减少奖励工程。", "result": "在模拟实验中显著优于基线方法，行为更准确可行。", "conclusion": "SkillBlender提供了一种高效、通用的人形机器人操作解决方案，并开源代码和基准测试以推动研究。"}}
{"id": "2506.09813", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2506.09813", "abs": "https://arxiv.org/abs/2506.09813", "authors": ["Ariel Procaccia", "Benjamin Schiffer", "Serena Wang", "Shirley Zhang"], "title": "Metritocracy: Representative Metrics for Lite Benchmarks", "comment": null, "summary": "A common problem in LLM evaluation is how to choose a subset of metrics from\na full suite of possible metrics. Subset selection is usually done for\nefficiency or interpretability reasons, and the goal is often to select a\n``representative'' subset of metrics. However, ``representative'' is rarely\nclearly defined. In this work, we use ideas from social choice theory to\nformalize two notions of representation for the selection of a subset of\nevaluation metrics. We first introduce positional representation, which\nguarantees every alternative is sufficiently represented at every position\ncutoff. We then introduce positional proportionality, which guarantees no\nalternative is proportionally over- or under-represented by more than a small\nerror at any position. We prove upper and lower bounds on the smallest number\nof metrics needed to guarantee either of these properties in the worst case. We\nalso study a generalized form of each property that allows for additional input\non groups of metrics that must be represented. Finally, we tie theory to\npractice through real-world case studies on both LLM evaluation and hospital\nquality evaluation.", "AI": {"tldr": "论文提出两种形式化的评估指标子集选择方法，基于社会选择理论，确保指标的代表性和比例性，并通过案例研究验证实用性。", "motivation": "解决LLM评估中指标子集选择缺乏明确定义的问题，提升效率和可解释性。", "method": "引入位置代表性和位置比例性两种形式化概念，并研究其理论边界和实际应用。", "result": "证明了在最坏情况下所需最小指标数的上下界，并通过案例研究验证了方法的有效性。", "conclusion": "形式化的代表性和比例性方法为评估指标子集选择提供了理论支持，并在实际应用中展现出潜力。"}}
{"id": "2506.09665", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09665", "abs": "https://arxiv.org/abs/2506.09665", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "comment": null, "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "AI": {"tldr": "利用视频扩散模型、视频内在分解和基于物理的可微分渲染，通过文本提示或单张图像为3D模型生成高质量材质。", "motivation": "解决从文本或单张图像生成高质量3D模型材质的挑战，确保材质与几何和光照条件一致。", "method": "1. 使用视频扩散模型生成多视角视频；2. 提取内在属性（基色、粗糙度、金属性）；3. 结合可微分路径追踪生成PBR材质。", "result": "生成与常见内容创作工具兼容的高质量PBR材质。", "conclusion": "该方法通过结合扩散模型和物理渲染，实现了从文本或图像到3D模型材质的有效生成。"}}
{"id": "2506.09212", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09212", "abs": "https://arxiv.org/abs/2506.09212", "authors": ["Lucas Joos", "Gavin J. Mooney", "Maximilian T. Fischer", "Daniel A. Keim", "Falk Schreiber", "Helen C. Purchase", "Karsten Klein"], "title": "Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings", "comment": null, "summary": "The visual analysis of graphs in 3D has become increasingly popular,\naccelerated by the rise of immersive technology, such as augmented and virtual\nreality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,\nmaking perspective selection critical for revealing structural and relational\npatterns. Despite its importance, there is limited empirical evidence guiding\nwhat constitutes an effective or preferred viewpoint from the user's\nperspective. In this paper, we present a systematic investigation into\nuser-preferred viewpoints in 3D graph visualisations. We conducted a controlled\nstudy with 23 participants in a virtual reality environment, where users\nselected their most and least preferred viewpoints for 36 different graphs\nvarying in size and layout. From this data, enriched by qualitative feedback,\nwe distil common strategies underlying viewpoint choice. We further analyse the\nalignment of user preferences with classical 2D aesthetic criteria (e.g.,\nCrossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a\nnovel measure capturing the perceivability of a graph's principal axes\n(Isometric Viewpoint Deviation). Our data-driven analysis indicates that\nStress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint\nDeviation are key indicators of viewpoint preference. Beyond our findings, we\ncontribute a publicly available dataset consisting of the graphs and computed\naesthetic measures, supporting further research and the development of\nviewpoint evaluation measures for 3D graph drawing.", "AI": {"tldr": "本文系统研究了3D图可视化中用户偏好的视角，通过实验和数据分析确定了影响视角选择的关键指标。", "motivation": "3D图布局高度依赖视角，但目前缺乏关于用户偏好视角的实证研究，因此需要探索有效的视角选择标准。", "method": "在虚拟现实环境中对23名参与者进行控制实验，收集其对36种不同图的偏好视角，并结合定性反馈分析。", "result": "研究发现Stress、Crossings、Gabriel Ratio、Edge-Node Overlap和Isometric Viewpoint Deviation是影响视角偏好的关键指标。", "conclusion": "研究不仅提供了用户视角偏好的实证数据，还公开了数据集，支持未来3D图绘制视角评估的研究。"}}
{"id": "2506.09600", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09600", "abs": "https://arxiv.org/abs/2506.09600", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "title": "Effective Red-Teaming of Policy-Adherent Agents", "comment": null, "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "AI": {"tldr": "论文提出了一种针对任务导向LLM代理的威胁模型和评估方法，通过多代理红队系统CRAFT测试代理的稳健性，并提出了防御策略。", "motivation": "在严格政策领域（如退款或取消规则）中，确保LLM代理始终遵守规则并拒绝违规请求，同时保持自然交互，是一个挑战。", "method": "提出CRAFT多代理红队系统，利用策略感知的劝说策略测试代理的稳健性，并引入tau-break基准评估代理对抗操纵行为的能力。", "result": "CRAFT在客户服务场景中优于传统越狱方法，但现有防御策略仍不足以完全保护代理。", "conclusion": "需进一步研究更强健的防护措施，以保护策略遵守型代理免受对抗攻击。"}}
{"id": "2506.09273", "categories": ["eess.SY", "cs.DM", "cs.SY", "math.OC", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2506.09273", "abs": "https://arxiv.org/abs/2506.09273", "authors": ["Telema Harry", "Martin Guay", "Shimin Wang", "Richard D. Braatz"], "title": "Data-Driven Nonlinear Regulation: Gaussian Process Learning", "comment": null, "summary": "This article addresses the output regulation problem for a class of nonlinear\nsystems using a data-driven approach. An output feedback controller is proposed\nthat integrates a traditional control component with a data-driven learning\nalgorithm based on Gaussian Process (GP) regression to learn the nonlinear\ninternal model. Specifically, a data-driven technique is employed to directly\napproximate the unknown internal model steady-state map from observed\ninput-output data online. Our method does not rely on model-based observers\nutilized in previous studies, making it robust and suitable for systems with\nmodelling errors and model uncertainties. Finally, we demonstrate through\nnumerical examples and detailed stability analysis that, under suitable\nconditions, the closed-loop system remains bounded and converges to a compact\nset, with the size of this set decreasing as the accuracy of the data-driven\nmodel improves over time.", "AI": {"tldr": "本文提出了一种基于数据驱动的方法解决非线性系统的输出调节问题，结合传统控制和Gaussian Process回归学习非线性内部模型。", "motivation": "解决非线性系统输出调节问题，避免依赖模型观测器，提高对建模误差和不确定性的鲁棒性。", "method": "提出输出反馈控制器，结合传统控制与基于GP回归的数据驱动学习算法，在线近似未知内部模型稳态映射。", "result": "数值实验和稳定性分析表明，闭环系统有界且收敛于一个紧集，其大小随数据驱动模型精度提高而减小。", "conclusion": "该方法适用于存在建模误差和不确定性的系统，具有鲁棒性和收敛性。"}}
{"id": "2506.09448", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09448", "abs": "https://arxiv.org/abs/2506.09448", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "comment": "Accepted to Interspeech 2025", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "AI": {"tldr": "该论文提出了一种结合上下文偏置（CB）方法与预训练语音基础模型（OWSM v3.1）的方法，以提升对罕见和未见单词的识别能力，同时保持预训练模型的优势。", "motivation": "语音基础模型（SFMs）在识别罕见和未见单词时表现不佳，而现有的上下文偏置方法因缺乏预训练知识而性能较低。", "method": "将现有的CB方法与OWSM v3.1结合，冻结预训练参数，利用SFMs的知识实现有效的CB。", "result": "在LibriSpeech 100测试集上，B-WER提升了11.6点，整体WER提升0.9点，实时因子降低7.5%。", "conclusion": "该方法在保持SFMs优势的同时，显著提升了罕见单词的识别性能。"}}
{"id": "2506.09084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09084", "abs": "https://arxiv.org/abs/2506.09084", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "comment": null, "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "AI": {"tldr": "论文提出PageLLM，一种基于用户反馈的奖励微调方法，用于优化预训练大语言模型在整页优化（WPO）中的表现，结合页面级和项目级奖励机制，显著提升了推荐系统的效果。", "motivation": "预训练大语言模型（LLMs）在复杂任务如整页优化（WPO）中面临微调挑战，尤其是需要大量人工标注数据来减少幻觉和模型不稳定性。用户反馈虽可用但噪声大，需要更高效的监督方法。", "method": "提出PageLLM，采用混合粒度的奖励机制，结合页面级奖励（评估整体质量和连贯性）和项目级奖励（关注关键推荐的准确性和相关性）。", "result": "在公开和工业数据集上验证，PageLLM优于基线模型，并在在线A/B测试中实现0.44%的GMV增长，覆盖超过1000万用户。", "conclusion": "PageLLM通过用户反馈驱动的奖励机制，有效解决了LLMs在WPO中的微调问题，展示了实际应用中的显著效果。"}}
{"id": "2506.09068", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "BG-HOP是一种生成先验模型，用于建模3D中的双手-物体交互，扩展了现有的单手生成先验，解决了数据不足的问题。", "motivation": "解决双手交互数据有限的问题，扩展单手生成先验以建模双手与物体的联合分布。", "method": "通过扩展现有的单手生成先验，构建BG-HOP模型，生成双手交互并合成物体抓取。", "result": "实验表明模型能生成双手交互，并为给定物体合成抓取动作。", "conclusion": "BG-HOP为双手-物体交互建模提供了有效方法，代码和模型已公开。"}}
{"id": "2506.09383", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.09383", "abs": "https://arxiv.org/abs/2506.09383", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "comment": null, "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "AI": {"tldr": "该论文提出了一种层次化控制流程，通过全身肌肉骨骼系统模拟人类平衡，揭示了平衡的动态特性及肌肉损伤的影响，并验证了髋部外骨骼辅助对平衡的改善效果。", "motivation": "静态平衡和跌倒的定量理解有限，研究旨在填补这一空白，为平衡障碍患者提供干预基础，并支持人形机器人系统的发展。", "method": "采用层次化控制流程，结合全身肌肉骨骼系统模拟，分析稳定站立时的时空动态，研究肌肉损伤影响，并测试髋部外骨骼辅助效果。", "result": "揭示了平衡的动态特性，生成了与临床数据一致的跌倒接触模式，外骨骼辅助显著改善了平衡维持并减少了肌肉努力。", "conclusion": "该研究提供了难以通过实验获取的肌肉级平衡动态见解，为平衡障碍干预和人形机器人发展奠定了基础。"}}
{"id": "2506.09817", "categories": ["eess.SY", "cs.GT", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09817", "abs": "https://arxiv.org/abs/2506.09817", "authors": ["Dhrumil Bhatt", "Nirbhay Singhal"], "title": "Enhanced V2X Communication Using Game-Theory Based Adaptive MAC Protocols", "comment": "Accepted at the 16th ICCCNT", "summary": "This paper presents an enhanced Vehicle-to-Everything (V2X) communication\nsystem featuring adaptive Medium Access Control (MAC) using game theory. Our\napproach integrates dynamic transmission power control, dynamic beacon rates,\ncontention window adaptation, and implicit acknowledgment mechanisms within a\nManhattan-like grid-based mobility scenario. Simulations are conducted in a\ncircular coverage area, incorporating refined signal propagation models and\nprobabilistic vehicle mobility with boundary reflection. The results\ndemonstrate effective beacon delivery with average delays under 0.35 s and\npacket loss rates less than 1% in high-density conditions specifically, with up\nto 80 vehicles operating within a 250 m radius. Key innovations include game\ntheory-based environment-aware transmission parameter adaptation and a scalable\ndesign suited for interference-prone V2X deployments.", "AI": {"tldr": "本文提出了一种基于博弈论的自适应MAC增强V2X通信系统，通过动态传输参数优化，在高密度场景下实现低延迟和低丢包率。", "motivation": "解决高密度V2X场景中的通信效率和可靠性问题。", "method": "结合动态传输功率控制、信标速率调整、竞争窗口适应和隐式确认机制，采用博弈论优化参数。", "result": "在80辆车的高密度场景中，平均延迟低于0.35秒，丢包率小于1%。", "conclusion": "该方案通过环境感知的传输参数自适应和可扩展设计，适用于干扰严重的V2X部署。"}}
{"id": "2506.09909", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.09909", "abs": "https://arxiv.org/abs/2506.09909", "authors": ["Yijie Deng", "Lei Han", "Lu Fang"], "title": "TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model", "comment": null, "summary": "Neural rendering algorithms have revolutionized computer graphics, yet their\nimpact on real-time rendering under arbitrary lighting conditions remains\nlimited due to strict latency constraints in practical applications. The key\nchallenge lies in formulating a compact yet expressive material representation.\nTo address this, we propose TransGI, a novel neural rendering method for\nreal-time, high-fidelity global illumination. It comprises an object-centric\nneural transfer model for material representation and a radiance-sharing\nlighting system for efficient illumination. Traditional BSDF representations\nand spatial neural material representations lack expressiveness, requiring\nthousands of ray evaluations to converge to noise-free colors. Conversely,\nreal-time methods trade quality for efficiency by supporting only diffuse\nmaterials. In contrast, our object-centric neural transfer model achieves\ncompactness and expressiveness through an MLP-based decoder and vertex-attached\nlatent features, supporting glossy effects with low memory overhead. For\ndynamic, varying lighting conditions, we introduce local light probes capturing\nscene radiance, coupled with an across-probe radiance-sharing strategy for\nefficient probe generation. We implemented our method in a real-time rendering\nengine, combining compute shaders and CUDA-based neural networks. Experimental\nresults demonstrate that our method achieves real-time performance of less than\n10 ms to render a frame and significantly improved rendering quality compared\nto baseline methods.", "AI": {"tldr": "TransGI是一种新型神经渲染方法，用于实时高保真全局光照，通过对象中心神经传输模型和辐射共享照明系统实现高效渲染。", "motivation": "解决现有神经渲染算法在实时渲染和任意光照条件下表达性和效率不足的问题。", "method": "采用基于MLP的解码器和顶点附着潜在特征的对象中心神经传输模型，结合局部光探针和辐射共享策略。", "result": "实验结果显示，TransGI在每帧渲染时间小于10毫秒的同时，显著提升了渲染质量。", "conclusion": "TransGI在实时渲染中实现了高效且高质量的全局光照效果，优于现有基线方法。"}}
{"id": "2506.09216", "categories": ["cs.HC", "cs.CY", "H.5"], "pdf": "https://arxiv.org/pdf/2506.09216", "abs": "https://arxiv.org/abs/2506.09216", "authors": ["Qing", "Xia", "Advait Sarkar", "Duncan Brumby", "Anna Cox"], "title": "\"How do you even know that stuff?\": Barriers to expertise sharing among spreadsheet users", "comment": "Accepted at CSCW 2025", "summary": "Spreadsheet collaboration provides valuable opportunities for learning and\nexpertise sharing between colleagues. Sharing expertise is essential for the\nretention of important technical skillsets within organisations, but previous\nstudies suggest that spreadsheet experts often fail to disseminate their\nknowledge to others. We suggest that social norms and beliefs surrounding the\nvalue of spreadsheet use significantly influence user engagement in sharing\nbehaviours. To explore this, we conducted 31 semi-structured interviews with\nprofessional spreadsheet users from two separate samples. We found that\nspreadsheet providers face challenges in adapting highly personalised\nstrategies to often subjective standards and evaluating the appropriate social\ntiming of sharing. In addition, conflicted self-evaluations of one's\nspreadsheet expertise, dismissive normative beliefs about the value of this\nknowledge, and concerns about the potential disruptions associated with\ncollaboration can further deter sharing. We suggest these observations reflect\nthe challenges of long-term learning in feature-rich software designed\nprimarily with initial learnability in mind. We therefore provide implications\nfor design to navigate this tension. Overall, our findings demonstrate how the\ncomplex interaction between technology design and social dynamics can shape\ncollaborative learning behaviours in the context of feature-rich software.", "AI": {"tldr": "研究发现，社交规范和个人信念影响电子表格专家分享知识的行为，技术设计与社会动态的复杂互动塑造了协作学习行为。", "motivation": "探讨电子表格专家为何难以分享知识，以及社交规范和技术设计如何影响协作学习。", "method": "对31名专业电子表格用户进行半结构化访谈。", "result": "专家面临个性化策略适应、自我评价冲突、规范信念和协作担忧等挑战。", "conclusion": "技术设计需平衡初始易学性与长期学习需求，以促进知识分享。"}}
{"id": "2506.09388", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09388", "abs": "https://arxiv.org/abs/2506.09388", "authors": ["Sijia Geng", "Thomas Lee", "Dharik Mallapragada", "Audun Botterud"], "title": "Integer-Clustering Optimization of Hydrogen and Battery EV Fleets Considering DERs", "comment": "10 pages, 9 figures", "summary": "Electrified transportation leads to a tighter integration between\ntransportation and energy distribution systems. In this work, we develop\nscalable optimization models to co-design hydrogen and battery electric vehicle\n(EV) fleets, distributed energy resources, and fast-charging and\nhydrogen-fueling infrastructure to efficiently meet transportation demands. A\nnovel integer-clustering formulation is used for optimizing fleet-level EV\noperation while maintaining accurate individual vehicle dispatch, which\nsignificantly improves the computation efficiency with guaranteed performance.\nWe apply the optimization model to Boston's public transit bus network using\nreal geospatial data and cost parameters. Realistic insights are provided into\nthe future evolution of coupled electricity-transportation-hydrogen systems,\nincluding the effects of electricity price structure, hydrogen fuel cost,\ncarbon emission constraint, temperature effects on EV range, and distribution\nsystem upgrade cost.", "AI": {"tldr": "该论文提出了一种可扩展的优化模型，用于共同设计氢能和电池电动车车队、分布式能源资源以及快速充电和氢燃料基础设施，以高效满足交通需求。", "motivation": "电动化交通导致交通与能源分配系统更紧密地结合，需要优化模型来协调这些系统。", "method": "采用新颖的整数聚类公式优化车队级电动车运营，同时保持准确的单车调度，显著提高计算效率。", "result": "应用于波士顿公共交通巴士网络，提供了关于电力-交通-氢能系统未来发展的现实见解。", "conclusion": "该模型为耦合系统的优化设计提供了高效且可靠的解决方案。"}}
{"id": "2506.09487", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.LO", "eess.AS", "I.2.6; H.5.5; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.09487", "abs": "https://arxiv.org/abs/2506.09487", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "AI": {"tldr": "BemaGANv2是一种基于GAN的高保真音频生成模型，通过引入AMP模块和MED架构改进了原始BemaGAN，提升了长期音频生成的性能。", "motivation": "改进传统GAN在音频生成中的局限性，特别是长期依赖性和周期性建模的不足。", "method": "在生成器中用AMP模块替换ResBlocks，使用Snake激活函数；在判别器中引入MED和MRD架构，结合多种配置进行系统评估。", "result": "通过客观指标（FAD、SSIM等）和主观评价（MOS、SMOS）验证了模型的优越性能。", "conclusion": "BemaGANv2在音频生成中表现出色，提供了详细的实现指南和开源代码以促进复现。"}}
{"id": "2506.09085", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09085", "abs": "https://arxiv.org/abs/2506.09085", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "comment": null, "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "AI": {"tldr": "提出了一种结合LLM和ML的团队框架，通过四步流程提升特征转换的稳定性和有效性，实验显示下游性能提升5%，错误减少近半。", "motivation": "传统ML方法生成有效性低，LLM方法生成不稳定，需结合两者优势解决特征转换问题。", "method": "四步框架：1) 生成高质量示例；2) 嵌入与搜索特征转换序列；3) 学生LLM知识蒸馏；4) LLM-ML解码器团队协作。", "result": "下游性能提升5%，错误减少近半，团队策略高效且稳健。", "conclusion": "结合LLM和ML的团队框架有效解决了特征转换的稳定性和有效性问题，并揭示了LLM对原始数据的理解能力。"}}
{"id": "2506.09071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "提出了一种基于多模态语义引导的建筑立面墙窗自动分割模型SAAF，结合自然语言处理技术提升语义理解，并通过端到端训练框架提高自动化与鲁棒性。实验表明，SAAF在多个数据集上优于现有方法。", "motivation": "建筑数字化发展中，墙窗自动分割是提升建筑信息模型和CAD效率的关键步骤。", "method": "结合多模态语义协作特征提取机制和端到端训练框架，融合文本描述与图像特征。", "result": "在多个立面数据集上，SAAF的分割结果在mIoU指标上优于现有方法。", "conclusion": "SAAF在墙窗分割任务的精度和泛化能力上取得进展，为建筑计算机视觉技术和多模态学习应用提供了新思路。"}}
{"id": "2506.09384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09384", "abs": "https://arxiv.org/abs/2506.09384", "authors": ["Chendong Xin", "Mingrui Yu", "Yongpeng Jiang", "Zhefeng Zhang", "Xiang Li"], "title": "Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation", "comment": null, "summary": "Kinematic retargeting from human hands to robot hands is essential for\ntransferring dexterity from humans to robots in manipulation teleoperation and\nimitation learning. However, due to mechanical differences between human and\nrobot hands, completely reproducing human motions on robot hands is impossible.\nExisting works on retargeting incorporate various optimization objectives,\nfocusing on different aspects of hand configuration. However, the lack of\nexperimental comparative studies leaves the significance and effectiveness of\nthese objectives unclear. This work aims to analyze these retargeting\nobjectives for dexterous manipulation through extensive real-world comparative\nexperiments. Specifically, we propose a comprehensive retargeting objective\nformulation that integrates intuitively crucial factors appearing in recent\napproaches. The significance of each factor is evaluated through experimental\nablation studies on the full objective in kinematic posture retargeting and\nreal-world teleoperated manipulation tasks. Experimental results and\nconclusions provide valuable insights for designing more accurate and effective\nretargeting algorithms for real-world dexterous manipulation.", "AI": {"tldr": "该论文通过实验比较分析了从人手到机器人手的运动重定向目标，提出了一种综合目标公式，并通过实验验证了各因素的重要性。", "motivation": "由于人手与机器人手之间的机械差异，完全复现人手运动是不可能的。现有研究缺乏对这些重定向目标的实验比较，因此其重要性和有效性尚不明确。", "method": "提出了一种综合重定向目标公式，整合了近期方法中的关键因素，并通过实验消融研究评估了各因素的显著性。", "result": "实验结果表明，综合目标公式在运动姿势重定向和实际遥操作任务中表现优异。", "conclusion": "研究结果为设计更准确有效的重定向算法提供了宝贵见解。"}}
{"id": "2506.09997", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09997", "abs": "https://arxiv.org/abs/2506.09997", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "AI": {"tldr": "DGS-LRM是首个基于单目视频的动态场景3D高斯点云预测方法，解决了动态场景重建的挑战，包括数据稀缺和3D表示问题。", "motivation": "现有前馈模型多限于静态场景，无法重建动态物体运动，需解决数据稀缺和3D表示问题。", "method": "提出合成数据集、可变形3D高斯表示和大型Transformer网络，实现实时动态重建。", "result": "DGS-LRM在动态重建质量上媲美优化方法，优于现有预测方法，且适用于长程3D跟踪。", "conclusion": "DGS-LRM为动态场景重建提供了高效、通用的解决方案，性能优越。"}}
{"id": "2506.09220", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09220", "abs": "https://arxiv.org/abs/2506.09220", "authors": ["Karen Joy", "Tawfiq Ammari", "Alyssa Sheehan"], "title": "Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use", "comment": null, "summary": "This paper examines the gap between the promises and real-world performance\nof emerging AI personal assistants. Drawing on interviews with early adopters\nof devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and\nDocus, we map user experiences through the lens of Uses and Gratifications and\nUncertainty Reduction Theory. We identify three core types of user uncertainty,\nfunctional, interactional, and social, and explore how each disrupts different\nuser gratifications. We show that while marketing hype fuels initial adoption,\nunmet expectations often result in frustration or abandonment. Our findings\nhighlight the importance of transparency, task-specific design, and user\ncontrol over contextual memory and personalization. We provide design and\npolicy recommendations, including user-facing explainability tools and calls\nfor regulatory benchmarks such as CI Bench, to guide ethical and interpretable\nAI integration. Our study offers actionable insights for creating more usable,\ntrustworthy, and socially aligned AI assistants.", "AI": {"tldr": "论文探讨了新兴AI个人助手在承诺与实际表现之间的差距，通过用户访谈和理论分析，揭示了用户不确定性的三种类型及其影响，并提出了设计和政策建议。", "motivation": "研究动机是理解AI个人助手在现实中的表现与宣传之间的差距，以及用户在使用过程中遇到的挑战。", "method": "方法包括对早期用户的访谈，并结合“使用与满足理论”和“不确定性减少理论”进行分析。", "result": "研究发现用户面临功能、交互和社交三种不确定性，这些未满足的期望导致挫败感或放弃使用。", "conclusion": "结论强调了透明度、任务导向设计和用户控制的重要性，并提出了设计和政策建议，以推动更可信赖的AI助手发展。"}}
{"id": "2506.09392", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09392", "abs": "https://arxiv.org/abs/2506.09392", "authors": ["Hao Li", "Rizwan S. Peerla", "Frank Barrows", "Francesco Caravelli", "Bibhu Datta Sahoo"], "title": "Voltage-Controlled Oscillator and Memristor-Based Analog Computing for Solving Systems of Linear Equations", "comment": "11 pages, 22 figures, Journal", "summary": "Matrix computations have become increasingly significant in many data-driven\napplications. However, Moores law for digital computers has been gradually\napproaching its limit in recent years. Moreover, digital computers encounter\nsubstantial complexity when performing matrix computations and need a long time\nto finish the computations, and existing analog matrix computation schemes\nrequire a large chip area and power consumption. This paper proposes a linear\nalgebra system of equations based on integrators, which features low power\nconsumption, compact area, and fast computation time. Due to the simple\nstructure of the ring oscillator, the ring oscillator-based integrator exhibits\na compact area and low power consumption. Therefore, ring oscillator-based\nintegrators are introduced into the linear algebra system of equations, and\nthis system can be used to compute the linear algebra equations of the matrix\nwith either positive or negative values. This paper provides a detailed\nanalysis and verification of the proposed circuit structure. Compared to\nsimilar circuits, this work has significant advantages in terms of area, power\nconsumption, and computation speed.", "AI": {"tldr": "提出了一种基于积分器的线性代数方程组，具有低功耗、小面积和快速计算的特点。", "motivation": "数字计算机在矩阵计算中存在复杂性和耗时问题，现有模拟方案则占用较大芯片面积和功耗。", "method": "采用环形振荡器积分器构建线性代数方程组，适用于正负值矩阵计算。", "result": "相比同类电路，该方案在面积、功耗和计算速度上具有显著优势。", "conclusion": "该电路结构为高效矩阵计算提供了可行方案。"}}
{"id": "2506.09709", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09709", "abs": "https://arxiv.org/abs/2506.09709", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "comment": "Interspeech 2025", "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "AI": {"tldr": "Factorized MKL-VC是一种无需训练的改进方法，用于kNN-VC流程，通过5秒参考音频实现高质量跨语言语音转换。", "motivation": "解决kNN-VC在短参考音频下内容保留和鲁棒性不足的问题。", "method": "用因子化的最优传输映射替换kNN回归，基于WavLM嵌入子空间和Monge-Kantorovich线性解。", "result": "在LibriSpeech和FLEURS数据集上表现优于kNN-VC，尤其在跨语言语音转换中接近FACodec性能。", "conclusion": "MKL-VC在短参考音频下显著提升性能，适用于跨语言语音转换。"}}
{"id": "2506.09087", "categories": ["cs.LG", "math.PR", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09087", "abs": "https://arxiv.org/abs/2506.09087", "authors": ["Sophie Jaffard", "Giulia Mezzadri", "Patricia Reynaud-Bouret", "Etienne Tanré"], "title": "Spiking Neural Models for Decision-Making Tasks with Learning", "comment": null, "summary": "In cognition, response times and choices in decision-making tasks are\ncommonly modeled using Drift Diffusion Models (DDMs), which describe the\naccumulation of evidence for a decision as a stochastic process, specifically a\nBrownian motion, with the drift rate reflecting the strength of the evidence.\nIn the same vein, the Poisson counter model describes the accumulation of\nevidence as discrete events whose counts over time are modeled as Poisson\nprocesses, and has a spiking neurons interpretation as these processes are used\nto model neuronal activities. However, these models lack a learning mechanism\nand are limited to tasks where participants have prior knowledge of the\ncategories. To bridge the gap between cognitive and biological models, we\npropose a biologically plausible Spiking Neural Network (SNN) model for\ndecision-making that incorporates a learning mechanism and whose neurons\nactivities are modeled by a multivariate Hawkes process. First, we show a\ncoupling result between the DDM and the Poisson counter model, establishing\nthat these two models provide similar categorizations and reaction times and\nthat the DDM can be approximated by spiking Poisson neurons. To go further, we\nshow that a particular DDM with correlated noise can be derived from a Hawkes\nnetwork of spiking neurons governed by a local learning rule. In addition, we\ndesigned an online categorization task to evaluate the model predictions. This\nwork provides a significant step toward integrating biologically relevant\nneural mechanisms into cognitive models, fostering a deeper understanding of\nthe relationship between neural activity and behavior.", "AI": {"tldr": "论文提出了一种基于生物可解释性的脉冲神经网络（SNN）模型，用于决策任务，弥补了传统模型缺乏学习机制的不足，并通过实验验证了其有效性。", "motivation": "传统决策模型（如漂移扩散模型和泊松计数器模型）缺乏学习机制，且仅适用于参与者已具备类别知识的任务。本研究旨在填补认知模型与生物模型之间的鸿沟。", "method": "提出了一种基于多变量霍克斯过程的SNN模型，结合了学习机制，并通过耦合结果证明了其与传统模型的相似性。此外，设计了一项在线分类任务来验证模型预测。", "result": "证明了漂移扩散模型与泊松计数器模型的耦合关系，并展示了霍克斯网络如何通过局部学习规则生成特定漂移扩散模型。实验验证了模型的有效性。", "conclusion": "该研究为将生物相关神经机制整合到认知模型中提供了重要进展，深化了对神经活动与行为关系的理解。"}}
{"id": "2506.09079", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "论文提出了两个新数据集（DarkEventInfer和MixVidQA）和VersaVid-R1模型，填补了视频推理领域的空白，并在多个任务中表现优异。", "motivation": "视频推理领域因缺乏高质量数据和有效训练方法而发展不足，需填补这一空白。", "method": "通过DarkEventInfer和MixVidQA数据集训练模型，结合强化学习，开发VersaVid-R1模型。", "result": "VersaVid-R1在多项任务中显著优于现有模型。", "conclusion": "VersaVid-R1是首个多功能视频理解与推理模型，填补了领域空白并表现出色。"}}
{"id": "2506.09406", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09406", "abs": "https://arxiv.org/abs/2506.09406", "authors": ["Minji Kang", "Chanwoo Baek", "Yoonsang Lee"], "title": "Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems", "comment": null, "summary": "Quadruped robots have made significant advances in locomotion, extending\ntheir capabilities from controlled environments to real-world applications.\nBeyond movement, recent work has explored loco-manipulation using the legs to\nperform tasks such as pressing buttons or opening doors. While these efforts\ndemonstrate the feasibility of leg-based manipulation, most have focused on\nrelatively static tasks. In this work, we propose a framework that enables\nquadruped robots to collect objects without additional actuators by leveraging\nthe agility of their legs. By attaching a simple scoop-like add-on to one leg,\nthe robot can scoop objects and toss them into a collection tray mounted on its\nback. Our method employs a hierarchical policy structure comprising two expert\npolicies-one for scooping and tossing, and one for approaching object\npositions-and a meta-policy that dynamically switches between them. The expert\npolicies are trained separately, followed by meta-policy training for\ncoordinated multi-object collection. This approach demonstrates how quadruped\nlegs can be effectively utilized for dynamic object manipulation, expanding\ntheir role beyond locomotion.", "AI": {"tldr": "提出了一种四足机器人利用腿部敏捷性进行动态物体收集的框架，无需额外执行器。", "motivation": "探索四足机器人腿部在动态任务中的多功能性，超越传统静态操作。", "method": "采用分层策略结构，包括两个专家策略（舀取和抛掷、接近物体位置）和一个动态切换的元策略。", "result": "展示了四足机器人腿部在动态物体操作中的有效性。", "conclusion": "扩展了四足机器人腿部的功能，证明了其在动态任务中的潜力。"}}
{"id": "2506.09485", "categories": ["cs.RO", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.09485", "abs": "https://arxiv.org/abs/2506.09485", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "comment": null, "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "AI": {"tldr": "Adv-BMT框架通过双向运动变换器生成多样且真实的对抗性交互，解决了自动驾驶测试中长尾安全关键场景数据不足的问题。", "motivation": "现有数据集中长尾安全关键场景稀缺，限制了自动驾驶系统的测试效果。", "method": "Adv-BMT框架采用双向运动变换器（BMT）进行逆向交通运动预测，无需碰撞数据预训练，生成多样且真实的碰撞交互。", "result": "实验表明，使用Adv-BMT生成的数据训练可将碰撞率降低20%。", "conclusion": "Adv-BMT有效提升了自动驾驶测试场景的多样性和真实性。"}}
{"id": "2506.09236", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09236", "abs": "https://arxiv.org/abs/2506.09236", "authors": ["Erin Argo", "Tanim Ahmed", "Sarah Gable", "Callie Hampton", "Jeronimo Grandi", "Regis Kopper"], "title": "Augmented Reality User Interfaces for First Responders: A Scoping Literature Review", "comment": "19 pages, 4 figures, 8 tables", "summary": "During the past decade, there has been a significant increase in research\nfocused on integrating AR User Interfaces into public safety applications,\nparticularly for first responders in the domains of Emergency Medical Services,\nFirefighting, and Law Enforcement. This paper presents the results of a scoping\nreview involving the application of AR user interfaces in the public safety\ndomain and applies an established systematic review methodology to provide a\ncomprehensive analysis of the current research landscape, identifying key\ntrends, challenges, and gaps in the literature. This review includes\npeer-reviewed publications indexed by the major scientific databases up to\nApril 2025. A basic keyword search retrieved 1,751 papers, of which 90 were\ndeemed relevant for this review. An in-depth analysis of the literature allowed\nthe development of a faceted taxonomy that categorizes AR user interfaces for\npublic safety. This classification lays a solid foundation for future research,\nwhile also highlighting key design considerations, challenges, and gaps in the\nliterature. This review serves as a valuable resource for researchers and\ndevelopers, offering insights that can drive further advances in the field.", "AI": {"tldr": "本文通过系统综述方法分析了AR用户界面在公共安全领域的应用，总结了关键趋势、挑战和文献中的空白。", "motivation": "过去十年，AR用户界面在公共安全领域的应用研究显著增加，但缺乏系统性的综述分析。", "method": "采用系统综述方法，筛选了1751篇论文中的90篇相关文献，并开发了分类法。", "result": "提出了一个分类法，总结了设计考虑、挑战和文献空白，为未来研究奠定基础。", "conclusion": "本文为研究人员和开发者提供了有价值的资源，推动该领域的进一步发展。"}}
{"id": "2506.09410", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09410", "abs": "https://arxiv.org/abs/2506.09410", "authors": ["H. A. Krog", "Y. Jooss", "H. Fyhn", "P. Nekså", "I. Hjorth"], "title": "Large-scale LH2 pipeline infrastructure concept for airports", "comment": null, "summary": "Infrastructure and processes for handling of liquid hydrogen (LH2) is needed\nto enable large-scale decarbonization of aviation with hydrogen aircraft. At\nlarge airports, pipeline and hydrant systems will be important for a mature\nhydrogen-powered air travel market. As the vaporization of LH2 is a challenge\nin fuel handling, the pipeline infrastructure must be designed and operated\nsuch that the fuel is subcooled. Through modelling and simulation of aircraft\ntanks refuelling by a pipeline infrastructure concept, it is found that\ncontinuous recycling of LH2 within the system is needed to maintain subcooling,\nand the pump operation is important for preventing flashing. With the proposed\nconcept, some hydrogen vapor is formed in the aircraft tank, but the vapor can\nbe utilised by hydrogen-powered ground support equipment.", "AI": {"tldr": "论文探讨了液态氢（LH2）基础设施和流程对航空脱碳的重要性，提出了一种管道系统概念以维持LH2的过冷状态，并模拟了飞机油箱加注过程。", "motivation": "为了实现氢动力航空的大规模脱碳，需要建立液态氢的基础设施和流程，尤其是在大型机场。", "method": "通过建模和模拟飞机油箱通过管道系统加注液态氢的过程，研究了维持LH2过冷状态所需的连续循环系统。", "result": "研究发现，连续循环系统可以维持LH2的过冷状态，泵的操作对防止闪蒸至关重要，飞机油箱中会产生少量氢气蒸汽，但可被地面设备利用。", "conclusion": "提出的管道系统概念能够有效支持氢动力航空的基础设施需求，同时优化燃料处理流程。"}}
{"id": "2506.09792", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09792", "abs": "https://arxiv.org/abs/2506.09792", "authors": ["Wenxuan Wu", "Shuai Wang", "Xixin Wu", "Helen Meng", "Haizhou Li"], "title": "Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction", "comment": "Accepted by Interspeech 2025", "summary": "Audio-visual target speaker extraction (AV-TSE) models primarily rely on\ntarget visual cues to isolate the target speaker's voice from others. We know\nthat humans leverage linguistic knowledge, such as syntax and semantics, to\nsupport speech perception. Inspired by this, we explore the potential of\npre-trained speech-language models (PSLMs) and pre-trained language models\n(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose\nincorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE\nmodel as additional supervision signals. Without introducing any extra\ncomputational cost during inference, the proposed approach consistently\nimproves speech quality and intelligibility. Furthermore, we evaluate our\nmethod in multi-language settings and visual cue-impaired scenarios and show\nrobust performance gains.", "AI": {"tldr": "论文提出利用预训练的语音-语言模型（PSLMs）和语言模型（PLMs）的辅助知识，通过引入语言约束作为额外监督信号，提升音频-视觉目标说话人提取（AV-TSE）模型的性能。", "motivation": "人类利用语言知识（如语法和语义）辅助语音感知，启发研究者探索预训练模型作为AV-TSE的辅助知识源。", "method": "将PSLMs或PLMs的语言约束作为额外监督信号融入AV-TSE模型，无需增加推理时的计算成本。", "result": "方法显著提升了语音质量和清晰度，在多语言和视觉线索受损场景中表现稳健。", "conclusion": "研究表明，引入语言约束是一种有效的AV-TSE性能提升策略，且无需额外推理成本。"}}
{"id": "2506.09090", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09090", "abs": "https://arxiv.org/abs/2506.09090", "authors": ["Arthur Oghlukyan", "Nuria Gomez Blas"], "title": "Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications", "comment": null, "summary": "This paper presents a comprehensive analysis of an enhanced asynchronous\nAdaBoost framework for federated learning (FL), focusing on its application\nacross five distinct domains: computer vision on edge devices, blockchain-based\nmodel transparency, on-device mobile personalization, IoT anomaly detection,\nand federated healthcare diagnostics. The proposed algorithm incorporates\nadaptive communication scheduling and delayed weight compensation to reduce\nsynchronization frequency and communication overhead while preserving or\nimproving model accuracy. We examine how these innovations improve\ncommunication efficiency, scalability, convergence, and robustness in each\ndomain. Comparative metrics including training time, communication overhead,\nconvergence iterations, and classification accuracy are evaluated using data\nand estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical\nresults show, for example, training time reductions on the order of 20-35% and\ncommunication overhead reductions of 30-40% compared to baseline AdaBoost, with\nconvergence achieved in significantly fewer boosting rounds. Tables and charts\nsummarize these improvements by domain. Mathematical formulations of the\nadaptive scheduling rule and error-driven synchronization thresholds are\nprovided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency\nand robustness across diverse FL scenarios, suggesting broad applicability of\nthe approach.", "AI": {"tldr": "本文提出了一种增强的异步AdaBoost框架，用于联邦学习（FL），通过自适应通信调度和延迟权重补偿减少同步频率和通信开销，同时保持或提高模型准确性。", "motivation": "研究旨在解决联邦学习中高通信开销和同步频率的问题，提升效率、可扩展性和鲁棒性。", "method": "采用自适应通信调度和延迟权重补偿技术，减少同步频率和通信开销。", "result": "实验结果显示训练时间减少20-35%，通信开销降低30-40%，收敛轮次显著减少。", "conclusion": "增强的AdaBoost框架在多种FL场景中表现出高效性和鲁棒性，具有广泛适用性。"}}
{"id": "2506.09081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM是一个开源的多模态模型评估框架，支持多种视觉-语言任务，通过独立评估服务和高效工具提升评估效率。", "motivation": "为多模态研究提供一个灵活、高效的评估工具，帮助研究者全面分析模型性能。", "method": "通过独立评估服务解耦模型推理与评估，结合vLLM、SGLang等加速工具和异步数据加载。", "result": "实验表明FlagEvalMM能准确高效地分析模型优缺点。", "conclusion": "FlagEvalMM是一个有价值的工具，可推动多模态研究发展。"}}
{"id": "2506.09422", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09422", "abs": "https://arxiv.org/abs/2506.09422", "authors": ["Ye Niu", "Sanping Zhou", "Yizhe Li", "Ye Den", "Le Wang"], "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation", "comment": null, "summary": "In many complex scenarios, robotic manipulation relies on generative models\nto estimate the distribution of multiple successful actions. As the diffusion\nmodel has better training robustness than other generative models, it performs\nwell in imitation learning through successful robot demonstrations. However,\nthe diffusion-based policy methods typically require significant time to\niteratively denoise robot actions, which hinders real-time responses in robotic\nmanipulation. Moreover, existing diffusion policies model a time-varying action\ndenoising process, whose temporal complexity increases the difficulty of model\ntraining and leads to suboptimal action accuracy. To generate robot actions\nefficiently and accurately, we present the Time-Unified Diffusion Policy\n(TUDP), which utilizes action recognition capabilities to build a time-unified\ndenoising process. On the one hand, we build a time-unified velocity field in\naction space with additional action discrimination information. By unifying all\ntimesteps of action denoising, our velocity field reduces the difficulty of\npolicy learning and speeds up action generation. On the other hand, we propose\nan action-wise training method, which introduces an action discrimination\nbranch to supply additional action discrimination information. Through\naction-wise training, the TUDP implicitly learns the ability to discern\nsuccessful actions to better denoising accuracy. Our method achieves\nstate-of-the-art performance on RLBench with the highest success rate of 82.6%\non a multi-view setup and 83.8% on a single-view setup. In particular, when\nusing fewer denoising iterations, TUDP achieves a more significant improvement\nin success rate. Additionally, TUDP can produce accurate actions for a wide\nrange of real-world tasks.", "AI": {"tldr": "论文提出了一种时间统一的扩散策略（TUDP），通过动作识别能力构建时间统一的去噪过程，提高了机器人动作生成的效率和准确性。", "motivation": "扩散模型在机器人模仿学习中表现良好，但现有方法存在实时性差和动作精度低的问题。", "method": "TUDP构建了时间统一的速度场，并引入动作判别分支提供额外信息，通过动作级训练提高去噪精度。", "result": "在RLBench上取得最高成功率（多视图82.6%，单视图83.8%），尤其在较少去噪迭代时表现更优。", "conclusion": "TUDP显著提升了机器人动作生成的效率和准确性，适用于广泛的实际任务。"}}
{"id": "2506.09292", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09292", "abs": "https://arxiv.org/abs/2506.09292", "authors": ["Brooklyn J. Corbett", "Jason M. Tangen"], "title": "AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education", "comment": "Originally posted as https://doi.org/10.31234/osf.io/x4wqh_v1", "summary": "Misconceptions in psychology and education persist despite clear\ncontradictory evidence, resisting traditional correction methods. This study\ninvestigated whether personalised AI dialogue could effectively correct these\nstubborn beliefs. In a preregistered experiment (N = 375), participants holding\nstrong psychology misconceptions engaged in one of three interventions: (1)\npersonalised AI dialogue targeting their specific misconception, (2) generic\ntextbook-style refutation, or (3) neutral AI dialogue (control). Results showed\nthat personalised AI dialogue produced significantly larger immediate belief\nreductions compared to both textbook reading and neutral dialogue. This\nadvantage persisted at 10-day follow-up but diminished by 2 months, where AI\ndialogue and textbook conditions converged while both remained superior to\ncontrol. Both AI conditions generated significantly higher engagement and\nconfidence than textbook reading, demonstrating the motivational benefits of\nconversational interaction. These findings demonstrate that AI dialogue can\naccelerate initial belief correction through personalised, interactive\nengagement that disrupts the cognitive processes maintaining misconceptions.\nHowever, the convergence of effects over time suggests brief interventions\nrequire reinforcement for lasting change. Future applications should integrate\nAI tutoring into structured educational programs with spaced reinforcement to\nsustain the initial advantages of personalised dialogue.", "AI": {"tldr": "研究表明，个性化AI对话能更有效地纠正心理学和教育中的顽固误解，但效果随时间减弱。", "motivation": "探讨个性化AI对话是否能有效纠正心理学和教育中的顽固误解。", "method": "通过预注册实验，比较个性化AI对话、通用教科书式反驳和中性AI对话的效果。", "result": "个性化AI对话在短期内显著减少误解，但长期效果与教科书式反驳趋同。", "conclusion": "AI对话能加速初始纠正，但需结合结构化教育和强化以维持效果。"}}
{"id": "2506.09447", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09447", "abs": "https://arxiv.org/abs/2506.09447", "authors": ["Wenxin Liu", "Jiakun Fang", "Shichang Cui", "Iskandar Abdullaev", "Suyang Zhou", "Xiaomeng Ai", "Jinyu Wen"], "title": "Optimization and Control Technologies for Renewable-Dominated Hydrogen-Blended Integrated Gas-Electricity System: A Review", "comment": null, "summary": "The growing coupling among electricity, gas, and hydrogen systems is driven\nby green hydrogen blending into existing natural gas pipelines, paving the way\ntoward a renewable-dominated energy future. However, the integration poses\nsignificant challenges, particularly ensuring efficient and safe operation\nunder varying hydrogen penetration and infrastructure adaptability. This paper\nreviews progress in optimization and control technologies for hydrogen-blended\nintegrated gas-electricity system. First, key technologies and international\ndemonstration projects are introduced to provide an overview of current\ndevelopments. Besides, advances in gas-electricity system integration,\nincluding modeling, scheduling, planning and market design, are reviewed\nrespectively. Then, the potential for cross-system fault propagation is\nhighlighted, and practical methods for safety analysis and control are\nproposed. Finally, several possible research directions are introduced, aiming\nto ensure efficient renewable integration and reliable operation.", "AI": {"tldr": "论文综述了氢气混合天然气管道在电力-天然气系统中的优化与控制技术进展，包括建模、调度、规划和市场设计，并提出了安全分析与控制方法。", "motivation": "研究电力、天然气和氢气系统耦合的挑战，尤其是高效安全运行和基础设施适应性。", "method": "综述现有技术和国际示范项目，分析系统集成、建模、调度、规划和市场设计，提出安全控制方法。", "result": "总结了当前进展，提出了跨系统故障传播的潜在风险及应对措施。", "conclusion": "未来研究方向应聚焦可再生能源高效集成和系统可靠运行。"}}
{"id": "2506.09874", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09874", "abs": "https://arxiv.org/abs/2506.09874", "authors": ["Neta Glazer", "Aviv Navon", "Yael Segal", "Aviv Shamsian", "Hilit Segev", "Asaf Buchnick", "Menachem Pirchi", "Gil Hetz", "Joseph Keshet"], "title": "UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching", "comment": null, "summary": "Recent advances in Text-to-Speech (TTS) have enabled highly natural speech\nsynthesis, yet integrating speech with complex background environments remains\nchallenging. We introduce UmbraTTS, a flow-matching based TTS model that\njointly generates both speech and environmental audio, conditioned on text and\nacoustic context. Our model allows fine-grained control over background volume\nand produces diverse, coherent, and context-aware audio scenes. A key challenge\nis the lack of data with speech and background audio aligned in natural\ncontext. To overcome the lack of paired training data, we propose a\nself-supervised framework that extracts speech, background audio, and\ntranscripts from unannotated recordings. Extensive evaluations demonstrate that\nUmbraTTS significantly outperformed existing baselines, producing natural,\nhigh-quality, environmentally aware audios.", "AI": {"tldr": "UmbraTTS是一种基于流匹配的TTS模型，可同时生成语音和环境音频，解决了复杂背景环境中语音合成的挑战。", "motivation": "现有TTS技术虽能生成自然语音，但在复杂背景环境中的集成仍具挑战性。", "method": "提出自监督框架，从无标注录音中提取语音、背景音频和文本，训练流匹配模型。", "result": "UmbraTTS显著优于现有基线，生成自然、高质量且环境感知的音频。", "conclusion": "UmbraTTS为复杂环境中的语音合成提供了有效解决方案。"}}
{"id": "2506.09091", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.09091", "abs": "https://arxiv.org/abs/2506.09091", "authors": ["Kenric Nelson", "Igor Oliveira", "Amenah Al-Najafi", "Fode Zhang", "Hon Keung Tony Ng"], "title": "Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy", "comment": "11 pages, 2 figures, AGI-25", "summary": "We introduce an optimization framework for variational inference based on the\ncoupled free energy, extending variational inference techniques to account for\nthe curved geometry of the coupled exponential family. This family includes\nimportant heavy-tailed distributions such as the generalized Pareto and the\nStudent's t. By leveraging the coupled free energy, which is equal to the\ncoupled evidence lower bound (ELBO) of the inverted probabilities, we improve\nthe accuracy and robustness of the learned model. The coupled generalization of\nFisher Information metric and the affine connection. The method is applied to\nthe design of a coupled variational autoencoder (CVAE). By using the coupling\nfor both the distributions and cost functions, the reconstruction metric is\nderived to still be the mean-square average loss with modified constants. The\nnovelty comes from sampling the heavy-tailed latent distribution with its\nassociated coupled probability, which has faster decaying tails. The result is\nthe ability to train a model with high penalties in the tails, while assuring\nthat the training samples have a reduced number of outliers. The Wasserstein-2\nor Fr\\'echet Inception Distance of the reconstructed CelebA images shows the\nCVAE has a 3\\% improvement over the VAE after 5 epochs of training.", "AI": {"tldr": "提出了一种基于耦合自由能的变分推断优化框架，扩展了变分推断技术以处理耦合指数族的曲率几何，提高了模型的准确性和鲁棒性。", "motivation": "传统变分推断技术未考虑耦合指数族的曲率几何，限制了其在重尾分布（如广义帕累托和学生t分布）中的应用。", "method": "通过耦合自由能（等于倒概率的耦合证据下界）改进模型，设计耦合变分自编码器（CVAE），利用重尾潜在分布采样。", "result": "在CelebA图像重建中，CVAE的Wasserstein-2或Fréchet Inception Distance比VAE提高了3%。", "conclusion": "该方法通过耦合分布和成本函数，显著减少了训练样本中的异常值，同时提高了模型性能。"}}
{"id": "2506.09082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "AVA-Bench是一个新的基准测试，旨在通过解耦14种原子视觉能力（AVAs）来系统评估视觉基础模型（VFMs），解决了现有VQA基准测试的数据分布不对齐和多能力混淆问题。", "motivation": "现有评估协议存在两个盲点：指令调优数据与VQA测试分布不匹配，以及VQA基准测试需要多种视觉能力，难以定位错误来源。", "method": "引入AVA-Bench，明确解耦14种AVAs（如定位、深度估计等），并在每种能力内匹配训练和测试分布，以精确评估VFMs。", "result": "AVA-Bench揭示了VFMs的独特“能力指纹”，并发现0.5B参数的LLM与7B参数的LLM在VFM排名上表现相似，但GPU时间减少8倍。", "conclusion": "AVA-Bench为下一代VFMs提供了全面透明的评估基准，使VFM选择从猜测变为有原则的工程。"}}
{"id": "2506.09444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09444", "abs": "https://arxiv.org/abs/2506.09444", "authors": ["Paul Tucan", "Nadim Al Hajjar", "Calin Vaida", "Alexandru Pusca", "Tiberiu Antal", "Corina Radu", "Daniel Jucan", "Adrian Pisla", "Damien Chablat", "Doina Pisla"], "title": "Design of an innovative robotic surgical instrument for circular stapling", "comment": null, "summary": "Esophageal cancer remains a highly aggressive malignancy with low survival\nrates, requiring advanced surgical interventions like esophagectomy.\nTraditional manual techniques, including circular staplers, face challenges\nsuch as limited precision, prolonged recovery times, and complications like\nleaks and tissue misalignment. This paper presents a novel robotic circular\nstapler designed to enhance the dexterity in confined spaces, improve tissue\nalignment, and reduce post-operative risks. Integrated with a cognitive robot\nthat serves as a surgeon's assistant, the surgical stapler uses three actuators\nto perform anvil motion, cutter/stapler motion and allows a 75-degree bending\nof the cartridge (distal tip). Kinematic analysis is used to compute the\nstapler tip's position, ensuring synchronization with a robotic system.", "AI": {"tldr": "本文介绍了一种新型机器人圆形吻合器，旨在提高食管癌手术中的精确度和减少术后风险。", "motivation": "食管癌手术中传统手动吻合器存在精度不足、恢复时间长和并发症多的问题。", "method": "设计了一种集成认知机器人的吻合器，通过三个执行器实现运动，并利用运动学分析确保同步。", "result": "吻合器能够实现75度弯曲，提高组织对齐和手术精确度。", "conclusion": "新型机器人吻合器有望改善食管癌手术效果，减少并发症。"}}
{"id": "2506.09354", "categories": ["cs.HC", "cs.AI", "H.5.0"], "pdf": "https://arxiv.org/pdf/2506.09354", "abs": "https://arxiv.org/abs/2506.09354", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "comment": null, "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "AI": {"tldr": "论文探讨了AI驱动的LLM如何提升同伴支持的质量，通过模拟客户和实时建议，研究发现其潜力与局限性，强调标准化培训的重要性。", "motivation": "心理健康问题日益严重，AI驱动的解决方案可能扩展心理支持的可及性，但同伴支持的质量和安全性存在挑战。", "method": "开发了一个AI支持系统，包括LLM模拟客户、实时建议和情绪可视化，并通过混合方法研究评估其效果。", "result": "同伴支持者和专家认可系统的潜力，但也发现同伴支持者在关键问题上的不足，如忽略痛苦信号和过早建议。", "conclusion": "需标准化培训，LLM系统需谨慎设计并接受专家指导，以负责任地整合AI于心理健康领域。"}}
{"id": "2506.09512", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09512", "abs": "https://arxiv.org/abs/2506.09512", "authors": ["Donglin Wang", "Anjie Qiu", "Qiuheng Zhou", "Hans D. Schotten"], "title": "A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications", "comment": "7 pages, 1 figure", "summary": "The rapid advancement of Vehicle-to-Everything (V2X) communication is\ntransforming Intelligent Transportation Systems (ITS), with 6G networks\nexpected to provide ultra-reliable, low-latency, and high-capacity connectivity\nfor Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and\nMachine Learning (ML) have emerged as key enablers in optimizing V2X\ncommunication by enhancing network management, predictive analytics, security,\nand cooperative driving due to their outstanding performance across various\ndomains, such as natural language processing and computer vision. This survey\ncomprehensively reviews recent advances in AI and ML models applied to 6G-V2X\ncommunication. It focuses on state-of-the-art techniques, including Deep\nLearning (DL), Reinforcement Learning (RL), Generative Learning (GL), and\nFederated Learning (FL), with particular emphasis on developments from the past\ntwo years. Notably, AI, especially GL, has shown remarkable progress and\nemerging potential in enhancing the performance, adaptability, and intelligence\nof 6G-V2X systems. Despite these advances, a systematic summary of recent\nresearch efforts in this area remains lacking, which this survey aims to\naddress. We analyze their roles in 6G-V2X applications, such as intelligent\nresource allocation, beamforming, intelligent traffic management, and security\nmanagement. Furthermore, we explore the technical challenges, including\ncomputational complexity, data privacy, and real-time decision-making\nconstraints, while identifying future research directions for AI-driven 6G-V2X\ndevelopment. This study aims to provide valuable insights for researchers,\nengineers, and policymakers working towards realizing intelligent, AI-powered\nV2X ecosystems in 6G communication.", "AI": {"tldr": "本文综述了AI和ML在6G-V2X通信中的最新进展，重点介绍了深度学习、强化学习、生成学习和联邦学习等技术，并探讨了技术挑战和未来研究方向。", "motivation": "6G网络为车联网（V2X）提供了超可靠、低延迟和高容量的连接，而AI和ML在优化V2X通信中表现出巨大潜力，但缺乏系统性总结。", "method": "通过综述过去两年的研究，分析AI和ML模型（如DL、RL、GL、FL）在6G-V2X中的应用，如资源分配、波束成形和安全管理。", "result": "AI（尤其是生成学习）显著提升了6G-V2X系统的性能、适应性和智能化水平。", "conclusion": "尽管AI在6G-V2X中取得进展，但仍面临计算复杂度、数据隐私等技术挑战，未来需进一步研究以实现智能化的V2X生态系统。"}}
{"id": "2506.09984", "categories": ["cs.CV", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.09984", "abs": "https://arxiv.org/abs/2506.09984", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "AI": {"tldr": "提出了一种新框架，通过区域特定的条件绑定实现多概念人类动画的精确控制。", "motivation": "现有方法仅能全局控制单一主体，无法处理多概念交互场景。", "method": "利用掩码预测器自动推断布局信息，并通过迭代方式注入局部音频条件。", "result": "实验验证了显式布局控制对多模态条件的有效性。", "conclusion": "新框架实现了高质量、可控的多概念人类中心视频生成。"}}
{"id": "2506.09092", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09092", "abs": "https://arxiv.org/abs/2506.09092", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "AI": {"tldr": "论文提出了一种名为FSR的框架，结合LLMs自动生成和优化高性能CUDA程序，显著提升GPU内核的执行效率。", "motivation": "尽管LLMs在通用代码生成上表现优异，但在生成硬件特定、架构感知和高性能的GPU代码方面仍面临挑战。", "method": "提出FSR框架，联合优化编译、功能正确性和运行时性能，通过实际GPU内核执行延迟验证。", "result": "FSR增强的LLMs生成的内核在正确率上表现稳定，执行速度最高可达人工编写代码的179倍。", "conclusion": "结合LLMs与性能强化技术，有望自动化生成硬件特定、架构敏感和高性能的GPU程序。"}}
{"id": "2506.09083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "BakuFlow是一种半自动标注工具，通过像素级手动修正、交互式数据增强、标签传播和自动标注模块，显著减少计算机视觉任务中的标注工作量。", "motivation": "解决大规模计算机视觉任务中手动标注耗时且易出错的问题。", "method": "结合可调节放大镜、交互式数据增强、标签传播和基于改进YOLOE框架的自动标注模块。", "result": "BakuFlow显著提升了标注效率，适用于动态数据集和工业场景。", "conclusion": "BakuFlow为计算机视觉任务提供了一种高效、灵活的标注解决方案。"}}
{"id": "2506.09362", "categories": ["cs.HC", "cs.AI", "H.5.0"], "pdf": "https://arxiv.org/pdf/2506.09362", "abs": "https://arxiv.org/abs/2506.09362", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "comment": null, "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "AI": {"tldr": "该研究通过访谈新加坡的20名同伴支持者，探讨了数字平台在心理健康支持中的作用，并提出了文化响应式AI工具的设计方向。", "motivation": "研究动机是探索数字平台在亚洲背景下如何支持心理健康，并填补对同伴支持者实践的研究空白。", "method": "采用访谈研究方法，对20名新加坡同伴支持者进行主题分析，涵盖线上、线下和混合环境。", "result": "研究发现同伴支持者的动机、情感劳动及社会文化因素对其实践的影响，并提出了AI如何负责任地增强支持的设计方向。", "conclusion": "研究为心理健康领域提供了文化敏感的AI设计建议，强调了以人为中心的计算视角。"}}
{"id": "2506.09523", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09523", "abs": "https://arxiv.org/abs/2506.09523", "authors": ["Renjie Ma", "Ziyao Qu", "Zhijian Hu", "Dong Zhao", "Marios M. Polycarpou"], "title": "Adaptive event-triggered robust tracking control of soft robots", "comment": "8 pages, 7 figures", "summary": "Soft robots manufactured with flexible materials can be highly compliant and\nadaptive to their surroundings, which facilitates their application in areas\nsuch as dexterous manipulation and environmental exploration. This paper aims\nat investigating the tracking control problem for soft robots under uncertainty\nsuch as unmodeled dynamics and external disturbance. First, we establish a\nnovel switching function and design the compensated tracking error dynamics by\nvirtue of the command filter. Then, based on the backstepping methodology, the\nvirtual controllers and the adaptive logic estimating the supremum of\nuncertainty impacts are developed for synthesizing an event-triggered control\nstrategy. In addition, the uniformed finite-time stability certification is\nderived for different scenarios of the switching function. Finally, we perform\na case study of a soft robot to illustrate the effectiveness of the proposed\ncontrol algorithm.", "AI": {"tldr": "本文研究了软体机器人在未建模动态和外部干扰等不确定性下的跟踪控制问题，提出了一种基于事件触发的控制策略。", "motivation": "软体机器人因其高柔顺性和环境适应性在灵巧操作和环境探索中有广泛应用，但不确定性因素限制了其控制性能。", "method": "通过建立新的切换函数和补偿跟踪误差动态，结合反步法和自适应逻辑，设计了一种事件触发控制策略。", "result": "通过软体机器人的案例研究验证了所提控制算法的有效性。", "conclusion": "提出的方法能够有效处理不确定性，并实现软体机器人的稳定跟踪控制。"}}
{"id": "2506.09093", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09093", "abs": "https://arxiv.org/abs/2506.09093", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "comment": null, "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "AI": {"tldr": "提出了一种名为LwPTV的方法，通过构建显著性分数对任务向量进行层间剪枝，以提升多任务学习在域外数据集上的性能。", "motivation": "当前多任务学习模型合并方法主要关注域内性能，忽视了域外数据集的适用性。", "method": "通过构建显著性分数测量任务向量参数冗余度，生成掩码向量进行层间剪枝，保留预训练模型参数。", "result": "实验表明，该方法显著提升了域外任务性能，同时保持了域内任务的能力。", "conclusion": "LwPTV方法灵活且高效，可无缝集成现有模型合并方法，提升多任务学习的泛化能力。"}}
{"id": "2506.09106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "研究探讨生成AI模型中的偏见机制，发现偏见偏移较小，但受属性分类器影响显著，需改进评估框架。", "motivation": "生成AI模型的广泛应用引发对偏见和歧视性结果的担忧，但偏见机制尤其是无条件生成中的偏见仍未明确。", "method": "训练无条件图像生成模型，采用常用偏见评估框架分析训练与生成分布间的偏见偏移。", "result": "实验显示偏见偏移较小，但属性分类器对结果影响显著，尤其在连续属性上。", "conclusion": "需改进标签实践、深入评估框架，并考虑属性的社会复杂性以更准确评估偏见。"}}
{"id": "2506.09491", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09491", "abs": "https://arxiv.org/abs/2506.09491", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "comment": null, "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "AI": {"tldr": "DCIRNet是一种新型多模态深度补全网络，通过融合RGB图像和深度图提升透明和反射物体的深度估计质量，显著提高抓取成功率。", "motivation": "透明和反射物体的独特视觉特性导致深度传感器估计不准确，影响下游视觉任务。", "method": "提出DCIRNet，结合RGB图像和深度图，采用多模态特征融合模块和多阶段监督策略。", "result": "在公开数据集上表现优异，抓取成功率提升44%。", "conclusion": "DCIRNet有效解决了透明和反射物体的深度估计问题，具有强泛化能力。"}}
{"id": "2506.09696", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09696", "abs": "https://arxiv.org/abs/2506.09696", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi"], "title": "Patterns of Patterns III", "comment": "18 pages; submitted to Pattern Languages of Programs 2025", "summary": "Building on earlier installments, this paper re-examines the PLACARD pattern.\nWe report on a series of workshops where PLACARD was used to scaffold\ncollaborative reflection, speculative inquiry, and stimulate design pattern\ngeneration. These accounts are enriched by a comparison case: virtual workshops\ncarried out with simple AI-based chatbots. We discuss limitations and lessons\nlearned from both the human and multi-agent settings. We conclude by outlining\na future development strategy at the intersection of AI agents, design\npatterns, and institutional governance.", "AI": {"tldr": "本文重新审视了PLACARD模式，通过工作坊和虚拟聊天机器人案例的比较，探讨了其在协作反思和设计模式生成中的应用，并提出了未来发展方向。", "motivation": "研究PLACARD模式在协作反思和设计模式生成中的潜力，并探索AI代理在此过程中的作用。", "method": "通过系列工作坊和虚拟聊天机器人案例的比较分析。", "result": "揭示了PLACARD模式在协作中的优势，同时指出了人类与AI代理结合的局限性。", "conclusion": "提出了结合AI代理、设计模式和机构治理的未来发展策略。"}}
{"id": "2506.09573", "categories": ["eess.SY", "cs.SY", "math.OC", "62F15, 65K10, 93E10", "G.3; F.2.1; I.1.2"], "pdf": "https://arxiv.org/pdf/2506.09573", "abs": "https://arxiv.org/abs/2506.09573", "authors": ["Dominik Friml", "Pavel Václavek"], "title": "Probability-One Optimization of Generalized Rayleigh Quotient Sum For Multi-Source Generalized Total Least-Squares", "comment": "This is the preprint version prior to peer review", "summary": "This paper addresses the global optimization of the sum of the Rayleigh\nquotient and the generalized Rayleigh quotient on the unit sphere. While\nvarious methods have been proposed for this problem, they do not guarantee\nconvergence to the global maximizer. To overcome this limitation, we introduce\na probability-one homotopy optimization method that, under certain conditions,\nguarantees convergence to the global maximizer. The proposed method is analyzed\nalongside state-of-the-art approaches through numerical experiments, evaluating\ntheir performance in terms of convergence speed and ability to reach the global\nmaximizer. Furthermore, we demonstrate how this ties in with the multi-source\nBayesian Generalized Total Least-Squares (B-GTLS) problem, illustrating its\napplicability.", "AI": {"tldr": "本文提出了一种概率为一的同伦优化方法，用于在单位球面上全局优化瑞利商和广义瑞利商的和，确保在特定条件下收敛到全局最大值。", "motivation": "现有方法无法保证收敛到全局最大值，因此需要一种更可靠的方法来解决这一问题。", "method": "引入概率为一的同伦优化方法，并通过数值实验与现有方法进行比较。", "result": "该方法在收敛速度和达到全局最大值的能力上表现优异，并与B-GTLS问题相关。", "conclusion": "提出的方法在全局优化问题中具有优越性，并展示了其在实际问题中的适用性。"}}
{"id": "2506.09096", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09096", "abs": "https://arxiv.org/abs/2506.09096", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "title": "Intra-Trajectory Consistency for Reward Modeling", "comment": "Under review", "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "AI": {"tldr": "本文提出了一种利用生成概率增强奖励模型的方法，通过响应轨迹中的过程一致性提高奖励学习的细粒度信号。", "motivation": "当前奖励模型依赖粗粒度的响应级评分，难以识别与评分真正相关的具体组件，导致泛化能力差。", "method": "利用生成概率建立响应轨迹中过程间的一致性，提出内部轨迹一致性正则化方法。", "result": "改进的奖励模型在RewardBench上表现更优，同时提升了DPO对齐策略和BON推理验证结果。", "conclusion": "通过细粒度信号和一致性正则化，显著提升了奖励模型的泛化能力和性能。"}}
{"id": "2506.09109", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "CAIRe是一种新的评估指标，用于衡量图像的文化相关性，解决了跨文化偏见测量难题，并在性能上优于基线方法28%。", "motivation": "文本到图像模型在多元文化背景下的公平性能需求迫切，但现有方法存在性能损失、事实错误或冒犯性输出等问题，且缺乏可靠的偏见测量工具。", "method": "提出CAIRe框架，通过将图像中的实体和概念与知识库关联，利用事实信息对每个文化标签进行独立分级评估。", "result": "CAIRe在手动构建的文化显著但罕见的数据集上优于基线28% F1分数，并在两个文化通用概念数据集上与人类评分的相关性达到0.56和0.66。", "conclusion": "CAIRe能有效衡量图像的文化相关性，与人类判断高度一致，适用于多样化的图像来源。"}}
{"id": "2506.09494", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09494", "abs": "https://arxiv.org/abs/2506.09494", "authors": ["Alberto San-Miguel-Tello", "Gennaro Scarati", "Alejandro Hernández", "Mario Cavero-Vidal", "Aakash Maroti", "Néstor García"], "title": "Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications", "comment": "7 pages, 2 figures", "summary": "This paper presents advances on the Universal Manipulation Interface (UMI), a\nlow-cost hand-held gripper for robot Learning from Demonstration (LfD), for\ncomplex in-the-wild scenarios found in agricultural settings. The focus is on\nimproving the acquisition of suitable samples with minimal additional setup.\nFirstly, idle times and user's cognitive load are reduced through the\nextraction of individual samples from a continuous demonstration considering\ntask events. Secondly, reliability on the generation of task sample's\ntrajectories is increased through the combination on-board inertial\nmeasurements and external visual marker localization usage using Extended\nKalman Filtering (EKF). Results are presented for a fruit harvesting task,\noutperforming the default pipeline.", "AI": {"tldr": "UMI改进为低成本手持夹爪，用于农业场景中的机器人学习演示，通过任务事件提取样本和结合惯性测量与视觉定位提升性能。", "motivation": "解决农业复杂场景中机器人学习演示的样本采集问题，减少空闲时间和用户认知负担。", "method": "1. 从连续演示中提取任务事件样本；2. 结合惯性测量和视觉标记定位（EKF）生成轨迹。", "result": "在水果采摘任务中表现优于默认流程。", "conclusion": "UMI改进方法有效提升了样本采集和轨迹生成的可靠性。"}}
{"id": "2506.09801", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.09801", "abs": "https://arxiv.org/abs/2506.09801", "authors": ["Qihan Yang", "Xin Zhou", "Adam J. Spiers"], "title": "Investigating the Perception of Translational Shape-Changing Haptic Interfaces", "comment": "7 pages, 8 figures. Accepted version to appear in: Proceedings of the\n  IEEE World Haptics Conference (WHC), 2025", "summary": "Shape-changing haptic interfaces (SCHIs) are a promising and emerging field.\nHowever, compared to more established stimulus modalities, such as vibration,\nthere is sparse literature on the perception of dynamic shapes. Furthermore,\nthe influence of properties such as grasp types and displacement\nmagnitude/direction has not been formally evaluated. This work attempts to\ninitiate a formal perceptual evaluation of SCHIs via a psychophysical user\nstudy involving a 1-DOF translational shape-changing interface that can move\nits body with 1.25-micrometer resolution. Participants completed a Method of\nConstant Stimulus study while holding the device with three different grasps.\nStimuli direction occurred both toward and away from the thumb, while the\nstandard stimuli varied between small (0.48 mm) and large (6 mm). Our results\nindicate that translational SCHIs should maximize the translation magnitude\nrather than the number of fingers in contact. We also demonstrated how to apply\nour findings to real-world applications via a simple 'paddle game', where we\ncompared conventional linear mapping with non-linear mapping derived from our\nperceptual experiment outcomes between the device position and its represented\nvalue. Results indicate that the non-linear mapping was more effective, with\nimproved error distribution. We hope this work inspires further formal\nperceptual investigation into other SCHI morphologies.", "AI": {"tldr": "本文通过心理物理学用户研究，评估了1-DOF平移形变触觉界面（SCHI）的感知特性，探讨了抓握类型和位移大小/方向的影响，并提出了优化建议。", "motivation": "形变触觉界面（SCHIs）是一个新兴领域，但关于动态形状感知的研究较少，且抓握类型和位移特性的影响尚未正式评估。", "method": "采用心理物理学用户研究，使用1-DOF平移SCHI设备，参与者以三种不同抓握方式完成恒定刺激法实验，测试不同位移大小和方向。", "result": "结果表明，平移SCHI应最大化位移幅度而非接触手指数量；非线性的设备位置映射在实际应用中更有效。", "conclusion": "本研究为SCHI的感知特性提供了初步评估，并希望激发对其他SCHI形态的进一步研究。"}}
{"id": "2506.09685", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.09685", "abs": "https://arxiv.org/abs/2506.09685", "authors": ["Armin Gießler", "Albertus Johannes Malan", "Sören Hohmann"], "title": "Bridging Continuous-time LQR and Reinforcement Learning via Gradient Flow of the Bellman Error", "comment": "submitted to Conference on Decision and Control", "summary": "In this paper, we present a novel method for computing the optimal feedback\ngain of the infinite-horizon Linear Quadratic Regulator (LQR) problem via an\nordinary differential equation. We introduce a novel continuous-time Bellman\nerror, derived from the Hamilton-Jacobi-Bellman (HJB) equation, which\nquantifies the suboptimality of stabilizing policies and is parametrized in\nterms of the feedback gain. We analyze its properties, including its effective\ndomain, smoothness, coerciveness and show the existence of a unique stationary\npoint within the stability region. Furthermore, we derive a closed-form\ngradient expression of the Bellman error that induces a gradient flow. This\nconverges to the optimal feedback and generates a unique trajectory which\nexclusively comprises stabilizing feedback policies. Additionally, this work\nadvances interesting connections between LQR theory and Reinforcement Learning\n(RL) by redefining suboptimality of the Algebraic Riccati Equation (ARE) as a\nBellman error, adapting a state-independent formulation, and leveraging\nLyapunov equations to overcome the infinite-horizon challenge. We validate our\nmethod in a simulation and compare it to the state of the art.", "AI": {"tldr": "本文提出了一种通过常微分方程计算无限时域线性二次调节器（LQR）最优反馈增益的新方法，并分析了其性质与收敛性。", "motivation": "传统方法在求解LQR问题时存在局限性，需要一种更高效且理论完备的方法。", "method": "引入基于HJB方程的连续时间贝尔曼误差，分析其性质并推导闭式梯度表达式，生成梯度流以收敛至最优反馈。", "result": "方法在仿真中验证，并优于现有技术。", "conclusion": "该方法不仅解决了LQR问题，还为强化学习提供了新的理论连接。"}}
{"id": "2506.09099", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09099", "abs": "https://arxiv.org/abs/2506.09099", "authors": ["Joshua Barron", "Devin White"], "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "AI": {"tldr": "研究探讨了大语言模型中记忆与泛化的关系，发现模型容量与学习模式之间存在权衡。", "motivation": "探索大语言模型中记忆与泛化的关系，以理解模型容量如何影响学习行为。", "method": "通过预训练容量受限的Transformer模型，在合成字符级任务上分别测试泛化和记忆能力。", "result": "小模型泛化能力强但记忆能力弱，大模型反之；中等容量模型倾向于记忆；联合训练时无模型能成功泛化。", "conclusion": "预训练可能固有地偏向某种学习模式，研究为小语言模型的设计提供了启示。"}}
{"id": "2506.09113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "Seedance 1.0是一款高性能视频生成基础模型，通过多源数据、高效架构设计和优化训练方法，显著提升了视频生成的提示跟随、运动合理性和视觉质量。", "motivation": "当前基础模型在视频生成中难以平衡提示跟随、运动合理性和视觉质量，Seedance 1.0旨在解决这一问题。", "method": "采用多源数据增强、高效架构设计、联合学习多任务、精细化训练优化和模型加速策略。", "result": "Seedance 1.0在1080p分辨率下5秒视频生成仅需41.4秒，具有高质量、快速生成和优秀时空流畅性。", "conclusion": "Seedance 1.0在复杂多主体场景中表现优异，支持多镜头叙事连贯性，是视频生成领域的重大突破。"}}
{"id": "2506.09548", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09548", "abs": "https://arxiv.org/abs/2506.09548", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "comment": "Robotics and Automation Letters", "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "AI": {"tldr": "提出了一种基于LiDAR-IMU-腿里程计的紧耦合方法，通过在线学习腿部运动学模型，适应复杂环境（如无特征区域和可变形地形）。", "motivation": "解决在无特征环境和可变形地形下里程计的鲁棒性问题，提升机器人适应负载变化和地形条件的能力。", "method": "开发了基于神经网络的腿部运动学模型，结合触觉信息，通过因子图联合优化在线训练和里程计估计。", "result": "在沙地和校园等复杂环境中验证，性能优于现有方法。", "conclusion": "该方法显著提升了里程计的鲁棒性和适应性，适用于复杂任务场景。"}}
{"id": "2506.09968", "categories": ["cs.HC", "I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.09968", "abs": "https://arxiv.org/abs/2506.09968", "authors": ["Wentao Ge", "Yuqing Sun", "Ziyan Wang", "Haoyue Zheng", "Weiyang He", "Piaohong Wang", "Qianyu Zhu", "Benyou Wang"], "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance", "comment": "14 pages", "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.", "AI": {"tldr": "论文研究了大学生自我调节学习（SRL）的挑战，并开发了SRLAgent系统，通过游戏化和LLM支持提升SRL技能，实验证明其有效性。", "motivation": "大学生缺乏SRL技能会导致学习习惯混乱、动力不足和时间管理问题，影响学业表现。", "method": "基于Zimmerman的三阶段SRL框架，开发了SRLAgent系统，结合游戏化和LLM实时反馈，通过实验对比评估效果。", "result": "SRLAgent组在SRL技能上有显著提升（p < .001, Cohen's d = 0.234），且比对照组更受欢迎。", "conclusion": "研究表明，将SRL支持和AI实时反馈嵌入游戏化环境，对教育技术设计有重要启示。"}}
{"id": "2506.09101", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09101", "abs": "https://arxiv.org/abs/2506.09101", "authors": ["Míriam Barrabés", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "title": "Feature Shift Localization Network", "comment": "9 pages, 2 figures, 4 tables", "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "AI": {"tldr": "FSL-Net是一种神经网络方法，用于在大规模高维数据中快速准确地定位特征偏移。", "motivation": "解决异构数据源、噪声测量或不一致处理导致特征偏移的问题，避免下游分析性能下降。", "method": "提出FSL-Net，通过训练学习数据集的统计特性，无需重新训练即可定位新数据集中的特征偏移。", "result": "FSL-Net能够高效且准确地定位特征偏移，适用于大规模高维数据。", "conclusion": "FSL-Net为解决特征偏移定位问题提供了一种可扩展且高效的解决方案。"}}
{"id": "2506.09229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "论文提出了一种名为CREPA的新方法，用于改进视频扩散模型（VDMs）的微调，通过跨帧表示对齐提升视觉保真度和语义一致性。", "motivation": "用户级微调VDMs以生成反映特定训练数据属性的视频具有实际重要性，但现有方法在保持帧间语义一致性方面表现不佳。", "method": "提出CREPA方法，通过将帧的隐藏状态与相邻帧的外部特征对齐，优化VDMs的微调效果。", "result": "在CogVideoX-5B和Hunyuan Video等大规模VDMs上，CREPA显著提升了视觉质量和帧间语义一致性。", "conclusion": "CREPA是一种广泛适用的正则化技术，有效解决了VDMs微调中的语义一致性问题。"}}
{"id": "2506.09552", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09552", "abs": "https://arxiv.org/abs/2506.09552", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "comment": "Preprint, Journal of Intelligent & Robotic Systems", "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "AI": {"tldr": "本文提出了一种用于3D点云数据语义分割的Sim2Real域适应方法，专注于人机协作（HRC）应用。通过结合DGCNN和CNN的双流网络架构（FUSION），在工业环境中实现了高精度和鲁棒性。", "motivation": "在HRC应用中，安全性和操作效率至关重要，而语义分割是实现环境理解的关键。由于真实工业标注数据的稀缺，需要一种能够从模拟环境适应到真实场景的方法。", "method": "提出了一种双流网络架构（FUSION），结合了动态图卷积神经网络（DGCNN）和卷积神经网络（CNN），并增加了残差层，用于Sim2Real域适应。", "result": "在真实HRC和模拟工业点云数据上评估，模型达到了97.76%的分割准确率，性能优于现有方法。", "conclusion": "该方法显著提升了语义分割在HRC中的实用性，为安全人机协作提供了有效支持。"}}
{"id": "2506.09104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09104", "abs": "https://arxiv.org/abs/2506.09104", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "comment": "Preprint", "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "AI": {"tldr": "论文提出了一种名为UPQ的渐进量化框架，用于将指令调优的大语言模型（LLM）从FP16量化到INT2，结合了块级后训练量化和蒸馏量化感知训练，实现了高性能的2-bit量化。", "motivation": "随着大语言模型规模的快速扩展，资源受限设备上的部署面临挑战，现有2-bit量化方法仅限于预训练模型，未扩展到指令调优模型。", "method": "UPQ框架分两步：1）使用块级后训练量化（PTQ）将FP16模型量化到INT4；2）通过蒸馏量化感知训练（Distill-QAT）进一步量化到INT2，最小化广义Jensen-Shannon散度（JSD）。", "result": "UPQ首次实现了开源指令调优LLM的INT2量化，无需专有数据，在MMLU和IFEval基准测试中达到最优性能。", "conclusion": "UPQ为指令调优LLM的极低比特量化提供了有效解决方案，填补了现有研究的空白。"}}
{"id": "2506.09237", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "PatchGuard是一种基于Vision Transformer的对抗鲁棒异常检测与定位方法，通过引入伪异常样本和定位掩码，显著提升了对抗攻击下的性能。", "motivation": "当前异常检测与定位方法因训练数据限制（仅包含正常样本）易受对抗攻击影响，需开发更鲁棒的方法。", "method": "提出Foreground-Aware Pseudo-Anomalies，结合ViT架构和对抗训练，使用新损失函数增强模型鲁棒性。", "result": "在工业和医学数据集上，PatchGuard在对抗环境下AD性能提升53.2%，AL提升68.5%，且在非对抗环境下仍具竞争力。", "conclusion": "PatchGuard通过伪异常样本和理论指导的对抗训练，显著提升了异常检测与定位的对抗鲁棒性。"}}
{"id": "2506.09581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09581", "abs": "https://arxiv.org/abs/2506.09581", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "David Sobrín-Hidalgo", "Ángel Manuel Guerrero-Higueras", "Vicente MatellÁn-Olivera"], "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities", "comment": "10 pages, 4 figures, Submitted to 3rd edition of the Workshop on\n  Ontologies and Standards for Robotics and Automation (WOSRA) at ICRA 2024", "summary": "Large Language Models (LLMs) have experienced great advancements in the last\nyear resulting in an increase of these models in several fields to face natural\nlanguage tasks. The integration of these models in robotics can also help to\nimprove several aspects such as human-robot interaction, navigation, planning\nand decision-making. Therefore, this paper introduces llama\\_ros, a tool\ndesigned to integrate quantized Large Language Models (LLMs) into robotic\nsystems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,\nllama\\_ros enables the efficient execution of quantized LLMs as edge artificial\nintelligence (AI) in robotics systems with resource-constrained environments,\naddressing the challenges of computational efficiency and memory limitations.\nBy deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural\nlanguage understanding and generation for enhanced decision-making and\ninteraction which can be paired with prompt engineering, knowledge graphs,\nontologies or other tools to improve the capabilities of autonomous robots.\nAdditionally, this paper provides insights into some use cases of using\nllama\\_ros for planning and explainability in robotics.", "AI": {"tldr": "llama_ros工具将量化大型语言模型（LLMs）集成到ROS 2机器人系统中，提升自然语言处理能力，适用于资源受限环境。", "motivation": "通过LLMs提升机器人在人机交互、导航、规划和决策等方面的能力。", "method": "利用llama.cpp高效运行量化LLMs，结合ROS 2实现边缘AI部署。", "result": "llama_ros成功支持机器人在资源受限环境中高效运行LLMs，增强决策和交互能力。", "conclusion": "llama_ros为机器人系统提供了高效的自然语言处理工具，扩展了自主机器人的应用潜力。"}}
{"id": "2506.09876", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.09876", "abs": "https://arxiv.org/abs/2506.09876", "authors": ["Jisheng Xu", "Ding Lin", "Pangkit Fong", "Chongrong Fang", "Xiaoming Duan", "Jianping He"], "title": "Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization", "comment": null, "summary": "This paper introduces an underwater multi-robot platform, named Aucamp,\ncharacterized by cost-effective monocular-camera-based sensing, distributed\nprotocol and robust orientation control for localization. We utilize the\nclarity feature to measure the distance, present the monocular imaging model,\nand estimate the position of the target object. We achieve global positioning\nin our platform by designing a distributed update protocol. The distributed\nalgorithm enables the perception process to simultaneously cover a broader\nrange, and greatly improves the accuracy and robustness of the positioning.\nMoreover, the explicit dynamics model of the robot in our platform is obtained,\nbased on which, we propose a robust orientation control framework. The control\nsystem ensures that the platform maintains a balanced posture for each robot,\nthereby ensuring the stability of the localization system. The platform can\nswiftly recover from an forced unstable state to a stable horizontal posture.\nAdditionally, we conduct extensive experiments and application scenarios to\nevaluate the performance of our platform. The proposed new platform may provide\nsupport for extensive marine exploration by underwater sensor networks.", "AI": {"tldr": "本文介绍了一种名为Aucamp的水下多机器人平台，具有低成本单目相机感知、分布式协议和鲁棒姿态控制的特点，用于定位。", "motivation": "为水下传感器网络提供广泛海洋探索支持，解决水下机器人的定位和稳定性问题。", "method": "利用单目相机清晰度特征测距，设计分布式更新协议实现全局定位，提出鲁棒姿态控制框架。", "result": "平台能同时覆盖更广范围，显著提高定位精度和鲁棒性，并能快速从非稳定状态恢复。", "conclusion": "Aucamp平台为水下多机器人系统提供了一种高效、稳定的解决方案，适用于海洋探索。"}}
{"id": "2506.09105", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.09105", "abs": "https://arxiv.org/abs/2506.09105", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "comment": null, "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "AI": {"tldr": "MetaTT是一个统一的Tensor Train适配器框架，用于预训练Transformer的全局低秩微调，相比LoRA参数更少且性能相当。", "motivation": "解决LoRA独立微调权重矩阵导致参数冗余的问题，提出共享TT分解的全局低秩微调方法。", "method": "使用单一共享TT分解所有Transformer子模块，通过索引结构轴（如层和矩阵类型）实现参数压缩。", "result": "在标准语言建模基准测试中，MetaTT参数显著减少，性能与LoRA相当甚至优于其他张量方法。", "conclusion": "MetaTT通过TT分解和成熟优化方法，实现了高效、可扩展的多任务共享适配器设计。"}}
{"id": "2506.09278", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "UFM模型通过统一训练在光流和宽基线匹配任务中表现优异，准确率提升28%，误差减少62%，速度提升6.7倍。", "motivation": "解决传统方法在光流估计和宽基线匹配中分离处理的问题，探索统一训练的可能性。", "method": "使用简单的通用Transformer架构，直接回归(u,v)流，避免传统的粗到细成本体积方法。", "result": "UFM在光流任务中比Unimatch准确率高28%，在宽基线匹配中比RoMa误差少62%且快6.7倍。", "conclusion": "统一训练可以超越专门化方法，为多模态、长距离和实时对应任务开辟新方向。"}}
{"id": "2506.09583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09583", "abs": "https://arxiv.org/abs/2506.09583", "authors": ["Miguel Á. González-Santamarta", "Francisco J. Rodríguez-Lera", "Vicente Matellán-Olivera"], "title": "VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots", "comment": "15 pages, 5 figures, Submitted to WAF 2023: Workshop de Agentes\n  Fisicos", "summary": "Localization plays a crucial role in the navigation capabilities of\nautonomous robots, and while indoor environments can rely on wheel odometry and\n2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,\npresent unique challenges that necessitate real-time localization and\nconsistent mapping. Addressing this need, this paper introduces the VAULT\nprototype, a ROS 2-based mobile mapping system (MMS) that combines various\nsensors to enable robust outdoor and indoor localization. The proposed solution\nharnesses the power of Global Navigation Satellite System (GNSS) data,\nvisual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the\nExtended Kalman Filter (EKF) to generate reliable 3D odometry. To further\nenhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting\nin the creation of a comprehensive 3D point cloud map. By leveraging these\nsensor technologies and advanced algorithms, the prototype offers a\ncomprehensive solution for outdoor localization in autonomous mobile robots,\nenabling them to navigate and map their surroundings with confidence and\nprecision.", "AI": {"tldr": "论文提出了一种基于ROS 2的移动测绘系统（VAULT），结合多种传感器和算法，实现室内外环境的鲁棒定位和3D建图。", "motivation": "解决户外环境（如农业和林业）中自主机器人实时定位和一致建图的挑战。", "method": "结合GNSS、VIO、IMU和EKF生成可靠3D里程计，并利用VSLAM提高定位精度，构建3D点云地图。", "result": "开发了VAULT原型，能够为自主移动机器人提供高精度的户外定位和建图能力。", "conclusion": "VAULT系统通过多传感器融合和先进算法，为户外自主导航提供了全面解决方案。"}}
{"id": "2506.09108", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09108", "abs": "https://arxiv.org/abs/2506.09108", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "title": "SensorLM: Learning the Language of Wearable Sensors", "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "AI": {"tldr": "SensorLM是一种传感器-语言基础模型，通过自然语言理解可穿戴传感器数据，解决了传感器数据与语言对齐的挑战，并在多个任务中表现优异。", "motivation": "解决传感器数据与自然语言对齐的困难，尤其是在缺乏标注数据的情况下。", "method": "采用分层标题生成管道，结合统计、结构和语义信息，并扩展多模态预训练架构（如CLIP、CoCa）。", "result": "构建了最大的传感器-语言数据集（59.7百万小时数据），在零样本识别、少样本学习和跨模态检索等任务中表现优于现有技术。", "conclusion": "SensorLM在传感器数据理解和语言对齐方面具有显著优势，并展示了扩展性和泛化能力。"}}
{"id": "2506.09299", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "本文提出了一种轻量级且节能的空中应急图像目标检测方案，采用YOLOv4-Tiny模型并通过INT8量化优化，在自定义数据集上验证其性能优于YOLOv5-small。", "motivation": "现有公开数据集中缺乏无人机视角的应急图像，因此需要构建专用数据集并优化模型以适应低功耗边缘设备的实时检测需求。", "method": "使用YOLOv4-Tiny模型，通过INT8量化优化，并在自定义的10,820张标注图像数据集上进行训练。", "result": "量化后的模型大小减少71%（22.5 MB到6.4 MB），推理速度提升44%，检测性能与YOLOv5-small相当。", "conclusion": "量化YOLOv4-Tiny模型适合在低功耗边缘设备上实现实时应急检测。"}}
{"id": "2506.09588", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09588", "abs": "https://arxiv.org/abs/2506.09588", "authors": ["Junzhe He", "Chong Zhang", "Fabian Jenelten", "Ruben Grandia", "Moritz BÄcher", "Marco Hutter"], "title": "Attention-Based Map Encoding for Learning Generalized Legged Locomotion", "comment": "Original draft prior to peer review. Significant revisions and new\n  materials are expected after formal publication release", "summary": "Dynamic locomotion of legged robots is a critical yet challenging topic in\nexpanding the operational range of mobile robots. It requires precise planning\nwhen possible footholds are sparse, robustness against uncertainties and\ndisturbances, and generalizability across diverse terrains. While traditional\nmodel-based controllers excel at planning on complex terrains, they struggle\nwith real-world uncertainties. Learning-based controllers offer robustness to\nsuch uncertainties but often lack precision on terrains with sparse steppable\nareas. Hybrid methods achieve enhanced robustness on sparse terrains by\ncombining both methods but are computationally demanding and constrained by the\ninherent limitations of model-based planners. To achieve generalized legged\nlocomotion on diverse terrains while preserving the robustness of\nlearning-based controllers, this paper proposes to learn an attention-based map\nencoding conditioned on robot proprioception, which is trained as part of the\nend-to-end controller using reinforcement learning. We show that the network\nlearns to focus on steppable areas for future footholds when the robot\ndynamically navigates diverse and challenging terrains. We synthesize behaviors\nthat exhibit robustness against uncertainties while enabling precise and agile\ntraversal of sparse terrains. Additionally, our method offers a way to\ninterpret the topographical perception of a neural network. We have trained two\ncontrollers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot\nrespectively and tested the resulting controllers in the real world under\nvarious challenging indoor and outdoor scenarios, including ones unseen during\ntraining.", "AI": {"tldr": "提出了一种基于注意力机制的地图编码方法，结合机器人本体感知，通过强化学习训练端到端控制器，实现四足和双足机器人在复杂地形上的鲁棒动态运动。", "motivation": "传统模型控制器在复杂地形规划上表现优异但难以应对现实不确定性，而学习控制器虽鲁棒但在地形稀疏时精度不足。混合方法计算量大且受限于模型规划器的固有缺陷。", "method": "学习基于注意力的地图编码，结合机器人本体感知，通过强化学习训练端到端控制器。", "result": "网络学会在动态导航时聚焦可落脚区域，实现鲁棒性和精确性。控制器在训练未见的室内外场景中表现优异。", "conclusion": "该方法结合了学习控制器的鲁棒性和模型控制器的精确性，为神经网络地形感知提供可解释性。"}}
{"id": "2506.09110", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09110", "abs": "https://arxiv.org/abs/2506.09110", "authors": ["Jingying Ma", "Feng Wu", "Qika Lin", "Yucheng Xing", "Chenyu Liu", "Ziyu Jia", "Mengling Feng"], "title": "CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model", "comment": null, "summary": "Electroencephalography (EEG) provides real-time insights into brain activity\nand is widely used in neuroscience. However, variations in channel\nconfigurations, sequence lengths, and task objectives limit the transferability\nof traditional task-specific models. Although recent EEG foundation models\n(EFMs) aim to learn generalizable representations, they struggle with limited\nheterogeneous representation capacity and inefficiency in capturing multi-scale\nbrain dependencies. To address these challenges, we propose CodeBrain, an\nefficient EFM structurally aligned with brain organization, trained in two\nstages. (1) We introduce a TFDual-Tokenizer that independently tokenizes\nheterogeneous temporal and frequency components, enabling a quadratic expansion\nof the discrete representation space. This also offers a degree of\ninterpretability through cross-domain token analysis. (2) We propose the\nEEGSSM, which combines a structured global convolution architecture and a\nsliding window attention mechanism to jointly model sparse long-range and local\ndependencies. Unlike fully connected Transformer models, EEGSSM better reflects\nthe brain's small-world topology and efficiently captures EEG's inherent\nmulti-scale structure. EEGSSM is trained with a masked self-supervised learning\nobjective to predict token indices obtained in TFDual-Tokenizer. Comprehensive\nexperiments on 10 public EEG datasets demonstrate the generalizability of\nCodeBrain with linear probing. By offering biologically informed and\ninterpretable EEG modeling, CodeBrain lays the foundation for future\nneuroscience research. Both code and pretraining weights will be released in\nthe future version.", "AI": {"tldr": "CodeBrain是一种高效的EEG基础模型，通过TFDual-Tokenizer和EEGSSM解决传统模型在EEG数据中的泛化性和多尺度依赖问题。", "motivation": "传统EEG模型因通道配置、序列长度和任务目标差异导致泛化性差，现有基础模型在异构表示和多尺度依赖建模上效率不足。", "method": "提出两阶段训练方法：1) TFDual-Tokenizer独立编码时频分量，扩展表示空间；2) EEGSSM结合全局卷积和滑动窗口注意力，建模多尺度依赖。", "result": "在10个公开EEG数据集上验证了CodeBrain的泛化能力，并通过线性探测展示了其性能。", "conclusion": "CodeBrain为EEG建模提供了生物启发和可解释的方法，为未来神经科学研究奠定了基础。"}}
{"id": "2506.09300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "论文展示了量化YOLOv4-Tiny模型在资源受限设备（Raspberry Pi 5）上的部署与性能评估，用于实时检测航空紧急图像中的目标。", "motivation": "研究旨在验证量化模型在边缘设备上的实时性和低功耗表现，以支持紧急响应应用。", "method": "使用TensorFlow Lite后训练量化技术将YOLOv4-Tiny模型量化为INT8精度，并评估其在嵌入式条件下的检测速度、功耗和热可行性。", "result": "量化模型每张图像推理时间为28.2毫秒，平均功耗13.85瓦，相比FP32版本显著降低功耗，同时保持对关键紧急类别的检测准确性。", "conclusion": "结果表明低功耗嵌入式AI系统在安全关键紧急响应应用中具有实时部署潜力。"}}
{"id": "2506.09623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09623", "abs": "https://arxiv.org/abs/2506.09623", "authors": ["Lipei Xie", "Yingxin Li", "Huiping Zhuang"], "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "comment": null, "summary": "Embodied foundation models are crucial for Artificial Intelligence (AI)\ninteracting with the physical world by integrating multi-modal inputs, such as\nproprioception, vision and language, to understand human intentions and\ngenerate actions to control robots. While these models demonstrate strong\ngeneralization and few-shot learning capabilities, they face significant\nchallenges in continually acquiring new skills without forgetting previously\nlearned skills, a problem known as catastrophic forgetting. To address this\nissue, we propose the Analytic Task Scheduler (ATS), a novel framework for\ncontinual learning in embodied foundation models. ATS consists of a\ntask-specific model library, where each model is fine-tuned independently on a\nsingle task, and an analytic scheduler trained using recursive least squares\n(RLS) to learn the mapping between language instructions and task-specific\nmodels. This architecture enables accurate task recognition and dynamic model\nselection while fundamentally avoiding parameter interference across tasks. The\nscheduler updates its parameters incrementally using only statistics\n(autocorrelation and cross-correlation matrices), enabling forgetting-resistant\nlearning without the need to revisit historical data. We validate ATS on a\nreal-world robot platform (RM65B), demonstrating superior resistance to\nforgetting and strong adaptability to task variations. The results highlight\nATS as an effective, scalable, and deployable solution for continual learning\nin embodied foundation models operating in complex, dynamic environments. Our\ncode will be available at\nhttps://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler", "AI": {"tldr": "论文提出了一种名为ATS的新框架，用于解决具身基础模型中的持续学习问题，通过任务特定模型库和分析调度器避免灾难性遗忘。", "motivation": "具身基础模型在整合多模态输入以理解人类意图和控制机器人方面表现出色，但面临持续学习新技能时遗忘旧技能的挑战。", "method": "ATS框架包括任务特定模型库和分析调度器，调度器使用递归最小二乘法（RLS）学习语言指令与任务模型的映射，避免参数干扰。", "result": "在真实机器人平台（RM65B）上验证，ATS表现出优异的抗遗忘能力和任务适应能力。", "conclusion": "ATS是一种高效、可扩展且可部署的解决方案，适用于复杂动态环境中的具身基础模型持续学习。"}}
{"id": "2506.09114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09114", "abs": "https://arxiv.org/abs/2506.09114", "authors": ["Jialin Chen", "Ziyu Zhao", "Gaukhar Nurbek", "Aosong Feng", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval", "comment": null, "summary": "The ubiquity of dynamic data in domains such as weather, healthcare, and\nenergy underscores a growing need for effective interpretation and retrieval of\ntime-series data. These data are inherently tied to domain-specific contexts,\nsuch as clinical notes or weather narratives, making cross-modal retrieval\nessential not only for downstream tasks but also for developing robust\ntime-series foundation models by retrieval-augmented generation (RAG). Despite\nthe increasing demand, time-series retrieval remains largely underexplored.\nExisting methods often lack semantic grounding, struggle to align heterogeneous\nmodalities, and have limited capacity for handling multi-channel signals. To\naddress this gap, we propose TRACE, a generic multimodal retriever that grounds\ntime-series embeddings in aligned textual context. TRACE enables fine-grained\nchannel-level alignment and employs hard negative mining to facilitate\nsemantically meaningful retrieval. It supports flexible cross-modal retrieval\nmodes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking\nlinguistic descriptions with complex temporal patterns. By retrieving\nsemantically relevant pairs, TRACE enriches downstream models with informative\ncontext, leading to improved predictive accuracy and interpretability. Beyond a\nstatic retrieval engine, TRACE also serves as a powerful standalone encoder,\nwith lightweight task-specific tuning that refines context-aware\nrepresentations while maintaining strong cross-modal alignment. These\nrepresentations achieve state-of-the-art performance on downstream forecasting\nand classification tasks. Extensive experiments across multiple domains\nhighlight its dual utility, as both an effective encoder for downstream\napplications and a general-purpose retriever to enhance time-series models.", "AI": {"tldr": "TRACE是一种多模态检索器，通过文本与时间序列数据的对齐，实现细粒度检索，提升下游任务的预测准确性和可解释性。", "motivation": "动态数据（如天气、医疗和能源领域）的普遍性需要有效的时间序列数据检索方法，现有方法缺乏语义基础且难以处理多模态对齐。", "method": "提出TRACE，通过文本上下文对齐时间序列嵌入，支持细粒度通道级对齐和硬负样本挖掘，实现跨模态检索。", "result": "TRACE在下游预测和分类任务中达到最先进性能，同时作为通用检索器增强时间序列模型。", "conclusion": "TRACE不仅是一个强大的检索器，还能作为独立编码器，为时间序列模型提供上下文感知表示。"}}
{"id": "2506.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "提出了一种多模态自监督学习框架，通过自适应掩码策略和多任务目标，显著提升了遥感图像解释任务的性能。", "motivation": "解决高质量标注数据获取成本高的问题，利用多模态数据提升模型性能。", "method": "采用多模态自监督学习框架，结合RGB图像、多光谱数据和DSM，设计自适应掩码策略和跨模态掩码机制。", "result": "在15个数据集上验证，性能优于现有方法，如Potsdam和Vaihingen语义分割任务mIoU达78.30%和76.50%。", "conclusion": "该方法在多任务遥感应用中表现优异，代码和数据集已开源。"}}
{"id": "2506.09629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09629", "abs": "https://arxiv.org/abs/2506.09629", "authors": ["Maurice Brunner", "Edoardo Ghignone", "Nicolas Baumann", "Michele Magno"], "title": "R-CARLA: High-Fidelity Sensor Simulations with Interchangeable Dynamics for Autonomous Racing", "comment": null, "summary": "Autonomous racing has emerged as a crucial testbed for autonomous driving\nalgorithms, necessitating a simulation environment for both vehicle dynamics\nand sensor behavior. Striking the right balance between vehicle dynamics and\nsensor accuracy is crucial for pushing vehicles to their performance limits.\nHowever, autonomous racing developers often face a trade-off between accurate\nvehicle dynamics and high-fidelity sensor simulations. This paper introduces\nR-CARLA, an enhancement of the CARLA simulator that supports holistic\nfull-stack testing, from perception to control, using a single system. By\nseamlessly integrating accurate vehicle dynamics with sensor simulations,\nopponents simulation as NPCs, and a pipeline for creating digital twins from\nreal-world robotic data, R-CARLA empowers researchers to push the boundaries of\nautonomous racing development. Furthermore, it is developed using CARLA's rich\nsuite of sensor simulations. Our results indicate that incorporating the\nproposed digital-twin framework into R-CARLA enables more realistic full-stack\ntesting, demonstrating a significant reduction in the Sim-to-Real gap of car\ndynamics simulation by 42% and by 82% in the case of sensor simulation across\nvarious testing scenarios.", "AI": {"tldr": "R-CARLA是CARLA模拟器的增强版，支持从感知到控制的全栈测试，通过整合车辆动力学和传感器模拟，显著缩小了仿真与现实的差距。", "motivation": "自动驾驶赛车开发中，车辆动力学与传感器模拟的平衡是关键，但现有工具难以兼顾，因此需要一种更全面的解决方案。", "method": "R-CARLA通过整合精确的车辆动力学、传感器模拟、对手NPC模拟和数字孪生技术，提供全栈测试环境。", "result": "实验表明，R-CARLA将车辆动力学和传感器模拟的仿真与现实差距分别减少了42%和82%。", "conclusion": "R-CARLA为自动驾驶赛车开发提供了更真实的测试环境，显著提升了仿真效果。"}}
{"id": "2506.09163", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09163", "abs": "https://arxiv.org/abs/2506.09163", "authors": ["Daniel Jenson", "Jhonathan Navott", "Piotr Grynfelder", "Mengyan Zhang", "Makkunda Sharma", "Elizaveta Semenova", "Seth Flaxman"], "title": "Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes", "comment": null, "summary": "Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nWhile early architectures were developed primarily as a scalable alternative to\nGaussian Processes (GPs), modern NPs tackle far more complex and data hungry\napplications spanning geology, epidemiology, climate, and robotics. These\napplications have placed increasing pressure on the scalability of these\nmodels, with many architectures compromising accuracy for scalability. In this\npaper, we demonstrate that this tradeoff is often unnecessary, particularly\nwhen modeling fully or partially translation invariant processes. We propose a\nversatile new architecture, the Biased Scan Attention Transformer Neural\nProcess (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),\ngroup-invariant attention biases, and memory-efficient Biased Scan Attention\n(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models\nwhile often training in a fraction of the time, (2) exhibit translation\ninvariance, enabling learning at multiple resolutions simultaneously, (3)\ntransparently model processes that evolve in both space and time, (4) support\nhigh dimensional fixed effects, and (5) scale gracefully -- running inference\nwith over 1M test points with 100K context points in under a minute on a single\n24GB GPU.", "AI": {"tldr": "BSA-TNP是一种新型神经过程模型，通过引入KRBlocks和BSA技术，在保持高精度的同时显著提升可扩展性，适用于复杂应用场景。", "motivation": "现代神经过程模型在复杂应用中面临精度与可扩展性的权衡，而BSA-TNP旨在证明这种权衡并非必要。", "method": "提出BSA-TNP架构，结合KRBlocks、群不变注意力偏置和高效的BSA技术。", "result": "BSA-TNP在精度、训练速度、多分辨率学习、时空建模和高维固定效应支持方面表现优异，且能高效处理大规模数据。", "conclusion": "BSA-TNP为神经过程模型提供了一种高效且通用的解决方案，适用于复杂和大规模应用。"}}
{"id": "2506.09343", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "论文提出首个基于手册的电器操作基准CheckManual，通过大模型辅助生成手册数据，并设计相关挑战和评估指标。", "motivation": "电器操作需依赖手册，但现有研究忽视手册作用或仅限问答任务，无法处理多页手册。", "method": "设计大模型辅助的人工修订数据生成流程，创建手册并建立基准，提出ManualPlan模型作为基线。", "result": "建立了CheckManual基准，包含手册生成、操作挑战和评估环境，ManualPlan模型为基准提供基线。", "conclusion": "CheckManual填补了手册与电器操作间的空白，为未来研究提供新方向。"}}
{"id": "2506.09697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09697", "abs": "https://arxiv.org/abs/2506.09697", "authors": ["Paolo Franceschi", "Andrea Bussolan", "Vincenzo Pomponi", "Oliver Avram", "Stefano Baraldo", "Anna Valente"], "title": "Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling", "comment": null, "summary": "Nowadays, industries are showing a growing interest in human-robot\ncollaboration, particularly for shared tasks. This requires intelligent\nstrategies to plan a robot's motions, considering both task constraints and\nhuman-specific factors such as height and movement preferences. This work\nintroduces a novel approach to generate personalized trajectories using Dynamic\nMovement Primitives (DMPs), enhanced with real-time velocity scaling based on\nhuman feedback. The method was rigorously tested in industrial-grade\nexperiments, focusing on the collaborative transport of an engine cowl lip\nsection. Comparative analysis between DMP-generated trajectories and a\nstate-of-the-art motion planner (BiTRRT) highlights their adaptability combined\nwith velocity scaling. Subjective user feedback further demonstrates a clear\npreference for DMP- based interactions. Objective evaluations, including\nphysiological measurements from brain and skin activity, reinforce these\nfindings, showcasing the advantages of DMPs in enhancing human-robot\ninteraction and improving user experience.", "AI": {"tldr": "论文提出了一种基于动态运动基元（DMPs）和实时速度调整的新方法，用于生成个性化机器人轨迹，提升人机协作体验。", "motivation": "工业界对人机协作的需求增长，需要智能策略规划机器人运动，同时考虑任务约束和人类因素（如身高和运动偏好）。", "method": "采用动态运动基元（DMPs）生成轨迹，并结合实时速度调整技术，基于人类反馈优化运动。", "result": "实验表明，DMP生成的轨迹在适应性和用户体验上优于BiTRRT方法，用户主观反馈和生理数据均支持这一结论。", "conclusion": "DMP方法显著提升了人机交互效果和用户体验，适用于工业协作任务。"}}
{"id": "2506.09171", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2506.09171", "abs": "https://arxiv.org/abs/2506.09171", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "AI": {"tldr": "提出了一种新型LLM代理框架，通过上下文学习增强规划能力，利用原子事实增强和递归前瞻搜索，无需微调即可提升多步推理能力。", "motivation": "现有方法在复杂交互环境中难以适应新信息或高效利用历史经验进行多步推理，需要改进。", "method": "通过原子事实增强和递归前瞻搜索，动态提取任务关键事实并用于动作提议、世界模型模拟和状态值估计。", "result": "代理在交互任务中表现出更高的性能和适应性，如TextFrozenLake和ALFWorld任务。", "conclusion": "该框架通过经验积累优化行为，证明了基于事实抽象和LLM模拟的潜力。"}}
{"id": "2506.09345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "提出了一种多模态动作识别解决方案，通过数据增强、迁移学习、时空特征提取和预测增强方法，实现了高精度识别。", "motivation": "由于三模态数据的稀缺性，多模态动作识别任务面临挑战，需有效利用多模态信息。", "method": "优化数据增强技术扩展数据规模，预训练骨干网络，结合2D CNNs和TSM提取时空特征，并采用SWA、Ensemble和TTA等预测增强方法。", "result": "在竞赛排行榜上取得Top-1准确率99%和Top-5准确率100%。", "conclusion": "该解决方案在多模态动作识别任务中表现出优越性。"}}
{"id": "2506.09765", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09765", "abs": "https://arxiv.org/abs/2506.09765", "authors": ["Shuai Li", "Azarakhsh Keipour", "Sicong Zhao", "Srinath Rajagopalan", "Charles Swan", "Kostas E. Bekris"], "title": "Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction", "comment": "The 19th International Symposium on Experimental Robotics (ISER\n  2025); 6-10 July 2025, Santa Fe, New Mexico, USA; 10 pages", "summary": "Warehouse automation plays a pivotal role in enhancing operational\nefficiency, minimizing costs, and improving resilience to workforce\nvariability. While prior research has demonstrated the potential of machine\nlearning (ML) models to increase picking success rates in large-scale robotic\nfleets by prioritizing high-probability picks and packages, these efforts\nprimarily focused on predicting success probabilities for picks sampled using\nheuristic methods. Limited attention has been given, however, to leveraging\ndata-driven approaches to directly optimize sampled picks for better\nperformance at scale. In this study, we propose an ML-based framework that\npredicts transform adjustments as well as improving the selection of suction\ncups for multi-suction end effectors for sampled picks to enhance their success\nprobabilities. The framework was integrated and evaluated in test workcells\nthat resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,\nwhich is used for package manipulation. Evaluated on over 2 million picks, the\nproposed method achieves a 20\\% reduction in pick failure rates compared to a\nheuristic-based pick sampling baseline, demonstrating its effectiveness in\nlarge-scale warehouse automation scenarios.", "AI": {"tldr": "本文提出了一种基于机器学习的框架，通过预测调整和优化吸盘选择，显著提升了仓库自动化中的拣选成功率。", "motivation": "仓库自动化在提升效率、降低成本方面至关重要，但现有研究多依赖启发式方法，缺乏对数据驱动优化的直接关注。", "method": "提出了一种ML框架，用于预测拣选的调整参数和优化吸盘选择，以提升拣选成功率。", "result": "在超过200万次拣选中，该方法比启发式基线减少了20%的失败率。", "conclusion": "该框架在大规模仓库自动化中表现出显著效果，验证了数据驱动方法的优势。"}}
{"id": "2506.09172", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09172", "abs": "https://arxiv.org/abs/2506.09172", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "AI": {"tldr": "MultiNet是一个开源的基准测试和软件生态系统，用于评估和适应视觉、语言和动作领域的模型，提供标准化评估协议和1.3万亿标记的复合数据集。", "motivation": "开发通用代理系统需要结合视觉理解、语言理解和动作生成，但目前缺乏统一的评估工具和数据。", "method": "引入MultiNet基准测试和软件生态系统，包括标准化评估协议、开源软件和复合数据集。", "result": "MultiNet已用于下游研究，揭示了视觉-语言-动作模型的泛化局限性。", "conclusion": "MultiNet为多模态动作模型的研究提供了重要工具和数据支持。"}}
{"id": "2506.09350", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "提出了一种自回归对抗后训练方法（AAPT），将预训练的潜在视频扩散模型转化为实时交互式视频生成器，实现高效的单步生成。", "motivation": "现有大规模视频生成模型计算量大，难以应用于实时交互场景。", "method": "采用自回归对抗训练，单步生成潜在帧，支持实时流式输出和交互控制。", "result": "8B模型在单H100上实现24fps、736x416分辨率的实时视频生成，或在8xH100上支持1280x720分辨率长达1分钟。", "conclusion": "AAPT方法通过对抗训练和高效架构设计，显著提升了视频生成的实时性和交互能力。"}}
{"id": "2506.09800", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09800", "abs": "https://arxiv.org/abs/2506.09800", "authors": ["Haochen Liu", "Tianyu Li", "Haohan Yang", "Li Chen", "Caojun Wang", "Ke Guo", "Haochen Tian", "Hongchen Li", "Hongyang Li", "Chen Lv"], "title": "Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\ndirectly mapping sensor inputs to planning maneuvers using learning-based\nmodular integrations. However, existing imitation learning (IL)-based models\nsuffer from generalization to hard cases, and a lack of corrective feedback\nloop under post-deployment. While reinforcement learning (RL) offers a\npotential solution to tackle hard cases with optimality, it is often hindered\nby overfitting to specific driving cases, resulting in catastrophic forgetting\nof generalizable knowledge and sample inefficiency. To overcome these\nchallenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),\na novel learning pipeline that constantly refines hard domain while keeping\ngeneralizable driving policy for model-agnostic end-to-end driving systems.\nThrough reinforcement fine-tuning and policy expansion that facilitates\ncontinuous improvement, R2SE features three key components: 1) Generalist\nPretraining with hard-case allocation trains a generalist imitation learning\n(IL) driving system while dynamically identifying failure-prone cases for\ntargeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes\nresidual corrections using reinforcement learning (RL) to improve performance\nin hard case domain while preserving global driving knowledge; 3) Self-aware\nAdapter Expansion dynamically integrates specialist policies back into the\ngeneralist model, enhancing continuous performance improvement. Experimental\nresults in closed-loop simulation and real-world datasets demonstrate\nimprovements in generalization, safety, and long-horizon policy robustness over\nstate-of-the-art E2E systems, highlighting the effectiveness of reinforce\nrefinement for scalable autonomous driving.", "AI": {"tldr": "论文提出R2SE方法，通过强化学习和模仿学习的结合，解决自动驾驶端到端系统中的泛化性和样本效率问题。", "motivation": "现有模仿学习模型在泛化性和部署后反馈方面存在不足，而强化学习容易过拟合特定场景。R2SE旨在结合两者优势，提升自动驾驶系统的性能。", "method": "R2SE包含三个关键组件：1) 通用预训练与硬案例分配；2) 残差强化专家微调；3) 自感知适配器扩展。", "result": "实验表明，R2SE在泛化性、安全性和长期策略鲁棒性上优于现有端到端系统。", "conclusion": "R2SE通过强化细化方法，为可扩展的自动驾驶提供了有效解决方案。"}}
{"id": "2506.09173", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09173", "abs": "https://arxiv.org/abs/2506.09173", "authors": ["Michael Cooper", "Rohan Wadhawan", "John Michael Giorgi", "Chenhao Tan", "Davis Liang"], "title": "The Curious Language Model: Strategic Test-Time Information Acquisition", "comment": "39 pages", "summary": "Decision-makers often possess insufficient information to render a confident\ndecision. In these cases, the decision-maker can often undertake actions to\nacquire the necessary information about the problem at hand, e.g., by\nconsulting knowledgeable authorities or by conducting experiments. Importantly,\ndifferent levers of information acquisition come with different costs, posing\nthe challenge of selecting the actions that are both informative and\ncost-effective. In this work, we propose CuriosiTree, a heuristic-based,\ntest-time policy for zero-shot information acquisition in large language models\n(LLMs). CuriosiTree employs a greedy tree search to estimate the expected\ninformation gain of each action and strategically chooses actions based on a\nbalance of anticipated information gain and associated cost. Empirical\nvalidation in a clinical diagnosis simulation shows that CuriosiTree enables\ncost-effective integration of heterogenous sources of information, and\noutperforms baseline action selection strategies in selecting action sequences\nthat enable accurate diagnosis.", "AI": {"tldr": "CuriosiTree是一种基于启发式的零样本信息获取策略，通过贪心树搜索平衡信息增益与成本，在临床诊断模拟中表现优于基线策略。", "motivation": "决策者在信息不足时需选择成本效益高的信息获取方式。", "method": "提出CuriosiTree，使用贪心树搜索评估每个动作的预期信息增益，并基于信息增益与成本选择动作。", "result": "在临床诊断模拟中，CuriosiTree能有效整合异构信息源，诊断准确性优于基线策略。", "conclusion": "CuriosiTree是一种高效且成本效益高的信息获取策略。"}}
{"id": "2506.09357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "提出了一种基于变分框架和微分同胚变换的2D图像分割方法，结合形状分析和LDDMM框架，无需依赖大数据集。", "motivation": "传统图像分割方法（如边缘检测）和深度学习需要大量数据，而新方法旨在提供更灵活且理论扎实的解决方案。", "method": "通过微分同胚变换将模板曲线变形，利用LDDMM框架和varifold表示几何形状的损失函数指导曲线演化。", "result": "实现了高精度的图像分割，且方法灵活、理论扎实。", "conclusion": "该方法为图像分割提供了一种不依赖大数据集的新思路，具有理论和实践价值。"}}
{"id": "2506.09859", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09859", "abs": "https://arxiv.org/abs/2506.09859", "authors": ["Huajian Liu", "Yixuan Feng", "Wei Dong", "Kunpeng Fan", "Chao Wang", "Yongzhuo Gao"], "title": "Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints", "comment": null, "summary": "In this paper, we propose a novel hierarchical framework for robot navigation\nin dynamic environments with heterogeneous constraints. Our approach leverages\na graph neural network trained via reinforcement learning (RL) to efficiently\nestimate the robot's cost-to-go, formulated as local goal recommendations. A\nspatio-temporal path-searching module, which accounts for kinematic\nconstraints, is then employed to generate a reference trajectory to facilitate\nsolving the non-convex optimization problem used for explicit constraint\nenforcement. More importantly, we introduce an incremental action-masking\nmechanism and a privileged learning strategy, enabling end-to-end training of\nthe proposed planner. Both simulation and real-world experiments demonstrate\nthat the proposed method effectively addresses local planning in complex\ndynamic environments, achieving state-of-the-art (SOTA) performance. Compared\nwith existing learning-optimization hybrid methods, our approach eliminates the\ndependency on high-fidelity simulation environments, offering significant\nadvantages in computational efficiency and training scalability. The code will\nbe released as open-source upon acceptance of the paper.", "AI": {"tldr": "提出了一种新颖的分层框架，用于动态环境中机器人导航，结合图神经网络和强化学习，通过增量动作掩蔽机制和特权学习策略实现端到端训练，性能达到SOTA。", "motivation": "解决动态环境中机器人导航的复杂性和非凸优化问题，同时减少对高保真仿真环境的依赖。", "method": "使用图神经网络和强化学习估计成本，结合时空路径搜索模块生成轨迹，并引入增量动作掩蔽和特权学习策略。", "result": "仿真和实际实验表明，该方法在复杂动态环境中表现优异，计算效率和训练可扩展性显著提升。", "conclusion": "该方法在动态环境中具有高效性和可扩展性，为机器人导航提供了新的解决方案。"}}
{"id": "2506.09174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09174", "abs": "https://arxiv.org/abs/2506.09174", "authors": ["Chenheng Xu", "Dan Wu", "Yixin Zhu", "Ying Nian Wu"], "title": "Multivariate Long-term Time Series Forecasting with Fourier Neural Filter", "comment": null, "summary": "Multivariate long-term time series forecasting has been suffering from the\nchallenge of capturing both temporal dependencies within variables and spatial\ncorrelations across variables simultaneously. Current approaches predominantly\nrepurpose backbones from natural language processing or computer vision (e.g.,\nTransformers), which fail to adequately address the unique properties of time\nseries (e.g., periodicity). The research community lacks a dedicated backbone\nwith temporal-specific inductive biases, instead relying on domain-agnostic\nbackbones supplemented with auxiliary techniques (e.g., signal decomposition).\nWe introduce FNF as the backbone and DBD as the architecture to provide\nexcellent learning capabilities and optimal learning pathways for\nspatio-temporal modeling, respectively. Our theoretical analysis proves that\nFNF unifies local time-domain and global frequency-domain information\nprocessing within a single backbone that extends naturally to spatial modeling,\nwhile information bottleneck theory demonstrates that DBD provides superior\ngradient flow and representation capacity compared to existing unified or\nsequential architectures. Our empirical evaluation across 11 public benchmark\ndatasets spanning five domains (energy, meteorology, transportation,\nenvironment, and nature) confirms state-of-the-art performance with consistent\nhyperparameter settings. Notably, our approach achieves these results without\nany auxiliary techniques, suggesting that properly designed neural\narchitectures can capture the inherent properties of time series, potentially\ntransforming time series modeling in scientific and industrial applications.", "AI": {"tldr": "论文提出了一种新的时间序列预测方法FNF和DBD架构，解决了现有方法无法同时捕捉时间依赖性和空间相关性的问题，并在多个领域的数据集上验证了其优越性。", "motivation": "现有方法主要借用自然语言处理或计算机视觉的模型（如Transformer），未能充分考虑时间序列的独特性质（如周期性），缺乏专门针对时间序列的模型。", "method": "提出FNF作为骨干网络，结合DBD架构，统一处理局部时间域和全局频率域信息，并通过信息瓶颈理论优化梯度流和表示能力。", "result": "在11个公共基准数据集上验证了方法的优越性，无需辅助技术即可达到最先进性能。", "conclusion": "研究表明，设计合理的神经网络架构可以捕捉时间序列的固有特性，可能推动科学和工业应用中的时间序列建模。"}}
{"id": "2506.09363", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "SAGE提出了一种语义增强擦除方法，通过循环自检和自擦除将概念词擦除转化为概念域擦除，同时结合全局-局部协作保留机制，提升扩散模型的安全生成能力。", "motivation": "扩散模型在文本到图像生成中表现优异，但预训练中可能包含敏感信息，导致安全风险。现有方法将不安全概念视为固定词反复擦除，限制了泛化能力。", "method": "SAGE通过语义空间关系探索概念域边界表示，无需额外数据；同时采用全局语义对齐与局部噪声保留的协作机制，减少无关概念的退化。", "result": "实验表明SAGE在安全生成方面全面优于其他方法。", "conclusion": "SAGE通过语义增强擦除和协作保留机制，有效解决了扩散模型的安全问题，代码和权重将开源。"}}
{"id": "2506.09183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09183", "abs": "https://arxiv.org/abs/2506.09183", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "title": "Multi-Task Reward Learning from Human Ratings", "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "AI": {"tldr": "提出一种新的强化学习方法，通过联合考虑多任务来模拟人类决策，优于现有基于评分的RL方法。", "motivation": "当前RLHF方法简化了人类决策过程，未能充分模拟人类的多策略整合能力。", "method": "利用无奖励环境中的人类评分推断奖励函数，引入可学习权重平衡分类和回归模型。", "result": "实验表明，该方法优于现有基于评分的RL方法，有时甚至超越传统RL方法。", "conclusion": "新方法能更好地模拟人类决策的不确定性，适应性强。"}}
{"id": "2506.09369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "本文提出了一种名为ScaleLSD的自监督学习方法，用于高效且高性能的线几何检测，适用于任何自然图像。", "motivation": "研究线几何检测（LSD）问题，旨在学习一个适用于任何自然图像的领域无关的鲁棒LSD模型。", "method": "通过重新审视和简化LSD方法的基本设计，提出了一种名为ScaleLSD的高效自监督学习模型，利用超过1000万未标记的真实图像进行训练。", "result": "ScaleLSD在零样本协议下表现出色，在检测性能、单视图3D几何估计、两视图线段匹配和多视图3D线映射等方面均优于传统非深度LSD方法。", "conclusion": "ScaleLSD是首个在所有测试方面均超越传统非深度LSD方法的深度学习方法，显著扩展和强化了图像线几何的多样性。"}}
{"id": "2506.09914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09914", "abs": "https://arxiv.org/abs/2506.09914", "authors": ["Teng Guo"], "title": "From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications", "comment": "Ph.D. thesis", "summary": "The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots\nfrom start to goal configurations efficiently while avoiding collisions.\nDespite progress in solution quality and runtime, its complexity and industrial\nrelevance continue to drive research.\n  This dissertation introduces scalable MRPP methods with provable guarantees\nand practical heuristics. First, we study dense MRPP on 2D grids, relevant to\nwarehouse and parcel systems. We propose the Rubik Table method, achieving $(1\n+ \\delta)$-optimal makespan (with $\\delta \\in (0, 0.5]$) for up to $\\frac{m_1\nm_2}{2}$ robots, solving large instances efficiently and setting a new\ntheoretical benchmark.\n  Next, we address real-world MRPP. We design optimal layouts for structured\nenvironments (e.g., warehouses, parking systems) and propose a puzzle-based\nsystem for dense, deadlock-free autonomous vehicle parking. We also extend MRPP\nto Reeds-Shepp robots, introducing motion primitives and smoothing techniques\nto ensure feasible, efficient paths under nonholonomic constraints. Simulations\nand real-world tests validate the approach in urban driving and robotic\ntransport scenarios.", "AI": {"tldr": "本文提出可扩展的多机器人路径规划（MRPP）方法，包括理论保证和实用启发式算法，解决了密集和实际场景中的路径规划问题。", "motivation": "尽管MRPP问题在解质量和运行时间上有所进展，但其复杂性和工业应用需求仍推动研究发展。", "method": "提出了Rubik Table方法处理2D网格上的密集MRPP，设计最优布局和基于拼图的系统解决实际MRPP问题，并扩展至Reeds-Shepp机器人。", "result": "Rubik Table方法实现了(1+δ)-最优完成时间，解决了大规模实例；实际应用中验证了方法的有效性。", "conclusion": "本文的方法在理论和实际应用中均表现出色，为MRPP问题提供了新的解决方案和基准。"}}
{"id": "2506.09193", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09193", "abs": "https://arxiv.org/abs/2506.09193", "authors": ["Yilin Zhuang", "Karthik Duraisamy"], "title": "LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting", "comment": null, "summary": "Accurate probabilistic weather forecasting demands both high accuracy and\nefficient uncertainty quantification, challenges that overburden both ensemble\nnumerical weather prediction (NWP) and recent machine-learning methods. We\nintroduce LaDCast, the first global latent-diffusion framework for medium-range\nensemble forecasting, which generates hourly ensemble forecasts entirely in a\nlearned latent space. An autoencoder compresses high-dimensional ERA5\nreanalysis fields into a compact representation, and a transformer-based\ndiffusion model produces sequential latent updates with arbitrary hour\ninitialization. The model incorporates Geometric Rotary Position Embedding\n(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream\nattention mechanism for efficient conditioning, and sinusoidal temporal\nembeddings to capture seasonal patterns. LaDCast achieves deterministic and\nprobabilistic skill close to that of the European Centre for Medium-Range\nForecast IFS-ENS, without any explicit perturbations. Notably, LaDCast\ndemonstrates superior performance in tracking rare extreme events such as\ncyclones, capturing their trajectories more accurately than established models.\nBy operating in latent space, LaDCast reduces storage and compute by orders of\nmagnitude, demonstrating a practical path toward forecasting at kilometer-scale\nresolution in real time. We open-source our code and models and provide the\ntraining and evaluation pipelines at: https://github.com/tonyzyl/ladcast.", "AI": {"tldr": "LaDCast是一种基于潜在扩散模型的全球中期集合天气预报框架，通过潜在空间生成小时级集合预报，性能接近欧洲中期天气预报中心（IFS-ENS），且在极端事件（如气旋）跟踪上表现更优。", "motivation": "传统数值天气预报（NWP）和机器学习方法在高精度和高效不确定性量化方面存在挑战，需要一种更高效的解决方案。", "method": "使用自编码器压缩高维ERA5再分析数据，结合基于Transformer的扩散模型生成潜在更新，并引入GeoRoPE、双流注意力机制和正弦时间嵌入等技术。", "result": "LaDCast在确定性和概率性预报技能上接近IFS-ENS，且在极端事件跟踪上表现更优，同时显著降低了计算和存储需求。", "conclusion": "LaDCast为实时公里级分辨率天气预报提供了一条实用路径，并开源了代码和模型。"}}
{"id": "2506.09378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "UniForward是一个前馈高斯散射模型，通过未校准的稀疏视图图像实时重建3D场景和语义场，无需相机参数或深度真值。", "motivation": "结合3D场景与语义场以增强环境感知和理解，但需解决语义嵌入、实时重建和仅用图像输入的挑战。", "method": "嵌入语义特征到3D高斯中，通过双分支解码器预测，采用损失引导的视图采样器和端到端训练。", "result": "实现高质量3D渲染和视图一致的语义特征，支持开放词汇分割，性能领先。", "conclusion": "UniForward在3D场景与语义场统一重建中表现优异，具有实际应用潜力。"}}
{"id": "2506.09930", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09930", "abs": "https://arxiv.org/abs/2506.09930", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "comment": "Under review", "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "AI": {"tldr": "论文提出了一个统一的仿真任务套件，用于评估视觉-语言-动作（VLA）模型的泛化能力，发现VLM预训练虽能提升感知和规划能力，但动作执行仍存在问题。", "motivation": "当前VLA模型的评估不足，缺乏标准化任务套件，且现有研究难以复现和比较。", "method": "引入50个仿真任务，涵盖语言指令、视觉和物体操作，系统评估多种VLA架构的泛化能力。", "result": "VLM预训练赋予VLA模型良好的感知和规划能力，但动作执行在分布外观察中表现不佳，且微调可能损害泛化能力。", "conclusion": "发布任务套件和评估代码，推动未来研究以弥合感知与动作执行的差距。"}}
{"id": "2506.09199", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.09199", "abs": "https://arxiv.org/abs/2506.09199", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "comment": "21 pages, 12 figures", "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "AI": {"tldr": "FLoRIST是一种联邦学习框架，通过低秩适应（LoRA）高效微调大型语言模型，解决了通信效率、模型准确性和计算成本之间的平衡问题。", "motivation": "现有联邦LoRA方法在通信效率、模型准确性和计算成本之间存在挑战，尤其是在异构客户端环境中。", "method": "FLoRIST通过单独对堆叠的本地适配器进行奇异值分解，在紧凑的中间空间中表示信息，并引入可调奇异值阈值选择最优秩，构建全局低秩适配器。", "result": "FLoRIST在多种数据集和LLM上表现出色，在通信效率和性能之间取得了最佳平衡。", "conclusion": "FLoRIST为联邦学习中的高效微调提供了一种有效解决方案，适用于异构和同构环境。"}}
{"id": "2506.09385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09385", "abs": "https://arxiv.org/abs/2506.09385", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "comment": null, "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "AI": {"tldr": "论文提出了一种新的多模态行人重识别问题（OM-ReID），并构建了首个高质量多模态数据集ORBench，同时提出了多模态学习框架ReID5o。", "motivation": "现有方法和数据集在多模态行人重识别任务中存在局限性，无法满足实际需求。", "method": "构建ORBench数据集（包含5种模态），并提出ReID5o框架，支持任意模态组合的协同融合和跨模态对齐。", "result": "实验验证了ORBench的先进性和实用性，ReID5o在多种模型比较中表现最佳。", "conclusion": "ORBench和ReID5o为多模态行人重识别研究提供了理想平台和解决方案。"}}
{"id": "2506.09934", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09934", "abs": "https://arxiv.org/abs/2506.09934", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "AI": {"tldr": "该论文提出了一种通过定制不透射线标记物实现微导管形状和姿态估计的方法，以减少神经介入手术中的感知负担。", "motivation": "目前神经介入手术中，医生需要从双平面透视图像中重建和预测导管运动，负担较重，且现有跟踪方法不适用于微导管。", "method": "在导管上布置定制不透射线标记物，并通过设计优化标记物排列以减少跟踪不确定性。", "result": "在直径小于2mm的微导管上测试，形状跟踪误差小于1mm，导管滚动误差低于40度。", "conclusion": "该方法可实现双平面成像下导管的自主导航。"}}
{"id": "2506.09200", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09200", "abs": "https://arxiv.org/abs/2506.09200", "authors": ["Val Andrei Fajardo", "David B. Emerson", "Amandeep Singh", "Veronica Chatrath", "Marcelo Lotif", "Ravi Theja", "Alex Cheung", "Izuki Matsubi"], "title": "FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems", "comment": "9 pages, 4 figures, 2 tables. Accepted for the CODEML Workshop at\n  ICML 2025. Framework code available at\n  https://github.com/VectorInstitute/fed-rag", "summary": "Retrieval-augmented generation (RAG) systems have been shown to be effective\nin addressing many of the drawbacks of relying solely on the parametric memory\nof large language models. Recent work has demonstrated that RAG systems can be\nimproved via fine-tuning of their retriever and generator models. In this work,\nwe introduce FedRAG, a framework for fine-tuning RAG systems across centralized\nand federated architectures. FedRAG supports state-of-the-art fine-tuning\nmethods, offering a simple and intuitive interface and a seamless conversion\nfrom centralized to federated training tasks. FedRAG is also deeply integrated\nwith the modern RAG ecosystem, filling a critical gap in available tools.", "AI": {"tldr": "FedRAG是一个框架，用于在集中式和联邦式架构中微调RAG系统，填补了现有工具的空白。", "motivation": "解决仅依赖大型语言模型参数记忆的缺点，并通过微调检索器和生成器模型提升RAG系统性能。", "method": "提出FedRAG框架，支持最先进的微调方法，提供简单接口和从集中式到联邦式任务的转换。", "result": "FedRAG深度集成现代RAG生态系统，填补了工具空白。", "conclusion": "FedRAG为RAG系统的微调提供了高效且灵活的解决方案。"}}
{"id": "2506.09399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09399", "abs": "https://arxiv.org/abs/2506.09399", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "comment": null, "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "AI": {"tldr": "提出了一种动态调整先验几何的方法，通过实时更新协方差矩阵改进OOD检测性能。", "motivation": "现有基于子空间的方法因静态提取信息几何而无法处理分布不良样本导致的几何扭曲。", "method": "动态更新先验协方差矩阵，沿实时输入特征方向减少协方差，并在残差空间约束调整。", "result": "在CIFAR和ImageNet-1k数据集上显著提升了OOD检测性能。", "conclusion": "动态调整先验几何能有效纠正分布不良样本的影响，提升OOD检测效果。"}}
{"id": "2506.09937", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09937", "abs": "https://arxiv.org/abs/2506.09937", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "comment": "Project Page: https://vla-safe.github.io/", "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "AI": {"tldr": "论文提出了一种名为SAFE的多任务失败检测器，用于通用机器人策略（如VLA），能够在新任务和环境中检测失败。", "motivation": "现有失败检测器仅针对特定任务训练和测试，而VLA需要检测器在未见任务和环境中也能泛化。", "method": "分析VLA特征空间，利用其内部特征设计SAFE，预测任务失败的可能性。", "result": "SAFE在模拟和真实环境中测试，表现优于基线方法，实现了最佳准确性和检测时间平衡。", "conclusion": "SAFE为通用机器人策略提供了高效且泛化的失败检测解决方案。"}}
{"id": "2506.09202", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09202", "abs": "https://arxiv.org/abs/2506.09202", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "comment": null, "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "AI": {"tldr": "论文提出了一种新的任务：从离线强化学习数据集中聚类轨迹，每个聚类中心代表生成轨迹的策略。通过KL散度和策略诱导分布的混合，提出了聚类目标，并提出了PG-Kmeans和CAAE两种方法。理论证明了PG-Kmeans的收敛性，并指出离线轨迹聚类的挑战。实验验证了方法的有效性。", "motivation": "离线强化学习数据集中轨迹的聚类有助于理解策略生成的行为模式，但现有方法未充分探索策略与轨迹分布的关系。", "method": "提出了PG-Kmeans（基于行为克隆策略迭代聚类）和CAAE（类似VQ-VAE的框架，引导潜在表示聚类）。", "result": "在D4RL数据集和GridWorld环境中验证了方法的有效性，能够将轨迹划分为有意义的聚类。", "conclusion": "PG-Kmeans和CAAE为基于策略的轨迹聚类提供了有效框架，适用于离线强化学习及其他领域。"}}
{"id": "2506.09403", "categories": ["cs.CV", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.09403", "abs": "https://arxiv.org/abs/2506.09403", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "AI": {"tldr": "本文提出了一种基于Segment Anything Model（SAM）的源自由域适应方法（SRPL-SFDA），通过增强伪标签质量和可靠性，显著提升了医学图像分割在目标域的性能。", "motivation": "解决源自由域适应（SFDA）中目标域无标签数据监督不足的问题，同时应对隐私和数据访问限制的挑战。", "method": "1）测试时三分支强度增强（T3IE）提升伪标签质量；2）基于多SAM输出一致性（CMSO）的可靠伪标签选择模块；3）可靠性感知训练，结合熵最小化。", "result": "在两个医学图像分割数据集上，SRPL-SFDA性能优于现有SFDA方法，接近目标域有监督训练的效果。", "conclusion": "SRPL-SFDA通过SAM引导的可靠伪标签方法，有效提升了SFDA的性能，为医学图像分割的域适应提供了新思路。"}}
{"id": "2506.09979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09979", "abs": "https://arxiv.org/abs/2506.09979", "authors": ["Zachary Olkin", "Aaron D. Ames"], "title": "Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control", "comment": "Submitted to Humanoids 2025", "summary": "Computing stabilizing and optimal control actions for legged locomotion in\nreal time is difficult due to the nonlinear, hybrid, and high dimensional\nnature of these robots. The hybrid nature of the system introduces a\ncombination of discrete and continuous variables which causes issues for\nnumerical optimal control. To address these challenges, we propose a layered\narchitecture that separates the choice of discrete variables and a smooth Model\nPredictive Controller (MPC). The layered formulation allows for online\nflexibility and optimality without sacrificing real-time performance through a\ncombination of gradient-free and gradient-based methods. The architecture\nleverages a sampling-based method for determining discrete variables, and a\nclassical smooth MPC formulation using these fixed discrete variables. We\ndemonstrate the results on a quadrupedal robot stepping over gaps and onto\nterrain with varying heights. In simulation, we demonstrate the controller on a\nhumanoid robot for gap traversal. The layered approach is shown to be more\noptimal and reliable than common heuristic-based approaches and faster to\ncompute than pure sampling methods.", "AI": {"tldr": "提出了一种分层架构，结合梯度无关和梯度方法，解决腿式机器人实时控制中的非线性、混合和高维问题。", "motivation": "腿式机器人的非线性、混合和高维特性导致实时稳定和最优控制困难。", "method": "采用分层架构，分离离散变量选择和光滑MPC，结合采样和梯度方法。", "result": "在四足和双足机器人上验证，比启发式方法更优、比纯采样方法更快。", "conclusion": "分层架构在实时性和最优性上表现优异。"}}
{"id": "2506.09207", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.09207", "abs": "https://arxiv.org/abs/2506.09207", "authors": ["William Anderson", "Kevin Chung", "Youngsoo Choi"], "title": "mLaSDI: Multi-stage latent space dynamics identification", "comment": null, "summary": "Determining accurate numerical solutions of partial differential equations\n(PDEs) is an important task in many scientific disciplines. However, solvers\ncan be computationally expensive, leading to the development of reduced-order\nmodels (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was\nproposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the\ntraining data using an autoencoder and learns a system of user-chosen ordinary\ndifferential equations (ODEs), which govern the latent space dynamics. This\nallows for rapid predictions by interpolating and evolving the low-dimensional\nODEs in the latent space. While LaSDI has produced effective ROMs for numerous\nproblems, the autoencoder can have difficulty accurately reconstructing\ntraining data while also satisfying the imposed dynamics in the latent space,\nparticularly in complex or high-frequency regimes. To address this, we propose\nmulti-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several\nautoencoders are trained sequentially in stages, where each autoencoder learns\nto correct the error of the previous stages. We find that applying mLaSDI with\nsmall autoencoders results in lower prediction and reconstruction errors, while\nalso reducing training time compared to LaSDI.", "AI": {"tldr": "论文提出了一种改进的降阶模型方法mLaSDI，通过多阶段训练自编码器来提升对复杂或高频数据的重建和预测能力。", "motivation": "传统LaSDI方法在复杂或高频数据下难以同时满足数据重建和潜在空间动力学的要求，需要改进。", "method": "采用多阶段自编码器训练（mLaSDI），每阶段自编码器修正前一阶段的误差，使用小型自编码器降低训练时间和误差。", "result": "mLaSDI在预测和重建误差上表现更优，同时减少了训练时间。", "conclusion": "mLaSDI是一种高效且准确的降阶模型方法，适用于复杂或高频数据场景。"}}
{"id": "2506.09411", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09411", "abs": "https://arxiv.org/abs/2506.09411", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "comment": null, "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "AI": {"tldr": "提出一种基于姿态迁移的合成人类动作视频数据生成方法，用于解决合成数据在视频理解任务中的不足，并在多个数据集上验证其有效性。", "motivation": "合成数据在人类动作视频理解任务中常因不自然特征而效果受限，影响了其在手势识别、自动驾驶等领域的应用潜力。", "method": "使用可控3D高斯虚拟人模型进行姿态迁移，生成合成人类动作视频数据。", "result": "在Toyota Smarthome和NTU RGB+D数据集上验证，该方法提升了动作识别性能，并能有效扩展少样本数据集。", "conclusion": "该方法不仅提升了合成数据的实用性，还开源了相关数据集和工具，为后续研究提供了支持。"}}
{"id": "2506.09990", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09990", "abs": "https://arxiv.org/abs/2506.09990", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "comment": null, "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "AI": {"tldr": "Chain-of-Action (CoA) 是一种基于轨迹自回归建模的新型视觉运动策略范式，通过反向推理生成完整轨迹，实现了任务目标的全局约束。", "motivation": "传统方法仅预测下一步动作，缺乏全局目标约束。CoA 旨在通过反向推理和任务目标驱动的动作链（CoT）过程，提升策略的全局性和空间泛化能力。", "method": "CoA 采用自回归结构：首动作标记为关键帧动作，编码任务目标；后续动作标记基于关键帧和先前动作生成。设计包括连续动作标记表示、动态停止、反向时间集成和多标记预测。", "result": "CoA 在 60 个 RLBench 任务和 8 个真实世界操作任务中达到最先进性能。", "conclusion": "CoA 通过反向推理和全局约束设计，显著提升了视觉运动策略的空间泛化能力和任务性能。"}}
{"id": "2506.09215", "categories": ["cs.LG", "cs.AI", "68T07 (Primary), 68P30, 68T45 (Secondary)", "E.4; I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.09215", "abs": "https://arxiv.org/abs/2506.09215", "authors": ["Greyson Brothers"], "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "AI": {"tldr": "研究了基于Transformer嵌入模型的池化方法设计，提出了一种注意力自适应池化方法，优于传统方法（AvgPool、MaxPool、ClsToken），在信号噪声比波动时表现更稳健。", "motivation": "传统池化方法在信号噪声比波动时性能下降，需要一种更稳健的池化方法。", "method": "将池化问题建模为向量量化，提出注意力自适应池化方法，理论上逼近信号最优量化器。", "result": "在合成数据集和实际任务（关系推理、多智能体强化学习、视觉）中验证了方法的优越性。", "conclusion": "自适应池化方法在信号噪声比波动时表现更稳健，适用于多种任务。"}}
{"id": "2506.09416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09416", "abs": "https://arxiv.org/abs/2506.09416", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "Noise Conditional Variational Score Distillation", "comment": null, "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "AI": {"tldr": "NCVSD是一种将预训练扩散模型蒸馏为生成去噪器的新方法，通过揭示无条件评分函数隐含表征去噪后验分布的评分函数，实现了快速生成与迭代优化的平衡。", "motivation": "探索如何高效地将扩散模型蒸馏为生成去噪器，同时保留快速生成和高质量样本的能力。", "method": "将无条件评分函数的洞察整合到VSD框架中，学习能够近似不同噪声水平下后验分布的生成去噪器。", "result": "NCVSD在类条件图像生成和逆问题求解中表现优异，超越教师扩散模型，并与更大规模的Consistency模型相当。", "conclusion": "NCVSD通过灵活的测试时计算扩展，实现了快速生成与高质量样本的平衡，并在逆问题中创下LPIPS记录。"}}
{"id": "2506.09994", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09994", "abs": "https://arxiv.org/abs/2506.09994", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "comment": null, "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "AI": {"tldr": "eFlesh是一种低成本、易定制、基于磁力的触觉传感器，用于机器人操作，填补了现有触觉传感器的空白。", "motivation": "机器人需要在非结构化环境中感知物理交互的力，但现有触觉传感器缺乏通用性和可定制性，导致解决方案分散或完全忽略力的感知。", "method": "eFlesh传感器由3D打印的微结构、磁铁和磁力计电路板组成，通过开源设计工具实现几何和机械响应的定制。", "result": "实验显示，eFlesh在接触定位和力预测上表现优异，并显著提升了机器人操作的精度和成功率。", "conclusion": "eFlesh为机器人触觉感知提供了低成本、高定制化的解决方案，推动了非结构化环境中的机器人操作能力。"}}
{"id": "2506.09227", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09227", "abs": "https://arxiv.org/abs/2506.09227", "authors": ["Jie Ren", "Yue Xing", "Yingqian Cui", "Charu C. Aggarwal", "Hui Liu"], "title": "SoK: Machine Unlearning for Large Language Models", "comment": null, "summary": "Large language model (LLM) unlearning has become a critical topic in machine\nlearning, aiming to eliminate the influence of specific training data or\nknowledge without retraining the model from scratch. A variety of techniques\nhave been proposed, including Gradient Ascent, model editing, and re-steering\nhidden representations. While existing surveys often organize these methods by\ntheir technical characteristics, such classifications tend to overlook a more\nfundamental dimension: the underlying intention of unlearning--whether it seeks\nto truly remove internal knowledge or merely suppress its behavioral effects.\nIn this SoK paper, we propose a new taxonomy based on this intention-oriented\nperspective. Building on this taxonomy, we make three key contributions. First,\nwe revisit recent findings suggesting that many removal methods may\nfunctionally behave like suppression, and explore whether true removal is\nnecessary or achievable. Second, we survey existing evaluation strategies,\nidentify limitations in current metrics and benchmarks, and suggest directions\nfor developing more reliable and intention-aligned evaluations. Third, we\nhighlight practical challenges--such as scalability and support for sequential\nunlearning--that currently hinder the broader deployment of unlearning methods.\nIn summary, this work offers a comprehensive framework for understanding and\nadvancing unlearning in generative AI, aiming to support future research and\nguide policy decisions around data removal and privacy.", "AI": {"tldr": "本文提出了一种基于意图的新分类法，探讨了LLM遗忘技术的真实目标，并分析了现有方法的局限性与未来方向。", "motivation": "研究LLM遗忘技术的根本意图，区分真实知识移除与行为抑制，以推动更可靠的遗忘方法发展。", "method": "提出基于意图的分类法，分析现有遗忘技术的功能性与局限性，并探讨评估策略的改进方向。", "result": "发现许多遗忘方法实际为行为抑制，提出真实移除的必要性与可行性问题，并指出评估与实践挑战。", "conclusion": "为生成AI中的遗忘技术提供了全面框架，支持未来研究与数据隐私政策制定。"}}
{"id": "2506.09417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09417", "abs": "https://arxiv.org/abs/2506.09417", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "title": "ODG: Occupancy Prediction Using Dual Gaussians", "comment": null, "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "AI": {"tldr": "论文提出了一种结合BEV和稀疏点表示的新方法ODG，用于3D占用预测，解决了现有方法在小物体和平坦表面上的不足。", "motivation": "现有3D占用预测方法在计算成本、小物体信息丢失或平坦表面建模效率上存在不足，需要一种更高效且全面的解决方案。", "method": "采用双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息，最终融合输出预测的3D占用。", "result": "在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，同时推理速度与最新高效方法相当。", "conclusion": "ODG结合了BEV和稀疏点的优势，显著提升了3D占用预测的性能和效率。"}}
{"id": "2506.09247", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09247", "abs": "https://arxiv.org/abs/2506.09247", "authors": ["Karl Löwenmark", "Daniel Strömbergsson", "Chang Liu", "Marcus Liwicki", "Fredrik Sandin"], "title": "Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation", "comment": null, "summary": "Condition monitoring (CM) plays a crucial role in ensuring reliability and\nefficiency in the process industry. Although computerised maintenance systems\neffectively detect and classify faults, tasks like fault severity estimation,\nand maintenance decisions still largely depend on human expert analysis. The\nanalysis and decision making automatically performed by current systems\ntypically exhibit considerable uncertainty and high false alarm rates, leading\nto increased workload and reduced efficiency.\n  This work integrates large language model (LLM)-based reasoning agents with\nCM workflows to address analyst and industry needs, namely reducing false\nalarms, enhancing fault severity estimation, improving decision support, and\noffering explainable interfaces. We propose MindRAG, a modular framework\ncombining multimodal retrieval-augmented generation (RAG) with novel vector\nstore structures designed specifically for CM data. The framework leverages\nexisting annotations and maintenance work orders as surrogates for labels in a\nsupervised learning protocol, addressing the common challenge of training\npredictive models on unlabelled and noisy real-world datasets.\n  The primary contributions include: (1) an approach for structuring industry\nCM data into a semi-structured multimodal vector store compatible with\nLLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM\ndata; (3) developing practical reasoning agents capable of addressing\nreal-world CM queries; and (4) presenting an experimental framework for\nintegrating and evaluating such agents in realistic industrial scenarios.\nPreliminary results, evaluated with the help of an experienced analyst,\nindicate that MindRAG provide meaningful decision support for more efficient\nmanagement of alarms, thereby improving the interpretability of CM systems.", "AI": {"tldr": "论文提出MindRAG框架，结合多模态检索增强生成（RAG）和新型向量存储结构，用于工业条件监测（CM）系统，旨在减少误报、提升故障严重性估计和决策支持。", "motivation": "当前计算机化维护系统在故障检测和分类上有效，但在故障严重性估计和维护决策上仍依赖人工，且存在高误报率和不确定性，增加了工作负担。", "method": "提出MindRAG框架，结合多模态RAG技术和专为CM数据设计的向量存储结构，利用现有注释和维护工单作为监督学习的标签替代。", "result": "初步实验表明，MindRAG能提供有意义的决策支持，提升报警管理效率和CM系统的可解释性。", "conclusion": "MindRAG通过LLM驱动的推理代理和多模态RAG技术，有效解决了工业CM系统中的实际问题，提升了自动化决策的准确性和效率。"}}
{"id": "2506.09427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09427", "abs": "https://arxiv.org/abs/2506.09427", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "comment": null, "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "AI": {"tldr": "论文介绍了InterSyn数据集和SEIR方法，用于提升多模态模型的图像-文本交织生成能力，并提出了SynJudge评估工具。", "motivation": "当前多模态模型在生成交织图像-文本输出时表现不佳，主要因训练数据集规模、质量和指令丰富度不足。", "method": "提出SEIR方法构建InterSyn数据集，包含多轮指令驱动对话和交织图像-文本响应，并通过SynJudge评估模型。", "result": "实验表明SEIR显著提升数据集质量，基于InterSyn训练的模型在所有评估指标上均有提升。", "conclusion": "InterSyn和SynJudge为多模态系统的进步提供了有效工具。"}}
{"id": "2506.09258", "categories": ["cs.LG", "stat.ML", "62D10", "I.5.1"], "pdf": "https://arxiv.org/pdf/2506.09258", "abs": "https://arxiv.org/abs/2506.09258", "authors": ["Vaidotas Simkus", "Michael U. Gutmann"], "title": "CFMI: Flow Matching for Missing Data Imputation", "comment": null, "summary": "We introduce conditional flow matching for imputation (CFMI), a new\ngeneral-purpose method to impute missing data. The method combines continuous\nnormalising flows, flow-matching, and shared conditional modelling to deal with\nintractabilities of traditional multiple imputation. Our comparison with nine\nclassical and state-of-the-art imputation methods on 24 small to\nmoderate-dimensional tabular data sets shows that CFMI matches or outperforms\nboth traditional and modern techniques across a wide range of metrics. Applying\nthe method to zero-shot imputation of time-series data, we find that it matches\nthe accuracy of a related diffusion-based method while outperforming it in\nterms of computational efficiency. Overall, CFMI performs at least as well as\ntraditional methods on lower-dimensional data while remaining scalable to\nhigh-dimensional settings, matching or exceeding the performance of other deep\nlearning-based approaches, making it a go-to imputation method for a wide range\nof data types and dimensionalities.", "AI": {"tldr": "CFMI是一种新的通用缺失数据填补方法，结合了连续归一化流、流匹配和共享条件建模，优于传统和现代填补技术。", "motivation": "解决传统多重填补方法的计算难题，提供一种高效且通用的填补方案。", "method": "结合连续归一化流、流匹配和共享条件建模，处理缺失数据。", "result": "在24个数据集上表现优于9种传统和现代方法，计算效率高，适用于不同维度和数据类型。", "conclusion": "CFMI是一种高效、通用的填补方法，适用于广泛的数据类型和维度。"}}
{"id": "2506.09429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09429", "abs": "https://arxiv.org/abs/2506.09429", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "comment": null, "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "AI": {"tldr": "提出了一种轻量级Transformer架构，通过降低编码器维度并使用蒸馏版GPT-2解码器，结合知识蒸馏和边缘感知增强策略，显著提升了遥感图像描述质量。", "motivation": "解决Transformer模型在遥感图像描述中计算成本高和忽略细粒度结构特征的问题。", "method": "采用轻量级Transformer架构，降低编码器维度，使用蒸馏版GPT-2解码器，结合知识蒸馏和边缘感知增强策略。", "result": "实验结果表明，该方法显著提升了描述质量，优于现有方法。", "conclusion": "轻量级架构和边缘感知策略有效提升了遥感图像描述的准确性和效率。"}}
{"id": "2506.09270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09270", "abs": "https://arxiv.org/abs/2506.09270", "authors": ["Rodrigo Carrasco-Davis", "Sebastian Lee", "Claudia Clopath", "Will Dabney"], "title": "Uncertainty Prioritized Experience Replay", "comment": "Accepted at Reinforcement Learning Conference", "summary": "Prioritized experience replay, which improves sample efficiency by selecting\nrelevant transitions to update parameter estimates, is a crucial component of\ncontemporary value-based deep reinforcement learning models. Typically,\ntransitions are prioritized based on their temporal difference error. However,\nthis approach is prone to favoring noisy transitions, even when the value\nestimation closely approximates the target mean. This phenomenon resembles the\nnoisy TV problem postulated in the exploration literature, in which\nexploration-guided agents get stuck by mistaking noise for novelty. To mitigate\nthe disruptive effects of noise in value estimation, we propose using epistemic\nuncertainty estimation to guide the prioritization of transitions from the\nreplay buffer. Epistemic uncertainty quantifies the uncertainty that can be\nreduced by learning, hence reducing transitions sampled from the buffer\ngenerated by unpredictable random processes. We first illustrate the benefits\nof epistemic uncertainty prioritized replay in two tabular toy models: a simple\nmulti-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our\nprioritization scheme on the Atari suite, outperforming quantile regression\ndeep Q-learning benchmarks; thus forging a path for the use of uncertainty\nprioritized replay in reinforcement learning agents.", "AI": {"tldr": "提出了一种基于认知不确定性估计的经验回放优先策略，以减少噪声对价值估计的干扰，并在实验中获得优于基准的结果。", "motivation": "传统基于时序差分误差的优先经验回放容易受噪声干扰，类似于探索中的噪声电视问题，因此需要一种更鲁棒的优先策略。", "method": "利用认知不确定性估计指导经验回放优先选择，减少随机过程产生的噪声样本的影响。", "result": "在表格模型和Atari游戏中验证了方法的有效性，性能优于分位数回归DQN基准。", "conclusion": "认知不确定性优先回放为强化学习提供了一种有效的噪声抑制策略。"}}
{"id": "2506.09445", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09445", "abs": "https://arxiv.org/abs/2506.09445", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "comment": null, "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "AI": {"tldr": "论文提出了一种弱监督下的视频问答模型TOGA，无需时间标注即可生成开放答案及时间定位。", "motivation": "解决弱监督下视频问答中时间定位的问题，避免依赖时间标注。", "method": "提出TOGA模型，通过指令调优联合生成答案和时间定位，利用伪标签和一致性约束确保时间定位的有效性。", "result": "在NExT-GQA、MSVD-QA和ActivityNet-QA基准测试中取得最优性能。", "conclusion": "TOGA在弱监督下能有效提升视频问答和时间定位的性能。"}}
{"id": "2506.09508", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09508", "abs": "https://arxiv.org/abs/2506.09508", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "comment": null, "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "AI": {"tldr": "研究基于人类反馈的强化学习，提出一种随机探索的元算法，解决轨迹级偏好比较中的计算和理论挑战，并通过批量查询优化实验设计。", "motivation": "在马尔可夫决策过程中，如何从轨迹级偏好比较中高效学习奖励函数是一个关键挑战，需要设计既能保证理论性能又计算高效的算法。", "method": "提出基于随机探索的元算法，避免乐观方法的计算复杂性，并通过批量查询和最优实验设计优化查询效率。", "result": "在温和的强化学习假设下，算法实现了遗憾和最终迭代保证，实验表明其与基于奖励的强化学习性能相当且查询次数少。", "conclusion": "该算法在理论和实践中均表现优异，适用于需要高效人类反馈的强化学习场景。"}}
{"id": "2506.09272", "categories": ["cs.LG", "stat.ML", "68T05, 68U20, 62F15", "I.2.6; I.6.5; G.3"], "pdf": "https://arxiv.org/pdf/2506.09272", "abs": "https://arxiv.org/abs/2506.09272", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Antonin Berthon", "Mihaela van der Schaar"], "title": "G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration", "comment": "Accepted at the 42nd International Conference on Machine Learning\n  (ICML 2025). 9 pages, 3 figures", "summary": "Constructing robust simulators is essential for asking \"what if?\" questions\nand guiding policy in critical domains like healthcare and logistics. However,\nexisting methods often struggle, either failing to generalize beyond historical\ndata or, when using Large Language Models (LLMs), suffering from inaccuracies\nand poor empirical alignment. We introduce G-Sim, a hybrid framework that\nautomates simulator construction by synergizing LLM-driven structural design\nwith rigorous empirical calibration. G-Sim employs an LLM in an iterative loop\nto propose and refine a simulator's core components and causal relationships,\nguided by domain knowledge. This structure is then grounded in reality by\nestimating its parameters using flexible calibration techniques. Specifically,\nG-Sim can leverage methods that are both likelihood-free and gradient-free with\nrespect to the simulator, such as gradient-free optimization for direct\nparameter estimation or simulation-based inference for obtaining a posterior\ndistribution over parameters. This allows it to handle non-differentiable and\nstochastic simulators. By integrating domain priors with empirical evidence,\nG-Sim produces reliable, causally-informed simulators, mitigating\ndata-inefficiency and enabling robust system-level interventions for complex\ndecision-making.", "AI": {"tldr": "G-Sim是一个混合框架，结合LLM驱动的结构设计和严格的实证校准，自动化构建可靠模拟器。", "motivation": "现有方法在泛化性和准确性上存在不足，无法满足关键领域（如医疗和物流）的需求。", "method": "G-Sim通过LLM迭代设计模拟器核心组件和因果关系，并使用灵活校准技术（如梯度自由优化）估计参数。", "result": "G-Sim能够处理非可微和随机模拟器，生成可靠且因果感知的模拟器。", "conclusion": "G-Sim通过结合领域先验和实证证据，解决了数据效率低的问题，支持复杂决策。"}}
{"id": "2506.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09446", "abs": "https://arxiv.org/abs/2506.09446", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "comment": null, "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "AI": {"tldr": "HAM是一个基于CLIP的领域泛化框架，通过无冲突样本增强和模型合并优化，提升模型在未见领域的泛化能力。", "motivation": "解决多源训练中的样本冲突和优化冲突问题，提升CLIP在领域泛化中的性能。", "method": "提出HAM框架，包括无冲突样本增强、模型更新方向协调和冗余感知的历史模型合并。", "result": "在五个基准数据集上实现最优性能。", "conclusion": "HAM通过协调和合并源模型，显著提升了领域泛化能力。"}}
{"id": "2506.09276", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09276", "abs": "https://arxiv.org/abs/2506.09276", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "Özgür Şimşek", "Anders Jonsson"], "title": "Learning The Minimum Action Distance", "comment": null, "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "AI": {"tldr": "提出一种仅从状态轨迹学习的MDP状态表示框架，无需奖励信号或动作信息，通过最小动作距离（MAD）捕捉环境结构，支持下游任务。", "motivation": "现有方法依赖奖励或动作信息，限制了应用范围。目标是开发一种自监督学习框架，仅需状态轨迹即可学习环境结构。", "method": "提出最小动作距离（MAD）作为核心度量，构建嵌入空间，使嵌入距离对应MAD，支持对称和非对称近似。", "result": "在多种环境中验证，包括确定性和随机动态、离散和连续状态空间，以及噪声观测。方法在表示质量上显著优于现有方法。", "conclusion": "MAD框架能高效学习准确的状态表示，适用于多样化环境，为下游任务提供有力支持。"}}
{"id": "2506.09460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09460", "abs": "https://arxiv.org/abs/2506.09460", "authors": ["Amirreza Khoshbakht", "Erchan Aptoula"], "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization", "comment": null, "summary": "Open-set domain generalization(OSDG) for hyperspectral image classification\npresents significant challenges due to the presence of unknown classes in\ntarget domains and the need for models to generalize across multiple unseen\ndomains without target-specific adaptation. Existing domain adaptation methods\nassume access to target domain data during training and fail to address the\nfundamental issue of domain shift when unknown classes are present, leading to\nnegative transfer and reduced classification performance. To address these\nlimitations, we propose a novel open-set domain generalization framework that\ncombines four key components: Spectrum-Invariant Frequency Disentanglement\n(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network\n(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning\n(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty\nDisentanglement (SSUD) for reliable open-set classification. The SIFD module\nextracts domain-invariant spectral features in the frequency domain through\nattention-weighted frequency analysis and domain-agnostic regularization, while\nDCRN captures complementary spectral and spatial information via parallel\npathways with adaptive fusion. EDL provides principled uncertainty estimation\nusing Dirichlet distributions, enabling the SSUD module to make reliable\nopen-set decisions through uncertainty-aware pathway weighting and adaptive\nrejection thresholding. Experimental results on three cross-scene hyperspectral\nclassification tasks show that our approach achieves performance comparable to\nstate-of-the-art domain adaptation methods while requiring no access to the\ntarget domain during training. The implementation will be made available at\nhttps://github.com/amir-khb/SSUDOSDG upon acceptance.", "AI": {"tldr": "本文提出了一种新的开放集域泛化框架，用于高光谱图像分类，解决了目标域中存在未知类别和跨域泛化的问题。", "motivation": "现有域适应方法假设训练时可访问目标域数据，且无法处理未知类别导致的域偏移问题，导致负迁移和分类性能下降。", "method": "结合频谱不变频率解耦（SIFD）、双通道残差网络（DCRN）、证据深度学习（EDL）和光谱空间不确定性解耦（SSUD）四个关键组件。", "result": "在三个跨场景高光谱分类任务中，性能与最先进的域适应方法相当，且无需训练时访问目标域数据。", "conclusion": "提出的框架有效解决了开放集域泛化问题，具有实际应用潜力。"}}
{"id": "2506.09650", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.09650", "abs": "https://arxiv.org/abs/2506.09650", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "AI": {"tldr": "本文提出了一种基于文本参考的多人物动作分割方法，并引入了首个相关数据集RHAS133。通过提出的HopaDIFF框架，结合跨输入门注意力xLSTM和傅里叶条件，显著提升了动作分割性能。", "motivation": "现有方法主要针对单人物固定动作序列，忽略了多人物场景。本文旨在解决多人物环境下的动作分割问题，并通过文本描述指定目标人物。", "method": "提出了HopaDIFF框架，结合跨输入门注意力xLSTM进行整体-局部长程推理，并引入傅里叶条件以增强动作分割生成的细粒度控制。", "result": "HopaDIFF在RHAS133数据集上实现了最先进的性能。", "conclusion": "本文通过新数据集和HopaDIFF框架，为多人物动作分割任务提供了有效解决方案，并展示了其优越性能。"}}
{"id": "2506.09279", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.09279", "abs": "https://arxiv.org/abs/2506.09279", "authors": ["Ziyi Chen", "Yiyang Liu", "Mattia Prosperi", "Krishna Vaddiparti", "Robert L Cook", "Jiang Bian", "Yi Guo", "Yonghui Wu"], "title": "A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV", "comment": null, "summary": "Objective: To characterize stigma dimensions, social, and related behavioral\ncircumstances in people living with HIV (PLWHs) seeking care, using natural\nlanguage processing methods applied to a large collection of electronic health\nrecord (EHR) clinical notes from a large integrated health system in the\nsoutheast United States. Methods: We identified 9,140 cohort of PLWHs from the\nUF Health IDR and performed topic modeling analysis using Latent Dirichlet\nAllocation (LDA) to uncover stigma dimensions, social, and related behavioral\ncircumstances. Domain experts created a seed list of HIV-related stigma\nkeywords, then applied a snowball strategy to iteratively review notes for\nadditional terms until saturation was reached. To identify more target topics,\nwe tested three keyword-based filtering strategies. Domain experts manually\nreviewed the detected topics using the prevalent terms and key discussion\ntopics. Word frequency analysis was used to highlight the prevalent terms\nassociated with each topic. In addition, we conducted topic variation analysis\namong subgroups to examine differences across age and sex-specific\ndemographics. Results and Conclusion: Topic modeling on sentences containing at\nleast one keyword uncovered a wide range of topic themes associated with\nHIV-related stigma, social, and related behaviors circumstances, including\n\"Mental Health Concern and Stigma\", \"Social Support and Engagement\", \"Limited\nHealthcare Access and Severe Illness\", \"Treatment Refusal and Isolation\" and so\non. Topic variation analysis across age subgroups revealed differences.\nExtracting and understanding the HIV-related stigma dimensions, social, and\nrelated behavioral circumstances from EHR clinical notes enables scalable,\ntime-efficient assessment, overcoming the limitations of traditional\nquestionnaires and improving patient outcomes.", "AI": {"tldr": "该研究利用自然语言处理技术分析电子健康记录，揭示HIV感染者面临的污名化、社会及行为问题，并提出了一种高效评估方法。", "motivation": "传统问卷评估HIV相关污名化和社会行为问题存在局限性，研究旨在通过电子健康记录分析提供更高效的解决方案。", "method": "使用LDA主题建模分析9,140名HIV感染者的临床记录，结合专家关键词筛选和手动审查，识别污名化及相关主题。", "result": "发现多个与HIV污名化相关的主题，如心理健康问题、社会支持、医疗资源限制等，并在不同年龄组中观察到差异。", "conclusion": "通过电子健康记录分析，可以高效评估HIV相关污名化问题，改进传统方法的不足，提升患者结果。"}}
{"id": "2506.09469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09469", "abs": "https://arxiv.org/abs/2506.09469", "authors": ["Maria Damanaki", "Nikos Piperigkos", "Alexandros Gkillas", "Aris S. Lalos"], "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing", "comment": "2025 IEEE International Conference on Multimedia and Expo Workshops,\n  3DMM - 3D Multimedia Analytics, Search and Generation", "summary": "Multi-Object Tracking (MOT) plays a crucial role in autonomous driving\nsystems, as it lays the foundations for advanced perception and precise path\nplanning modules. Nonetheless, single agent based MOT lacks in sensing\nsurroundings due to occlusions, sensors failures, etc. Hence, the integration\nof multiagent information is essential for comprehensive understanding of the\nenvironment. This paper proposes a novel Cooperative MOT framework for tracking\nobjects in 3D LiDAR scene by formulating and solving a graph topology-aware\noptimization problem so as to fuse information coming from multiple vehicles.\nBy exploiting a fully connected graph topology defined by the detected bounding\nboxes, we employ the Graph Laplacian processing optimization technique to\nsmooth the position error of bounding boxes and effectively combine them. In\nthat manner, we reveal and leverage inherent coherences of diverse multi-agent\ndetections, and associate the refined bounding boxes to tracked objects at two\nstages, optimizing localization and tracking accuracies. An extensive\nevaluation study has been conducted, using the real-world V2V4Real dataset,\nwhere the proposed method significantly outperforms the baseline frameworks,\nincluding the state-of-the-art deep-learning DMSTrack and V2V4Real, in various\ntesting sequences.", "AI": {"tldr": "提出了一种基于图拓扑感知优化的多智能体协同多目标跟踪（MOT）框架，通过融合多车信息提升跟踪精度。", "motivation": "单智能体MOT因遮挡和传感器故障等问题难以全面感知环境，多智能体信息融合是解决这一问题的关键。", "method": "利用检测到的边界框构建全连接图拓扑，通过图拉普拉斯优化技术平滑位置误差并融合多智能体检测信息，分两阶段关联跟踪对象。", "result": "在V2V4Real数据集上的实验表明，该方法显著优于基线框架及当前最先进的DMSTrack和V2V4Real方法。", "conclusion": "提出的协同MOT框架通过多智能体信息融合有效提升了跟踪精度，适用于自动驾驶系统。"}}
{"id": "2506.09748", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09748", "abs": "https://arxiv.org/abs/2506.09748", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "comment": "8 pages, 6 figures", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "AI": {"tldr": "提出了一种用于无人机绝对定位的分层跨源图像匹配方法，结合语义感知和结构约束的粗匹配模块与轻量级细粒度匹配模块，显著提升了定位精度和鲁棒性。", "motivation": "在GNSS信号不可用时，无人机绝对定位面临挑战，现有视觉定位方法因跨源差异和时间变化导致性能受限。", "method": "采用分层匹配方法，先通过语义感知和结构约束进行粗匹配，再通过细粒度匹配模块实现像素级对应，构建无需相对定位技术的视觉定位流程。", "result": "在公开基准数据集和新CS-UAV数据集上验证了方法的优越性和鲁棒性。", "conclusion": "该方法在多种挑战性条件下表现出高效性，为GNSS缺失场景下的无人机定位提供了可靠解决方案。"}}
{"id": "2506.09286", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2506.09286", "abs": "https://arxiv.org/abs/2506.09286", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "comment": null, "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "AI": {"tldr": "该论文提出了一种基于约束优化（ASP）的方法，用于从时间序列数据中学习因果图结构，解决了因采样频率不匹配导致的因果图模糊问题，并在模拟和实际数据中验证了其优越性。", "motivation": "解决时间序列数据中因采样频率不匹配导致的因果图模糊问题，提高因果图推断的准确性。", "method": "采用约束优化方法（ASP）推导因果图，结合图理论剪枝，生成更准确且高效的解集。", "result": "在模拟和实际数据（如脑结构连接）中验证了方法的优越性，F1分数平均提升12%，且在采样率变化时表现稳健。", "conclusion": "该方法显著提升了因果图推断的准确性和效率，适用于高采样率变化场景，并可作为现有方法的补充。"}}
{"id": "2506.09473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09473", "abs": "https://arxiv.org/abs/2506.09473", "authors": ["Cheng Chen", "Yunpeng Zhai", "Yifan Zhao", "Jinyang Gao", "Bolin Ding", "Jia Li"], "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning", "comment": "10 pages, 6 figures, CVPR 2025", "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims\nat enhancing the performance of large language models by providing clear task\nguidance and examples, improving their capability in task understanding and\nexecution. This paper investigates ICL on Large Vision-Language Models (LVLMs)\nand explores the policies of multi-modal demonstration selection. Existing\nresearch efforts in ICL face significant challenges: First, they rely on\npre-defined demonstrations or heuristic selecting strategies based on human\nintuition, which are usually inadequate for covering diverse task requirements,\nleading to sub-optimal solutions; Second, individually selecting each\ndemonstration fails in modeling the interactions between them, resulting in\ninformation redundancy. Unlike these prevailing efforts, we propose a new\nexploration-exploitation reinforcement learning framework, which explores\npolicies to fuse multi-modal information and adaptively select adequate\ndemonstrations as an integrated whole. The framework allows LVLMs to optimize\nthemselves by continually refining their demonstrations through\nself-exploration, enabling the ability to autonomously identify and generate\nthe most effective selection policies for in-context learning. Experimental\nresults verify the superior performance of our approach on four Visual\nQuestion-Answering (VQA) datasets, demonstrating its effectiveness in enhancing\nthe generalization capability of few-shot LVLMs.", "AI": {"tldr": "本文提出了一种基于探索-利用强化学习框架的多模态演示选择方法，用于提升大型视觉语言模型的上下文学习能力。", "motivation": "现有上下文学习方法依赖预定义演示或启发式选择策略，无法覆盖多样化任务需求且忽略演示间交互，导致性能受限。", "method": "采用探索-利用强化学习框架，自适应地选择并融合多模态演示，优化模型自我探索能力。", "result": "在四个视觉问答数据集上验证了方法的优越性，显著提升了少样本学习的泛化能力。", "conclusion": "该方法通过自主优化演示选择策略，有效增强了大型视觉语言模型的上下文学习性能。"}}
{"id": "2506.09839", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09839", "abs": "https://arxiv.org/abs/2506.09839", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "title": "OctoNav: Towards Generalist Embodied Navigation", "comment": "31 pages, 25 figures", "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "AI": {"tldr": "该论文提出了一种通用导航代理OctoNav-R1，支持多模态和多能力的自由指令导航，并设计了OctoNav-Bench基准和TBA-CoT数据集。通过混合训练范式（HTP）提升模型推理能力，性能优于现有方法。", "motivation": "现有导航研究分散于不同任务/能力（如ObjNav、ImgNav和VLN），缺乏通用性。本文旨在开发能处理多模态和多能力自由指令的通用导航代理。", "method": "提出OctoNav-Bench基准和TBA-CoT数据集，构建基于MLLMs的OctoNav-R1模型，采用混合训练范式（HTP）分三阶段训练：Action-/TBA-SFT、Nav-GPRO和Online RL。", "result": "OctoNav-R1在性能上优于现有方法。", "conclusion": "通过TBA-CoT数据集和HTP训练范式，实现了通用导航代理的推理能力提升，为多模态导航研究提供了新方向。"}}
{"id": "2506.09316", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09316", "abs": "https://arxiv.org/abs/2506.09316", "authors": ["Yeonju Ro", "Zhenyu Zhang", "Souvik Kundu", "Zhangyang Wang", "Aditya Akella"], "title": "On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention", "comment": null, "summary": "Large language models (LLMs) excel at capturing global token dependencies via\nself-attention but face prohibitive compute and memory costs on lengthy inputs.\nWhile sub-quadratic methods (e.g., linear attention) can reduce these costs,\nthey often degrade accuracy due to overemphasizing recent tokens. In this work,\nwe first propose \\textit{dual-state linear attention} (\\textbf{\\dsla}), a novel\ndesign that maintains two specialized hidden states-one for preserving\nhistorical context and one for tracking recency-thereby mitigating the\nshort-range bias typical of linear-attention architectures. To further balance\nefficiency and accuracy under dynamic workload conditions, we introduce\n\\textbf{\\serve}, an online \\textit{adaptive distillation} framework that\nprogressively replaces Transformer layers with DSLA layers at inference time,\nguided by a sensitivity-based layer ordering. \\serve\\ uses a chained\nfine-tuning strategy to ensure that each newly converted DSLA layer remains\nconsistent with previously replaced layers, preserving the overall quality.\nExtensive evaluations on commonsense reasoning, long-context QA, and text\nsummarization demonstrate that \\serve\\ yields \\textbf{2.3x} faster inference\nthan Llama2-7B and \\textbf{3.0x} faster than the hybrid Zamba-7B, while\nretaining comparable performance across downstream tasks. Our ablation studies\nshow that DSLA's dual states capture both global and local dependencies,\naddressing the historical-token underrepresentation seen in prior linear\nattentions. Codes are available at https://github.com/utnslab/DSLA-Serve.", "AI": {"tldr": "论文提出了一种双状态线性注意力（DSLA）和自适应蒸馏框架（Serve），以解决长输入下LLMs的计算和内存成本问题，同时保持准确性。", "motivation": "解决线性注意力方法在长输入下因过度关注近期token而导致的准确性下降问题。", "method": "提出DSLA，通过维护两个隐藏状态（历史上下文和近期性）来缓解线性注意力的短程偏差；引入Serve框架，动态替换Transformer层为DSLA层，并使用链式微调策略。", "result": "Serve比Llama2-7B快2.3倍，比Zamba-7B快3.0倍，同时在下游任务中保持可比性能。", "conclusion": "DSLA和Serve在效率和准确性之间取得了平衡，解决了线性注意力中历史token表示不足的问题。"}}
{"id": "2506.09476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09476", "abs": "https://arxiv.org/abs/2506.09476", "authors": ["Tianxiang Hao", "Lixian Zhang", "Yingjia Zhang", "Mengxuan Chen", "Jinxiao Zhang", "Haohuan Fu"], "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries", "comment": null, "summary": "Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,\noffers rare insights into understanding early urban development and long-term\ntransformation. However, severe quality degradation (e.g., distortion,\nmisalignment, and spectral scarcity) and annotation absence have long hindered\nsemantic segmentation on such historical RS imagery. To bridge this gap and\nenhance understanding of urban development, we introduce\n$\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on\nhistorical satellite imagery with the earliest observation time among all\nexisting segmentation datasets, along with a benchmark framework for\nunsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First,\n$\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic\nsegmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering\n1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the\nearliest segmentation dataset of its kind, it provides a pioneering benchmark\nfor historical urban understanding. Second,\n$\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel\nunsupervised semantic segmentation framework for historical RS imagery. It\nemploys a confidence-aware alignment mechanism and focal-confidence loss based\non a self-supervised learning architecture, which generates robust\npseudo-labels and adaptively prioritizes prediction difficulty and label\nreliability to improve unsupervised segmentation on noisy historical data\nwithout manual supervision. Experiments show Urban1960SatUSM significantly\noutperforms existing unsupervised segmentation methods on Urban1960SatSeg for\nsegmenting historical urban scenes, promising in paving the way for\nquantitative studies of long-term urban change using modern computer vision.\nOur benchmark and supplementary material are available at\nhttps://github.com/Tianxiang-Hao/Urban1960SatSeg.", "AI": {"tldr": "论文提出了Urban1960SatBench数据集和Urban1960SatUSM框架，用于解决历史卫星图像的语义分割问题，填补了早期城市发展研究的空白。", "motivation": "历史卫星图像（如20世纪中叶的Keyhole数据）因质量退化（失真、错位、光谱稀缺）和缺乏标注，难以进行语义分割。研究旨在通过新数据集和框架提升对城市发展的理解。", "method": "1. 构建Urban1960SatBench数据集，覆盖1240平方公里，标注关键城市类别；2. 提出Urban1960SatUSM框架，采用置信对齐机制和焦点置信损失，实现无监督分割。", "result": "Urban1960SatUSM在历史城市场景分割中显著优于现有无监督方法，为长期城市变化的定量研究提供了可能。", "conclusion": "Urban1960SatBench和Urban1960SatUSM为历史卫星图像的语义分割提供了新工具，推动了城市发展研究的进展。"}}
{"id": "2506.09981", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09981", "abs": "https://arxiv.org/abs/2506.09981", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "title": "ReSim: Reliable World Simulation for Autonomous Driving", "comment": "Project page: https://opendrivelab.com/ReSim", "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "AI": {"tldr": "论文提出了一种通过结合真实世界和模拟驾驶数据构建可控世界模型的方法，以可靠模拟多样化驾驶场景，包括危险和非专家行为。", "motivation": "现有驾驶世界模型仅基于真实世界的安全专家轨迹数据，难以模拟罕见但重要的危险或非专家行为，限制了其应用范围。", "method": "通过整合真实人类驾驶数据和模拟器（如CARLA）收集的非专家数据，构建了一个可控世界模型，并采用扩散变换器架构的视频生成器，结合多种策略提高预测可控性和保真度。", "result": "提出的ReSim模型在视觉保真度上提高了44%，对专家和非专家行为的可控性提升超过50%，并在NAVSIM上分别提高了规划和策略选择性能2%和25%。", "conclusion": "ReSim方法通过结合真实和模拟数据，显著提升了驾驶场景模拟的多样性和可靠性，为政策评估等任务提供了更强大的工具。"}}
{"id": "2506.09332", "categories": ["cs.LG", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09332", "abs": "https://arxiv.org/abs/2506.09332", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "title": "Natural Language Guided Ligand-Binding Protein Design", "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "AI": {"tldr": "InstructPro是一种基于自然语言指令设计蛋白质的AI模型，能够根据功能描述和配体公式生成功能一致的蛋白质序列，性能优于现有基线模型。", "motivation": "设计能够结合特定配体的蛋白质在生物学和化学中具有广泛应用，但现有AI模型依赖稀缺的蛋白质-配体复合物数据。人类整理的文本描述提供了丰富的信息，因此提出利用自然语言指令生成蛋白质。", "method": "提出InstructPro模型，包括1B和3B参数两个变体，使用大规模数据集InstructProBench（包含9,592,829个三元组）进行训练和评估。", "result": "InstructPro-1B和InstructPro-3B均优于基线模型（ProGen2、ESM3、Pinal），其中InstructPro-3B的平均RMSD最低（2.527Å），InstructPro-1B的对接成功率最高（81.52%）。", "conclusion": "InstructPro能够根据自然语言指令生成功能一致的蛋白质序列，为蛋白质设计提供了高效的新方法。"}}
{"id": "2506.09479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09479", "abs": "https://arxiv.org/abs/2506.09479", "authors": ["Zetian Song", "Jiaye Fu", "Jiaqi Zhang", "Xiaohan Lu", "Chuanmin Jia", "Siwei Ma", "Wen Gao"], "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation", "comment": null, "summary": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a\nnew paradigm to reconstruct 3D scenes. Using neural networks trained on\nlarge-scale multi-view datasets, it can directly infer 3DGS representations\nfrom sparse input views. Although the feedforward approach achieves high\nreconstruction speed, it still suffers from the substantial storage cost of 3D\nGaussians. Existing 3DGS compression methods relying on scene-wise optimization\nare not applicable due to architectural incompatibilities. To overcome this\nlimitation, we propose TinySplat, a complete feedforward approach for\ngenerating compact 3D scene representations. Built upon standard feedforward\n3DGS methods, TinySplat integrates a training-free compression framework that\nsystematically eliminates key sources of redundancy. Specifically, we introduce\nView-Projection Transformation (VPT) to reduce geometric redundancy by\nprojecting geometric parameters into a more compact space. We further present\nVisibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy\nby aligning feature energy along dominant viewing directions via basis\ntransformation. Lastly, spatial redundancy is addressed through an\noff-the-shelf video codec. Comprehensive experimental results on multiple\nbenchmark datasets demonstrate that TinySplat achieves over 100x compression\nfor 3D Gaussian data generated by feedforward methods. Compared to the\nstate-of-the-art compression approach, we achieve comparable quality with only\n6% of the storage size. Meanwhile, our compression framework requires only 25%\nof the encoding time and 1% of the decoding time.", "AI": {"tldr": "TinySplat提出了一种无需训练的压缩框架，通过消除几何、感知和空间冗余，显著减少了3D高斯数据的存储成本，同时保持高质量和高效性能。", "motivation": "现有的3D高斯重建方法虽然速度快，但存储成本高，且传统压缩方法因架构不兼容无法适用。", "method": "TinySplat结合了View-Projection Transformation（VPT）减少几何冗余，Visibility-Aware Basis Reduction（VABR）减少感知冗余，以及视频编解码器减少空间冗余。", "result": "实验表明，TinySplat实现了100倍以上的压缩比，存储大小仅为现有方法的6%，编码时间减少75%，解码时间减少99%。", "conclusion": "TinySplat为3D高斯数据提供了一种高效、高质量的压缩解决方案。"}}
{"id": "2506.09347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09347", "abs": "https://arxiv.org/abs/2506.09347", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "comment": "12 pages", "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "AI": {"tldr": "论文提出了一种名为ErrorEraser的通用插件，通过识别和消除由数据偏差引起的错误记忆，提升持续学习中的知识保留和迁移能力。", "motivation": "现有持续学习方法忽视了真实数据中的偏差，导致模型学习到虚假相关性，影响知识保留和迁移。", "method": "ErrorEraser包含两个模块：错误识别（通过特征空间概率密度分布识别偏差样本）和错误擦除（通过调整决策空间消除错误知识）。", "result": "实验表明，ErrorEraser显著减少了数据偏差的负面影响，在三种持续学习方法中实现了更高的准确率和更低的遗忘率。", "conclusion": "ErrorEraser通过主动消除错误记忆，提升了持续学习的性能，为处理数据偏差提供了新思路。"}}
{"id": "2506.09482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09482", "abs": "https://arxiv.org/abs/2506.09482", "authors": ["Dingcheng Zhen", "Qian Qiao", "Tan Yu", "Kangxi Wu", "Ziwei Zhang", "Siyuan Liu", "Shunshun Yin", "Ming Tao"], "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression", "comment": null, "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.", "AI": {"tldr": "TransDiff结合自回归Transformer和扩散模型，显著提升图像生成性能，并引入多参考自回归（MRAR）进一步优化生成质量。", "motivation": "结合自回归Transformer和扩散模型的优势，提升图像生成的质量和效率。", "method": "TransDiff通过联合建模框架编码标签和图像语义特征，利用扩散模型估计图像样本分布，并引入MRAR实现多参考生成。", "result": "在ImageNet 256x256上，FID为1.61，IS为293.4，推理速度显著优于现有方法；MRAR进一步将FID降至1.42。", "conclusion": "TransDiff为图像生成领域开辟了新方向，结合MRAR进一步提升了性能。"}}
{"id": "2506.09348", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.09348", "abs": "https://arxiv.org/abs/2506.09348", "authors": ["Natalie S. Frank"], "title": "Adversarial Surrogate Risk Bounds for Binary Classification", "comment": "37 pages, 2 figures", "summary": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest.", "AI": {"tldr": "本文研究了对抗训练中替代风险与分类风险收敛速率的关系，并提出了替代风险边界以量化这一速率。", "motivation": "对抗训练是训练鲁棒分类器的流行方法，但此前研究未量化替代风险最小化序列的分类风险收敛速率。", "method": "通过理论分析，推导了替代风险边界，量化了对抗分类风险的收敛速率。", "result": "提出了替代风险边界，证明了其在对抗和非对抗学习中的有效性。", "conclusion": "本文填补了对抗训练中收敛速率量化的空白，并提供了通用的替代风险边界。"}}
{"id": "2506.09510", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.09510", "abs": "https://arxiv.org/abs/2506.09510", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "comment": null, "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "AI": {"tldr": "论文提出了一种广义高斯熵模型和动态调整似然区间的MED方法，显著提升了点云属性压缩的性能。", "motivation": "当前方法中熵参数估计未充分利用信息，且固定似然区间限制了模型性能。", "method": "引入广义高斯熵模型控制尾部形状，并提出MED动态调整似然区间。", "result": "实验表明，该方法显著提升了三种VAE模型的率失真性能，并适用于其他压缩任务。", "conclusion": "广义高斯熵模型和MED方法有效提升了压缩性能，具有广泛适用性。"}}
{"id": "2506.09368", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09368", "abs": "https://arxiv.org/abs/2506.09368", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "comment": "20 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "AI": {"tldr": "综述探讨了扩散模型在异常检测与生成中的协同作用，分析了其理论、实现及未来方向。", "motivation": "异常检测在多个领域至关重要，扩散模型因其学习复杂数据分布的能力成为研究热点。", "method": "通过教程式分析扩散模型的异常检测与生成方法，提出分类法并讨论其优劣。", "result": "揭示了检测与生成的协同关系，提出未来研究方向如高效架构和基础模型集成。", "conclusion": "为研究人员和实践者提供了利用扩散模型解决异常检测问题的指导。"}}
{"id": "2506.09518", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09518", "abs": "https://arxiv.org/abs/2506.09518", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Chengxuan Qian", "Juyuan Kang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene", "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppresses\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.", "AI": {"tldr": "HAIF-GS是一种动态3D场景重建框架，通过稀疏锚点驱动变形解决现有方法的冗余更新、运动监督不足和非刚性变形建模弱的问题。", "motivation": "单目视频动态3D场景重建是3D视觉的基础挑战，现有3D高斯泼溅（3DGS）方法在动态场景中存在冗余更新、运动监督不足和非刚性变形建模弱的问题。", "method": "HAIF-GS通过锚点过滤器识别运动相关区域，利用自监督流引导变形模块和多级锚点传播机制实现高效动态建模。", "result": "实验表明，HAIF-GS在渲染质量、时间一致性和重建效率上显著优于现有动态3DGS方法。", "conclusion": "HAIF-GS为动态3D场景重建提供了一种高效且一致的解决方案。"}}
{"id": "2506.09373", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09373", "abs": "https://arxiv.org/abs/2506.09373", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "comment": null, "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "AI": {"tldr": "论文提出了一种名为LPO的新方法，通过位置偏好优化提升GUI交互精度，优于现有方法。", "motivation": "现有GUI代理在空间定位上表现不佳，尤其是SFT方法在位置感知上存在局限，而强化学习等方法难以有效评估位置准确性。", "method": "LPO利用位置数据和信息熵预测交互位置，并引入动态位置奖励函数，结合GRPO优化交互偏好。", "result": "实验表明LPO在离线和在线评估中均达到SOTA性能。", "conclusion": "LPO显著提升了GUI交互的精确性，代码将开源。"}}
{"id": "2506.09522", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09522", "abs": "https://arxiv.org/abs/2506.09522", "authors": ["Beomsik Cho", "Jaehyung Kim"], "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "AI": {"tldr": "ReVisiT是一种简单有效的解码方法，通过引用视觉标记来指导大型视觉语言模型（LVLM）的文本生成，提升视觉信息的利用。", "motivation": "传统解码策略未能充分利用视觉信息，导致视觉无关的响应。现有方法通常需要额外训练或多步推理，ReVisiT旨在以低计算成本解决这一问题。", "method": "ReVisiT通过将视觉标记投影到文本标记分布空间，并动态选择最相关的视觉标记来优化输出分布，从而更好地融入视觉语义。", "result": "在三个LVLM幻觉基准测试中，ReVisiT显著提升了视觉相关性，计算成本降低至2倍。", "conclusion": "ReVisiT在提升视觉相关性的同时，以较低计算成本实现了与现有最佳方法相当或更优的结果。"}}
{"id": "2506.09376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09376", "abs": "https://arxiv.org/abs/2506.09376", "authors": ["Bowen Zheng", "Tianming Yang"], "title": "Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation", "comment": "ICML 2025", "summary": "Diffusion distillation is a widely used technique to reduce the sampling cost\nof diffusion models, yet it often requires extensive training, and the student\nperformance tends to be degraded. Recent studies show that incorporating a GAN\nobjective may alleviate these issues, yet the underlying mechanism remains\nunclear. In this work, we first identify a key limitation of distillation:\nmismatched step sizes and parameter numbers between the teacher and the student\nmodel lead them to converge to different local minima, rendering direct\nimitation suboptimal. We further demonstrate that a standalone GAN objective,\nwithout relying a distillation loss, overcomes this limitation and is\nsufficient to convert diffusion models into efficient one-step generators.\nBased on this finding, we propose that diffusion training may be viewed as a\nform of generative pre-training, equipping models with capabilities that can be\nunlocked through lightweight GAN fine-tuning. Supporting this view, we create a\none-step generation model by fine-tuning a pre-trained model with 85% of\nparameters frozen, achieving strong performance with only 0.2M images and\nnear-SOTA results with 5M images. We further present a frequency-domain\nanalysis that may explain the one-step generative capability gained in\ndiffusion training. Overall, our work provides a new perspective for diffusion\ntraining, highlighting its role as a powerful generative pre-training process,\nwhich can be the basis for building efficient one-step generation models.", "AI": {"tldr": "该论文提出扩散蒸馏技术存在局限性，通过引入GAN目标克服了这一问题，将扩散模型转化为高效的一步生成器。", "motivation": "扩散蒸馏技术通常需要大量训练且学生模型性能下降，GAN目标可能缓解这一问题，但其机制尚不明确。", "method": "研究发现扩散蒸馏的局限性源于师生模型步长和参数不匹配，提出仅用GAN目标即可实现高效一步生成，并通过轻量级微调验证。", "result": "通过冻结85%参数的预训练模型微调，仅需0.2M图像即可实现强性能，5M图像接近SOTA结果。", "conclusion": "扩散训练可视为生成预训练过程，为构建高效一步生成模型提供新视角。"}}
{"id": "2506.09534", "categories": ["cs.CV", "I.4.5"], "pdf": "https://arxiv.org/pdf/2506.09534", "abs": "https://arxiv.org/abs/2506.09534", "authors": ["Tao Wang", "Mengyu Li", "Geduo Zeng", "Cheng Meng", "Qiong Zhang"], "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS", "comment": "18 pages, 8 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance\nfield rendering, but it typically requires millions of redundant Gaussian\nprimitives, overwhelming memory and rendering budgets. Existing compaction\napproaches address this by pruning Gaussians based on heuristic importance\nscores, without global fidelity guarantee. To bridge this gap, we propose a\nnovel optimal transport perspective that casts 3DGS compaction as global\nGaussian mixture reduction. Specifically, we first minimize the composite\ntransport divergence over a KD-tree partition to produce a compact geometric\nrepresentation, and then decouple appearance from geometry by fine-tuning color\nand opacity attributes with far fewer Gaussian primitives. Experiments on\nbenchmark datasets show that our method (i) yields negligible loss in rendering\nquality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;\nand (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.\nNotably, our method is applicable to any stage of vanilla or accelerated 3DGS\npipelines, providing an efficient and agnostic pathway to lightweight neural\nrendering.", "AI": {"tldr": "提出了一种基于最优传输的3D高斯分布压缩方法，显著减少冗余高斯基元，同时保持渲染质量。", "motivation": "传统3D高斯分布渲染技术存在大量冗余高斯基元，占用过多内存和渲染资源，现有压缩方法缺乏全局保真度保证。", "method": "通过最优传输视角将3D高斯分布压缩建模为全局高斯混合缩减，利用KD树分区最小化复合传输散度，并解耦几何与外观属性进行微调。", "result": "实验表明，该方法仅需10%的高斯基元即可实现与原始方法相当的渲染质量（PSNR、SSIM、LPIPS），且优于现有压缩技术。", "conclusion": "该方法为轻量级神经渲染提供了一种高效且通用的解决方案，适用于任何3D高斯分布渲染流程。"}}
{"id": "2506.09398", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.09398", "abs": "https://arxiv.org/abs/2506.09398", "authors": ["Haiyang Yu", "Yuchao Lin", "Xuan Zhang", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames", "comment": "Code available at: https://github.com/divelab/AIRS", "summary": "We consider the task of predicting Hamiltonian matrices to accelerate\nelectronic structure calculations, which plays an important role in physics,\nchemistry, and materials science. Motivated by the inherent relationship\nbetween the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local\nframe, we propose a novel and efficient network, called QHNetV2, that achieves\nglobal SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor\nproducts. This is achieved by introducing a set of new efficient and powerful\nSO(2)-equivariant operations and performing all off-diagonal feature updates\nand message passing within SO(2) local frames, thereby eliminating the need of\nSO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed\nwithin the SO(2) local frame at each node to fuse node features, mimicking the\nsymmetric contraction operation. Extensive experiments on the large QH9 and\nMD17 datasets demonstrate that our model achieves superior performance across a\nwide range of molecular structures and trajectories, highlighting its strong\ngeneralization capability. The proposed SO(2) operations on SO(2) local frames\noffer a promising direction for scalable and symmetry-aware learning of\nelectronic structures. Our code will be released as part of the AIRS library\nhttps://github.com/divelab/AIRS.", "AI": {"tldr": "提出了一种名为QHNetV2的高效网络，通过SO(2)局部框架实现全局SO(3)等变性，避免了昂贵的SO(3)张量积操作，显著提升了电子结构计算的效率。", "motivation": "电子结构计算在物理、化学和材料科学中至关重要，但传统方法计算成本高。通过利用哈密顿矩阵的非对角块与SO(2)局部框架的关系，设计了一种更高效的网络。", "method": "引入新的SO(2)等变操作，在SO(2)局部框架内完成非对角特征更新和消息传递，避免了SO(3)张量积，并通过连续SO(2)张量积融合节点特征。", "result": "在QH9和MD17数据集上的实验表明，模型在多种分子结构和轨迹上表现优异，具有强泛化能力。", "conclusion": "SO(2)局部框架操作为可扩展且对称感知的电子结构学习提供了新方向，代码将开源。"}}
{"id": "2506.09538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09538", "abs": "https://arxiv.org/abs/2506.09538", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "comment": null, "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "AI": {"tldr": "本文研究了文本到图像（T2I）扩散模型生成的对抗性补丁在物理世界中的角度鲁棒性问题，提出了Angle-Robust Concept Learning（AngleRoCL）方法，显著提升了补丁的视角鲁棒性。", "motivation": "现有方法忽略了T2I对抗性补丁在不同视角下的攻击效果，本文旨在解决这一问题，揭示补丁的角度鲁棒性问题。", "method": "提出AngleRoCL方法，通过学习通用概念（文本嵌入）来生成具有视角鲁棒性的对抗性补丁。", "result": "实验表明，AngleRoCL显著提升了补丁的视角鲁棒性，攻击成功率在挑战性视角下仍保持较高水平。", "conclusion": "本研究深化了对物理视角鲁棒性补丁的理解，揭示了文本概念与T2I生成内容物理属性之间的关系。"}}
{"id": "2506.09541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09541", "abs": "https://arxiv.org/abs/2506.09541", "authors": ["Yi Zhang", "Yi Wang", "Yawen Cui", "Lap-Pui Chau"], "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection\napproach that effectively handles single- and multi-view RGB images in indoor\nand outdoor environments, showcasing its general-purpose applicability. The key\nchallenge for image-based 3D object detection tasks is the lack of 3D geometric\ncues, which leads to ambiguity in establishing correspondences between images\nand 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D\ngeometric representations in both explicit and implicit manners based on\npredicted depth information. Specifically, we utilize the predicted depth to\nlearn voxel occupancy and optimize the voxelized 3D feature volume explicitly\nthrough the proposed voxel occupancy attention. To further enhance 3D\nawareness, the feature volume is integrated with an implicit 3D representation,\nthe truncated signed distance function (TSDF). Without requiring supervision\nfrom 3D signals, we significantly improve the model's comprehension of 3D\ngeometry by leveraging intermediate 3D representations and achieve end-to-end\ntraining. Our approach surpasses the performance of state-of-the-art\nimage-based methods on both single- and multi-view benchmark datasets across\ndiverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D\ndataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19\nAP3D@0.7 improvement on the KITTI dataset. The project page is available at:\nhttps://cindy0725.github.io/3DGeoDet/.", "AI": {"tldr": "3DGeoDet是一种新颖的几何感知3D物体检测方法，通过显式和隐式3D几何表示提升性能，在多个数据集上表现优异。", "motivation": "解决基于图像的3D物体检测中缺乏3D几何线索的问题，提升模型对3D几何的理解。", "method": "结合预测深度生成显式（体素占用）和隐式（TSDF）3D表示，通过体素占用注意力优化特征体积。", "result": "在SUN RGB-D、ScanNetV2和KITTI数据集上分别提升9.3、3.3和0.19的指标。", "conclusion": "3DGeoDet通过几何表示显著提升性能，适用于多种环境和视角。"}}
{"id": "2506.09433", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09433", "abs": "https://arxiv.org/abs/2506.09433", "authors": ["Shurui Gui", "Shuiwang Ji"], "title": "Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training", "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin language modeling, recent studies reveal that they often fail on\nout-of-distribution (OOD) samples due to spurious correlations acquired during\npre-training. Here, we aim to mitigate such spurious correlations through\ncausality-aware post-training (CAPT). By decomposing a biased prediction into\ntwo unbiased steps, known as \\textit{event estimation} and \\textit{event\nintervention}, we reduce LLMs' pre-training biases without incurring additional\nfine-tuning biases, thus enhancing the model's generalization ability.\nExperiments on the formal causal inference benchmark CLadder and the logical\nreasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with\nCAPT can outperform both traditional SFT and larger LLMs on in-distribution\n(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the\neffectiveness and sample efficiency of CAPT.", "AI": {"tldr": "论文提出了一种因果感知的后训练方法（CAPT），通过分解有偏预测为两个无偏步骤，减少大语言模型（LLM）的预训练偏差，提升泛化能力。实验表明，CAPT在少量样本下优于传统方法。", "motivation": "解决大语言模型在分布外样本上因预训练中的虚假相关性而表现不佳的问题。", "method": "采用因果感知的后训练（CAPT），将预测分解为事件估计和事件干预两个无偏步骤。", "result": "在CLadder和PrOntoQA数据集上，CAPT在少量样本下优于传统方法和更大规模的LLM。", "conclusion": "CAPT能有效减少预训练偏差，提升模型的泛化能力和样本效率。"}}
{"id": "2506.09553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09553", "abs": "https://arxiv.org/abs/2506.09553", "authors": ["Ligao Deng", "Yupeng Deng", "Yu Meng", "Jingbo Chen", "Zhihao Xi", "Diyou Liu", "Qifeng Chu"], "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images", "comment": null, "summary": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.", "AI": {"tldr": "GLD-Road是一种两阶段模型，结合全局效率和局部精度，显著提升道路网络提取性能并减少计算时间。", "motivation": "手动标注道路网络成本高，现有深度学习方法存在效率或精度不足的问题。", "method": "GLD-Road分两阶段：全局检测道路节点并连接，局部迭代修复断裂道路。", "result": "实验显示GLD-Road在APLS指标上优于现有方法，计算时间大幅减少。", "conclusion": "GLD-Road在道路网络提取任务中实现了高效与高精度的平衡。"}}
{"id": "2506.09438", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.09438", "abs": "https://arxiv.org/abs/2506.09438", "authors": ["Haoxiang Ye", "Tao Sun", "Qing Ling"], "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity", "comment": null, "summary": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings.", "AI": {"tldr": "本文分析了去中心化学习在异构数据下的泛化误差，包括无攻击和拜占庭攻击场景，揭示了数据异质性、模型初始化和随机梯度噪声的影响。", "motivation": "去中心化学习的泛化误差研究较少，但其对模型在真实场景中的性能至关重要。", "method": "在异构数据和温和假设下，对无攻击和拜占庭攻击场景进行细粒度泛化误差分析。", "result": "发现数据异质性、模型初始化和随机梯度噪声对泛化误差有显著影响，拜占庭攻击的负面影响与数据异质性相关。", "conclusion": "数值实验验证了理论发现，强调了泛化误差分析在去中心化学习中的重要性。"}}
{"id": "2506.09557", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09557", "abs": "https://arxiv.org/abs/2506.09557", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "AI": {"tldr": "AD^2-Bench是首个针对恶劣天气和复杂场景下自动驾驶的Chain-of-Thought（CoT）评测基准，填补了现有评测的空白。", "motivation": "现有评测基准忽视了在恶劣天气和复杂交通环境下对CoT过程的严格评估需求。", "method": "AD^2-Bench通过覆盖多样恶劣环境、细粒度标注支持多步推理，并设计了专用评测框架。", "result": "评测显示当前MLLMs的准确率低于60%，凸显了基准的难度和提升需求。", "conclusion": "AD^2-Bench为自动驾驶中的MLLMs推理提供了标准化评测平台，推动了研究进展。"}}
{"id": "2506.09451", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09451", "abs": "https://arxiv.org/abs/2506.09451", "authors": ["Runxue Bao", "Quanchao Lu", "Yanfu Zhang"], "title": "Safe Screening Rules for Group SLOPE", "comment": "Accepted by ECML PKDD 2025", "summary": "Variable selection is a challenging problem in high-dimensional sparse\nlearning, especially when group structures exist. Group SLOPE performs well for\nthe adaptive selection of groups of predictors. However, the block\nnon-separable group effects in Group SLOPE make existing methods either invalid\nor inefficient. Consequently, Group SLOPE tends to incur significant\ncomputational costs and memory usage in practical high-dimensional scenarios.\nTo overcome this issue, we introduce a safe screening rule tailored for the\nGroup SLOPE model, which efficiently identifies inactive groups with zero\ncoefficients by addressing the block non-separable group effects. By excluding\nthese inactive groups during training, we achieve considerable gains in\ncomputational efficiency and memory usage. Importantly, the proposed screening\nrule can be seamlessly integrated into existing solvers for both batch and\nstochastic algorithms. Theoretically, we establish that our screening rule can\nbe safely employed with existing optimization algorithms, ensuring the same\nresults as the original approaches. Experimental results confirm that our\nmethod effectively detects inactive feature groups and significantly boosts\ncomputational efficiency without compromising accuracy.", "AI": {"tldr": "提出了一种针对Group SLOPE模型的安全筛选规则，用于高效识别无效特征组，显著提升计算效率和内存使用。", "motivation": "解决Group SLOPE中块不可分组效应导致的计算成本高和内存占用大的问题。", "method": "设计了一种安全筛选规则，识别并排除无效特征组，适用于批量和随机算法。", "result": "实验证明该方法能有效检测无效组，显著提升效率且不影响准确性。", "conclusion": "提出的筛选规则安全可靠，能显著优化Group SLOPE的计算性能。"}}
{"id": "2506.09565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09565", "abs": "https://arxiv.org/abs/2506.09565", "authors": ["Qijing Li", "Jingxiang Sun", "Liang An", "Zhaoqi Su", "Hongwen Zhang", "Yebin Liu"], "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields", "comment": null, "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.", "AI": {"tldr": "SemanticSplat提出了一种基于3D高斯和潜在语义属性的前馈式语义感知3D重建方法，解决了现有方法在几何重建和语义理解上的局限性。", "motivation": "现有方法在3D场景理解中无法同时处理几何、外观和语义信息，且依赖密集输入视图，限制了实用性。", "method": "SemanticSplat通过融合多样特征场（如LSeg、SAM）和成本体积表示，预测语义各向异性高斯，并采用两阶段蒸馏框架从稀疏视图图像重建多模态语义特征场。", "result": "实验表明，该方法在可提示和开放词汇分割等3D场景理解任务中表现优异。", "conclusion": "SemanticSplat为联合建模几何-外观-语义提供了一种高效且实用的解决方案。"}}
{"id": "2506.09452", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "pdf": "https://arxiv.org/pdf/2506.09452", "abs": "https://arxiv.org/abs/2506.09452", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "AI": {"tldr": "论文提出了一种名为Stained Glass Transform的方法，通过随机变换LLM的词嵌入来保护输入数据的隐私，同时保持模型性能。", "motivation": "由于AI计算基础设施的高成本和LLM服务的共享性，企业数据所有者对在共享环境中使用敏感数据存在顾虑。", "method": "提出Stained Glass Transform，一种基于高斯混合模型互信息的随机变换方法，保护输入隐私。", "result": "通过理论和实验验证，该方法在保护隐私的同时保持了LLM的性能。", "conclusion": "Stained Glass Transform为共享计算环境中的隐私保护提供了一种有效解决方案。"}}
{"id": "2506.09612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09612", "abs": "https://arxiv.org/abs/2506.09612", "authors": ["Mingxiao LI", "mang ning", "Marie-Francine Moens"], "title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "comment": "17 pages, 9. figures", "summary": "Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.", "AI": {"tldr": "提出了一种名为Zigzag Sampling with Asymmetric Prompts and Visual Sharing的训练无关采样策略，用于提升视觉故事生成中的主题一致性。", "motivation": "现有方法在生成多张图像时难以保持主题一致性，而现有解决方案要么资源密集，要么效果有限。", "method": "采用Zigzag采样机制，结合非对称提示和视觉共享模块，以增强主题一致性。", "result": "实验表明，该方法在生成连贯且一致的视觉故事方面显著优于先前方法。", "conclusion": "该方法为视觉故事生成提供了一种高效且无需训练的一致性增强方案。"}}
{"id": "2506.09454", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09454", "abs": "https://arxiv.org/abs/2506.09454", "authors": ["Yuanhao Pu", "Defu Lian", "Xiaolong Chen", "Xu Huang", "Jin Chen", "Enhong Chen"], "title": "NDCG-Consistent Softmax Approximation with Accelerated Convergence", "comment": "35 pages", "summary": "Ranking tasks constitute fundamental components of extreme similarity\nlearning frameworks, where extremely large corpora of objects are modeled\nthrough relative similarity relationships adhering to predefined ordinal\nstructures. Among various ranking surrogates, Softmax (SM) Loss has been widely\nadopted due to its natural capability to handle listwise ranking via global\nnegative comparisons, along with its flexibility across diverse application\nscenarios. However, despite its effectiveness, SM Loss often suffers from\nsignificant computational overhead and scalability limitations when applied to\nlarge-scale object spaces. To address this challenge, we propose novel loss\nformulations that align directly with ranking metrics: the\nRanking-Generalizable \\textbf{squared} (RG$^2$) Loss and the\nRanking-Generalizable interactive (RG$^\\times$) Loss, both derived through\nTaylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic\nmechanisms underlying weighted squared losses (WSL) in ranking methods and\nuncovers fundamental connections between sampling-based and non-sampling-based\nloss paradigms. Furthermore, we integrate the proposed RG losses with the\nhighly efficient Alternating Least Squares (ALS) optimization method, providing\nboth generalization guarantees and convergence rate analyses. Empirical\nevaluations on real-world datasets demonstrate that our approach achieves\ncomparable or superior ranking performance relative to SM Loss, while\nsignificantly accelerating convergence. This framework offers the similarity\nlearning community both theoretical insights and practically efficient tools,\nwith methodologies applicable to a broad range of tasks where balancing ranking\nquality and computational efficiency is essential.", "AI": {"tldr": "论文提出了两种新的损失函数（RG²和RG×），通过泰勒展开Softmax Loss解决其计算开销和扩展性问题，并结合ALS优化方法，在保持排名性能的同时显著加速收敛。", "motivation": "Softmax Loss在大规模对象空间中存在显著的计算开销和扩展性限制，需要更高效的替代方案。", "method": "通过泰勒展开Softmax Loss，提出RG²和RG×两种损失函数，并结合ALS优化方法。", "result": "实验证明，新方法在排名性能上与Softmax Loss相当或更优，同时显著加速收敛。", "conclusion": "该框架为相似性学习提供了理论见解和高效工具，适用于需要平衡排名质量和计算效率的任务。"}}
{"id": "2506.09626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09626", "abs": "https://arxiv.org/abs/2506.09626", "authors": ["Giacomo Rosin", "Muhammad Rameez Ur Rahman", "Sebastiano Vascon"], "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting", "comment": "IJCNN 2025", "summary": "Human trajectory forecasting is crucial in applications such as autonomous\ndriving, robotics and surveillance. Accurate forecasting requires models to\nconsider various factors, including social interactions, multi-modal\npredictions, pedestrian intention and environmental context. While existing\nmethods account for these factors, they often overlook the impact of the\nenvironment, which leads to collisions with obstacles. This paper introduces\nECAM (Environmental Collision Avoidance Module), a contrastive learning-based\nmodule to enhance collision avoidance ability with the environment. The\nproposed module can be integrated into existing trajectory forecasting models,\nimproving their ability to generate collision-free predictions. We evaluate our\nmethod on the ETH/UCY dataset and quantitatively and qualitatively demonstrate\nits collision avoidance capabilities. Our experiments show that\nstate-of-the-art methods significantly reduce (-40/50%) the collision rate when\nintegrated with the proposed module. The code is available at\nhttps://github.com/CVML-CFU/ECAM.", "AI": {"tldr": "论文提出ECAM模块，通过对比学习增强轨迹预测模型的环境避障能力，实验表明其显著降低碰撞率。", "motivation": "现有轨迹预测方法常忽略环境影响，导致碰撞问题，需改进环境避障能力。", "method": "提出ECAM（环境避障模块），基于对比学习，可集成到现有模型中。", "result": "在ETH/UCY数据集上验证，碰撞率降低40-50%。", "conclusion": "ECAM有效提升轨迹预测的环境避障能力，代码已开源。"}}
{"id": "2506.09477", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09477", "abs": "https://arxiv.org/abs/2506.09477", "authors": ["Yunhao Tang", "Rémi Munos"], "title": "On a few pitfalls in KL divergence gradient estimation for RL", "comment": null, "summary": "We point out a few pitfalls in implementing gradient estimation for KL\ndivergence in RL training for LLM, as seen in a number of open source projects\nand papers. The first major pitfall is to differentiate through the KL estimate\nas loss functions to minimize KL divergence. We show that such implementations\nare generally incorrect and do not produce the desired KL gradient. Secondly,\nwe show that some implementations do not account for the sequential nature of\nthe estimation problem and produce a partial gradient at best. We demonstrate\nthe impact of such issues with illustrative tabular and LLM experiments, and\nshow the correct way to implement the KL gradient.", "AI": {"tldr": "论文指出了在LLM强化学习中KL散度梯度估计的几个常见错误，并提供了正确实现方法。", "motivation": "发现开源项目和论文中KL散度梯度估计的实现存在误区，影响训练效果。", "method": "分析错误实现（如将KL估计作为损失函数微分），并通过实验展示问题。", "result": "错误实现无法生成正确的KL梯度，部分实现仅提供部分梯度。", "conclusion": "论文明确了KL梯度估计的正确实现方式，并通过实验验证其有效性。"}}
{"id": "2506.09634", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09634", "abs": "https://arxiv.org/abs/2506.09634", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "AI": {"tldr": "HSENet提出了一种用于3D医学图像和语言理解的混合空间编码网络，通过双3D视觉编码器和空间打包器提升诊断准确性。", "motivation": "现有多模态大语言模型主要针对2D医学图像，难以捕捉复杂3D解剖结构，导致误诊和诊断幻觉。", "method": "HSENet采用双3D视觉编码器感知全局和细节信息，并结合空间打包器压缩高分辨率3D区域为视觉标记。", "result": "在3D语言-视觉检索、医学报告生成和视觉问答任务中，HSENet均取得显著性能提升。", "conclusion": "HSENet通过高效视觉感知和投影，显著提升了3D医学图像与语言理解的准确性和鲁棒性。"}}
{"id": "2506.09496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09496", "abs": "https://arxiv.org/abs/2506.09496", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "comment": null, "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "AI": {"tldr": "EnerBridge-DPO是一种新型蛋白质逆折叠框架，通过结合Markov Bridges和Direct Preference Optimization（DPO），直接生成低能量、高稳定性的蛋白质序列。", "motivation": "当前深度学习方法主要关注序列恢复率，而忽略了生成序列的能量稳定性。本研究旨在克服这一限制，开发能够直接生成低能量稳定序列的模型。", "method": "提出EnerBridge-DPO框架，结合Markov Bridges和DPO，引入能量约束损失，从信息丰富的先验序列出发优化生成低能量序列。", "result": "EnerBridge-DPO能够设计出能量更低且序列恢复率与现有模型相当的蛋白质序列，并能准确预测序列间的ΔΔG值。", "conclusion": "EnerBridge-DPO在蛋白质逆折叠中实现了能量稳定性和序列恢复率的平衡，为蛋白质设计提供了新思路。"}}
{"id": "2506.09644", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09644", "abs": "https://arxiv.org/abs/2506.09644", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "comment": null, "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "AI": {"tldr": "DGAE通过扩散模型引导解码器，提升高压缩比下自编码器的性能，同时减少潜在空间维度，实现更高效的表示。", "motivation": "解决GAN导致的训练不稳定问题，并优化高压缩比下的性能退化，同时最小化潜在空间维度。", "method": "提出DGAE，利用扩散模型引导解码器恢复潜在表示中未完全解码的信息信号。", "result": "DGAE在高空间压缩率下有效缓解性能退化，潜在空间缩小2倍，与扩散模型结合在ImageNet-1K图像生成中表现优异。", "conclusion": "DGAE通过改进解码器表达力，实现了更高效的潜在表示，并加速扩散模型的收敛。"}}
{"id": "2506.09499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09499", "abs": "https://arxiv.org/abs/2506.09499", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "comment": "12 Pages", "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "AI": {"tldr": "论文提出了Option Kernel Bellman Equations (OKBEs)，用于无奖励马尔可夫决策过程，通过状态-时间选项核（STOK）直接优化预测映射，以实现目标完成概率最大化并避免约束违反。STOK具有模块化、可组合和可解释性，支持长期规划和内在动机。", "motivation": "传统基于奖励最大化的方法在组合性、模块化和可解释性方面存在冲突，OKBEs旨在解决这些问题，支持可验证的长期规划和动态高维世界模型。", "method": "通过构造和优化STOK，利用Chapman-Kolmogorov方程进行时空预测，分解高维状态转移模型为局部STOK和目标条件策略，实现高效规划和任务复用。", "result": "OKBEs支持灵活的策略合成、规划表示复用，并通过内在动机函数（如empowerment）验证目标合理性，适用于高维动态环境。", "conclusion": "OKBEs提供了一种替代奖励最大化的方法，强调组合性、模块化和可解释性，为长期规划和内在动机提供了可扩展的解决方案。"}}
{"id": "2506.09663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09663", "abs": "https://arxiv.org/abs/2506.09663", "authors": ["Haowen Wang", "Xiaoping Yuan", "Zhao Jin", "Zhen Zhao", "Zhengping Che", "Yousong Xue", "Jin Tian", "Yakun Huang", "Jian Tang"], "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation", "comment": null, "summary": "Articulated objects are ubiquitous in everyday life, and accurate 3D\nrepresentations of their geometry and motion are critical for numerous\napplications. However, in the absence of human annotation, existing approaches\nstill struggle to build a unified representation for objects that contain\nmultiple movable parts. We introduce DeGSS, a unified framework that encodes\narticulated objects as deformable 3D Gaussian fields, embedding geometry,\nappearance, and motion in one compact representation. Each interaction state is\nmodeled as a smooth deformation of a shared field, and the resulting\ndeformation trajectories guide a progressive coarse-to-fine part segmentation\nthat identifies distinct rigid components, all in an unsupervised manner. The\nrefined field provides a spatially continuous, fully decoupled description of\nevery part, supporting part-level reconstruction and precise modeling of their\nkinematic relationships. To evaluate generalization and realism, we enlarge the\nsynthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset\nthat pairs RGB captures with accurately reverse-engineered 3D models. Extensive\nexperiments demonstrate that our method outperforms existing methods in both\naccuracy and stability.", "AI": {"tldr": "DeGSS提出了一种基于可变形3D高斯场的统一框架，用于无监督建模多部件物体的几何、外观和运动。", "motivation": "现有方法在缺乏人工标注时难以统一表示多部件物体的几何和运动。", "method": "DeGSS将物体建模为可变形3D高斯场，通过平滑变形轨迹实现无监督部件分割。", "result": "方法在准确性和稳定性上优于现有方法，支持部件级重建和运动关系建模。", "conclusion": "DeGSS为多部件物体提供了一种紧凑且连续的表示，适用于实际应用。"}}
{"id": "2506.09526", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09526", "abs": "https://arxiv.org/abs/2506.09526", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "title": "Neural Functions for Learning Periodic Signal", "comment": null, "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "AI": {"tldr": "本文提出了一种新型神经网络架构，用于学习周期性信号，通过提取周期性模式提升泛化能力和外推性能。", "motivation": "现有的基于坐标的多层感知机（MLPs）在学习连续神经表示时存在过拟合和外推能力不足的问题，尤其是对周期性信号。", "method": "提出了一种新型网络架构，通过提取周期性模式并利用这些信息来表示信号。", "result": "实验表明，该方法在周期性微分方程解、时间序列插补和预测等任务中表现优异。", "conclusion": "该方法有效提升了周期性信号的泛化能力和外推性能，适用于多种实际任务。"}}
{"id": "2506.09668", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09668", "abs": "https://arxiv.org/abs/2506.09668", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "comment": "Work currently under revision for IEEE TMI", "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "AI": {"tldr": "CINeMA是一种新型框架，用于在低数据环境下创建高分辨率、时空多模态脑图谱，显著提高了效率和灵活性。", "motivation": "研究胎儿和新生儿大脑的快速神经发育需要高分辨率脑图谱，但传统方法依赖大数据，难以应对病理情况下的数据稀缺问题。", "method": "CINeMA在潜在空间中操作，避免了计算密集的图像配准，支持基于解剖特征的灵活条件设置。", "result": "CINeMA在准确性、效率和多功能性上超越现有方法，支持组织分割、年龄预测及合成数据生成。", "conclusion": "CINeMA为脑研究提供了强大工具，代码和图谱已开源。"}}
{"id": "2506.09532", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09532", "abs": "https://arxiv.org/abs/2506.09532", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "AI": {"tldr": "Athena-PRM是一种多模态过程奖励模型，用于评估复杂推理问题解决中每一步的奖励分数，通过弱与强完成者预测一致性生成高质量标签，显著提升性能。", "motivation": "传统自动标注方法（如蒙特卡洛估计）产生噪声标签且计算成本高，需高效生成高质量过程标注数据。", "method": "利用弱与强完成者预测一致性识别可靠过程标签，并采用ORM初始化和负数据上采样策略优化PRM性能。", "result": "Athena-PRM在多个场景和基准测试中表现优异，仅需5000样本，显著提升模型性能（如Qwen2.5-VL-7B在WeMath和MathVista上分别提升10.2和7.1分）。", "conclusion": "Athena-PRM在多任务中表现卓越，成为视觉过程评估的新标杆，并通过奖励排序微调进一步优化模型性能。"}}
{"id": "2506.09677", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09677", "abs": "https://arxiv.org/abs/2506.09677", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "comment": null, "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "AI": {"tldr": "论文评估了三种先进推理模型对误导性用户输入的鲁棒性，发现其在面对否定提示时准确率显著下降，并提出了新基准GaslightingBench-R进一步验证这一脆弱性。", "motivation": "探索推理模型在面对误导性用户输入时的鲁棒性，填补现有研究的空白。", "method": "系统评估了三种推理模型（o4-mini、Claude-3.7-Sonnet、Gemini-2.5-Flash）在三个多模态基准（MMMU、MathVista、CharXiv）上的表现，并设计了新基准GaslightingBench-R。", "result": "模型在否定提示下准确率平均下降25-29%，在新基准上下降超过53%。", "conclusion": "推理模型在信念坚持方面存在根本性局限，揭示了逐步推理与信念持久性之间的差距。"}}
{"id": "2506.09544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09544", "abs": "https://arxiv.org/abs/2506.09544", "authors": ["Yang Yang", "Du Yin", "Hao Xue", "Flora Salim"], "title": "STOAT: Spatial-Temporal Probabilistic Causal Inference Network", "comment": null, "summary": "Spatial-temporal causal time series (STC-TS) involve region-specific temporal\nobservations driven by causally relevant covariates and interconnected across\ngeographic or network-based spaces. Existing methods often model spatial and\ntemporal dynamics independently and overlook causality-driven probabilistic\nforecasting, limiting their predictive power. To address this, we propose STOAT\n(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework\nfor probabilistic forecasting in STC-TS. The proposed method extends a causal\ninference approach by incorporating a spatial relation matrix that encodes\ninterregional dependencies (e.g. proximity or connectivity), enabling spatially\ninformed causal effect estimation. The resulting latent series are processed by\ndeep probabilistic models to estimate the parameters of the distributions,\nenabling calibrated uncertainty modeling. We further explore multiple output\ndistributions (e.g., Gaussian, Student's-$t$, Laplace) to capture\nregion-specific variability. Experiments on COVID-19 data across six countries\ndemonstrate that STOAT outperforms state-of-the-art probabilistic forecasting\nmodels (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,\nparticularly in regions with strong spatial dependencies. By bridging causal\ninference and geospatial probabilistic forecasting, STOAT offers a\ngeneralizable framework for complex spatial-temporal tasks, such as epidemic\nmanagement.", "AI": {"tldr": "STOAT是一个新颖的空间-时间概率因果推理网络框架，用于空间-时间因果时间序列的概率预测，通过结合空间关系矩阵和深度概率模型，显著提升了预测性能。", "motivation": "现有方法通常独立建模空间和时间动态，并忽略因果驱动的概率预测，限制了预测能力。", "method": "STOAT扩展了因果推理方法，引入空间关系矩阵编码区域间依赖关系，并通过深度概率模型估计分布参数，支持多种输出分布。", "result": "在COVID-19数据上的实验表明，STOAT在关键指标上优于现有模型，尤其在空间依赖强的区域表现突出。", "conclusion": "STOAT通过结合因果推理和地理空间概率预测，为复杂空间-时间任务提供了通用框架。"}}
{"id": "2506.09691", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "提出了一种在推理时通过结构化图像和文本来提升视觉语言模型（VLM）组合性的方法，无需额外训练即可显著提升性能。", "motivation": "现有双编码器视觉语言模型（如CLIP）在组合性任务上表现不佳，推理时技术研究较少。", "method": "在推理时，将图像分割为小块，提取文本片段（对象、属性和关系），利用VLM匹配图像块与文本片段，并聚合相似度得分。", "result": "方法在多个数据集上显著提升了VLM的组合性性能，尤其在属性-对象绑定任务中表现突出。", "conclusion": "推理时结构化处理图像和文本是提升组合性的有效方法，未来可进一步优化推理时技术。"}}
{"id": "2506.09574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09574", "abs": "https://arxiv.org/abs/2506.09574", "authors": ["Gaurav Chaudhary", "Wassim Uddin Mondal", "Laxmidhar Behera"], "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning", "comment": null, "summary": "Sample efficiency and exploration remain critical challenges in Deep\nReinforcement Learning (DRL), particularly in complex domains. Offline RL,\nwhich enables agents to learn optimal policies from static, pre-collected\ndatasets, has emerged as a promising alternative. However, offline RL is\nconstrained by issues such as out-of-distribution (OOD) actions that limit\npolicy performance and generalization. To overcome these limitations, we\npropose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework\nthat unifies offline and online RL for efficient and scalable learning. While\nprevious hybrid methods rely on extensive design components and added\ncomputational complexity to utilize offline data effectively, MOORL introduces\na meta-policy that seamlessly adapts across offline and online trajectories.\nThis enables the agent to leverage offline data for robust initialization while\nutilizing online interactions to drive efficient exploration. Our theoretical\nanalysis demonstrates that the hybrid approach enhances exploration by\neffectively combining the complementary strengths of offline and online data.\nFurthermore, we demonstrate that MOORL learns a stable Q-function without added\ncomplexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL\nbenchmarks validate its effectiveness, showing consistent improvements over\nstate-of-the-art offline and hybrid RL baselines. With minimal computational\noverhead, MOORL achieves strong performance, underscoring its potential for\npractical applications in real-world scenarios.", "AI": {"tldr": "MOORL是一种结合离线与在线强化学习的混合框架，通过元策略实现高效学习，解决了样本效率和探索问题。", "motivation": "解决深度强化学习中样本效率和探索的挑战，特别是离线RL中的分布外动作问题。", "method": "提出MOORL框架，利用元策略无缝适应离线与在线轨迹，结合两者的优势。", "result": "在28个任务上验证了MOORL的有效性，性能优于现有离线与混合RL方法。", "conclusion": "MOORL通过简单设计实现了高效学习，具有实际应用潜力。"}}
{"id": "2506.09695", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09695", "abs": "https://arxiv.org/abs/2506.09695", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "comment": "11 pages, 5 figures", "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "AI": {"tldr": "论文提出FasterSNN，一种混合神经架构，用于高效、稳定地诊断阿尔茨海默病（AD），特别针对轻度认知障碍（MCI）阶段。", "motivation": "早期诊断AD的挑战在于主观评估和多模态成像的高成本，而现有深度学习方法能效低且计算需求高。SNNs因其稀疏、事件驱动的特性适合AD诊断，但存在表达力弱和训练不稳定的问题。", "method": "FasterSNN结合了生物启发的LIF神经元、区域自适应卷积和多尺度脉冲注意力，以高效处理3D MRI数据。", "result": "实验表明，FasterSNN在保持诊断准确性的同时显著提高了效率和稳定性。", "conclusion": "FasterSNN为AD筛查提供了一种实用且高效的方法，代码已开源。"}}
{"id": "2506.09593", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09593", "abs": "https://arxiv.org/abs/2506.09593", "authors": ["Achim Hekler", "Lukas Kuhn", "Florian Buettner"], "title": "Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks", "comment": null, "summary": "Reliable uncertainty calibration is essential for safely deploying deep\nneural networks in high-stakes applications. Deep neural networks are known to\nexhibit systematic overconfidence, especially under distribution shifts.\nAlthough foundation models such as ConvNeXt, EVA and BEiT have demonstrated\nsignificant improvements in predictive performance, their calibration\nproperties remain underexplored. This paper presents a comprehensive\ninvestigation into the calibration behavior of foundation models, revealing\ninsights that challenge established paradigms. Our empirical analysis shows\nthat these models tend to be underconfident in in-distribution predictions,\nresulting in higher calibration errors, while demonstrating improved\ncalibration under distribution shifts. Furthermore, we demonstrate that\nfoundation models are highly responsive to post-hoc calibration techniques in\nthe in-distribution setting, enabling practitioners to effectively mitigate\nunderconfidence bias. However, these methods become progressively less reliable\nunder severe distribution shifts and can occasionally produce counterproductive\nresults. Our findings highlight the complex, non-monotonic effects of\narchitectural and training innovations on calibration, challenging established\nnarratives of continuous improvement.", "AI": {"tldr": "本文研究了基础模型（如ConvNeXt、EVA和BEiT）的校准行为，发现其在分布内预测中倾向于不自信，而在分布偏移下校准表现更好。后处理校准技术在分布内有效，但在严重偏移下可能失效。", "motivation": "深度神经网络在高风险应用中需要可靠的校准，但基础模型的校准特性尚未充分研究。", "method": "通过实证分析基础模型的校准行为，并测试后处理校准技术的效果。", "result": "基础模型在分布内预测中不自信，校准误差较高；在分布偏移下校准表现更好。后处理技术在分布内有效，但在严重偏移下可能失效。", "conclusion": "基础模型的校准行为复杂，挑战了持续改进的传统观点，需进一步研究。"}}
{"id": "2506.09699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09699", "abs": "https://arxiv.org/abs/2506.09699", "authors": ["Mattia Nardon", "Mikel Mujika Agirre", "Ander González Tomé", "Daniel Sedano Algarabel", "Josep Rueda Collell", "Ana Paola Caro", "Andrea Caraffa", "Fabio Poiesi", "Paul Ian Chippendale", "Davide Boscaini"], "title": "CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings", "comment": "Technical report", "summary": "Accurate 6D pose estimation of complex objects in 3D environments is\nessential for effective robotic manipulation. Yet, existing benchmarks fall\nshort in evaluating 6D pose estimation methods under realistic industrial\nconditions, as most datasets focus on household objects in domestic settings,\nwhile the few available industrial datasets are limited to artificial setups\nwith objects placed on tables. To bridge this gap, we introduce CHIP, the first\ndataset designed for 6D pose estimation of chairs manipulated by a robotic arm\nin a real-world industrial environment. CHIP includes seven distinct chairs\ncaptured using three different RGBD sensing technologies and presents unique\nchallenges, such as distractor objects with fine-grained differences and severe\nocclusions caused by the robotic arm and human operators. CHIP comprises 77,811\nRGBD images annotated with ground-truth 6D poses automatically derived from the\nrobot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP\nusing three zero-shot 6D pose estimation methods, assessing performance across\ndifferent sensor types, localization priors, and occlusion levels. Results show\nsubstantial room for improvement, highlighting the unique challenges posed by\nthe dataset. CHIP will be publicly released.", "AI": {"tldr": "CHIP是一个针对工业环境中椅子6D姿态估计的首个数据集，填补了现有数据集的不足，包含77,811张RGBD图像和自动标注的真实姿态。", "motivation": "现有6D姿态估计数据集多关注家庭环境，缺乏工业场景的真实性，CHIP旨在解决这一问题。", "method": "使用三种RGBD传感技术捕捉七种椅子，结合机器人运动学自动标注真实姿态，并引入干扰物和遮挡挑战。", "result": "通过三种零样本6D姿态估计方法评估，结果显示数据集具有显著挑战性，性能提升空间大。", "conclusion": "CHIP填补了工业场景6D姿态估计数据集的空白，为未来研究提供了新基准。"}}
{"id": "2506.09594", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09594", "abs": "https://arxiv.org/abs/2506.09594", "authors": ["Wenjin Qin", "Hailin Wang", "Jingyao Hou", "Jianjun Wang"], "title": "Accelerating Large-Scale Regularized High-Order Tensor Recovery", "comment": null, "summary": "Currently, existing tensor recovery methods fail to recognize the impact of\ntensor scale variations on their structural characteristics. Furthermore,\nexisting studies face prohibitive computational costs when dealing with\nlarge-scale high-order tensor data. To alleviate these issue, assisted by the\nKrylov subspace iteration, block Lanczos bidiagonalization process, and random\nprojection strategies, this article first devises two fast and accurate\nrandomized algorithms for low-rank tensor approximation (LRTA) problem.\nTheoretical bounds on the accuracy of the approximation error estimate are\nestablished. Next, we develop a novel generalized nonconvex modeling framework\ntailored to large-scale tensor recovery, in which a new regularization paradigm\nis exploited to achieve insightful prior representation for large-scale\ntensors. On the basis of the above, we further investigate new unified\nnonconvex models and efficient optimization algorithms, respectively, for\nseveral typical high-order tensor recovery tasks in unquantized and quantized\nsituations. To render the proposed algorithms practical and efficient for\nlarge-scale tensor data, the proposed randomized LRTA schemes are integrated\ninto their central and time-intensive computations. Finally, we conduct\nextensive experiments on various large-scale tensors, whose results demonstrate\nthe practicability, effectiveness and superiority of the proposed method in\ncomparison with some state-of-the-art approaches.", "AI": {"tldr": "论文提出了一种快速随机化算法和广义非凸建模框架，用于解决大规模高阶张量恢复问题，显著降低了计算成本并提高了准确性。", "motivation": "现有张量恢复方法未考虑张量尺度变化对结构特性的影响，且计算成本高昂，难以处理大规模高阶张量数据。", "method": "结合Krylov子空间迭代、块Lanczos双对角化过程和随机投影策略，设计了两种快速随机化算法；开发了广义非凸建模框架，并提出了新的正则化范式。", "result": "理论证明了近似误差估计的准确性，实验结果表明所提方法在实用性和效率上优于现有方法。", "conclusion": "所提方法为大规模张量恢复任务提供了高效且准确的解决方案，具有显著优势。"}}
{"id": "2506.09718", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09718", "abs": "https://arxiv.org/abs/2506.09718", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "comment": null, "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "AI": {"tldr": "论文提出LADH数据集，结合RGB和红外视频提升远程光电容积描记术（rPPG）在长期健康监测中的准确性和鲁棒性，解决了高海拔环境下的光照变化和遮挡问题。", "motivation": "解决rPPG在长期个人护理场景（如高海拔环境）中因光照变化、手部遮挡和动态面部姿势带来的挑战。", "method": "提出LADH数据集，包含240个同步RGB和红外面部视频，结合多任务学习提升生理信号监测性能。", "result": "结合RGB和红外视频输入，心率估计的平均绝对误差（MAE）为4.99 BPM。", "conclusion": "LADH数据集和多任务学习显著提升了rPPG在复杂环境下的性能，为长期健康监测提供了实用解决方案。"}}
{"id": "2506.09613", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09613", "abs": "https://arxiv.org/abs/2506.09613", "authors": ["Kaiwen Tuo", "Huan Wang"], "title": "SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot", "comment": null, "summary": "State-space language models such as Mamba match Transformer quality while\npermitting linear complexity inference, yet still comprise billions of\nparameters that hinder deployment. Existing one-shot pruning methods are\ntailored to attention blocks and fail to account for the time-shared and\ndiscretized state-transition matrix at the heart of the selective state-space\nmodule (SSM). In this paper, we introduce SparseSSM, the first training-free\npruning framework that extends the classic optimal brain surgeon (OBS)\nframework to state space architectures. Our layer-wise algorithm (i) derives an\napproximate second-order saliency score that aggregates Hessian-trace\ninformation across time steps, (ii) incorporates a component sensitivity\nanalysis to guide feed-forward network (FFN) pruning, which also sheds light on\nwhere redundancy resides in mamba architecture, (iii) can be easily extended to\nsemi-structured and structured sparsity. Empirically, we prune 50% of SSM\nweights without fine-tuning and observe no zero-shot accuracy loss, achieving\nthe current state-of-the-art pruning algorithm for Mamba-based LLMs.", "AI": {"tldr": "SparseSSM是一种无需训练的剪枝框架，针对状态空间模型（如Mamba）设计，首次将经典OBS框架扩展到此类架构，实现50%权重剪枝且零精度损失。", "motivation": "现有剪枝方法针对注意力块设计，无法处理状态空间模块的时间共享和离散化状态转移矩阵，限制了Mamba等模型的部署效率。", "method": "提出层间算法：(i) 基于Hessian迹的时间步聚合近似二阶显著性评分，(ii) 结合FFN剪枝的组件敏感性分析，(iii) 支持半结构化和结构化稀疏性。", "result": "在无需微调的情况下剪枝50% SSM权重，零精度损失，成为Mamba类LLM的当前最优剪枝算法。", "conclusion": "SparseSSM为状态空间模型提供了高效剪枝方案，揭示了Mamba架构中的冗余分布，并支持多种稀疏性扩展。"}}
{"id": "2506.09724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09724", "abs": "https://arxiv.org/abs/2506.09724", "authors": ["Ye Zhang", "Yu Zhou", "Yifeng Wang", "Jun Xiao", "Ziyue Wang", "Yongbing Zhang", "Jianxu Chen"], "title": "The Four Color Theorem for Cell Instance Segmentation", "comment": "Accepted at ICML 2025", "summary": "Cell instance segmentation is critical to analyzing biomedical images, yet\naccurately distinguishing tightly touching cells remains a persistent\nchallenge. Existing instance segmentation frameworks, including\ndetection-based, contour-based, and distance mapping-based approaches, have\nmade significant progress, but balancing model performance with computational\nefficiency remains an open problem. In this paper, we propose a novel cell\ninstance segmentation method inspired by the four-color theorem. By\nconceptualizing cells as countries and tissues as oceans, we introduce a\nfour-color encoding scheme that ensures adjacent instances receive distinct\nlabels. This reformulation transforms instance segmentation into a constrained\nsemantic segmentation problem with only four predicted classes, substantially\nsimplifying the instance differentiation process. To solve the training\ninstability caused by the non-uniqueness of four-color encoding, we design an\nasymptotic training strategy and encoding transformation method. Extensive\nexperiments on various modes demonstrate our approach achieves state-of-the-art\nperformance. The code is available at https://github.com/zhangye-zoe/FCIS.", "AI": {"tldr": "提出了一种基于四色定理的细胞实例分割方法，通过四色编码简化实例区分，结合渐进训练策略，实现了高效且高性能的分割。", "motivation": "解决生物医学图像中紧密接触细胞的准确分割问题，同时平衡模型性能与计算效率。", "method": "将细胞类比为国家，组织类比为海洋，引入四色编码方案，将实例分割转化为约束语义分割问题，并设计渐进训练策略解决编码不唯一性问题。", "result": "在多种模态上实现了最先进的性能。", "conclusion": "该方法通过四色编码和渐进训练策略，显著简化了实例分割问题，同时保持了高性能。"}}
{"id": "2506.09625", "categories": ["cs.LG", "68T07, 15A66"], "pdf": "https://arxiv.org/pdf/2506.09625", "abs": "https://arxiv.org/abs/2506.09625", "authors": ["Ekaterina Filimoshina", "Dmitry Shirokov"], "title": "GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras", "comment": "Accepted to ICML 2025", "summary": "We propose, implement, and compare with competitors a new architecture of\nequivariant neural networks based on geometric (Clifford) algebras: Generalized\nLipschitz Group Equivariant Neural Networks (GLGENN). These networks are\nequivariant to all pseudo-orthogonal transformations, including rotations and\nreflections, of a vector space with any non-degenerate or degenerate symmetric\nbilinear form. We propose a weight-sharing parametrization technique that takes\ninto account the fundamental structures and operations of geometric algebras.\nDue to this technique, GLGENN architecture is parameter-light and has less\ntendency to overfitting than baseline equivariant models. GLGENN outperforms or\nmatches competitors on several benchmarking equivariant tasks, including\nestimation of an equivariant function and a convex hull experiment, while using\nsignificantly fewer optimizable parameters.", "AI": {"tldr": "提出了一种基于几何代数的等变神经网络架构GLGENN，具有轻量化和抗过拟合特性，在多项任务中表现优异。", "motivation": "解决传统等变模型参数过多和过拟合问题，同时实现对多种伪正交变换的等变性。", "method": "利用几何代数的结构和操作设计权重共享参数化技术，构建GLGENN架构。", "result": "GLGENN在多个基准任务中优于或匹配竞争对手，且参数更少。", "conclusion": "GLGENN是一种高效且通用的等变神经网络架构，适用于多种几何变换任务。"}}
{"id": "2506.09735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09735", "abs": "https://arxiv.org/abs/2506.09735", "authors": ["Chuang Ma", "Shaokai Zhao", "Dongdong Zhou", "Yu Pei", "Zhiguo Luo", "Liang Xie", "Ye Yan", "Erwei Yin"], "title": "MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition", "comment": null, "summary": "Micro-expression recognition (MER), a critical subfield of affective\ncomputing, presents greater challenges than macro-expression recognition due to\nits brief duration and low intensity. While incorporating prior knowledge has\nbeen shown to enhance MER performance, existing methods predominantly rely on\nsimplistic, singular sources of prior knowledge, failing to fully exploit\nmulti-source information. This paper introduces the Multi-Prior Fusion Network\n(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We\npropose two complementary encoders: the Generic Feature Encoder (GFE) and the\nAdvanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with\nCoordinate Attention (CA) mechanisms, to improve the model's ability to capture\nspatiotemporal and channel-specific features. Inspired by developmental\npsychology, we present two variants of MPFNet--MPFNet-P and\nMPFNet-C--corresponding to two fundamental modes of infant cognitive\ndevelopment: parallel and hierarchical processing. These variants enable the\nevaluation of different strategies for integrating prior knowledge. Extensive\nexperiments demonstrate that MPFNet significantly improves MER accuracy while\nmaintaining balanced performance across categories, achieving accuracies of\n0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.\nTo the best of our knowledge, our approach achieves state-of-the-art\nperformance on the SMIC and SAMM datasets.", "AI": {"tldr": "论文提出了一种多先验融合网络（MPFNet），通过渐进式训练策略优化微表情识别任务，结合通用和高级特征编码器，显著提升了识别准确率。", "motivation": "微表情识别因持续时间短和强度低而更具挑战性，现有方法未能充分利用多源先验知识。", "method": "提出MPFNet，包含通用特征编码器（GFE）和高级特征编码器（AFE），基于I3D和坐标注意力机制，并设计了MPFNet-P和MPFNet-C两种变体。", "result": "在SMIC、CASME II和SAMM数据集上分别达到0.811、0.924和0.857的准确率，部分数据集达到最优性能。", "conclusion": "MPFNet通过多先验融合和渐进训练，显著提升了微表情识别性能。"}}
{"id": "2506.09630", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09630", "abs": "https://arxiv.org/abs/2506.09630", "authors": ["Pol G. Recasens", "Alberto Gutierrez", "Jordi Torres", "Josep. Ll Berral", "Anisa Halimi", "Kieran Fraser"], "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation", "comment": "Paper accepted at ICML 2025 workshop DIG-BUG", "summary": "Large Language Models (LLMs) are increasingly used for synthetic tabular data\ngeneration through in-context learning (ICL), offering a practical solution for\ndata augmentation in data scarce scenarios. While prior work has shown the\npotential of LLMs to improve downstream task performance through augmenting\nunderrepresented groups, these benefits often assume access to a subset of\nunbiased in-context examples, representative of the real dataset. In real-world\nsettings, however, data is frequently noisy and demographically skewed. In this\npaper, we systematically study how statistical biases within in-context\nexamples propagate to the distribution of synthetic tabular data, showing that\neven mild in-context biases lead to global statistical distortions. We further\nintroduce an adversarial scenario where a malicious contributor can inject bias\ninto the synthetic dataset via a subset of in-context examples, ultimately\ncompromising the fairness of downstream classifiers for a targeted and\nprotected subgroup. Our findings demonstrate a new vulnerability associated\nwith LLM-based data generation pipelines that rely on in-context prompts with\nin sensitive domains.", "AI": {"tldr": "LLMs用于合成表格数据时，上下文示例中的统计偏差会传播到合成数据中，导致全局统计失真，甚至可能被恶意利用，影响下游分类器的公平性。", "motivation": "研究LLMs在合成表格数据时，上下文示例中的统计偏差如何影响合成数据的分布，并揭示潜在的安全风险。", "method": "系统研究上下文示例中的统计偏差对合成数据分布的影响，并模拟恶意注入偏差的场景。", "result": "即使轻微的上下文偏差也会导致全局统计失真，恶意注入偏差会损害下游分类器的公平性。", "conclusion": "LLM基于上下文提示的数据生成管道在敏感领域存在新的脆弱性，需谨慎使用。"}}
{"id": "2506.09736", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09736", "abs": "https://arxiv.org/abs/2506.09736", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "comment": "Technical Report", "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "AI": {"tldr": "研究发现，仅语言模型在提供图像描述时表现优于多模态大语言模型（MLLMs），表明当前MLLMs在视觉整合方面存在不足。为此，作者提出了一种简单的视觉扰动框架，无需算法修改或额外数据即可提升感知鲁棒性。", "motivation": "当前多模态大语言模型（MLLMs）在视觉处理方面表现不佳，仅语言模型通过图像描述即可达到更好效果，表明MLLMs在视觉整合上存在问题。", "method": "提出视觉扰动框架，包括三种扰动策略：干扰拼接、保持主导性的混合和随机旋转，可轻松集成到现有训练流程中。", "result": "实验表明，该方法在数学推理任务中表现一致提升，效果与算法修改相当，并在开源7B RL调优模型中取得竞争力。", "conclusion": "视觉扰动在多模态数学推理中起关键作用，提升视觉处理能力可显著改善推理效果。"}}
{"id": "2506.09638", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09638", "abs": "https://arxiv.org/abs/2506.09638", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "AI": {"tldr": "本文提出了FedVLMBench，首个系统化的联邦学习VLM微调基准，涵盖多种架构、策略和任务，揭示了数据异质性对任务的影响，并提供了优化配置。", "motivation": "现有VLM微调方法依赖集中训练，难以满足隐私敏感领域需求，且缺乏联邦学习下的全面评估基准。", "method": "整合两种主流VLM架构、四种微调策略、五种FL算法、六个多模态数据集，覆盖单任务和多任务场景。", "result": "发现2层MLP连接器与并发调优是最优配置，且FL方法对视觉任务的数据异质性更敏感。", "conclusion": "FedVLMBench为隐私保护的多模态模型联邦训练提供了标准化平台和实用指导。"}}
{"id": "2506.09740", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09740", "abs": "https://arxiv.org/abs/2506.09740", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "comment": null, "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "AI": {"tldr": "论文提出了一种评估扩散模型中像素级图像与文本对齐的方法，并通过ELBO-T2IAlign校准对齐问题。", "motivation": "当前方法假设扩散模型中文本与图像完美对齐，但实际并非如此，因此需要评估和校准对齐问题。", "method": "使用零样本参考图像分割作为代理任务，分析像素-文本不对齐问题，并提出基于ELBO的校准方法ELBO-T2IAlign。", "result": "实验验证了ELBO-T2IAlign在图像分割和生成任务中的有效性。", "conclusion": "ELBO-T2IAlign是一种无需训练且通用的方法，能有效校准扩散模型中的像素-文本对齐问题。"}}
{"id": "2506.09660", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.09660", "abs": "https://arxiv.org/abs/2506.09660", "authors": ["Baran Can Gül", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "title": "SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization", "comment": "Preprint version. Accepted for publication at IEEE ETFA 2025", "summary": "As Federated Learning (FL) expands to larger and more distributed\nenvironments, consistency in training is challenged by network-induced delays,\nclock unsynchronicity, and variability in client updates. This combination of\nfactors may contribute to misaligned contributions that undermine model\nreliability and convergence. Existing methods like staleness-aware aggregation\nand model versioning address lagging updates heuristically, yet lack mechanisms\nto quantify staleness, especially in latency-sensitive and cross-regional\ndeployments. In light of these considerations, we introduce \\emph{SyncFed}, a\ntime-aware FL framework that employs explicit synchronization and timestamping\nto establish a common temporal reference across the system. Staleness is\nquantified numerically based on exchanged timestamps under the Network Time\nProtocol (NTP), enabling the server to reason about the relative freshness of\nclient updates and apply temporally informed weighting during aggregation. Our\nempirical evaluation on a geographically distributed testbed shows that, under\n\\emph{SyncFed}, the global model evolves within a stable temporal context,\nresulting in improved accuracy and information freshness compared to\nround-based baselines devoid of temporal semantics.", "AI": {"tldr": "SyncFed是一个时间感知的联邦学习框架，通过显式同步和时间戳量化延迟，提升模型准确性和信息新鲜度。", "motivation": "联邦学习在大规模分布式环境中面临网络延迟、时钟不同步和客户端更新不一致等问题，影响模型可靠性和收敛性。", "method": "SyncFed采用显式同步和时间戳（基于NTP协议）量化延迟，服务器根据时间戳加权聚合客户端更新。", "result": "实验表明，SyncFed在分布式测试环境中显著提升模型准确性和信息新鲜度。", "conclusion": "SyncFed通过时间感知机制解决了联邦学习中的延迟问题，优于传统轮次基线方法。"}}
{"id": "2506.09745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09745", "abs": "https://arxiv.org/abs/2506.09745", "authors": ["Yangrui Zhu", "Junhua Bao", "Yipan Wei", "Yapeng Li", "Bo Du"], "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets", "comment": null, "summary": "Existing multimodal methods typically assume that different modalities share\nthe same category set. However, in real-world applications, the category\ndistributions in multimodal data exhibit inconsistencies, which can hinder the\nmodel's ability to effectively utilize cross-modal information for recognizing\nall categories. In this work, we propose the practical setting termed\nMulti-Modal Heterogeneous Category-set Learning (MMHCL), where models are\ntrained in heterogeneous category sets of multi-modal data and aim to recognize\ncomplete classes set of all modalities during test. To effectively address this\ntask, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).\nSpecifically, CSCF aligns modality-specific features to a shared semantic space\nto enable knowledge transfer between seen and unseen classes. It then selects\nthe most discriminative modality for decision fusion through uncertainty\nestimation. Finally, it integrates cross-modal information based on class\nsimilarity, where the auxiliary modality refines the prediction of the dominant\none. Experimental results show that our method significantly outperforms\nexisting state-of-the-art (SOTA) approaches on multiple benchmark datasets,\neffectively addressing the MMHCL task.", "AI": {"tldr": "论文提出了一种多模态异构类别集学习（MMHCL）任务，并提出了基于类别相似性的跨模态融合模型（CSCF），以解决多模态数据中类别分布不一致的问题。", "motivation": "现实应用中，多模态数据的类别分布不一致，现有方法假设模态共享相同类别集，导致模型无法有效利用跨模态信息识别所有类别。", "method": "CSCF将模态特征对齐到共享语义空间，通过不确定性估计选择最具判别性的模态进行决策融合，并基于类别相似性整合跨模态信息。", "result": "实验表明，CSCF在多个基准数据集上显著优于现有方法，有效解决了MMHCL任务。", "conclusion": "CSCF通过跨模态知识迁移和类别相似性融合，成功解决了多模态异构类别集学习问题。"}}
{"id": "2506.09674", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09674", "abs": "https://arxiv.org/abs/2506.09674", "authors": ["Alessandro Licciardi", "Davide Leo", "Davide Carbone"], "title": "Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables the training of machine learning models\nacross decentralized clients while preserving data privacy. However, the\npresence of anomalous or corrupted clients - such as those with faulty sensors\nor non representative data distributions - can significantly degrade model\nperformance. Detecting such clients without accessing raw data remains a key\nchallenge. We propose WAFFLE (Wavelet and Fourier representations for Federated\nLearning) a detection algorithm that labels malicious clients {\\it before\ntraining}, using locally computed compressed representations derived from\neither the Wavelet Scattering Transform (WST) or the Fourier Transform. Both\napproaches provide low-dimensional, task-agnostic embeddings suitable for\nunsupervised client separation. A lightweight detector, trained on a\ndistillated public dataset, performs the labeling with minimal communication\nand computational overhead. While both transforms enable effective detection,\nWST offers theoretical advantages, such as non-invertibility and stability to\nlocal deformations, that make it particularly well-suited to federated\nscenarios. Experiments on benchmark datasets show that our method improves\ndetection accuracy and downstream classification performance compared to\nexisting FL anomaly detection algorithms, validating its effectiveness as a\npre-training alternative to online detection strategies.", "AI": {"tldr": "WAFFLE算法通过小波散射变换或傅里叶变换的压缩表示，在联邦学习训练前检测异常客户端，提升模型性能和隐私保护。", "motivation": "联邦学习中异常客户端（如数据分布不具代表性或传感器故障）会显著降低模型性能，且在不访问原始数据的情况下检测这些客户端是一大挑战。", "method": "提出WAFFLE算法，利用小波散射变换或傅里叶变换生成低维任务无关嵌入，通过轻量级检测器在公共数据集上训练并标记异常客户端。", "result": "实验表明，WAFFLE在检测准确性和下游分类性能上优于现有联邦学习异常检测算法。", "conclusion": "WAFFLE作为预训练替代方案，有效解决了联邦学习中的异常客户端检测问题，尤其小波散射变换因其理论优势更适用于联邦场景。"}}
{"id": "2506.09682", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09682", "abs": "https://arxiv.org/abs/2506.09682", "authors": ["Iulia Duta", "Pietro Liò"], "title": "Wasserstein Hypergraph Neural Network", "comment": null, "summary": "The ability to model relational information using machine learning has driven\nadvancements across various domains, from medicine to social science. While\ngraph representation learning has become mainstream over the past decade,\nrepresenting higher-order relationships through hypergraphs is rapidly gaining\nmomentum. In the last few years, numerous hypergraph neural networks have\nemerged, most of them falling under a two-stage, set-based framework. The\nmessages are sent from nodes to edges and then from edges to nodes. However,\nmost of the advancement still takes inspiration from the graph counterpart,\noften simplifying the aggregations to basic pooling operations. In this paper\nwe are introducing Wasserstein Hypergraph Neural Network, a model that treats\nthe nodes and hyperedge neighbourhood as distributions and aggregate the\ninformation using Sliced Wasserstein Pooling. Unlike conventional aggregators\nsuch as mean or sum, which only capture first-order statistics, our approach\nhas the ability to preserve geometric properties like the shape and spread of\ndistributions. This enables the learned embeddings to reflect how easily one\nhyperedge distribution can be transformed into another, following principles of\noptimal transport. Experimental results demonstrate that applying Wasserstein\npooling in a hypergraph setting significantly benefits node classification\ntasks, achieving top performance on several real-world datasets.", "AI": {"tldr": "论文提出了一种基于Wasserstein距离的超图神经网络，通过分布聚合信息，显著提升了节点分类任务的性能。", "motivation": "传统超图神经网络采用两阶段框架，信息聚合方式简单，仅能捕获一阶统计信息，无法保留分布的几何特性。", "method": "提出Wasserstein超图神经网络，将节点和超边邻域视为分布，使用Sliced Wasserstein Pooling进行信息聚合。", "result": "实验表明，该方法在多个真实数据集上显著提升了节点分类任务的性能。", "conclusion": "Wasserstein聚合方法能够保留分布的形状和扩散特性，优于传统聚合方式。"}}
{"id": "2506.09777", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09777", "abs": "https://arxiv.org/abs/2506.09777", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "comment": null, "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "AI": {"tldr": "DarkerBB是一种从黑盒识别模型中仅使用相似性分数重建彩色人脸图像的新方法，通过PCA特征空间零阶优化实现，在多个基准测试中表现优异。", "motivation": "解决仅通过相似性分数进行模型反演这一更具挑战性的隐私威胁问题。", "method": "在PCA特征空间中执行零阶优化，重建彩色人脸图像。", "result": "在LFW、AgeDB-30和CFP-FP基准测试中达到相似性仅设置下的最先进验证准确率，查询效率高。", "conclusion": "DarkerBB在有限信息下仍能高效重建人脸图像，验证了其方法的有效性。"}}
{"id": "2506.09701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09701", "abs": "https://arxiv.org/abs/2506.09701", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "comment": null, "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "AI": {"tldr": "TRIDENT是一种无需重新训练的通用推理算法，确保LLM输出满足线性时序逻辑约束。", "motivation": "LLM等模型在生成任务中表现优异，但无法保证输出满足时序约束。", "method": "将LTLf公式编译为DFA，指导约束束搜索，动态屏蔽违规路径并重新排序。", "result": "实验证明TRIDENT完美满足约束，同时提升输出质量和效率。", "conclusion": "TRIDENT是一种高效、通用的方法，适用于多种任务，确保约束满足且提升性能。"}}
{"id": "2506.09782", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09782", "abs": "https://arxiv.org/abs/2506.09782", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "comment": "20 pages", "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "AI": {"tldr": "Q-SAM2是一种针对SAM2的低比特量化方法，通过线性层校准和量化感知训练提升效率，同时保持高精度。", "motivation": "SAM2的计算和内存消耗高，限制了其在资源受限场景中的应用。", "method": "提出线性层校准方法和量化感知训练流程，优化权重分布并抑制异常值。", "result": "Q-SAM2在超低2比特量化下表现优异，比未校准模型提升66% mIoU。", "conclusion": "Q-SAM2显著提升了SAM2的效率，同时保持了高精度，适用于资源受限场景。"}}
{"id": "2506.09714", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09714", "abs": "https://arxiv.org/abs/2506.09714", "authors": ["Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "title": "Auto-Compressing Networks", "comment": null, "summary": "Deep neural networks with short residual connections have demonstrated\nremarkable success across domains, but increasing depth often introduces\ncomputational redundancy without corresponding improvements in representation\nquality. In this work, we introduce Auto-Compressing Networks (ACNs), an\narchitectural variant where additive long feedforward connections from each\nlayer to the output replace traditional short residual connections. ACNs\nshowcase a unique property we coin as \"auto-compression\", the ability of a\nnetwork to organically compress information during training with gradient\ndescent, through architectural design alone. Through auto-compression,\ninformation is dynamically \"pushed\" into early layers during training,\nenhancing their representational quality and revealing potential redundancy in\ndeeper ones. We theoretically show that this property emerges from layer-wise\ntraining patterns present in ACNs, where layers are dynamically utilized during\ntraining based on task requirements. We also find that ACNs exhibit enhanced\nnoise robustness compared to residual networks, superior performance in\nlow-data settings, improved transfer learning capabilities, and mitigate\ncatastrophic forgetting suggesting that they learn representations that\ngeneralize better despite using fewer parameters. Our results demonstrate up to\n18% reduction in catastrophic forgetting and 30-80% architectural compression\nwhile maintaining accuracy across vision transformers, MLP-mixers, and BERT\narchitectures. Furthermore, we demonstrate that coupling ACNs with traditional\npruning techniques, enables significantly better sparsity-performance\ntrade-offs compared to conventional architectures. These findings establish\nACNs as a practical approach to developing efficient neural architectures that\nautomatically adapt their computational footprint to task complexity, while\nlearning robust representations.", "AI": {"tldr": "Auto-Compressing Networks (ACNs) 通过长前馈连接替代短残差连接，实现自动压缩信息，提升表示质量，减少冗余，并在噪声鲁棒性、低数据性能和迁移学习方面表现优异。", "motivation": "传统深度神经网络增加深度时可能引入计算冗余，而表示质量未显著提升，因此需要一种能自动压缩信息并提升效率的架构。", "method": "提出 ACNs，用长前馈连接替代短残差连接，通过梯度下降动态压缩信息，优化层间训练模式。", "result": "ACNs 在减少灾难性遗忘、压缩架构、噪声鲁棒性和低数据性能方面表现优异，同时保持准确性。", "conclusion": "ACNs 是一种高效且自动适应任务复杂度的神经网络架构，能学习鲁棒表示并减少冗余。"}}
{"id": "2506.09784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09784", "abs": "https://arxiv.org/abs/2506.09784", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "comment": "Technical report", "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "AI": {"tldr": "FreeZeV2是一种无需训练的6D姿态估计方法，通过利用预训练的几何和视觉基础模型，实现了对未见物体的强泛化能力，并在速度和精度上均优于前代FreeZe。", "motivation": "解决现有方法需要大量任务特定训练数据的问题，探索是否可以通过预训练模型实现高效且准确的6D姿态估计。", "method": "提出稀疏特征提取策略、特征感知评分机制和模块化设计，支持实例分割模型集成。", "result": "在BOP Benchmark的七个核心数据集上达到新SOTA，速度提升8倍，精度提高5%。", "conclusion": "FreeZeV2证明了无需任务特定训练即可实现高效准确的6D姿态估计，并在BOP Challenge 2024中获最佳方法奖。"}}
{"id": "2506.09733", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2506.09733", "abs": "https://arxiv.org/abs/2506.09733", "authors": ["Minjong Cheon"], "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "comment": null, "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "AI": {"tldr": "AtmosMJ挑战了传统观点，证明标准经纬度网格也能实现长期稳定的天气预测，通过创新的GRF机制在500天内保持稳定，且训练成本低。", "motivation": "探讨是否能在标准经纬度网格上实现长期稳定的天气预测，而非依赖非标准空间域转换。", "method": "引入AtmosMJ，一种直接在ERA5数据上运行的深度卷积网络，采用GRF机制防止误差累积。", "result": "AtmosMJ在500天内保持稳定，10天预测精度与先进模型相当，训练成本仅5.7天。", "conclusion": "高效架构设计比非标准数据表示更能实现稳定且计算高效的长期天气预测。"}}
{"id": "2506.09814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09814", "abs": "https://arxiv.org/abs/2506.09814", "authors": ["Xiandong Zou", "Ruihao Xia", "Hongsong Wang", "Pan Zhou"], "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "comment": null, "summary": "While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.", "AI": {"tldr": "论文提出了3D-MeshPref数据集和RewardCS奖励模型，结合DreamCS框架，解决了现有文本到3D生成方法因2D偏好偏差导致的几何伪影问题。", "motivation": "现有文本到3D生成方法因依赖2D偏好数据而难以生成符合人类偏好的3D资产，存在几何伪影问题。", "method": "构建3D-MeshPref数据集，开发RewardCS奖励模型，提出DreamCS框架整合到文本到3D流程中。", "result": "实验表明DreamCS优于现有方法，生成的3D资产几何更准确且更符合人类偏好。", "conclusion": "通过3D-MeshPref和RewardCS，DreamCS有效解决了2D偏好偏差问题，提升了3D生成质量。"}}
{"id": "2506.09738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09738", "abs": "https://arxiv.org/abs/2506.09738", "authors": ["Xin Wang", "Zeyang Zhang", "Linxin Xiao", "Haibo Chen", "Chendi Ge", "Wenwu Zhu"], "title": "Towards Multi-modal Graph Large Language Model", "comment": null, "summary": "Multi-modal graphs, which integrate diverse multi-modal features and\nrelations, are ubiquitous in real-world applications. However, existing\nmulti-modal graph learning methods are typically trained from scratch for\nspecific graph data and tasks, failing to generalize across various multi-modal\ngraph data and tasks. To bridge this gap, we explore the potential of\nMulti-modal Graph Large Language Models (MG-LLM) to unify and generalize across\ndiverse multi-modal graph data and tasks. We propose a unified framework of\nmulti-modal graph data, task, and model, discovering the inherent\nmulti-granularity and multi-scale characteristics in multi-modal graphs.\nSpecifically, we present five key desired characteristics for MG-LLM: 1)\nunified space for multi-modal structures and attributes, 2) capability of\nhandling diverse multi-modal graph tasks, 3) multi-modal graph in-context\nlearning, 4) multi-modal graph interaction with natural language, and 5)\nmulti-modal graph reasoning. We then elaborate on the key challenges, review\nrelated works, and highlight promising future research directions towards\nrealizing these ambitious characteristics. Finally, we summarize existing\nmulti-modal graph datasets pertinent for model training. We believe this paper\ncan contribute to the ongoing advancement of the research towards MG-LLM for\ngeneralization across multi-modal graph data and tasks.", "AI": {"tldr": "本文探讨了多模态图大语言模型（MG-LLM）的潜力，旨在统一和泛化多样化的多模态图数据与任务，提出了一个统一的框架，并总结了相关数据集和研究方向。", "motivation": "现有方法无法泛化到不同的多模态图数据与任务，因此需要探索MG-LLM的潜力以实现统一和泛化。", "method": "提出了一个统一的多模态图数据、任务和模型框架，并定义了MG-LLM的五个关键特性。", "result": "总结了相关数据集，并指出了实现MG-LLM的关键挑战和未来研究方向。", "conclusion": "本文为MG-LLM在多模态图数据与任务中的泛化研究提供了重要贡献。"}}
{"id": "2506.09834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09834", "abs": "https://arxiv.org/abs/2506.09834", "authors": ["Chuang Maa", "Yu Peia", "Jianhang Zhanga", "Shaokai Zhaoa", "Bowen Jib", "Liang Xiea", "Ye Yana", "Erwei Yin"], "title": "MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion", "comment": null, "summary": "Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an\nindividual's genuine emotional state. Their analysis has attracted considerable\ninterest due to its promising applications in fields such as healthcare,\ncriminal investigation, and human-computer interaction. However, existing ME\nresearch is limited to single visual modality, overlooking the rich emotional\ninformation conveyed by other physiological modalities, resulting in ME\nrecognition and spotting performance far below practical application needs.\nTherefore, exploring the cross-modal association mechanism between ME visual\nfeatures and physiological signals (PS), and developing a multimodal fusion\nframework, represents a pivotal step toward advancing ME analysis. This study\nintroduces a novel ME dataset, MMME, which, for the first time, enables\nsynchronized collection of facial action signals (MEs), central nervous system\nsignals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming\nthe constraints of existing ME corpora, MMME comprises 634 MEs, 2,841\nmacro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,\nestablishing a robust foundation for investigating ME neural mechanisms and\nconducting multimodal fusion-based analyses. Extensive experiments validate the\ndataset's reliability and provide benchmarks for ME analysis, demonstrating\nthat integrating MEs with PS significantly enhances recognition and spotting\nperformance. To the best of our knowledge, MMME is the most comprehensive ME\ndataset to date in terms of modality diversity. It provides critical data\nsupport for exploring the neural mechanisms of MEs and uncovering the\nvisual-physiological synergistic effects, driving a paradigm shift in ME\nresearch from single-modality visual analysis to multimodal fusion. The dataset\nwill be publicly available upon acceptance of this paper.", "AI": {"tldr": "论文提出了一种新的微表情数据集MMME，首次同步采集面部动作信号、中枢神经系统信号和外周生理信号，通过多模态融合显著提升了微表情识别和检测性能。", "motivation": "现有微表情研究局限于单一视觉模态，忽略了其他生理模态传递的丰富情感信息，导致性能不足。探索视觉特征与生理信号的跨模态关联机制是推动微表情分析的关键。", "method": "研究引入了MMME数据集，包含634个微表情、2,841个宏表情和2,890组同步多模态生理信号，并通过实验验证其可靠性和性能提升。", "result": "实验表明，结合生理信号显著提升了微表情的识别和检测性能，MMME是目前模态多样性最全面的微表情数据集。", "conclusion": "MMME为探索微表情神经机制和视觉-生理协同效应提供了关键数据支持，推动了微表情研究从单模态视觉分析向多模态融合的范式转变。"}}
{"id": "2506.09742", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09742", "abs": "https://arxiv.org/abs/2506.09742", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "comment": "Accepted at AAMAS 2025", "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "AI": {"tldr": "提出了一种基于特征工程的认知架构，用于提升机器学习模型监控输出的可解释性，通过重构、分解和编译三个步骤优化决策过程。", "motivation": "传统机器学习模型监控方法输出冗长且难以解释，影响决策效率。", "method": "采用特征工程原则，通过Refactor、Break Down和Compile三个步骤优化LLM代理的监控输出。", "result": "实验证明该方法显著提高了监控输出的准确性和可解释性，优于多种基线方法。", "conclusion": "结合特征工程和选择性LLM利用的架构，为机器学习监控提供了高效、可解释的决策支持。"}}
{"id": "2506.09836", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09836", "abs": "https://arxiv.org/abs/2506.09836", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "comment": null, "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "AI": {"tldr": "DynaSplat通过动态-静态分离和分层运动建模扩展高斯泼溅技术，提升动态场景重建的准确性和真实感。", "motivation": "现有方法难以处理真实世界动态场景的复杂性，DynaSplat旨在解决这一问题。", "method": "结合变形偏移统计和2D运动流一致性分类静态与动态元素，采用分层运动建模处理全局与局部运动，并基于物理的透明度估计提升视觉一致性。", "result": "在复杂数据集上表现优于现有方法，提供更准确、真实、紧凑且高效的动态场景重建。", "conclusion": "DynaSplat为动态场景重建提供了更优的解决方案。"}}
{"id": "2506.09769", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09769", "abs": "https://arxiv.org/abs/2506.09769", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "AI": {"tldr": "Load-aware Tram-FL通过引入训练调度机制，优化分散式联邦学习的计算和通信负载，显著减少训练时间并加速收敛。", "motivation": "解决分散式联邦学习中因计算和通信负载不均导致的总训练时间过长问题。", "method": "将调度问题分解为节点级子问题，引入方差约束以平衡非独立同分布数据利用率，并通过目标函数最小化训练延迟。", "result": "在MNIST和CIFAR-10上的仿真结果显示，Load-aware Tram-FL比基线方法显著减少训练时间并加速收敛。", "conclusion": "Load-aware Tram-FL通过优化调度机制，有效提升了分散式联邦学习的效率和性能。"}}
{"id": "2506.09781", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09781", "abs": "https://arxiv.org/abs/2506.09781", "authors": ["Chungpa Lee", "Sehee Lim", "Kibok Lee", "Jy-yong Sohn"], "title": "On the Similarities of Embeddings in Contrastive Learning", "comment": "contrastive learning, representation learning, embedding, similarity,\n  negative pair, positive pair", "summary": "Contrastive learning (CL) operates on a simple yet effective principle:\nembeddings of positive pairs are pulled together, while those of negative pairs\nare pushed apart. Although various forms of contrastive loss have been proposed\nand analyzed from different perspectives, prior works lack a comprehensive\nframework that systematically explains a broad class of these objectives. In\nthis paper, we present a unified framework for understanding CL, which is based\non analyzing the cosine similarity between embeddings of positive and negative\npairs. In full-batch settings, we show that perfect alignment of positive pairs\nis unattainable when similarities of negative pairs fall below a certain\nthreshold, and that this misalignment can be alleviated by incorporating\nwithin-view negative pairs. In mini-batch settings, we demonstrate that smaller\nbatch sizes incur stronger separation among negative pairs within batches,\nwhich leads to higher variance in similarities of negative pairs. To address\nthis limitation of mini-batch CL, we introduce an auxiliary loss term that\nreduces the variance of similarities of negative pairs in CL. Empirical results\ndemonstrate that incorporating the proposed loss consistently improves the\nperformance of CL methods in small-batch training.", "AI": {"tldr": "本文提出了一个统一的对比学习框架，通过分析正负对嵌入的余弦相似性，揭示了全批次和小批次训练中的局限性，并提出了一种减少负对相似性方差的辅助损失项。", "motivation": "现有对比学习目标缺乏系统性解释框架，本文旨在填补这一空白。", "method": "基于余弦相似性分析，提出全批次和小批次训练中的理论框架，并引入辅助损失项以减少负对相似性方差。", "result": "实验证明，辅助损失项能显著提升小批次训练中对比学习的性能。", "conclusion": "本文框架为对比学习提供了统一视角，并通过辅助损失项解决了小批次训练的局限性。"}}
{"id": "2506.09846", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09846", "abs": "https://arxiv.org/abs/2506.09846", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "comment": "17 pages, 10 figures, Under Review", "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "AI": {"tldr": "提出了一种新的损失函数，利用Wasserstein距离优化手写文本识别模型，通过字符频率分布对齐提升准确性和鲁棒性。", "motivation": "手写文本识别因字符集的演化和上下文依赖性而具有挑战性，传统模型在特定子集上表现不佳。", "method": "提出了一种结合Wasserstein距离的损失函数，惩罚预测文本与目标字符频率分布的差异。", "result": "实验证明该方法在多个数据集和架构上提升了模型的泛化能力和性能。", "conclusion": "通过字符分布对齐，无需重新训练即可提升现有模型，代码已开源。"}}
{"id": "2506.09785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09785", "abs": "https://arxiv.org/abs/2506.09785", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "AI": {"tldr": "论文提出了一种针对连续依赖数据的对比自监督学习框架，通过硬和软相似性度量改进传统方法，并在时空数据任务中表现优异。", "motivation": "传统对比自监督学习方法假设样本间语义独立，不适用于依赖数据（如时空数据），因此需要一种新框架来捕捉复杂相关性。", "method": "提出硬和软相似性度量，推导出适应依赖数据的相似性矩阵，并设计依赖感知的损失函数。", "result": "在UEA和UCR基准测试中分别提升4.17%和2.08%的准确率，干旱分类任务中ROC-AUC提高7%。", "conclusion": "依赖感知的损失函数能有效捕捉时空依赖关系，显著提升性能。"}}
{"id": "2506.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09849", "abs": "https://arxiv.org/abs/2506.09849", "authors": ["Florian Bordes", "Quentin Garrido", "Justine T Kao", "Adina Williams", "Michael Rabbat", "Emmanuel Dupoux"], "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments", "comment": null, "summary": "We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.", "AI": {"tldr": "IntPhys 2是一个视频基准测试，用于评估深度学习模型对直观物理的理解能力，基于四个核心原则，结果显示当前模型与人类表现存在显著差距。", "motivation": "研究动机是评估深度学习模型是否具备类似人类早期童年发展的直观物理理解能力。", "method": "方法是通过违反期望框架设计测试，挑战模型在虚拟环境中区分可能和不可能事件的能力。", "result": "结果表明，当前模型在复杂场景中对直观物理的理解表现接近随机水平（50%），远低于人类的近乎完美表现。", "conclusion": "结论指出当前模型与人类直观物理理解存在差距，需改进模型架构和训练方法。"}}
{"id": "2506.09803", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09803", "abs": "https://arxiv.org/abs/2506.09803", "authors": ["Longzhu He", "Chaozhuo Li", "Peng Tang", "Litian Zhang", "Sen Su"], "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols", "comment": null, "summary": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses.", "AI": {"tldr": "论文提出了一种针对本地隐私图学习协议的数据投毒攻击，并探讨了防御策略。", "motivation": "现实中的图数据常包含敏感信息，本地隐私图学习协议虽能保护隐私，但可能面临数据投毒攻击的威胁。", "method": "攻击者通过注入虚假用户并操纵其与真实用户的链接，发送精心设计的数据以破坏学习效果。", "result": "攻击在理论和实验上均被证明有效，现有防御策略效果有限。", "conclusion": "需开发更鲁棒的防御机制以确保隐私保护图学习的安全性。"}}
{"id": "2506.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09881", "abs": "https://arxiv.org/abs/2506.09881", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "comment": null, "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "AI": {"tldr": "Vireo是一个单阶段框架，首次将开放词汇语义分割（OVSS）和领域泛化语义分割（DGSS）结合，提出开放词汇领域泛化语义分割（OV-DGSS），通过几何和文本模态的融合实现鲁棒预测。", "motivation": "解决开放词汇和领域泛化在语义分割中的互补性问题，适用于自动驾驶等动态环境。", "method": "基于冻结视觉基础模型（VFMs），引入深度VFMs提取领域不变特征，提出GeoText Prompts、CMPE和DOV-VEH三个关键组件。", "result": "Vireo在领域泛化和开放词汇识别上大幅超越现有方法，达到最先进性能。", "conclusion": "Vireo为动态环境中的鲁棒视觉理解提供了统一且可扩展的解决方案。"}}
{"id": "2506.09810", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.09810", "abs": "https://arxiv.org/abs/2506.09810", "authors": ["Minoh Jeong", "Alfred Hero"], "title": "Generalizing Supervised Contrastive learning: A Projection Perspective", "comment": null, "summary": "Self-supervised contrastive learning (SSCL) has emerged as a powerful\nparadigm for representation learning and has been studied from multiple\nperspectives, including mutual information and geometric viewpoints. However,\nsupervised contrastive (SupCon) approaches have received comparatively little\nattention in this context: for instance, while InfoNCE used in SSCL is known to\nform a lower bound on mutual information (MI), the relationship between SupCon\nand MI remains unexplored. To address this gap, we introduce ProjNCE, a\ngeneralization of the InfoNCE loss that unifies supervised and self-supervised\ncontrastive objectives by incorporating projection functions and an adjustment\nterm for negative pairs. We prove that ProjNCE constitutes a valid MI bound and\naffords greater flexibility in selecting projection strategies for class\nembeddings. Building on this flexibility, we further explore the centroid-based\nclass embeddings in SupCon by exploring a variety of projection methods.\nExtensive experiments on multiple datasets and settings demonstrate that\nProjNCE consistently outperforms both SupCon and standard cross-entropy\ntraining. Our work thus refines SupCon along two complementary\nperspective--mutual information interpretation and projection design--and\noffers broadly applicable improvements whenever SupCon serves as the\nfoundational contrastive objective.", "AI": {"tldr": "论文提出ProjNCE，一种统一监督和自监督对比学习目标的损失函数，并通过实验证明其优于现有方法。", "motivation": "监督对比学习（SupCon）与互信息（MI）的关系尚未被探索，现有方法如InfoNCE仅适用于自监督学习。", "method": "引入ProjNCE，扩展InfoNCE以支持监督学习，通过投影函数和负对调整项统一目标，并验证其作为MI下界的有效性。", "result": "ProjNCE在多个数据集和设置中表现优于SupCon和标准交叉熵训练。", "conclusion": "ProjNCE从互信息解释和投影设计两方面改进了SupCon，为对比学习提供了更灵活和高效的方法。"}}
{"id": "2506.09883", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09883", "abs": "https://arxiv.org/abs/2506.09883", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "AI": {"tldr": "提出了一种轻量级、无需标注的微调框架Geometric Distillation，将几何线索注入预训练的视觉语言模型（VLM），提升其3D空间理解能力。", "motivation": "现有视觉语言模型在3D空间结构理解上存在局限，需要一种高效方法增强其几何感知能力。", "method": "通过从现成的3D基础模型中提取稀疏对应、相对深度关系和密集成本体积，注入预训练VLM中，不改变其架构。", "result": "在3D视觉语言推理和感知基准测试中表现优异，显著提升了3D空间推理能力且计算成本更低。", "conclusion": "为2D训练的VLM与3D理解之间提供了一条可扩展且高效的路径，适用于空间多模态任务。"}}
{"id": "2506.09885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09885", "abs": "https://arxiv.org/abs/2506.09885", "authors": ["Haoru Wang", "Kai Ye", "Yangyan Li", "Wenzheng Chen", "Baoquan Chen"], "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "comment": null, "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .", "AI": {"tldr": "论文研究了通用新颖视图合成（NVS）问题，提出了一种减少3D先验知识和姿态依赖的方法，通过数据驱动实现高质量视图合成。", "motivation": "传统方法依赖强3D先验知识和已知相机姿态，限制了通用性。本文探讨减少3D依赖的可行性，并发现数据规模扩大时，减少3D依赖的方法性能提升更快。", "method": "提出了一种最小化3D先验和姿态依赖的NVS框架，直接从稀疏2D图像学习隐式3D感知，无需3D先验或姿态标注。", "result": "实验表明，该方法能生成逼真且3D一致的新视图，性能与依赖姿态输入的方法相当。", "conclusion": "数据驱动范式在减少3D依赖方面具有可行性，为大规模数据时代的NVS提供了新思路。"}}
{"id": "2506.09816", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09816", "abs": "https://arxiv.org/abs/2506.09816", "authors": ["Cecilia Casolo", "Sören Becker", "Niki Kilbertus"], "title": "Identifiability Challenges in Sparse Linear Ordinary Differential Equations", "comment": "9 pages, 4 figures", "summary": "Dynamical systems modeling is a core pillar of scientific inquiry across\nnatural and life sciences. Increasingly, dynamical system models are learned\nfrom data, rendering identifiability a paramount concept. For systems that are\nnot identifiable from data, no guarantees can be given about their behavior\nunder new conditions and inputs, or about possible control mechanisms to steer\nthe system. It is known in the community that \"linear ordinary differential\nequations (ODE) are almost surely identifiable from a single trajectory.\"\nHowever, this only holds for dense matrices. The sparse regime remains\nunderexplored, despite its practical relevance with sparsity arising naturally\nin many biological, social, and physical systems. In this work, we address this\ngap by characterizing the identifiability of sparse linear ODEs. Contrary to\nthe dense case, we show that sparse systems are unidentifiable with a positive\nprobability in practically relevant sparsity regimes and provide lower bounds\nfor this probability. We further study empirically how this theoretical\nunidentifiability manifests in state-of-the-art methods to estimate linear ODEs\nfrom data. Our results corroborate that sparse systems are also practically\nunidentifiable. Theoretical limitations are not resolved through inductive\nbiases or optimization dynamics. Our findings call for rethinking what can be\nexpected from data-driven dynamical system modeling and allows for quantitative\nassessments of how much to trust a learned linear ODE.", "AI": {"tldr": "本文探讨稀疏线性ODE的可识别性，发现稀疏系统在实际相关稀疏度下存在不可识别性，并提供了概率下界。", "motivation": "研究稀疏线性ODE的可识别性，填补现有研究空白，因其在生物、社会和物理系统中的实际应用。", "method": "通过理论分析和实证研究，评估稀疏线性ODE的可识别性及其对现有估计方法的影响。", "result": "稀疏线性ODE在实际稀疏度下存在不可识别性，且理论限制无法通过归纳偏置或优化动态解决。", "conclusion": "研究呼吁重新审视数据驱动动态系统建模的期望，并为评估学习线性ODE的可信度提供量化依据。"}}
{"id": "2506.09895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09895", "abs": "https://arxiv.org/abs/2506.09895", "authors": ["Athinoulla Konstantinou", "Georgios Leontidis", "Mamatha Thota", "Aiden Durrant"], "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "comment": "19 pages, 11 Figures, 13 Tables", "summary": "Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.", "AI": {"tldr": "论文提出EquiCaps，一种基于胶囊网络的自我监督方法，无需专用预测器即可实现姿态感知，并在姿态估计任务中表现优异。", "motivation": "探索胶囊网络在姿态感知表示中的潜力，避免依赖专用预测器实现等变性。", "method": "利用胶囊网络的固有姿态感知能力，设计EquiCaps，并通过多几何变换任务验证其性能。", "result": "EquiCaps在3DIEBench-T数据集上旋转预测任务中表现优于现有方法，R²达0.78。", "conclusion": "EquiCaps展示了胶囊网络在等变性任务中的潜力，且在多几何变换下仍保持稳健性能。"}}
{"id": "2506.09824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09824", "abs": "https://arxiv.org/abs/2506.09824", "authors": ["Johan Erbani", "Sonia Ben Mokhtar", "Pierre-Edouard Portier", "Elod Egyed-Zsigmond", "Diana Nurbakova"], "title": "Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity", "comment": null, "summary": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness.", "AI": {"tldr": "论文提出了一种名为WoLA的加权损失方法，用于在联邦学习中解决数据异构性下拜占庭攻击的问题，显著优于现有方法。", "motivation": "联邦学习（FL）在隐私保护方面具有优势，但存在拜占庭参与者提交有害梯度的安全威胁。现有方法在异构数据环境下难以区分诚实与拜占庭梯度。", "method": "引入Worker Label Alignment Loss（WoLA），通过加权损失对齐诚实工作者的梯度，便于识别拜占庭梯度。", "result": "WoLA在异构数据环境下显著优于现有方法，并通过理论和实验验证其有效性。", "conclusion": "WoLA为解决联邦学习中的拜占庭攻击问题提供了有效解决方案，尤其在数据异构性强的场景中表现突出。"}}
{"id": "2506.09897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09897", "abs": "https://arxiv.org/abs/2506.09897", "authors": ["Tao Liu", "Zhenchao Cui"], "title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects", "comment": null, "summary": "Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.", "AI": {"tldr": "论文提出E-FPN-BS架构，解决特征金字塔网络中高层特征因零正锚点导致的语义缺失问题，通过多尺度特征增强和自适应优化提升小目标检测性能。", "motivation": "传统特征金字塔网络中高层特征（P5-P6）常因零正锚点而无法训练，导致语义表示缺失，同时低层特征缺乏语义上下文。", "method": "提出E-FPN-BS架构，包含上下文增强模块（CEM）和前景-背景分离模块（FBSM），并引入动态梯度平衡损失（DCLoss）解决梯度不平衡问题。", "result": "在多个基准数据集上的实验表明，该方法具有出色的性能和泛化能力。", "conclusion": "E-FPN-BS有效解决了高层特征语义缺失问题，提升了小目标检测的鲁棒性。"}}
{"id": "2506.09862", "categories": ["cs.LG", "cs.AI", "hep-ex", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.09862", "abs": "https://arxiv.org/abs/2506.09862", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarcón", "Sofia Vallecorsa", "Michele Grossi"], "title": "Guided Graph Compression for Quantum Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "AI": {"tldr": "论文提出了一种名为GGC的框架，通过图自动编码器压缩图数据，以提升量子或经典分类器的性能，并在高能物理的喷注标记任务中验证其优越性。", "motivation": "解决图神经网络（GNNs）在大规模图数据上的内存和计算效率问题，同时探索量子计算（QC）在图数据上的潜力。", "method": "提出Guided Graph Compression（GGC）框架，利用图自动编码器压缩节点数量和特征维度，并优化下游分类任务性能。", "result": "GGC在喷注标记任务中表现优于单独使用自动编码器或经典GNN分类器，同时支持在真实数据集上测试量子图神经网络（QGNNs）。", "conclusion": "GGC为大规模图数据处理和量子算法测试提供了有效解决方案，展示了量子计算在图数据上的应用潜力。"}}
{"id": "2506.09916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09916", "abs": "https://arxiv.org/abs/2506.09916", "authors": ["Tilemachos Aravanis", "Panagiotis Filntisis", "Petros Maragos", "George Retsinas"], "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage", "comment": null, "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.", "AI": {"tldr": "论文提出Only-Style方法，通过自适应调整参数来减少风格一致生成中的内容泄漏问题，并提出了新的评估框架。", "motivation": "现有方法在风格一致生成中难以有效分离语义内容和风格元素，导致内容泄漏问题。", "method": "Only-Style通过定位内容泄漏并自适应调整风格对齐参数，同时提出独立组件和评估框架。", "result": "方法在多样实例中显著优于现有技术，实现了无内容泄漏的稳健风格一致性。", "conclusion": "Only-Style有效解决了内容泄漏问题，同时保持了风格一致性，为相关任务提供了新思路。"}}
{"id": "2506.09867", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09867", "abs": "https://arxiv.org/abs/2506.09867", "authors": ["Amit Baran Dey", "Wasim Arif", "Rakhesh Singh Kshetrimayum"], "title": "Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing", "comment": "6 pages, 11 figures, Accepted to IEEE INDISCON 2025", "summary": "This paper proposes a machine learning-based methodology for the\nclassification of various oil samples based on their dielectric properties,\nutilizing a microwave resonant sensor. The dielectric behaviour of oils,\ngoverned by their molecular composition, induces distinct shifts in the\nsensor's resonant frequency and amplitude response. These variations are\nsystematically captured and processed to extract salient features, which serve\nas inputs for multiple machine learning classifiers. The microwave resonant\nsensor operates in a non-destructive, low-power manner, making it particularly\nwell-suited for real-time industrial applications. A comprehensive dataset is\ndeveloped by varying the permittivity of oil samples and acquiring the\ncorresponding sensor responses. Several classifiers are trained and evaluated\nusing the extracted resonant features to assess their capability in\ndistinguishing between oil types. Experimental results demonstrate that the\nproposed approach achieves a high classification accuracy of 99.41% with the\nrandom forest classifier, highlighting its strong potential for automated oil\nidentification. The system's compact form factor, efficiency, and high\nperformance underscore its viability for fast and reliable oil characterization\nin industrial environments.", "AI": {"tldr": "提出了一种基于机器学习的微波谐振传感器方法，用于根据介电特性分类油样，准确率达99.41%。", "motivation": "油样的分子组成决定了其介电行为，通过微波谐振传感器捕捉这些特性，实现非破坏性、低功耗的实时工业应用。", "method": "利用微波谐振传感器捕获油样介电特性引起的频率和振幅变化，提取特征后输入多种机器学习分类器。", "result": "随机森林分类器达到99.41%的分类准确率，验证了方法的有效性。", "conclusion": "该方法高效、紧凑，适用于工业环境中的快速可靠油样分类。"}}
{"id": "2506.09919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09919", "abs": "https://arxiv.org/abs/2506.09919", "authors": ["He Zhang", "Chentao Song", "Hongwen Zhang", "Tao Yu"], "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "comment": null, "summary": "We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.", "AI": {"tldr": "MetricHMR提出了一种从单目图像中恢复具有准确全局平移的度量尺度人体网格的方法，解决了现有HMR方法在尺度和深度上的模糊性问题。", "motivation": "现有HMR方法存在严重的尺度和深度模糊性，导致重建结果在几何上不合理。MetricHMR旨在解决这一问题，实现度量尺度的准确重建。", "method": "通过系统分析现有HMR方法的相机模型，强调标准透视投影模型的关键作用，并提出基于射线图的新方法，联合编码边界框信息、相机参数和几何线索，实现端到端的度量尺度HMR。", "result": "实验表明，MetricHMR在度量姿态、形状和全局平移估计上达到最先进性能，优于现有方法。", "conclusion": "MetricHMR通过标准透视投影模型和射线图方法，实现了度量尺度的人体网格恢复，解决了尺度和深度模糊性问题。"}}
{"id": "2506.09870", "categories": ["cs.LG", "cs.DC", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09870", "abs": "https://arxiv.org/abs/2506.09870", "authors": ["Maximilian Egger", "Rawad Bitar"], "title": "Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning", "comment": null, "summary": "Ensuring resilience to Byzantine clients while maintaining the privacy of the\nclients' data is a fundamental challenge in federated learning (FL). When the\nclients' data is homogeneous, suitable countermeasures were studied from an\ninformation-theoretic perspective utilizing secure aggregation techniques while\nensuring robust aggregation of the clients' gradients. However, the\ncountermeasures used fail when the clients' data is heterogeneous. Suitable\npre-processing techniques, such as nearest neighbor mixing, were recently shown\nto enhance the performance of those countermeasures in the heterogeneous\nsetting. Nevertheless, those pre-processing techniques cannot be applied with\nthe introduced privacy-preserving mechanisms.\n  We propose a multi-stage method encompassing a careful co-design of\nverifiable secret sharing, secure aggregation, and a tailored symmetric private\ninformation retrieval scheme to achieve information-theoretic privacy\nguarantees and Byzantine resilience under data heterogeneity. We evaluate the\neffectiveness of our scheme on a variety of attacks and show how it outperforms\nthe previously known techniques. Since the communication overhead of secure\naggregation is non-negligible, we investigate the interplay with zero-order\nestimation methods that reduce the communication cost in state-of-the-art FL\ntasks and thereby make private aggregation scalable.", "AI": {"tldr": "提出了一种多阶段方法，结合可验证秘密共享、安全聚合和定制对称私有信息检索方案，以在数据异构性下实现信息论隐私保证和拜占庭弹性。", "motivation": "解决联邦学习中拜占庭客户端攻击与数据隐私保护的挑战，尤其是在数据异构的情况下。", "method": "采用多阶段方法，结合可验证秘密共享、安全聚合和定制对称私有信息检索方案。", "result": "在多种攻击下验证了方案的有效性，并优于现有技术。同时通过零阶估计方法降低通信成本。", "conclusion": "该方法在数据异构性下实现了隐私保护和拜占庭弹性，同时通过通信优化使其具有可扩展性。"}}
{"id": "2506.09920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09920", "abs": "https://arxiv.org/abs/2506.09920", "authors": ["Jianhan Qi", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "comment": null, "summary": "Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.", "AI": {"tldr": "论文提出了一种结合结构-光谱图卷积算子（SSGCO）和证据引导的自适应边缘学习（EGAEL）模块的方法，用于高光谱图像（HSI）聚类，显著提升了聚类精度。", "motivation": "现有基于图神经网络（GNNs）的方法无法充分利用HSI的光谱信息，且超像素拓扑图的不准确性可能导致类语义混淆。", "method": "提出SSGCO提取空间和光谱特征，并设计EGAEL模块自适应优化边缘权重，结合对比学习框架实现聚类。", "result": "在四个HSI数据集上，聚类精度分别提升了2.61%、6.06%、4.96%和3.15%。", "conclusion": "SSGCO和EGAEL模块有效提升了HSI聚类的性能和鲁棒性。"}}
{"id": "2506.09887", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.09887", "abs": "https://arxiv.org/abs/2506.09887", "authors": ["Nirmit Joshi", "Hugo Koubbi", "Theodor Misiakiewicz", "Nathan Srebro"], "title": "Learning single-index models via harmonic decomposition", "comment": "80 pages", "summary": "We study the problem of learning single-index models, where the label $y \\in\n\\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through\nan unknown one-dimensional projection $\\langle\n\\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under\nGaussian inputs, the statistical and computational complexity of recovering\n$\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.\nIn this paper, we propose a new perspective: we argue that \"spherical\nharmonics\" -- rather than \"Hermite polynomials\" -- provide the natural basis\nfor this problem, as they capture its intrinsic \"rotational symmetry\". Building\non this insight, we characterize the complexity of learning single-index models\nunder arbitrary spherically symmetric input distributions. We introduce two\nfamilies of estimators -- based on tensor unfolding and online SGD -- that\nrespectively achieve either optimal sample complexity or optimal runtime, and\nargue that estimators achieving both may not exist in general. When specialized\nto Gaussian inputs, our theory not only recovers and clarifies existing results\nbut also reveals new phenomena that had previously been overlooked.", "AI": {"tldr": "论文研究了单指标模型的学习问题，提出球谐函数比Hermite多项式更适合作为基础，并基于此设计了两种估计器。", "motivation": "探索单指标模型学习问题的统计和计算复杂性，特别是输入分布的旋转对称性。", "method": "提出基于球谐函数的新视角，设计两种估计器：张量展开和在线SGD。", "result": "在任意球对称输入分布下，两种估计器分别实现了最优样本复杂度和最优运行时间。", "conclusion": "球谐函数视角不仅统一了现有结果，还揭示了新现象，表明同时实现样本和计算最优的估计器可能不存在。"}}
{"id": "2506.09932", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09932", "abs": "https://arxiv.org/abs/2506.09932", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "comment": "4 Pages, 5 Figures", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "AI": {"tldr": "HadaNorm是一种新型线性变换方法，通过归一化激活特征通道并结合Hadamard变换，有效减少异常值，实现更激进的激活量化，提升扩散模型的效率和性能。", "motivation": "扩散模型在图像生成领域表现优异，但其高内存和计算需求限制了在资源受限设备上的部署。现有的后训练量化方法在处理异常值和实现高压缩时存在困难。", "method": "提出HadaNorm方法，通过归一化激活特征通道并应用Hadamard变换，减少量化误差，实现更高效的矩阵操作压缩。", "result": "HadaNorm在多种Transformer模块中均能减少量化误差，并在效率与性能的权衡上优于现有方法。", "conclusion": "HadaNorm为扩散模型在资源受限设备上的部署提供了一种高效的量化解决方案。"}}
{"id": "2506.09891", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2506.09891", "abs": "https://arxiv.org/abs/2506.09891", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "title": "Causal Climate Emulation with Bayesian Filtering", "comment": "32 pages, 21 figures", "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "AI": {"tldr": "论文提出了一种基于因果表示学习的可解释气候模型模拟器，通过物理信息方法和贝叶斯滤波器实现长期稳定的自回归模拟。", "motivation": "传统气候模型计算成本高，限制了气候变化预测和分析。机器学习虽能快速模拟气候数据，但现有方法无法结合物理因果关系。", "method": "开发了一种基于因果表示学习的可解释模拟器，采用物理信息方法和贝叶斯滤波器进行长期自回归模拟。", "result": "模拟器能准确学习气候动态，并在合成数据集和两种广泛使用的气候模型数据上验证了各组件的重要性。", "conclusion": "该模拟器为气候模型提供了一种高效且可解释的替代方案，结合了物理因果关系的优势。"}}
{"id": "2506.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09935", "abs": "https://arxiv.org/abs/2506.09935", "authors": ["Jiangyong Huang", "Xiaojian Ma", "Xiongkun Linghu", "Yue Fan", "Junchao He", "Wenxin Tan", "Qing Li", "Song-Chun Zhu", "Yixin Chen", "Baoxiong Jia", "Siyuan Huang"], "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation", "comment": "Project page: https://leo-vl.github.io", "summary": "Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.", "AI": {"tldr": "LEO-VL是一种基于高效场景表示（CFG）的3D-VL模型，通过大规模训练和多样化任务数据，实现了在多个3D QA基准上的领先性能。", "motivation": "解决3D-VL通用模型在能力和鲁棒性上落后于2D模型的问题，特别是数据可扩展性和高效场景表示的障碍。", "method": "提出LEO-VL模型，采用CFG（压缩特征网格）作为高效场景表示，并结合大规模多任务数据训练。", "result": "在SQA3D、MSQA和Beacon3D等基准测试中取得最佳性能，验证了CFG的高效性和数据多样性的重要性。", "conclusion": "LEO-VL和SceneDPO为开发可扩展且鲁棒的3D-VL通用模型提供了有效方法。"}}
{"id": "2506.09896", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09896", "abs": "https://arxiv.org/abs/2506.09896", "authors": ["Attanasia Garuso", "Silvija Kokalj-Filipovic", "Yagna Kaasaragadda"], "title": "A look at adversarial attacks on radio waveforms from discrete latent space", "comment": null, "summary": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks.", "AI": {"tldr": "该论文研究了VQVAE在对抗攻击下的防御能力，通过分析其对高信噪比射频数据的攻击抑制效果，并比较了保留相位和不保留相位的攻击方式。", "motivation": "研究VQVAE在对抗攻击下的表现，探索其在保护数字无线电波形分类中的潜力。", "method": "设计保留相位的对抗攻击，比较其与不保留相位攻击的效果，并通过分类器评估VQVAE的重建能力。", "result": "VQVAE显著降低了攻击的有效性，并揭示了离散潜在空间的特性。", "conclusion": "VQVAE在对抗攻击下表现出良好的防御能力，其潜在空间特性可能有助于攻击检测。"}}
{"id": "2506.09943", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2506.09943", "abs": "https://arxiv.org/abs/2506.09943", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "AI": {"tldr": "CausalVQA是一个用于视频问答（VQA）的基准数据集，专注于模型对物理世界中因果关系的理解，填补了现有数据集的空白。", "motivation": "现有VQA数据集多关注表面感知或狭窄的物理推理问题，缺乏对真实场景中因果关系的深入探究。", "method": "CausalVQA设计了五种问题类型（反事实、假设、预期、规划和描述性），并通过质量控制机制避免模型利用语言捷径。", "result": "当前前沿多模态模型在基准测试中表现远低于人类，尤其在预期和假设问题上。", "conclusion": "CausalVQA揭示了当前系统在时空推理、物理原理理解和替代方案预测方面的挑战。"}}
{"id": "2506.09901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09901", "abs": "https://arxiv.org/abs/2506.09901", "authors": ["Noel Brindise", "Vijeth Hebbar", "Riya Shah", "Cedric Langbort"], "title": "\"What are my options?\": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)", "comment": null, "summary": "In this work, we provide an extended discussion of a new approach to\nexplainable Reinforcement Learning called Diverse Near-Optimal Alternatives\n(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable \"options\" for\ntrajectory-planning agents, optimizing policies to produce qualitatively\ndiverse trajectories in Euclidean space. In the spirit of explainability, these\ndistinct policies are used to \"explain\" an agent's options in terms of\navailable trajectory shapes from which a human user may choose. In particular,\nDNA applies to value function-based policies on Markov decision processes where\nagents are limited to continuous trajectories. Here, we describe DNA, which\nuses reward shaping in local, modified Q-learning problems to solve for\ndistinct policies with guaranteed epsilon-optimality. We show that it\nsuccessfully returns qualitatively different policies that constitute\nmeaningfully different \"options\" in simulation, including a brief comparison to\nrelated approaches in the stochastic optimization field of Quality Diversity.\nBeyond the explanatory motivation, this work opens new possibilities for\nexploration and adaptive planning in RL.", "AI": {"tldr": "本文提出了一种名为DNA的可解释强化学习新方法，通过生成多样化的近似最优轨迹选项，帮助人类用户理解代理的决策。", "motivation": "旨在通过提供多样化的轨迹选项，增强强化学习代理的可解释性，同时为探索和自适应规划开辟新可能性。", "method": "使用奖励塑造和局部改进的Q学习问题，求解具有保证ε最优性的多样化策略。", "result": "成功生成了具有显著差异的轨迹选项，并在模拟中验证了其有效性。", "conclusion": "DNA方法不仅提升了可解释性，还为强化学习的探索和规划提供了新思路。"}}
{"id": "2506.09952", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09952", "abs": "https://arxiv.org/abs/2506.09952", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "comment": "Accepted to CVPR 2025", "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "AI": {"tldr": "UniPre3D是一种统一预训练方法，适用于任意尺度的点云数据和任意架构的3D模型，通过预测高斯基元和使用可微分高斯渲染实现端到端优化。", "motivation": "点云数据的尺度多样性导致缺乏统一的3D表示学习方法，现有预训练方法无法同时适用于对象和场景级点云。", "method": "提出UniPre3D，预测高斯基元作为预训练任务，利用可微分高斯渲染实现像素级监督，并结合预训练图像模型的特征引入纹理知识。", "result": "在多种对象和场景级任务中验证了方法的通用有效性。", "conclusion": "UniPre3D是首个适用于任意尺度和架构的统一预训练方法，为3D视觉提供了新的解决方案。"}}
{"id": "2506.09923", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09923", "abs": "https://arxiv.org/abs/2506.09923", "authors": ["Liou Tang", "James Joshi", "Ashish Kundu"], "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning", "comment": null, "summary": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples.", "AI": {"tldr": "论文提出了一种新的隐私攻击方法Apollo，针对机器遗忘（MU）场景，仅需访问遗忘模型的标签输出即可推断数据样本是否被遗忘，攻击效果优于现有方法。", "motivation": "现有针对MU的隐私推断攻击依赖较弱的威胁模型（需访问原始模型和遗忘模型），限制了实际应用。本文旨在设计一种更严格的威胁模型下的高效攻击方法。", "method": "提出A Posteriori Label-Only Membership Inference Attack（Apollo），仅利用遗忘模型的标签输出推断样本是否被遗忘。", "result": "实验表明，Apollo在仅需较少模型访问权限的情况下，仍能高精度推断遗忘样本的成员状态。", "conclusion": "Apollo攻击方法在严格威胁模型下高效，揭示了MU场景中潜在的隐私风险。"}}
{"id": "2506.09953", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09953", "abs": "https://arxiv.org/abs/2506.09953", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "comment": null, "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "AI": {"tldr": "论文介绍了基于视频的视觉问答任务（OK-VQA），提出了一种新数据集，要求模型结合视觉信息和外部知识进行对话。", "motivation": "探索在视频对话中结合视觉信息和外部知识的挑战，填补现有研究的空白。", "method": "构建了一个包含2017个视频和5986个人工标注对话的数据集，模型需识别视频片段并利用外部知识回答问题。", "result": "提供了多个基线模型，展示了任务的挑战性，并公开了数据集。", "conclusion": "该任务为未来研究提供了新方向，数据集有助于推动相关领域发展。"}}
{"id": "2506.09928", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09928", "abs": "https://arxiv.org/abs/2506.09928", "authors": ["Ruixuan Xu", "Xiangxiang Weng"], "title": "Bayesian Probabilistic Matrix Factorization", "comment": "11 pages, 4 figures", "summary": "Matrix factorization is a widely used technique in recommendation systems.\nProbabilistic Matrix Factorization (PMF) [1] extends traditional matrix\nfactorization by incorporating probability distributions over latent factors,\nallowing for uncertainty quantification. However, computing the posterior\ndistribution is intractable due to the high-dimensional integral. To address\nthis, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)\n[2] and Variational Inference (VI) [3] to approximate the posterior. We\nevaluate their performance on MovieLens dataset and compare their convergence\nspeed, predictive accuracy, and computational efficiency. Experimental results\ndemonstrate that VI offers faster convergence, while MCMC provides more\naccurate posterior estimates.", "AI": {"tldr": "论文比较了两种贝叶斯推断方法（MCMC和VI）在概率矩阵分解中的应用，发现VI收敛更快，而MCMC后验估计更准确。", "motivation": "传统矩阵分解缺乏不确定性量化，概率矩阵分解（PMF）通过引入概率分布解决了这一问题，但后验分布计算困难。", "method": "采用马尔可夫链蒙特卡洛（MCMC）和变分推断（VI）两种贝叶斯推断方法近似后验分布。", "result": "在MovieLens数据集上，VI收敛速度更快，而MCMC的后验估计更准确。", "conclusion": "VI适合需要快速收敛的场景，而MCMC适合需要高精度后验估计的场景。"}}
{"id": "2506.09954", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09954", "abs": "https://arxiv.org/abs/2506.09954", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "title": "Vision Generalist Model: A Survey", "comment": "Accepted by International Journal of Computer Vision (IJCV)", "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "AI": {"tldr": "本文综述了视觉通用模型的特点、能力及其在计算机视觉任务中的应用，包括背景回顾、框架设计、性能提升技术、相关领域联系以及未来研究方向。", "motivation": "通用模型在自然语言处理中表现优异，但其在计算机视觉任务中的应用面临输入输出多样化的挑战，因此需要系统研究。", "method": "回顾背景（数据集、任务、基准）、分析现有框架设计、介绍性能提升技术、探讨相关领域联系。", "result": "总结了视觉通用模型的特点、能力及实际应用场景，并指出当前挑战。", "conclusion": "视觉通用模型潜力巨大，但仍需解决多样化输入输出的统一表示问题，未来研究应关注框架优化和多领域协同。"}}
{"id": "2506.09940", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.09940", "abs": "https://arxiv.org/abs/2506.09940", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "comment": "Accepted at ICML 2025", "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "AI": {"tldr": "本文提出了一种样本高效的算法，用于在信息不对称和知识迁移挑战下学习系统动态，并在强化学习中实现ε最优策略。", "motivation": "信息不对称和知识迁移是多智能体系统中的核心挑战，尤其是在经济学和社会科学中。本文旨在解决这些挑战，以优化智能体的策略学习。", "method": "提出了一种样本高效的算法，通过非独立同分布动作学习混杂变量，并在在线战略交互模型中实现知识迁移。", "result": "算法在样本复杂度为O(1/ε²)的情况下，可证明地学习到ε最优策略。", "conclusion": "该方法有效解决了信息不对称和知识迁移问题，为强化学习中的策略优化提供了理论保障。"}}
{"id": "2506.09958", "categories": ["cs.CV", "cs.LG", "68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing)", "I.2.10; I.2.6; J.3"], "pdf": "https://arxiv.org/pdf/2506.09958", "abs": "https://arxiv.org/abs/2506.09958", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "comment": null, "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "AI": {"tldr": "Kvasir-VQA-x1是一个新的大规模胃肠道内窥镜数据集，扩展了原有数据集，增加了159,549个问题-答案对，旨在测试更深层次的临床推理。", "motivation": "现有MedVQA数据集缺乏临床复杂性和视觉多样性，限制了临床决策支持系统的发展。", "method": "使用大语言模型生成分层复杂性的问题，并引入视觉增强以模拟常见成像伪影。", "result": "数据集支持标准VQA性能和模型鲁棒性评估，为临床场景提供更具挑战性的基准。", "conclusion": "Kvasir-VQA-x1旨在加速开发更可靠的多模态AI系统，并遵循FAIR数据原则，为研究社区提供资源。"}}
{"id": "2506.09955", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09955", "abs": "https://arxiv.org/abs/2506.09955", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "title": "Canonical Latent Representations in Conditional Diffusion Models", "comment": "45 pages,41 figures", "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "AI": {"tldr": "论文提出了一种名为CLAReps的潜在表示方法，用于解决条件扩散模型（CDMs）在生成任务中类别特征与无关上下文纠缠的问题，并开发了基于扩散的特征蒸馏范式CaDistill。", "motivation": "条件扩散模型（CDMs）在生成任务中表现出色，但其建模能力导致类别特征与无关上下文纠缠，难以提取鲁棒且可解释的表示。", "method": "通过识别CLAReps（保留类别信息、丢弃无关信号的潜在代码），并开发CaDistill范式，利用CDM作为教师模型，仅通过CLAReps传递核心类别知识。", "result": "实验表明，学生模型仅使用10%的训练数据（CLAReps）即可实现强对抗鲁棒性和泛化能力，更专注于类别信号而非虚假背景线索。", "conclusion": "CDMs不仅可以作为图像生成器，还能作为紧凑、可解释的教师模型，推动鲁棒表示学习。"}}
{"id": "2506.09965", "categories": ["cs.CV", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2506.09965", "abs": "https://arxiv.org/abs/2506.09965", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "comment": null, "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "AI": {"tldr": "论文提出了一种新的多模态推理范式，通过视觉绘图操作增强大型视觉语言模型的空间推理能力。", "motivation": "现有方法在多模态推理中主要依赖纯文本方式，难以处理需要精确几何理解和空间跟踪的任务。", "method": "提出了一种基于绘图操作的视觉推理方法，包括标注边界框和辅助线，并通过三阶段训练框架（冷启动训练、反射拒绝采样和强化学习）提升模型能力。", "result": "模型VILASR在多个空间推理任务中平均提升18.4%，显著优于现有方法。", "conclusion": "通过视觉绘图操作，模型能够更有效地表达和分析空间关系，突破了纯文本方法的局限性。"}}
{"id": "2506.09991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09991", "abs": "https://arxiv.org/abs/2506.09991", "authors": ["Xinyu Yang", "Yuwei An", "Hongyi Liu", "Tianqi Chen", "Beidi Chen"], "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation", "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.", "AI": {"tldr": "Multiverse是一种新型生成模型，通过MapReduce范式实现原生并行生成，性能与领先的自回归大语言模型相当，且具有更高的效率和扩展性。", "motivation": "受自回归大语言模型（AR-LLMs）中隐含并行性的启发，开发一种能够原生并行生成的模型，以提高效率和性能。", "method": "Multiverse采用MapReduce范式，分为Map（任务分解）、Process（并行执行子任务）和Reduce（结果合成）三个阶段。通过数据、算法和系统的协同设计，实现快速迁移和高效训练。", "result": "Multiverse-32B在3小时微调后，性能与同规模领先AR-LLMs相当（AIME24 & 25得分54%和46%），且在相同上下文长度下平均表现优于AR-LLMs 1.87%，速度提升高达2倍。", "conclusion": "Multiverse展示了原生并行生成模型的潜力，为高效、高性能的生成任务提供了新方向，并开源了整个生态系统以促进研究。"}}
{"id": "2506.09969", "categories": ["cs.CV", "I.3.3; I.5"], "pdf": "https://arxiv.org/pdf/2506.09969", "abs": "https://arxiv.org/abs/2506.09969", "authors": ["Jeripothula Prudviraj", "Vikram Jamwal"], "title": "Vectorized Region Based Brush Strokes for Artistic Rendering", "comment": null, "summary": "Creating a stroke-by-stroke evolution process of a visual artwork tries to\nbridge the emotional and educational gap between the finished static artwork\nand its creation process. Recent stroke-based painting systems focus on\ncapturing stroke details by predicting and iteratively refining stroke\nparameters to maximize the similarity between the input image and the rendered\noutput. However, these methods often struggle to produce stroke compositions\nthat align with artistic principles and intent. To address this, we explore an\nimage-to-painting method that (i) facilitates semantic guidance for brush\nstrokes in targeted regions, (ii) computes the brush stroke parameters, and\n(iii) establishes a sequence among segments and strokes to sequentially render\nthe final painting. Experimental results on various input image types, such as\nface images, paintings, and photographic images, show that our method aligns\nwith a region-based painting strategy while rendering a painting with high\nfidelity and superior stroke quality.", "AI": {"tldr": "提出了一种基于语义引导的图像到绘画方法，通过优化笔触参数和顺序渲染，提升绘画的艺术性和质量。", "motivation": "解决现有笔触绘画系统在捕捉艺术原则和意图上的不足，弥合作品与创作过程之间的情感与教育鸿沟。", "method": "结合语义引导、笔触参数计算和顺序渲染，实现区域化的绘画策略。", "result": "在多种输入图像上验证了方法的高保真度和优质笔触效果。", "conclusion": "该方法能更好地满足艺术原则和意图，提升绘画质量。"}}
{"id": "2506.09998", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09998", "abs": "https://arxiv.org/abs/2506.09998", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Schölkopf"], "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "comment": "Technical Report v1 (21 pages, 14 figures)", "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "AI": {"tldr": "论文研究了LLMs在描述概率分布与生成样本之间的差距，提出了一种基于自然语言的拒绝采样方法（VRS），显著减少了采样偏差，并展示了如何将经典概率工具嵌入LLM工作流以提高可靠性。", "motivation": "LLMs能准确描述概率分布但生成样本时存在偏差，限制了其在需要可靠随机性的任务中的应用。", "method": "提出Verbalized Rejection Sampling (VRS)，一种基于自然语言的拒绝采样方法，通过提示LLM接受或拒绝样本。", "result": "VRS显著减少了采样偏差，理论分析表明其在直接采样基础上有所改进。", "conclusion": "VRS展示了如何将经典概率工具嵌入LLM工作流以提高可靠性，无需访问模型内部或复杂提示设计。"}}
{"id": "2506.09980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09980", "abs": "https://arxiv.org/abs/2506.09980", "authors": ["Jiaxiang Tang", "Ruijie Lu", "Zhaoshuo Li", "Zekun Hao", "Xuan Li", "Fangyin Wei", "Shuran Song", "Gang Zeng", "Ming-Yu Liu", "Tsung-Yi Lin"], "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "comment": "Code: https://github.com/NVlabs/PartPacker Project Page:\n  https://research.nvidia.com/labs/dir/partpacker/", "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.", "AI": {"tldr": "提出了一种新的端到端框架，用于生成具有任意数量语义有意义部分的3D对象。", "motivation": "现有方法生成的3D对象通常将所有部分融合为一个整体，限制了编辑和操纵单个部分的能力。", "method": "采用双体积打包策略，将所有部分组织到两个互补的体积中，生成完整且交错的部件。", "result": "实验表明，该方法在质量、多样性和泛化能力上优于现有基于图像的部分级生成方法。", "conclusion": "该方法成功解决了生成具有可变数量部分的3D对象的挑战。"}}
{"id": "2505.14156", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "I.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2505.14156", "abs": "https://arxiv.org/abs/2505.14156", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "comment": null, "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs.", "AI": {"tldr": "论文提出了一种名为Symbolic Graph Ranker (SGR)的方法，结合文本和图结构信息，利用大语言模型(LLMs)提升会话搜索的效果。", "motivation": "当前会话搜索方法侧重于顺序建模或通用图结构表示，忽略了词级语义和图形结构的结合。", "method": "SGR通过符号语法规则将会话图转换为文本，并设计自监督任务（如链接预测和节点内容生成）增强LLMs对图结构的理解。", "result": "在AOL和Tiangong-ST数据集上的实验证明了SGR的优越性。", "conclusion": "SGR为传统搜索策略与现代LLMs之间架起了桥梁，提供了一种新颖有效的方法。"}}
{"id": "2506.09982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09982", "abs": "https://arxiv.org/abs/2506.09982", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "comment": "Project Page: https://animateanymesh.github.io/AnimateAnyMesh/", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "AI": {"tldr": "AnimateAnyMesh是一个前馈框架，首次实现基于文本的高效3D网格动画生成，通过DyMeshVAE架构和Rectified Flow训练策略，显著提升了4D内容生成的效率和质量。", "motivation": "当前4D内容生成面临建模时空分布的复杂性和训练数据稀缺的挑战，AnimateAnyMesh旨在解决这些问题。", "method": "采用DyMeshVAE架构分离时空特征并保持局部拓扑结构，结合Rectified Flow训练策略实现文本条件生成。", "result": "实验表明，该方法能在几秒内生成语义准确且时序连贯的网格动画，质量和效率均优于现有方法。", "conclusion": "AnimateAnyMesh显著推动了4D内容生成的实用性和可访问性，相关数据和模型将开源。"}}
{"id": "2506.09987", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09987", "abs": "https://arxiv.org/abs/2506.09987", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "comment": null, "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "AI": {"tldr": "论文提出了MVP基准测试，用于评估视频语言模型的物理世界理解能力，通过最小变化对避免模型依赖表面线索。", "motivation": "现有基准测试因依赖表面视觉或文本线索而难以准确评估模型的时空理解能力。", "method": "引入MVP基准测试，包含55K个高质量多选题视频QA样本，每个样本配有最小变化对。", "result": "人类表现92.9%，最佳开源模型40.2%，随机表现25%。", "conclusion": "MVP基准测试能更准确地评估模型的物理理解能力，避免表面线索的干扰。"}}
{"id": "2506.09988", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09988", "abs": "https://arxiv.org/abs/2506.09988", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "comment": null, "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "AI": {"tldr": "EditInspector是一个新的基准，用于评估文本引导的图像编辑质量，基于人工标注。研究发现当前模型在全面评估编辑时表现不佳，并提出了两种新方法以改进。", "motivation": "随着生成式AI的发展，文本引导的图像编辑日益普及，但缺乏一个全面的框架来验证和评估这些编辑的质量。", "method": "引入EditInspector基准，利用人工标注评估编辑质量，并测试现有模型在多个维度上的表现。提出了两种新方法以改进模型表现。", "result": "当前模型在评估编辑时表现不佳，容易产生幻觉。新方法在伪影检测和差异描述生成上优于现有技术。", "conclusion": "EditInspector为文本引导编辑评估提供了新基准，新方法显著提升了评估效果。"}}
{"id": "2506.09989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09989", "abs": "https://arxiv.org/abs/2506.09989", "authors": ["Yiming Dou", "Wonseok Oh", "Yuqing Luo", "Antonio Loquercio", "Andrew Owens"], "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "comment": "CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ ,\n  Code: https://github.com/Dou-Yiming/hearing_hands/", "summary": "We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/", "AI": {"tldr": "研究如何通过预测人手与3D场景物理交互的声音，实现交互式3D场景重建。", "motivation": "探索如何通过声音增强3D场景的交互性，使其更真实。", "method": "录制人手操作3D场景中物体的视频，利用动作-声音对训练修正流模型，将3D手部轨迹映射到对应音频。", "result": "生成的音频能准确传达材质属性和动作，人类观察者难以区分其与真实声音。", "conclusion": "该方法成功实现了通过手部动作预测声音，提升了3D场景的交互体验。"}}
{"id": "2506.09993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09993", "abs": "https://arxiv.org/abs/2506.09993", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "title": "Text-Aware Image Restoration with Diffusion Models", "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "AI": {"tldr": "本文提出了一种新的图像修复任务TAIR，专注于同时恢复视觉内容和文本保真度，并提出了TeReDiff框架和SA-Text基准数据集，显著提升了文本识别准确性。", "motivation": "现有基于扩散的图像修复方法在自然图像修复中表现良好，但在文本区域重建时容易产生错误的文本模式（文本图像幻觉）。", "method": "提出了TeReDiff框架，结合扩散模型和文本检测模块，通过联合训练提取丰富的文本表征作为去噪提示。", "result": "实验表明，该方法在文本识别准确性上显著优于现有方法。", "conclusion": "TAIR任务和TeReDiff框架为文本感知图像修复提供了有效解决方案。"}}
{"id": "2506.09995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09995", "abs": "https://arxiv.org/abs/2506.09995", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "title": "PlayerOne: Egocentric World Simulator", "comment": "Project page: https://playerone-hku.github.io/", "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.", "AI": {"tldr": "PlayerOne是首个以自我为中心的逼真世界模拟器，通过用户提供的场景图像生成动态视频，严格对齐真实场景中的人体运动。", "motivation": "旨在实现沉浸式且无限制的动态环境探索，推动世界建模及其多样化应用的研究。", "method": "采用由粗到细的训练流程，包括大规模文本-视频对预训练和同步运动-视频数据微调，设计了部分解耦的运动注入方案和联合重建框架。", "result": "实验表明其在精确控制人体运动和多样化场景一致性建模方面具有强大泛化能力。", "conclusion": "PlayerOne开创了自我中心真实世界模拟的先河，为世界建模及其应用开辟了新方向。"}}
