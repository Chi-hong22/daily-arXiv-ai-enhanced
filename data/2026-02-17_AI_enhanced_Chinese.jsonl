{"id": "2602.13264", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13264", "abs": "https://arxiv.org/abs/2602.13264", "authors": ["Souradeep Chattopadhyay", "Brendan Kennedy", "Sai Munikoti", "Soumik Sarkar", "Karl Pazdernik"], "title": "Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models", "comment": null, "summary": "In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u5411\u6027\u96c6\u4e2d\u5ea6\u4e0d\u786e\u5b9a\u6027\uff08DCU\uff09\u7684\u65b0\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u51af\u00b7\u7c73\u585e\u65af-\u8d39\u820d\u5c14\u5206\u5e03\u6d4b\u91cf\u5d4c\u5165\u5411\u91cf\u7684\u51e0\u4f55\u5206\u6563\u5ea6\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u96be\u4ee5\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u6cdb\u5316\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u901a\u7528\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u65b9\u5411\u6027\u96c6\u4e2d\u5ea6\u4e0d\u786e\u5b9a\u6027\uff08DCU\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u51af\u00b7\u7c73\u585e\u65af-\u8d39\u820d\u5c14\u5206\u5e03\u91cf\u5316\u5d4c\u5165\u5411\u91cf\u7684\u96c6\u4e2d\u5ea6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d4b\u91cf\u8bed\u8a00\u6a21\u578b\u591a\u4e2a\u751f\u6210\u8f93\u51fa\u7684\u8fde\u7eed\u5d4c\u5165\u5411\u91cf\u7684\u51e0\u4f55\u5206\u6563\u5ea6\u6765\u6355\u83b7\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\u3002", "result": "DCU\u5728\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u8bed\u4e49\u71b5\u7b49\u5148\u524d\u65b9\u6cd5\u7684\u6821\u51c6\u6c34\u5e73\uff0c\u5728\u66f4\u590d\u6742\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DCU\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u6709\u6548\u7684\u6846\u67b6\uff0c\u5177\u6709\u5728\u591a\u6a21\u6001\u548c\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u96c6\u6210\u7684\u5e7f\u6cdb\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.13345", "categories": ["cs.LG", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13345", "abs": "https://arxiv.org/abs/2602.13345", "authors": ["Ethan Seefried", "Ran Eldegaway", "Sanjay Das", "Nathaniel Blanchard", "Tirthankar Ghosal"], "title": "BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents", "comment": "20 pages 8 main + 12 appendix + references", "summary": "Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.\n  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.", "AI": {"tldr": "Blueprint\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u5de5\u7a0b\u56fe\u7eb8\u6863\u6848\u7684\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u533a\u57df\u68c0\u6d4b\u3001OCR\u8bc6\u522b\u3001\u6807\u8bc6\u7b26\u6807\u51c6\u5316\u548c\u6df7\u5408\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u7a0b\u56fe\u7eb8\u7684\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u6570\u5341\u5e74\u7684\u5de5\u7a0b\u56fe\u7eb8\u548c\u6280\u672f\u8bb0\u5f55\u88ab\u9501\u5b9a\u5728\u9057\u7559\u6863\u6848\u4e2d\uff0c\u5143\u6570\u636e\u4e0d\u4e00\u81f4\u6216\u7f3a\u5931\uff0c\u5bfc\u81f4\u68c0\u7d22\u56f0\u96be\u4e14\u901a\u5e38\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u5de5\u7a0b\u6863\u6848\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u7cfb\u7edf\u68c0\u6d4b\u6807\u51c6\u56fe\u7eb8\u533a\u57df\uff0c\u5e94\u7528\u533a\u57df\u9650\u5236\u7684VLM-based OCR\uff0c\u6807\u51c6\u5316\u6807\u8bc6\u7b26\uff08\u5982DWG\u3001\u96f6\u4ef6\u3001\u8bbe\u65bd\uff09\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u533a\u57df\u7ea7\u91cd\u6392\u5668\u878d\u5408\u8bcd\u6c47\u68c0\u7d22\u548c\u5bc6\u96c6\u68c0\u7d22\u3002", "result": "\u5728\u5305\u542b350\u4e2a\u4e13\u5bb6\u7b56\u5212\u67e5\u8be2\u76845k\u6587\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBlueprint\u5728Success@3\u4e0a\u83b7\u5f9710.1%\u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u5728nDCG@3\u4e0a\u83b7\u5f9718.9%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u4f18\u4e8e\u6700\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7ebf\u3002", "conclusion": "Blueprint\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u5143\u6570\u636e\uff0c\u652f\u6301\u8de8\u8bbe\u65bd\u641c\u7d22\uff0c\u4e3a\u9057\u7559\u5de5\u7a0b\u6863\u6848\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u91ca\u653e\u4e86\u6240\u6709\u67e5\u8be2\u3001\u8fd0\u884c\u3001\u6ce8\u91ca\u548c\u4ee3\u7801\u3002"}}
{"id": "2602.13359", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13359", "abs": "https://arxiv.org/abs/2602.13359", "authors": ["Hannes Kath", "Thiago S. Gouv\u00eaa", "Daniel Sonntag"], "title": "The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric", "comment": null, "summary": "Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\"\u52a0\u901f\u56e0\u5b50\"\u4f5c\u4e3a\u4e3b\u52a8\u5b66\u4e60\u8bc4\u4f30\u7684\u65b0\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u67e5\u8be2\u65b9\u6cd5\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u6240\u9700\u7684\u6837\u672c\u6bd4\u4f8b\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u65b9\u6cd5\u5f00\u53d1\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u8fed\u4ee3\u8fc7\u7a0b\u8bc4\u4f30\u6307\u6807\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u51c6\u786e\u8861\u91cf\u67e5\u8be2\u65b9\u6cd5\u5728\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u6548\u679c\u3002", "method": "\u56de\u987e\u516b\u5e74\u4e3b\u52a8\u5b66\u4e60\u8bc4\u4f30\u6587\u732e\uff0c\u6b63\u5f0f\u63d0\u51fa\"\u52a0\u901f\u56e0\u5b50\"\u4f5c\u4e3a\u591a\u8fed\u4ee3\u67e5\u8be2\u65b9\u6cd5\u6027\u80fd\u6307\u6807\uff0c\u4f7f\u7528\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u6570\u636e\u96c6\u548c\u4e03\u79cd\u4e0d\u540c\u7c7b\u578b\u67e5\u8be2\u65b9\u6cd5\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u52a0\u901f\u56e0\u5b50\u7684\u7406\u8bba\u5047\u8bbe\uff0c\u8bc1\u660e\u5176\u80fd\u51c6\u786e\u6355\u6349\u6240\u9700\u6837\u672c\u6bd4\u4f8b\uff0c\u5e76\u663e\u793a\u51fa\u5728\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u76f8\u6bd4\u73b0\u6709\u6307\u6807\u7684\u4f18\u8d8a\u7a33\u5b9a\u6027\u3002", "conclusion": "\u52a0\u901f\u56e0\u5b50\u662f\u4e3b\u52a8\u5b66\u4e60\u8bc4\u4f30\u7684\u6709\u6548\u5b9a\u91cf\u6307\u6807\uff0c\u80fd\u51c6\u786e\u8861\u91cf\u67e5\u8be2\u65b9\u6cd5\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u4e3b\u52a8\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.13912", "categories": ["cs.AI", "cs.CL", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.13912", "abs": "https://arxiv.org/abs/2602.13912", "authors": ["Sha Li", "Stefano Petrangeli", "Yu Shen", "Xiang Chen"], "title": "From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design", "comment": null, "summary": "We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.", "AI": {"tldr": "LaySPA\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\u73af\u5883\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u663e\u5f0f\u3001\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7528\u4e8e\u5185\u5bb9\u611f\u77e5\u7684\u56fe\u5f62\u5e03\u5c40\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u900f\u660e\u53ef\u63a7\u7684\u5e03\u5c40\u8bbe\u8ba1\u3002", "method": "\u5c06\u5e03\u5c40\u8bbe\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u7a7a\u95f4\u73af\u5883\u4e0a\u7684\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u76ee\u6807\u7a7a\u95f4\u8bc4\u4f30\uff08\u51e0\u4f55\u6709\u6548\u6027\u3001\u5173\u7cfb\u4e00\u81f4\u6027\u3001\u7f8e\u5b66\u4e00\u81f4\u6027\uff09\u548c\u76f8\u5bf9\u7ec4\u4f18\u5316\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "LaySPA\u5728\u7ed3\u6784\u6709\u6548\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u4e13\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u4e13\u95e8\u7684SOTA\u5e03\u5c40\u751f\u6210\u5668\u76f8\u5f53\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u6807\u6ce8\u6837\u672c\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "LaySPA\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u5e03\u5c40\u8bbe\u8ba1\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u5c40\u9650\u6027\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u89e3\u91ca\u7684\u56fe\u5f62\u5e03\u5c40\u751f\u6210\u3002"}}
{"id": "2602.13413", "categories": ["cs.LG", "math.NA", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13413", "abs": "https://arxiv.org/abs/2602.13413", "authors": ["Yuchen Fang", "James Demmel", "Javad Lavaei"], "title": "Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise", "comment": null, "summary": "We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \\in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\\mathcal{O}(T^{-\\frac{p-1}{3p-2}})$ when problem parameters are known, and $\\mathcal{O}(T^{-\\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u968f\u673a\u9884\u6761\u4ef6\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SPSGD)\u53ca\u5176\u52a0\u901f\u53d8\u4f53\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u5f52\u4e00\u5316\u65b9\u6cd5\u80fd\u4fdd\u8bc1\u6536\u655b\uff0c\u800c\u88c1\u526a\u65b9\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u53ef\u80fd\u5931\u8d25\uff0c\u8fd9\u89e3\u91ca\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u5f52\u4e00\u5316\u4f18\u4e8e\u88c1\u526a\u7684\u5b9e\u8bc1\u73b0\u8c61\u3002", "motivation": "\u81ea\u9002\u5e94\u65b9\u6cd5\u5982Adam\u3001RMSProp\u548cShampoo\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u7406\u8bba\u4fdd\u8bc1\u4e0d\u8db3\u3002\u9700\u8981\u7406\u89e3\u968f\u673a\u9884\u6761\u4ef6\u4e0b\u4e0d\u540c\u7a33\u5b9a\u5316\u6280\u672f\uff08\u88c1\u526a\u548c\u5f52\u4e00\u5316\uff09\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5f00\u53d1\u4e86\u968f\u673a\u9884\u6761\u4ef6\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SPSGD)\u53ca\u5176\u52a0\u901f\u53d8\u4f53\u7684\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\u7406\u8bba\u3002\u5047\u8bbe\u968f\u673a\u68af\u5ea6\u566a\u58f0\u5177\u6709\u6709\u9650p\u9636\u77e9(p\u2208(1,2])\uff0c\u5206\u6790T\u6b21\u8fed\u4ee3\u540e\u7684\u6536\u655b\u6027\u3002\u5f00\u53d1\u4e86\u65b0\u7684\u5411\u91cf\u503cBurkholder\u578b\u4e0d\u7b49\u5f0f\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u3002", "result": "\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5df2\u77e5\u95ee\u9898\u53c2\u6570\u65f6\u6536\u655b\u7387\u4e3aO(T^{-(p-1)/(3p-2)})\uff0c\u672a\u77e5\u53c2\u6570\u65f6\u4e3aO(T^{-(p-1)/(2p)})\uff0c\u4e0e\u5f52\u4e00\u5316SGD\u7684\u6700\u4f18\u7387\u5339\u914d\u3002\u800c\u88c1\u526a\u65b9\u6cd5\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u53ef\u80fd\u56e0\u968f\u673a\u9884\u6761\u4ef6\u5668\u4e0e\u68af\u5ea6\u4f30\u8ba1\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\u800c\u65e0\u6cd5\u6536\u655b\u3002", "conclusion": "\u5728\u968f\u673a\u9884\u6761\u4ef6\u8bbe\u7f6e\u4e0b\uff0c\u5f52\u4e00\u5316\u6bd4\u88c1\u526a\u5177\u6709\u66f4\u597d\u7684\u6700\u574f\u60c5\u51b5\u7406\u8bba\u4fdd\u8bc1\uff0c\u8fd9\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u5f52\u4e00\u5316\u4f18\u4e8e\u88c1\u526a\u7684\u5b9e\u8bc1\u504f\u597d\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002\u65b0\u5f00\u53d1\u7684\u5411\u91cf\u503cBurkholder\u4e0d\u7b49\u5f0f\u5177\u6709\u72ec\u7acb\u7684\u7406\u8bba\u4ef7\u503c\u3002"}}
{"id": "2602.13416", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13416", "abs": "https://arxiv.org/abs/2602.13416", "authors": ["Haiwen Guan", "Moein Darman", "Dibyajyoti Chakraborty", "Troy Arcomano", "Ashesh Chattopadhyay", "Romit Maulik"], "title": "High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator", "comment": null, "summary": "The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u964d\u5c3a\u5ea6\u6846\u67b6\uff0c\u4f7f\u7528\u6982\u7387\u6269\u6563\u751f\u6210\u6a21\u578b\u5c06\u7c97\u5206\u8fa8\u7387\uff08~300km\uff09\u7684LUCIE\u6c14\u5019\u6a21\u62df\u5668\u8f93\u51fa\u964d\u5c3a\u5ea6\u523025km\u5206\u8fa8\u7387\uff0c\u4ee5\u652f\u6301\u533a\u57df\u6c14\u5019\u5f71\u54cd\u8bc4\u4f30\u3002", "motivation": "\u867d\u7136LUCIE\u6c14\u5019\u6a21\u62df\u5668\u80fd\u591f\u51c6\u786e\u518d\u73b0\u957f\u671f\u6c14\u5019\u7edf\u8ba1\u7279\u5f81\uff0c\u4f46\u5176\u539f\u751f\u5206\u8fa8\u7387\uff08\u7ea6300\u516c\u91cc\uff09\u4e0d\u8db3\u4ee5\u8fdb\u884c\u8be6\u7ec6\u7684\u533a\u57df\u5f71\u54cd\u8bc4\u4f30\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u5206\u8fa8\u7387\u7684\u964d\u5c3a\u5ea6\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6982\u7387\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u6761\u4ef6\u91c7\u6837\u548c\u540e\u9a8c\u91c7\u6837\u6846\u67b6\uff0c\u5c06\u7c97\u5206\u8fa8\u7387\u7684LUCIE\u8f93\u51fa\u964d\u5c3a\u5ea6\u523025\u516c\u91cc\u5206\u8fa8\u7387\u3002\u6a21\u578b\u4f7f\u75282000-2009\u5e74\u7ea614,000\u4e2aERA5\u65f6\u95f4\u6b65\u957f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u57282010-2020\u5e74\u7684LUCIE\u9884\u6d4b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u7eac\u5ea6\u5e73\u5747RMSE\u3001\u529f\u7387\u8c31\u3001\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u548c\u7eac\u5411\u98ce\u7684\u7b2c\u4e00\u7ecf\u9a8c\u6b63\u4ea4\u51fd\u6570\u7b49\u591a\u79cd\u6307\u6807\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301LUCIE\u7c97\u7c92\u5ea6\u52a8\u529b\u5b66\u7279\u5f81\u7684\u540c\u65f6\uff0c\u751f\u6210\u7ea628\u516c\u91cc\u5206\u8fa8\u7387\u7684\u7cbe\u7ec6\u5c3a\u5ea6\u6c14\u5019\u7edf\u8ba1\u7279\u5f81\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u964d\u5c3a\u5ea6\u6846\u67b6\u6210\u529f\u5730\u5c06LUCIE\u6c14\u5019\u6a21\u62df\u5668\u7684\u5206\u8fa8\u7387\u4ece300\u516c\u91cc\u63d0\u5347\u523025\u516c\u91cc\uff0c\u4e3a\u533a\u57df\u6c14\u5019\u5f71\u54cd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u5206\u8fa8\u7387\u7684\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u6a21\u578b\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.14493", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.14493", "abs": "https://arxiv.org/abs/2602.14493", "authors": ["Xinpeng Liu", "Fumio Okura"], "title": "Gaussian Mesh Renderer for Lightweight Differentiable Rendering", "comment": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026). GitHub: https://github.com/huntorochi/Gaussian-Mesh-Renderer", "summary": "3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.", "AI": {"tldr": "\u63d0\u51faGaussian Mesh Renderer (GMR)\uff0c\u5c063D\u9ad8\u65af\u6e85\u5c04\u7684\u9ad8\u6548\u5149\u6805\u5316\u8fc7\u7a0b\u4e0e\u4f20\u7edf\u7f51\u683c\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u53ef\u5fae\u5206\u7f51\u683c\u6e32\u67d3\u5668", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7f51\u683c\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u5668\u5b58\u5728\u4f18\u5316\u901f\u5ea6\u6162\u6216\u8ba1\u7b97\u91cf\u5927\u7684\u95ee\u9898\uff0c\u800c3D\u9ad8\u65af\u6e85\u5c04(3DGS)\u867d\u7136\u6e32\u67d3\u5feb\u4f46\u7f3a\u4e4f\u7f51\u683c\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf", "method": "\u63d0\u51faGaussian Mesh Renderer (GMR)\uff0c\u4ece\u5bf9\u5e94\u7684\u7f51\u683c\u4e09\u89d2\u5f62\u4e2d\u89e3\u6790\u63a8\u5bfc\u51fa\u9ad8\u65af\u57fa\u5143\uff0c\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u5e76\u5b9e\u73b0\u68af\u5ea6\u6d41\u52a8\u3002\u5229\u75283DGS\u7684\u9ad8\u6548\u5149\u6805\u5316\u8fc7\u7a0b", "result": "\u76f8\u6bd4\u4f20\u7edf\u7f51\u683c\u6e32\u67d3\u5668\uff0cGMR\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u68af\u5ea6\uff0c\u7279\u522b\u6709\u52a9\u4e8e\u5728\u5185\u5b58\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u5c0f\u6279\u91cf\u8fdb\u884c\u66f4\u597d\u7684\u4f18\u5316", "conclusion": "GMR\u6210\u529f\u5730\u5c06\u9ad8\u65af\u548c\u7f51\u683c\u8868\u793a\u7d27\u5bc6\u96c6\u6210\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u53ef\u5fae\u5206\u7f51\u683c\u6e32\u67d3\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e863DGS\u7684\u9ad8\u6548\u6e32\u67d3\u548c\u7f51\u683c\u7684\u7ed3\u6784\u4f18\u52bf"}}
{"id": "2602.13418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13418", "abs": "https://arxiv.org/abs/2602.13418", "authors": ["Karish Grover", "Hanqing Zeng", "Yinglong Xia", "Christos Faloutsos", "Geoffrey J. Gordon"], "title": "Text Has Curvature", "comment": null, "summary": "Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faTexture\uff0c\u4e00\u79cd\u6587\u672c\u539f\u751f\u7684\u79bb\u6563\u66f2\u7387\u4fe1\u53f7\uff0c\u8bc1\u660e\u8bed\u8a00\u5177\u6709\u5185\u5728\u66f2\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u8bed\u8a00\u5efa\u6a21\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5f2f\u66f2\u51e0\u4f55\uff08\u5982\u7528\u4e8e\u5c42\u6b21\u7ed3\u6784\u7684\u53cc\u66f2\u7a7a\u95f4\u3001\u7528\u4e8e\u7ec4\u5408\u7ed3\u6784\u7684\u6df7\u5408\u66f2\u7387\u6d41\u5f62\uff09\uff0c\u4f46\u4e00\u4e2a\u57fa\u672c\u79d1\u5b66\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff1a\u66f2\u7387\u5bf9\u6587\u672c\u672c\u8eab\u610f\u5473\u7740\u4ec0\u4e48\uff1f\u9700\u8981\u627e\u5230\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u5d4c\u5165\u7a7a\u95f4\u9009\u62e9\u3001\u771f\u6b63\u5c5e\u4e8e\u8bed\u8a00\u672c\u8eab\u7684\u66f2\u7387\u5b9a\u4e49\u3002", "method": "\u63d0\u51faTexture\uff0c\u4e00\u79cd\u6587\u672c\u539f\u751f\u7684\u8bcd\u7ea7\u79bb\u6563\u66f2\u7387\u4fe1\u53f7\u3002\u901a\u8fc7\u8c03\u548c\u63a9\u7801\u8bcd\u5de6\u53f3\u4e0a\u4e0b\u6587\u4fe1\u5ff5\u7684Schr\u00f6dinger\u6865\u5b9a\u4e49\u66f2\u7387\u573a\uff1a\u6b63\u66f2\u7387\u8868\u793a\u4e0a\u4e0b\u6587\u805a\u7126\u610f\u4e49\uff0c\u8d1f\u66f2\u7387\u8868\u793a\u4e0a\u4e0b\u6587\u53d1\u6563\u4e3a\u7ade\u4e89\u6027\u5ef6\u7eed\u3002\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7ecf\u9a8c\u8bc1\u660e\u8bed\u8a00\u5177\u6709\u56fa\u6709\u66f2\u7387\u3002", "result": "\u5efa\u7acb\u4e86\u6587\u672c\u539f\u751f\u66f2\u7387\u8303\u5f0f\uff0c\u4f7f\u66f2\u7387\u53ef\u6d4b\u91cf\u4e14\u5b9e\u7528\u3002\u5728\u4ee3\u8868\u6027\u4efb\u52a1\u4e2d\u5b9e\u4f8b\u5316Texture\uff1a\u901a\u8fc7\u66f2\u7387\u5f15\u5bfc\u7684\u538b\u7f29\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u901a\u8fc7\u66f2\u7387\u5f15\u5bfc\u7684\u8def\u7531\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "conclusion": "\u6587\u672c\u786e\u5b9e\u5177\u6709\u66f2\u7387\uff0cTexture\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u6d4b\u91cf\u548c\u63a7\u5236\u539f\u8bed\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u51e0\u4f55\u8bad\u7ec3\u7684\u51e0\u4f55\u65b9\u6cd5\uff0c\u4f7f\u66f2\u7387\u5728\u8bed\u8a00\u5904\u7406\u4e2d\u53d8\u5f97\u53ef\u6d4b\u91cf\u548c\u5b9e\u7528\u3002"}}
{"id": "2602.13730", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2602.13730", "abs": "https://arxiv.org/abs/2602.13730", "authors": ["Joshua Hutchinson", "J. Michael Herrmann", "Sim\u00f3n C. Smith"], "title": "Discrete Gene Crossover Accelerates Solution Discovery in Quality-Diversity Algorithms", "comment": "11 pages, 6 figures, submitted to GECCO 2026", "summary": "Quality-Diversity (QD) algorithms aim to discover diverse, high-performing solutions across behavioral niches. However, QD search often stagnates as incremental variation operators struggle to propagate building blocks across large populations. Existing mutation operators rely on gradual variation to solutions, limiting their ability to efficiently explore regions of the search space distant from parent solutions or to spread beneficial genetic material through the population. We propose a mutation operator which augments variation-based operators with discrete, gene-level crossover, enabling rapid recombination of elite genetic material. This crossover mechanism mirrors the biological principle of meiosis and facilitates both the direct transfer of genetic material and the exploration of novel genotype configurations beyond the existing elite hypervolume. We evaluate operators on three locomotion environments, demonstrating improvements in QD score, coverage, and max fitness, with particularly strong performance in later stages of optimization once building blocks have been established in the archive. These results show that the addition of a discrete crossover mutation provides a complementary exploration mechanism that sustains quality-diversity growth beyond the performance demonstrated by existing operators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u57fa\u56e0\u7ea7\u4ea4\u53c9\u7684\u53d8\u5f02\u7b97\u5b50\uff0c\u7528\u4e8e\u589e\u5f3a\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u7684\u641c\u7d22\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4f18\u5316\u540e\u671f\u9636\u6bb5\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u5728\u641c\u7d22\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u505c\u6ede\uff0c\u56e0\u4e3a\u589e\u91cf\u53d8\u5f02\u7b97\u5b50\u96be\u4ee5\u5728\u5927\u578b\u79cd\u7fa4\u4e2d\u4f20\u64ad\u6784\u5efa\u5757\u3002\u73b0\u6709\u7684\u53d8\u5f02\u7b97\u5b50\u4f9d\u8d56\u6e10\u8fdb\u5f0f\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6709\u6548\u63a2\u7d22\u8fdc\u79bb\u7236\u4ee3\u89e3\u7684\u641c\u7d22\u7a7a\u95f4\u533a\u57df\u6216\u4f20\u64ad\u6709\u76ca\u9057\u4f20\u7269\u8d28\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d8\u5f02\u7b97\u5b50\uff0c\u901a\u8fc7\u79bb\u6563\u7684\u57fa\u56e0\u7ea7\u4ea4\u53c9\u589e\u5f3a\u57fa\u4e8e\u53d8\u5f02\u7684\u7b97\u5b50\uff0c\u5b9e\u73b0\u7cbe\u82f1\u9057\u4f20\u7269\u8d28\u7684\u5feb\u901f\u91cd\u7ec4\u3002\u8fd9\u79cd\u4ea4\u53c9\u673a\u5236\u6a21\u62df\u4e86\u751f\u7269\u5b66\u7684\u51cf\u6570\u5206\u88c2\u539f\u7406\uff0c\u4fc3\u8fdb\u9057\u4f20\u7269\u8d28\u7684\u76f4\u63a5\u8f6c\u79fb\u548c\u8d85\u8d8a\u73b0\u6709\u7cbe\u82f1\u8d85\u4f53\u79ef\u7684\u65b0\u57fa\u56e0\u578b\u914d\u7f6e\u63a2\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u8fd0\u52a8\u73af\u5883\u4e2d\u8bc4\u4f30\u7b97\u5b50\uff0c\u7ed3\u679c\u663e\u793a\u5728QD\u5206\u6570\u3001\u8986\u76d6\u7387\u548c\u6700\u5927\u9002\u5e94\u5ea6\u65b9\u9762\u5747\u6709\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u4f18\u5316\u540e\u671f\u9636\u6bb5\uff0c\u4e00\u65e6\u6784\u5efa\u5757\u5728\u5b58\u6863\u4e2d\u5efa\u7acb\uff0c\u8868\u73b0\u51fa\u7279\u522b\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "\u6dfb\u52a0\u79bb\u6563\u4ea4\u53c9\u53d8\u5f02\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e92\u8865\u7684\u63a2\u7d22\u673a\u5236\uff0c\u80fd\u591f\u6301\u7eed\u63a8\u52a8\u8d28\u91cf\u591a\u6837\u6027\u589e\u957f\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7b97\u5b50\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.13596", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13596", "abs": "https://arxiv.org/abs/2602.13596", "authors": ["Zhe Ye", "Xiangui Kang", "Jiayi He", "Chengxin Chen", "Wei Zhu", "Kai Wu", "Yin Yang", "Jiwu Huang"], "title": "BreathNet: Generalizable Audio Deepfake Detection via Breath-Cue-Guided Feature Refinement", "comment": "Under Review", "summary": "As deepfake audio becomes more realistic and diverse, developing generalizable countermeasure systems has become crucial. Existing detection methods primarily depend on XLS-R front-end features to improve generalization. Nonetheless, their performance remains limited, partly due to insufficient attention to fine-grained information, such as physiological cues or frequency-domain features. In this paper, we propose BreathNet, a novel audio deepfake detection framework that integrates fine-grained breath information to improve generalization. Specifically, we design BreathFiLM, a feature-wise linear modulation mechanism that selectively amplifies temporal representations based on the presence of breathing sounds. BreathFiLM is trained jointly with the XLS-R extractor, in turn encouraging the extractor to learn and encode breath-related cues into the temporal features. Then, we use the frequency front-end to extract spectral features, which are then fused with temporal features to provide complementary information introduced by vocoders or compression artifacts. Additionally, we propose a group of feature losses comprising Positive-only Supervised Contrastive Loss (PSCL), center loss, and contrast loss. These losses jointly enhance the discriminative ability, encouraging the model to separate bona fide and deepfake samples more effectively in the feature space. Extensive experiments on five benchmark datasets demonstrate state-of-the-art (SOTA) performance. Using the ASVspoof 2019 LA training set, our method attains 1.99% average EER across four related eval benchmarks, with particularly strong performance on the In-the-Wild dataset, where it achieves 4.70% EER. Moreover, under the ASVspoof5 evaluation protocol, our method achieves an EER of 4.94% on this latest benchmark.", "AI": {"tldr": "BreathNet\uff1a\u4e00\u79cd\u96c6\u6210\u7ec6\u7c92\u5ea6\u547c\u5438\u4fe1\u606f\u7684\u65b0\u578b\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7BreathFiLM\u673a\u5236\u9009\u62e9\u6027\u589e\u5f3a\u547c\u5438\u58f0\u97f3\u76f8\u5173\u7684\u65f6\u5e8f\u8868\u5f81\uff0c\u7ed3\u5408\u9891\u8c31\u7279\u5f81\u878d\u5408\u548c\u591a\u7279\u5f81\u635f\u5931\u51fd\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u53d8\u5f97\u66f4\u52a0\u903c\u771f\u548c\u591a\u6837\u5316\uff0c\u5f00\u53d1\u6cdb\u5316\u6027\u5f3a\u7684\u68c0\u6d4b\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56XLS-R\u524d\u7aef\u7279\u5f81\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u6027\u80fd\u4ecd\u7136\u6709\u9650\uff0c\u90e8\u5206\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u751f\u7406\u7ebf\u7d22\u6216\u9891\u57df\u7279\u5f81\u7b49\u7ec6\u7c92\u5ea6\u4fe1\u606f\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51faBreathNet\u6846\u67b6\uff1a1\uff09\u8bbe\u8ba1BreathFiLM\u7279\u5f81\u7ebf\u6027\u8c03\u5236\u673a\u5236\uff0c\u57fa\u4e8e\u547c\u5438\u58f0\u5b58\u5728\u9009\u62e9\u6027\u589e\u5f3a\u65f6\u5e8f\u8868\u5f81\uff1b2\uff09\u7ed3\u5408\u9891\u7387\u524d\u7aef\u63d0\u53d6\u9891\u8c31\u7279\u5f81\uff0c\u4e0e\u65f6\u5e8f\u7279\u5f81\u878d\u5408\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\uff1b3\uff09\u63d0\u51fa\u5305\u542b\u6b63\u6837\u672c\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u3001\u4e2d\u5fc3\u635f\u5931\u548c\u5bf9\u6bd4\u635f\u5931\u7684\u7279\u5f81\u635f\u5931\u7ec4\uff0c\u589e\u5f3a\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff1a\u4f7f\u7528ASVspoof 2019 LA\u8bad\u7ec3\u96c6\uff0c\u5728\u56db\u4e2a\u76f8\u5173\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5e73\u5747EER\u4e3a1.99%\uff0c\u5728In-the-Wild\u6570\u636e\u96c6\u4e0aEER\u4e3a4.70%\uff1b\u5728ASVspoof5\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0c\u6700\u65b0\u57fa\u51c6\u4e0aEER\u4e3a4.94%\u3002", "conclusion": "BreathNet\u901a\u8fc7\u6574\u5408\u7ec6\u7c92\u5ea6\u547c\u5438\u4fe1\u606f\u548c\u591a\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4e3a\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13291", "categories": ["cs.MA", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13291", "abs": "https://arxiv.org/abs/2602.13291", "authors": ["Ziyang Wang"], "title": "Agent Mars: Multi-Agent Simulation for Multi-Planetary Life Exploration and Settlement", "comment": null, "summary": "Artificial Intelligence (AI) has transformed robotics, healthcare, industry, and scientific discovery, yet a major frontier may lie beyond Earth. Space exploration and settlement offer vast environments and resources, but impose constraints unmatched on Earth: delayed/intermittent communications, extreme resource scarcity, heterogeneous expertise, and strict safety, accountability, and command authority. The key challenge is auditable coordination among specialised humans, robots, and digital services in a safety-critical system-of-systems. We introduce Agent Mars, an open, end-to-end multi-agent simulation framework for Mars base operations. Agent Mars formalises a realistic organisation with a 93-agent roster across seven layers of command and execution (human roles and physical assets), enabling base-scale studies beyond toy settings. It implements hierarchical and cross-layer coordination that preserves chain-of-command while allowing vetted cross-layer exchanges with audit trails; supports dynamic role handover with automatic failover under outages; and enables phase-dependent leadership for routine operations, emergencies, and science campaigns. Agent Mars further models mission-critical mechanisms-scenario-aware short/long-horizon memory, configurable propose-vote consensus, and translator-mediated heterogeneous protocols-to capture how teams align under stress. To quantify behaviour, we propose the Agent Mars Performance Index (AMPI), an interpretable composite score with diagnostic sub-metrics. Across 13 reproducible Mars-relevant operational scripts, Agent Mars reveals coordination trade-offs and identifies regimes where curated cross-layer collaboration and functional leadership reduce overhead without sacrificing reliability. Agent Mars provides a benchmarkable, auditable foundation for Space AI.", "AI": {"tldr": "Agent Mars\u662f\u4e00\u4e2a\u7528\u4e8e\u706b\u661f\u57fa\u5730\u64cd\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u4eff\u771f\u6846\u67b6\uff0c\u5305\u542b93\u4e2a\u667a\u80fd\u4f53\u30017\u5c42\u6307\u6325\u6267\u884c\u5c42\u7ea7\uff0c\u652f\u6301\u5206\u5c42\u534f\u8c03\u3001\u52a8\u6001\u89d2\u8272\u4ea4\u63a5\u548c\u9636\u6bb5\u4f9d\u8d56\u9886\u5bfc\uff0c\u65e8\u5728\u89e3\u51b3\u592a\u7a7a\u63a2\u7d22\u4e2d\u7684\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u534f\u8c03\u6311\u6218\u3002", "motivation": "\u592a\u7a7a\u63a2\u7d22\u548c\u5b9a\u5c45\u9762\u4e34\u5730\u7403\u73af\u5883\u65e0\u6cd5\u6bd4\u62df\u7684\u7ea6\u675f\uff1a\u5ef6\u8fdf/\u95f4\u6b47\u901a\u4fe1\u3001\u6781\u7aef\u8d44\u6e90\u7a00\u7f3a\u3001\u5f02\u6784\u4e13\u4e1a\u77e5\u8bc6\u3001\u4e25\u683c\u7684\u5b89\u5168\u6027\u548c\u6307\u6325\u6743\u5a01\u8981\u6c42\u3002\u6838\u5fc3\u6311\u6218\u662f\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u3001\u673a\u5668\u4eba\u548c\u6570\u5b57\u670d\u52a1\u4e4b\u95f4\u7684\u53ef\u5ba1\u8ba1\u534f\u8c03\u3002", "method": "\u5f00\u53d1\u4e86Agent Mars\u6846\u67b6\uff0c\u5305\u542b93\u4e2a\u667a\u80fd\u4f53\u76847\u5c42\u6307\u6325\u6267\u884c\u7ec4\u7ec7\u7ed3\u6784\uff0c\u5b9e\u73b0\u5206\u5c42\u548c\u8de8\u5c42\u534f\u8c03\u673a\u5236\uff0c\u652f\u6301\u52a8\u6001\u89d2\u8272\u4ea4\u63a5\u548c\u6545\u969c\u8f6c\u79fb\uff0c\u91c7\u7528\u573a\u666f\u611f\u77e5\u8bb0\u5fc6\u3001\u53ef\u914d\u7f6e\u6295\u7968\u5171\u8bc6\u548c\u534f\u8bae\u7ffb\u8bd1\u7b49\u5173\u952e\u673a\u5236\u3002", "result": "\u901a\u8fc713\u4e2a\u53ef\u590d\u73b0\u7684\u706b\u661f\u76f8\u5173\u64cd\u4f5c\u811a\u672c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u534f\u8c03\u6743\u8861\uff0c\u5e76\u8bc6\u522b\u51fa\u7ecf\u8fc7\u7b56\u5212\u7684\u8de8\u5c42\u534f\u4f5c\u548c\u529f\u80fd\u9886\u5bfc\u80fd\u591f\u5728\u4e0d\u727a\u7272\u53ef\u9760\u6027\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u5f00\u9500\u7684\u673a\u5236\u3002", "conclusion": "Agent Mars\u4e3a\u592a\u7a7aAI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u57fa\u51c6\u6d4b\u8bd5\u3001\u53ef\u5ba1\u8ba1\u7684\u57fa\u7840\u6846\u67b6\uff0c\u80fd\u591f\u7814\u7a76\u5927\u89c4\u6a21\u57fa\u5730\u64cd\u4f5c\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u8d85\u8d8a\u4e86\u73a9\u5177\u8bbe\u7f6e\u7684\u9650\u5236\u3002"}}
{"id": "2602.13422", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.13422", "abs": "https://arxiv.org/abs/2602.13422", "authors": ["Yuxi Liu", "Junqiang Peng", "Mingyu Xiao"], "title": "The Complexity of Tournament Fixing: Subset FAS Number and Acyclic Neighborhoods", "comment": null, "summary": "The \\textsc{Tournament Fixing Problem} (TFP) asks whether a knockout tournament can be scheduled to guarantee that a given player $v^*$ wins. Although TFP is NP-hard in general, it is known to be \\emph{fixed-parameter tractable} (FPT) when parameterized by the feedback arc/vertex set number, or the in/out-degree of $v^*$ (AAAI 17; IJCAI 18; AAAI 23; AAAI 26). However, it remained open whether TFP is FPT with respect to the \\emph{subset FAS number of $v^*$} -- the minimum number of arcs intersecting all cycles containing $v^*$ -- a parameter that is never larger than the aforementioned ones (AAAI 26). In this paper, we resolve this question negatively by proving that TFP stays NP-hard even when the subset FAS number of $v^*$ is constant $\\geq 1$ and either the subgraph induced by the in-neighbors $D[N_{\\mathrm{in}}(v^*)]$ or the out-neighbors $D[N_{\\mathrm{out}}(v^*)]$ is acyclic. Conversely, when both $D[N_{\\mathrm{in}}(v^*)]$ and $D[N_{\\mathrm{out}}(v^*)]$ are acyclic, we show that TFP becomes FPT parameterized by the subset FAS number of $v^*$. Furthermore, we provide sufficient conditions under which $v^*$ can win even when this parameter is unbounded.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u9526\u6807\u8d5b\u5b89\u6392\u95ee\u9898(TFP)\u5173\u4e8e\u5b50\u96c6FAS\u6570\u7684\u53c2\u6570\u5316\u590d\u6742\u6027\uff1a\u8bc1\u660e\u4e86\u5373\u4f7f\u5b50\u96c6FAS\u6570\u4e3a\u5e38\u6570\u4e14\u5165\u90bb\u57df\u6216\u51fa\u90bb\u57df\u662f\u65e0\u73af\u7684\uff0cTFP\u4ecd\u4e3aNP\u96be\uff1b\u4f46\u5f53\u4e24\u8005\u90fd\u65e0\u73af\u65f6\uff0cTFP\u5173\u4e8e\u5b50\u96c6FAS\u6570\u662fFPT\u7684\u3002", "motivation": "\u867d\u7136\u5df2\u77e5TFP\u5728\u67d0\u4e9b\u53c2\u6570\uff08\u5982\u53cd\u9988\u5f27/\u9876\u70b9\u96c6\u6570\u3001\u5165/\u51fa\u5ea6\uff09\u4e0b\u662f\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\u7684\uff0c\u4f46\u5bf9\u4e8e\u5b50\u96c6FAS\u6570\u8fd9\u4e00\u66f4\u5c0f\u7684\u53c2\u6570\uff0c\u5176\u53c2\u6570\u5316\u590d\u6742\u6027\u4e00\u76f4\u672a\u89e3\u51b3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6784\u9020\u6027\u8bc1\u660e\uff1a1) \u8bc1\u660e\u5373\u4f7f\u5b50\u96c6FAS\u6570\u4e3a\u5e38\u6570\u4e14\u5165\u90bb\u57df\u6216\u51fa\u90bb\u57df\u65e0\u73af\uff0cTFP\u4ecd\u4e3aNP\u96be\uff1b2) \u8bc1\u660e\u5f53\u5165\u90bb\u57df\u548c\u51fa\u90bb\u57df\u90fd\u65e0\u73af\u65f6\uff0cTFP\u5173\u4e8e\u5b50\u96c6FAS\u6570\u662fFPT\u7684\uff1b3) \u63d0\u4f9b\u5728\u53c2\u6570\u65e0\u754c\u65f6v*\u4ecd\u80fd\u83b7\u80dc\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "1) TFP\u5728\u5b50\u96c6FAS\u6570\u4e3a\u5e38\u6570\u4e14\u5165\u90bb\u57df\u6216\u51fa\u90bb\u57df\u65e0\u73af\u65f6\u662fNP\u96be\u7684\uff1b2) \u5f53\u5165\u90bb\u57df\u548c\u51fa\u90bb\u57df\u90fd\u65e0\u73af\u65f6\uff0cTFP\u5173\u4e8e\u5b50\u96c6FAS\u6570\u662fFPT\u7684\uff1b3) \u786e\u5b9a\u4e86\u53c2\u6570\u65e0\u754c\u65f6v*\u4ecd\u80fd\u83b7\u80dc\u7684\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u5b8c\u6574\u523b\u753b\u4e86TFP\u5173\u4e8e\u5b50\u96c6FAS\u6570\u7684\u53c2\u6570\u5316\u590d\u6742\u6027\uff1a\u5728\u5165\u90bb\u57df\u6216\u51fa\u90bb\u57df\u65e0\u73af\u7684\u8f83\u5f31\u6761\u4ef6\u4e0b\u4ecd\u4e3aNP\u96be\uff0c\u4f46\u5728\u4e24\u8005\u90fd\u65e0\u73af\u7684\u8f83\u5f3a\u6761\u4ef6\u4e0b\u662fFPT\u7684\uff0c\u4e3a\u8fd9\u4e00\u957f\u671f\u5f00\u653e\u95ee\u9898\u63d0\u4f9b\u4e86\u6700\u7ec8\u7b54\u6848\u3002"}}
{"id": "2602.13482", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13482", "abs": "https://arxiv.org/abs/2602.13482", "authors": ["Sadra Sabouri", "Alireza Zolanvari", "Sepand Haghighi"], "title": "Comparing Classifiers: A Case Study Using PyCM", "comment": "13 pages, 3 figures, 2 tables", "summary": "Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.", "AI": {"tldr": "PyCM\u5e93\u6559\u7a0b\uff1a\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u6df1\u5165\u5206\u6790\u591a\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u5f3a\u8c03\u6807\u51c6\u6307\u6807\u53ef\u80fd\u5ffd\u7565\u7ec6\u5fae\u6027\u80fd\u5dee\u5f02", "motivation": "\u9009\u62e9\u6700\u4f18\u5206\u7c7b\u6a21\u578b\u9700\u8981\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u5168\u9762\u6df1\u5165\u7684\u7406\u89e3\uff0c\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u6027\u80fd\u7684\u7ec6\u5fae\u5dee\u5f02\u548c\u6743\u8861", "method": "\u4f7f\u7528PyCM\u5e93\u8fdb\u884c\u591a\u5206\u7c7b\u5668\u7684\u6df1\u5ea6\u8bc4\u4f30\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e0d\u540c\u6848\u4f8b\u573a\u666f\u5c55\u793a\u8bc4\u4f30\u6307\u6807\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6548\u80fd\u89e3\u91ca", "result": "\u8bc4\u4f30\u6307\u6807\u7684\u9009\u62e9\u4f1a\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5bf9\u6a21\u578b\u6548\u80fd\u7684\u89e3\u91ca\uff0c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u5bf9\u4e8e\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u4e2d\u5fae\u5c0f\u4f46\u91cd\u8981\u7684\u5dee\u5f02\u81f3\u5173\u91cd\u8981", "conclusion": "\u9700\u8981\u91c7\u7528\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u6765\u5168\u9762\u7406\u89e3\u5206\u7c7b\u6a21\u578b\u6027\u80fd\uff0c\u6807\u51c6\u6307\u6807\u53ef\u80fd\u9519\u8fc7\u7ec6\u5fae\u7684\u6027\u80fd\u6743\u8861\uff0cPyCM\u5e93\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177"}}
{"id": "2602.13309", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13309", "abs": "https://arxiv.org/abs/2602.13309", "authors": ["Yexin Li", "Jinjin Guo", "Haoyu Zhang", "Yuhan Zhao", "Yiwen Sun", "Zihao Jiao"], "title": "Adaptive Value Decomposition: Coordinating a Varying Number of Agents in Urban Systems", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) provides a promising paradigm for coordinating multi-agent systems (MAS). However, most existing methods rely on restrictive assumptions, such as a fixed number of agents and fully synchronous action execution. These assumptions are often violated in urban systems, where the number of active agents varies over time, and actions may have heterogeneous durations, resulting in a semi-MARL setting. Moreover, while sharing policy parameters among agents is commonly adopted to improve learning efficiency, it can lead to highly homogeneous actions when a subset of agents make decisions concurrently under similar observations, potentially degrading coordination quality. To address these challenges, we propose Adaptive Value Decomposition (AVD), a cooperative MARL framework that adapts to a dynamically changing agent population. AVD further incorporates a lightweight mechanism to mitigate action homogenization induced by shared policies, thereby encouraging behavioral diversity and maintaining effective cooperation among agents. In addition, we design a training-execution strategy tailored to the semi-MARL setting that accommodates asynchronous decision-making when some agents act at different times. Experiments on real-world bike-sharing redistribution tasks in two major cities, London and Washington, D.C., demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability.", "AI": {"tldr": "AVD\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u4ef7\u503c\u5206\u89e3\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u52a8\u6001\u53d8\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u673a\u5236\u7f13\u89e3\u5171\u4eab\u7b56\u7565\u5bfc\u81f4\u7684\u884c\u52a8\u540c\u8d28\u5316\u95ee\u9898\uff0c\u5e76\u5728\u534aMARL\u8bbe\u7f6e\u4e2d\u652f\u6301\u5f02\u6b65\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u667a\u80fd\u4f53\u6570\u91cf\u548c\u5b8c\u5168\u540c\u6b65\u884c\u52a8\u6267\u884c\u7684\u9650\u5236\u6027\u5047\u8bbe\uff0c\u8fd9\u5728\u57ce\u5e02\u7cfb\u7edf\u4e2d\u5e38\u5e38\u88ab\u8fdd\u53cd\u3002\u540c\u65f6\uff0c\u5171\u4eab\u7b56\u7565\u53c2\u6570\u867d\u7136\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff0c\u4f46\u5728\u76f8\u4f3c\u89c2\u6d4b\u4e0b\u53ef\u80fd\u5bfc\u81f4\u884c\u52a8\u540c\u8d28\u5316\uff0c\u964d\u4f4e\u534f\u8c03\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u4ef7\u503c\u5206\u89e3\uff08AVD\uff09\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u9002\u5e94\u52a8\u6001\u53d8\u5316\u667a\u80fd\u4f53\u6570\u91cf\u7684\u673a\u5236\uff1b2\uff09\u8f7b\u91cf\u7ea7\u673a\u5236\u7f13\u89e3\u5171\u4eab\u7b56\u7565\u5bfc\u81f4\u7684\u884c\u52a8\u540c\u8d28\u5316\uff1b3\uff09\u9488\u5bf9\u534aMARL\u8bbe\u7f6e\u7684\u8bad\u7ec3-\u6267\u884c\u7b56\u7565\uff0c\u652f\u6301\u5f02\u6b65\u51b3\u7b56\u3002", "result": "\u5728\u4f26\u6566\u548c\u534e\u76db\u987f\u7279\u533a\u7684\u771f\u5b9e\u4e16\u754c\u5171\u4eab\u5355\u8f66\u518d\u5206\u914d\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cAVD\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AVD\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4ef7\u503c\u5206\u89e3\u548c\u884c\u52a8\u591a\u6837\u5316\u673a\u5236\uff0c\u5728\u534aMARL\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u5408\u4f5c\u3002"}}
{"id": "2602.13896", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13896", "abs": "https://arxiv.org/abs/2602.13896", "authors": ["Naoki Hashima", "Hikaru Hoshino", "Luis David Pab\u00f3n Ospina", "Eiko Furutani"], "title": "Probabilistic Reachability Analysis of Multi-scale Voltage Dynamics Using Reinforcement Learning", "comment": null, "summary": "Voltage stability in modern power systems involves coupled dynamics across multiple time scales. Conventional methods based on time-scale separation or static stability margins may overlook instabilities caused by the coupling of slow and fast transients. Uncertainty in operating conditions further complicates stability assessment, and high computational cost of Monte Carlo simulations limit its applicability to multi-scale dynamics. This paper presents a deep reinforcement learning-based framework for probabilistic reachability analysis of multi-scale voltage dynamics. By formulating each instability mechanism as a distinct absorbing state and introducing a multi-critic architecture for mechanism-specific learning, the proposed method enables consistent learning of risk probabilities associated with multiple instability types within a unified framework. The approach is demonstrated on a four-bus system with load tap changers and over-excitation limiters, illustrating effectiveness of the proposed learning-based reachability analysis in identifying and quantifying the mechanisms leading to voltage collapse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u65f6\u95f4\u5c3a\u5ea6\u7535\u538b\u52a8\u6001\u7684\u7a33\u5b9a\u6027\u98ce\u9669\uff0c\u80fd\u591f\u540c\u65f6\u91cf\u5316\u591a\u79cd\u5931\u7a33\u673a\u5236\u7684\u6982\u7387\u3002", "motivation": "\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u7684\u7535\u538b\u7a33\u5b9a\u6027\u6d89\u53ca\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u7684\u8026\u5408\u52a8\u6001\u3002\u4f20\u7edf\u65b9\u6cd5\u57fa\u4e8e\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u6216\u9759\u6001\u7a33\u5b9a\u88d5\u5ea6\uff0c\u53ef\u80fd\u5ffd\u7565\u7531\u6162\u901f\u548c\u5feb\u901f\u6682\u6001\u8026\u5408\u5f15\u8d77\u7684\u5931\u7a33\u3002\u8fd0\u884c\u6761\u4ef6\u7684\u4e0d\u786e\u5b9a\u6027\u8fdb\u4e00\u6b65\u4f7f\u7a33\u5b9a\u6027\u8bc4\u4f30\u590d\u6742\u5316\uff0c\u800c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5728\u591a\u5c3a\u5ea6\u52a8\u6001\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u6846\u67b6\uff0c\u5c06\u6bcf\u79cd\u5931\u7a33\u673a\u5236\u5efa\u6a21\u4e3a\u4e0d\u540c\u7684\u5438\u6536\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u591a\u6279\u8bc4\u5bb6\u67b6\u6784\u8fdb\u884c\u673a\u5236\u7279\u5b9a\u7684\u5b66\u4e60\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u7edf\u4e00\u6846\u67b6\u5185\u4e00\u81f4\u5730\u5b66\u4e60\u4e0e\u591a\u79cd\u5931\u7a33\u7c7b\u578b\u76f8\u5173\u7684\u98ce\u9669\u6982\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5e26\u6709\u8d1f\u8f7d\u5206\u63a5\u5934\u53d8\u6362\u5668\u548c\u8fc7\u52b1\u78c1\u9650\u5236\u5668\u7684\u56db\u6bcd\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u53ef\u8fbe\u6027\u5206\u6790\u5728\u8bc6\u522b\u548c\u91cf\u5316\u5bfc\u81f4\u7535\u538b\u5d29\u6e83\u7684\u673a\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u65f6\u95f4\u5c3a\u5ea6\u7535\u538b\u52a8\u6001\u7684\u7a33\u5b9a\u6027\u8bc4\u4f30\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7535\u529b\u7cfb\u7edf\u7a33\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2602.13451", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.13451", "abs": "https://arxiv.org/abs/2602.13451", "authors": ["Natalie Collina", "Surbhi Goel", "Aaron Roth", "Mirah Shi"], "title": "Personalization Aids Pluralistic Alignment Under Competition", "comment": null, "summary": "Can competition among misaligned AI providers yield aligned outcomes for a diverse population of users, and what role does model personalization play? We study a setting where multiple competing AI providers interact with multiple users who must make downstream decisions but differ in preferences. Providers have their own objectives over users' actions and strategically deploy AI models to advance them. We model the interaction as a Stackelberg game with multiple leaders (providers) and followers (users): providers commit to conversational policies, and users choose which model to use, how to converse, and how to act. With user-specific personalization, we show that under a Weak Market Alignment condition, every equilibrium gives each user outcomes comparable to those from a perfectly aligned common model -- so personalization can induce pluralistically aligned outcomes, even when providers are self-interested. In contrast, when providers must deploy a single anonymous policy, there exist equilibria with uninformative behavior under the same condition. We then give a stronger alignment condition that guarantees each user their optimal utility in the anonymous setting.", "AI": {"tldr": "\u7814\u7a76\u7ade\u4e89\u6027AI\u63d0\u4f9b\u5546\u4e0e\u591a\u6837\u5316\u7528\u6237\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u63a2\u8ba8\u4e2a\u6027\u5316\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u5bf9\u9f50\u7ed3\u679c\u3002\u5728\u4e2a\u6027\u5316\u8bbe\u7f6e\u4e0b\uff0c\u5373\u4f7f\u63d0\u4f9b\u5546\u81ea\u5229\uff0c\u4e5f\u80fd\u5b9e\u73b0\u7c7b\u4f3c\u5b8c\u7f8e\u5bf9\u9f50\u7684\u7ed3\u679c\uff1b\u800c\u5728\u533f\u540d\u7b56\u7565\u4e0b\uff0c\u53ef\u80fd\u51fa\u73b0\u65e0\u4fe1\u606f\u5747\u8861\u3002", "motivation": "\u7814\u7a76\u5f53AI\u63d0\u4f9b\u5546\u4e0e\u7528\u6237\u504f\u597d\u4e0d\u4e00\u81f4\u65f6\uff0c\u5e02\u573a\u7ade\u4e89\u80fd\u5426\u4ea7\u751f\u5bf9\u9f50\u7ed3\u679c\uff0c\u4ee5\u53ca\u6a21\u578b\u4e2a\u6027\u5316\u5728\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002\u5173\u6ce8\u81ea\u5229\u63d0\u4f9b\u5546\u5982\u4f55\u901a\u8fc7\u6218\u7565\u90e8\u7f72\u6a21\u578b\u5f71\u54cd\u7528\u6237\u51b3\u7b56\uff0c\u4ee5\u53ca\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u5747\u8861\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u591a\u9886\u5bfc\u8005\uff08\u63d0\u4f9b\u5546\uff09\u548c\u591a\u8ffd\u968f\u8005\uff08\u7528\u6237\uff09\u7684Stackelberg\u535a\u5f08\u6a21\u578b\u3002\u63d0\u4f9b\u5546\u627f\u8bfa\u5bf9\u8bdd\u7b56\u7565\uff0c\u7528\u6237\u9009\u62e9\u4f7f\u7528\u54ea\u4e2a\u6a21\u578b\u3001\u5982\u4f55\u5bf9\u8bdd\u4ee5\u53ca\u5982\u4f55\u884c\u52a8\u3002\u5206\u6790\u4e24\u79cd\u8bbe\u7f6e\uff1a\u7528\u6237\u7279\u5b9a\u4e2a\u6027\u5316\u6a21\u578b\u548c\u5355\u4e00\u533f\u540d\u7b56\u7565\u3002", "result": "\u5728\u4e2a\u6027\u5316\u8bbe\u7f6e\u4e0b\uff0c\u5f53\u6ee1\u8db3\"\u5f31\u5e02\u573a\u5bf9\u9f50\"\u6761\u4ef6\u65f6\uff0c\u6bcf\u4e2a\u5747\u8861\u90fd\u80fd\u7ed9\u7528\u6237\u5e26\u6765\u4e0e\u5b8c\u7f8e\u5bf9\u9f50\u901a\u7528\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u5728\u533f\u540d\u7b56\u7565\u8bbe\u7f6e\u4e0b\uff0c\u540c\u4e00\u6761\u4ef6\u4e0b\u53ef\u80fd\u5b58\u5728\u65e0\u4fe1\u606f\u5747\u8861\u3002\u63d0\u51fa\u4e86\u66f4\u5f3a\u7684\u5bf9\u9f50\u6761\u4ef6\u6765\u4fdd\u8bc1\u533f\u540d\u8bbe\u7f6e\u4e2d\u7528\u6237\u83b7\u5f97\u6700\u4f18\u6548\u7528\u3002", "conclusion": "\u6a21\u578b\u4e2a\u6027\u5316\u53ef\u4ee5\u8bf1\u5bfc\u591a\u5143\u5bf9\u9f50\u7ed3\u679c\uff0c\u5373\u4f7f\u63d0\u4f9b\u5546\u662f\u81ea\u5229\u7684\u3002\u5728\u4e2a\u6027\u5316\u8bbe\u7f6e\u4e0b\uff0c\u5e02\u573a\u7ade\u4e89\u80fd\u591f\u4ea7\u751f\u7c7b\u4f3c\u5b8c\u7f8e\u5bf9\u9f50\u7684\u7ed3\u679c\uff1b\u800c\u5728\u533f\u540d\u7b56\u7565\u4e0b\u9700\u8981\u66f4\u5f3a\u7684\u5bf9\u9f50\u6761\u4ef6\u6765\u4fdd\u8bc1\u7528\u6237\u83b7\u5f97\u6700\u4f18\u6548\u7528\u3002"}}
{"id": "2602.13402", "categories": ["cs.HC", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.13402", "abs": "https://arxiv.org/abs/2602.13402", "authors": ["Ioannis Dravilas", "Ioannis Kapetangeorgis", "Anastasios Latsoudis", "Conor McCarthy", "Gon\u00e7alo Marcelino", "Marcel Worring"], "title": "InfoCIR: Multimedia Analysis for Composed Image Retrieval", "comment": "9+2 pages, 8 figures. Accepted for publication in IEEE PacificVis 2026 (Conference Track). Interactive composed image retrieval (CIR) and ranking explanation", "summary": "Composed Image Retrieval (CIR) allows users to search for images by combining a reference image with a text prompt that describes desired modifications. While vision-language models like CLIP have popularized this task by embedding multiple modalities into a joint space, developers still lack tools that reveal how these multimodal prompts interact with embedding spaces and why small wording changes can dramatically alter the results. We present InfoCIR, a visual analytics system that closes this gap by coupling retrieval, explainability, and prompt engineering in a single, interactive dashboard. InfoCIR integrates a state-of-the-art CIR back-end (SEARLE arXiv:2303.15247) with a six-panel interface that (i) lets users compose image + text queries, (ii) projects the top-k results into a low-dimensional space using Uniform Manifold Approximation and Projection (UMAP) for spatial reasoning, (iii) overlays similarity-based saliency maps and gradient-derived token-attribution bars for local explanation, and (iv) employs an LLM-powered prompt enhancer that generates counterfactual variants and visualizes how these changes affect the ranking of user-selected target images. A modular architecture built on Plotly-Dash allows new models, datasets, and attribution methods to be plugged in with minimal effort. We argue that InfoCIR helps diagnose retrieval failures, guides prompt enhancement, and accelerates insight generation during model development. All source code allowing for a reproducible demo is available at https://github.com/giannhskp/InfoCIR.", "AI": {"tldr": "InfoCIR\u662f\u4e00\u4e2a\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u7528\u4e8e\u5206\u6790\u548c\u8c03\u8bd5\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u754c\u9762\u7ed3\u5408\u68c0\u7d22\u3001\u53ef\u89e3\u91ca\u6027\u548c\u63d0\u793a\u5de5\u7a0b\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u7f3a\u4e4f\u5de5\u5177\u6765\u63ed\u793a\u591a\u6a21\u6001\u63d0\u793a\u5982\u4f55\u4e0e\u5d4c\u5165\u7a7a\u95f4\u4ea4\u4e92\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u5fae\u5c0f\u7684\u6587\u672c\u53d8\u5316\u4f1a\u663e\u8457\u6539\u53d8\u68c0\u7d22\u7ed3\u679c\u3002\u7814\u7a76\u4eba\u5458\u9700\u8981\u66f4\u597d\u7684\u5de5\u5177\u6765\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u7684\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u4e86InfoCIR\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u6700\u5148\u8fdb\u7684CIR\u540e\u7aef\uff08SEARLE\uff09\uff0c\u63d0\u4f9b\u516d\u9762\u677f\u4ea4\u4e92\u754c\u9762\uff1a\u652f\u6301\u56fe\u50cf+\u6587\u672c\u67e5\u8be2\u7ec4\u5408\u3001\u4f7f\u7528UMAP\u5c06\u7ed3\u679c\u6295\u5f71\u5230\u4f4e\u7ef4\u7a7a\u95f4\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\u3001\u53e0\u52a0\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u663e\u8457\u56fe\u548c\u68af\u5ea6\u6d3e\u751f\u7684\u4ee4\u724c\u5f52\u56e0\u6761\u8fdb\u884c\u5c40\u90e8\u89e3\u91ca\u3001\u4f7f\u7528LLM\u9a71\u52a8\u7684\u63d0\u793a\u589e\u5f3a\u5668\u751f\u6210\u53cd\u4e8b\u5b9e\u53d8\u4f53\u5e76\u53ef\u89c6\u5316\u8fd9\u4e9b\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u76ee\u6807\u56fe\u50cf\u6392\u540d\u3002", "result": "InfoCIR\u7cfb\u7edf\u80fd\u591f\u5e2e\u52a9\u8bca\u65ad\u68c0\u7d22\u5931\u8d25\u3001\u6307\u5bfc\u63d0\u793a\u589e\u5f3a\uff0c\u5e76\u52a0\u901f\u6a21\u578b\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u6d1e\u5bdf\u751f\u6210\u3002\u7cfb\u7edf\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u65b0\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u5f52\u56e0\u65b9\u6cd5\u3002", "conclusion": "InfoCIR\u586b\u8865\u4e86\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u9886\u57df\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7406\u89e3\u591a\u6a21\u6001\u63d0\u793a\u4e0e\u5d4c\u5165\u7a7a\u95f4\u4ea4\u4e92\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u6a21\u578b\u5f00\u53d1\u548c\u63d0\u793a\u5de5\u7a0b\u3002"}}
{"id": "2602.13483", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13483", "abs": "https://arxiv.org/abs/2602.13483", "authors": ["Gabriel Franco", "Lucas M. Tassis", "Azalea Rohr", "Mark Crovella"], "title": "Finding Highly Interpretable Prompt-Specific Circuits in Language Models", "comment": null, "summary": "Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u4f20\u7edf\u7535\u8def\u5206\u6790\u5047\u8bbe\u6bcf\u4e2a\u4efb\u52a1\u5b58\u5728\u5355\u4e00\u7a33\u5b9a\u673a\u5236\u7684\u89c2\u70b9\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u56fa\u5b9a\u4efb\u52a1\u4e2d\uff0c\u7535\u8def\u4e5f\u662f\u63d0\u793a\u7279\u5b9a\u7684\u3002\u4f5c\u8005\u6539\u8fdb\u4e86\u6ce8\u610f\u529b\u56e0\u679c\u901a\u4fe1\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86ACC++\uff0c\u80fd\u591f\u4ece\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u63d0\u53d6\u66f4\u6e05\u6670\u3001\u4f4e\u7ef4\u7684\u56e0\u679c\u4fe1\u53f7\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u4e2a\u6a21\u578b\u4e2d\u7684\u95f4\u63a5\u5bbe\u8bed\u8bc6\u522b\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u901a\u5e38\u5728\u4efb\u52a1\u5c42\u9762\u8bc6\u522b\u7535\u8def\uff0c\u5047\u8bbe\u6bcf\u4e2a\u4efb\u52a1\u5b58\u5728\u5355\u4e00\u7a33\u5b9a\u673a\u5236\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u5047\u8bbe\u53ef\u80fd\u63a9\u76d6\u4e86\u7535\u8def\u7ed3\u6784\u7684\u91cd\u8981\u6765\u6e90\uff1a\u5373\u4f7f\u5728\u56fa\u5b9a\u4efb\u52a1\u4e2d\uff0c\u7535\u8def\u4e5f\u662f\u63d0\u793a\u7279\u5b9a\u7684\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6355\u6349\u8fd9\u79cd\u63d0\u793a\u7279\u5b9a\u6027\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u5728\u6ce8\u610f\u529b\u56e0\u679c\u901a\u4fe1\uff08ACC\uff09\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86ACC++\u6539\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u63d0\u53d6\u66f4\u6e05\u6670\u3001\u4f4e\u7ef4\u7684\u6ce8\u610f\u529b\u5934\u5185\u90e8\u56e0\u679c\u4fe1\u53f7\uff0c\u51cf\u5c11\u5f52\u56e0\u566a\u58f0\u3002\u5c06ACC++\u5e94\u7528\u4e8eGPT-2\u3001Pythia\u548cGemma 2\u6a21\u578b\u4e2d\u7684\u95f4\u63a5\u5bbe\u8bed\u8bc6\u522b\u4efb\u52a1\uff0c\u5206\u6790\u4e0d\u540c\u63d0\u793a\u6a21\u677f\u4e0b\u7684\u7535\u8def\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4efb\u4f55\u6a21\u578b\u4e2d\u90fd\u4e0d\u5b58\u5728\u5355\u4e00\u7684\u95f4\u63a5\u5bbe\u8bed\u8bc6\u522b\u7535\u8def\uff1a\u4e0d\u540c\u63d0\u793a\u6a21\u677f\u4f1a\u5f15\u53d1\u7cfb\u7edf\u6027\u7684\u4e0d\u540c\u673a\u5236\u3002\u5c3d\u7ba1\u5b58\u5728\u8fd9\u79cd\u53d8\u5316\uff0c\u4f46\u63d0\u793a\u53ef\u4ee5\u805a\u7c7b\u4e3a\u5177\u6709\u76f8\u4f3c\u7535\u8def\u7684\u63d0\u793a\u5bb6\u65cf\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5bb6\u65cf\u63d0\u51fa\u4e86\u4ee3\u8868\u6027\u7535\u8def\u4f5c\u4e3a\u5206\u6790\u5355\u5143\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u53ef\u89e3\u91ca\u6027\u6d41\u7a0b\uff0c\u4f7f\u7528ACC++\u4fe1\u53f7\u63d0\u53d6\u4eba\u7c7b\u53ef\u89e3\u91ca\u7279\u5f81\u5e76\u6784\u5efa\u673a\u5236\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u5206\u6790\u5355\u5143\u4ece\u4efb\u52a1\u8f6c\u79fb\u5230\u63d0\u793a\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u7535\u8def\u4f5c\u4e3a\u6709\u610f\u4e49\u7684\u7814\u7a76\u5bf9\u8c61\uff0c\u4f7f\u5f97\u5728\u5b58\u5728\u63d0\u793a\u7279\u5b9a\u673a\u5236\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u7535\u8def\u63cf\u8ff0\u3002\u8fd9\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2602.13787", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.13787", "abs": "https://arxiv.org/abs/2602.13787", "authors": ["Lorenzo Picinali", "Robert Baumgartner", "Valerie Gaveau", "Antonino Greco", "Stefanie Liebe", "Paul Oomen", "Christoph Braun"], "title": "Enhancing spatial hearing with cochlear implants: exploring the role of AI, multimodal interaction and perceptual training", "comment": null, "summary": "Cochlear implants (CIs) have been developed to the point where they can restore hearing and speech understanding in a large proportion of patients. Although spatial hearing is central to controlling and directing attention and to enabling speech understanding in noisy environments, it has been largely neglected in the past. We propose here a multi-disciplinary research framework in which physicians, psychologists and engineers collaborate to improve spatial hearing for CI users.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5b66\u79d1\u7814\u7a76\u6846\u67b6\u6539\u5584\u4eba\u5de5\u8033\u8717\u4f7f\u7528\u8005\u7684\u7a7a\u95f4\u542c\u89c9\u80fd\u529b", "motivation": "\u867d\u7136\u4eba\u5de5\u8033\u8717\u5728\u6062\u590d\u542c\u529b\u548c\u8a00\u8bed\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7a7a\u95f4\u542c\u89c9\u8fd9\u4e00\u5bf9\u6ce8\u610f\u529b\u63a7\u5236\u548c\u5608\u6742\u73af\u5883\u4e2d\u8a00\u8bed\u7406\u89e3\u81f3\u5173\u91cd\u8981\u7684\u80fd\u529b\u5728\u8fc7\u53bb\u88ab\u5ffd\u89c6\uff0c\u9700\u8981\u6539\u8fdb", "method": "\u63d0\u51fa\u533b\u751f\u3001\u5fc3\u7406\u5b66\u5bb6\u548c\u5de5\u7a0b\u5e08\u534f\u4f5c\u7684\u591a\u5b66\u79d1\u7814\u7a76\u6846\u67b6", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u6846\u67b6\uff0c\u4f46\u5c1a\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u9700\u8981\u901a\u8fc7\u591a\u5b66\u79d1\u534f\u4f5c\u6765\u6539\u5584\u4eba\u5de5\u8033\u8717\u4f7f\u7528\u8005\u7684\u7a7a\u95f4\u542c\u89c9\u80fd\u529b"}}
{"id": "2602.13312", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13312", "abs": "https://arxiv.org/abs/2602.13312", "authors": ["Yishu Wang", "Wei Liu", "Yifan Li", "Shengxiang Xu", "Xujie Yuan", "Ran Li", "Yuyu Luo", "Jia Zhu", "Shimin Di", "Min-Ling Zhang", "Guixiang Li"], "title": "PeroMAS: A Multi-agent System of Perovskite Material Discovery", "comment": null, "summary": "As a pioneer of the third-generation photovoltaic revolution, Perovskite Solar Cells (PSCs) are renowned for their superior optoelectronic performance and cost potential. The development process of PSCs is precise and complex, involving a series of closed-loop workflows such as literature retrieval, data integration, experimental design, and synthesis. However, existing AI perovskite approaches focus predominantly on discrete models, including material design, process optimization,and property prediction. These models fail to propagate physical constraints across the workflow, hindering end-to-end optimization. In this paper, we propose a multi-agent system for perovskite material discovery, named PeroMAS. We first encapsulated a series of perovskite-specific tools into Model Context Protocols (MCPs). By planning and invoking these tools, PeroMAS can design perovskite materials under multi-objective constraints, covering the entire process from literature retrieval and data extraction to property prediction and mechanism analysis. Furthermore, we construct an evaluation benchmark by perovskite human experts to assess this multi-agent system. Results demonstrate that, compared to single Large Language Model (LLM) or traditional search strategies, our system significantly enhances discovery efficiency. It successfully identified candidate materials satisfying multi-objective constraints. Notably, we verify PeroMAS's effectiveness in the physical world through real synthesis experiments.", "AI": {"tldr": "PeroMAS\u662f\u4e00\u4e2a\u7528\u4e8e\u9499\u949b\u77ff\u6750\u6599\u53d1\u73b0\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c01\u88c5\u4e13\u4e1a\u5de5\u5177\u5b9e\u73b0\u4ece\u6587\u732e\u68c0\u7d22\u5230\u5b9e\u9a8c\u5408\u6210\u7684\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u53d1\u73b0\u6548\u7387\u3002", "motivation": "\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u5f00\u53d1\u8fc7\u7a0b\u590d\u6742\u4e14\u6d89\u53ca\u591a\u4e2a\u95ed\u73af\u5de5\u4f5c\u6d41\u7a0b\uff0c\u73b0\u6709AI\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u79bb\u6563\u6a21\u578b\uff08\u5982\u6750\u6599\u8bbe\u8ba1\u3001\u5de5\u827a\u4f18\u5316\u3001\u6027\u80fd\u9884\u6d4b\uff09\uff0c\u65e0\u6cd5\u5728\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u4f20\u64ad\u7269\u7406\u7ea6\u675f\uff0c\u963b\u788d\u7aef\u5230\u7aef\u4f18\u5316\u3002", "method": "\u63d0\u51faPeroMAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u4e00\u7cfb\u5217\u9499\u949b\u77ff\u4e13\u7528\u5de5\u5177\u5c01\u88c5\u4e3a\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCPs\uff09\uff0c\u901a\u8fc7\u89c4\u5212\u548c\u8c03\u7528\u8fd9\u4e9b\u5de5\u5177\uff0c\u5728\u591a\u76ee\u6807\u7ea6\u675f\u4e0b\u8bbe\u8ba1\u9499\u949b\u77ff\u6750\u6599\uff0c\u8986\u76d6\u4ece\u6587\u732e\u68c0\u7d22\u3001\u6570\u636e\u63d0\u53d6\u5230\u6027\u80fd\u9884\u6d4b\u548c\u673a\u7406\u5206\u6790\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "\u76f8\u6bd4\u5355\u4e00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u4f20\u7edf\u641c\u7d22\u7b56\u7565\uff0cPeroMAS\u663e\u8457\u63d0\u5347\u4e86\u53d1\u73b0\u6548\u7387\uff0c\u6210\u529f\u8bc6\u522b\u51fa\u6ee1\u8db3\u591a\u76ee\u6807\u7ea6\u675f\u7684\u5019\u9009\u6750\u6599\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PeroMAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9499\u949b\u77ff\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u7aef\u5230\u7aef\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u4e13\u4e1a\u5de5\u5177\u548c\u5de5\u4f5c\u6d41\u7a0b\u4f20\u64ad\u7269\u7406\u7ea6\u675f\uff0c\u4e3a\u9499\u949b\u77ff\u592a\u9633\u80fd\u7535\u6c60\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13957", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13957", "abs": "https://arxiv.org/abs/2602.13957", "authors": ["Li Xiaojie", "Yin Xunyuan"], "title": "Learning-based data-enabled moving horizon estimation with application to membrane-based biological wastewater treatment process", "comment": null, "summary": "In this paper, we propose a data-enabled moving horizon estimation (MHE) approach for nonlinear systems. While the approach is formulated by leveraging Koopman theory, its implementation does not require explicit Koopman modeling. Lifting functions are learned from the state and input data of the original nonlinear system to project the system trajectories into the lifted space, where the resulting trajectories implicitly describe the Koopman representation for the original nonlinear system. A convex data-enabled MHE formulation is developed to provide real-time state estimates of the Koopman representation, from which the states of the nonlinear system can be reconstructed. Sufficient conditions are derived to ensure the stability of the estimation error. The effectiveness of the proposed method is illustrated using a membrane-based biological water treatment process.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u79fb\u52a8\u6c34\u5e73\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\uff0c\u65e0\u9700\u663e\u5f0fKoopman\u5efa\u6a21", "motivation": "\u9488\u5bf9\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u7cfb\u7edf\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5229\u7528Koopman\u7406\u8bba\u4f46\u65e0\u9700\u663e\u5f0f\u5efa\u6a21\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u6709\u6548\u7684\u72b6\u6001\u4f30\u8ba1", "method": "1. \u4ece\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u72b6\u6001\u548c\u8f93\u5165\u6570\u636e\u4e2d\u5b66\u4e60\u63d0\u5347\u51fd\u6570\uff0c\u5c06\u7cfb\u7edf\u8f68\u8ff9\u6295\u5f71\u5230\u63d0\u5347\u7a7a\u95f4\uff1b2. \u5728\u63d0\u5347\u7a7a\u95f4\u4e2d\uff0c\u8f68\u8ff9\u9690\u5f0f\u63cf\u8ff0\u539f\u59cb\u975e\u7ebf\u6027\u7cfb\u7edf\u7684Koopman\u8868\u793a\uff1b3. \u5f00\u53d1\u51f8\u6570\u636e\u9a71\u52a8MHE\u516c\u5f0f\uff0c\u5b9e\u65f6\u4f30\u8ba1Koopman\u8868\u793a\u7684\u72b6\u6001\uff1b4. \u4eceKoopman\u8868\u793a\u91cd\u6784\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\uff1b5. \u63a8\u5bfc\u786e\u4fdd\u4f30\u8ba1\u8bef\u5dee\u7a33\u5b9a\u6027\u7684\u5145\u5206\u6761\u4ef6", "result": "1. \u6210\u529f\u5f00\u53d1\u4e86\u6570\u636e\u9a71\u52a8\u7684MHE\u65b9\u6cd5\uff1b2. \u63a8\u5bfc\u4e86\u4f30\u8ba1\u8bef\u5dee\u7a33\u5b9a\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\uff1b3. \u5728\u819c\u57fa\u751f\u7269\u6c34\u5904\u7406\u8fc7\u7a0b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8MHE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\uff0c\u65e0\u9700\u663e\u5f0fKoopman\u5efa\u6a21\uff0c\u5177\u6709\u7406\u8bba\u7a33\u5b9a\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.13897", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.13897", "abs": "https://arxiv.org/abs/2602.13897", "authors": ["Bhaskar Ray Chaudhury", "Jugal Garg", "Eklavya Sharma", "Jiaxin Song"], "title": "Revenue-Optimal Pricing for Budget-Constrained Buyers in Data Markets", "comment": null, "summary": "We study revenue-optimal pricing in data markets with rational, budget-constrained buyers. Such a market offers multiple datasets for sale, and buyers aim to improve the accuracy of their prediction tasks by acquiring data bundles. For each dataset, the market sets a pricing function, which maps the number of records purchased from the dataset to a non-negative price. The market's objective is to set these pricing functions to maximize total revenue, considering that buyers with quasi-linear utilities choose their bundles optimally under budget constraints.\n  We analyze optimal pricing when each dataset's pricing function is only required to be monotone and lower-continuous. Surprisingly, even with this generality, optimal pricing has a highly structured form: it is piecewise linear and convex (PLC) and can be computed efficiently via an LP. Moreover, the total number of kinks across all pricing functions is bounded by the number of buyers. Thus, when datasets far outnumber buyers, most pricing functions are effectively linear.\n  This motivates studying linear pricing, where each record in a dataset is priced uniformly. Although competitive equilibrium gives revenue-optimal linear prices in rivalrous markets with quasi-linear buyers, we show that revenue maximization under linear pricing in data markets is APX-hard. Hence, a striking computational dichotomy emerges: fully general (nonlinear) pricing admits a polynomial-time algorithm, while the simpler linear scheme is APX-hard.\n  Despite the hardness, we design a 2-approximation algorithm when datasets arrive online, and a $(1-1/e)^{-1}$-approximation algorithm for the offline setting. Our framework lays the groundwork for exploring more general pricing schemes, richer utility models, and a deeper understanding of how market structure -- rivalrous versus non-rivalrous -- shapes revenue-optimal pricing.", "AI": {"tldr": "\u6570\u636e\u5e02\u573a\u4e2d\u9884\u7b97\u7ea6\u675f\u4e70\u5bb6\u7684\u6536\u76ca\u6700\u4f18\u5b9a\u4ef7\u7814\u7a76\uff1a\u975e\u7ebf\u6027\u5b9a\u4ef7\u53ef\u591a\u9879\u5f0f\u65f6\u95f4\u6c42\u89e3\uff0c\u800c\u7ebf\u6027\u5b9a\u4ef7\u5374\u662fAPX\u96be\u7684\uff0c\u5f62\u6210\u8ba1\u7b97\u590d\u6742\u6027\u4e8c\u5206\u6cd5", "motivation": "\u7814\u7a76\u6570\u636e\u5e02\u573a\u4e2d\u6536\u76ca\u6700\u4f18\u5b9a\u4ef7\u95ee\u9898\uff0c\u5e02\u573a\u51fa\u552e\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u4e70\u5bb6\u901a\u8fc7\u8d2d\u4e70\u6570\u636e\u5305\u6765\u63d0\u9ad8\u9884\u6d4b\u4efb\u52a1\u51c6\u786e\u6027\u3002\u4e70\u5bb6\u5177\u6709\u51c6\u7ebf\u6027\u6548\u7528\u548c\u9884\u7b97\u7ea6\u675f\uff0c\u5e02\u573a\u9700\u8981\u8bbe\u5b9a\u5b9a\u4ef7\u51fd\u6570\u4ee5\u6700\u5927\u5316\u603b\u6536\u5165\u3002", "method": "\u5206\u6790\u5355\u8c03\u4e14\u4e0b\u534a\u8fde\u7eed\u7684\u5b9a\u4ef7\u51fd\u6570\uff0c\u53d1\u73b0\u6700\u4f18\u5b9a\u4ef7\u5177\u6709\u5206\u6bb5\u7ebf\u6027\u51f8\uff08PLC\uff09\u7ed3\u6784\uff0c\u53ef\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u9ad8\u6548\u8ba1\u7b97\u3002\u540c\u65f6\u7814\u7a76\u7ebf\u6027\u5b9a\u4ef7\u65b9\u6848\uff0c\u8bc1\u660e\u5176APX\u96be\u6027\uff0c\u5e76\u8bbe\u8ba1\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u6700\u4f18\u975e\u7ebf\u6027\u5b9a\u4ef7\u5177\u6709\u9ad8\u5ea6\u7ed3\u6784\u5316\u5f62\u5f0f\uff08PLC\uff09\uff0c\u53ef\u591a\u9879\u5f0f\u65f6\u95f4\u8ba1\u7b97\uff1b\u800c\u7ebf\u6027\u5b9a\u4ef7\u662fAPX\u96be\u7684\u3002\u8bbe\u8ba1\u4e86\u5728\u7ebf\u8bbe\u7f6e\u76842-\u8fd1\u4f3c\u7b97\u6cd5\u548c\u79bb\u7ebf\u8bbe\u7f6e\u7684(1-1/e)^{-1}-\u8fd1\u4f3c\u7b97\u6cd5\u3002", "conclusion": "\u6570\u636e\u5e02\u573a\u4e2d\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u6027\u4e8c\u5206\u6cd5\uff1a\u5b8c\u5168\u901a\u7528\u7684\u975e\u7ebf\u6027\u5b9a\u4ef7\u53ef\u9ad8\u6548\u6c42\u89e3\uff0c\u800c\u66f4\u7b80\u5355\u7684\u7ebf\u6027\u5b9a\u4ef7\u65b9\u6848\u5374\u8ba1\u7b97\u56f0\u96be\u3002\u8be5\u6846\u67b6\u4e3a\u63a2\u7d22\u66f4\u4e30\u5bcc\u7684\u5b9a\u4ef7\u65b9\u6848\u548c\u6548\u7528\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.13424", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13424", "abs": "https://arxiv.org/abs/2602.13424", "authors": ["Shuhao Ma", "John Zimmerman", "Valentina Nisi", "Nuno Jardim Nunes"], "title": "Revisiting Worker-Centered Design: Tensions, Blind Spots, and Action Spaces", "comment": "19 pages, 7 figure, accepted by CHI'26", "summary": "Worker-Centered Design (WCD) has gained prominence over the past decade, offering researchers and practitioners ways to engage worker agency and support collective actions for workers. Yet few studies have systematically revisited WCD itself, examining its implementations, challenges, and practical impact. Through a four-lens analytical framework that examines multiple facets of WCD within food delivery industry, we identify critical tensions and blind spots from a Multi-Laborer System perspective. Our analysis reveals conflicts across labor chains, distorted implementations of WCD, designers' sometimes limited political-economic understanding, and workers as active agents of change. These insights further inform a Diagnostic-Generative pathway that helps to address recurring risks, including labor conflicts and institutional reframing, while cultivating designers' policy and economic imagination. Following the design criticism tradition, and through a four-lens reflexive analysis, this study expands the action space for WCD and strengthens its relevance to real-world practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u56db\u900f\u955c\u5206\u6790\u6846\u67b6\u91cd\u65b0\u5ba1\u89c6\u4ee5\u5de5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1(WCD)\uff0c\u5728\u98df\u54c1\u914d\u9001\u884c\u4e1a\u4e2d\u53d1\u73b0\u5173\u952e\u5f20\u529b\u4e0e\u76f2\u70b9\uff0c\u63d0\u51fa\u8bca\u65ad-\u751f\u6210\u8def\u5f84\u4ee5\u89e3\u51b3\u52b3\u52a8\u51b2\u7a81\u7b49\u98ce\u9669", "motivation": "\u5c3d\u7ba1\u4ee5\u5de5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1(WCD)\u5728\u8fc7\u53bb\u5341\u5e74\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u91cd\u65b0\u5ba1\u89c6WCD\u672c\u8eab\uff0c\u8003\u5bdf\u5176\u5b9e\u9645\u5b9e\u65bd\u3001\u6311\u6218\u548c\u5b9e\u9645\u5f71\u54cd\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u98df\u54c1\u914d\u9001\u884c\u4e1a\u80cc\u666f\u4e0b", "method": "\u91c7\u7528\u56db\u900f\u955c\u5206\u6790\u6846\u67b6\uff0c\u4ece\u591a\u52b3\u5de5\u7cfb\u7edf\u89c6\u89d2\u8003\u5bdfWCD\u7684\u591a\u4e2a\u65b9\u9762\u3002\u901a\u8fc7\u8bca\u65ad-\u751f\u6210\u8def\u5f84\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bbe\u8ba1\u6279\u8bc4\u4f20\u7edf\u548c\u56db\u900f\u955c\u53cd\u601d\u5206\u6790", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u52b3\u52a8\u94fe\u4e2d\u7684\u51b2\u7a81\uff1b2) WCD\u7684\u626d\u66f2\u5b9e\u65bd\uff1b3) \u8bbe\u8ba1\u5e08\u6709\u65f6\u6709\u9650\u7684\u653f\u6cbb\u7ecf\u6d4e\u7406\u89e3\uff1b4) \u5de5\u4eba\u4f5c\u4e3a\u53d8\u9769\u7684\u79ef\u6781\u4e3b\u4f53\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86WCD\u5b9e\u65bd\u4e2d\u7684\u5173\u952e\u5f20\u529b\u548c\u76f2\u70b9", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86WCD\u7684\u884c\u52a8\u7a7a\u95f4\uff0c\u589e\u5f3a\u4e86\u5176\u4e0e\u73b0\u5b9e\u4e16\u754c\u5b9e\u8df5\u7684\u76f8\u5173\u6027\uff0c\u901a\u8fc7\u8bca\u65ad-\u751f\u6210\u8def\u5f84\u5e2e\u52a9\u89e3\u51b3\u52b3\u52a8\u51b2\u7a81\u548c\u5236\u5ea6\u91cd\u6784\u7b49\u91cd\u590d\u98ce\u9669\uff0c\u540c\u65f6\u57f9\u517b\u8bbe\u8ba1\u5e08\u7684\u653f\u7b56\u548c\u7ecf\u6d4e\u60f3\u8c61\u529b"}}
{"id": "2602.13769", "categories": ["cs.AI", "cs.CE", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.13769", "abs": "https://arxiv.org/abs/2602.13769", "authors": ["Qi Liu", "Wanjing Ma"], "title": "OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery", "comment": null, "summary": "Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.", "AI": {"tldr": "OR-Agent\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u591a\u667a\u80fd\u4f53\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u72b6\u5de5\u4f5c\u6d41\u7ba1\u7406\u5047\u8bbe\u751f\u6210\u548c\u7cfb\u7edf\u56de\u6eaf\uff0c\u7ed3\u5408\u8fdb\u5316-\u7cfb\u7edf\u6784\u601d\u673a\u5236\u548c\u5206\u5c42\u4f18\u5316\u53cd\u601d\u7cfb\u7edf\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u548c\u534f\u540c\u9a7e\u9a76\u573a\u666f\u4e2d\u8d85\u8d8a\u8fdb\u5316\u57fa\u7ebf\u3002", "motivation": "\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u5728\u590d\u6742\u5b9e\u9a8c\u9a71\u52a8\u9886\u57df\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u7a0b\u5e8f\u8fed\u4ee3\u53d8\u5f02\uff0c\u9700\u8981\u7ed3\u6784\u5316\u7684\u5047\u8bbe\u7ba1\u7406\u3001\u73af\u5883\u4ea4\u4e92\u548c\u6709\u539f\u5219\u7684\u53cd\u601d\u673a\u5236\u3002", "method": "\u63d0\u51faOR-Agent\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u6811\u72b6\u5de5\u4f5c\u6d41\u7ec4\u7ec7\u7814\u7a76\uff0c\u663e\u5f0f\u5efa\u6a21\u5206\u652f\u5047\u8bbe\u751f\u6210\u548c\u7cfb\u7edf\u56de\u6eaf\uff1b2\uff09\u8fdb\u5316-\u7cfb\u7edf\u6784\u601d\u673a\u5236\u7edf\u4e00\u8fdb\u5316\u9009\u62e9\u3001\u7814\u7a76\u8ba1\u5212\u751f\u6210\u548c\u534f\u8c03\u63a2\u7d22\uff1b3\uff09\u5206\u5c42\u4f18\u5316\u53cd\u601d\u7cfb\u7edf\u5305\u62ec\u77ed\u671f\u5b9e\u9a8c\u53cd\u601d\uff08\u53e3\u5934\u68af\u5ea6\uff09\u3001\u957f\u671f\u53cd\u601d\uff08\u53e3\u5934\u52a8\u91cf\uff09\u548c\u8bb0\u5fc6\u538b\u7f29\uff08\u6b63\u5219\u5316\u673a\u5236\uff09\u3002", "result": "\u5728\u7ecf\u5178\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\uff08\u65c5\u884c\u5546\u3001\u8f66\u8f86\u8def\u5f84\u3001\u88c5\u7bb1\u3001\u5b9a\u5411\u3001\u591a\u80cc\u5305\u95ee\u9898\uff09\u548c\u4eff\u771f\u534f\u540c\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cOR-Agent\u4f18\u4e8e\u5f3a\u8fdb\u5316\u57fa\u7ebf\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u3001\u53ef\u68c0\u67e5\u7684AI\u8f85\u52a9\u79d1\u5b66\u53d1\u73b0\u6846\u67b6\u3002", "conclusion": "OR-Agent\u901a\u8fc7\u7ed3\u6784\u5316\u5047\u8bbe\u7ba1\u7406\u3001\u8fdb\u5316-\u7cfb\u7edf\u6784\u601d\u548c\u5206\u5c42\u53cd\u601d\u673a\u5236\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u67b6\u6784\uff0c\u5728\u590d\u6742\u5b9e\u9a8c\u9a71\u52a8\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.13353", "categories": ["cs.MA", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.13353", "abs": "https://arxiv.org/abs/2602.13353", "authors": ["Bhavini Jeloka", "Yue Guan", "Panagiotis Tsiotras"], "title": "Robust Mean-Field Games with Risk Aversion and Bounded Rationality", "comment": "25 pages, 2 figures", "summary": "Recent advances in mean-field game literature enable the reduction of large-scale multi-agent problems to tractable interactions between a representative agent and a population distribution. However, existing approaches typically assume a fixed initial population distribution and fully rational agents, limiting robustness under distributional uncertainty and cognitive constraints. We address these limitations by introducing risk aversion with respect to the initial population distribution and by incorporating bounded rationality to model deviations from fully rational decision-making agents. The combination of these two elements yields a new and more general equilibrium concept, which we term the mean-field risk-averse quantal response equilibrium (MF-RQE). We establish existence results and prove convergence of fixed-point iteration and fictitious play to MF-RQE. Building on these insights, we develop a scalable reinforcement learning algorithm for scenarios with large state-action spaces. Numerical experiments demonstrate that MF-RQE policies achieve improved robustness relative to classical mean-field approaches that optimize expected cumulative rewards under a fixed initial distribution and are restricted to entropy-based regularizers.", "AI": {"tldr": "\u63d0\u51faMF-RQE\uff08\u5747\u503c\u573a\u98ce\u9669\u89c4\u907f\u91cf\u5316\u54cd\u5e94\u5747\u8861\uff09\u65b0\u6982\u5ff5\uff0c\u7ed3\u5408\u98ce\u9669\u89c4\u907f\u548c\u6709\u9650\u7406\u6027\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u7ea6\u675f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5747\u503c\u573a\u535a\u5f08\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u7684\u521d\u59cb\u79cd\u7fa4\u5206\u5e03\u548c\u5b8c\u5168\u7406\u6027\u667a\u80fd\u4f53\uff0c\u9650\u5236\u4e86\u5728\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u7ea6\u675f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e24\u4e2a\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u5bf9\u521d\u59cb\u79cd\u7fa4\u5206\u5e03\u7684\u98ce\u9669\u89c4\u907f\u673a\u5236\uff0c\u5e76\u7eb3\u5165\u6709\u9650\u7406\u6027\u6765\u5efa\u6a21\u667a\u80fd\u4f53\u504f\u79bb\u5b8c\u5168\u7406\u6027\u51b3\u7b56\u7684\u884c\u4e3a\u3002\u7ed3\u5408\u8fd9\u4e24\u4e2a\u8981\u7d20\u5f62\u6210\u65b0\u7684\u5747\u8861\u6982\u5ff5MF-RQE\uff0c\u5efa\u7acb\u5b58\u5728\u6027\u8bc1\u660e\u548c\u6536\u655b\u6027\u5206\u6790\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86MF-RQE\u7684\u5b58\u5728\u6027\uff0c\u8bc1\u660e\u4e86\u5b9a\u70b9\u8fed\u4ee3\u548c\u865a\u62df\u535a\u5f08\u7684\u6536\u655b\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u7ecf\u5178\u7684\u5747\u503c\u573a\u65b9\u6cd5\uff0cMF-RQE\u7b56\u7565\u5728\u56fa\u5b9a\u521d\u59cb\u5206\u5e03\u4e0b\u4f18\u5316\u671f\u671b\u7d2f\u79ef\u5956\u52b1\u5e76\u4ec5\u9650\u4e8e\u57fa\u4e8e\u71b5\u7684\u6b63\u5219\u5316\u5668\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MF-RQE\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u901a\u7528\u3001\u66f4\u9c81\u68d2\u7684\u5747\u8861\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u98ce\u9669\u89c4\u907f\u548c\u6709\u9650\u7406\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u7ea6\u675f\u3002"}}
{"id": "2602.14092", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14092", "abs": "https://arxiv.org/abs/2602.14092", "authors": ["Jan-Hendrik Ewering", "Max Bartholdt", "Simon F. G. Ehlers", "Niklas Wahlstr\u00f6m", "Thomas B. Sch\u00f6n", "Thomas Seel"], "title": "Simultaneous State Estimation and Online Model Learning in a Soft Robotic System", "comment": "8 pages, 3 figures, 2 tables", "summary": "Operating complex real-world systems, such as soft robots, can benefit from precise predictive control schemes that require accurate state and model knowledge. This knowledge is typically not available in practical settings and must be inferred from noisy measurements. In particular, it is challenging to simultaneously estimate unknown states and learn a model online from sequentially arriving measurements. In this paper, we show how a recently proposed gray-box system identification tool enables the estimation of a soft robot's current pose while at the same time learning a bending stiffness model. For estimation and learning, we rely solely on a nominal constant-curvature robot model and measurements of the robot's base reactions (e.g., base forces). The estimation scheme -- relying on a marginalized particle filter -- allows us to conveniently interface nominal constant-curvature equations with a Gaussian Process (GP) bending stiffness model to be learned. This, in contrast to estimation via a random walk over stiffness values, enables prediction of bending stiffness and improves overall model quality. We demonstrate, using real-world soft-robot data, that the method learns a bending stiffness model online while accurately estimating the robot's pose. Notably, reduced multi-step forward-prediction errors indicate that the learned bending-stiffness GP improves overall model quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u5316\u7c92\u5b50\u6ee4\u6ce2\u7684\u8f6f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4e0e\u521a\u5ea6\u6a21\u578b\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ec5\u9700\u540d\u4e49\u6052\u5b9a\u66f2\u7387\u6a21\u578b\u548c\u57fa\u5ea7\u53cd\u529b\u6d4b\u91cf\uff0c\u53ef\u540c\u65f6\u4f30\u8ba1\u673a\u5668\u4eba\u59ff\u6001\u548c\u5b66\u4e60\u5f2f\u66f2\u521a\u5ea6\u6a21\u578b", "motivation": "\u8f6f\u673a\u5668\u4eba\u7b49\u590d\u6742\u73b0\u5b9e\u7cfb\u7edf\u9700\u8981\u7cbe\u786e\u7684\u9884\u6d4b\u63a7\u5236\uff0c\u4f46\u5b9e\u9645\u4e2d\u96be\u4ee5\u540c\u65f6\u83b7\u5f97\u51c6\u786e\u7684\u72b6\u6001\u548c\u6a21\u578b\u4fe1\u606f\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5728\u7ebf\u4f30\u8ba1\u672a\u77e5\u72b6\u6001\u5e76\u4ece\u987a\u5e8f\u5230\u8fbe\u7684\u6d4b\u91cf\u4e2d\u5b66\u4e60\u6a21\u578b", "method": "\u91c7\u7528\u7070\u7bb1\u7cfb\u7edf\u8fa8\u8bc6\u5de5\u5177\uff0c\u7ed3\u5408\u8fb9\u7f18\u5316\u7c92\u5b50\u6ee4\u6ce2\uff0c\u5c06\u540d\u4e49\u6052\u5b9a\u66f2\u7387\u6a21\u578b\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5f2f\u66f2\u521a\u5ea6\u6a21\u578b\u63a5\u53e3\uff0c\u4ec5\u4f9d\u8d56\u57fa\u5ea7\u53cd\u529b\u6d4b\u91cf\u8fdb\u884c\u5728\u7ebf\u72b6\u6001\u4f30\u8ba1\u548c\u6a21\u578b\u5b66\u4e60", "result": "\u4f7f\u7528\u771f\u5b9e\u8f6f\u673a\u5668\u4eba\u6570\u636e\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u5728\u7ebf\u5b66\u4e60\u5f2f\u66f2\u521a\u5ea6\u6a21\u578b\u5e76\u51c6\u786e\u4f30\u8ba1\u673a\u5668\u4eba\u59ff\u6001\uff0c\u591a\u6b65\u524d\u5411\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u8868\u660e\u5b66\u4e60\u7684GP\u521a\u5ea6\u6a21\u578b\u63d0\u5347\u4e86\u6574\u4f53\u6a21\u578b\u8d28\u91cf", "conclusion": "\u63d0\u51fa\u7684\u8fb9\u7f18\u5316\u7c92\u5b50\u6ee4\u6ce2\u65b9\u6cd5\u80fd\u6709\u6548\u540c\u65f6\u8fdb\u884c\u8f6f\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u548c\u521a\u5ea6\u6a21\u578b\u5728\u7ebf\u5b66\u4e60\uff0c\u76f8\u6bd4\u968f\u673a\u6e38\u8d70\u521a\u5ea6\u503c\u4f30\u8ba1\uff0c\u80fd\u9884\u6d4b\u5f2f\u66f2\u521a\u5ea6\u5e76\u6539\u5584\u6a21\u578b\u8d28\u91cf"}}
{"id": "2602.14076", "categories": ["cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14076", "abs": "https://arxiv.org/abs/2602.14076", "authors": ["Reshef Meir", "Jonathan Wagner", "Omer Ben-Porat"], "title": "Truthful Reporting of Competence with Minimal Verification", "comment": "Full version of a paper accepted to AAMAS 2026", "summary": "Suppose you run a home exam, where students should report their own scores but can cheat freely. You can, if needed, call a limited number of students to class and verify their actual performance against their reported score. We consider the class of mechanisms where truthful reporting is a dominant strategy, and truthful agents are never penalized -- even off-equilibrium.\n  How many students do we need to verify, in expectation, if we want to minimize the bias, i.e., the difference between agents' competence and their expected grade? When perfect verification is available, we characterize the best possible tradeoff between these requirements and provide a simple parametrized mechanism that is optimal in the class for any distribution of agents' types. When verification is noisy, the task becomes much more challenging. We show how proper scoring rules can be leveraged in different ways to construct truthful mechanisms with a good (though not necessarily optimal) tradeoff.", "AI": {"tldr": "\u7814\u7a76\u5bb6\u5ead\u8003\u8bd5\u4e2d\u5982\u4f55\u901a\u8fc7\u6709\u9650\u9a8c\u8bc1\u6765\u6700\u5c0f\u5316\u5b66\u751f\u80fd\u529b\u4e0e\u9884\u671f\u6210\u7ee9\u4e4b\u95f4\u7684\u504f\u5dee\uff0c\u540c\u65f6\u786e\u4fdd\u8bda\u5b9e\u62a5\u544a\u662f\u5360\u4f18\u7b56\u7565\u4e14\u8bda\u5b9e\u8005\u4e0d\u53d7\u7f5a", "motivation": "\u5728\u5bb6\u5ead\u8003\u8bd5\u4e2d\uff0c\u5b66\u751f\u53ef\u4ee5\u81ea\u7531\u4f5c\u5f0a\u62a5\u544a\u81ea\u5df1\u7684\u5206\u6570\uff0c\u6559\u5e08\u53ea\u80fd\u6709\u9650\u5730\u9a8c\u8bc1\u5b66\u751f\u5b9e\u9645\u8868\u73b0\u3002\u9700\u8981\u8bbe\u8ba1\u673a\u5236\u786e\u4fdd\u8bda\u5b9e\u62a5\u544a\u662f\u5360\u4f18\u7b56\u7565\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5b66\u751f\u80fd\u529b\u4e0e\u9884\u671f\u6210\u7ee9\u4e4b\u95f4\u7684\u504f\u5dee", "method": "1. \u5b8c\u7f8e\u9a8c\u8bc1\u60c5\u51b5\u4e0b\uff1a\u53c2\u6570\u5316\u673a\u5236\u8bbe\u8ba1\uff0c\u8868\u5f81\u6700\u4f18\u6743\u8861\uff1b2. \u566a\u58f0\u9a8c\u8bc1\u60c5\u51b5\u4e0b\uff1a\u5229\u7528\u9002\u5f53\u8bc4\u5206\u89c4\u5219\u6784\u5efa\u8bda\u5b9e\u673a\u5236", "result": "1. \u5b8c\u7f8e\u9a8c\u8bc1\u65f6\uff1a\u7ed9\u51fa\u4e86\u6700\u4f18\u6743\u8861\u7684\u8868\u5f81\u548c\u53c2\u6570\u5316\u6700\u4f18\u673a\u5236\uff1b2. \u566a\u58f0\u9a8c\u8bc1\u65f6\uff1a\u6784\u5efa\u4e86\u5177\u6709\u826f\u597d\uff08\u4e0d\u4e00\u5b9a\u6700\u4f18\uff09\u6743\u8861\u7684\u8bda\u5b9e\u673a\u5236", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5bb6\u5ead\u8003\u8bd5\u4e2d\u7684\u9a8c\u8bc1\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5728\u5b8c\u7f8e\u9a8c\u8bc1\u4e0b\u8fbe\u5230\u6700\u4f18\uff0c\u5728\u566a\u58f0\u9a8c\u8bc1\u4e0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.13433", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13433", "abs": "https://arxiv.org/abs/2602.13433", "authors": ["Ching-Yi Tsai", "Nicole Tacconi", "Andrew D. Wilson", "Parastoo Abtahi"], "title": "Uncertain Pointer: Situated Feedforward Visualizations for Ambiguity-Aware AR Target Selection", "comment": "Accepted at the 2026 CHI Conference on Human Factors in Computing Systems (CHI 2026). 31 Pages, 10 figures, 16 tables", "summary": "Target disambiguation is crucial in resolving input ambiguity in augmented reality (AR), especially for queries over distant objects or cluttered scenes on the go. Yet, visual feedforward techniques that support this process remain underexplored. We present Uncertain Pointer, a systematic exploration of feedforward visualizations that annotate multiple candidate targets before user confirmation, either by adding distinct visual identities (e.g., colors) to support disambiguation or by modulating visual intensity (e.g., opacity) to convey system uncertainty. First, we construct a pointer space of 25 pointers by analyzing existing placement strategies and visual signifiers used in target visualizations across 30 years of relevant literature. We then evaluate them through two online experiments (n = 60 and 40), measuring user preference, confidence, mental ease, target visibility, and identifiability across varying object distances and sparsities. Finally, from the results, we derive design recommendations in choosing different Uncertain Pointers based on AR context and disambiguation techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\"\u4e0d\u786e\u5b9a\u6307\u9488\"\u7cfb\u7edf\uff0c\u901a\u8fc7\u524d\u9988\u53ef\u89c6\u5316\u6280\u672f\u6807\u6ce8\u591a\u4e2a\u5019\u9009\u76ee\u6807\u6765\u652f\u6301AR\u4e2d\u7684\u76ee\u6807\u6d88\u6b67\uff0c\u5305\u542b25\u79cd\u6307\u9488\u8bbe\u8ba1\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u7528\u6237\u504f\u597d\u548c\u6027\u80fd\u3002", "motivation": "\u5728\u589e\u5f3a\u73b0\u5b9e(AR)\u4e2d\uff0c\u9488\u5bf9\u8fdc\u8ddd\u79bb\u7269\u4f53\u6216\u6742\u4e71\u573a\u666f\u7684\u67e5\u8be2\u5b58\u5728\u8f93\u5165\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u76ee\u6807\u6d88\u6b67\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u652f\u6301\u8fd9\u4e00\u8fc7\u7a0b\u7684\u524d\u9988\u53ef\u89c6\u5316\u6280\u672f\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u6784\u5efa\u5305\u542b25\u79cd\u6307\u9488\u7684\u6307\u9488\u7a7a\u95f4\uff0c\u57fa\u4e8e30\u5e74\u76f8\u5173\u6587\u732e\u5206\u6790\u73b0\u6709\u653e\u7f6e\u7b56\u7565\u548c\u89c6\u89c9\u7b26\u53f7\uff1b2. \u901a\u8fc7\u4e24\u4e2a\u5728\u7ebf\u5b9e\u9a8c\uff08n=60\u548c40\uff09\u8bc4\u4f30\u7528\u6237\u504f\u597d\u3001\u4fe1\u5fc3\u3001\u5fc3\u7406\u8f7b\u677e\u5ea6\u3001\u76ee\u6807\u53ef\u89c1\u6027\u548c\u53ef\u8bc6\u522b\u6027\uff1b3. \u6839\u636e\u5b9e\u9a8c\u7ed3\u679c\u9488\u5bf9\u4e0d\u540cAR\u573a\u666f\u548c\u6d88\u6b67\u6280\u672f\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u6d4b\u91cf\u4e86\u4e0d\u540c\u7269\u4f53\u8ddd\u79bb\u548c\u7a00\u758f\u5ea6\u4e0b\u7684\u7528\u6237\u504f\u597d\u3001\u4fe1\u5fc3\u3001\u5fc3\u7406\u8f7b\u677e\u5ea6\u3001\u76ee\u6807\u53ef\u89c1\u6027\u548c\u53ef\u8bc6\u522b\u6027\uff0c\u4e3a\u9009\u62e9\u4e0d\u540c\"\u4e0d\u786e\u5b9a\u6307\u9488\"\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u4ece\u7ed3\u679c\u4e2d\u63a8\u5bfc\u51fa\u57fa\u4e8eAR\u4e0a\u4e0b\u6587\u548c\u6d88\u6b67\u6280\u672f\u9009\u62e9\u4e0d\u540c\"\u4e0d\u786e\u5b9a\u6307\u9488\"\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4e3aAR\u7cfb\u7edf\u4e2d\u7684\u76ee\u6807\u6d88\u6b67\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6307\u5bfc\u3002"}}
{"id": "2602.13212", "categories": ["cs.RO", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13212", "abs": "https://arxiv.org/abs/2602.13212", "authors": ["Ziyi Zhang", "Xiyu Deng", "Guannan Qu", "Yorie Nakahira"], "title": "UAVGENT: A Language-Guided Distributed Control Framework", "comment": null, "summary": "We study language-in-the-loop control for multi-drone systems that execute evolving, high-level missions while retaining formal robustness guarantees at the physical layer. We propose a three-layer architecture in which (i) a human operator issues natural-language instructions, (ii) an LLM-based supervisor periodically interprets, verifies, and corrects the commanded task in the context of the latest state and target estimates, and (iii) a distributed inner-loop controller tracks the resulting reference using only local relative information. We derive a theoretical guarantee that characterizes tracking performance under bounded disturbances and piecewise-smooth references with discrete jumps induced by LLM updates. Overall, our results illustrate how centralized language-based task reasoning can be combined with distributed feedback control to achieve complex behaviors with provable robustness and stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u8bed\u8a00\u5728\u73af\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001LLM\u76d1\u7763\u548c\u5206\u5e03\u5f0f\u5185\u73af\u63a7\u5236\uff0c\u5728\u7269\u7406\u5c42\u4fdd\u6301\u5f62\u5f0f\u5316\u9c81\u68d2\u6027\u4fdd\u8bc1", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u8bed\u8a00\u5728\u73af\u63a7\u5236\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u6267\u884c\u4e0d\u65ad\u6f14\u5316\u7684\u9ad8\u5c42\u4efb\u52a1\uff0c\u540c\u65f6\u5728\u7269\u7406\u5c42\u4fdd\u6301\u5f62\u5f0f\u5316\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u89e3\u51b3\u8bed\u8a00\u6307\u4ee4\u4e0e\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\u4e4b\u95f4\u7684\u96c6\u6210\u95ee\u9898", "method": "\u63d0\u51fa\u4e09\u5c42\u67b6\u6784\uff1a1) \u4eba\u7c7b\u64cd\u4f5c\u5458\u53d1\u51fa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff1b2) LLM\u76d1\u7763\u5668\u5b9a\u671f\u89e3\u91ca\u3001\u9a8c\u8bc1\u548c\u4fee\u6b63\u6307\u4ee4\u4efb\u52a1\uff1b3) \u5206\u5e03\u5f0f\u5185\u73af\u63a7\u5236\u5668\u4ec5\u4f7f\u7528\u672c\u5730\u76f8\u5bf9\u4fe1\u606f\u8ddf\u8e2a\u751f\u6210\u7684\u53c2\u8003\u8f68\u8ff9", "result": "\u63a8\u5bfc\u4e86\u5728\u6709\u754c\u6270\u52a8\u548cLLM\u66f4\u65b0\u5f15\u8d77\u7684\u5206\u6bb5\u5149\u6ed1\u53c2\u8003\u8f68\u8ff9\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\u7406\u8bba\u4fdd\u8bc1\uff0c\u5c55\u793a\u4e86\u96c6\u4e2d\u5f0f\u8bed\u8a00\u4efb\u52a1\u63a8\u7406\u4e0e\u5206\u5e03\u5f0f\u53cd\u9988\u63a7\u5236\u76f8\u7ed3\u5408\u5b9e\u73b0\u590d\u6742\u884c\u4e3a\u7684\u53ef\u884c\u6027", "conclusion": "\u901a\u8fc7\u5c06\u96c6\u4e2d\u5f0f\u57fa\u4e8e\u8bed\u8a00\u7684\u4efb\u52a1\u63a8\u7406\u4e0e\u5206\u5e03\u5f0f\u53cd\u9988\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5177\u6709\u53ef\u8bc1\u660e\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u7684\u590d\u6742\u884c\u4e3a\uff0c\u4e3a\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u8bed\u8a00\u5728\u73af\u63a7\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.13486", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.13486", "abs": "https://arxiv.org/abs/2602.13486", "authors": ["Fei Wu", "Jia Hu", "Geyong Min", "Shiqiang Wang"], "title": "Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity", "comment": null, "summary": "Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86raFLoRA\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u4f4e\u79e9\u9002\u5e94\u4e2d\u7531\u4e8e\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u79e9\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u79e9\u5206\u533a\u805a\u5408\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u4fdd\u6301\u4e86\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u5728\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u5ba2\u6237\u7aef\u5728\u7cfb\u7edf\u8d44\u6e90\u548c\u6570\u636e\u5206\u5e03\u4e0a\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u4e0d\u540c\u5ba2\u6237\u7aef\u4f7f\u7528\u4e0d\u540c\u7684LoRA\u79e9\u3002\u4f5c\u8005\u53d1\u73b0\u4e86\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u73b0\u8c61\uff1a\u5728\u5f02\u6784FedLoRA\u4e2d\u4f1a\u51fa\u73b0\u79e9\u5d29\u6e83\uff0c\u5373\u5168\u5c40\u66f4\u65b0\u7684\u80fd\u91cf\u96c6\u4e2d\u5728\u6700\u5c0f\u5171\u4eab\u79e9\u4e0a\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u5bf9\u79e9\u914d\u7f6e\u7684\u9ad8\u654f\u611f\u6027\u3002", "method": "\u63d0\u51fa\u4e86raFLoRA\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u79e9\u5206\u533a\u805a\u5408\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u672c\u5730\u66f4\u65b0\u5206\u89e3\u4e3a\u79e9\u5206\u533a\uff0c\u7136\u540e\u6839\u636e\u6bcf\u4e2a\u5206\u533a\u7684\u6709\u6548\u5ba2\u6237\u7aef\u8d21\u732e\u8fdb\u884c\u52a0\u6743\u805a\u5408\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u79e9\u5d29\u6e83\u7684\u6839\u672c\u539f\u56e0\uff1a\u79e9\u65e0\u5173\u7684\u805a\u5408\u6743\u91cd\u4e0e\u79e9\u76f8\u5173\u7684\u5ba2\u6237\u7aef\u8d21\u732e\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "result": "\u5728\u5206\u7c7b\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0craFLoRA\u80fd\u591f\u9632\u6b62\u79e9\u5d29\u6e83\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u901a\u4fe1\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684FedLoRA\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "raFLoRA\u901a\u8fc7\u89e3\u51b3\u5f02\u6784\u8054\u90a6\u4f4e\u79e9\u9002\u5e94\u4e2d\u7684\u79e9\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u901a\u4fe1\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5f02\u8d28\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2602.14404", "categories": ["cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.14404", "abs": "https://arxiv.org/abs/2602.14404", "authors": ["William L. Tong", "Ege Cakar", "Cengiz Pehlevan"], "title": "Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces", "comment": "38 pages, 11 figures, code available at https://github.com/wtong98/boule-or-baguette", "summary": "Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PITA\u6570\u636e\u96c6\u6765\u7814\u7a76\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u63a8\u7406\u8f68\u8ff9\u6a21\u578b\u5728\u5bbd\u800c\u6d45\u7684\u4efb\u52a1\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u5728\u7a84\u800c\u6df1\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u8f68\u8ff9\u8303\u5f0f\u7684\u57fa\u672c\u4f18\u52bf\u548c\u9650\u5236\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u6a21\u578b\u8fd1\u5e74\u6765\u53d6\u5f97\u4e86\u98de\u901f\u8fdb\u5c55\uff0c\u4f46\u6211\u4eec\u5bf9\u63a8\u7406\u8f68\u8ff9\u5982\u4f55\u652f\u6301\u63a8\u7406\u4ee5\u53ca\u8be5\u8303\u5f0f\u7684\u5c40\u9650\u6027\u4ecd\u7f3a\u4e4f\u5b8c\u6574\u7406\u89e3\u3002\u4e3a\u4e86\u66f4\u6e05\u6670\u5730\u7406\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u521b\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6765\u7814\u7a76\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\u3002", "method": "1. \u5f15\u5165PITA\u6570\u636e\u96c6\uff1a\u5305\u542b\u8d85\u8fc72300\u4e07\u6761\u547d\u9898\u903b\u8f91\u8bed\u53e5\u53ca\u5176\u5bf9\u5e94\u8bc1\u660e\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u4efb\u52a1\u6df1\u5ea6\u548c\u4efb\u52a1\u5e7f\u5ea6\u7684\u6982\u5ff5\u6765\u8861\u91cf\u63a8\u7406\u590d\u6742\u5ea6\uff1b3. \u5728PITA\u7684\u4e0d\u540c\u5b50\u96c6\u4e0a\u53d8\u5316\u8fd9\u4e9b\u91cf\uff1b4. \u4e0e\u57fa\u4e8e\u4e09\u6bb5\u8bba\u7684\u7b80\u5355\u5408\u6210\u4efb\u52a1\u8fdb\u884c\u6bd4\u8f83\u9a8c\u8bc1\u7ed3\u679c\u7684\u666e\u904d\u6027\u3002", "result": "\u63a8\u7406\u8f68\u8ff9\u6a21\u578b\u5728\u5e7f\u5ea6\u548c\u6d45\u5ea6\u4efb\u52a1\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u5728\u7a84\u5ea6\u548c\u6df1\u5ea6\u4efb\u52a1\u4e0a\u76f8\u5bf9\u4e8e\u975e\u63a8\u7406\u8f68\u8ff9\u57fa\u7ebf\u8868\u73b0\u6076\u5316\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u63a8\u7406\u8f68\u8ff9\u6a21\u578b\u5728\u6df1\u5ea6\u4efb\u52a1\u4e0a\u7684\u57fa\u672c\u7f29\u653e\u9650\u5236\uff0c\u540c\u65f6\u7a81\u51fa\u4e86\u5b83\u4eec\u5728\u5e7f\u5ea6\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc6\u522b\u4e86\u4f7f\u7528\u63a8\u7406\u8f68\u8ff9\u7684\u5185\u5728\u57fa\u672c\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u9650\u5236\u63a8\u7406\u8f68\u8ff9\u6a21\u578b\u5728\u6df1\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u7684\u57fa\u672c\u7f29\u653e\u89c4\u5f8b\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u5e7f\u5ea6\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u4f18\u52bf\uff0c\u4e3a\u7406\u89e3\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.13835", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.13835", "abs": "https://arxiv.org/abs/2602.13835", "authors": ["Sripathi Sridhar", "Prem Seetharaman", "Oriol Nieto", "Mark Cartwright", "Justin Salamon"], "title": "Audiocards: Structured Metadata Improves Audio Language Models For Sound Design", "comment": "Accepted at ICASSP 2026", "summary": "Sound designers search for sounds in large sound effects libraries using aspects such as sound class or visual context. However, the metadata needed for such search is often missing or incomplete, and requires significant manual effort to add. Existing solutions to automate this task by generating metadata, i.e. captioning, and search using learned embeddings, i.e. text-audio retrieval, are not trained on metadata with the structure and information pertinent to sound design. To this end we propose audiocards, structured metadata grounded in acoustic attributes and sonic descriptors, by exploiting the world knowledge of LLMs. We show that training on audiocards improves downstream text-audio retrieval, descriptive captioning, and metadata generation on professional sound effects libraries. Moreover, audiocards also improve performance on general audio captioning and retrieval over the baseline single-sentence captioning approach. We release a curated dataset of sound effects audiocards to invite further research in audio language modeling for sound design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faaudiocards\u7ed3\u6784\u5316\u5143\u6570\u636e\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u4e16\u754c\u77e5\u8bc6\u751f\u6210\u57fa\u4e8e\u58f0\u5b66\u5c5e\u6027\u548c\u58f0\u97f3\u63cf\u8ff0\u7b26\u7684\u5143\u6570\u636e\uff0c\u6539\u5584\u58f0\u97f3\u8bbe\u8ba1\u7684\u6587\u672c-\u97f3\u9891\u68c0\u7d22\u548c\u63cf\u8ff0\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u58f0\u97f3\u8bbe\u8ba1\u5e08\u5728\u5927\u578b\u97f3\u6548\u5e93\u4e2d\u641c\u7d22\u58f0\u97f3\u65f6\u4f9d\u8d56\u5143\u6570\u636e\uff0c\u4f46\u73b0\u6709\u5143\u6570\u636e\u5f80\u5f80\u7f3a\u5931\u6216\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u3002\u73b0\u6709\u7684\u81ea\u52a8\u5143\u6570\u636e\u751f\u6210\u548c\u6587\u672c-\u97f3\u9891\u68c0\u7d22\u65b9\u6cd5\u6ca1\u6709\u9488\u5bf9\u58f0\u97f3\u8bbe\u8ba1\u7279\u6709\u7684\u7ed3\u6784\u548c\u4fe1\u606f\u8fdb\u884c\u8bad\u7ec3\u3002", "method": "\u63d0\u51faaudiocards\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e16\u754c\u77e5\u8bc6\u751f\u6210\u7ed3\u6784\u5316\u5143\u6570\u636e\uff0c\u8fd9\u4e9b\u5143\u6570\u636e\u57fa\u4e8e\u58f0\u5b66\u5c5e\u6027\u548c\u58f0\u97f3\u63cf\u8ff0\u7b26\u3002\u521b\u5efa\u4e86\u97f3\u6548audiocards\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u4e13\u4e1a\u97f3\u6548\u5e93\u4e0a\uff0c\u57fa\u4e8eaudiocards\u7684\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c-\u97f3\u9891\u68c0\u7d22\u3001\u63cf\u8ff0\u6027\u5b57\u5e55\u751f\u6210\u548c\u5143\u6570\u636e\u751f\u6210\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5728\u901a\u7528\u97f3\u9891\u5b57\u5e55\u548c\u68c0\u7d22\u4efb\u52a1\u4e0a\u4e5f\u4f18\u4e8e\u4f20\u7edf\u7684\u5355\u53e5\u5b57\u5e55\u65b9\u6cd5\u3002", "conclusion": "audiocards\u4e3a\u58f0\u97f3\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ed3\u6784\u5316\u5143\u6570\u636e\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528LLM\u7684\u4e16\u754c\u77e5\u8bc6\u6539\u5584\u4e86\u97f3\u9891\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2602.13370", "categories": ["cs.MA", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13370", "abs": "https://arxiv.org/abs/2602.13370", "authors": ["Karim Ben Khaled", "Davy Monticolo"], "title": "G2CP: A Graph-Grounded Communication Protocol for Verifiable and Efficient Multi-Agent Reasoning", "comment": null, "summary": "Multi-agent systems powered by Large Language Models face a critical challenge: agents communicate through natural language, leading to semantic drift, hallucination propagation, and inefficient token consumption. We propose G2CP (Graph-Grounded Communication Protocol), a structured agent communication language where messages are graph operations rather than free text. Agents exchange explicit traversal commands, subgraph fragments, and update operations over a shared knowledge graph, enabling verifiable reasoning traces and eliminating ambiguity. We validate G2CP within an industrial knowledge management system where specialized agents (Diagnostic, Procedural, Synthesis, and Ingestion) coordinate to answer complex queries. Experimental results on 500 industrial scenarios and 21 real-world maintenance cases show that G2CP reduces inter-agent communication tokens by 73%, improves task completion accuracy by 34% over free-text baselines, eliminates cascading hallucinations, and produces fully auditable reasoning chains. G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with implications for any domain requiring precise agent coordination. Code, data, and evaluation scripts are publicly available.", "AI": {"tldr": "G2CP\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u667a\u80fd\u4f53\u901a\u4fe1\u534f\u8bae\uff0c\u7528\u56fe\u64cd\u4f5c\u4ee3\u66ff\u81ea\u7136\u8bed\u8a00\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3001\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u6d88\u9664\u5e7b\u89c9\u4f20\u64ad\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u8bed\u4e49\u6f02\u79fb\u3001\u5e7b\u89c9\u4f20\u64ad\u548c\u4f4e\u6548\u4ee4\u724c\u6d88\u8017\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u7ed3\u6784\u5316\u901a\u4fe1\u65b9\u5f0f\u3002", "method": "\u63d0\u51faG2CP\uff08\u56fe\u57fa\u901a\u4fe1\u534f\u8bae\uff09\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u56fe\u64cd\u4f5c\uff08\u904d\u5386\u547d\u4ee4\u3001\u5b50\u56fe\u7247\u6bb5\u3001\u66f4\u65b0\u64cd\u4f5c\uff09\u5728\u5171\u4eab\u77e5\u8bc6\u56fe\u4e0a\u8fdb\u884c\u7ed3\u6784\u5316\u901a\u4fe1\uff0c\u800c\u975e\u81ea\u7531\u6587\u672c\u3002", "result": "\u5728500\u4e2a\u5de5\u4e1a\u573a\u666f\u548c21\u4e2a\u5b9e\u9645\u7ef4\u62a4\u6848\u4f8b\u4e2d\uff0cG2CP\u51cf\u5c1173%\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u4ee4\u724c\uff0c\u63d0\u9ad834%\u7684\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\uff0c\u6d88\u9664\u7ea7\u8054\u5e7b\u89c9\uff0c\u4ea7\u751f\u5b8c\u5168\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u94fe\u3002", "conclusion": "G2CP\u4ee3\u8868\u4e86\u4ece\u8bed\u8a00\u901a\u4fe1\u5230\u7ed3\u6784\u5316\u901a\u4fe1\u7684\u6839\u672c\u8f6c\u53d8\uff0c\u5bf9\u9700\u8981\u7cbe\u786e\u667a\u80fd\u4f53\u534f\u8c03\u7684\u4efb\u4f55\u9886\u57df\u90fd\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.14120", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14120", "abs": "https://arxiv.org/abs/2602.14120", "authors": ["Juan Carlos Carbajal", "Ahuva Mualem"], "title": "Evaluating the Performance of Approximation Mechanisms under Budget Constraints", "comment": null, "summary": "We study revenue maximization in a buyer-seller setting where the seller has a single object and the buyer has both a private valuation and a private budget. The presence of private budgets complicates the classic single-product monopoly problem, making optimal mechanisms difficult to analyze. To overcome this, we evaluate the robust performance of approximation mechanisms relative to optimal mechanisms. We work with three measures of performance: the guaranteed fraction of optimal revenue (GFOR) for restricted classes of mechanisms, the maximal value of relaxation (MVR) for relaxed classes, and a revenue non-monotonicity gap for either relaxed or restricted classes. Our analysis reveals sharp contrasts. On the positive side, we show that for distributions with bounded support, simple mechanisms with poly-logarithmic menu size can approximate optimal revenue arbitrarily well, regardless of correlation between valuations and budgets. On the negative side, we establish strong impossibility results: for distributions with unbounded support, or even bounded distributions concentrated in the unit square, no simple mechanism - or indeed any mechanism with a finite or sublinear menu - can guarantee a positive fraction of the optimal revenue. We also demonstrate unbounded revenue gains from certain relaxations when valuations and budgets are negatively correlated, and highlight cases of revenue non-monotonicity. Taken together, our results underscore the fragility of approximation approaches in the presence of private budgets: except for a narrow set of conditions, approximation mechanisms incur large revenue losses, pointing to fundamental limits of simplicity and robustness in mechanism design. Our analysis highlights that approximation results are highly sensitive to details of the design environment.", "AI": {"tldr": "\u7814\u7a76\u4e70\u65b9\u6709\u79c1\u6709\u4f30\u503c\u548c\u79c1\u6709\u9884\u7b97\u65f6\u7684\u6536\u5165\u6700\u5927\u5316\u95ee\u9898\uff0c\u53d1\u73b0\u8fd1\u4f3c\u673a\u5236\u5728\u79c1\u6709\u9884\u7b97\u5b58\u5728\u65f6\u8868\u73b0\u8106\u5f31\uff1a\u6709\u754c\u5206\u5e03\u4e0b\u7b80\u5355\u673a\u5236\u53ef\u8fd1\u4f3c\u6700\u4f18\u6536\u5165\uff0c\u4f46\u65e0\u754c\u5206\u5e03\u6216\u5355\u4f4d\u6b63\u65b9\u5f62\u96c6\u4e2d\u5206\u5e03\u4e0b\u4efb\u4f55\u6709\u9650\u83dc\u5355\u673a\u5236\u90fd\u65e0\u6cd5\u4fdd\u8bc1\u6b63\u6bd4\u4f8b\u7684\u6700\u4f18\u6536\u5165\u3002", "motivation": "\u79c1\u6709\u9884\u7b97\u7684\u5b58\u5728\u4f7f\u7ecf\u5178\u5355\u4ea7\u54c1\u5784\u65ad\u95ee\u9898\u590d\u6742\u5316\uff0c\u6700\u4f18\u673a\u5236\u96be\u4ee5\u5206\u6790\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd1\u4f3c\u673a\u5236\u76f8\u5bf9\u4e8e\u6700\u4f18\u673a\u5236\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u63a2\u8ba8\u5728\u79c1\u6709\u9884\u7b97\u7ea6\u675f\u4e0b\u7b80\u5355\u673a\u5236\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6027\u80fd\u5ea6\u91cf\uff1a\u53d7\u9650\u673a\u5236\u7c7b\u7684\u6700\u4f18\u6536\u5165\u4fdd\u8bc1\u6bd4\u4f8b(GFOR)\u3001\u677e\u5f1b\u673a\u5236\u7c7b\u7684\u6700\u5927\u677e\u5f1b\u503c(MVR)\u3001\u4ee5\u53ca\u677e\u5f1b\u6216\u53d7\u9650\u7c7b\u7684\u6536\u5165\u975e\u5355\u8c03\u6027\u95f4\u9699\u3002\u5206\u6790\u6709\u754c\u548c\u65e0\u754c\u5206\u5e03\u4e0b\u7684\u673a\u5236\u8868\u73b0\uff0c\u8003\u8651\u4f30\u503c\u548c\u9884\u7b97\u7684\u76f8\u5173\u6027\u3002", "result": "\u6709\u754c\u5206\u5e03\u4e0b\uff0c\u65e0\u8bba\u4f30\u503c\u548c\u9884\u7b97\u76f8\u5173\u6027\u5982\u4f55\uff0c\u5177\u6709\u591a\u5bf9\u6570\u83dc\u5355\u5927\u5c0f\u7684\u7b80\u5355\u673a\u5236\u53ef\u4efb\u610f\u597d\u5730\u8fd1\u4f3c\u6700\u4f18\u6536\u5165\u3002\u65e0\u754c\u5206\u5e03\u6216\u5355\u4f4d\u6b63\u65b9\u5f62\u96c6\u4e2d\u5206\u5e03\u4e0b\uff0c\u4efb\u4f55\u6709\u9650\u6216\u6b21\u7ebf\u6027\u83dc\u5355\u673a\u5236\u90fd\u65e0\u6cd5\u4fdd\u8bc1\u6b63\u6bd4\u4f8b\u7684\u6700\u4f18\u6536\u5165\u3002\u8d1f\u76f8\u5173\u65f6\u67d0\u4e9b\u677e\u5f1b\u53ef\u5e26\u6765\u65e0\u9650\u6536\u5165\u589e\u76ca\uff0c\u5b58\u5728\u6536\u5165\u975e\u5355\u8c03\u6027\u60c5\u51b5\u3002", "conclusion": "\u8fd1\u4f3c\u65b9\u6cd5\u5728\u79c1\u6709\u9884\u7b97\u5b58\u5728\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u9664\u5c11\u6570\u6761\u4ef6\u5916\uff0c\u8fd1\u4f3c\u673a\u5236\u4f1a\u5bfc\u81f4\u5927\u91cf\u6536\u5165\u635f\u5931\uff0c\u63ed\u793a\u4e86\u673a\u5236\u8bbe\u8ba1\u4e2d\u7b80\u5355\u6027\u548c\u9c81\u68d2\u6027\u7684\u57fa\u672c\u9650\u5236\u3002\u8fd1\u4f3c\u7ed3\u679c\u5bf9\u8bbe\u8ba1\u73af\u5883\u7ec6\u8282\u9ad8\u5ea6\u654f\u611f\u3002"}}
{"id": "2602.13469", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13469", "abs": "https://arxiv.org/abs/2602.13469", "authors": ["Ricardo E. Gonzalez Penuela", "Crescentia Jung", "Sharon Y Lin", "Ruiying Hu", "Shiri Azenkot"], "title": "How Multimodal Large Language Models Support Access to Visual Information: A Diary Study With Blind and Low Vision People", "comment": "24 pages, 17 figures, 3 tables, appendix section, to appear main track CHI 2026", "summary": "Multimodal large language models (MLLMs) are changing how Blind and Low Vision (BLV) people access visual information in their daily lives. Unlike traditional visual interpretation tools that provide access through captions and OCR (text recognition through camera input), MLLM-enabled applications support access through conversational assistance, where users can ask questions to obtain goal-relevant details. However, evidence about their performance in the real-world and their implications for BLV people's everyday life remain limited. To address this, we conducted a two-week diary study, where we captured 20 BLV participants' use of an MLLM-enabled visual interpretation application. Although participants rated the visual interpretations of the application as \"somewhat trustworthy\" (mean=3.76 out of 5, max=very trustworthy) and \"somewhat satisfying\" (mean=4.13 out of 5, max=very satisfying), the AI often produced incorrect answers (22.2%) or abstained (10.8%) from responding to follow-up requests. Our work demonstrates that MLLMs can improve the accuracy of descriptive visual interpretations, but that supporting everyday use also depends on the \"visual assistant\" skill -- a set of behaviors for providing goal-directed, reliable assistance. We conclude by proposing the \"visual assistant\" skill and practical guidelines to help future MLLM-enabled visual interpretation applications better support BLV people's access to visual information.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6b63\u5728\u6539\u53d8\u89c6\u969c\u4eba\u58eb\u83b7\u53d6\u89c6\u89c9\u4fe1\u606f\u7684\u65b9\u5f0f\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528\u4e2dAI\u7ecf\u5e38\u4ea7\u751f\u9519\u8bef\u7b54\u6848\u6216\u62d2\u7edd\u56de\u7b54\uff0c\u9700\u8981\u53d1\u5c55\"\u89c6\u89c9\u52a9\u624b\"\u6280\u80fd\u6765\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u76ee\u6807\u5bfc\u5411\u5e2e\u52a9\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u89e3\u91ca\u5de5\u5177\u901a\u8fc7\u5b57\u5e55\u548cOCR\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u89c6\u89c9\u4fe1\u606f\u8bbf\u95ee\uff0c\u800cMLLM\u5e94\u7528\u652f\u6301\u5bf9\u8bdd\u5f0f\u534f\u52a9\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u548c\u5bf9\u89c6\u969c\u4eba\u58eb\u65e5\u5e38\u751f\u6d3b\u5f71\u54cd\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u8fdb\u884c\u4e86\u4e3a\u671f\u4e24\u5468\u7684\u65e5\u8bb0\u7814\u7a76\uff0c\u6536\u96c6\u4e8620\u540d\u89c6\u969c\u53c2\u4e0e\u8005\u4f7f\u7528MLLM\u89c6\u89c9\u89e3\u91ca\u5e94\u7528\u7684\u6570\u636e\uff0c\u8bc4\u4f30\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u3001\u6ee1\u610f\u5ea6\u4ee5\u53caAI\u56de\u7b54\u7684\u51c6\u786e\u6027\u3002", "result": "\u53c2\u4e0e\u8005\u5bf9\u89c6\u89c9\u89e3\u91ca\u7684\u4fe1\u4efb\u5ea6\u4e3a3.76/5\uff08\u4e2d\u7b49\u4fe1\u4efb\uff09\uff0c\u6ee1\u610f\u5ea6\u4e3a4.13/5\uff08\u4e2d\u7b49\u6ee1\u610f\uff09\uff0c\u4f46AI\u7ecf\u5e38\u4ea7\u751f\u9519\u8bef\u7b54\u6848\uff0822.2%\uff09\u6216\u62d2\u7edd\u56de\u7b54\u540e\u7eed\u8bf7\u6c42\uff0810.8%\uff09\u3002", "conclusion": "MLLMs\u53ef\u4ee5\u63d0\u9ad8\u63cf\u8ff0\u6027\u89c6\u89c9\u89e3\u91ca\u7684\u51c6\u786e\u6027\uff0c\u4f46\u652f\u6301\u65e5\u5e38\u4f7f\u7528\u8fd8\u9700\u8981\"\u89c6\u89c9\u52a9\u624b\"\u6280\u80fd\u2014\u2014\u4e00\u5957\u63d0\u4f9b\u76ee\u6807\u5bfc\u5411\u3001\u53ef\u9760\u534f\u52a9\u7684\u884c\u4e3a\u3002\u63d0\u51fa\u4e86\"\u89c6\u89c9\u52a9\u624b\"\u6280\u80fd\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e2e\u52a9\u672a\u6765\u7684MLLM\u89c6\u89c9\u89e3\u91ca\u5e94\u7528\u66f4\u597d\u5730\u652f\u6301\u89c6\u969c\u4eba\u58eb\u83b7\u53d6\u89c6\u89c9\u4fe1\u606f\u3002"}}
{"id": "2602.13252", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.13252", "abs": "https://arxiv.org/abs/2602.13252", "authors": ["Xiaodong Zhang", "Baorui Lv", "Xavier Tao", "Xiong Wang", "Jie Bao", "Yong He", "Yue Chen", "Zijiang Yang"], "title": "DORA: Dataflow Oriented Robotic Architecture", "comment": null, "summary": "Robotic middleware serves as the foundational infrastructure, enabling complex robotic systems to operate in a coordinated and modular manner. In data-intensive robotic applications, especially in industrial scenarios, communication efficiency directly impact system responsiveness, stability, and overall productivity. However, existing robotic middleware exhibit several limitations: (1) they rely heavily on (de)serialization mechanisms, introducing significant overhead for large-sized data; (2) they lack efficient and flexible support for heterogeneous data sizes, particularly in intra-robot communication and Python-based execution environments. To address these challenges, we propose Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. We implement the proposed framework as an open-source system and evaluate it through extensive experiments in both simulation and real-world robotic environments. Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.", "AI": {"tldr": "DORA\u662f\u4e00\u79cd\u9762\u5411\u6570\u636e\u6d41\u7684\u673a\u5668\u4eba\u67b6\u6784\uff0c\u901a\u8fc7\u663e\u5f0f\u6570\u636e\u4f9d\u8d56\u89c4\u8303\u548c\u96f6\u62f7\u8d1d\u6570\u636e\u4f20\u8f93\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u5728\u5927\u6570\u636e\u91cf\u5904\u7406\u548c\u5f02\u6784\u6570\u636e\u652f\u6301\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a1\uff09\u4e25\u91cd\u4f9d\u8d56\uff08\u53cd\uff09\u5e8f\u5217\u5316\u673a\u5236\uff0c\u5904\u7406\u5927\u6570\u636e\u65f6\u5e26\u6765\u663e\u8457\u5f00\u9500\uff1b2\uff09\u7f3a\u4e4f\u5bf9\u5f02\u6784\u6570\u636e\u5927\u5c0f\u7684\u9ad8\u6548\u7075\u6d3b\u652f\u6301\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u5185\u90e8\u901a\u4fe1\u548cPython\u6267\u884c\u73af\u5883\u4e2d\u3002\u8fd9\u4e9b\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u6570\u636e\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u5e94\u7528\uff08\u5c24\u5176\u662f\u5de5\u4e1a\u573a\u666f\uff09\u7684\u54cd\u5e94\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6574\u4f53\u751f\u4ea7\u529b\u3002", "method": "\u63d0\u51fa\u6570\u636e\u6d41\u5bfc\u5411\u7684\u673a\u5668\u4eba\u67b6\u6784\uff08DORA\uff09\uff0c\u652f\u6301\u663e\u5f0f\u6570\u636e\u4f9d\u8d56\u89c4\u8303\u548c\u9ad8\u6548\u7684\u96f6\u62f7\u8d1d\u6570\u636e\u4f20\u8f93\u3002\u5c06\u6846\u67b6\u5b9e\u73b0\u4e3a\u5f00\u6e90\u7cfb\u7edf\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u4e2d\u95f4\u4ef6\u76f8\u6bd4\uff0cDORA\u5728\u5ef6\u8fdf\u548cCPU\u5f00\u9500\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "DORA\u901a\u8fc7\u6570\u636e\u6d41\u5bfc\u5411\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u5728\u5927\u6570\u636e\u5904\u7406\u548c\u5f02\u6784\u6570\u636e\u652f\u6301\u65b9\u9762\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u6570\u636e\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2602.13498", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13498", "abs": "https://arxiv.org/abs/2602.13498", "authors": ["Peng Cheng", "Jiucheng Zang", "Qingnan Li", "Liheng Ma", "Yufei Cui", "Yingxue Zhang", "Boxing Chen", "Ming Jian", "Wen Tong"], "title": "TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers", "comment": null, "summary": "Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\\textbf{T}rust \\textbf{R}egion \\textbf{A}daptive \\textbf{S}caling \\textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.", "AI": {"tldr": "TrasMuon\u4f18\u5316\u5668\u5728Muon\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u5168\u5c40RMS\u6821\u51c6\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u4fe1\u4efb\u533a\u57df\u88c1\u526a\uff0c\u65e2\u4fdd\u6301\u4e86Muon\u7684\u8fd1\u7b49\u8ddd\u51e0\u4f55\u7279\u6027\uff0c\u53c8\u89e3\u51b3\u4e86\u5176\u5bf9\u6b65\u957f\u8d85\u53c2\u6570\u654f\u611f\u548c\u6613\u53d7\u9ad8\u80fd\u91cf\u7206\u53d1\u5f71\u54cd\u7684\u95ee\u9898\u3002", "motivation": "Muon\u4f18\u5316\u5668\u4f7f\u7528\u725b\u987f-\u8212\u5c14\u8328\u8fed\u4ee3\u6b63\u4ea4\u5316\u66f4\u65b0\uff0c\u5728\u51e0\u4f55\u7279\u6027\u4e0a\u4f18\u4e8eAdam\u7cfb\u5217\u65b9\u6cd5\uff0c\u4f46\u6b63\u4ea4\u5316\u8fc7\u7a0b\u4e22\u5f03\u4e86\u5e45\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u8bad\u7ec3\u5bf9\u6b65\u957f\u8d85\u53c2\u6570\u654f\u611f\u4e14\u5bb9\u6613\u53d7\u5230\u9ad8\u80fd\u91cf\u7206\u53d1\u7684\u5f71\u54cd\u3002", "method": "TrasMuon\u901a\u8fc7\u4e24\u79cd\u673a\u5236\u6539\u8fdbMuon\uff1a1) \u5168\u5c40RMS\u6821\u51c6\u6765\u91cd\u65b0\u5f15\u5165\u81ea\u9002\u5e94\u7f29\u653e\uff0c2) \u57fa\u4e8e\u76f8\u5bf9\u80fd\u91cf\u6bd4\u7684\u4fe1\u4efb\u533a\u57df\u88c1\u526a\u6765\u9650\u5236\u66f4\u65b0\u5728\u7a33\u5b9a\u533a\u57df\u5185\uff0c\u9632\u6b62\u9ad8\u80fd\u91cf\u5f02\u5e38\u503c\u5bfc\u81f4\u7684\u5931\u7a33\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrasMuon\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u66f4\u5feb\u3002\u5728\u6ca1\u6709\u9884\u70ed\u9636\u6bb5\u7684\u5b9e\u9a8c\u4e2d\uff0cTrasMuon\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "TrasMuon\u6210\u529f\u89e3\u51b3\u4e86Muon\u4f18\u5316\u5668\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5176\u51e0\u4f55\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u548c\u4fe1\u4efb\u533a\u57df\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2602.14445", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.14445", "abs": "https://arxiv.org/abs/2602.14445", "authors": ["Hasi Hays"], "title": "Selective Synchronization Attention", "comment": null, "summary": "The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u540c\u6b65\u6ce8\u610f\u529b\uff08SSA\uff09\uff0c\u57fa\u4e8e\u8026\u5408\u632f\u5b50Kuramoto\u6a21\u578b\u7684\u7a33\u6001\u89e3\uff0c\u66ff\u4ee3Transformer\u4e2d\u7684\u70b9\u79ef\u81ea\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u81ea\u7136\u7a00\u758f\u6027\u3001\u7edf\u4e00\u4f4d\u7f6e\u8bed\u4e49\u7f16\u7801\u548c\u5355\u6b21\u95ed\u5f0f\u8ba1\u7b97\u3002", "motivation": "Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u751f\u7269\u5b66\u795e\u7ecf\u8ba1\u7b97\u57fa\u7840\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u5c06\u6bcf\u4e2atoken\u8868\u793a\u4e3a\u5177\u6709\u53ef\u5b66\u4e60\u81ea\u7136\u9891\u7387\u548c\u76f8\u4f4d\u7684\u632f\u5b50\uff0c\u57fa\u4e8e\u9891\u7387\u4f9d\u8d56\u8026\u5408\u548c\u76f8\u4f4d\u9501\u5b9a\u6761\u4ef6\u8ba1\u7b97\u540c\u6b65\u5f3a\u5ea6\u4f5c\u4e3a\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u5f62\u6210\u95ed\u5f0f\u8fd0\u7b97\u7b26\u3002", "result": "SSA\u5177\u6709\u81ea\u7136\u7a00\u758f\u6027\uff08\u76f8\u4f4d\u9501\u5b9a\u9608\u503c\uff09\u3001\u7edf\u4e00\u4f4d\u7f6e\u8bed\u4e49\u7f16\u7801\uff08\u81ea\u7136\u9891\u7387\u8c31\uff09\u3001\u5355\u6b21\u95ed\u5f0f\u8ba1\u7b97\u7b49\u4f18\u52bf\uff0c\u540c\u6b65\u77e9\u9635\u5206\u6790\u663e\u793a\u521d\u59cb\u5316\u65f6\u5373\u4ea7\u751f\u975e\u5747\u5300\u3001\u5934\u591a\u6837\u5316\u7684\u8026\u5408\u6a21\u5f0f\u3002", "conclusion": "SSA\u4e3aTransformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u632f\u5b50\u540c\u6b65\u7406\u8bba\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\uff0c\u53ef\u4f5c\u4e3aTransformer\u5757\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u3002"}}
{"id": "2602.13891", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13891", "abs": "https://arxiv.org/abs/2602.13891", "authors": ["Maohao Shen", "Tejas Jayashankar", "Osama Hanna", "Naoyuki Kanda", "Yancheng Wang", "Kate\u0159ina \u017dmol\u00edkov\u00e1", "Ruiming Xie", "Niko Moritz", "Anfeng Xu", "Yashesh Gaur", "Gregory Wornell", "Qing He", "Jilong Wu"], "title": "GSRM: Generative Speech Reward Model for Speech RLHF", "comment": null, "summary": "Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF.", "AI": {"tldr": "\u63d0\u51fa\u751f\u6210\u5f0f\u8bed\u97f3\u5956\u52b1\u6a21\u578bGSRM\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u58f0\u5b66\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u7279\u5f81\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u8bc4\u4f30\u8bed\u97f3\u81ea\u7136\u5ea6\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u5206\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o Voice Mode\u548cGemini Live\uff09\u7684\u751f\u6210\u8d28\u91cf\u867d\u6709\u8fdb\u6b65\uff0c\u4f46\u5408\u6210\u97f3\u9891\u7684\u7f8e\u5b66\u81ea\u7136\u5ea6\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u8bed\u97f3\u3002\u63d0\u5347\u751f\u6210\u8d28\u91cf\u9700\u8981\u53ef\u9760\u7684\u8bed\u97f3\u81ea\u7136\u5ea6\u8bc4\u4f30\u5668\uff0c\u800c\u73b0\u6709\u8bc4\u4f30\u5668\u901a\u5e38\u5c06\u539f\u59cb\u97f3\u9891\u56de\u5f52\u4e3a\u6807\u91cf\u5206\u6570\uff0c\u89e3\u91ca\u6027\u6709\u9650\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u5206\u7c7b\u7684\u8bed\u97f3\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u8bed\u97f3\u5956\u52b1\u6a21\u578bGSRM\uff0c\u91c7\u7528\u63a8\u7406\u4e2d\u5fc3\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002\u6a21\u578b\u5c06\u8bed\u97f3\u81ea\u7136\u5ea6\u8bc4\u4f30\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u53ef\u89e3\u91ca\u7684\u58f0\u5b66\u7279\u5f81\u63d0\u53d6\uff1b2\uff09\u57fa\u4e8e\u7279\u5f81\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5224\u65ad\u3002\u4e3a\u6b64\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4eba\u7c7b\u53cd\u9988\u6570\u636e\u96c6\uff0c\u5305\u542b31k\u4e13\u5bb6\u8bc4\u5206\u548c\u771f\u5b9e\u4e16\u754c\u7528\u6237-\u52a9\u624b\u8bed\u97f3\u4ea4\u4e92\u7684\u57df\u5916\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGSRM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bed\u97f3\u81ea\u7136\u5ea6\u9884\u6d4b\u5668\uff0c\u5176\u81ea\u7136\u5ea6\u5206\u6570\u9884\u6d4b\u7684\u6a21\u578b-\u4eba\u7c7b\u76f8\u5173\u6027\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u3002\u8fdb\u4e00\u6b65\u5c55\u793aGSRM\u53ef\u4f5c\u4e3a\u5728\u7ebfRLHF\u7684\u6709\u6548\u9a8c\u8bc1\u5668\uff0c\u63d0\u5347\u8bed\u97f3LLM\u751f\u6210\u7684\u81ea\u7136\u5ea6\u3002", "conclusion": "GSRM\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u58f0\u5b66\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u7279\u5f81\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8bed\u97f3\u81ea\u7136\u5ea6\u7684\u6709\u6548\u8bc4\u4f30\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u80fd\u4f5c\u4e3aRLHF\u7684\u9a8c\u8bc1\u5668\u63d0\u5347\u8bed\u97f3\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u8bed\u97f3\u81ea\u7136\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13671", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13671", "abs": "https://arxiv.org/abs/2602.13671", "authors": ["Guangyi Liu", "Haojun Lin", "Huan Zeng", "Heng Wang", "Quanming Yao"], "title": "MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time", "comment": null, "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or \"one-size-fits-all\" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness.", "AI": {"tldr": "MASFly\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684SOP\u5b9e\u4f8b\u5316\u548c\u7ecf\u9a8c\u5f15\u5bfc\u7684\u76d1\u7763\u673a\u5236\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u7684\u52a8\u6001\u81ea\u9002\u5e94\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u6216\"\u4e00\u5200\u5207\"\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u90e8\u7f72\u540e\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002\u53d7\u751f\u7269\u7cfb\u7edf\u81ea\u9002\u5e94\u80fd\u529b\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u52a8\u6001\u9002\u5e94\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "MASFly\u91c7\u7528\u53cc\u673a\u5236\uff1a1\uff09\u68c0\u7d22\u589e\u5f3a\u7684SOP\u5b9e\u4f8b\u5316\u673a\u5236\uff0c\u5229\u7528\u81ea\u6784\u5efa\u7684\u6210\u529f\u534f\u4f5c\u6a21\u5f0f\u5e93\uff0c\u4e3a\u65b0\u7684\u67e5\u8be2\u7ec4\u88c5\u5b9a\u5236\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1b2\uff09\u7ecf\u9a8c\u5f15\u5bfc\u7684\u76d1\u7763\u673a\u5236\uff0c\u4e13\u95e8\u7684Watcher\u667a\u80fd\u4f53\u76d1\u63a7\u7cfb\u7edf\u884c\u4e3a\uff0c\u53c2\u8003\u4e2a\u6027\u5316\u7ecf\u9a8c\u6c60\u63d0\u4f9b\u5b9e\u65f6\u5e72\u9884\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cMASFly\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728TravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523061.7%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u4efb\u52a1\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MASFly\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u89e3\u51b3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.14382", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14382", "abs": "https://arxiv.org/abs/2602.14382", "authors": ["Amit Shivam", "Kiran Kumari", "Fernando A. C. C. Fontes"], "title": "Prescribed-Performance-Aware Hybrid-Gain-Based Robust Controller", "comment": "Under reveiw in VSS 2026", "summary": "This paper proposes a prescribed performance function aware hybrid gain finite time sliding mode control framework for a class of nonlinear systems subject to matched disturbances. The hybrid gain structure ensures bounded control effort while retaining finite time convergence, and the incorporation of PPFs enables explicit enforcement of transient performance requirements. Theoretical guarantees are first established for first order systems, characterizing finite time convergence, disturbance rejection, and residual bounds. The approach is then extended to second order dynamics, where a sliding manifold is designed using PPF constraints to facilitate controlled shaping of position and velocity transients. Simulation studies illustrate the proposed design under matched peak control conditions. Comparative results for second-order systems demonstrate that, while a well tuned non-PPF hybrid gain controller achieves competitive tracking performance, the PPF-aware formulation strictly enforces prescribed transient constraints and yields consistent reductions of approximately 9 to 12 percent in integral error and control energy metrics without increasing peak actuation effort.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89c4\u5b9a\u6027\u80fd\u51fd\u6570\u548c\u6df7\u5408\u589e\u76ca\u7684\u6709\u9650\u65f6\u95f4\u6ed1\u6a21\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5e26\u6709\u5339\u914d\u6270\u52a8\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u80fd\u5728\u6709\u9650\u65f6\u95f4\u5185\u6536\u655b\u5e76\u4fdd\u8bc1\u6682\u6001\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u975e\u7ebf\u6027\u7cfb\u7edf\u5728\u5339\u914d\u6270\u52a8\u4e0b\u7684\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u6536\u655b\u3001\u63a7\u5236\u8f93\u5165\u6709\u754c\u4ee5\u53ca\u660e\u786e\u7684\u6682\u6001\u6027\u80fd\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df7\u5408\u589e\u76ca\u7ed3\u6784\u786e\u4fdd\u63a7\u5236\u8f93\u5165\u6709\u754c\u5e76\u4fdd\u6301\u6709\u9650\u65f6\u95f4\u6536\u655b\uff0c\u7ed3\u5408\u89c4\u5b9a\u6027\u80fd\u51fd\u6570\uff08PPF\uff09\u6765\u663e\u5f0f\u5f3a\u5236\u6267\u884c\u6682\u6001\u6027\u80fd\u8981\u6c42\u3002\u9996\u5148\u4e3a\u4e00\u9636\u7cfb\u7edf\u5efa\u7acb\u7406\u8bba\u4fdd\u8bc1\uff0c\u7136\u540e\u6269\u5c55\u5230\u4e8c\u9636\u7cfb\u7edf\uff0c\u901a\u8fc7PPF\u7ea6\u675f\u8bbe\u8ba1\u6ed1\u6a21\u6d41\u5f62\u6765\u7cbe\u786e\u63a7\u5236\u4f4d\u7f6e\u548c\u901f\u5ea6\u7684\u6682\u6001\u54cd\u5e94\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u76f8\u6bd4\u8c03\u4f18\u826f\u597d\u7684\u975ePPF\u6df7\u5408\u589e\u76ca\u63a7\u5236\u5668\uff0cPPF\u611f\u77e5\u65b9\u6cd5\u80fd\u4e25\u683c\u5f3a\u5236\u6267\u884c\u89c4\u5b9a\u7684\u6682\u6001\u7ea6\u675f\uff0c\u5728\u4e0d\u589e\u52a0\u5cf0\u503c\u6267\u884c\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u79ef\u5206\u8bef\u5dee\u548c\u63a7\u5236\u80fd\u91cf\u6307\u6807\u6301\u7eed\u964d\u4f4e\u7ea69-12%\u3002", "conclusion": "\u63d0\u51fa\u7684PPF\u611f\u77e5\u6df7\u5408\u589e\u76ca\u6709\u9650\u65f6\u95f4\u6ed1\u6a21\u63a7\u5236\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5339\u914d\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u6536\u655b\u3001\u63a7\u5236\u8f93\u5165\u6709\u754c\uff0c\u5e76\u80fd\u7cbe\u786e\u63a7\u5236\u6682\u6001\u6027\u80fd\uff0c\u5728\u591a\u79cd\u6027\u80fd\u6307\u6807\u4e0a\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2602.14223", "categories": ["cs.GT", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2602.14223", "abs": "https://arxiv.org/abs/2602.14223", "authors": ["Tim J. Boonen", "Kenneth Tsz Hin Ng", "Tak Wa Ng", "Thai Nguyen"], "title": "Pareto and Bowley Reinsurance Games in Peer-to-Peer Insurance", "comment": null, "summary": "We propose a peer-to-peer (P2P) insurance scheme comprising a risk-sharing pool and a reinsurer. A plan manager determines how risks are allocated among members and ceded to the reinsurer, while the reinsurer sets the reinsurance loading. Our work focuses on the strategic interaction between the plan manager and the reinsurer, and this focus leads to two game-theoretic contract designs: a Pareto design and a Bowley design, for which we derive closed-form optimal contracts. In the Pareto design, cooperation between the reinsurer and the plan manager leads to multiple Pareto-optimal contracts, which are further refined by introducing the notion of coalitional stability. In contrast, the Bowley design yields a unique optimal contract through a leader-follower framework, and we provide a rigorous verification of the individual rationality constraints via pointwise comparisons of payoff vectors. Comparing the two designs, we prove that the Bowley-optimal contract is never Pareto optimal and typically yields lower total welfare. In our numerical examples, the presence of reinsurance improves welfare, especially with Pareto designs and a less risk-averse reinsurer. We further analyze the impact of the single-loading restriction, which disproportionately favors members with riskier losses.", "AI": {"tldr": "\u63d0\u51faP2P\u4fdd\u9669\u65b9\u6848\uff0c\u5305\u542b\u98ce\u9669\u5171\u4eab\u6c60\u548c\u518d\u4fdd\u9669\u4eba\uff0c\u7814\u7a76\u8ba1\u5212\u7ecf\u7406\u4e0e\u518d\u4fdd\u9669\u4eba\u7684\u7b56\u7565\u4e92\u52a8\uff0c\u63a8\u5bfc\u4e24\u79cd\u535a\u5f08\u8bba\u5408\u540c\u8bbe\u8ba1\u7684\u95ed\u5f0f\u6700\u4f18\u89e3", "motivation": "\u7814\u7a76P2P\u4fdd\u9669\u4e2d\u8ba1\u5212\u7ecf\u7406\u4e0e\u518d\u4fdd\u9669\u4eba\u4e4b\u95f4\u7684\u7b56\u7565\u4e92\u52a8\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u8bbe\u8ba1\u6700\u4f18\u98ce\u9669\u5206\u914d\u5408\u540c", "method": "\u63d0\u51fa\u4e24\u79cd\u535a\u5f08\u8bba\u5408\u540c\u8bbe\u8ba1\uff1a\u5e15\u7d2f\u6258\u8bbe\u8ba1\u548c\u9c8d\u5229\u8bbe\u8ba1\u3002\u5e15\u7d2f\u6258\u8bbe\u8ba1\u57fa\u4e8e\u5408\u4f5c\u535a\u5f08\uff0c\u5f15\u5165\u8054\u76df\u7a33\u5b9a\u6027\u6982\u5ff5\uff1b\u9c8d\u5229\u8bbe\u8ba1\u91c7\u7528\u9886\u5bfc\u8005-\u8ffd\u968f\u8005\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u70b9\u6bd4\u8f83\u9a8c\u8bc1\u4e2a\u4f53\u7406\u6027\u7ea6\u675f", "result": "\u63a8\u5bfc\u51fa\u4e24\u79cd\u8bbe\u8ba1\u7684\u95ed\u5f0f\u6700\u4f18\u5408\u540c\uff0c\u8bc1\u660e\u9c8d\u5229\u6700\u4f18\u5408\u540c\u4ece\u4e0d\u5e15\u7d2f\u6258\u6700\u4f18\u4e14\u901a\u5e38\u4ea7\u751f\u66f4\u4f4e\u7684\u603b\u798f\u5229\u3002\u6570\u503c\u793a\u4f8b\u663e\u793a\u518d\u4fdd\u9669\u63d0\u9ad8\u798f\u5229\uff0c\u5c24\u5176\u5e15\u7d2f\u6258\u8bbe\u8ba1\u548c\u98ce\u9669\u538c\u6076\u7a0b\u5ea6\u8f83\u4f4e\u7684\u518d\u4fdd\u9669\u4eba", "conclusion": "P2P\u4fdd\u9669\u4e2d\u518d\u4fdd\u9669\u80fd\u6539\u5584\u798f\u5229\uff0c\u5e15\u7d2f\u6258\u8bbe\u8ba1\u4f18\u4e8e\u9c8d\u5229\u8bbe\u8ba1\uff0c\u5355\u4e00\u8d39\u7387\u9650\u5236\u5bf9\u9ad8\u98ce\u9669\u635f\u5931\u6210\u5458\u66f4\u6709\u5229"}}
{"id": "2602.13488", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13488", "abs": "https://arxiv.org/abs/2602.13488", "authors": ["Belu Ticona", "Amna Liaqat", "Antonios Anastasopoulos"], "title": "What Do We Mean by 'Pilot Study': Early Findings from a Meta-Review of Pilot Study Reporting at CHI", "comment": null, "summary": "Pilot studies (PS) are ubiquitous in HCI research. CHI papers routinely reference 'pilot studies', 'pilot tests', or 'preliminary studies' to justify design decisions, verify procedures, or motivate methodological choices. Yet despite their frequency, the role of pilot studies in HCI remains conceptually vague and empirically underexamined. Unlike fields such as medicine, nursing, and education, where pilot and feasibility studies have well-established definitions, guidelines, reporting standards and even a dedicated research journal, the CHI community lacks a shared understanding of what constitutes a pilot study, why they are conducted, and how they should be reported. Many papers reference pilots 'in passing', without details about design, outcomes, or how the pilot informed the main study. This variability suggests a methodological blind spot in our community.", "AI": {"tldr": "HCI\u9886\u57df\u666e\u904d\u4f7f\u7528\u8bd5\u70b9\u7814\u7a76\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u5b9a\u4e49\u3001\u6307\u5357\u548c\u62a5\u544a\u6807\u51c6\uff0c\u5bfc\u81f4\u6982\u5ff5\u6a21\u7cca\u3001\u62a5\u544a\u4e0d\u5b8c\u6574", "motivation": "\u5c3d\u7ba1\u8bd5\u70b9\u7814\u7a76\u5728HCI\u7814\u7a76\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u4f46\u5176\u89d2\u8272\u6982\u5ff5\u6a21\u7cca\u4e14\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\uff0c\u4e0e\u533b\u5b66\u3001\u62a4\u7406\u3001\u6559\u80b2\u7b49\u9886\u57df\u76f8\u6bd4\uff0cCHI\u793e\u533a\u7f3a\u4e4f\u5bf9\u8bd5\u70b9\u7814\u7a76\u7684\u7edf\u4e00\u5b9a\u4e49\u3001\u6307\u5357\u548c\u62a5\u544a\u6807\u51c6", "method": "\u901a\u8fc7\u5206\u6790CHI\u8bba\u6587\u4e2d\u8bd5\u70b9\u7814\u7a76\u7684\u5f15\u7528\u60c5\u51b5\uff0c\u63ed\u793a\u5f53\u524dHCI\u9886\u57df\u5bf9\u8bd5\u70b9\u7814\u7a76\u7684\u6982\u5ff5\u7406\u89e3\u3001\u5b9e\u65bd\u76ee\u7684\u548c\u62a5\u544a\u65b9\u5f0f\u7684\u73b0\u72b6", "result": "\u53d1\u73b0CHI\u8bba\u6587\u4e2d\u8bd5\u70b9\u7814\u7a76\u5f15\u7528\u666e\u904d\u4f46\u7f3a\u4e4f\u7ec6\u8282\uff0c\u8bb8\u591a\u8bba\u6587\u4ec5\"\u987a\u4fbf\u63d0\u53ca\"\u8bd5\u70b9\u7814\u7a76\uff0c\u6ca1\u6709\u63d0\u4f9b\u8bbe\u8ba1\u3001\u7ed3\u679c\u6216\u5982\u4f55\u5f71\u54cd\u4e3b\u7814\u7a76\u7684\u4fe1\u606f", "conclusion": "HCI\u793e\u533a\u5728\u8bd5\u70b9\u7814\u7a76\u65b9\u9762\u5b58\u5728\u65b9\u6cd5\u8bba\u76f2\u70b9\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u5b9a\u4e49\u3001\u6307\u5357\u548c\u62a5\u544a\u6807\u51c6\uff0c\u4ee5\u63d0\u9ad8\u7814\u7a76\u900f\u660e\u5ea6\u548c\u8d28\u91cf"}}
{"id": "2602.13436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13436", "abs": "https://arxiv.org/abs/2602.13436", "authors": ["Noah Rubin", "Ava Schraeder", "Hrishikesh Sahu", "Thomas C. Bulea", "Lillian Chin"], "title": "High-Fidelity, Customizable Force Sensing for the Wearable Human-Robot Interface", "comment": "6 pages, 7 figures, submitted to BioRob 2026", "summary": "Mechanically characterizing the human-machine interface is essential to understanding user behavior and optimizing wearable robot performance. This interface has been challenging to sensorize due to manufacturing complexity and non-linear sensor responses. Here, we measure human limb-device interaction via fluidic innervation, creating a 3D-printed silicone pad with embedded air channels to measure forces. As forces are applied to the pad, the air channels compress, resulting in a pressure change measurable by off-the-shelf pressure transducers. We demonstrate in benchtop testing that pad pressure is highly linearly related to applied force ($R^2 = 0.998$). This is confirmed with clinical dynamometer correlations with isometric knee torque, where above-knee pressure was highly correlated with flexion torque ($R^2 = 0.95$), while below-knee pressure was highly correlated with extension torque ($R^2 = 0.75$). We build on these idealized settings to test pad performance in more unconstrained settings. We place the pad over \\textit{biceps brachii} during cyclic curls and stepwise isometric holds, observing a correlation between pressure and elbow angle. Finally, we integrated the sensor into the strap of a lower-extremity robotic exoskeleton and recorded pad pressure during repeated squats with the device unpowered. Pad pressure tracked squat phase and overall task dynamics consistently. Overall, our preliminary results suggest fluidic innervation is a readily customizable sensing modality with high signal-to-noise ratio and temporal resolution for capturing human-machine mechanical interaction. In the long-term, this modality may provide an alternative real-time sensing input to control / optimize wearable robotic systems and to capture user function during device use.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u4f53\u795e\u7ecf\u652f\u914d\u76843D\u6253\u5370\u7845\u80f6\u57ab\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u6d4b\u91cf\u4eba\u673a\u754c\u9762\u7684\u673a\u68b0\u4ea4\u4e92\u529b\uff0c\u8be5\u4f20\u611f\u5668\u901a\u8fc7\u5d4c\u5165\u5f0f\u7a7a\u6c14\u901a\u9053\u538b\u7f29\u4ea7\u751f\u7684\u538b\u529b\u53d8\u5316\u6765\u6d4b\u91cf\u529b\uff0c\u5728\u591a\u79cd\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7ebf\u6027\u5173\u7cfb\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u4eba\u673a\u754c\u9762\u7684\u673a\u68b0\u7279\u6027\u8868\u5f81\u5bf9\u4e8e\u7406\u89e3\u7528\u6237\u884c\u4e3a\u548c\u4f18\u5316\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5236\u9020\u590d\u6742\u6027\u548c\u4f20\u611f\u5668\u975e\u7ebf\u6027\u54cd\u5e94\uff0c\u8fd9\u4e00\u754c\u9762\u4e00\u76f4\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u4f20\u611f\u3002", "method": "\u91c7\u7528\u6d41\u4f53\u795e\u7ecf\u652f\u914d\u65b9\u6cd5\uff0c\u521b\u5efa\u5e26\u6709\u5d4c\u5165\u5f0f\u7a7a\u6c14\u901a\u9053\u76843D\u6253\u5370\u7845\u80f6\u57ab\uff0c\u5f53\u529b\u65bd\u52a0\u5230\u57ab\u5b50\u4e0a\u65f6\uff0c\u7a7a\u6c14\u901a\u9053\u538b\u7f29\u4ea7\u751f\u53ef\u6d4b\u91cf\u7684\u538b\u529b\u53d8\u5316\uff0c\u4f7f\u7528\u73b0\u6210\u7684\u538b\u529b\u4f20\u611f\u5668\u8fdb\u884c\u6d4b\u91cf\u3002", "result": "\u53f0\u67b6\u6d4b\u8bd5\u663e\u793a\u57ab\u5b50\u538b\u529b\u4e0e\u65bd\u52a0\u529b\u9ad8\u5ea6\u7ebf\u6027\u76f8\u5173\uff08R\u00b2=0.998\uff09\uff1b\u4e34\u5e8a\u6d4b\u529b\u8ba1\u6d4b\u8bd5\u663e\u793a\u819d\u4e0a\u538b\u529b\u4e0e\u5c48\u66f2\u626d\u77e9\u9ad8\u5ea6\u76f8\u5173\uff08R\u00b2=0.95\uff09\uff0c\u819d\u4e0b\u538b\u529b\u4e0e\u4f38\u5c55\u626d\u77e9\u9ad8\u5ea6\u76f8\u5173\uff08R\u00b2=0.75\uff09\uff1b\u5728\u80b1\u4e8c\u5934\u808c\u6d4b\u8bd5\u4e2d\u89c2\u5bdf\u5230\u538b\u529b\u4e0e\u8098\u5173\u8282\u89d2\u5ea6\u7684\u76f8\u5173\u6027\uff1b\u96c6\u6210\u5230\u5916\u9aa8\u9abc\u540e\u80fd\u7a33\u5b9a\u8ddf\u8e2a\u6df1\u8e72\u76f8\u4f4d\u548c\u4efb\u52a1\u52a8\u6001\u3002", "conclusion": "\u6d41\u4f53\u795e\u7ecf\u652f\u914d\u662f\u4e00\u79cd\u6613\u4e8e\u5b9a\u5236\u7684\u4f20\u611f\u65b9\u5f0f\uff0c\u5177\u6709\u9ad8\u4fe1\u566a\u6bd4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u53ef\u7528\u4e8e\u6355\u6349\u4eba\u673a\u673a\u68b0\u4ea4\u4e92\uff0c\u957f\u671f\u6765\u770b\u53ef\u80fd\u4e3a\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236/\u4f18\u5316\u63d0\u4f9b\u66ff\u4ee3\u4f20\u611f\u8f93\u5165\u3002"}}
{"id": "2602.13878", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13878", "abs": "https://arxiv.org/abs/2602.13878", "authors": ["Martina Baiardi", "Samuele Burattini", "Giovanni Ciatto", "Danilo Pianini"], "title": "Testing BDI-based Multi-Agent Systems using Discrete Event Simulation", "comment": null, "summary": "Multi-agent systems are designed to deal with open, distributed systems with unpredictable dynamics, which makes them inherently hard to test. The value of using simulation for this purpose is recognized in the literature, although achieving sufficient fidelity (i.e., the degree of similarity between the simulation and the real-world system) remains a challenging task. This is exacerbated when dealing with cognitive agent models, such as the Belief Desire Intention (BDI) model, where the agent codebase is not suitable to run unchanged in simulation environments, thus increasing the reality gap between the deployed and simulated systems. We argue that BDI developers should be able to test in simulation the same specification that will be later deployed, with no surrogate representations. Thus, in this paper, we discuss how the control flow of BDI agents can be mapped onto a Discrete Event Simulation (DES), showing that such integration is possible at different degrees of granularity. We substantiate our claims by producing an open-source prototype integration between two pre-existing tools (JaKtA and Alchemist), showing that it is possible to produce a simulation-based testing environment for distributed BDI} agents, and that different granularities in mapping BDI agents over DESs may lead to different degrees of fidelity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u5c06BDI\u667a\u80fd\u4f53\u6620\u5c04\u5230\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u4e2d\uff0c\u5b9e\u73b0\u4eff\u771f\u6d4b\u8bd5\u73af\u5883\uff0c\u89e3\u51b3\u8ba4\u77e5\u667a\u80fd\u4f53\u6a21\u578b\u5728\u4eff\u771f\u4e2d\u7684\u4fdd\u771f\u5ea6\u95ee\u9898\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5f00\u653e\u3001\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u96be\u4ee5\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u5bf9\u4e8eBDI\u7b49\u8ba4\u77e5\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u5176\u4ee3\u7801\u5e93\u4e0d\u9002\u5408\u76f4\u63a5\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5bfc\u81f4\u90e8\u7f72\u7cfb\u7edf\u4e0e\u4eff\u771f\u7cfb\u7edf\u4e4b\u95f4\u5b58\u5728\u73b0\u5b9e\u5dee\u8ddd\u3002\u9700\u8981\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u5728\u4eff\u771f\u4e2d\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u76f8\u540c\u7684\u89c4\u8303\uff0c\u800c\u65e0\u9700\u66ff\u4ee3\u8868\u793a\u3002", "method": "\u7814\u7a76\u5982\u4f55\u5c06BDI\u667a\u80fd\u4f53\u7684\u63a7\u5236\u6d41\u6620\u5c04\u5230\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u4e2d\uff0c\u5c55\u793a\u8fd9\u79cd\u96c6\u6210\u53ef\u4ee5\u5728\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u5b9e\u73b0\u3002\u901a\u8fc7\u5c06\u4e24\u4e2a\u73b0\u6709\u5de5\u5177\uff08JaKtA\u548cAlchemist\uff09\u8fdb\u884c\u5f00\u6e90\u539f\u578b\u96c6\u6210\uff0c\u6784\u5efa\u5206\u5e03\u5f0fBDI\u667a\u80fd\u4f53\u7684\u4eff\u771f\u6d4b\u8bd5\u73af\u5883\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u57fa\u4e8e\u4eff\u771f\u7684\u5206\u5e03\u5f0fBDI\u667a\u80fd\u4f53\u6d4b\u8bd5\u73af\u5883\uff0c\u8bc1\u660e\u5c06BDI\u667a\u80fd\u4f53\u6620\u5c04\u5230\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u4e0d\u540c\u7c92\u5ea6\u7684\u6620\u5c04\u53ef\u80fd\u5bfc\u81f4\u4e0d\u540c\u7a0b\u5ea6\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5c06BDI\u667a\u80fd\u4f53\u63a7\u5236\u6d41\u6620\u5c04\u5230\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4eff\u771f\u6d4b\u8bd5\u73af\u5883\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u76f8\u540c\u7684\u89c4\u8303\uff0c\u89e3\u51b3\u8ba4\u77e5\u667a\u80fd\u4f53\u6a21\u578b\u5728\u4eff\u771f\u4e2d\u7684\u4fdd\u771f\u5ea6\u6311\u6218\u3002"}}
{"id": "2602.14436", "categories": ["eess.SY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14436", "abs": "https://arxiv.org/abs/2602.14436", "authors": ["Jaehan Im", "David Fridovich-Keil", "Ufuk Topcu"], "title": "Noncooperative Virtual Queue Coordination via Uncertainty-Aware Correlated Equilibria", "comment": null, "summary": "Collaborative virtual queueing has been proposed as a mechanism to mitigate airport surface congestion while preserving airline autonomy over aircraft-level pushback decisions. A central coordinator can regulate aggregate pushback capacity but cannot directly control which specific aircraft are released, limiting its ability to steer system-level performance. We propose a noncooperative coordination mechanism for collaborative virtual queueing based on the correlated equilibrium concept, which enables the coordinator to provide incentive-compatible recommendations on aircraft-level pushback decisions without overriding airline autonomy. To account for uncertainty in airlines' internal cost assessments, we introduce chance constraints into the correlated equilibrium formulation. This formulation provides explicit probabilistic guarantees on incentive compatibility, allowing the coordinator to adjust the confidence level with which airlines are expected to follow the recommended actions. We further propose a scalable algorithm for computing chance-constrained correlated equilibria by exploiting a reduced-rank structure. Numerical experiments demonstrate that the proposed method scales to realistic traffic levels up to 210 eligible pushbacks per hour, reduces accumulated delay by up to approximately 8.9% compared to current first-come-first-served schemes, and reveals a trade-off between confidence level, deviation robustness, and achievable cost efficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u5173\u5747\u8861\u7684\u534f\u4f5c\u865a\u62df\u6392\u961f\u673a\u5236\uff0c\u901a\u8fc7\u6982\u7387\u4fdd\u8bc1\u7684\u6fc0\u52b1\u517c\u5bb9\u63a8\u8350\u6765\u7f13\u89e3\u673a\u573a\u62e5\u5835\uff0c\u540c\u65f6\u4fdd\u6301\u822a\u7a7a\u516c\u53f8\u81ea\u4e3b\u6743", "motivation": "\u73b0\u6709\u534f\u4f5c\u865a\u62df\u6392\u961f\u673a\u5236\u4e2d\uff0c\u4e2d\u592e\u534f\u8c03\u5458\u53ea\u80fd\u8c03\u8282\u603b\u4f53\u63a8\u51fa\u5bb9\u91cf\uff0c\u65e0\u6cd5\u63a7\u5236\u5177\u4f53\u822a\u73ed\u63a8\u51fa\u51b3\u7b56\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u80fd\u529b", "method": "\u63d0\u51fa\u57fa\u4e8e\u76f8\u5173\u5747\u8861\u6982\u5ff5\u7684\u975e\u5408\u4f5c\u534f\u8c03\u673a\u5236\uff0c\u5f15\u5165\u673a\u4f1a\u7ea6\u675f\u5904\u7406\u822a\u7a7a\u516c\u53f8\u5185\u90e8\u6210\u672c\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7b97\u6cd5\u8ba1\u7b97\u673a\u4f1a\u7ea6\u675f\u76f8\u5173\u5747\u8861", "result": "\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u6bcf\u5c0f\u65f6210\u4e2a\u822a\u73ed\uff0c\u76f8\u6bd4\u5148\u5230\u5148\u670d\u52a1\u65b9\u6848\u51cf\u5c11\u7ea68.9%\u7d2f\u79ef\u5ef6\u8bef\uff0c\u63ed\u793a\u4e86\u7f6e\u4fe1\u6c34\u5e73\u3001\u504f\u79bb\u9c81\u68d2\u6027\u548c\u6210\u672c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861", "conclusion": "\u57fa\u4e8e\u673a\u4f1a\u7ea6\u675f\u76f8\u5173\u5747\u8861\u7684\u534f\u4f5c\u865a\u62df\u6392\u961f\u673a\u5236\u80fd\u5728\u4fdd\u6301\u822a\u7a7a\u516c\u53f8\u81ea\u4e3b\u6743\u7684\u540c\u65f6\u6709\u6548\u7f13\u89e3\u673a\u573a\u62e5\u5835\uff0c\u63d0\u4f9b\u6fc0\u52b1\u517c\u5bb9\u7684\u822a\u73ed\u63a8\u51fa\u63a8\u8350"}}
{"id": "2602.14278", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14278", "abs": "https://arxiv.org/abs/2602.14278", "authors": ["Mayank Kejriwal", "Shilpa Thomas", "Hongyu Li"], "title": "Characterizing Robustness of Strategies to Novelty in Zero-Sum Open Worlds", "comment": "25 pages, 10 tables, 15 figures", "summary": "In open-world environments, artificial agents must often contend with novel conditions that deviate from their training or design assumptions. This paper studies the robustness of fixed-strategy agents to such novelty within the setting of two-player zero-sum games. We present a general framework for characterizing the impact of environmental novelties, such as changes in payoff structure or action constraints, on agent performance in two distinct domains: Iterated Prisoner's Dilemma (IPD) and heads-up Texas Hold'em Poker. Novelty is operationalized as a perturbation of the game's rules or scoring mechanics, while agent behavior remains fixed. To measure the effects, we introduce two metrics: per-agent robustness, quantifying the relative performance shift of each strategy across novelties, and global impact, summarizing the population-wide disruption caused by a novelty. Our experiments, comprising 30 IPD agents across 20 payoff matrix novelties and 10 Poker agents across 5 rule-based novelties, reveal systematic patterns in robustness and highlight certain novelties that induce severe destabilization. The results offer insights into agent generalizability under perturbation and provide a quantitative basis for designing safer and more resilient autonomous systems in adversarial and dynamic environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u56fa\u5b9a\u7b56\u7565\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5bf9\u6e38\u620f\u89c4\u5219\u65b0\u9896\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\u6846\u67b6\uff0c\u5728\u56da\u5f92\u56f0\u5883\u548c\u5fb7\u5dde\u6251\u514b\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91cf\u5316\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u53d8\u5316\u548c\u5168\u5c40\u5f71\u54cd\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u4eba\u5de5\u667a\u80fd\u4f53\u7ecf\u5e38\u9762\u4e34\u504f\u79bb\u8bad\u7ec3\u6216\u8bbe\u8ba1\u5047\u8bbe\u7684\u65b0\u9896\u6761\u4ef6\u3002\u7814\u7a76\u56fa\u5b9a\u7b56\u7565\u667a\u80fd\u4f53\u5728\u53cc\u4eba\u96f6\u548c\u6e38\u620f\u4e2d\u5bf9\u73af\u5883\u65b0\u9896\u6027\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u66f4\u5177\u5f39\u6027\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5c06\u65b0\u9896\u6027\u64cd\u4f5c\u5316\u4e3a\u6e38\u620f\u89c4\u5219\u6216\u8ba1\u5206\u673a\u5236\u7684\u6270\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u667a\u80fd\u4f53\u884c\u4e3a\u56fa\u5b9a\u3002\u5f15\u5165\u4e24\u4e2a\u5ea6\u91cf\u6307\u6807\uff1a\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u9c81\u68d2\u6027\uff08\u91cf\u5316\u7b56\u7565\u5728\u65b0\u9896\u6027\u4e0b\u7684\u76f8\u5bf9\u6027\u80fd\u53d8\u5316\uff09\u548c\u5168\u5c40\u5f71\u54cd\uff08\u603b\u7ed3\u65b0\u9896\u6027\u5f15\u8d77\u7684\u7fa4\u4f53\u8303\u56f4\u6270\u52a8\uff09\u3002\u5728\u8fed\u4ee3\u56da\u5f92\u56f0\u5883\uff0830\u4e2a\u667a\u80fd\u4f53\uff0c20\u79cd\u6536\u76ca\u77e9\u9635\u65b0\u9896\u6027\uff09\u548c\u5fb7\u5dde\u6251\u514b\uff0810\u4e2a\u667a\u80fd\u4f53\uff0c5\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u65b0\u9896\u6027\uff09\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u6027\u6a21\u5f0f\uff0c\u5e76\u8bc6\u522b\u51fa\u67d0\u4e9b\u80fd\u5f15\u8d77\u4e25\u91cd\u4e0d\u7a33\u5b9a\u7684\u65b0\u9896\u6027\u3002\u5728\u56da\u5f92\u56f0\u5883\u548c\u5fb7\u5dde\u6251\u514b\u4e2d\u90fd\u89c2\u5bdf\u5230\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u7684\u663e\u8457\u53d8\u5316\uff0c\u67d0\u4e9b\u65b0\u9896\u6027\u5bf9\u667a\u80fd\u4f53\u7fa4\u4f53\u4ea7\u751f\u4e86\u4e25\u91cd\u7684\u7834\u574f\u6027\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u5bf9\u6297\u6027\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u66f4\u5177\u5f39\u6027\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9a\u91cf\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5728\u6270\u52a8\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u7cfb\u7edf\u8bc4\u4f30\u73af\u5883\u53d8\u5316\u5bf9\u56fa\u5b9a\u7b56\u7565\u667a\u80fd\u4f53\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.13598", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.13598", "abs": "https://arxiv.org/abs/2602.13598", "authors": ["Zhanming Chen", "Alisha Ghaju", "May Hang", "Juan F. Maestre", "Ji Youn Shin"], "title": "Designing Health Technologies for Immigrant Communities: Exploring Healthcare Providers' Communication Strategies with Patients", "comment": "19 pages, Conference", "summary": "Patient-provider communication is an important aspect of successful healthcare, as it can directly lead to positive health outcomes. Previous studies examined factors that facilitate communication between healthcare providers and patients in socially marginalized communities, especially developing countries, and applied identified factors to technology development. However, there is limited understanding of how providers work with patients from immigrant populations in a developed country. By conducting semi-structured interviews with 15 providers working with patients from an immigrant community with unique cultural characteristics, we identified providers' effective communication strategies, including acknowledgment, community involvement, gradual care, and adaptive communication practices (i.e., adjusting the communication style). Based on our findings, we highlight cultural competence and discuss design implications for technologies to support health communication in immigrant communities. Our suggestions propose approaches for HCI researchers to identify practical, contextualized cultural competence for their health technology design.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u53d1\u8fbe\u56fd\u5bb6\u4e2d\uff0c\u533b\u7597\u670d\u52a1\u63d0\u4f9b\u8005\u5982\u4f55\u4e0e\u5177\u6709\u72ec\u7279\u6587\u5316\u7279\u5f81\u7684\u79fb\u6c11\u60a3\u8005\u7fa4\u4f53\u8fdb\u884c\u6709\u6548\u6c9f\u901a\uff0c\u5e76\u63d0\u51fa\u4e86\u652f\u6301\u79fb\u6c11\u793e\u533a\u5065\u5eb7\u6c9f\u901a\u7684\u6280\u672f\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u533b\u60a3\u6c9f\u901a\u5bf9\u533b\u7597\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u53d1\u5c55\u4e2d\u56fd\u5bb6\u8fb9\u7f18\u5316\u793e\u533a\u7684\u533b\u60a3\u6c9f\u901a\uff0c\u7f3a\u4e4f\u5bf9\u53d1\u8fbe\u56fd\u5bb6\u79fb\u6c11\u60a3\u8005\u7fa4\u4f53\u6c9f\u901a\u7b56\u7565\u7684\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf915\u540d\u4e0e\u5177\u6709\u72ec\u7279\u6587\u5316\u7279\u5f81\u7684\u79fb\u6c11\u793e\u533a\u60a3\u8005\u5de5\u4f5c\u7684\u533b\u7597\u670d\u52a1\u63d0\u4f9b\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u8bc6\u522b\u6709\u6548\u7684\u6c9f\u901a\u7b56\u7565\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u6709\u6548\u7684\u6c9f\u901a\u7b56\u7565\uff1a\u8ba4\u53ef\u3001\u793e\u533a\u53c2\u4e0e\u3001\u6e10\u8fdb\u5f0f\u62a4\u7406\u548c\u9002\u5e94\u6027\u6c9f\u901a\u5b9e\u8df5\uff08\u5373\u8c03\u6574\u6c9f\u901a\u98ce\u683c\uff09\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6587\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u8ba8\u8bba\u4e86\u652f\u6301\u79fb\u6c11\u793e\u533a\u5065\u5eb7\u6c9f\u901a\u7684\u6280\u672f\u8bbe\u8ba1\u542f\u793a\uff0c\u4e3aHCI\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u8bc6\u522b\u5b9e\u7528\u3001\u60c5\u5883\u5316\u6587\u5316\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u6307\u5bfc\u5065\u5eb7\u6280\u672f\u8bbe\u8ba1\u3002"}}
{"id": "2602.13444", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13444", "abs": "https://arxiv.org/abs/2602.13444", "authors": ["Huajian Zeng", "Lingyun Chen", "Jiaqi Yang", "Yuantai Zhang", "Fan Shi", "Peidong Liu", "Xingxing Zuo"], "title": "FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation", "comment": "Project Page: https://huajian-zeng.github.io/projects/flowhoi/", "summary": "Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.", "AI": {"tldr": "FlowHOI\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6d41\u5339\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8bed\u4e49\u57fa\u7840\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u624b-\u7269\u4ea4\u4e92\u5e8f\u5217\uff0c\u5305\u62ec\u624b\u90e8\u59ff\u6001\u3001\u7269\u4f53\u59ff\u6001\u548c\u63a5\u89e6\u72b6\u6001\uff0c\u57fa\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c2\u5bdf\u3001\u8bed\u8a00\u6307\u4ee4\u548c3D\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u5408\u7406\u7684\u672b\u7aef\u6267\u884c\u5668\u8fd0\u52a8\uff0c\u4f46\u5728\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u4e2d\u5e38\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u660e\u786e\u7684\u624b-\u7269\u4ea4\u4e92\u7ed3\u6784\u8868\u793a\u3002\u9700\u8981\u4e00\u4e2a\u4e0e\u5177\u4f53\u673a\u5668\u4eba\u65e0\u5173\u7684\u4ea4\u4e92\u8868\u793a\u6765\u4f7f\u64cd\u4f5c\u884c\u4e3a\u66f4\u5bb9\u6613\u9a8c\u8bc1\u548c\u8de8\u673a\u5668\u4eba\u8fc1\u79fb\u3002", "method": "\u63d0\u51faFlowHOI\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5c06\u51e0\u4f55\u4e2d\u5fc3\u7684\u6293\u53d6\u4e0e\u8bed\u4e49\u4e2d\u5fc3\u7684\u64cd\u63a7\u89e3\u8026\uff1b2\uff09\u4f7f\u7528\u7d27\u51d1\u76843D\u573a\u666f\u6807\u8bb0\u548c\u8fd0\u52a8-\u6587\u672c\u5bf9\u9f50\u635f\u5931\u6765\u8bed\u4e49\u5730\u57fa\u7840\u5316\u751f\u6210\u7684\u4ea4\u4e92\uff1b3\uff09\u5f15\u5165\u91cd\u5efa\u6d41\u7a0b\u4ece\u5927\u89c4\u6a21\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e2d\u6062\u590d\u5bf9\u9f50\u7684\u624b-\u7269\u8f68\u8ff9\u548c\u7f51\u683c\uff0c\u4e3a\u9c81\u68d2\u751f\u6210\u63d0\u4f9bHOI\u5148\u9a8c\u3002", "result": "\u5728GRAB\u548cHOT3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlowHOI\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u52a8\u4f5c\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u7684\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u7ebf\u9ad8\u51fa1.7\u500d\u7684\u7269\u7406\u6a21\u62df\u6210\u529f\u7387\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u534740\u500d\u3002\u5728\u56db\u4e2a\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u771f\u5b9e\u673a\u5668\u4eba\u6267\u884c\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u7684HOI\u8868\u793a\u53ef\u4ee5\u91cd\u5b9a\u5411\u5230\u771f\u5b9e\u673a\u5668\u4eba\u6267\u884c\u6d41\u7a0b\u3002", "conclusion": "FlowHOI\u901a\u8fc7\u663e\u5f0f\u8868\u793a\u624b-\u7269\u4ea4\u4e92\u7ed3\u6784\uff0c\u6210\u529f\u751f\u6210\u4e86\u8bed\u4e49\u57fa\u7840\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u4ea4\u4e92\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u3001\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u4ece\u751f\u6210\u5230\u771f\u5b9e\u673a\u5668\u4eba\u6267\u884c\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.13524", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13524", "abs": "https://arxiv.org/abs/2602.13524", "authors": ["Gabriel Franco", "Carson Loughridge", "Mark Crovella"], "title": "Singular Vectors of Attention Heads Align with Features", "comment": null, "summary": "Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u5947\u5f02\u5411\u91cf\u4e0e\u7279\u5f81\u5bf9\u9f50\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5947\u5f02\u5411\u91cf\u53ef\u4ee5\u53ef\u9760\u5730\u8bc6\u522b\u7279\u5f81\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u64cd\u4f5c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u5b58\u5728\u4e00\u4e2a\u9690\u542b\u5047\u8bbe\uff1a\u5947\u5f02\u5411\u91cf\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u63a8\u65ad\u7279\u5f81\u8868\u793a\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76\u5947\u5f02\u5411\u91cf\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u80fd\u4e0e\u7279\u5f81\u5bf9\u9f50\u3002", "method": "1. \u5728\u7279\u5f81\u53ef\u76f4\u63a5\u89c2\u5bdf\u7684\u6a21\u578b\u4e2d\u9a8c\u8bc1\u5947\u5f02\u5411\u91cf\u4e0e\u7279\u5f81\u7684\u5bf9\u9f50\u6027\uff1b2. \u7406\u8bba\u5206\u6790\u5bf9\u9f50\u53d1\u751f\u7684\u6761\u4ef6\uff1b3. \u63d0\u51fa\u7a00\u758f\u6ce8\u610f\u529b\u5206\u89e3\u4f5c\u4e3a\u53ef\u64cd\u4f5c\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u6a21\u578b\u4e2d\u68c0\u9a8c\u3002", "result": "1. \u5728\u7279\u5f81\u53ef\u89c2\u5bdf\u7684\u6a21\u578b\u4e2d\uff0c\u5947\u5f02\u5411\u91cf\u786e\u5b9e\u4e0e\u7279\u5f81\u7a33\u5065\u5bf9\u9f50\uff1b2. \u7406\u8bba\u5206\u6790\u8868\u660e\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u8fd9\u79cd\u5bf9\u9f50\u662f\u9884\u671f\u7684\uff1b3. \u771f\u5b9e\u6a21\u578b\u4e2d\u51fa\u73b0\u4e86\u4e0e\u9884\u6d4b\u4e00\u81f4\u7684\u7a00\u758f\u6ce8\u610f\u529b\u5206\u89e3\u6a21\u5f0f\u3002", "conclusion": "\u5947\u5f02\u5411\u91cf\u4e0e\u7279\u5f81\u7684\u5bf9\u9f50\u53ef\u4ee5\u4f5c\u4e3a\u8bed\u8a00\u6a21\u578b\u4e2d\u7279\u5f81\u8bc6\u522b\u7684\u53ef\u9760\u7406\u8bba\u57fa\u7840\uff0c\u4e3a\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u53ef\u64cd\u4f5c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2602.14490", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.14490", "abs": "https://arxiv.org/abs/2602.14490", "authors": ["Buze Zhang", "Jinkai Tao", "Zilang Zeng", "Neil He", "Ali Maatouk", "Menglin Yang", "Rex Ying"], "title": "Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts", "comment": "15 pages, 11 figures", "summary": "Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.", "AI": {"tldr": "MoSLoRA\uff1a\u4e00\u79cd\u6df7\u5408\u51e0\u4f55\u7a7a\u95f4\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u51e0\u4f55\u7a7a\u95f4\uff08\u5982\u53cc\u66f2\u3001\u7403\u9762\u7b49\uff09\u6765\u5b66\u4e60\u66f4\u4e30\u5bcc\u7684\u66f2\u7387\u611f\u77e5\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u6355\u6349\u8bed\u8a00\u6570\u636e\u4e2d\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\u3002\u867d\u7136\u53cc\u66f2\u51e0\u4f55\uff08\u9002\u5408\u5c42\u6b21\u6570\u636e\uff09\u548c\u7403\u9762\u6d41\u5f62\uff08\u9002\u5408\u5faa\u73af\u6a21\u5f0f\uff09\u7b49\u66ff\u4ee3\u51e0\u4f55\u7a7a\u95f4\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u5c06\u8868\u793a\u5f3a\u5236\u653e\u5165\u5355\u4e00\u6d41\u5f62\u7c7b\u578b\u6700\u7ec8\u4f1a\u9650\u5236\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u7a7a\u95f4\u6846\u67b6\uff0c\u540c\u65f6\u5229\u7528\u591a\u79cd\u51e0\u4f55\u7a7a\u95f4\u5b66\u4e60\u66f4\u4e30\u5bcc\u7684\u66f2\u7387\u611f\u77e5\u8868\u793a\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1MoSLoRA\uff0c\u5c06\u4f4e\u79e9\u9002\u5e94\u6269\u5c55\u5230\u5f02\u6784\u51e0\u4f55\u4e13\u5bb6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u8f93\u5165\u4e0a\u4e0b\u6587\u52a8\u6001\u9009\u62e9\u6216\u7ec4\u5408\u9002\u5f53\u7684\u51e0\u4f55\u7a7a\u95f4\u3002\u4e3a\u51cf\u5c11\u9891\u7e41\u6d41\u5f62\u5207\u6362\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u8def\u7531\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoSLoRA\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728MATH500\u4e0a\u5b9e\u73b05.6%\u7684\u6539\u8fdb\uff0c\u5728MAWPS\u4e0a\u5b9e\u73b015.9%\u7684\u6539\u8fdb\u3002\u8fd8\u63d0\u4f9b\u4e86\u66f2\u7387\u4f18\u5316\u5982\u4f55\u5f71\u54cd\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002", "conclusion": "\u6df7\u5408\u7a7a\u95f4\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u5229\u7528\u591a\u79cd\u51e0\u4f55\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.13954", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13954", "abs": "https://arxiv.org/abs/2602.13954", "authors": ["Dan Zhang", "Yishu Lei", "Jing Hu", "Shuwei He", "Songhe Deng", "Xianlong Luo", "Danxiang Zhu", "Shikun Feng", "Rui Liu", "Jingzhou He", "Yu Sun", "Hua Wu", "Haifeng Wang"], "title": "Eureka-Audio: Triggering Audio Intelligence in Compact Language Models", "comment": "23 pages, 4 figures", "summary": "We present Eureka-Audio, a compact yet high-performance audio language model that achieves competitive performance against models that are 4 to 18 times larger across a broad range of audio understanding benchmarks. Despite containing only 1.7B parameters, Eureka-Audio demonstrates strong performance on automatic speech recognition (ASR), audio understanding, and dense audio captioning, matching or surpassing multiple 7B to 30B audio and omni-modal baselines. The model adopts a unified end-to-end architecture composed of a lightweight language backbone, a Whisper-based audio encoder, and a sparsely activated Mixture-of-Experts (MoE) adapter that explicitly accounts for audio heterogeneity and alleviates cross-modal optimization conflicts under limited capacity. To further enhance paralinguistic reasoning, we introduce DataFlux, a closed loop audio instruction data synthesis and verification pipeline that constructs high quality, logically consistent supervision from raw audio. Extensive evaluations across ASR, knowledge reasoning, safety, instruction following, and paralinguistic benchmarks, demonstrate that Eureka-Audio achieves an efficient balance between computational cost and performance. These results establish Eureka Audio as a strong and practical baseline for lightweight audio understanding models.", "AI": {"tldr": "Eureka-Audio\u662f\u4e00\u4e2a\u4ec5\u542b17\u4ebf\u53c2\u6570\u7684\u7d27\u51d1\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u9879\u97f3\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u5ab2\u7f8e4-18\u500d\u5927\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u53c2\u6570\u5e9e\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u4f46\u9ad8\u6027\u80fd\u7684\u6a21\u578b\uff0c\u4ee5\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u5e7f\u6cdb\u7684\u97f3\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5305\u62ec\u8bed\u97f3\u8bc6\u522b\u3001\u97f3\u9891\u7406\u89e3\u548c\u5bc6\u96c6\u97f3\u9891\u63cf\u8ff0\u7b49\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7aef\u5230\u7aef\u67b6\u6784\uff1a\u8f7b\u91cf\u7ea7\u8bed\u8a00\u4e3b\u5e72\u3001Whisper\u97f3\u9891\u7f16\u7801\u5668\u548c\u7a00\u758f\u6fc0\u6d3b\u7684MoE\u9002\u914d\u5668\u5904\u7406\u97f3\u9891\u5f02\u8d28\u6027\uff1b\u5f15\u5165DataFlux\u95ed\u73af\u97f3\u9891\u6307\u4ee4\u6570\u636e\u5408\u6210\u4e0e\u9a8c\u8bc1\u7ba1\u9053\uff0c\u4ece\u539f\u59cb\u97f3\u9891\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u903b\u8f91\u4e00\u81f4\u7684\u76d1\u7763\u6570\u636e\u3002", "result": "\u5728ASR\u3001\u77e5\u8bc6\u63a8\u7406\u3001\u5b89\u5168\u6027\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u526f\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEureka-Audio\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a\u591a\u4e2a70\u4ebf\u5230300\u4ebf\u53c2\u6570\u7684\u97f3\u9891\u548c\u5168\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u95f4\u5b9e\u73b0\u9ad8\u6548\u5e73\u8861\u3002", "conclusion": "Eureka-Audio\u4e3a\u8f7b\u91cf\u7ea7\u97f3\u9891\u7406\u89e3\u6a21\u578b\u5efa\u7acb\u4e86\u5f3a\u5927\u5b9e\u7528\u7684\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u67b6\u6784\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u5728\u97f3\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e0e\u5927\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u3002"}}
{"id": "2602.14471", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14471", "abs": "https://arxiv.org/abs/2602.14471", "authors": ["Furkan Mumcu", "Yasin Yilmaz"], "title": "Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems", "comment": null, "summary": "Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $\u03bb\\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $\u03b2$, we show that SWA induces a critical threshold $\u03bb^*=(n-\u03b2)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.", "AI": {"tldr": "SWA\u6846\u67b6\u901a\u8fc7\u793e\u4f1a\u6743\u91cd\u03bb\u5728\u4e2a\u4f53\u76ee\u6807\u4e0e\u7fa4\u4f53\u798f\u5229\u95f4\u6743\u8861\uff0c\u5728\u5171\u4eab\u8d44\u6e90\u62e5\u585e\u6e38\u620f\u4e2d\u5b9e\u73b0\u4ece\u6301\u7eed\u62e5\u585e\u5230\u7a33\u5b9a\u8fd0\u884c\u7684\u76f8\u53d8\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u5728\u5171\u4eab\u73af\u5883\u4e2d\u90e8\u7f72LLM\u667a\u80fd\u4f53\u65f6\uff0c\u4e2a\u4f53\u7406\u6027\u51b3\u7b56\u4f1a\u4ea7\u751f\u8d1f\u5916\u90e8\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u5728\u4e2a\u4f53\u5bf9\u9f50\u4e0e\u96c6\u4f53\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u793e\u4f1a\u52a0\u6743\u5bf9\u9f50(SWA)\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u793e\u4f1a\u6743\u91cd\u03bb\u2208[0,1]\u5728\u4e2a\u4f53\u79c1\u6709\u76ee\u6807\u4e0e\u7fa4\u4f53\u798f\u5229\u4f30\u8ba1\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff0c\u5728\u5171\u4eab\u8d44\u6e90\u62e5\u585e\u6e38\u620f\u4e2d\u5206\u6790\u5176\u6548\u679c\u3002", "result": "\u5728n\u4e2a\u667a\u80fd\u4f53\u3001\u62e5\u585e\u4e25\u91cd\u7a0b\u5ea6\u03b2\u7684\u5171\u4eab\u8d44\u6e90\u6e38\u620f\u4e2d\uff0cSWA\u8bf1\u5bfc\u51fa\u4e34\u754c\u9608\u503c\u03bb*=(n-\u03b2)/(n-1)\uff0c\u8d85\u8fc7\u6b64\u9608\u503c\u540e\u667a\u80fd\u4f53\u5728\u8fc7\u8f7d\u65f6\u4e0d\u518d\u6709\u589e\u52a0\u9700\u6c42\u7684\u8fb9\u9645\u6fc0\u52b1\uff0c\u5b9e\u73b0\u4ece\u6301\u7eed\u62e5\u585e\u5230\u7a33\u5b9a\u8fd0\u884c\u7684\u76f8\u53d8\u3002", "conclusion": "SWA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u65f6\u7b97\u6cd5\u5b9e\u73b0\uff0c\u901a\u8fc7\u793e\u4f1a\u6743\u91cd\u8c03\u8282\u4e2a\u4f53\u4e0e\u96c6\u4f53\u5229\u76ca\u7684\u5e73\u8861\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u5171\u4eab\u73af\u5883\u4e2dLLM\u667a\u80fd\u4f53\u7684\u8d1f\u5916\u90e8\u6027\u95ee\u9898\u3002"}}
{"id": "2602.14660", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14660", "abs": "https://arxiv.org/abs/2602.14660", "authors": ["Fan Zhang", "Deyuan Meng", "Ying Tan"], "title": "Segment-Based Two-Loop Adaptive Iterative Learning Control for Spacecraft Position and Attitude Tracking", "comment": "13 pages", "summary": "Proximity operations of rigid bodies, such as spacecraft rendezvous and docking, require precise tracking of both position and attitude over finite time intervals. These operations are often repeated under uncertain conditions, with unknown but repeatable parameters and disturbances. Adaptive iterative learning control (ILC) is well suited to such tasks, as it can track desired trajectories while learning unknown, iteration-invariant signals or parameters. However, conventional adaptive ILC faces two challenges: (i) the coupling between rotational and translational dynamics complicates the design of the two coordinated learning loops for position and attitude, and (ii) standard adaptive ILC designs cannot guarantee bounded control inputs. To address these issues, we propose a dual-number-based, segment-based two-loop adaptive ILC framework for simultaneous high-precision position and attitude tracking. The framework employs two learning loops that interact through a dual-number representation of tracking errors, combining position and attitude errors into a single mathematical object for unified control design. A segment-based dynamic projection mechanism ensures that both parameter estimates and control inputs remain bounded without prior knowledge of uncertainties. Mathematical analysis and numerical simulations demonstrate that the proposed framework significantly enhances tracking performance under unknown but repeatable uncertainties and strong rotational-translational coupling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u5076\u6570\u7684\u5206\u6bb5\u5f0f\u53cc\u73af\u81ea\u9002\u5e94\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u521a\u6027\u4f53\u63a5\u8fd1\u64cd\u4f5c\u4e2d\u4f4d\u7f6e\u548c\u59ff\u6001\u7684\u534f\u540c\u8ddf\u8e2a\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u63a7\u5236\u8f93\u5165\u65e0\u754c\u548c\u52a8\u529b\u5b66\u8026\u5408\u6311\u6218\u3002", "motivation": "\u822a\u5929\u5668\u4ea4\u4f1a\u5bf9\u63a5\u7b49\u521a\u6027\u4f53\u63a5\u8fd1\u64cd\u4f5c\u9700\u8981\u5728\u6709\u9650\u65f6\u95f4\u95f4\u9694\u5185\u7cbe\u786e\u8ddf\u8e2a\u4f4d\u7f6e\u548c\u59ff\u6001\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u5e38\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u91cd\u590d\u8fdb\u884c\uff0c\u5b58\u5728\u672a\u77e5\u4f46\u53ef\u91cd\u590d\u7684\u53c2\u6570\u548c\u6270\u52a8\u3002\u4f20\u7edf\u81ea\u9002\u5e94\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u65cb\u8f6c\u548c\u5e73\u79fb\u52a8\u529b\u5b66\u8026\u5408\u4f7f\u4f4d\u7f6e\u548c\u59ff\u6001\u7684\u534f\u8c03\u5b66\u4e60\u73af\u8bbe\u8ba1\u590d\u6742\u5316\uff1b\u6807\u51c6\u81ea\u9002\u5e94ILC\u8bbe\u8ba1\u65e0\u6cd5\u4fdd\u8bc1\u63a7\u5236\u8f93\u5165\u6709\u754c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u5076\u6570\u7684\u5206\u6bb5\u5f0f\u53cc\u73af\u81ea\u9002\u5e94\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u6846\u67b6\uff1a1) \u4f7f\u7528\u5bf9\u5076\u6570\u8868\u793a\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5c06\u4f4d\u7f6e\u548c\u59ff\u6001\u8bef\u5dee\u7ed3\u5408\u4e3a\u5355\u4e00\u6570\u5b66\u5bf9\u8c61\u8fdb\u884c\u7edf\u4e00\u63a7\u5236\u8bbe\u8ba1\uff1b2) \u91c7\u7528\u4e24\u4e2a\u901a\u8fc7\u5bf9\u5076\u6570\u4ea4\u4e92\u7684\u5b66\u4e60\u73af\uff1b3) \u5f15\u5165\u5206\u6bb5\u5f0f\u52a8\u6001\u6295\u5f71\u673a\u5236\uff0c\u786e\u4fdd\u53c2\u6570\u4f30\u8ba1\u548c\u63a7\u5236\u8f93\u5165\u6709\u754c\uff0c\u65e0\u9700\u5148\u9a8c\u4e0d\u786e\u5b9a\u6027\u77e5\u8bc6\u3002", "result": "\u6570\u5b66\u5206\u6790\u548c\u6570\u503c\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u672a\u77e5\u4f46\u53ef\u91cd\u590d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5f3a\u65cb\u8f6c-\u5e73\u79fb\u8026\u5408\u6761\u4ef6\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u521a\u6027\u4f53\u63a5\u8fd1\u64cd\u4f5c\u4e2d\u4f4d\u7f6e\u548c\u59ff\u6001\u534f\u540c\u8ddf\u8e2a\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u5076\u6570\u8868\u793a\u548c\u5206\u6bb5\u6295\u5f71\u673a\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u81ea\u9002\u5e94\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u822a\u5929\u5668\u4ea4\u4f1a\u5bf9\u63a5\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13531", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.13531", "abs": "https://arxiv.org/abs/2602.13531", "authors": ["Abdallah Aaraba", "Soumaya Cherkaoui", "Ola Ahmad", "Shengrui Wang"], "title": "QuaRK: A Quantum Reservoir Kernel for Time Series Learning", "comment": null, "summary": "Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.", "AI": {"tldr": "QuaRK\u6846\u67b6\uff1a\u5c06\u786c\u4ef6\u73b0\u5b9e\u7684\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u4e0e\u6838\u57fa\u8bfb\u51fa\u65b9\u6848\u7ed3\u5408\uff0c\u901a\u8fc7\u7ecf\u5178\u9634\u5f71\u5c42\u6790\u9ad8\u6548\u6d4b\u91cfk-\u5c40\u90e8\u53ef\u89c2\u6d4b\u91cf\uff0c\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60", "motivation": "\u91cf\u5b50\u50a8\u5c42\u8ba1\u7b97\u4e3a\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9ad8\u6548\u53ef\u5b9e\u73b0\u7684\u91cf\u5b50\u50a8\u5c42\u67b6\u6784\u548c\u6a21\u578b\u5b66\u4e60\u4fdd\u8bc1", "method": "\u63d0\u51faQuaRK\u7aef\u5230\u7aef\u6846\u67b6\uff1a\u786c\u4ef6\u73b0\u5b9e\u7684\u91cf\u5b50\u50a8\u5c42\u7279\u5f81\u63d0\u53d6\u5668 + \u6838\u57fa\u8bfb\u51fa\u65b9\u6848\uff1b\u4f7f\u7528\u7ecf\u5178\u9634\u5f71\u5c42\u6790\u9ad8\u6548\u6d4b\u91cfk-\u5c40\u90e8\u53ef\u89c2\u6d4b\u91cf\uff1b\u7ed3\u5408\u663e\u5f0f\u6b63\u5219\u5316\u548c\u5feb\u901f\u4f18\u5316\u7684\u7ecf\u5178\u6838\u65b9\u6cd5", "result": "\u63d0\u4f9b\u4e86\u4f9d\u8d56\u65f6\u95f4\u6570\u636e\u7684\u6cdb\u5316\u7406\u8bba\u4fdd\u8bc1\uff0c\u5c06\u8bbe\u8ba1\u548c\u8d44\u6e90\u9009\u62e9\u4e0e\u6709\u9650\u6837\u672c\u6027\u80fd\u8054\u7cfb\u8d77\u6765\uff1b\u901a\u8fc7\u5408\u6210beta\u6df7\u5408\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u9a8c\u8bc1\u4e86\u9884\u6d4b\u7684\u63d2\u503c\u548c\u6cdb\u5316\u884c\u4e3a", "conclusion": "QuaRK\u6846\u67b6\u4e3a\u6784\u5efa\u53ef\u9760\u7684\u65f6\u95f4\u5b66\u4e60\u5668\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\uff0c\u901a\u8fc7\u6e05\u6670\u7684\u7535\u8def\u5bbd\u5ea6\u3001\u6df1\u5ea6\u548c\u6d4b\u91cf\u9884\u7b97\u7b49\u8ba1\u7b97\u65cb\u94ae\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u65b9\u6cd5\u5efa\u6a21\u975e\u7ebf\u6027\u65f6\u95f4\u6cdb\u51fd\u7684\u7075\u6d3b\u6027"}}
{"id": "2602.14771", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.14771", "abs": "https://arxiv.org/abs/2602.14771", "authors": ["Shih-Fang Chen", "Jun-Cheng Chen", "I-Hong Jhuo", "Yen-Yu Lin"], "title": "GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture", "comment": "Learning Model Adaptation for Adverse and Dynamic Environments", "summary": "The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.", "AI": {"tldr": "GOT-JEPA\u662f\u4e00\u4e2a\u6a21\u578b\u9884\u6d4b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u4ece\u5e72\u51c0\u548c\u635f\u574f\u7684\u5e27\u4e2d\u9884\u6d4b\u4f2a\u8ddf\u8e2a\u6a21\u578b\uff0c\u63d0\u5347\u8ddf\u8e2a\u5668\u5728\u906e\u6321\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff1bOccuSolver\u5219\u901a\u8fc7\u70b9\u4e2d\u5fc3\u8ddf\u8e2a\u5668\u589e\u5f3a\u906e\u6321\u611f\u77e5\uff0c\u8fed\u4ee3\u4f18\u5316\u53ef\u89c1\u6027\u72b6\u6001\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u80fd\u591f\u6574\u5408\u5386\u53f2\u4fe1\u606f\u3001\u9002\u5e94\u573a\u666f\u53d8\u5316\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u906e\u6321\u63a8\u7406\uff0c\u800c\u73b0\u6709\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u901a\u5e38\u9488\u5bf9\u8bad\u7ec3\u76ee\u6807\u4f18\u5316\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u906e\u6321\u63a8\u7406\u8f83\u4e3a\u7c97\u7cd9\uff0c\u7f3a\u4e4f\u5bf9\u906e\u6321\u6a21\u5f0f\u7684\u8be6\u7ec6\u5efa\u6a21\u3002", "method": "\u63d0\u51faGOT-JEPA\u6846\u67b6\uff0c\u5c06JEPA\u4ece\u9884\u6d4b\u56fe\u50cf\u7279\u5f81\u6269\u5c55\u5230\u9884\u6d4b\u8ddf\u8e2a\u6a21\u578b\uff1a\u6559\u5e08\u9884\u6d4b\u5668\u4ece\u5e72\u51c0\u5f53\u524d\u5e27\u751f\u6210\u4f2a\u8ddf\u8e2a\u6a21\u578b\uff0c\u5b66\u751f\u9884\u6d4b\u5668\u4ece\u635f\u574f\u7248\u672c\u5b66\u4e60\u9884\u6d4b\u76f8\u540c\u6a21\u578b\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u4f2a\u76d1\u7763\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faOccuSolver\uff0c\u91c7\u7528\u70b9\u4e2d\u5fc3\u70b9\u8ddf\u8e2a\u5668\u8fdb\u884c\u76ee\u6807\u611f\u77e5\u7684\u53ef\u89c1\u6027\u4f30\u8ba1\u548c\u906e\u6321\u6a21\u5f0f\u6355\u83b7\uff0c\u57fa\u4e8e\u8ddf\u8e2a\u5668\u8fed\u4ee3\u751f\u6210\u7684\u76ee\u6807\u5148\u9a8c\u9010\u6b65\u4f18\u5316\u53ef\u89c1\u6027\u72b6\u6001\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8ddf\u8e2a\u5668\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GOT-JEPA\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u9884\u8bad\u7ec3\u6846\u67b6\u589e\u5f3a\u4e86\u8ddf\u8e2a\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800cOccuSolver\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u906e\u6321\u611f\u77e5\u548c\u8fed\u4ee3\u4f18\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u906e\u6321\u5904\u7406\u6027\u80fd\uff0c\u5171\u540c\u89e3\u51b3\u4e86\u73b0\u6709\u8ddf\u8e2a\u5668\u5728\u6cdb\u5316\u548c\u906e\u6321\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.14127", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.14127", "abs": "https://arxiv.org/abs/2602.14127", "authors": ["Reda Bensaid", "Amine Ouasfi", "Yassir Bendou", "Ilyass Moummad", "Vincent Gripon", "Fran\u00e7ois Leduc-Primeau", "Adnane Boukhayma"], "title": "MUKA: Multi Kernel Audio Adaptation Of Audio-Language Models", "comment": null, "summary": "Multimodal foundation models have demonstrated impressive generalization capabilities, yet efficiently adapting them to new tasks in a few-shot setting remains a critical challenge. In this work, we investigate the few-shot adaptation of Large Audio-Language Models (ALMs) through both training-based and training-free approaches. We introduce MUKA, a multi-kernel adaptation framework that combines the fine-grained, context-dependent representations of instruction-tuning based models like Pengi with the global semantic representations of contrastive pretraining methods like CLAP. By constructing a product kernel that aligns local similarity with global semantics, MUKA enhances representational power while preserving the theoretical guarantees of kernel methods and avoiding additional training. Extensive experiments across 11 diverse audio datasets demonstrate that MUKA achieves state-of-the-art performance among training-free methods and even surpasses training-based adapters in several scenarios, offering a compelling balance between adaptability and efficiency.", "AI": {"tldr": "MUKA\u662f\u4e00\u4e2a\u591a\u6838\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u8868\u793a\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5168\u5c40\u8bed\u4e49\u8868\u793a\uff0c\u5728\u5c11\u6837\u672c\u97f3\u9891\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u9ad8\u6548\u9002\u5e94\u65b0\u4efb\u52a1\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08ALMs\uff09\u7684\u5c11\u6837\u672c\u9002\u5e94\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u8bad\u7ec3\u548c\u4e0d\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMUKA\u591a\u6838\u9002\u5e94\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff08\u5982Pengi\uff09\u7684\u7ec6\u7c92\u5ea6\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u8868\u793a\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982CLAP\uff09\u7684\u5168\u5c40\u8bed\u4e49\u8868\u793a\u3002\u901a\u8fc7\u6784\u5efa\u5bf9\u9f50\u5c40\u90e8\u76f8\u4f3c\u6027\u4e0e\u5168\u5c40\u8bed\u4e49\u7684\u4e58\u79ef\u6838\uff0c\u589e\u5f3a\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u65b9\u6cd5\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u907f\u514d\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u572811\u4e2a\u4e0d\u540c\u7684\u97f3\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cMUKA\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u57fa\u4e8e\u8bad\u7ec3\u7684\u9002\u914d\u5668\u65b9\u6cd5\uff0c\u5728\u9002\u5e94\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "MUKA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u7c7b\u578b\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4e3a\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.14331", "categories": ["cs.GT", "cs.HC", "econ.TH"], "pdf": "https://arxiv.org/pdf/2602.14331", "abs": "https://arxiv.org/abs/2602.14331", "authors": ["Saurabh Amin", "Amine Bennouna", "Daniel Huttenlocher", "Dingwen Kong", "Liang Lyu", "Asuman Ozdaglar"], "title": "A Bayesian Framework for Human-AI Collaboration: Complementarity and Correlation Neglect", "comment": null, "summary": "We develop a decision-theoretic model of human-AI interaction to study when AI assistance improves or impairs human decision-making. A human decision-maker observes private information and receives a recommendation from an AI system, but may combine these signals imperfectly. We show that the effect of AI assistance decomposes into two main forces: the marginal informational value of the AI beyond what the human already knows, and a behavioral distortion arising from how the human uses the AI's recommendation. Central to our analysis is a micro-founded measure of informational overlap between human and AI knowledge. We study an empirically relevant form of imperfect decision-making -- correlation neglect -- whereby humans treat AI recommendations as independent of their own information despite shared evidence. Under this model, we characterize how overlap and AI capabilities shape the Human-AI interaction regime between augmentation, impairment, complementarity, and automation, and draw key insights.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u4eba\u673a\u4ea4\u4e92\u51b3\u7b56\u7406\u8bba\u6a21\u578b\uff0c\u5206\u6790AI\u8f85\u52a9\u4f55\u65f6\u6539\u5584\u6216\u635f\u5bb3\u4eba\u7c7b\u51b3\u7b56\uff0c\u91cd\u70b9\u5173\u6ce8\u4fe1\u606f\u91cd\u53e0\u548c\u884c\u4e3a\u626d\u66f2\u4e24\u4e2a\u6838\u5fc3\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3AI\u8f85\u52a9\u5bf9\u4eba\u7c7b\u51b3\u7b56\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u53ef\u80fd\u4e0d\u5b8c\u7f8e\u5730\u7ed3\u5408\u81ea\u8eab\u4fe1\u606f\u4e0eAI\u5efa\u8bae\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522bAI\u8f85\u52a9\u4f55\u65f6\u771f\u6b63\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u51b3\u7b56\u7406\u8bba\u6a21\u578b\uff0c\u4eba\u7c7b\u51b3\u7b56\u8005\u89c2\u5bdf\u79c1\u4eba\u4fe1\u606f\u5e76\u63a5\u6536AI\u5efa\u8bae\uff0c\u4f46\u53ef\u80fd\u4e0d\u5b8c\u7f8e\u5730\u7ed3\u5408\u8fd9\u4e9b\u4fe1\u53f7\u3002\u6a21\u578b\u5f15\u5165\u4e86\u4fe1\u606f\u91cd\u53e0\u7684\u5fae\u89c2\u6d4b\u91cf\uff0c\u5e76\u7814\u7a76\u4e86\"\u76f8\u5173\u6027\u5ffd\u89c6\"\u8fd9\u79cd\u5b9e\u8bc1\u76f8\u5173\u7684\u51b3\u7b56\u7f3a\u9677\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u8f85\u52a9\u6548\u679c\u53ef\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e3b\u8981\u529b\u91cf\uff1aAI\u8d85\u8d8a\u4eba\u7c7b\u5df2\u77e5\u4fe1\u606f\u7684\u8fb9\u9645\u4fe1\u606f\u4ef7\u503c\uff0c\u4ee5\u53ca\u4eba\u7c7b\u4f7f\u7528AI\u5efa\u8bae\u65b9\u5f0f\u5f15\u8d77\u7684\u884c\u4e3a\u626d\u66f2\u3002\u7814\u7a76\u8fd8\u523b\u753b\u4e86\u4fe1\u606f\u91cd\u53e0\u548cAI\u80fd\u529b\u5982\u4f55\u5851\u9020\u4eba\u673a\u4ea4\u4e92\u7684\u56db\u79cd\u6a21\u5f0f\u3002", "conclusion": "AI\u8f85\u52a9\u7684\u6548\u679c\u53d6\u51b3\u4e8e\u4fe1\u606f\u91cd\u53e0\u7a0b\u5ea6\u548cAI\u80fd\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u51b3\u7b56\u589e\u5f3a\u3001\u635f\u5bb3\u3001\u4e92\u8865\u6216\u81ea\u52a8\u5316\u7b49\u4e0d\u540c\u7ed3\u679c\u3002\u7406\u89e3\u8fd9\u4e9b\u673a\u5236\u5bf9\u4e8e\u8bbe\u8ba1\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.13476", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13476", "abs": "https://arxiv.org/abs/2602.13476", "authors": ["Noriaki Hirose", "Catherine Glossop", "Dhruv Shah", "Sergey Levine"], "title": "AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge", "comment": "13 pages, 9 figures, 2 tables", "summary": "Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.", "AI": {"tldr": "AsyncVLA\u662f\u4e00\u4e2a\u5f02\u6b65\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u4e0e\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u6267\u884c\u5668\u89e3\u8026\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u9ad8\u5ef6\u8fdf\u5bfc\u81f4\u7684\u5b9e\u65f6\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u867d\u7136\u901a\u8fc7\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\u3002\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u4f1a\u7834\u574f\u63a7\u5236\u56de\u8def\uff0c\u4f7f\u5f97\u5f3a\u5927\u7684\u6a21\u578b\u65e0\u6cd5\u5b89\u5168\u5730\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "AsyncVLA\u91c7\u7528\u5f02\u6b65\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u5927\u578b\u57fa\u7840\u6a21\u578b\u90e8\u7f72\u5728\u8fdc\u7a0b\u5de5\u4f5c\u7ad9\u63d0\u4f9b\u9ad8\u7ea7\u6307\u5bfc\uff0c\u540c\u65f6\u8f7b\u91cf\u7ea7\u7684\u677f\u8f7d\u8fb9\u7f18\u9002\u914d\u5668\u4ee5\u9ad8\u9891\u7387\u6301\u7eed\u4f18\u5316\u52a8\u4f5c\u3002\u901a\u8fc7\u7aef\u5230\u7aef\u5fae\u8c03\u534f\u8bae\u548c\u8f68\u8ff9\u91cd\u52a0\u6743\u7b56\u7565\u6765\u5f25\u5408\u5f02\u6b65\u6d41\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "result": "\u5728\u901a\u4fe1\u5ef6\u8fdf\u9ad8\u8fbe6\u79d2\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cAsyncVLA\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e8640%\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "AsyncVLA\u6709\u6548\u5730\u5f25\u5408\u4e86\u5927\u578b\u6a21\u578b\u7684\u8bed\u4e49\u667a\u80fd\u4e0e\u8fb9\u7f18\u673a\u5668\u4eba\u6240\u9700\u7684\u53cd\u5e94\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13532", "categories": ["cs.LG", "eess.AS", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13532", "abs": "https://arxiv.org/abs/2602.13532", "authors": ["Nobutaka Ono"], "title": "Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction", "comment": "11 pages, 4 figures", "summary": "In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5143\u7d20\u9009\u62e9\u7684\u5feb\u901f\u964d\u7ef4\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u9009\u62e9\u8f93\u5165\u5411\u91cf\u7684\u5b50\u96c6\u5b9e\u73b0\u65e0\u4e58\u6cd5\u964d\u7ef4\uff0c\u76f8\u6bd4PCA\u7b49\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e", "motivation": "\u964d\u7ef4\u662f\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3001\u7f13\u89e3\u8fc7\u62df\u5408\u3001\u52a0\u901f\u8bad\u7ec3\u63a8\u7406\u7684\u91cd\u8981\u6280\u672f\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5982PCA\u4f9d\u8d56\u77e9\u9635\u4e58\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e2d\u4e58\u6cd5\u8ba1\u7b97\u672c\u8eab\u53ef\u80fd\u6210\u4e3a\u74f6\u9888\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e0\u4e58\u6cd5\u7684\u964d\u7ef4\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5143\u7d20\u9009\u62e9\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7ebf\u6027\u56de\u5f52\u7684\u5747\u65b9\u8bef\u5dee\u6765\u8bc4\u4f30\u5019\u9009\u5b50\u96c6\u3002\u5f53\u7f3a\u4e4f\u660e\u786e\u76ee\u6807\u65f6\uff0c\u4f7f\u7528\u8f93\u5165\u81ea\u8eab\u4f5c\u4e3a\u91cd\u5efa\u76ee\u6807\u3002\u91c7\u7528\u57fa\u4e8e\u77e9\u9635\u6c42\u9006\u5f15\u7406\u7684\u9ad8\u6548\u76ee\u6807\u51fd\u6570\u53d8\u5316\u516c\u5f0f\uff0c\u901a\u8fc7\u4ea4\u6362\u5f0f\u5c40\u90e8\u641c\u7d22\u8fed\u4ee3\u5e94\u7528\u76ee\u6807\u51fd\u6570\u964d\u4f4e\u7684\u4ea4\u6362\u64cd\u4f5c\u3002", "result": "\u5728MNIST\u624b\u5199\u6570\u5b57\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u65e0\u4e58\u6cd5\u964d\u7ef4\u3002", "conclusion": "\u5143\u7d20\u9009\u62e9\u4f5c\u4e3a\u4e00\u79cd\u65e0\u4e58\u6cd5\u7684\u964d\u7ef4\u5f62\u5f0f\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u4ea4\u6362\u5f0f\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e2d\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u964d\u7ef4\u6548\u679c\u3002"}}
{"id": "2602.14172", "categories": ["cs.SD", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14172", "abs": "https://arxiv.org/abs/2602.14172", "authors": ["Keinichi Fujita", "Yusuke Ijima"], "title": "Investigation for Relative Voice Impression Estimation", "comment": "5 pages,3 figures, Accepted to Speech Prosody 2026", "summary": "Paralinguistic and non-linguistic aspects of speech strongly influence listener impressions. While most research focuses on absolute impression scoring, this study investigates relative voice impression estimation (RIE), a framework for predicting the perceptual difference between two utterances from the same speaker. The estimation target is a low-dimensional vector derived from subjective evaluations, quantifying the perceptual shift of the second utterance relative to the first along an antonymic axis (e.g., ``Dark--Bright''). To isolate expressive and prosodic variation, we used recordings of a professional speaker reading a text in various styles. We compare three modeling approaches: classical acoustic features commonly used for speech emotion recognition, self-supervised speech representations, and multimodal large language models (MLLMs). Our results demonstrate that models using self-supervised representations outperform methods with classical acoustic features, particularly in capturing complex and dynamic impressions (e.g., ``Cold--Warm'') where classical features fail. In contrast, current MLLMs prove unreliable for this fine-grained pairwise task. This study provides the first systematic investigation of RIE and demonstrates the strength of self-supervised speech models in capturing subtle perceptual variations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u76f8\u5bf9\u58f0\u97f3\u5370\u8c61\u4f30\u8ba1\uff08RIE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u540c\u4e00\u8bf4\u8bdd\u8005\u4e24\u4e2a\u8bdd\u8bed\u4e4b\u95f4\u7684\u611f\u77e5\u5dee\u5f02\uff0c\u800c\u975e\u4f20\u7edf\u7684\u7edd\u5bf9\u8bc4\u5206\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff1a\u4f20\u7edf\u58f0\u5b66\u7279\u5f81\u3001\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u526f\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u56e0\u7d20\u5bf9\u542c\u4f17\u5370\u8c61\u6709\u91cd\u8981\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7edd\u5bf9\u5370\u8c61\u8bc4\u5206\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u76f8\u5bf9\u58f0\u97f3\u5370\u8c61\u4f30\u8ba1\uff0c\u5373\u9884\u6d4b\u540c\u4e00\u8bf4\u8bdd\u8005\u4e24\u4e2a\u8bdd\u8bed\u4e4b\u95f4\u7684\u611f\u77e5\u5dee\u5f02\uff0c\u8fd9\u80fd\u66f4\u7cbe\u7ec6\u5730\u6355\u6349\u8bed\u97f3\u8868\u8fbe\u7684\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u4e13\u4e1a\u8bf4\u8bdd\u8005\u4ee5\u4e0d\u540c\u98ce\u683c\u6717\u8bfb\u540c\u4e00\u6587\u672c\u7684\u5f55\u97f3\uff0c\u4ee5\u9694\u79bb\u8868\u8fbe\u6027\u548c\u97f5\u5f8b\u53d8\u5316\u3002\u6bd4\u8f83\u4e09\u79cd\u5efa\u6a21\u65b9\u6cd5\uff1a1\uff09\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u7ecf\u5178\u58f0\u5b66\u7279\u5f81\uff1b2\uff09\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\uff1b3\uff09\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u4f30\u8ba1\u76ee\u6807\u662f\u57fa\u4e8e\u4e3b\u89c2\u8bc4\u4f30\u7684\u4f4e\u7ef4\u5411\u91cf\uff0c\u91cf\u5316\u7b2c\u4e8c\u4e2a\u8bdd\u8bed\u76f8\u5bf9\u4e8e\u7b2c\u4e00\u4e2a\u8bdd\u8bed\u5728\u53cd\u4e49\u8f74\uff08\u5982\"\u6697-\u4eae\"\uff09\u4e0a\u7684\u611f\u77e5\u53d8\u5316\u3002", "result": "\u4f7f\u7528\u81ea\u76d1\u7763\u8868\u793a\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u7ecf\u5178\u58f0\u5b66\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u590d\u6742\u52a8\u6001\u5370\u8c61\uff08\u5982\"\u51b7-\u6696\"\uff09\u65b9\u9762\uff0c\u7ecf\u5178\u7279\u5f81\u8868\u73b0\u4e0d\u4f73\u3002\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e2a\u7ec6\u7c92\u5ea6\u7684\u6210\u5bf9\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u53ef\u9760\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u76f8\u5bf9\u58f0\u97f3\u5370\u8c61\u4f30\u8ba1\u7684\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u5728\u6355\u6349\u7ec6\u5fae\u611f\u77e5\u53d8\u5316\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4e3a\u8bed\u97f3\u8868\u8fbe\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.14681", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14681", "abs": "https://arxiv.org/abs/2602.14681", "authors": ["Xingjian Wu", "Xvyuan Liu", "Junkai Lu", "Siyuan Wang", "Yang Shu", "Jilin Hu", "Chenjuan Guo", "Bin Yang"], "title": "ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies", "comment": null, "summary": "LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement.", "AI": {"tldr": "ST-EVO\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u65f6\u7a7a\u6f14\u5316\u89c6\u89d2\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u8c03\u5ea6\u5668\u5b9e\u73b0\u5bf9\u8bdd\u7ea7\u901a\u4fe1\u8c03\u5ea6\uff0c\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u81ea\u6211\u53cd\u9988\u5b66\u4e60\uff0c\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f975%-25%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u6f14\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u6f14\u5316\u6216\u65f6\u95f4\u6f14\u5316\u7684\u5355\u4e00\u7ef4\u5ea6\uff0c\u672a\u80fd\u5145\u5206\u6fc0\u53d1LLM\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65f6\u7a7a\u6f14\u5316\u65b9\u6cd5\u6765\u6784\u5efa\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u5de5\u4f5c\u6d41\u548c\u901a\u4fe1\u62d3\u6251\u3002", "method": "\u63d0\u51faST-EVO\u6846\u67b6\uff0c\u91c7\u7528\u65f6\u7a7a\u6f14\u5316\u89c6\u89d2\uff0c\u4f7f\u7528\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7d27\u51d1\u800c\u5f3a\u5927\u7684\u8c03\u5ea6\u5668\u5b9e\u73b0\u5bf9\u8bdd\u7ea7\u901a\u4fe1\u8c03\u5ea6\uff0c\u80fd\u591f\u611f\u77e5\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5177\u5907\u4ece\u79ef\u7d2f\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u81ea\u6211\u53cd\u9988\u80fd\u529b\u3002", "result": "\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u7ea65%-25%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "ST-EVO\u901a\u8fc7\u65f6\u7a7a\u6f14\u5316\u89c6\u89d2\u548c\u6d41\u5339\u914d\u8c03\u5ea6\u5668\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u4e3a\u81ea\u6f14\u5316MAS\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u6280\u672f\u8def\u7ebf\u3002"}}
{"id": "2602.14742", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14742", "abs": "https://arxiv.org/abs/2602.14742", "authors": ["Hadi Nemati", "\u00c1lvaro Ortega", "Enrique Lobato", "Luis Rouco"], "title": "A Multi-Bound Robust Optimization Approach for Renewable-Based VPP Market Participation Considering Intra-Hourly Uncertainty Exposure", "comment": null, "summary": "With the ongoing transition of electricity markets worldwide from hourly to intra-hourly bidding, market participants--especially Renewable Energy Sources (RES)--gain improved opportunities to adjust energy and reserve schedules and to benefit from more accurate higher-resolution forecasts. However, this shift requires participants to update decision-making frameworks and to strengthen uncertainty management in order to fully exploit the new market potential. In particular, Renewable-Based Virtual Power Plants (RVPPs) aggregating dispatchable and non-dispatchable RES must account for these changes through market-oriented scheduling methods that efficiently address multiple uncertainties, including electricity prices, RES generation, and demand consumption. In this vein, this paper proposes a multi-bound robust optimization framework to simultaneously capture these uncertainties, explicitly incorporate intra-hourly variability, and differentiate the deviation levels (frequent, moderate deviations and rare, extreme ones) of uncertain parameters. The proposed approach yields less conservative and more implementable bidding and scheduling decisions, thus improving RVPP profitability in both energy and reserve markets. Simulation studies compare the proposed method with standard robust optimization and evaluate the operational, market-strategy, and economic impacts of quarter-hourly versus hourly market resolution. Results indicate that the normalized absolute differences, across different uncertainty-handling strategies, between hourly and 15-minute schedules are 18.0--34.2% for day-ahead traded energy, and 28.7--65.6% and 10.1--16.3% for upward and downward reserve traded in the secondary reserve market, respectively. Furthermore, relative to classic robust optimization, the proposed multi-bound approach increases profit by 24.9--49.2% across the considered strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8fb9\u754c\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u865a\u62df\u7535\u5382\u5728\u65e5\u5185\u7535\u529b\u5e02\u573a\u4e2d\u7684\u6295\u6807\u4e0e\u8c03\u5ea6\u51b3\u7b56\uff0c\u901a\u8fc7\u533a\u5206\u4e0d\u540c\u504f\u5dee\u6c34\u5e73\u7684\u4e0d\u786e\u5b9a\u6027\u53c2\u6570\uff0c\u51cf\u5c11\u4fdd\u5b88\u6027\u5e76\u63d0\u9ad8\u5229\u6da6\u3002", "motivation": "\u968f\u7740\u5168\u7403\u7535\u529b\u5e02\u573a\u4ece\u5c0f\u65f6\u7ea7\u5411\u65e5\u5185\u7ea7\u6295\u6807\u8f6c\u53d8\uff0c\u53ef\u518d\u751f\u80fd\u6e90\u53c2\u4e0e\u8005\u9700\u8981\u66f4\u65b0\u51b3\u7b56\u6846\u67b6\u548c\u52a0\u5f3a\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\uff0c\u4ee5\u5145\u5206\u5229\u7528\u65b0\u7684\u5e02\u573a\u6f5c\u529b\u3002\u7279\u522b\u662f\u53ef\u518d\u751f\u80fd\u6e90\u865a\u62df\u7535\u5382\u9700\u8981\u5e02\u573a\u5bfc\u5411\u7684\u8c03\u5ea6\u65b9\u6cd5\u6765\u6709\u6548\u5e94\u5bf9\u7535\u4ef7\u3001\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u548c\u9700\u6c42\u6d88\u8d39\u7b49\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u591a\u8fb9\u754c\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u6355\u6349\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u660e\u786e\u7eb3\u5165\u65e5\u5185\u53d8\u5f02\u6027\uff0c\u5e76\u533a\u5206\u4e0d\u786e\u5b9a\u53c2\u6570\u7684\u504f\u5dee\u6c34\u5e73\uff08\u9891\u7e41\u3001\u9002\u5ea6\u504f\u5dee\u548c\u7f55\u89c1\u3001\u6781\u7aef\u504f\u5dee\uff09\u3002\u8be5\u65b9\u6cd5\u4ea7\u751f\u66f4\u5c11\u4fdd\u5b88\u3001\u66f4\u53ef\u5b9e\u65bd\u7684\u6295\u6807\u548c\u8c03\u5ea6\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u9c81\u68d2\u4f18\u5316\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u591a\u8fb9\u754c\u65b9\u6cd5\u4f7f\u5229\u6da6\u63d0\u9ad8\u4e8624.9-49.2%\u3002\u5728\u5c0f\u65f6\u4e0e15\u5206\u949f\u5e02\u573a\u5206\u8fa8\u7387\u4e0b\uff0c\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u7b56\u7565\u4e4b\u95f4\u7684\u5f52\u4e00\u5316\u7edd\u5bf9\u5dee\u5f02\u4e3a\uff1a\u65e5\u524d\u4ea4\u6613\u80fd\u91cf18.0-34.2%\uff0c\u4e8c\u7ea7\u5907\u7528\u5e02\u573a\u4e2d\u5411\u4e0a\u5907\u752828.7-65.6%\uff0c\u5411\u4e0b\u5907\u752810.1-16.3%\u3002", "conclusion": "\u591a\u8fb9\u754c\u9c81\u68d2\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u53ef\u518d\u751f\u80fd\u6e90\u865a\u62df\u7535\u5382\u5728\u65e5\u5185\u7535\u529b\u5e02\u573a\u4e2d\u63d0\u4f9b\u66f4\u4f18\u7684\u6295\u6807\u548c\u8c03\u5ea6\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u9ad8\u5176\u5728\u80fd\u6e90\u548c\u5907\u7528\u5e02\u573a\u7684\u76c8\u5229\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5e02\u573a\u5206\u8fa8\u7387\u4ece\u5c0f\u65f6\u7ea7\u8f6c\u5411\u65e5\u5185\u7ea7\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.14476", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14476", "abs": "https://arxiv.org/abs/2602.14476", "authors": ["Pronoy Patra", "Sankarshan Damle", "Manisha Padala", "Sujit Gujar"], "title": "Truthful Reverse Auctions for Adaptive Selection via Contextual Multi-Armed Bandits", "comment": "20 pages, 6 figures", "summary": "We study the problem of selecting large language models (LLMs) for user queries in settings where multiple LLM providers submit the cost of solving a query. From the users' perspective, choosing an optimal model is a sequential, query-dependent decision problem: high-capacity models offer more reliable outputs but are costlier, while lightweight models are faster and cheaper. We formalize this interaction as a reverse auction design problem with contextual online learning, where the user adaptively discovers which model performs best while eliciting costs from competing LLM providers. Existing multi-armed bandit (MAB) mechanisms focus on forward auctions and social welfare, leaving open the challenges of reverse auctions, provider-optimal outcomes, and contextual adaptation. We address these gaps by designing a resampling-based procedure that generalizes truthful forward MAB mechanisms to reverse auctions and prove that any monotone allocation rule with this procedure is truthful. Using this, we propose a contextual MAB algorithm that learns query-dependent model quality with sublinear regret. Our framework unifies mechanism design and adaptive learning, enabling efficient, truthful, and query-aware LLM selection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5728\u7ebf\u5b66\u4e60\u7684\u53cd\u5411\u62cd\u5356\u673a\u5236\uff0c\u7528\u4e8e\u5728\u591aLLM\u63d0\u4f9b\u5546\u73af\u5883\u4e2d\u6839\u636e\u7528\u6237\u67e5\u8be2\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u8bc1\u8bda\u5b9e\u62a5\u4ef7\u3002", "motivation": "\u5728\u591aLLM\u63d0\u4f9b\u5546\u73af\u5883\u4e2d\uff0c\u7528\u6237\u9762\u4e34\u4e00\u4e2a\u987a\u5e8f\u7684\u3001\u67e5\u8be2\u76f8\u5173\u7684\u51b3\u7b56\u95ee\u9898\uff1a\u9ad8\u5bb9\u91cf\u6a21\u578b\u8f93\u51fa\u66f4\u53ef\u9760\u4f46\u6210\u672c\u66f4\u9ad8\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u66f4\u5feb\u66f4\u4fbf\u5b9c\u3002\u73b0\u6709MAB\u673a\u5236\u4e3b\u8981\u5173\u6ce8\u6b63\u5411\u62cd\u5356\u548c\u793e\u4f1a\u798f\u5229\uff0c\u7f3a\u4e4f\u5bf9\u53cd\u5411\u62cd\u5356\u3001\u63d0\u4f9b\u5546\u6700\u4f18\u7ed3\u679c\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u7684\u652f\u6301\u3002", "method": "\u5c06LLM\u9009\u62e9\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5177\u6709\u4e0a\u4e0b\u6587\u5728\u7ebf\u5b66\u4e60\u7684\u53cd\u5411\u62cd\u5356\u8bbe\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u91cd\u91c7\u6837\u8fc7\u7a0b\u5c06\u8bda\u5b9e\u7684\u6b63\u5411MAB\u673a\u5236\u63a8\u5e7f\u5230\u53cd\u5411\u62cd\u5356\uff0c\u8bc1\u660e\u4efb\u4f55\u5355\u8c03\u5206\u914d\u89c4\u5219\u7ed3\u5408\u6b64\u8fc7\u7a0b\u90fd\u662f\u8bda\u5b9e\u7684\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e0a\u4e0b\u6587MAB\u7b97\u6cd5\uff0c\u5b66\u4e60\u67e5\u8be2\u76f8\u5173\u7684\u6a21\u578b\u8d28\u91cf\u5e76\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002", "result": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u673a\u5236\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u8bda\u5b9e\u4e14\u67e5\u8be2\u611f\u77e5\u7684LLM\u9009\u62e9\uff0c\u7b97\u6cd5\u80fd\u591f\u5b66\u4e60\u67e5\u8be2\u76f8\u5173\u7684\u6a21\u578b\u8d28\u91cf\u5e76\u8fbe\u5230\u6b21\u7ebf\u6027\u9057\u61be\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u53cd\u5411\u62cd\u5356\u3001\u63d0\u4f9b\u5546\u6700\u4f18\u7ed3\u679c\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u7684\u6311\u6218\uff0c\u4e3a\u591aLLM\u63d0\u4f9b\u5546\u73af\u5883\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8bda\u5b9e\u4e14\u67e5\u8be2\u611f\u77e5\u7684LLM\u9009\u62e9\u673a\u5236\u3002"}}
{"id": "2602.13629", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13629", "abs": "https://arxiv.org/abs/2602.13629", "authors": ["Tarek Rahman", "Md Shaharia Hossen", "Mark Protik Mondol", "Jannatun Noor Mukta"], "title": "Search in Transition: A Study of University Students Perspectives on Using LLMs and Traditional Search Engines in English Test Problem Solving for Higher Study", "comment": "arXiv admin note: substantial text overlap with arXiv:2510.17726 by other authors", "summary": "With the growing integration of Artificial Intelligence (AI) in educational contexts, university students preparing for English language tests increasingly alternate between traditional search engines, such as Google, and large language models (LLMs) to support their test-related problem-solving. This study examines students perceptions of these tools, focusing on usability, efficiency, and their integration into English language test preparation workflows.Using a mixed-methods approach, we surveyed 140 university students from diverse academic disciplines and conducted in-depth interviews with 20 participants. Quantitative analyses, including ANOVA and chi-square tests, were employed to evaluate differences in perceived efficiency, satisfaction, and overall tool preference. The qualitative findings indicate that students frequently switch between GPT and Google depending on task demands, relying on Google for credible, multi-source information and rule verification, while using GPT for summarization, explanation, paraphrasing, and drafting responses for English test tasks. As neither tool alone was found to adequately support all aspects of English language test problem solving, participants expressed a strong preference for a hybrid solution. In response, we propose a prototype in the form of a chatbot embedded within a search interface, combining GPTs conversational strengths with Google reliability to improve English language test preparation and reduce cognitive load.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5927\u5b66\u751f\u5728\u82f1\u8bed\u8003\u8bd5\u51c6\u5907\u4e2d\u4ea4\u66ff\u4f7f\u7528Google\u548cGPT\u7b49AI\u5de5\u5177\u7684\u60c5\u51b5\uff0c\u53d1\u73b0\u5b66\u751f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u5207\u6362\u5de5\u5177\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u539f\u578b\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u9886\u57df\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u5927\u5b66\u751f\u5728\u51c6\u5907\u82f1\u8bed\u8003\u8bd5\u65f6\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\uff08\u5982Google\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT\uff09\u4e4b\u95f4\u5207\u6362\u6765\u652f\u6301\u8003\u8bd5\u76f8\u5173\u7684\u95ee\u9898\u89e3\u51b3\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5b66\u751f\u5bf9\u8fd9\u4e24\u79cd\u5de5\u5177\u7684\u8ba4\u77e5\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u878d\u5165\u82f1\u8bed\u8003\u8bd5\u51c6\u5907\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u8c03\u67e5\u4e86140\u540d\u6765\u81ea\u4e0d\u540c\u5b66\u79d1\u7684\u5927\u5b66\u751f\uff0c\u5e76\u5bf920\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u6df1\u5ea6\u8bbf\u8c08\u3002\u4f7f\u7528ANOVA\u548c\u5361\u65b9\u68c0\u9a8c\u7b49\u5b9a\u91cf\u5206\u6790\u65b9\u6cd5\u8bc4\u4f30\u611f\u77e5\u6548\u7387\u3001\u6ee1\u610f\u5ea6\u548c\u5de5\u5177\u504f\u597d\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u5728GPT\u548cGoogle\u4e4b\u95f4\u9891\u7e41\u5207\u6362\uff1a\u4f7f\u7528Google\u83b7\u53d6\u53ef\u4fe1\u7684\u591a\u6e90\u4fe1\u606f\u548c\u89c4\u5219\u9a8c\u8bc1\uff0c\u4f7f\u7528GPT\u8fdb\u884c\u603b\u7ed3\u3001\u89e3\u91ca\u3001\u6539\u5199\u548c\u4e3a\u82f1\u8bed\u8003\u8bd5\u4efb\u52a1\u8d77\u8349\u56de\u7b54\u3002\u5355\u4e00\u5de5\u5177\u65e0\u6cd5\u5145\u5206\u652f\u6301\u82f1\u8bed\u8003\u8bd5\u95ee\u9898\u89e3\u51b3\u7684\u6240\u6709\u65b9\u9762\uff0c\u53c2\u4e0e\u8005\u5f3a\u70c8\u504f\u597d\u6df7\u5408\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u539f\u578b\u89e3\u51b3\u65b9\u6848\uff1a\u5728\u641c\u7d22\u754c\u9762\u4e2d\u5d4c\u5165\u804a\u5929\u673a\u5668\u4eba\uff0c\u7ed3\u5408GPT\u7684\u5bf9\u8bdd\u4f18\u52bf\u548cGoogle\u7684\u53ef\u9760\u6027\uff0c\u4ee5\u6539\u5584\u82f1\u8bed\u8003\u8bd5\u51c6\u5907\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\u3002"}}
{"id": "2602.13577", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13577", "abs": "https://arxiv.org/abs/2602.13577", "authors": ["Faizan M. Tariq", "Avinash Singh", "Vipul Ramtekkar", "Jovin D'sa", "David Isele", "Yosuke Sakamoto", "Sangjae Bae"], "title": "ONRAP: Occupancy-driven Noise-Resilient Autonomous Path Planning", "comment": "8 pages, 9 figures - Presented at 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Dynamic path planning must remain reliable in the presence of sensing noise, uncertain localization, and incomplete semantic perception. We propose a practical, implementation-friendly planner that operates on occupancy grids and optionally incorporates occupancy-flow predictions to generate ego-centric, kinematically feasible paths that safely navigate through static and dynamic obstacles. The core is a nonlinear program in the spatial domain built on a modified bicycle model with explicit feasibility and collision-avoidance penalties. The formulation naturally handles unknown obstacle classes and heterogeneous agent motion by operating purely in occupancy space. The pipeline runs in real-time (faster than 10 Hz on average), requires minimal tuning, and interfaces cleanly with standard control stacks. We validate our approach in simulation with severe localization and perception noises, and on an F1TENTH platform, demonstrating smooth and safe maneuvering through narrow passages and rough routes. The approach provides a robust foundation for noise-resilient, prediction-aware planning, eliminating the need for handcrafted heuristics. The project website can be accessed at https://honda-research-institute.github.io/onrap/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5360\u7528\u7f51\u683c\u7684\u52a8\u6001\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u89c4\u5212\u751f\u6210\u6ee1\u8db3\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u5b89\u5168\u8def\u5f84\uff0c\u80fd\u591f\u5904\u7406\u611f\u77e5\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u65f6\u8fd0\u884c\u4e14\u65e0\u9700\u590d\u6742\u8c03\u53c2\u3002", "motivation": "\u52a8\u6001\u8def\u5f84\u89c4\u5212\u9700\u8981\u5728\u5b58\u5728\u611f\u77e5\u566a\u58f0\u3001\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u5b8c\u5168\u8bed\u4e49\u611f\u77e5\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u3001\u5b9e\u73b0\u53cb\u597d\u7684\u89c4\u5212\u5668\u6765\u5b89\u5168\u5bfc\u822a\u9759\u6001\u548c\u52a8\u6001\u969c\u788d\u7269\u3002", "method": "\u57fa\u4e8e\u5360\u7528\u7f51\u683c\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u53ef\u9009\u5730\u7ed3\u5408\u5360\u7528\u6d41\u9884\u6d4b\uff0c\u4f7f\u7528\u6539\u8fdb\u7684\u81ea\u884c\u8f66\u6a21\u578b\u6784\u5efa\u7a7a\u95f4\u57df\u975e\u7ebf\u6027\u89c4\u5212\uff0c\u5305\u542b\u663e\u5f0f\u53ef\u884c\u6027\u548c\u907f\u78b0\u60e9\u7f5a\uff0c\u5728\u5360\u7528\u7a7a\u95f4\u4e2d\u5904\u7406\u672a\u77e5\u969c\u788d\u7269\u7c7b\u522b\u548c\u5f02\u6784\u667a\u80fd\u4f53\u8fd0\u52a8\u3002", "result": "\u65b9\u6cd5\u5b9e\u65f6\u8fd0\u884c\uff08\u5e73\u5747\u5feb\u4e8e10Hz\uff09\uff0c\u9700\u8981\u6700\u5c0f\u8c03\u53c2\uff0c\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u5728\u4e25\u91cd\u5b9a\u4f4d\u548c\u611f\u77e5\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5728F1TENTH\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u901a\u8fc7\u72ed\u7a84\u901a\u9053\u548c\u7c97\u7cd9\u8def\u7ebf\u7684\u5e73\u6ed1\u5b89\u5168\u673a\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u566a\u58f0\u5f39\u6027\u548c\u9884\u6d4b\u611f\u77e5\u89c4\u5212\u63d0\u4f9b\u4e86\u9c81\u68d2\u57fa\u7840\uff0c\u6d88\u9664\u4e86\u5bf9\u624b\u5de5\u542f\u53d1\u5f0f\u89c4\u5219\u7684\u9700\u6c42\uff0c\u80fd\u591f\u53ef\u9760\u5904\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u5e76\u751f\u6210\u5b89\u5168\u53ef\u884c\u7684\u8def\u5f84\u3002"}}
{"id": "2602.14224", "categories": ["cs.SD", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.14224", "abs": "https://arxiv.org/abs/2602.14224", "authors": ["Ziyang Ma", "Ruiyang Xu", "Yinghao Ma", "Chao-Han Huck Yang", "Bohan Li", "Jaeyeon Kim", "Jin Xu", "Jinyu Li", "Carlos Busso", "Kai Yu", "Eng Siong Chng", "Xie Chen"], "title": "The Interspeech 2026 Audio Reasoning Challenge: Evaluating Reasoning Process Quality for Audio Reasoning Models and Agents", "comment": "The official website of the Audio Reasoning Challenge: https://audio-reasoning-challenge.github.io", "summary": "Recent Large Audio Language Models (LALMs) excel in understanding but often lack transparent reasoning. To address this \"black-box\" limitation, we organized the Audio Reasoning Challenge at Interspeech 2026, the first shared task dedicated to evaluating Chain-of-Thought (CoT) quality in the audio domain. The challenge introduced MMAR-Rubrics, a novel instance-level protocol assessing the factuality and logic of reasoning chains. Featured Single Model and Agent tracks, the competition attracting 156 teams from 18 countries and regions. Results show agent systems currently lead in reasoning quality, utilizing iterative tool orchestration and cross-modal analysis. Besides, single models are rapidly advancing via reinforcement learning and sophisticated data pipeline. We details the challenge design, methodology, and a comprehensive analysis of state-of-the-art systems, providing new insights for explainable audio intelligence.", "AI": {"tldr": "Interspeech 2026\u97f3\u9891\u63a8\u7406\u6311\u6218\u8d5b\u9996\u6b21\u8bc4\u4f30\u97f3\u9891\u9886\u57df\u7684\u601d\u7ef4\u94fe\u8d28\u91cf\uff0c\u5f15\u5165MMAR-Rubrics\u8bc4\u4f30\u6846\u67b6\uff0c\u5438\u5f15\u4e86\u5168\u7403156\u652f\u56e2\u961f\u53c2\u4e0e\uff0c\u7ed3\u679c\u663e\u793a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u9886\u5148\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u867d\u7136\u7406\u89e3\u80fd\u529b\u5f3a\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b58\u5728\"\u9ed1\u7bb1\"\u9650\u5236\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63d0\u5347\u97f3\u9891\u9886\u57df\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u7ec4\u7ec7Audio Reasoning Challenge\u7ade\u8d5b\uff0c\u5f15\u5165MMAR-Rubrics\u5b9e\u4f8b\u7ea7\u8bc4\u4f30\u534f\u8bae\uff0c\u8bc4\u4f30\u63a8\u7406\u94fe\u7684\u4e8b\u5b9e\u6027\u548c\u903b\u8f91\u6027\uff0c\u8bbe\u7f6e\u5355\u6a21\u578b\u548c\u667a\u80fd\u4f53\u4e24\u4e2a\u8d5b\u9053\u3002", "result": "\u5438\u5f15\u4e86\u6765\u81ea18\u4e2a\u56fd\u5bb6\u548c\u5730\u533a\u7684156\u652f\u56e2\u961f\u53c2\u4e0e\uff0c\u7ed3\u679c\u663e\u793a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u8fc7\u8fed\u4ee3\u5de5\u5177\u7f16\u6392\u548c\u8de8\u6a21\u6001\u5206\u6790\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u9886\u5148\uff0c\u5355\u6a21\u578b\u7cfb\u7edf\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u590d\u6742\u6570\u636e\u7ba1\u9053\u4e5f\u5728\u5feb\u901f\u8fdb\u6b65\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u4e3a\u53ef\u89e3\u91ca\u97f3\u9891\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u97f3\u9891\u63a8\u7406\u6280\u672f\u7684\u53d1\u5c55\u73b0\u72b6\u548c\u672a\u6765\u65b9\u5411\uff0c\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5355\u6a21\u578b\u7cfb\u7edf\u5404\u6709\u4f18\u52bf\u5e76\u90fd\u5728\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2602.14780", "categories": ["cs.MA", "cs.CY", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14780", "abs": "https://arxiv.org/abs/2602.14780", "authors": ["Anna-Lena Schlamp", "Jeremias Gerner", "Klaus Bogenberger", "Werner Huber", "Stefanie Schmidtner"], "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic", "comment": "8 pages, 1 figure, 4 tables, 2026 IEEE International Conference on Intelligent Transportation Systems (ITSC)", "summary": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.", "AI": {"tldr": "ROSA\u7cfb\u7edf\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u4e0e\u534f\u8c03\u901f\u5ea6\u5f15\u5bfc\uff0c\u4e3a\u73af\u5c9b\u591a\u6a21\u6001\u6df7\u5408\u4ea4\u901a\u63d0\u4f9b\u4f18\u5316\u901f\u5ea6\u5efa\u8bae\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73af\u5c9b\u4f5c\u4e3a\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\uff0c\u5b58\u5728\u8f66\u8f86\u4e0e\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u6df7\u5408\u4ea4\u901a\uff0c\u9700\u8981\u6709\u6548\u7684\u9884\u6d4b\u548c\u534f\u8c03\u7cfb\u7edf\u6765\u63d0\u5347\u5b89\u5168\u6027\u548c\u901a\u884c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8054\u5408\u9884\u6d4b\u73af\u5c9b\u4e2d\u8f66\u8f86\u548cVRUs\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u901a\u8fc7\u5355\u6b65\u9884\u6d4b\u8bad\u7ec3\u548c\u81ea\u56de\u5f52\u90e8\u7f72\u751f\u6210\u786e\u5b9a\u6027\u8f93\u51fa\uff0c\u7ed3\u5408\u8fd0\u52a8\u52a8\u529b\u5b66\u548c\u8def\u7ebf\u610f\u56fe\u4fe1\u606f\uff0c\u57fa\u4e8e\u9884\u6d4b\u51b2\u7a81\u63d0\u4f9b\u5b9e\u65f6\u901f\u5ea6\u5efa\u8bae\u3002", "result": "\u6a21\u578b\u57285\u79d2\u9884\u6d4b\u8303\u56f4\u5185\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff08ADE: 1.29m, FDE: 2.99m\uff09\uff0c\u52a0\u5165\u8def\u7ebf\u610f\u56fe\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff08ADE: 1.10m, FDE: 2.36m\uff09\uff0c\u663e\u8457\u63d0\u5347\u8f66\u8f86\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4eceVRU\u89c6\u89d2\u4e5f\u6539\u5584\u4e86\u611f\u77e5\u5b89\u5168\u6027\u3002", "conclusion": "ROSA\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u548c\u534f\u8c03\u901f\u5ea6\u5f15\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73af\u5c9b\u6df7\u5408\u4ea4\u901a\u7684\u5b89\u5168\u548c\u6548\u7387\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u8f66\u8054\u7f51\u6570\u636e\u7684\u4ef7\u503c\uff0c\u6e90\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.14668", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14668", "abs": "https://arxiv.org/abs/2602.14668", "authors": ["Moshe Babaioff", "Gefen Frosh"], "title": "Near-Optimal Best-of-Both-Worlds Fairness for Few Agents", "comment": null, "summary": "We consider the problem of fair allocation of indivisible goods among agents with additive valuations, aiming for Best-of-Both-Worlds (BoBW) fairness: a distribution over allocations that is ex-ante fair, and additionally, it is supported only on deterministic allocations that are ex-post fair. We focus on BoBW for few agents, and our main result is the design of the first BoBW algorithms achieving near-optimal fairness for three agents. For three agents, we prove the existence of an ex-ante proportional distribution whose every allocation is Epistemic EFX (EEFX) and guarantees each agent at least $\\tfrac{9}{10}$ of her MMS. As MMS allocations do not exist for three additive agents, in every allocation at least one agent might not be getting her MMS. To compensate such an agent, we also guarantee that if an agent is not getting her MMS then she is EFX-satisfied - giving her the strongest achievable envy-based guarantee. Additionally, using an FPTAS for near-MMS partitions, we present an FPTAS to compute a BoBW distribution preserving all envy-based guarantees, and also preserving all value-based guarantees up to $(1-\\varepsilon)$. We further show that exact ex-ante proportionality can be restored when dropping EEFX. To do so, we first design, for two agents and any $\\varepsilon > 0$, a Fully Polynomial-Time Approximation Scheme (FPTAS) that outputs a distribution which is ex-ante envy-free (and thus proportional) and ex-post envy-free up to any good (EFX), while guaranteeing each agent at least a $(1-\\varepsilon)$-fraction of her maximin share (MMS). We then leverage this two-agent FPTAS algorithm as a subroutine to obtain, for three agents, the FPTAS guaranteeing exact ex-ante proportionality. We note that our result for two agents essentially matches the strongest fairness and efficiency guarantees achievable in polynomial time, and thus might be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u53ef\u5206\u5272\u7269\u54c1\u5728\u52a0\u6027\u4f30\u503c\u4e0b\u7684\u516c\u5e73\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\"\u4e24\u5168\u5176\u7f8e\"\uff08BoBW\uff09\u516c\u5e73\u6027\u6846\u67b6\uff1a\u65e2\u4fdd\u8bc1\u4e8b\u524d\u516c\u5e73\u7684\u5206\u5e03\uff0c\u53c8\u786e\u4fdd\u5206\u5e03\u4e2d\u6240\u6709\u786e\u5b9a\u6027\u5206\u914d\u90fd\u6ee1\u8db3\u4e8b\u540e\u516c\u5e73\u3002\u9488\u5bf9\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\uff0c\u8bbe\u8ba1\u4e86\u9996\u4e2a\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u516c\u5e73\u6027\u7684BoBW\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u516c\u5e73\u5206\u914d\u7814\u7a76\u901a\u5e38\u5355\u72ec\u8003\u8651\u4e8b\u524d\u516c\u5e73\uff08\u5982\u6bd4\u4f8b\u6027\uff09\u6216\u4e8b\u540e\u516c\u5e73\uff08\u5982EFX\u3001MMS\uff09\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u79cd\u516c\u5e73\u6027\u3002BoBW\u6846\u67b6\u65e8\u5728\u8bbe\u8ba1\u65e2\u6ee1\u8db3\u4e8b\u524d\u516c\u5e73\u5206\u5e03\uff0c\u53c8\u4fdd\u8bc1\u5206\u5e03\u4e2d\u6bcf\u4e2a\u5177\u4f53\u5206\u914d\u90fd\u6ee1\u8db3\u4e8b\u540e\u516c\u5e73\u7684\u5206\u914d\u65b9\u6848\uff0c\u8fd9\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "1. \u9488\u5bf9\u4e09\u4e2a\u667a\u80fd\u4f53\uff0c\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u4e2a\u4e8b\u524d\u6bd4\u4f8b\u5206\u5e03\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5206\u914d\u90fd\u6ee1\u8db3\u8ba4\u77e5EFX\uff08EEFX\uff09\uff0c\u5e76\u4fdd\u8bc1\u6bcf\u4e2a\u667a\u80fd\u4f53\u81f3\u5c11\u83b7\u5f97\u5176MMS\u76849/10\u30022. \u5f53\u667a\u80fd\u4f53\u672a\u83b7\u5f97MMS\u65f6\uff0c\u4fdd\u8bc1\u5176\u6ee1\u8db3EFX\u30023. \u4f7f\u7528FPTAS\u8fd1\u4f3cMMS\u5212\u5206\uff0c\u8bbe\u8ba1\u4e86\u8ba1\u7b97BoBW\u5206\u5e03\u7684FPTAS\u7b97\u6cd5\uff0c\u4fdd\u6301\u6240\u6709\u57fa\u4e8e\u5ac9\u5992\u7684\u4fdd\u8bc1\uff0c\u5e76\u5c06\u57fa\u4e8e\u4ef7\u503c\u7684\u4fdd\u8bc1\u4fdd\u6301\u5230(1-\u03b5)\u30024. \u5bf9\u4e8e\u4e24\u4e2a\u667a\u80fd\u4f53\uff0c\u8bbe\u8ba1\u4e86FPTAS\u8f93\u51fa\u4e8b\u524d\u65e0\u5ac9\u5992\uff08\u56e0\u6b64\u6bd4\u4f8b\uff09\u4e14\u4e8b\u540eEFX\u7684\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u8bc1\u6bcf\u4e2a\u667a\u80fd\u4f53\u81f3\u5c11\u83b7\u5f97(1-\u03b5)\u6bd4\u4f8b\u7684MMS\u30025. \u5c06\u4e24\u667a\u80fd\u4f53FPTAS\u4f5c\u4e3a\u5b50\u7a0b\u5e8f\uff0c\u4e3a\u4e09\u4e2a\u667a\u80fd\u4f53\u83b7\u5f97\u4fdd\u8bc1\u7cbe\u786e\u4e8b\u524d\u6bd4\u4f8b\u6027\u7684FPTAS\u3002", "result": "1. \u4e3a\u4e09\u4e2a\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e86\u9996\u4e2a\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u516c\u5e73\u6027\u7684BoBW\u7b97\u6cd5\u30022. \u8bc1\u660e\u4e86\u5b58\u5728\u4e8b\u524d\u6bd4\u4f8b\u5206\u5e03\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5206\u914d\u90fd\u6ee1\u8db3EEFX\u4e14\u4fdd\u8bc1\u6bcf\u4e2a\u667a\u80fd\u4f53\u81f3\u5c11\u83b7\u5f97\u5176MMS\u76849/10\u30023. \u5f53\u667a\u80fd\u4f53\u672a\u83b7\u5f97MMS\u65f6\uff0c\u4fdd\u8bc1\u5176\u6ee1\u8db3EFX\u30024. \u8bbe\u8ba1\u4e86FPTAS\u8ba1\u7b97BoBW\u5206\u5e03\uff0c\u4fdd\u6301\u6240\u6709\u5ac9\u5992\u4fdd\u8bc1\u548c(1-\u03b5)\u7684\u4ef7\u503c\u4fdd\u8bc1\u30025. \u5bf9\u4e8e\u4e24\u4e2a\u667a\u80fd\u4f53\uff0c\u8bbe\u8ba1\u4e86FPTAS\u5b9e\u73b0\u4e8b\u524d\u65e0\u5ac9\u5992\u3001\u4e8b\u540eEFX\uff0c\u5e76\u4fdd\u8bc1(1-\u03b5)\u6bd4\u4f8b\u7684MMS\uff0c\u8fd9\u57fa\u672c\u4e0a\u5339\u914d\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u8fbe\u5230\u7684\u6700\u5f3a\u516c\u5e73\u6027\u548c\u6548\u7387\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u8bba\u6587\u5728BoBW\u516c\u5e73\u5206\u914d\u6846\u67b6\u4e0b\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e09\u4e2a\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e86\u9996\u4e2a\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u516c\u5e73\u6027\u7684\u7b97\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u4e8b\u524d\u6bd4\u4f8b\u6027\u548c\u4e8b\u540e\u516c\u5e73\u6027\uff08EEFX\u3001MMS\u8fd1\u4f3c\u3001EFX\uff09\uff0c\u5e76\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u8fd9\u4e9b\u4fdd\u8bc1\uff0c\u4e3a\u516c\u5e73\u5206\u914d\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u89c6\u89d2\u3002\u4e24\u667a\u80fd\u4f53\u7684FPTAS\u7ed3\u679c\u5177\u6709\u72ec\u7acb\u7684\u7406\u8bba\u4ef7\u503c\uff0c\u57fa\u672c\u4e0a\u8fbe\u5230\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u8fbe\u5230\u7684\u6700\u5f3a\u516c\u5e73\u6027\u548c\u6548\u7387\u4fdd\u8bc1\u3002"}}
{"id": "2602.13675", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13675", "abs": "https://arxiv.org/abs/2602.13675", "authors": ["Fei Wang", "Yifan Zhang", "Brian Y. Lim"], "title": "Transferable XAI: Relating Understanding Across Domains with Explanation Transfer", "comment": "40 pages, accepted by IUI2026", "summary": "Current Explainable AI (XAI) focuses on explaining a single application, but when encountering related applications, users may rely on their prior understanding from previous explanations. This leads to either overgeneralization and AI overreliance, or burdensome independent memorization. Indeed, related decision tasks can share explanatory factors, but with some notable differences; e.g., body mass index (BMI) affects the risks for heart disease and diabetes at the same rate, but chest pain is more indicative of heart disease. Similarly, models using different attributes for the same task still share signals; e.g., temperature and pressure affect air pollution but in opposite directions due to the ideal gas law. Leveraging transfer of learning, we propose Transferable XAI to enable users to transfer understanding across related domains by explaining the relationship between domain explanations using a general affine transformation framework applied to linear factor explanations. The framework supports explanation transfer across various domain types: translation for data subspace (subsuming prior work on Incremental XAI), scaling for decision task, and mapping for attributes. Focusing on task and attributes domain types, in formative and summative user studies, we investigated how well participants could understand AI decisions from one domain to another. Compared to single-domain and domain-independent explanations, Transferable XAI was the most helpful for understanding the second domain, leading to the best decision faithfulness, factor recall, and ability to relate explanations between domains. This framework contributes to improving the reusability of explanations across related AI applications by explaining factor relationships between subspaces, tasks, and attributes.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u8fc1\u79fbXAI\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u56e0\u5b50\u89e3\u91ca\u7684\u4eff\u5c04\u53d8\u6362\uff0c\u8ba9\u7528\u6237\u80fd\u5728\u76f8\u5173\u9886\u57df\u95f4\u8fc1\u79fb\u7406\u89e3\uff0c\u89e3\u51b3\u73b0\u6709XAI\u53ea\u80fd\u89e3\u91ca\u5355\u4e00\u5e94\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91caAI\u53ea\u5173\u6ce8\u89e3\u91ca\u5355\u4e2a\u5e94\u7528\uff0c\u4f46\u5f53\u9047\u5230\u76f8\u5173\u5e94\u7528\u65f6\uff0c\u7528\u6237\u53ef\u80fd\u4f9d\u8d56\u5148\u524d\u89e3\u91ca\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u6cdb\u5316\u548cAI\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u6216\u9700\u8981\u8d1f\u62c5\u72ec\u7acb\u7684\u8bb0\u5fc6\u3002\u76f8\u5173\u51b3\u7b56\u4efb\u52a1\u53ef\u4ee5\u5171\u4eab\u89e3\u91ca\u56e0\u5b50\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u53ef\u8fc1\u79fbXAI\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ebf\u6027\u56e0\u5b50\u89e3\u91ca\u5e94\u7528\u4e8e\u4e00\u822c\u4eff\u5c04\u53d8\u6362\u6846\u67b6\uff0c\u89e3\u91ca\u9886\u57df\u95f4\u89e3\u91ca\u7684\u5173\u7cfb\uff0c\u652f\u6301\u8de8\u6570\u636e\u5b50\u7a7a\u95f4\u3001\u51b3\u7b56\u4efb\u52a1\u548c\u5c5e\u6027\u7684\u89e3\u91ca\u8fc1\u79fb\u3002", "result": "\u5728\u5f62\u6210\u6027\u548c\u603b\u7ed3\u6027\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u5355\u9886\u57df\u548c\u9886\u57df\u72ec\u7acb\u89e3\u91ca\uff0c\u53ef\u8fc1\u79fbXAI\u6700\u80fd\u5e2e\u52a9\u7406\u89e3\u7b2c\u4e8c\u4e2a\u9886\u57df\uff0c\u83b7\u5f97\u6700\u4f73\u51b3\u7b56\u5fe0\u5b9e\u5ea6\u3001\u56e0\u5b50\u56de\u5fc6\u80fd\u529b\u548c\u9886\u57df\u95f4\u89e3\u91ca\u5173\u8054\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u91ca\u5b50\u7a7a\u95f4\u3001\u4efb\u52a1\u548c\u5c5e\u6027\u95f4\u7684\u56e0\u5b50\u5173\u7cfb\uff0c\u63d0\u9ad8\u4e86\u76f8\u5173AI\u5e94\u7528\u4e2d\u89e3\u91ca\u7684\u53ef\u91cd\u7528\u6027\u3002"}}
{"id": "2602.13579", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13579", "abs": "https://arxiv.org/abs/2602.13579", "authors": ["Youngsun Wi", "Jessica Yin", "Elvis Xiang", "Akash Sharma", "Jitendra Malik", "Mustafa Mukadam", "Nima Fazeli", "Tess Hellebrekers"], "title": "TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment", "comment": "Website: https://yswi.github.io/tactalign/", "summary": "Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).", "AI": {"tldr": "TactAlign\u662f\u4e00\u79cd\u8de8\u5177\u8eab\u89e6\u89c9\u5bf9\u9f50\u65b9\u6cd5\uff0c\u53ef\u5c06\u4eba\u7c7b\u7a7f\u6234\u8bbe\u5907\u6536\u96c6\u7684\u89e6\u89c9\u4fe1\u53f7\u8f6c\u79fb\u5230\u4e0d\u540c\u5177\u8eab\u7684\u673a\u5668\u4eba\u4e0a\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u6216\u624b\u52a8\u6807\u6ce8\uff0c\u901a\u8fc7\u6574\u6d41\u6d41\u5b9e\u73b0\u5171\u4eab\u6f5c\u5728\u8868\u793a\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\uff08\u5982\u89e6\u89c9\u624b\u5957\uff09\u6536\u96c6\u7684\u6f14\u793a\u4e3a\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u5feb\u901f\u7075\u5de7\u7684\u76d1\u7763\uff0c\u4f46\u5982\u4f55\u5c06\u4eba\u7c7b\u6536\u96c6\u7684\u89e6\u89c9\u4fe1\u53f7\u8f6c\u79fb\u5230\u4e0d\u540c\u4f20\u611f\u6a21\u6001\u548c\u5177\u8eab\u7684\u673a\u5668\u4eba\u4e0a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u76f8\u540c\u89e6\u89c9\u4f20\u611f\u5668\u3001\u9700\u8981\u914d\u5bf9\u6570\u636e\u4e14\u5177\u8eab\u5dee\u8ddd\u5c0f\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faTactAlign\u65b9\u6cd5\uff0c\u4f7f\u7528\u6574\u6d41\u6d41\u5c06\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u7684\u89e6\u89c9\u89c2\u5bdf\u8f6c\u6362\u4e3a\u5171\u4eab\u6f5c\u5728\u8868\u793a\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u96c6\u3001\u624b\u52a8\u6807\u7b7e\u6216\u7279\u6743\u4fe1\u606f\u3002\u901a\u8fc7\u624b-\u7269\u4f53\u4ea4\u4e92\u884d\u751f\u7684\u4f2a\u5bf9\u5b9e\u73b0\u4f4e\u6210\u672c\u7684\u6f5c\u5728\u4f20\u8f93\u3002", "result": "TactAlign\u5728\u591a\u4e2a\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\uff08\u65cb\u8f6c\u3001\u63d2\u5165\u3001\u76d6\u5b50\u95ed\u5408\uff09\u4e2d\u6539\u8fdb\u4e86\u4eba\u673a\u7b56\u7565\u8f6c\u79fb\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u548c\u4efb\u52a1\uff08\u4f7f\u7528\u5c11\u4e8e5\u5206\u949f\u7684\u4eba\u7c7b\u6570\u636e\uff09\uff0c\u5e76\u5728\u9ad8\u5ea6\u7075\u5de7\u4efb\u52a1\uff08\u62e7\u706f\u6ce1\uff09\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u4eba\u673a\u8f6c\u79fb\u3002", "conclusion": "TactAlign\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8de8\u5177\u8eab\u89e6\u89c9\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u4eba\u7c7b\u89e6\u89c9\u4fe1\u53f7\u8f6c\u79fb\u5230\u4e0d\u540c\u5177\u8eab\u7684\u673a\u5668\u4eba\u4e0a\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u4eba\u673a\u7b56\u7565\u8f6c\u79fb\u7684\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2602.14291", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.14291", "abs": "https://arxiv.org/abs/2602.14291", "authors": ["H. M. Shadman Tabib", "Istiak Ahmmed Rifti", "Abdullah Muhammed Amimul Ehsan", "Somik Dasgupta", "Md Zim Mim Siddiqee Sowdha", "Abrar Jahin Sarker", "Md. Rafiul Islam Nijamy", "Tanvir Hossain", "Mst. Metaly Khatun", "Munzer Mahmood", "Rakesh Debnath", "Gourab Biswas", "Asif Karim", "Wahid Al Azad Navid", "Masnoon Muztahid", "Fuad Ahmed Udoy", "Shahad Shahriar Rahman", "Md. Tashdiqur Rahman Shifat", "Most. Sonia Khatun", "Mushfiqur Rahman", "Md. Miraj Hasan", "Anik Saha", "Mohammad Ninad Mahmud Nobo", "Soumik Bhattacharjee", "Tusher Bhomik", "Ahmmad Nur Swapnil", "Shahriar Kabir"], "title": "Bengali-Loop: Community Benchmarks for Long-Form Bangla ASR and Speaker Diarization", "comment": null, "summary": "Bengali (Bangla) remains under-resourced in long-form speech technology despite its wide use. We present Bengali-Loop, two community benchmarks to address this gap: (1) a long-form ASR corpus of 191 recordings (158.6 hours, 792k words) from 11 YouTube channels, collected via a reproducible subtitle-extraction pipeline and human-in-the-loop transcript verification; and (2) a speaker diarization corpus of 24 recordings (22 hours, 5,744 annotated segments) with fully manual speaker-turn labels in CSV format. Both benchmarks target realistic multi-speaker, long-duration content (e.g., Bangla drama/natok). We establish baselines (Tugstugi: 34.07% WER; pyannote.audio: 40.08% DER) and provide standardized evaluation protocols (WER/CER, DER), annotation rules, and data formats to support reproducible benchmarking and future model development for Bangla long-form ASR and diarization.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Bengali-Loop\uff0c\u8fd9\u662f\u4e24\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u957f\u8bed\u97f3\u5904\u7406\u7684\u793e\u533a\u57fa\u51c6\uff1a\u4e00\u4e2a\u5305\u542b191\u4e2a\u5f55\u97f3\uff08158.6\u5c0f\u65f6\uff0c79.2\u4e07\u5b57\uff09\u7684\u957f\u8bed\u97f3ASR\u8bed\u6599\u5e93\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5305\u542b24\u4e2a\u5f55\u97f3\uff0822\u5c0f\u65f6\uff0c5744\u4e2a\u6807\u6ce8\u7247\u6bb5\uff09\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bed\u957f\u8bed\u97f3\u6280\u672f\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5b5f\u52a0\u62c9\u8bed\uff08Bangla\uff09\u4f7f\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u957f\u8bed\u97f3\u6280\u672f\u65b9\u9762\u4ecd\u7136\u8d44\u6e90\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5b5f\u52a0\u62c9\u8bed\u7684\u957f\u8bed\u97f3\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\uff08diarization\uff09\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u53ef\u91cd\u590d\u7684\u5b57\u5e55\u63d0\u53d6\u6d41\u7a0b\u548c\u4eba\u5de5\u53c2\u4e0e\u5faa\u73af\u7684\u8f6c\u5f55\u9a8c\u8bc1\uff0c\u4ece11\u4e2aYouTube\u9891\u9053\u6536\u96c6\u4e86191\u4e2a\u957f\u8bed\u97f3\u5f55\u97f3\u3002\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u8bed\u6599\u5e93\u5305\u542b24\u4e2a\u5f55\u97f3\uff0c\u91c7\u7528\u5b8c\u5168\u624b\u52a8\u6807\u6ce8\u7684\u8bf4\u8bdd\u4eba\u8f6c\u6362\u6807\u7b7e\uff08CSV\u683c\u5f0f\uff09\u3002\u4e24\u4e2a\u57fa\u51c6\u90fd\u9488\u5bf9\u73b0\u5b9e\u7684\u591a\u8bf4\u8bdd\u4eba\u3001\u957f\u65f6\u957f\u5185\u5bb9\uff08\u5982\u5b5f\u52a0\u62c9\u620f\u5267\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u57fa\u7ebf\u6027\u80fd\uff1aTugstugi\u6a21\u578b\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4e3a34.07%\uff0cpyannote.audio\u6a21\u578b\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u9519\u8bef\u7387\uff08DER\uff09\u4e3a40.08%\u3002\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u534f\u8bae\uff08WER/CER\uff0cDER\uff09\u3001\u6807\u6ce8\u89c4\u5219\u548c\u6570\u636e\u683c\u5f0f\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u672a\u6765\u6a21\u578b\u5f00\u53d1\u3002", "conclusion": "Bengali-Loop\u4e3a\u5b5f\u52a0\u62c9\u8bed\u957f\u8bed\u97f3ASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u63d0\u4f9b\u4e86\u9996\u4e2a\u793e\u533a\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u8be5\u8bed\u8a00\u5728\u957f\u8bed\u97f3\u6280\u672f\u65b9\u9762\u7684\u8d44\u6e90\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.15006", "categories": ["cs.MA", "cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2602.15006", "abs": "https://arxiv.org/abs/2602.15006", "authors": ["Meet Gandhi", "George P. Kontoudis"], "title": "Distributed Quantum Gaussian Processes for Multi-Agent Systems", "comment": "9 pages, 4 figures, accepted at AAMAS 2026 (International Conference on Autonomous Agents and Multiagent Systems)", "summary": "Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u91cf\u5b50\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u589e\u5f3a\u5efa\u6a21\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u9ece\u66fcADMM\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u5728\u590d\u6742\u5927\u89c4\u6a21\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u53d7\u9650\u4e8e\u7ecf\u5178\u6838\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u91cf\u5b50\u8ba1\u7b97\u901a\u8fc7\u5728\u6307\u6570\u7ea7\u5927\u7684\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5d4c\u5165\u6570\u636e\uff0c\u53ef\u4ee5\u6355\u6349\u7ecf\u5178\u8ba1\u7b97\u65e0\u6cd5\u8bbf\u95ee\u7684\u590d\u6742\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u91cf\u5b50\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\uff0c\u5f00\u53d1\u5206\u5e03\u5f0f\u5171\u8bc6\u9ece\u66fc\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\u7b97\u6cd5\uff0c\u5c06\u5c40\u90e8\u667a\u80fd\u4f53\u6a21\u578b\u805a\u5408\u4e3a\u5168\u5c40\u6a21\u578b\uff0c\u89e3\u51b3\u975e\u6b27\u51e0\u91cc\u5f97\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u7ecf\u5178\u786c\u4ef6\u7684\u91cf\u5b50\u6a21\u62df\u5668\u4e0a\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u4f7f\u7528NASA\u822a\u5929\u98de\u673a\u96f7\u8fbe\u5730\u5f62\u4efb\u52a1\u7684\u771f\u5b9e\u975e\u5e73\u7a33\u9ad8\u7a0b\u6570\u636e\u96c6\u548c\u91cf\u5b50\u9ad8\u65af\u8fc7\u7a0b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5c55\u793a\u4e86\u5efa\u6a21\u4f18\u52bf\uff0c\u8fd8\u7a81\u663e\u4e86\u91cf\u5b50\u786c\u4ef6\u5728\u9ad8\u65af\u8fc7\u7a0b\u548c\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u53ef\u80fd\u63d0\u4f9b\u7684\u8ba1\u7b97\u52a0\u901f\u6f5c\u529b\u3002"}}
{"id": "2602.14815", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14815", "abs": "https://arxiv.org/abs/2602.14815", "authors": ["Ioannis Caragiannis", "Anders Bo Ipsen", "Stratis Skoulakis"], "title": "Revenue Guarantees in Autobidding Platforms", "comment": null, "summary": "Motivated by autobidding systems in online advertising, we study revenue maximization in markets with divisible goods and budget-constrained buyers with linear valuations. Our aim is to compute a single price for each good and an allocation that maximizes total revenue. We show that the First-Price Pacing Equilibrium (FPPE) guarantees at least half of the optimal revenue, even when compared to the maximal revenue of buyer-specific prices. This guarantee is particularly striking in light of our hardness result: we prove that revenue maximization under individual rationality and single-price-per-good constraints is APX-hard.\n  We further extend our analysis in two directions: first, we introduce an online analogue of FPPE and show that it achieves a constant-factor revenue guarantee, specifically a $1/4$-approximation; second, we consider buyers with concave valuation functions, characterizing an FPPE-type outcome as the solution to an Eisenberg-Gale-style convex program and showing that the revenue approximation degrades gracefully with the degree of nonlinearity of the valuations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u7ebf\u5e7f\u544a\u81ea\u52a8\u7ade\u4ef7\u7cfb\u7edf\u4e2d\u7684\u6536\u76ca\u6700\u5927\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u7b2c\u4e00\u4ef7\u683c\u6b65\u8c03\u5747\u8861(FPPE)\u5728\u5355\u5546\u54c1\u5355\u4ef7\u683c\u7ea6\u675f\u4e0b\u81f3\u5c11\u80fd\u83b7\u5f97\u6700\u4f18\u6536\u76ca\u7684\u4e00\u534a\uff0c\u5e76\u6269\u5c55\u5230\u5728\u7ebf\u573a\u666f\u548c\u51f9\u4f30\u503c\u51fd\u6570\u60c5\u51b5\u3002", "motivation": "\u53d7\u5728\u7ebf\u5e7f\u544a\u81ea\u52a8\u7ade\u4ef7\u7cfb\u7edf\u7684\u542f\u53d1\uff0c\u7814\u7a76\u53ef\u5206\u5272\u5546\u54c1\u5e02\u573a\u4e2d\u9884\u7b97\u7ea6\u675f\u4e70\u5bb6\u7684\u6536\u76ca\u6700\u5927\u5316\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u6bcf\u4e2a\u5546\u54c1\u8ba1\u7b97\u5355\u4e00\u4ef7\u683c\u548c\u5206\u914d\u65b9\u6848\u4ee5\u6700\u5927\u5316\u603b\u6536\u76ca\u3002", "method": "\u91c7\u7528\u7b2c\u4e00\u4ef7\u683c\u6b65\u8c03\u5747\u8861(FPPE)\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5728\u5355\u5546\u54c1\u5355\u4ef7\u683c\u7ea6\u675f\u4e0b\u7684\u6536\u76ca\u4fdd\u8bc1\uff1b\u6269\u5c55\u5230\u5728\u7ebfFPPE\u7248\u672c\uff1b\u9488\u5bf9\u51f9\u4f30\u503c\u51fd\u6570\u4e70\u5bb6\uff0c\u5c06FPPE\u7c7b\u578b\u7ed3\u679c\u8868\u5f81\u4e3aEisenberg-Gale\u98ce\u683c\u51f8\u89c4\u5212\u7684\u89e3\u3002", "result": "FPPE\u4fdd\u8bc1\u81f3\u5c11\u83b7\u5f97\u6700\u4f18\u6536\u76ca\u7684\u4e00\u534a\uff1b\u5728\u7ebfFPPE\u83b7\u5f971/4\u8fd1\u4f3c\u4fdd\u8bc1\uff1b\u5bf9\u4e8e\u51f9\u4f30\u503c\u51fd\u6570\u4e70\u5bb6\uff0c\u6536\u76ca\u8fd1\u4f3c\u968f\u4f30\u503c\u975e\u7ebf\u6027\u7a0b\u5ea6\u800c\u4f18\u96c5\u4e0b\u964d\uff1b\u540c\u65f6\u8bc1\u660e\u4e86\u5728\u4e2a\u4f53\u7406\u6027\u548c\u5355\u5546\u54c1\u5355\u4ef7\u683c\u7ea6\u675f\u4e0b\u6536\u76ca\u6700\u5927\u5316\u95ee\u9898\u662fAPX\u96be\u7684\u3002", "conclusion": "FPPE\u5728\u5355\u4ef7\u683c\u7ea6\u675f\u4e0b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6536\u76ca\u4fdd\u8bc1\uff0c\u5373\u4f7f\u5728\u4e70\u5bb6\u7279\u5b9a\u4ef7\u683c\u7684\u6700\u4f18\u6536\u76ca\u5bf9\u6bd4\u4e0b\u4e5f\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff0c\u4e3a\u5728\u7ebf\u5e7f\u544a\u81ea\u52a8\u7ade\u4ef7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2602.13745", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13745", "abs": "https://arxiv.org/abs/2602.13745", "authors": ["Blessing Jerry", "Lourdes Moreno", "Paloma Mart\u00ednez"], "title": "Human Oversight-by-Design for Accessible Generative IUIs", "comment": "Preprint. Accepted for publication in CEUR Workshop Proceedings (IUI Workshops 2026). 15 pages, 1 figure", "summary": "LLM-generated interfaces are increasingly used in high-consequence workflows (e.g., healthcare communication), where how information is presented can impact downstream actions. These interfaces and their content support human interaction with AI-assisted decision-making and communication processes and should remain accessible and usable for people with disabilities. Accessible plain-language interfaces serve as an enabling infrastructure for meaningful human oversight. In these contexts, ethical and trustworthiness risks, including hallucinations, semantic distortion, bias, and accessibility barriers, can undermine reliability and limit users' ability to understand, monitor, and intervene in AI-supported processes. Yet, in practice, oversight is often treated as a downstream check, without clear rules for when human intervention is required or who is accountable. We propose oversight-by-design: embedding human judgment across the pipeline as an architectural commitment, implemented via escalation policies and explicit UI controls for risk signalling and intervention. Automated checks flag risk in generated UI communication that supports high-stakes workflows (e.g., readability, semantic fidelity, factual consistency, and standards-based accessibility constraints) and escalate to mandatory Human-in-the-Loop (HITL) review before release when thresholds are violated, or uncertainty is high. Human-on-the-Loop (HOTL) supervision monitors system-level signals over time (alerts, escalation rates, and compliance evidence) to tune policies and detect drift. Structured review feedback is translated into governance actions (rule and prompt updates, threshold calibration, and traceable audit logs), enabling scalable intervention and verifiable oversight for generative UI systems that support high-stakes workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u76d1\u7763\u8bbe\u8ba1\"\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u68c0\u67e5\u548c\u4eba\u5de5\u5e72\u9884\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u786e\u4fddLLM\u751f\u6210\u754c\u9762\u5728\u9ad8\u98ce\u9669\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u53ef\u9760\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u76d1\u7763\u6027\u3002", "motivation": "LLM\u751f\u6210\u7684\u754c\u9762\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u9ad8\u98ce\u9669\u5de5\u4f5c\u6d41\u7a0b\uff08\u5982\u533b\u7597\u4fdd\u5065\u901a\u4fe1\uff09\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u3001\u8bed\u4e49\u626d\u66f2\u3001\u504f\u89c1\u548c\u53ef\u8bbf\u95ee\u6027\u969c\u788d\u7b49\u98ce\u9669\uff0c\u53ef\u80fd\u7834\u574f\u53ef\u9760\u6027\u5e76\u9650\u5236\u7528\u6237\u7406\u89e3\u3001\u76d1\u63a7\u548c\u5e72\u9884AI\u652f\u6301\u6d41\u7a0b\u7684\u80fd\u529b\u3002\u5f53\u524d\u76d1\u7763\u901a\u5e38\u88ab\u5f53\u4f5c\u4e0b\u6e38\u68c0\u67e5\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u4eba\u5de5\u5e72\u9884\u89c4\u5219\u548c\u95ee\u8d23\u673a\u5236\u3002", "method": "\u63d0\u51fa\"\u76d1\u7763\u8bbe\u8ba1\"\u65b9\u6cd5\uff1a\u5c06\u4eba\u5de5\u5224\u65ad\u5d4c\u5165\u6574\u4e2a\u6d41\u7a0b\u4f5c\u4e3a\u67b6\u6784\u627f\u8bfa\uff0c\u901a\u8fc7\u5347\u7ea7\u7b56\u7565\u548c\u660e\u786e\u7684UI\u63a7\u5236\u5b9e\u73b0\u98ce\u9669\u4fe1\u53f7\u548c\u5e72\u9884\u3002\u81ea\u52a8\u5316\u68c0\u67e5\u6807\u8bb0\u751f\u6210UI\u901a\u4fe1\u4e2d\u7684\u98ce\u9669\uff08\u53ef\u8bfb\u6027\u3001\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u57fa\u4e8e\u6807\u51c6\u7684\u53ef\u8bbf\u95ee\u6027\u7ea6\u675f\uff09\uff0c\u5f53\u9608\u503c\u88ab\u8fdd\u53cd\u6216\u4e0d\u786e\u5b9a\u6027\u9ad8\u65f6\uff0c\u5728\u53d1\u5e03\u524d\u5347\u7ea7\u5230\u5f3a\u5236\u6027\u4eba\u5de5\u5728\u73af\u5ba1\u67e5\u3002\u4eba\u5de5\u5728\u73af\u76d1\u7763\u76d1\u63a7\u7cfb\u7edf\u7ea7\u4fe1\u53f7\u4ee5\u8c03\u6574\u7b56\u7565\u548c\u68c0\u6d4b\u6f02\u79fb\u3002", "result": "\u7ed3\u6784\u5316\u5ba1\u67e5\u53cd\u9988\u8f6c\u5316\u4e3a\u6cbb\u7406\u884c\u52a8\uff08\u89c4\u5219\u548c\u63d0\u793a\u66f4\u65b0\u3001\u9608\u503c\u6821\u51c6\u3001\u53ef\u8ffd\u6eaf\u5ba1\u8ba1\u65e5\u5fd7\uff09\uff0c\u4e3a\u652f\u6301\u9ad8\u98ce\u9669\u5de5\u4f5c\u6d41\u7a0b\u7684\u751f\u6210UI\u7cfb\u7edf\u5b9e\u73b0\u53ef\u6269\u5c55\u5e72\u9884\u548c\u53ef\u9a8c\u8bc1\u76d1\u7763\u3002", "conclusion": "\u76d1\u7763\u8bbe\u8ba1\u6846\u67b6\u901a\u8fc7\u5c06\u4eba\u5de5\u76d1\u7763\u5d4c\u5165\u751f\u6210AI\u7cfb\u7edf\u7684\u67b6\u6784\u4e2d\uff0c\u786e\u4fdd\u9ad8\u98ce\u9669\u5de5\u4f5c\u6d41\u7a0b\u4e2dLLM\u751f\u6210\u754c\u9762\u7684\u53ef\u9760\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u95ee\u8d23\u6027\uff0c\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u76d1\u7763\u3002"}}
{"id": "2602.13586", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13586", "abs": "https://arxiv.org/abs/2602.13586", "authors": ["Hayato Suzuki", "Shunnosuke Ikeda", "Yuichi Takano"], "title": "Interpretable clustering via optimal multiway-split decision trees", "comment": null, "summary": "Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u591a\u8def\u5206\u88c2\u51b3\u7b56\u6811\u7684\u89e3\u91ca\u6027\u805a\u7c7b\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a0-1\u6574\u6570\u7ebf\u6027\u4f18\u5316\uff0c\u63d0\u9ad8\u53ef\u89e3\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u805a\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u901a\u5e38\u6784\u5efa\u4e8c\u53c9\u51b3\u7b56\u6811\uff0c\u9700\u8981\u6c42\u89e3\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6613\u9677\u5165\u6b21\u4f18\u89e3\u3002\u540c\u65f6\uff0c\u4e8c\u53c9\u51b3\u7b56\u6811\u5f80\u5f80\u8fc7\u6df1\uff0c\u96be\u4ee5\u89e3\u91ca\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u53c8\u5177\u6709\u826f\u597d\u53ef\u89e3\u91ca\u6027\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u591a\u8def\u5206\u88c2\u51b3\u7b56\u6811\u7684\u89e3\u91ca\u6027\u805a\u7c7b\u65b9\u6cd5\uff1a1) \u5c06\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a0-1\u6574\u6570\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u53ef\u89e3\u6027\uff1b2) \u96c6\u6210\u4e00\u7ef4K-means\u7b97\u6cd5\u5bf9\u8fde\u7eed\u53d8\u91cf\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u6570\u636e\u9a71\u52a8\u5206\u652f\uff1b3) \u6784\u5efa\u591a\u8def\u5206\u88c2\u51b3\u7b56\u6811\u800c\u975e\u4e8c\u53c9\u51b3\u7b56\u6811\uff0c\u83b7\u5f97\u66f4\u7b80\u6d01\u7684\u51b3\u7b56\u89c4\u5219\u3002", "result": "\u5728\u516c\u5f00\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u805a\u7c7b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u80fd\u591f\u751f\u6210\u5177\u6709\u7b80\u6d01\u51b3\u7b56\u89c4\u5219\u7684\u591a\u8def\u5206\u88c2\u51b3\u7b56\u6811\uff0c\u540c\u65f6\u5728\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u805a\u7c7b\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a0-1\u6574\u6570\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u4e00\u7ef4K-means\u79bb\u6563\u5316\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u826f\u597d\u53ef\u89e3\u91ca\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u89e3\u91ca\u6027\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14664", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2602.14664", "abs": "https://arxiv.org/abs/2602.14664", "authors": ["Parth Khadse", "Sunil Kumar Kopparapu"], "title": "Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions", "comment": "A shorter version of this paper appeared in ACPR 2025", "summary": "An end-to-end (e2e) text-to-speech (TTS) system is a deep architecture that learns to associate a text string with acoustic speech patterns from a curated dataset. It is expected that all aspects associated with speech production, such as phone duration, speaker characteristics, and intonation among other things are captured in the trained TTS model to enable the synthesized speech to be natural and intelligible. Human speech is complex, involving smooth transitions between articulatory configurations (ACs). Due to anatomical constraints, some ACs are challenging to mimic or transition between. In this paper, we experimentally study if the constraints imposed by human anatomy have an implication on training an e2e-TTS systems. We experiment with two e2e-TTS architectures, namely, Tacotron-2 an autoregressive model and VITS-TTS a non-autoregressive model. In this study, we build TTS systems using (a) forward text, forward speech (conventional, e2e-TTS), (b) reverse text, reverse speech (r-e2e-TTS), and (c) reverse text, forward speech (rtfs-e2e-TTS). Experiments demonstrate that e2e-TTS systems are purely data-driven. Interestingly, the generated speech by r-e2e-TTS systems exhibits better fidelity, better perceptual intelligibility, and better naturalness", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u63a2\u8ba8\u4eba\u7c7b\u89e3\u5256\u5b66\u7ea6\u675f\u5bf9\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u6b63\u5411\u548c\u53cd\u5411\u6587\u672c-\u8bed\u97f3\u7ec4\u5408\u7684TTS\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u4eba\u7c7b\u89e3\u5256\u5b66\u7ea6\u675f\uff08\u5982\u53d1\u97f3\u5668\u5b98\u7684\u7269\u7406\u9650\u5236\u548c\u53d1\u97f3\u914d\u7f6e\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\uff09\u662f\u5426\u4f1a\u5f71\u54cd\u7aef\u5230\u7aefTTS\u7cfb\u7edf\u7684\u8bad\u7ec3\u6548\u679c\u3002\u4eba\u7c7b\u8bed\u97f3\u6d89\u53ca\u590d\u6742\u7684\u53d1\u97f3\u914d\u7f6e\u8f6c\u6362\uff0c\u67d0\u4e9b\u914d\u7f6e\u96be\u4ee5\u6a21\u4eff\u6216\u8fc7\u6e21\uff0c\u7814\u7a76\u8005\u60f3\u4e86\u89e3\u8fd9\u4e9b\u89e3\u5256\u5b66\u9650\u5236\u5bf9TTS\u6a21\u578b\u8bad\u7ec3\u662f\u5426\u6709\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u4e24\u79cd\u7aef\u5230\u7aefTTS\u67b6\u6784\uff1a\u81ea\u56de\u5f52\u6a21\u578bTacotron-2\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578bVITS-TTS\uff1b2\uff09\u6784\u5efa\u4e09\u79cd\u4e0d\u540c\u7684TTS\u7cfb\u7edf\uff1aa) \u6b63\u5411\u6587\u672c-\u6b63\u5411\u8bed\u97f3\uff08\u4f20\u7edfe2e-TTS\uff09\uff0cb) \u53cd\u5411\u6587\u672c-\u53cd\u5411\u8bed\u97f3\uff08r-e2e-TTS\uff09\uff0cc) \u53cd\u5411\u6587\u672c-\u6b63\u5411\u8bed\u97f3\uff08rtfs-e2e-TTS\uff09\uff1b3\uff09\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u7aef\u5230\u7aefTTS\u7cfb\u7edf\u662f\u7eaf\u7cb9\u6570\u636e\u9a71\u52a8\u7684\uff1b2\uff09\u6709\u8da3\u7684\u662f\uff0c\u53cd\u5411\u6587\u672c-\u53cd\u5411\u8bed\u97f3\u7cfb\u7edf\uff08r-e2e-TTS\uff09\u751f\u6210\u7684\u8bed\u97f3\u5728\u4fdd\u771f\u5ea6\u3001\u611f\u77e5\u53ef\u61c2\u5ea6\u548c\u81ea\u7136\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662f\u7aef\u5230\u7aefTTS\u7cfb\u7edf\u786e\u5b9e\u53d7\u5230\u4eba\u7c7b\u89e3\u5256\u5b66\u7ea6\u675f\u7684\u5f71\u54cd\uff0c\u53cd\u5411\u6587\u672c-\u53cd\u5411\u8bed\u97f3\u7684\u8bad\u7ec3\u65b9\u5f0f\u80fd\u591f\u4ea7\u751f\u8d28\u91cf\u66f4\u9ad8\u7684\u5408\u6210\u8bed\u97f3\uff0c\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u8fd9\u79cd\u8bad\u7ec3\u65b9\u5f0f\u66f4\u597d\u5730\u6a21\u62df\u4e86\u4eba\u7c7b\u53d1\u97f3\u5668\u5b98\u7684\u7269\u7406\u9650\u5236\u548c\u53d1\u97f3\u914d\u7f6e\u95f4\u7684\u81ea\u7136\u8fc7\u6e21\u3002"}}
{"id": "2602.14939", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14939", "abs": "https://arxiv.org/abs/2602.14939", "authors": ["Sidharthenee Nayak", "Victor Sam Moses Babu", "Chandrashekhar Narayan Bhende", "Pratyush Chakraborty", "Mayukha Pal"], "title": "Fault Detection in Electrical Distribution System using Autoencoders", "comment": null, "summary": "In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their practical application remains highly challenging. Given the probabilistic nature of fault occurrences and parameters, certain decision-making tasks could be approached from a probabilistic standpoint. Protective systems are tasked with the detection, classification, and localization of faulty voltage and current line magnitudes, culminating in the activation of circuit breakers to isolate the faulty line. An essential aspect of designing effective fault detection systems lies in obtaining reliable data for training and testing, which is often scarce. Leveraging deep learning techniques, particularly the powerful capabilities of pattern classifiers in learning, generalizing, and parallel processing, offers promising avenues for intelligent fault detection. To address this, our paper proposes an anomaly-based approach for fault detection in electrical power systems, employing deep autoencoders. Additionally, we utilize Convolutional Autoencoders (CAE) for dimensionality reduction, which, due to its fewer parameters, requires less training time compared to conventional autoencoders. The proposed method demonstrates superior performance and accuracy compared to alternative detection approaches by achieving an accuracy of 97.62% and 99.92% on simulated and publicly available datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u529b\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\uff0c\u901a\u8fc7\u5377\u79ef\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u964d\u7ef4\uff0c\u5728\u6a21\u62df\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523097.62%\u548c99.92%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7535\u529b\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u7814\u7a76\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u6311\u6218\u3002\u6545\u969c\u53d1\u751f\u5177\u6709\u6982\u7387\u6027\uff0c\u9700\u8981\u4ece\u6982\u7387\u89d2\u5ea6\u5904\u7406\u51b3\u7b56\u4efb\u52a1\u3002\u4fdd\u62a4\u7cfb\u7edf\u9700\u8981\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u5b9a\u4f4d\u6545\u969c\uff0c\u4f46\u53ef\u9760\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u3002\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7279\u522b\u662f\u6a21\u5f0f\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u3001\u6cdb\u5316\u548c\u5e76\u884c\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u964d\u7ef4\u3002\u76f8\u6bd4\u4f20\u7edf\u81ea\u7f16\u7801\u5668\uff0c\u5377\u79ef\u81ea\u7f16\u7801\u5668\u53c2\u6570\u66f4\u5c11\uff0c\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u5f0f\u5206\u7c7b\u5668\u7684\u5f3a\u5927\u80fd\u529b\u8fdb\u884c\u6545\u969c\u68c0\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.62%\u7684\u51c6\u786e\u7387\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.92%\u7684\u51c6\u786e\u7387\u3002\u76f8\u6bd4\u5176\u4ed6\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3a\u7535\u529b\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002\u5377\u79ef\u81ea\u7f16\u7801\u5668\u7684\u964d\u7ef4\u80fd\u529b\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.13784", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13784", "abs": "https://arxiv.org/abs/2602.13784", "authors": ["Yifan Zhang", "Tianle Ren", "Fei Wang", "Brian Y Lim"], "title": "Comparables XAI: Faithful Example-based AI Explanations with Counterfactual Trace Adjustments", "comment": "Accepted by CHI 2026", "summary": "Explaining with examples is an intuitive way to justify AI decisions. However, it is challenging to understand how a decision value should change relative to the examples with many features differing by large amounts. We draw from real estate valuation that uses Comparables-examples with known values for comparison. Estimates are made more accurate by hypothetically adjusting the attributes of each Comparable and correspondingly changing the value based on factors. We propose Comparables XAI for relatable example-based explanations of AI with Trace adjustments that trace counterfactual changes from each Comparable to the Subject, one attribute at a time, monotonically along the AI feature space. In modelling and user studies, Trace-adjusted Comparables achieved the highest XAI faithfulness and precision, user accuracy, and narrowest uncertainty bounds compared to linear regression, linearly adjusted Comparables, or unadjusted Comparables. This work contributes a new analytical basis for using example-based explanations to improve user understanding of AI decisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u6bd4\u6848\u4f8b\u7684AI\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u8c03\u6574\u673a\u5236\u6a21\u62df\u623f\u5730\u4ea7\u4f30\u4ef7\u4e2d\u7684\u53ef\u6bd4\u6848\u4f8b\u8c03\u6574\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u89e3\u91ca\u7684\u51c6\u786e\u6027\u548c\u7528\u6237\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6848\u4f8b\u7684AI\u89e3\u91ca\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff1a\u5f53\u6848\u4f8b\u4e0e\u5f85\u89e3\u91ca\u6837\u672c\u5728\u591a\u4e2a\u7279\u5f81\u4e0a\u5b58\u5728\u8f83\u5927\u5dee\u5f02\u65f6\uff0c\u96be\u4ee5\u7406\u89e3\u51b3\u7b56\u503c\u5e94\u5982\u4f55\u76f8\u5e94\u53d8\u5316\u3002\u53d7\u623f\u5730\u4ea7\u4f30\u4ef7\u4e2d\u4f7f\u7528\u53ef\u6bd4\u6848\u4f8b\u8fdb\u884c\u4ef7\u503c\u8c03\u6574\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u66f4\u6613\u7406\u89e3\u7684\u6848\u4f8b\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u63d0\u51faComparables XAI\u65b9\u6cd5\uff0c\u5305\u542bTrace\u8c03\u6574\u673a\u5236\uff1a\u4ece\u6bcf\u4e2a\u53ef\u6bd4\u6848\u4f8b\u51fa\u53d1\uff0c\u9010\u4e2a\u7279\u5f81\u8ffd\u8e2a\u5230\u5f85\u89e3\u91ca\u6837\u672c\u7684\u53cd\u4e8b\u5b9e\u53d8\u5316\uff0c\u6cbf\u7740AI\u7279\u5f81\u7a7a\u95f4\u5355\u8c03\u8c03\u6574\u3002\u8fd9\u79cd\u65b9\u6cd5\u6a21\u62df\u623f\u5730\u4ea7\u4f30\u4ef7\u4e2d\u8c03\u6574\u53ef\u6bd4\u6848\u4f8b\u5c5e\u6027\u7684\u8fc7\u7a0b\u3002", "result": "\u5728\u5efa\u6a21\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0cTrace\u8c03\u6574\u7684\u53ef\u6bd4\u6848\u4f8b\u5728XAI\u5fe0\u5b9e\u5ea6\u3001\u7cbe\u786e\u5ea6\u3001\u7528\u6237\u51c6\u786e\u6027\u548c\u6700\u7a84\u7684\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u7ebf\u6027\u56de\u5f52\u3001\u7ebf\u6027\u8c03\u6574\u7684\u53ef\u6bd4\u6848\u4f8b\u6216\u672a\u8c03\u6574\u7684\u53ef\u6bd4\u6848\u4f8b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f7f\u7528\u57fa\u4e8e\u6848\u4f8b\u7684\u89e3\u91ca\u6765\u6539\u5584\u7528\u6237\u5bf9AI\u51b3\u7b56\u7684\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u57fa\u7840\uff0cTrace\u8c03\u6574\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u89e3\u91ca\u7684\u51c6\u786e\u6027\u548c\u53ef\u7406\u89e3\u6027\u3002"}}
{"id": "2602.13640", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13640", "abs": "https://arxiv.org/abs/2602.13640", "authors": ["Siyuan Li", "Jiani Lu", "Yu Song", "Xianren Li", "Bo An", "Peng Liu"], "title": "Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation", "comment": null, "summary": "Existing robotic manipulation methods primarily rely on visual and proprioceptive observations, which may struggle to infer contact-related interaction states in partially observable real-world environments. Acoustic cues, by contrast, naturally encode rich interaction dynamics during contact, yet remain underexploited in current multimodal fusion literature. Most multimodal fusion approaches implicitly assume homogeneous roles across modalities, and thus design flat and symmetric fusion structures. However, this assumption is ill-suited for acoustic signals, which are inherently sparse and contact-driven. To achieve precise robotic manipulation through acoustic-informed perception, we propose a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. Our approach first conditions visual and proprioceptive representations on acoustic cues, and then explicitly models higher-order cross-modal interactions to capture complementary dependencies among modalities. The fused representation is leveraged by a diffusion-based policy to directly generate continuous robot actions from multimodal observations. The combination of end-to-end learning and hierarchical fusion structure enables the policy to exploit task-relevant acoustic information while mitigating interference from less informative modalities. The proposed method has been evaluated on real-world robotic manipulation tasks, including liquid pouring and cabinet opening. Extensive experiment results demonstrate that our approach consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not readily available from visual observations alone. Furthermore, a mutual information analysis is conducted to interpret the effect of audio cues in robotic manipulation via multimodal fusion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8868\u793a\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6574\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u6765\u5b9e\u73b0\u7cbe\u786e\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u7279\u522b\u9488\u5bf9\u63a5\u89e6\u9a71\u52a8\u7684\u58f0\u5b66\u4fe1\u53f7\u8bbe\u8ba1\u4e86\u975e\u5bf9\u79f0\u878d\u5408\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u96be\u4ee5\u63a8\u65ad\u63a5\u89e6\u76f8\u5173\u7684\u4ea4\u4e92\u72b6\u6001\u3002\u58f0\u5b66\u7ebf\u7d22\u81ea\u7136\u7f16\u7801\u4e86\u63a5\u89e6\u8fc7\u7a0b\u4e2d\u7684\u4e30\u5bcc\u4ea4\u4e92\u52a8\u6001\uff0c\u4f46\u5728\u5f53\u524d\u591a\u6a21\u6001\u878d\u5408\u7814\u7a76\u4e2d\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\u3002\u5927\u591a\u6570\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5047\u8bbe\u5404\u6a21\u6001\u4f5c\u7528\u540c\u8d28\uff0c\u8bbe\u8ba1\u4e86\u5e73\u5766\u5bf9\u79f0\u7684\u878d\u5408\u7ed3\u6784\uff0c\u8fd9\u4e0d\u9002\u5408\u7a00\u758f\u4e14\u63a5\u89e6\u9a71\u52a8\u7684\u58f0\u5b66\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8868\u793a\u878d\u5408\u6846\u67b6\uff0c\u9996\u5148\u5c06\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u8868\u793a\u57fa\u4e8e\u58f0\u5b66\u7ebf\u7d22\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u7136\u540e\u663e\u5f0f\u5efa\u6a21\u9ad8\u9636\u8de8\u6a21\u6001\u4ea4\u4e92\u4ee5\u6355\u6349\u6a21\u6001\u95f4\u7684\u4e92\u8865\u4f9d\u8d56\u5173\u7cfb\u3002\u878d\u5408\u8868\u793a\u88ab\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u5229\u7528\uff0c\u76f4\u63a5\u4ece\u591a\u6a21\u6001\u89c2\u6d4b\u751f\u6210\u8fde\u7eed\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u7aef\u5230\u7aef\u5b66\u4e60\u548c\u5206\u5c42\u878d\u5408\u7ed3\u6784\u7684\u7ed3\u5408\u4f7f\u7b56\u7565\u80fd\u591f\u5229\u7528\u4efb\u52a1\u76f8\u5173\u7684\u58f0\u5b66\u4fe1\u606f\uff0c\u540c\u65f6\u51cf\u8f7b\u4fe1\u606f\u8f83\u5c11\u6a21\u6001\u7684\u5e72\u6270\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08\u5305\u62ec\u6db2\u4f53\u503e\u5012\u548c\u67dc\u95e8\u5f00\u542f\uff09\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u58f0\u5b66\u7ebf\u7d22\u63d0\u4f9b\u89c6\u89c9\u89c2\u6d4b\u4e0d\u6613\u83b7\u5f97\u7684\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u65f6\u8868\u73b0\u66f4\u4f73\u3002\u901a\u8fc7\u4e92\u4fe1\u606f\u5206\u6790\u89e3\u91ca\u4e86\u58f0\u5b66\u7ebf\u7d22\u5728\u591a\u6a21\u6001\u878d\u5408\u4e2d\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u878d\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u58f0\u5b66\u7ebf\u7d22\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u63a5\u89e6\u9a71\u52a8\u7684\u4ea4\u4e92\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u5bf9\u79f0\u878d\u5408\u7ed3\u6784\u7684\u9650\u5236\uff0c\u4e3a\u591a\u6a21\u6001\u611f\u77e5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.13626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13626", "abs": "https://arxiv.org/abs/2602.13626", "authors": ["Mingqiao Zhang", "Qiyao Peng", "Yumeng Wang", "Chunyuan Liu", "Hongtao Liu"], "title": "Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?", "comment": null, "summary": "The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5e76\u9a8c\u8bc1\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u57fa\u51c6\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5373LLM\u5728\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u65f6\u63a5\u89e6\u5e76\u53ef\u80fd\u8bb0\u5fc6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6027\u80fd\u6307\u6807\u865a\u9ad8\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u53ef\u9760\u6027\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u8bc6\u522b\u548c\u7814\u7a76\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff1a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u57fa\u51c6\u6570\u636e\u6cc4\u9732\u73b0\u8c61\uff0c\u8fd9\u79cd\u73b0\u8c61\u4f1a\u5bfc\u81f4\u6027\u80fd\u6307\u6807\u865a\u9ad8\uff0c\u8bef\u5bfc\u5bf9\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u591a\u79cd\u6570\u636e\u6cc4\u9732\u573a\u666f\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u5305\u542b\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\u7684\u6df7\u5408\u8bed\u6599\u5e93\uff0c\u9a8c\u8bc1\u6570\u636e\u6cc4\u9732\u5bf9\u63a8\u8350\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6570\u636e\u6cc4\u9732\u7684\u53cc\u91cd\u6548\u5e94\uff1a\u5f53\u6cc4\u9732\u6570\u636e\u4e0e\u9886\u57df\u76f8\u5173\u65f6\uff0c\u4f1a\u5bfc\u81f4\u663e\u8457\u4f46\u865a\u5047\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bef\u5bfc\u6027\u5730\u5938\u5927\u6a21\u578b\u80fd\u529b\uff1b\u800c\u9886\u57df\u4e0d\u76f8\u5173\u7684\u6cc4\u9732\u901a\u5e38\u4f1a\u964d\u4f4e\u63a8\u8350\u51c6\u786e\u6027\uff0c\u51f8\u663e\u4e86\u8fd9\u79cd\u6c61\u67d3\u7684\u590d\u6742\u6027\u548c\u6761\u4ef6\u4f9d\u8d56\u6027\u3002", "conclusion": "\u6570\u636e\u6cc4\u9732\u662f\u57fa\u4e8eLLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u5173\u952e\u4e14\u5148\u524d\u672a\u88ab\u8003\u8651\u7684\u56e0\u7d20\uff0c\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30LLM\u63a8\u8350\u7cfb\u7edf\u65f6\u9700\u8981\u8003\u8651\u6570\u636e\u6cc4\u9732\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.13255", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13255", "abs": "https://arxiv.org/abs/2602.13255", "authors": ["Najmul Hasan", "Prashanth BusiReddyGari"], "title": "DPBench: Large Language Models Struggle with Simultaneous Coordination", "comment": "13 pages, 4 figures", "summary": "Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.", "AI": {"tldr": "DPBench\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e\u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\uff0c\u8bc4\u4f30LLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u8c03\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u987a\u5e8f\u51b3\u7b56\u65f6\u80fd\u6709\u6548\u534f\u8c03\uff0c\u4f46\u5728\u540c\u65f6\u51b3\u7b56\u65f6\u6b7b\u9501\u7387\u8d85\u8fc795%\uff0c\u4e14\u901a\u4fe1\u65e0\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u76ee\u524d\u7f3a\u4e4f\u80fd\u591f\u6d4b\u8bd5\u5b83\u4eec\u5728\u8d44\u6e90\u7ade\u4e89\u4e0b\u534f\u8c03\u80fd\u529b\u7684\u57fa\u51c6\u3002\u7814\u7a76\u8005\u9700\u8981\u8bc4\u4f30LLM\u5728\u9700\u8981\u5e76\u53d1\u8d44\u6e90\u8bbf\u95ee\u7684\u573a\u666f\u4e2d\u7684\u534f\u8c03\u8868\u73b0", "method": "\u57fa\u4e8e\u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u6784\u5efaDPBench\u57fa\u51c6\uff0c\u901a\u8fc78\u79cd\u4e0d\u540c\u6761\u4ef6\uff08\u51b3\u7b56\u65f6\u673a\u3001\u7fa4\u4f53\u89c4\u6a21\u3001\u901a\u4fe1\u80fd\u529b\uff09\u8bc4\u4f30LLM\u534f\u8c03\u80fd\u529b\u3002\u4f7f\u7528GPT-5.2\u3001Claude Opus 4.5\u548cGrok 4.1\u8fdb\u884c\u5b9e\u9a8c", "result": "\u53d1\u73b0\u663e\u8457\u7684\u4e0d\u5bf9\u79f0\u6027\uff1aLLM\u5728\u987a\u5e8f\u8bbe\u7f6e\u4e2d\u80fd\u6709\u6548\u534f\u8c03\uff0c\u4f46\u5728\u9700\u8981\u540c\u65f6\u51b3\u7b56\u65f6\u5931\u8d25\uff0c\u67d0\u4e9b\u6761\u4ef6\u4e0b\u6b7b\u9501\u7387\u8d85\u8fc795%\u3002\u901a\u4fe1\u4e0d\u4ec5\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898\uff0c\u53cd\u800c\u53ef\u80fd\u589e\u52a0\u6b7b\u9501\u7387\u3002\u5931\u8d25\u6e90\u4e8e\u6536\u655b\u63a8\u7406\u73b0\u8c61", "conclusion": "\u9700\u8981\u5e76\u53d1\u8d44\u6e90\u8bbf\u95ee\u7684\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u53ef\u80fd\u9700\u8981\u5916\u90e8\u534f\u8c03\u673a\u5236\uff0c\u800c\u975e\u4f9d\u8d56\u6d8c\u73b0\u7684\u534f\u8c03\u80fd\u529b\u3002DPBench\u4f5c\u4e3a\u5f00\u6e90\u57fa\u51c6\u53d1\u5e03\uff0c\u4e3a\u8bc4\u4f30LLM\u534f\u8c03\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177"}}
{"id": "2602.14858", "categories": ["cs.GT", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2602.14858", "abs": "https://arxiv.org/abs/2602.14858", "authors": ["Yuma Ichikawa"], "title": "Thermal Min-Max Games: Unifying Bounded Rationality and Typical-Case Equilibrium", "comment": "31 pages, 4 figures", "summary": "Strategic-form min-max game theory examines the existence, multiplicity, selection of equilibria, and the worst-case computational complexity under perfect rationality. However, in many applications, games are drawn from an ensemble, and players exhibit bounded rationality. We introduce thermal min-max games, a thermodynamic relaxation that unifies bounded and perfect rationality by assigning each player a temperature to regulate their rationality level. To analyze typical behavior in the large-strategy limit, we develop a nested replica framework for this relaxation. This theory provides tractable predictions for typical equilibrium values and mixed-strategy statistics as functions of rationality strength, strategy-count aspect ratio, and payoff randomness. Numerical experiments demonstrate that these asymptotic predictions accurately align with the equilibrium of finite games of moderate size.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u70ed\u529b\u5b66\u6700\u5c0f\u6700\u5927\u535a\u5f08\u7406\u8bba\uff0c\u901a\u8fc7\u6e29\u5ea6\u53c2\u6570\u7edf\u4e00\u5b8c\u7f8e\u7406\u6027\u548c\u6709\u9650\u7406\u6027\uff0c\u5e76\u5f00\u53d1\u5d4c\u5957\u590d\u5236\u6846\u67b6\u5206\u6790\u5927\u7b56\u7565\u6781\u9650\u4e0b\u7684\u5178\u578b\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u7b56\u7565\u5f62\u5f0f\u7684\u6700\u5c0f\u6700\u5927\u535a\u5f08\u7406\u8bba\u5047\u8bbe\u5b8c\u7f8e\u7406\u6027\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u535a\u5f08\u901a\u5e38\u6765\u81ea\u96c6\u5408\u4e14\u73a9\u5bb6\u8868\u73b0\u51fa\u6709\u9650\u7406\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5b8c\u7f8e\u7406\u6027\u548c\u6709\u9650\u7406\u6027\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5f15\u5165\u70ed\u529b\u5b66\u6700\u5c0f\u6700\u5927\u535a\u5f08\u4f5c\u4e3a\u70ed\u529b\u5b66\u677e\u5f1b\uff0c\u4e3a\u6bcf\u4e2a\u73a9\u5bb6\u5206\u914d\u6e29\u5ea6\u53c2\u6570\u8c03\u8282\u7406\u6027\u6c34\u5e73\u3002\u5f00\u53d1\u5d4c\u5957\u590d\u5236\u6846\u67b6\u5206\u6790\u5927\u7b56\u7565\u6781\u9650\u4e0b\u7684\u5178\u578b\u884c\u4e3a\u3002", "result": "\u7406\u8bba\u63d0\u4f9b\u4e86\u5173\u4e8e\u5178\u578b\u5747\u8861\u503c\u3001\u6df7\u5408\u7b56\u7565\u7edf\u8ba1\u91cf\u7684\u53ef\u5904\u7406\u9884\u6d4b\uff0c\u8fd9\u4e9b\u9884\u6d4b\u662f\u7406\u6027\u5f3a\u5ea6\u3001\u7b56\u7565\u6570\u91cf\u7eb5\u6a2a\u6bd4\u548c\u652f\u4ed8\u968f\u673a\u6027\u7684\u51fd\u6570\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u6e10\u8fd1\u9884\u6d4b\u4e0e\u4e2d\u7b49\u89c4\u6a21\u6709\u9650\u535a\u5f08\u7684\u5747\u8861\u51c6\u786e\u543b\u5408\u3002", "conclusion": "\u70ed\u529b\u5b66\u6700\u5c0f\u6700\u5927\u535a\u5f08\u7406\u8bba\u6210\u529f\u7edf\u4e00\u4e86\u5b8c\u7f8e\u7406\u6027\u548c\u6709\u9650\u7406\u6027\uff0c\u5d4c\u5957\u590d\u5236\u6846\u67b6\u4e3a\u5206\u6790\u5927\u7b56\u7565\u6781\u9650\u4e0b\u7684\u5178\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7406\u8bba\u9884\u6d4b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u51c6\u786e\u6027\u3002"}}
{"id": "2602.13817", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13817", "abs": "https://arxiv.org/abs/2602.13817", "authors": ["Shiping Chen", "Shu Zhong", "Duncan P. Brumby", "Anna L. Cox"], "title": "What happens when reviewers receive AI feedback in their reviews?", "comment": "ACM CHI 2026", "summary": "AI is reshaping academic research, yet its role in peer review remains polarising and contentious. Advocates see its potential to reduce reviewer burden and improve quality, while critics warn of risks to fairness, accountability, and trust. At ICLR 2025, an official AI feedback tool was deployed to provide reviewers with post-review suggestions. We studied this deployment through surveys and interviews, investigating how reviewers engaged with the tool and perceived its usability and impact. Our findings surface both opportunities and tensions when AI augments in peer review. This work contributes the first empirical evidence of such an AI tool in a live review process, documenting how reviewers respond to AI-generated feedback in a high-stakes review context. We further offer design implications for AI-assisted reviewing that aim to enhance quality while safeguarding human expertise, agency, and responsibility.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86ICLR 2025\u4f1a\u8bae\u4e2dAI\u5ba1\u7a3f\u53cd\u9988\u5de5\u5177\u7684\u5b9e\u9645\u90e8\u7f72\u6548\u679c\uff0c\u901a\u8fc7\u8c03\u67e5\u548c\u8bbf\u8c08\u4e86\u89e3\u5ba1\u7a3f\u4eba\u5982\u4f55\u4f7f\u7528\u8be5\u5de5\u5177\u53ca\u5176\u5bf9\u5ba1\u7a3f\u8fc7\u7a0b\u7684\u611f\u77e5\u5f71\u54cd\u3002", "motivation": "AI\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u4e89\u8bae\u3002\u652f\u6301\u8005\u8ba4\u4e3aAI\u80fd\u51cf\u8f7b\u5ba1\u7a3f\u8d1f\u62c5\u3001\u63d0\u9ad8\u8d28\u91cf\uff0c\u6279\u8bc4\u8005\u5219\u62c5\u5fc3\u5f71\u54cd\u516c\u5e73\u6027\u3001\u95ee\u8d23\u5236\u548c\u4fe1\u4efb\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u4e86\u89e3AI\u5de5\u5177\u5728\u5b9e\u9645\u5ba1\u7a3f\u73af\u5883\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u548c\u5f71\u54cd\u3002", "method": "\u5728ICLR 2025\u4f1a\u8bae\u4e2d\u90e8\u7f72\u5b98\u65b9AI\u53cd\u9988\u5de5\u5177\uff0c\u4e3a\u5ba1\u7a3f\u4eba\u63d0\u4f9b\u5ba1\u7a3f\u540e\u5efa\u8bae\u3002\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u548c\u8bbf\u8c08\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790\u5ba1\u7a3f\u4eba\u5982\u4f55\u4e0e\u8be5\u5de5\u5177\u4e92\u52a8\uff0c\u4ee5\u53ca\u4ed6\u4eec\u5bf9\u5de5\u5177\u53ef\u7528\u6027\u548c\u5f71\u54cd\u7684\u611f\u77e5\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u589e\u5f3a\u540c\u884c\u8bc4\u5ba1\u65e2\u5e26\u6765\u673a\u9047\u4e5f\u4ea7\u751f\u7d27\u5f20\u5173\u7cfb\u3002\u8fd9\u662f\u9996\u6b21\u5728\u771f\u5b9e\u5ba1\u7a3f\u8fc7\u7a0b\u4e2d\u6536\u96c6AI\u5de5\u5177\u4f7f\u7528\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u8bb0\u5f55\u4e86\u5ba1\u7a3f\u4eba\u5728\u9ad8\u98ce\u9669\u5ba1\u7a3f\u73af\u5883\u4e0b\u5bf9AI\u751f\u6210\u53cd\u9988\u7684\u53cd\u5e94\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u8f85\u52a9\u5ba1\u7a3f\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\uff0c\u65e8\u5728\u63d0\u9ad8\u5ba1\u7a3f\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u62a4\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3001\u81ea\u4e3b\u6743\u548c\u8d23\u4efb\u3002\u5f3a\u8c03\u4e86\u5728AI\u589e\u5f3a\u5ba1\u7a3f\u8fc7\u7a0b\u4e2d\u5e73\u8861\u6280\u672f\u4f18\u52bf\u4e0e\u4eba\u7c7b\u6838\u5fc3\u4ef7\u503c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.13641", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13641", "abs": "https://arxiv.org/abs/2602.13641", "authors": ["Yaoyu Li", "Chaosheng Huang", "Jun Li"], "title": "SPLIT: Sparse Incremental Learning of Error Dynamics for Control-Oriented Modeling in Autonomous Vehicles", "comment": "21 pages, 21 figures", "summary": "Accurate, computationally efficient, and adaptive vehicle models are essential for autonomous vehicle control. Hybrid models that combine a nominal model with a Gaussian Process (GP)-based residual model have emerged as a promising approach. However, the GP-based residual model suffers from the curse of dimensionality, high evaluation complexity, and the inefficiency of online learning, which impede the deployment in real-time vehicle controllers. To address these challenges, we propose SPLIT, a sparse incremental learning framework for control-oriented vehicle dynamics modeling. SPLIT integrates three key innovations: (i) Model Decomposition. We decompose the vehicle model into invariant elements calibrated by experiments, and variant elements compensated by the residual model to reduce feature dimensionality. (ii) Local Incremental Learning. We define the valid region in the feature space and partition it into subregions, enabling efficient online learning from streaming data. (iii) GP Sparsification. We use bayesian committee machine to ensure scalable online evaluation. Integrated into model-based controllers, SPLIT is evaluated in aggressive simulations and real-vehicle experiments. Results demonstrate that SPLIT improves model accuracy and control performance online. Moreover, it enables rapid adaptation to vehicle dynamics deviations and exhibits robust generalization to previously unseen scenarios.", "AI": {"tldr": "SPLIT\u662f\u4e00\u4e2a\u7528\u4e8e\u63a7\u5236\u5bfc\u5411\u8f66\u8f86\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u7a00\u758f\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5206\u89e3\u3001\u5c40\u90e8\u589e\u91cf\u5b66\u4e60\u548cGP\u7a00\u758f\u5316\u89e3\u51b3\u4f20\u7edfGP\u6b8b\u5dee\u6a21\u578b\u7684\u9ad8\u7ef4\u3001\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5728\u7ebf\u5b66\u4e60\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u9700\u8981\u51c6\u786e\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u8f66\u8f86\u6a21\u578b\u3002\u4f20\u7edf\u6df7\u5408\u6a21\u578b\u7ed3\u5408\u540d\u4e49\u6a21\u578b\u548cGP\u6b8b\u5dee\u6a21\u578b\u5b58\u5728\u7ef4\u5ea6\u707e\u96be\u3001\u8bc4\u4f30\u590d\u6742\u5ea6\u9ad8\u548c\u5728\u7ebf\u5b66\u4e60\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5b9e\u65f6\u8f66\u8f86\u63a7\u5236\u5668\u7684\u90e8\u7f72\u3002", "method": "SPLIT\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u6a21\u578b\u5206\u89e3\uff1a\u5c06\u8f66\u8f86\u6a21\u578b\u5206\u89e3\u4e3a\u5b9e\u9a8c\u6821\u51c6\u7684\u4e0d\u53d8\u5143\u7d20\u548c\u6b8b\u5dee\u6a21\u578b\u8865\u507f\u7684\u53d8\u4f53\u5143\u7d20\uff0c\u964d\u4f4e\u7279\u5f81\u7ef4\u5ea6\uff1b(2) \u5c40\u90e8\u589e\u91cf\u5b66\u4e60\uff1a\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u6709\u6548\u533a\u57df\u5e76\u5212\u5206\u4e3a\u5b50\u533a\u57df\uff0c\u5b9e\u73b0\u6d41\u6570\u636e\u7684\u9ad8\u6548\u5728\u7ebf\u5b66\u4e60\uff1b(3) GP\u7a00\u758f\u5316\uff1a\u4f7f\u7528\u8d1d\u53f6\u65af\u59d4\u5458\u4f1a\u673a\u5236\u786e\u4fdd\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u8bc4\u4f30\u3002", "result": "\u5728\u6fc0\u8fdb\u4eff\u771f\u548c\u5b9e\u8f66\u5b9e\u9a8c\u4e2d\uff0cSPLIT\u96c6\u6210\u5230\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u5668\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u548c\u63a7\u5236\u6027\u80fd\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u8f66\u8f86\u52a8\u529b\u5b66\u504f\u5dee\uff0c\u5e76\u5bf9\u672a\u89c1\u573a\u666f\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SPLIT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfGP\u6b8b\u5dee\u6a21\u578b\u5728\u5b9e\u65f6\u8f66\u8f86\u63a7\u5236\u4e2d\u7684\u90e8\u7f72\u969c\u788d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u81ea\u9002\u5e94\u7684\u8f66\u8f86\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13874", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13874", "abs": "https://arxiv.org/abs/2602.13874", "authors": ["Ben Kosa", "Hsuanling Lee", "Jasmine Li", "Sanbrita Mondal", "Yuhang Zhao", "Liang He"], "title": "Not Seeing the Whole Picture: Challenges and Opportunities in Using AI for Co-Making Physical DIY-AT for People with Visual Impairments", "comment": "20 pages, 4 figures, to be presented at CHI 2026", "summary": "Existing assistive technologies (AT) often adopt a one-size-fits-all approach, overlooking the diverse needs of people with visual impairments (PVI). Do-it-yourself AT (DIY-AT) toolkits offer one path toward customization, but most remain limited--targeting co-design with engineers or requiring programming expertise. Non-professionals with disabilities, including PVI, also face barriers such as inaccessible tools, lack of confidence, and insufficient technical knowledge. These gaps highlight the need for prototyping technologies that enable PVI to directly make their own AT. Building on emerging evidence that large language models (LLMs) can serve not only as visual aids but also as co-design partners, we present an exploratory study of how LLM-based AI can support PVI in the tangible DIY-AT co-making process. Our findings surface key challenges and design opportunities: the need for greater spatial and visual support, strategies for mitigating novel AI errors, and implications for designing more accessible AI-assisted prototypes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u652f\u6301\u89c6\u969c\u4eba\u58eb\uff08PVI\uff09\u53c2\u4e0eDIY\u8f85\u52a9\u6280\u672f\u7684\u5171\u540c\u5236\u4f5c\u8fc7\u7a0b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8f85\u52a9\u6280\u672f\u7f3a\u4e4f\u4e2a\u6027\u5316\u5b9a\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u6280\u672f\u901a\u5e38\u91c7\u7528\"\u4e00\u5200\u5207\"\u7684\u65b9\u6cd5\uff0c\u5ffd\u89c6\u4e86\u89c6\u969c\u4eba\u58eb\u7684\u591a\u6837\u5316\u9700\u6c42\u3002DIY\u8f85\u52a9\u6280\u672f\u5de5\u5177\u5305\u867d\u7136\u63d0\u4f9b\u4e86\u5b9a\u5236\u5316\u8def\u5f84\uff0c\u4f46\u5927\u591a\u5c40\u9650\u4e8e\u4e0e\u5de5\u7a0b\u5e08\u5171\u540c\u8bbe\u8ba1\u6216\u9700\u8981\u7f16\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u3002\u975e\u4e13\u4e1a\u7684\u6b8b\u969c\u4eba\u58eb\u9762\u4e34\u5de5\u5177\u4e0d\u53ef\u8bbf\u95ee\u3001\u7f3a\u4e4f\u4fe1\u5fc3\u548c\u6280\u672f\u77e5\u8bc6\u4e0d\u8db3\u7b49\u969c\u788d\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u652f\u6301\u89c6\u969c\u4eba\u58eb\u76f4\u63a5\u5236\u4f5c\u81ea\u5df1\u8f85\u52a9\u6280\u672f\u7684\u539f\u578b\u6280\u672f\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0d\u4ec5\u53ef\u4f5c\u4e3a\u89c6\u89c9\u8f85\u52a9\u5de5\u5177\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u5171\u540c\u8bbe\u8ba1\u4f19\u4f34\u7684\u65b0\u5174\u8bc1\u636e\uff0c\u672c\u7814\u7a76\u8fdb\u884c\u4e86\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u63a2\u8ba8LLM\u57fa\u7840\u7684\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u652f\u6301\u89c6\u969c\u4eba\u58eb\u53c2\u4e0e\u6709\u5f62\u7684DIY\u8f85\u52a9\u6280\u672f\u5171\u540c\u5236\u4f5c\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63ed\u793a\u4e86\u5173\u952e\u6311\u6218\u548c\u8bbe\u8ba1\u673a\u4f1a\uff1a\u9700\u8981\u66f4\u5f3a\u7684\u7a7a\u95f4\u548c\u89c6\u89c9\u652f\u6301\u3001\u51cf\u8f7b\u65b0\u578bAI\u9519\u8bef\u7684\u7b56\u7565\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u66f4\u6613\u8bbf\u95ee\u7684AI\u8f85\u52a9\u539f\u578b\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "LLM\u57fa\u7840\u7684\u4eba\u5de5\u667a\u80fd\u6709\u6f5c\u529b\u652f\u6301\u89c6\u969c\u4eba\u58eb\u53c2\u4e0eDIY\u8f85\u52a9\u6280\u672f\u7684\u5171\u540c\u5236\u4f5c\u8fc7\u7a0b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u7a7a\u95f4\u89c6\u89c9\u652f\u6301\u3001AI\u9519\u8bef\u7f13\u89e3\u548c\u53ef\u8bbf\u95ee\u6027\u8bbe\u8ba1\u7b49\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.13656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13656", "abs": "https://arxiv.org/abs/2602.13656", "authors": ["Zhongxiang Lei", "Lulu Cao", "Xuyang Wang", "Tianyi Qian", "Jinyan Liu", "Xuesong Li"], "title": "A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking", "comment": "18 pages, 8 figures,5 tables", "summary": "Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes' daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KungFuAthlete\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u6267\u884c\u9ad8\u52a8\u6001\u6b66\u672f\u52a8\u4f5c\u65f6\u7684\u8ddf\u8e2a\u4e0e\u6454\u5012\u6062\u590d\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u8fd0\u52a8\u8ddf\u8e2a\u7cfb\u7edf\u5728\u786c\u4ef6\u6027\u80fd\u6781\u9650\u548c\u7b97\u6cd5\u9c81\u68d2\u6027\u8fb9\u754c\u5b58\u5728\u4e0d\u8db3\u3002\u6b66\u672f\u4f5c\u4e3a\u9ad8\u52a8\u6001\u4eba\u4f53\u8fd0\u52a8\u7684\u6781\u7aef\u6848\u4f8b\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u8fd0\u52a8\u59cb\u7ec8\u5904\u4e8e\u5b89\u5168\u72b6\u6001\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u5b89\u5168\u72b6\u6001\u5efa\u6a21\u548c\u81ea\u4e3b\u6062\u590d\u7684\u7edf\u4e00\u7b56\u7565\u3002", "method": "\u6784\u5efaKungFuAthlete\u9ad8\u52a8\u6001\u6b66\u672f\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u5305\u542b\u5730\u9762\u548c\u8df3\u8dc3\u5b50\u96c6\u3002\u63d0\u51fa\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u5355\u4e00\u7b56\u7565\u80fd\u591f\u8054\u5408\u5b66\u4e60\u9ad8\u52a8\u6001\u8fd0\u52a8\u8ddf\u8e2a\u548c\u6454\u5012\u6062\u590d\uff0c\u5c06\u654f\u6377\u6267\u884c\u4e0e\u7a33\u5b9a\u5316\u7edf\u4e00\u5728\u4e00\u4e2a\u6846\u67b6\u5185\u3002", "result": "KungFuAthlete\u6570\u636e\u96c6\u7684\u8df3\u8dc3\u5b50\u96c6\u5728\u5173\u8282\u3001\u7ebf\u6027\u548c\u89d2\u901f\u5ea6\u65b9\u9762\u663e\u8457\u9ad8\u4e8eLAFAN1\u3001PHUMA\u548cAMASS\u7b49\u5e38\u7528\u6570\u636e\u96c6\uff0c\u8868\u660e\u8fd0\u52a8\u5f3a\u5ea6\u548c\u590d\u6742\u6027\u5927\u5e45\u589e\u52a0\u3002\u63d0\u51fa\u7684\u6846\u67b6\u5c06\u673a\u5668\u4eba\u80fd\u529b\u4ece\u7eaf\u8fd0\u52a8\u8ddf\u8e2a\u6269\u5c55\u5230\u652f\u6301\u6062\u590d\u7684\u6267\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4fc3\u8fdb\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u9ad8\u52a8\u6001\u573a\u666f\u4e2d\u66f4\u9c81\u68d2\u548c\u81ea\u4e3b\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u4ece\u7eaf\u8fd0\u52a8\u8ddf\u8e2a\u5230\u6062\u590d\u652f\u6301\u6267\u884c\u7684\u6269\u5c55\u3002"}}
{"id": "2602.13649", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13649", "abs": "https://arxiv.org/abs/2602.13649", "authors": ["Li Zhang", "Nital Patel", "Xiuqi Li", "Jessica Lin"], "title": "Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series", "comment": null, "summary": "Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \\textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u94fe\u5b9a\u4e49\uff0c\u7528\u4e8e\u5728\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u4e24\u4e2a\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u53d1\u73b0\u610f\u5916\u7684\u6f14\u5316\u8d8b\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u4e00\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u94fe\u5b9a\u4e49\u4ec5\u8003\u8651\u5728\u5355\u4e00\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u4e2d\u5bfb\u627e\u94fe\uff0c\u5bb9\u6613\u9519\u8fc7\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u8de8\u4e24\u4e2a\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u610f\u5916\u6f14\u5316\u6a21\u5f0f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e2d\u65ad\u548c\u8de8\u5e8f\u5217\u60c5\u51b5\u7684\u65b0\u5b9a\u4e49\u3002", "method": "\u63d0\u51fa\u4e86\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u94fe\u7684\u65b0\u5b9a\u4e49\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5728\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u4e24\u4e2a\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u53d1\u73b0\u610f\u5916\u6f14\u5316\u8d8b\u52bf\u3002\u8be5\u5b9a\u4e49\u91cd\u70b9\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u4e2d\u65ad\u6216\u95f4\u9694\u5f15\u8d77\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u6392\u5e8f\u6807\u51c6\u6765\u8bc6\u522b\u6700\u4f73\u94fe\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9a\u4f4d\u5f02\u5e38\u6f14\u5316\u6a21\u5f0f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u94fe\u5de5\u4f5c\u3002\u5728\u82f1\u7279\u5c14\u7684\u771f\u5b9e\u5236\u9020\u5e94\u7528\u4e2d\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8054\u5408\u65f6\u95f4\u5e8f\u5217\u94fe\u5b9a\u4e49\u80fd\u591f\u6709\u6548\u6355\u6349\u4e2d\u65ad\u65f6\u95f4\u5e8f\u5217\u6216\u8de8\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u6f14\u5316\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.14966", "categories": ["cs.GT", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.14966", "abs": "https://arxiv.org/abs/2602.14966", "authors": ["Umang Bhaskar", "Juhi Chaudhary", "Sushmita Gupta", "Pallavi Jain", "Sanjay Seetharaman"], "title": "Robust Value Maximization in Challenge the Champ Tournaments with Probabilistic Outcomes", "comment": "28 pages; full version of the paper to appear in AAMAS 2026", "summary": "Challenge the Champ is a simple tournament format, where an ordering of the players -- called a seeding -- is decided. The first player in this order is the initial champ, and faces the next player. The outcome of each match decides the current champion, who faces the next player in the order. Each player also has a popularity, and the value of each match is the popularity of the winner. Value maximization in tournaments has been previously studied when each match has a deterministic outcome. However, match outcomes are often probabilistic, rather than deterministic. We study robust value maximization in Challenge the Champ tournaments, when the winner of a match may be probabilistic. That is, we seek to maximize the total value that is obtained, irrespective of the outcome of probabilistic matches. We show that even in simple binary settings, for non-adaptive algorithms, the optimal robust value -- which we term the \\textsc{VnaR}, or the value not at risk -- is hard to approximate. However, if we allow adaptive algorithms that determine the order of challengers based on the outcomes of previous matches, or restrict the matches with probabilistic outcomes, we can obtain good approximations to the optimal \\textsc{VnaR}.", "AI": {"tldr": "\u7814\u7a76\u6311\u6218\u51a0\u519b\u9526\u6807\u8d5b\u4e2d\u7684\u9c81\u68d2\u4ef7\u503c\u6700\u5927\u5316\u95ee\u9898\uff0c\u8003\u8651\u6bd4\u8d5b\u7ed3\u679c\u5177\u6709\u6982\u7387\u6027\u7684\u60c5\u51b5\uff0c\u63a2\u7d22\u81ea\u9002\u5e94\u4e0e\u975e\u81ea\u9002\u5e94\u7b97\u6cd5\u4e0b\u7684\u6700\u4f18\u89e3\u8fd1\u4f3c\u96be\u5ea6\u3002", "motivation": "\u4f20\u7edf\u9526\u6807\u8d5b\u4ef7\u503c\u6700\u5927\u5316\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u6bd4\u8d5b\u7ed3\u679c\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u6bd4\u8d5b\u7ed3\u679c\u5f80\u5f80\u662f\u6982\u7387\u6027\u7684\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5f53\u6bd4\u8d5b\u7ed3\u679c\u5177\u6709\u6982\u7387\u6027\u65f6\uff0c\u5982\u4f55\u5728\u6311\u6218\u51a0\u519b\u9526\u6807\u8d5b\u4e2d\u5b9e\u73b0\u9c81\u68d2\u4ef7\u503c\u6700\u5927\u5316\u3002", "method": "\u7814\u7a76\u6311\u6218\u51a0\u519b\u9526\u6807\u8d5b\u683c\u5f0f\uff0c\u8003\u8651\u73a9\u5bb6\u6392\u5e8f\uff08\u79cd\u5b50\uff09\u548c\u6bd4\u8d5b\u7ed3\u679c\u7684\u6982\u7387\u6027\u3002\u5206\u6790\u975e\u81ea\u9002\u5e94\u7b97\u6cd5\u4e0b\u7684\u6700\u4f18\u9c81\u68d2\u4ef7\u503c\uff08VnaR\uff09\u8fd1\u4f3c\u96be\u5ea6\uff0c\u5e76\u63a2\u7d22\u81ea\u9002\u5e94\u7b97\u6cd5\uff08\u6839\u636e\u5148\u524d\u6bd4\u8d5b\u7ed3\u679c\u8c03\u6574\u6311\u6218\u8005\u987a\u5e8f\uff09\u548c\u9650\u5236\u6982\u7387\u6027\u6bd4\u8d5b\u60c5\u51b5\u4e0b\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u5728\u7b80\u5355\u7684\u4e8c\u5143\u8bbe\u7f6e\u4e2d\uff0c\u975e\u81ea\u9002\u5e94\u7b97\u6cd5\u4e0b\u7684\u6700\u4f18\u9c81\u68d2\u4ef7\u503c\uff08VnaR\uff09\u96be\u4ee5\u8fd1\u4f3c\u3002\u4f46\u901a\u8fc7\u81ea\u9002\u5e94\u7b97\u6cd5\u6216\u9650\u5236\u6982\u7387\u6027\u6bd4\u8d5b\uff0c\u53ef\u4ee5\u83b7\u5f97\u5bf9\u6700\u4f18VnaR\u7684\u826f\u597d\u8fd1\u4f3c\u3002", "conclusion": "\u5728\u6982\u7387\u6027\u6bd4\u8d5b\u7ed3\u679c\u7684\u6311\u6218\u51a0\u519b\u9526\u6807\u8d5b\u4e2d\uff0c\u81ea\u9002\u5e94\u7b97\u6cd5\u548c\u9650\u5236\u6982\u7387\u6027\u6bd4\u8d5b\u662f\u83b7\u5f97\u9c81\u68d2\u4ef7\u503c\u6700\u5927\u5316\u826f\u597d\u8fd1\u4f3c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u800c\u975e\u81ea\u9002\u5e94\u7b97\u6cd5\u5728\u6b64\u7c7b\u95ee\u9898\u4e2d\u9762\u4e34\u8ba1\u7b97\u56f0\u96be\u3002"}}
{"id": "2602.13689", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13689", "abs": "https://arxiv.org/abs/2602.13689", "authors": ["Wonju Lee", "Matteo Grimaldi", "Tao Yu"], "title": "Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation", "comment": "Accepted By ICRA2026", "summary": "Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that na\u00efve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing na\u00efve and gated fusion baselines and closely matching the privileged \"wrist + contact force\" configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9-\u89e6\u89c9\u878d\u5408\u7684\u8de8\u6a21\u6001Transformer\uff08CMT\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6574\u5408\u624b\u8155\u6444\u50cf\u5934\u89c2\u6d4b\u4e0e\u89e6\u89c9\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165\u7269\u7406\u4fe1\u606f\u6b63\u5219\u5316\u6765\u7a33\u5b9a\u89e6\u89c9\u5d4c\u5165\uff0c\u5728\u63d2\u5165\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8696.59%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u63d2\u5165\u4efb\u52a1\u9700\u8981\u7cbe\u786e\u7684\u63a5\u89e6\u4e30\u5bcc\u4ea4\u4e92\uff0c\u4ec5\u9760\u89c6\u89c9\u65e0\u6cd5\u89e3\u51b3\u3002\u867d\u7136\u89e6\u89c9\u53cd\u9988\u5177\u6709\u76f4\u89c2\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u7b80\u5355\u7684\u89c6\u89c9-\u89e6\u89c9\u878d\u5408\u5f80\u5f80\u65e0\u6cd5\u5e26\u6765\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001Transformer\uff08CMT\uff09\u7528\u4e8e\u89c6\u89c9-\u89e6\u89c9\u878d\u5408\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6574\u5408\u624b\u8155\u6444\u50cf\u5934\u89c2\u6d4b\u4e0e\u89e6\u89c9\u4fe1\u53f7\u3002\u8fdb\u4e00\u6b65\u5f15\u5165\u7269\u7406\u4fe1\u606f\u6b63\u5219\u5316\uff0c\u9f13\u52b1\u53cc\u8fb9\u529b\u5e73\u8861\uff0c\u53cd\u6620\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u539f\u7406\u3002", "result": "\u5728TacSL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e26\u6709\u5bf9\u79f0\u6b63\u5219\u5316\u7684CMT\u5b9e\u73b0\u4e8696.59%\u7684\u63d2\u5165\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86\u7b80\u5355\u878d\u5408\u548c\u95e8\u63a7\u878d\u5408\u57fa\u7ebf\uff0c\u5e76\u63a5\u8fd1\u7279\u6743\u914d\u7f6e\"\u624b\u8155+\u63a5\u89e6\u529b\"\u768496.09%\u6027\u80fd\u3002", "conclusion": "\u89e6\u89c9\u4f20\u611f\u5bf9\u4e8e\u7cbe\u786e\u5bf9\u9f50\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\uff0c\u800c\u539f\u5219\u6027\u7684\u591a\u6a21\u6001\u878d\u5408\u52a0\u4e0a\u7269\u7406\u4fe1\u606f\u6b63\u5219\u5316\u80fd\u591f\u89e3\u9501\u89c6\u89c9\u548c\u89e6\u89c9\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5728\u73b0\u5b9e\u4f20\u611f\u6761\u4ef6\u4e0b\u63a5\u8fd1\u7279\u6743\u6027\u80fd\u3002"}}
{"id": "2602.13651", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13651", "abs": "https://arxiv.org/abs/2602.13651", "authors": ["Stefan Behfar", "Richard Mortier"], "title": "Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation", "comment": null, "summary": "In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u6cd5\u2014\u2014\u7d2f\u79ef\u6548\u7528\u5747\u7b49\uff0c\u5173\u6ce8\u5ba2\u6237\u957f\u671f\u53c2\u4e0e\u673a\u4f1a\u7684\u516c\u5e73\u6027\u800c\u975e\u5355\u8f6e\u8bad\u7ec3\u7684\u516c\u5e73\u6027\uff0c\u89e3\u51b3\u4e86\u95f4\u6b47\u6027\u53c2\u4e0e\u5ba2\u6237\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53c2\u4e0e\u6761\u4ef6\u4e0b\u7684\u635f\u5931\u6216\u51c6\u786e\u7387\u5747\u7b49\u5316\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5ba2\u6237\u53c2\u4e0e\u662f\u95f4\u6b47\u6027\u3001\u5f02\u8d28\u6027\u4e14\u4e0e\u6570\u636e\u7279\u5f81\u6216\u8d44\u6e90\u7ea6\u675f\u76f8\u5173\u7684\u3002\u5f53\u53c2\u4e0e\u672c\u8eab\u4e0d\u5747\u8861\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u95f4\u6b47\u6027\u53ef\u7528\u5ba2\u6237\u88ab\u7cfb\u7edf\u6027\u4f4e\u4f30\uff0c\u5373\u4f7f\u6bcf\u8f6e\u6027\u80fd\u770b\u8d77\u6765\u516c\u5e73\u3002", "method": "\u63d0\u51fa\u7d2f\u79ef\u6548\u7528\u5747\u7b49\u539f\u5219\uff0c\u8bc4\u4f30\u5ba2\u6237\u662f\u5426\u83b7\u5f97\u53ef\u6bd4\u8f83\u7684\u957f\u671f\u5229\u76ca\u800c\u975e\u6bcf\u8f6e\u8bad\u7ec3\u7684\u5229\u76ca\u3002\u5f15\u5165\u53ef\u7528\u6027\u5f52\u4e00\u5316\u7d2f\u79ef\u6548\u7528\uff0c\u5c06\u4e0d\u53ef\u907f\u514d\u7684\u7269\u7406\u7ea6\u675f\u4e0e\u53ef\u907f\u514d\u7684\u7b97\u6cd5\u504f\u5dee\uff08\u6765\u81ea\u8c03\u5ea6\u548c\u805a\u5408\uff09\u89e3\u8026\u3002", "result": "\u5728\u65f6\u95f4\u504f\u659c\u3001\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u8054\u90a6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u671f\u8868\u793a\u5747\u7b49\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6027\u80fd\u3002", "conclusion": "\u7d2f\u79ef\u6548\u7528\u5747\u7b49\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u5ba2\u6237\u53c2\u4e0e\u4e0d\u5747\u8861\u7684\u573a\u666f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u62a4\u95f4\u6b47\u6027\u53c2\u4e0e\u5ba2\u6237\u7684\u5229\u76ca\u3002"}}
{"id": "2602.13587", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13587", "abs": "https://arxiv.org/abs/2602.13587", "authors": ["Joseph Corneli"], "title": "A First Proof Sprint", "comment": "144 pages, 7 color images. Submission to First Proof February 2026 (arxiv:2602.05192, https://1stproof.org/), uploaded 20:07 Friday, 13 February 2026 Pacific Time (PT)", "summary": "This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.", "AI": {"tldr": "\u672c\u6587\u62a5\u9053\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8bc1\u660e\u51b2\u523a\u9879\u76ee\uff0c\u9488\u5bf9\u5341\u4e2a\u7814\u7a76\u7ea7\u95ee\u9898\uff0c\u7ed3\u5408\u5feb\u901f\u8349\u7a3f\u751f\u6210\u3001\u5bf9\u6297\u6027\u9a8c\u8bc1\u3001\u9488\u5bf9\u6027\u4fee\u590d\u548c\u660e\u786e\u6eaf\u6e90\u3002\u901a\u8fc7\u4f9d\u8d56\u5173\u7cfb\u56fe\u5206\u89e3\u5b9a\u4f4d\u6f0f\u6d1e\u5e76\u534f\u8c03\u8bc4\u5ba1\u9a71\u52a8\u7684\u4fee\u8ba2\uff0c\u6700\u7ec8\u83b7\u5f97\u5f02\u8d28\u4f46\u660e\u786e\u7684\u6210\u679c\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u538b\u7f29\u65f6\u95f4\u6846\u67b6\u5185\u8fdb\u884c\u9ad8\u8d28\u91cf\u6570\u5b66\u8bc1\u660e\u9a8c\u8bc1\u7684\u65b9\u6cd5\u3002\u4f20\u7edf\u8bc1\u660e\u8fc7\u7a0b\u901a\u5e38\u8017\u65f6\u8f83\u957f\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u7ed3\u6784\u5316\u9a8c\u8bc1\u548c\u5206\u5c42\u7b56\u7565\u6765\u63d0\u9ad8\u8bc1\u660e\u51b2\u523a\u7684\u53ef\u9760\u6027\u548c\u6821\u51c6\u5ea6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u5feb\u901f\u8349\u7a3f\u751f\u6210\u4e0e\u5bf9\u6297\u6027\u9a8c\u8bc1\u3002\u4f7f\u7528\u63a5\u7ebf\u56fe\u5206\u89e3\u6765\u53ef\u89c6\u5316\u58f0\u660e\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u5b9a\u4f4d\u8bc1\u660e\u6f0f\u6d1e\u3002\u5b9e\u65bd\u9488\u5bf9\u6027\u4fee\u590d\u548c\u660e\u786e\u6eaf\u6e90\u673a\u5236\uff0c\u533a\u5206\u6570\u5b66\u72b6\u6001\u4e0eQC\u9a8c\u8bc1\u72b6\u6001\u3002\u91c7\u7528\u5206\u5c42\u5207\u6362\u7b56\u7565\uff0c\u5c06\u6570\u5b66\u8bc1\u660e\u4e0e\u9a8c\u8bc1\u8fc7\u7a0b\u5206\u79bb\u3002", "result": "\u5341\u4e2a\u95ee\u9898\u83b7\u5f97\u5f02\u8d28\u7ed3\u679c\uff1a\u95ee\u98983\u5728\u7279\u5b9a\u6807\u51c6\u4e0b\u83b7\u5f97\u9a8c\u8bc1\u5b8c\u6574\u7684\u8bc1\u660e\u8def\u5f84\uff1b\u95ee\u98985\u5728\u9650\u5b9a\u8303\u56f4\u5185\u89e3\u51b3\uff1b\u95ee\u989810\u5728\u660e\u786e\u5047\u8bbe\u4e0b\u89e3\u51b3\uff1b\u95ee\u98984\u548c6\u83b7\u5f97\u90e8\u5206\u7ed3\u679c\uff1b\u95ee\u98987\u901a\u8fc7\u65cb\u8f6c\u8def\u5f84\u5b9a\u7406\u94fe\u6682\u65f6\u5173\u95ed\u3002\u5728QC\u9a8c\u8bc1\u5c42\uff0c\u95ee\u98987\u548c9\u6709\u8282\u70b9\u7ea7\u9a8c\u8bc1\u4f46\u4ecd\u6709\u672a\u89e3\u51b3\u7684\u9a8c\u8bc1\u7f3a\u53e3\u3002", "conclusion": "\u7ed3\u6784\u5316\u9a8c\u8bc1\u548c\u5206\u5c42\u5207\u6362\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u8bc1\u660e\u51b2\u523a\u7684\u53ef\u9760\u6027\u548c\u6821\u51c6\u5ea6\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u660e\u786e\u533a\u5206\u6570\u5b66\u72b6\u6001\u548c\u9a8c\u8bc1\u72b6\u6001\u7684\u5f02\u8d28\u7ed3\u679c\uff0c\u4e3a\u5feb\u901f\u6570\u5b66\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.13739", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.13739", "abs": "https://arxiv.org/abs/2602.13739", "authors": ["Mal Fazliu", "Matthew Coombes", "Sen Wang", "Cunjia Liu"], "title": "XIT: Exploration and Exploitation Informed Trees for Active Gas Distribution Mapping in Unknown Environments", "comment": null, "summary": "Mobile robotic gas distribution mapping (GDM) provides critical situational awareness during emergency responses to hazardous gas releases. However, most systems still rely on teleoperation, limiting scalability and response speed. Autonomous active GDM is challenging in unknown and cluttered environments, because the robot must simultaneously explore traversable space, map the environment, and infer the gas distribution belief from sparse chemical measurements. We address this by formulating active GDM as a next-best-trajectory informative path planning (IPP) problem and propose XIT (Exploration-Exploitation Informed Trees), a sampling-based planner that balances exploration and exploitation by generating concurrent trajectories toward exploration-rich goals while collecting informative gas measurements en route. XIT draws batches of samples from an Upper Confidence Bound (UCB) information field derived from the current gas posterior and expands trees using a cost that trades off travel effort against gas concentration and uncertainty. To enable plume-aware exploration, we introduce the gas frontier concept, defined as unobserved regions adjacent to high gas concentrations, and propose the Wavefront Gas Frontier Detection (WGFD) algorithm for their identification. High-fidelity simulations and real-world experiments demonstrate the benefits of XIT in terms of GDM quality and efficiency. Although developed for active GDM, XIT is readily applicable to other robotic information-gathering tasks in unknown environments that face the exploration and exploitation trade-off.", "AI": {"tldr": "\u63d0\u51faXIT\u7b97\u6cd5\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u81ea\u4e3b\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\uff0c\u901a\u8fc7\u91c7\u6837\u89c4\u5212\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u7ed3\u5408\u6c14\u4f53\u524d\u6cbf\u6982\u5ff5\u63d0\u5347\u5efa\u56fe\u6548\u7387", "motivation": "\u5f53\u524d\u79fb\u52a8\u673a\u5668\u4eba\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u9065\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u54cd\u5e94\u901f\u5ea6\u3002\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u4e3b\u52a8\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u63a2\u7d22\u53ef\u901a\u884c\u7a7a\u95f4\u3001\u6784\u5efa\u73af\u5883\u5730\u56fe\u5e76\u4ece\u7a00\u758f\u5316\u5b66\u6d4b\u91cf\u4e2d\u63a8\u65ad\u6c14\u4f53\u5206\u5e03", "method": "\u5c06\u4e3b\u52a8\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\u5efa\u6a21\u4e3a\u4e0b\u4e00\u4e2a\u6700\u4f73\u8f68\u8ff9\u4fe1\u606f\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51faXIT\u91c7\u6837\u89c4\u5212\u5668\u3002\u8be5\u89c4\u5212\u5668\u4ece\u57fa\u4e8e\u5f53\u524d\u6c14\u4f53\u540e\u9a8c\u7684\u4e0a\u7f6e\u4fe1\u754c\u4fe1\u606f\u573a\u4e2d\u6279\u91cf\u91c7\u6837\uff0c\u4f7f\u7528\u6743\u8861\u65c5\u884c\u6210\u672c\u4e0e\u6c14\u4f53\u6d53\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u4ee3\u4ef7\u51fd\u6570\u6269\u5c55\u6811\u3002\u5f15\u5165\u6c14\u4f53\u524d\u6cbf\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u6ce2\u524d\u6c14\u4f53\u524d\u6cbf\u68c0\u6d4b\u7b97\u6cd5\u8bc6\u522b\u8fd9\u4e9b\u533a\u57df", "result": "\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660eXIT\u5728\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002\u867d\u7136\u4e3a\u4e3b\u52a8\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\u5f00\u53d1\uff0c\u4f46XIT\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u673a\u5668\u4eba\u4fe1\u606f\u6536\u96c6\u4efb\u52a1", "conclusion": "XIT\u7b97\u6cd5\u901a\u8fc7\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u81ea\u4e3b\u6c14\u4f53\u5206\u5e03\u5efa\u56fe\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u5efa\u56fe\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2602.13718", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13718", "abs": "https://arxiv.org/abs/2602.13718", "authors": ["Zhenchen Dong", "Jinna Fu", "Jiaming Wu", "Shengyuan Yu", "Fulin Chen", "Yide Liu"], "title": "HybridFlow: A Two-Step Generative Policy for Robotic Manipulation", "comment": null, "summary": "Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \\textbf{HybridFlow}, a \\textbf{3-stage method} with \\textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \\textbf{15--25\\%} in success rate while reducing inference time from 152ms to 19ms (\\textbf{8$\\times$ speedup}, \\textbf{$\\sim$52Hz}); it also achieves 70.0\\% success on unseen-color OOD grasping and 66.3\\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.", "AI": {"tldr": "HybridFlow\u662f\u4e00\u79cd3\u9636\u6bb52\u6b65\u63a8\u7406\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408MeanFlow\u7684\u5feb\u901f\u5355\u6b65\u751f\u6210\u548cReFlow\u7684\u7cbe\u786e\u7ec6\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u5b9e\u73b08\u500d\u52a0\u901f\u548c52Hz\u5b9e\u65f6\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u53d7\u9650\u4e8e\u63a8\u7406\u5ef6\u8fdf\uff0c\u7f3a\u4e4f\u8db3\u591f\u7684\u5b9e\u65f6\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002\u867d\u7136\u6d41\u5339\u914d\u7b49\u5feb\u901f\u751f\u6210\u65b9\u6cd5\u9010\u6e10\u53d6\u4ee3\u6269\u6563\u65b9\u6cd5\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u4ecd\u5728\u5bfb\u6c42\u66f4\u9002\u5408\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u63a7\u5236\u7684\u66f4\u5feb\u751f\u6210\u65b9\u6cd5\u3002MeanFlow\u4f5c\u4e3a\u6d41\u5339\u914d\u7684\u5355\u6b65\u53d8\u4f53\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u52a8\u4f5c\u751f\u6210\u7cbe\u5ea6\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u4e25\u683c\u8981\u6c42\u3002", "method": "\u63d0\u51faHybridFlow\uff0c\u4e00\u79cd3\u9636\u6bb52\u6b65\u63a8\u7406\u7684\u65b9\u6cd5\uff1a1\uff09MeanFlow\u6a21\u5f0f\u7684\u5168\u5c40\u8df3\u8dc3\uff0c2\uff09\u5206\u5e03\u5bf9\u9f50\u7684ReNoise\uff0c3\uff09ReFlow\u6a21\u5f0f\u7684\u5c40\u90e8\u7ec6\u5316\u3002\u8be5\u65b9\u6cd5\u5229\u7528MeanFlow\u5355\u6b65\u751f\u6210\u7684\u5feb\u901f\u4f18\u52bf\uff0c\u540c\u65f6\u901a\u8fc7\u6700\u5c0f\u751f\u6210\u6b65\u9aa4\u786e\u4fdd\u52a8\u4f5c\u7cbe\u5ea6\uff0c\u5e73\u8861\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cHybridFlow\u5728\u6210\u529f\u7387\u4e0a\u6bd416\u6b65\u6269\u6563\u7b56\u7565\u63d0\u9ad815-25%\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u65f6\u95f4\u4ece152ms\u51cf\u5c11\u523019ms\uff088\u500d\u52a0\u901f\uff0c\u7ea652Hz\uff09\u3002\u5728\u672a\u89c1\u989c\u8272OOD\u6293\u53d6\u4efb\u52a1\u4e0a\u8fbe\u523070.0%\u6210\u529f\u7387\uff0c\u5728\u53ef\u53d8\u5f62\u7269\u4f53\u6298\u53e0\u4efb\u52a1\u4e0a\u8fbe\u523066.3%\u6210\u529f\u7387\u3002", "conclusion": "HybridFlow\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u4f4e\u5ef6\u8fdf\u65b9\u6cd5\uff0c\u6709\u671b\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5e73\u8861\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13659", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.13659", "abs": "https://arxiv.org/abs/2602.13659", "authors": ["Valery Parfenov", "Grigoriy Evseev", "Andrey Veprikov", "Nikolay Bushkov", "Stanislav Moiseev", "Aleksandr Beznosikov"], "title": "Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling", "comment": null, "summary": "Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7b56\u7565\u5b66\u4e60\u7684\u96f6\u9636\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6270\u52a8\u65b9\u5411\u91c7\u6837\u5206\u5e03\u964d\u4f4e\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\uff0c\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u5185\u5b58\u6548\u7387", "motivation": "\u4f20\u7edf\u96f6\u9636\u65b9\u6cd5\u867d\u7136\u80fd\u907f\u514d\u53cd\u5411\u4f20\u64ad\u8282\u7701\u5185\u5b58\uff0c\u4f46\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u5bf9\u53c2\u6570\u7ef4\u5ea6\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u7ef4\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u5e94\u7528", "method": "\u5c06\u6270\u52a8\u65b9\u5411\u91c7\u6837\u5206\u5e03\u89c6\u4e3a\u53ef\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u66f4\u65b0\u8be5\u7b56\u7565\u6765\u964d\u4f4e\u65b9\u5411\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u5f00\u53d1\u4e86\u5b9e\u7528\u7b97\u6cd5\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u5b66\u4e60\u91c7\u6837\u5206\u5e03\u80fd\u63d0\u9ad8\u68af\u5ea6\u4fe1\u606f\u8d28\u91cf\u5e76\u653e\u677e\u6536\u655b\u754c\u5bf9\u7ef4\u5ea6d\u7684\u4f9d\u8d56\uff0c\u5728LLM\u5fae\u8c03\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u96f6\u9636\u57fa\u7ebf", "conclusion": "\u81ea\u9002\u5e94\u65b9\u5411\u91c7\u6837\u662f\u4f7f\u96f6\u9636\u5fae\u8c03\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u53ef\u884c\u7684\u6709\u524d\u666f\u9014\u5f84\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.14104", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14104", "abs": "https://arxiv.org/abs/2602.14104", "authors": ["Xinan Rong", "Changhuang Wan", "Aochen He", "Xiaolong Li", "Gangshan Jing"], "title": "Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects", "comment": "This paper has been accepted by IEEE Robotics and Automation Letters. The experimental video is avaialable at: https://www.youtube.com/watch?v=kcf9dVW0Dpo", "summary": "Precise in-hand manipulation of force-sensitive objects typically requires judicious coordinated force planning as well as accurate contact force feedback and control. Unlike multi-arm platforms with gripper end effectors, multi-fingered hands rely solely on fingertip point contacts and are not able to apply pull forces, therefore poses a more challenging problem. Furthermore, calibrated torque sensors are lacking in most commercial dexterous hands, adding to the difficulty. To address these challenges, we propose a dual-layer framework for multi-finger coordination, enabling high-precision manipulation of force-sensitive objects through joint control without tactile feedback. This approach solves coordinated contact force planning by incorporating graph rigidity and force closure constraints. By employing a force-to-position mapping, the planned force trajectory is converted to a joint trajectory. We validate the framework on a custom dexterous hand, demonstrating the capability to manipulate fragile objects-including a soft yarn, a plastic cup, and a raw egg-with high precision and safety.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5c42\u6846\u67b6\u5b9e\u73b0\u591a\u6307\u534f\u8c03\uff0c\u65e0\u9700\u89e6\u89c9\u53cd\u9988\u5373\u53ef\u901a\u8fc7\u5173\u8282\u63a7\u5236\u7cbe\u786e\u64cd\u7eb5\u529b\u654f\u611f\u7269\u4f53", "motivation": "\u591a\u6307\u624b\u4ec5\u4f9d\u8d56\u6307\u5c16\u70b9\u63a5\u89e6\u4e14\u65e0\u6cd5\u65bd\u52a0\u62c9\u529b\uff0c\u6bd4\u591a\u81c2\u5e73\u53f0\u66f4\u5177\u6311\u6218\u6027\uff1b\u5927\u591a\u6570\u5546\u4e1a\u7075\u5de7\u624b\u7f3a\u4e4f\u6821\u51c6\u626d\u77e9\u4f20\u611f\u5668\uff0c\u589e\u52a0\u4e86\u7cbe\u786e\u529b\u63a7\u7684\u96be\u5ea6", "method": "\u91c7\u7528\u53cc\u5c42\u6846\u67b6\uff1a1) \u7ed3\u5408\u56fe\u521a\u6027\u548c\u529b\u95ed\u5408\u7ea6\u675f\u89e3\u51b3\u534f\u8c03\u63a5\u89e6\u529b\u89c4\u5212\uff1b2) \u901a\u8fc7\u529b-\u4f4d\u7f6e\u6620\u5c04\u5c06\u89c4\u5212\u7684\u529b\u8f68\u8ff9\u8f6c\u6362\u4e3a\u5173\u8282\u8f68\u8ff9\uff0c\u5b9e\u73b0\u65e0\u89e6\u89c9\u53cd\u9988\u7684\u5173\u8282\u63a7\u5236", "result": "\u5728\u5b9a\u5236\u7075\u5de7\u624b\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5b89\u5168\u5730\u64cd\u7eb5\u8106\u5f31\u7269\u4f53\uff08\u8f6f\u7eb1\u7ebf\u3001\u5851\u6599\u676f\u3001\u751f\u9e21\u86cb\uff09", "conclusion": "\u63d0\u51fa\u7684\u53cc\u5c42\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u6307\u534f\u8c03\u4e2d\u7684\u529b\u89c4\u5212\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u89e6\u89c9\u53cd\u9988\u7684\u9ad8\u7cbe\u5ea6\u529b\u654f\u611f\u7269\u4f53\u64cd\u7eb5"}}
{"id": "2602.13720", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13720", "abs": "https://arxiv.org/abs/2602.13720", "authors": ["Chen Feng", "Yang Xu", "Shaojie Shen"], "title": "FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments", "comment": "8 pages, 8 figures, 3 tables", "summary": "Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.", "AI": {"tldr": "FC-Vision\u662f\u4e00\u4e2a\u5b9e\u65f6\u53ef\u89c1\u6027\u611f\u77e5\u91cd\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u81ea\u4e3b\u626b\u63cf\u76ee\u6807\u7ed3\u6784\uff0c\u4e3b\u52a8\u9632\u6b62\u76ee\u6807\u906e\u6321\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u626b\u63cf\u8986\u76d6\u7387\u548c\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u626b\u63cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u626b\u63cf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u907f\u78b0\u548c\u6548\u7387\uff0c\u4f46\u5ffd\u89c6\u4e86\u906e\u6321\u5bfc\u81f4\u7684\u53ef\u89c1\u6027\u9000\u5316\u95ee\u9898\uff0c\u8fd9\u4f1a\u4e25\u91cd\u635f\u5bb3\u626b\u63cf\u8d28\u91cf\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u65e0\u4eba\u673a\u9700\u8981\u5728\u98de\u884c\u8fc7\u7a0b\u4e2d\u9002\u5e94\u672a\u77e5\u969c\u788d\u7269\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u5728\u7ebf\u5904\u7406\u906e\u6321\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFC-Vision\u6846\u67b6\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u4e24\u7ea7\u5206\u89e3\u65b9\u6cd5\uff1a1) \u65e0\u906e\u6321\u89c6\u70b9\u4fee\u590d\uff0c\u5728\u4fdd\u6301\u8986\u76d6\u8303\u56f4\u7684\u540c\u65f6\u6700\u5c0f\u5316\u504f\u79bb\u539f\u59cb\u626b\u63cf\u610f\u56fe\uff1b2) \u57285\u81ea\u7531\u5ea6\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5206\u6bb5\u6e05\u6d01\u611f\u77e5\u8fde\u63a5\u3002\u6846\u67b6\u8fd8\u63d0\u4f9b\u4e86\u63d2\u4ef6\u5f0f\u96c6\u6210\u7b56\u7565\uff0c\u53ef\u4e0e\u73b0\u6709\u65e0\u4eba\u673a\u626b\u63cf\u7cfb\u7edf\u65e0\u7f1d\u5bf9\u63a5\u800c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u3002", "result": "\u7efc\u5408\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u8868\u660e\uff0cFC-Vision\u5728\u610f\u5916\u906e\u6321\u7269\u4e0b\u6301\u7eed\u63d0\u5347\u626b\u63cf\u8d28\u91cf\uff0c\u6700\u5927\u8986\u76d6\u589e\u76ca\u8fbe55.32%\uff0c\u906e\u6321\u7387\u964d\u4f4e73.17%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0c\u98de\u884c\u65f6\u95f4\u4ec5\u6709\u9002\u5ea6\u589e\u52a0\u3002", "conclusion": "FC-Vision\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5b9e\u65f6\u53ef\u89c1\u6027\u611f\u77e5\u91cd\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u4e3b\u52a8\u5b89\u5168\u5730\u9632\u6b62\u76ee\u6807\u906e\u6321\uff0c\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u81ea\u4e3b\u626b\u63cf\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u7cfb\u7edf\u517c\u5bb9\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.13660", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13660", "abs": "https://arxiv.org/abs/2602.13660", "authors": ["Jiayi Huang", "Amirmohammad Farzaneh", "Osvaldo Simeone"], "title": "Optimized Certainty Equivalent Risk-Controlling Prediction Sets", "comment": "Sumitted to EUSIPCO", "summary": "In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOCE-RCPS\u6846\u67b6\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u63d0\u4f9b\u57fa\u4e8e\u4f18\u5316\u786e\u5b9a\u6027\u7b49\u4ef7\u98ce\u9669\u5ea6\u91cf\u7684\u6982\u7387\u4fdd\u8bc1\uff0c\u8d85\u8d8a\u4f20\u7edf\u98ce\u9669\u63a7\u5236\u9884\u6d4b\u96c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u9884\u6d4b\u7cfb\u7edf\u9700\u8981\u63d0\u4f9b\u8d85\u8d8a\u4f20\u7edf\u671f\u671b\u635f\u5931\u63a7\u5236\u7684\u53ef\u9760\u6027\u4fdd\u8bc1\u3002\u73b0\u6709\u98ce\u9669\u63a7\u5236\u9884\u6d4b\u96c6\uff08RCPS\uff09\u867d\u7136\u63d0\u4f9b\u671f\u671b\u98ce\u9669\u7684\u6982\u7387\u4fdd\u8bc1\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u5c3e\u90e8\u884c\u4e3a\u548c\u6700\u574f\u60c5\u51b5\uff0c\u800c\u8fd9\u4e9b\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4f18\u5316\u786e\u5b9a\u6027\u7b49\u4ef7RCPS\uff08OCE-RCPS\uff09\u6846\u67b6\uff0c\u5229\u7528\u7f6e\u4fe1\u4e0a\u754c\u8bc6\u522b\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u98ce\u9669\u5bb9\u5fcd\u6c34\u5e73\u7684\u9884\u6d4b\u96c6\u53c2\u6570\uff0c\u63d0\u4f9b\u5305\u62ec\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\uff08CVaR\uff09\u548c\u71b5\u98ce\u9669\u5728\u5185\u7684\u5e7f\u4e49OCE\u98ce\u9669\u5ea6\u91cf\u7684\u9ad8\u6982\u7387\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660eOCE-RCPS\u5bf9\u8bef\u8986\u76d6\u7387\u548c\u5047\u9634\u6027\u7387\u7b49\u635f\u5931\u51fd\u6570\u6ee1\u8db3\u671f\u671b\u7684\u6982\u7387\u7ea6\u675f\u3002\u56fe\u50cf\u5206\u5272\u5b9e\u9a8c\u8868\u660e\uff0cOCE-RCPS\u5728\u5404\u79cd\u98ce\u9669\u5ea6\u91cf\u548c\u53ef\u9760\u6027\u914d\u7f6e\u4e0b\u59cb\u7ec8\u6ee1\u8db3\u76ee\u6807\u6ee1\u8db3\u7387\uff0c\u800cOCE-CRC\u65e0\u6cd5\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\u3002", "conclusion": "OCE-RCPS\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u53ef\u9760\u6027\u4fdd\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u63a7\u5236\u5c3e\u90e8\u98ce\u9669\u548c\u6700\u574f\u60c5\u51b5\uff0c\u5f25\u8865\u4e86\u4f20\u7edfRCPS\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.14222", "categories": ["cs.RO", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.14222", "abs": "https://arxiv.org/abs/2602.14222", "authors": ["Antonio Franchi"], "title": "Muscle Coactivation in the Sky: Geometry and Pareto Optimality of Energy vs. Promptness in Multirotors", "comment": null, "summary": "In robotics and human biomechanics, the tension between energy economy and kinematic readiness is well recognized; this work brings that fundamental principle to aerial multirotors. We show that the limited torque of the motors and the nonlinear aerodynamic map from rotor speed to thrust naturally give rise to the novel concept of promptness-a metric akin to dynamic aerodynamic manipulability. By treating energy consumption as a competing objective and introducing a geometric fiber-bundle formulation, we turn redundancy resolution into a principled multi-objective program on affine fibers. The use of the diffeomorphic transformation linearizing the signed-quadratic propulsion model allows us to lay the foundations for a rigorous study of the interplay between these costs. Through an illustrative case study on 4-DoF allocation on the hexarotor, we reveal that this interplay is fiber-dependent and physically shaped by hardware inequalities. For unidirectional thrusters, the feasible fibers are compact, yielding interior allocations and a short Pareto arc, while torque demands break symmetry and separate the optima. Conversely, with reversible propellers, the null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes. Ultimately, rather than relying on heuristic tuning or black box algorithms to empirically improve task execution, this framework provides a foundational understanding of why and how to achieve agility through geometry-aware control allocation, offering possible guidance for vehicle design, certification metrics, and threat-aware flight operation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u80fd\u91cf\u7ecf\u6d4e\u6027\u4e0e\u8fd0\u52a8\u51c6\u5907\u6027\u7684\u6743\u8861\u539f\u7406\u5e94\u7528\u4e8e\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\uff0c\u63d0\u51fa\u4e86\"\u654f\u6377\u6027\"\u5ea6\u91cf\u6982\u5ff5\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea4\u7ef4\u675f\u6846\u67b6\u5c06\u5197\u4f59\u89e3\u6790\u8f6c\u5316\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u63a8\u8fdb\u7cfb\u7edf\u914d\u7f6e\u4e0b\u80fd\u91cf\u6548\u7387\u4e0e\u654f\u6377\u6027\u7684\u57fa\u672c\u4e0d\u517c\u5bb9\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u548c\u4eba\u4f53\u751f\u7269\u529b\u5b66\u4e2d\u80fd\u91cf\u7ecf\u6d4e\u6027\u4e0e\u8fd0\u52a8\u51c6\u5907\u6027\u7684\u6743\u8861\u539f\u7406\u5728\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u9886\u57df\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\u8c03\u53c2\u6216\u9ed1\u76d2\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u80fd\u91cf\u6d88\u8017\u4e0e\u654f\u6377\u6027\u4e4b\u95f4\u6839\u672c\u5173\u7cfb\u7684\u7406\u8bba\u7406\u89e3\uff0c\u9700\u8981\u5efa\u7acb\u51e0\u4f55\u611f\u77e5\u7684\u63a7\u5236\u5206\u914d\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\"\u654f\u6377\u6027\"\u4f5c\u4e3a\u7c7b\u4f3c\u52a8\u6001\u7a7a\u6c14\u52a8\u529b\u53ef\u64cd\u4f5c\u6027\u7684\u5ea6\u91cf\u6307\u6807\uff1b\u91c7\u7528\u51e0\u4f55\u7ea4\u7ef4\u675f\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5c06\u5197\u4f59\u89e3\u6790\u8f6c\u5316\u4e3a\u4eff\u5c04\u7ea4\u7ef4\u4e0a\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff1b\u5229\u7528\u5fae\u5206\u540c\u80da\u53d8\u6362\u7ebf\u6027\u5316\u5e26\u7b26\u53f7\u4e8c\u6b21\u63a8\u8fdb\u6a21\u578b\uff1b\u901a\u8fc7\u516d\u65cb\u7ffc4\u81ea\u7531\u5ea6\u5206\u914d\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u786c\u4ef6\u4e0d\u7b49\u5f0f\u5982\u4f55\u7269\u7406\u5851\u9020\u6210\u672c\u6743\u8861\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u80fd\u91cf\u6d88\u8017\u4e0e\u654f\u6377\u6027\u7684\u6743\u8861\u5173\u7cfb\u5177\u6709\u7ea4\u7ef4\u4f9d\u8d56\u6027\uff1a\u5355\u5411\u63a8\u8fdb\u5668\u4ea7\u751f\u7d27\u51d1\u53ef\u884c\u7ea4\u7ef4\uff0c\u5bfc\u81f4\u5185\u90e8\u5206\u914d\u548c\u77ed\u5e15\u7d2f\u6258\u5f27\uff0c\u626d\u77e9\u9700\u6c42\u7834\u574f\u5bf9\u79f0\u6027\uff1b\u53ef\u9006\u63a8\u8fdb\u5668\u5219\u901a\u8fc7\u96f6\u7a7a\u95f4\u5b9e\u73b0\u5bf9\u6297\u6027\u8f6c\u5b50\u5171\u6536\u7f29\uff0c\u5c06\u654f\u6377\u6027\u63a8\u5411\u786c\u4ef6\u6781\u9650\uff0c\u4f7f\u5f97\u6700\u4f18\u7eed\u822a\u4e0e\u654f\u6377\u6027\u5728\u8fd9\u4e9b\u673a\u5236\u4e2d\u6839\u672c\u4e0d\u76f8\u5bb9\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u63a7\u5236\u5206\u914d\u5b9e\u73b0\u654f\u6377\u6027\u7684\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\"\u4e3a\u4f55\"\u4ee5\u53ca\"\u5982\u4f55\"\u5b9e\u73b0\u654f\u6377\u6027\uff0c\u800c\u975e\u4f9d\u8d56\u7ecf\u9a8c\u8c03\u53c2\u3002\u8fd9\u4e3a\u98de\u884c\u5668\u8bbe\u8ba1\u3001\u8ba4\u8bc1\u6307\u6807\u548c\u5a01\u80c1\u611f\u77e5\u98de\u884c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u80fd\u7684\u6307\u5bfc\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u63a8\u8fdb\u7cfb\u7edf\u914d\u7f6e\u4e0b\u80fd\u91cf\u6548\u7387\u4e0e\u654f\u6377\u6027\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2602.14553", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14553", "abs": "https://arxiv.org/abs/2602.14553", "authors": ["Qinqi Lin", "Ningning Ding", "Lingjie Duan", "Jianwei Huang"], "title": "Governing AI Forgetting: Auditing for Machine Unlearning Compliance", "comment": "Under review in IEEE Transactions on Mobile Computing", "summary": "Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u5ba1\u8ba1\u673a\u5668\u9057\u5fd8\u5408\u89c4\u6027\u7684\u7ecf\u6d4e\u6846\u67b6\uff0c\u7ed3\u5408\u8ba4\u8bc1\u9057\u5fd8\u7406\u8bba\u4e0e\u76d1\u7ba1\u6267\u884c\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u6a21\u578b\u5206\u6790\u5ba1\u8ba1\u8005\u4e0e\u8fd0\u8425\u8005\u4e4b\u95f4\u7684\u7b56\u7565\u4e92\u52a8\uff0c\u53d1\u73b0\u5ba1\u8ba1\u5f3a\u5ea6\u53ef\u80fd\u968f\u5220\u9664\u8bf7\u6c42\u589e\u52a0\u800c\u964d\u4f4e\uff0c\u4e14\u672a\u62ab\u9732\u5ba1\u8ba1\u867d\u5177\u4fe1\u606f\u4f18\u52bf\u4f46\u6210\u672c\u6548\u76ca\u8f83\u4f4e\u3002", "motivation": "\u5c3d\u7ba1\u6cd5\u5f8b\u89c4\u5b9a\u4e86\u88ab\u9057\u5fd8\u6743\uff0c\u4f46AI\u8fd0\u8425\u8005\u7ecf\u5e38\u672a\u80fd\u9075\u5b88\u6570\u636e\u5220\u9664\u8bf7\u6c42\u3002\u673a\u5668\u9057\u5fd8\u63d0\u4f9b\u4e86\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u4e2a\u4eba\u6570\u636e\u5f71\u54cd\u7684\u6280\u672f\u65b9\u6848\uff0c\u4f46\u786e\u4fdd\u5408\u89c4\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u673a\u5668\u9057\u5fd8\u7684\u6280\u672f\u53ef\u884c\u6027\u4e0e\u76d1\u7ba1\u5b9e\u65bd\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u5dee\u8ddd\u3002", "method": "1) \u4f7f\u7528\u8ba4\u8bc1\u9057\u5fd8\u7684\u5047\u8bbe\u68c0\u9a8c\u89e3\u91ca\u6765\u8868\u5f81\u673a\u5668\u9057\u5fd8\u56fa\u6709\u7684\u9a8c\u8bc1\u4e0d\u786e\u5b9a\u6027\uff0c\u63a8\u5bfc\u5ba1\u8ba1\u8005\u7684\u68c0\u6d4b\u80fd\u529b\uff1b2) \u63d0\u51fa\u535a\u5f08\u8bba\u6a21\u578b\u6355\u6349\u5ba1\u8ba1\u8005\u4e0e\u8fd0\u8425\u8005\u4e4b\u95f4\u7684\u7b56\u7565\u4e92\u52a8\uff1b3) \u5c06\u590d\u6742\u7684\u53cc\u53d8\u91cf\u975e\u7ebf\u6027\u56fa\u5b9a\u70b9\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u5355\u53d8\u91cf\u8f85\u52a9\u95ee\u9898\uff0c\u5b9e\u73b0\u7cfb\u7edf\u89e3\u8026\u5e76\u5efa\u7acb\u5747\u8861\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548c\u7ed3\u6784\u7279\u6027\u3002", "result": "\u53cd\u76f4\u89c9\u5730\u53d1\u73b0\uff0c\u968f\u7740\u5220\u9664\u8bf7\u6c42\u589e\u52a0\uff0c\u5ba1\u8ba1\u8005\u53ef\u4ee5\u6700\u4f18\u5730\u964d\u4f4e\u68c0\u67e5\u5f3a\u5ea6\uff0c\u56e0\u4e3a\u8fd0\u8425\u8005\u524a\u5f31\u7684\u9057\u5fd8\u80fd\u529b\u4f7f\u4e0d\u5408\u89c4\u66f4\u5bb9\u6613\u88ab\u68c0\u6d4b\u3002\u8fd9\u4e0e\u8fd1\u671f\u4e2d\u56fd\u5ba1\u8ba1\u51cf\u5c11\u4f46\u5220\u9664\u8bf7\u6c42\u589e\u52a0\u7684\u73b0\u8c61\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u8bc1\u660e\u867d\u7136\u672a\u62ab\u9732\u5ba1\u8ba1\u4e3a\u5ba1\u8ba1\u8005\u63d0\u4f9b\u4fe1\u606f\u4f18\u52bf\uff0c\u4f46\u76f8\u5bf9\u4e8e\u62ab\u9732\u5ba1\u8ba1\u53cd\u800c\u964d\u4f4e\u4e86\u76d1\u7ba1\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7ecf\u6d4e\u6846\u67b6\u4e3a\u673a\u5668\u9057\u5fd8\u5408\u89c4\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5ba1\u8ba1\u5f3a\u5ea6\u4e0e\u5220\u9664\u8bf7\u6c42\u4e4b\u95f4\u7684\u53cd\u76f4\u89c9\u5173\u7cfb\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u5ba1\u8ba1\u7b56\u7565\u7684\u6210\u672c\u6548\u76ca\uff0c\u4e3a\u76d1\u7ba1\u5b9e\u8df5\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.14045", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14045", "abs": "https://arxiv.org/abs/2602.14045", "authors": ["Gengchen Cao", "Tianke He", "Yixuan Liu", "RAY LC"], "title": "Audience in the Loop: Viewer Feedback-Driven Content Creation in Micro-drama Production on Social Media", "comment": "26 pages, 6 figures, CHI 2026", "summary": "The popularization of social media has led to increasing consumption of narrative content in byte-sized formats. Such micro-dramas contain fast-pace action and emotional cliffs, particularly attractive to emerging Chinese markets in platforms like Douyin and Kuaishou. Content writers for micro-dramas must adapt to fast-pace, audience-directed workflows, but previous research has focused instead on examining writers'experiences of platform affordances or their perceptions of platform bias, rather than the step-by-step processes through which they actually write and iterative content. In 28 semi-structured interviews with scriptwriters and writers specialized in micro-dramas, we found that the short-turn-around workflow leads to writers taking on multiple roles simultaneously, iteratively adapting to storylines in response to real-time audience feedback in the form of comments, reposts, and memes. We identified unique narrative styles such as AI-generated micro-dramas and audience-responsive micro-dramas. This work reveals audience interaction as a new paradigm for collaborative creative processes on social media.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u793e\u4ea4\u5a92\u4f53\u5fae\u77ed\u5267\u7f16\u5267\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u53d1\u73b0\u5feb\u901f\u8fed\u4ee3\u7684\u5de5\u4f5c\u6a21\u5f0f\u4f7f\u7f16\u5267\u540c\u65f6\u627f\u62c5\u591a\u91cd\u89d2\u8272\uff0c\u5e76\u57fa\u4e8e\u5b9e\u65f6\u89c2\u4f17\u53cd\u9988\uff08\u8bc4\u8bba\u3001\u8f6c\u53d1\u3001\u8868\u60c5\u5305\uff09\u8c03\u6574\u5267\u60c5\uff0c\u63ed\u793a\u4e86\u89c2\u4f17\u4e92\u52a8\u4f5c\u4e3a\u793e\u4ea4\u5a92\u4f53\u534f\u4f5c\u521b\u4f5c\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u666e\u53ca\u5bfc\u81f4\u5b57\u8282\u5316\u53d9\u4e8b\u5185\u5bb9\u6d88\u8d39\u589e\u52a0\uff0c\u5fae\u77ed\u5267\u56e0\u5176\u5feb\u8282\u594f\u548c\u60c5\u611f\u60ac\u5ff5\u5728\u4e2d\u56fd\u65b0\u5174\u5e02\u573a\uff08\u5982\u6296\u97f3\u3001\u5feb\u624b\uff09\u5e7f\u53d7\u6b22\u8fce\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f16\u5267\u5bf9\u5e73\u53f0\u529f\u80fd\u6216\u504f\u89c1\u7684\u611f\u77e5\uff0c\u800c\u975e\u4ed6\u4eec\u5b9e\u9645\u521b\u4f5c\u548c\u8fed\u4ee3\u5185\u5bb9\u7684\u5177\u4f53\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5fae\u77ed\u5267\u7f16\u5267\u7684\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u901a\u8fc7\u5bf928\u4f4d\u5fae\u77ed\u5267\u7f16\u5267\u548c\u811a\u672c\u4f5c\u5bb6\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u7814\u7a76\u4ed6\u4eec\u4ece\u521b\u4f5c\u5230\u8fed\u4ee3\u7684\u5168\u8fc7\u7a0b\uff0c\u7279\u522b\u5173\u6ce8\u5982\u4f55\u5e94\u5bf9\u5feb\u901f\u5468\u8f6c\u7684\u5de5\u4f5c\u6d41\u7a0b\u4ee5\u53ca\u5982\u4f55\u5904\u7406\u5b9e\u65f6\u89c2\u4f17\u53cd\u9988\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5feb\u901f\u5468\u8f6c\u7684\u5de5\u4f5c\u6d41\u7a0b\u5bfc\u81f4\u7f16\u5267\u540c\u65f6\u627f\u62c5\u591a\u91cd\u89d2\u8272\uff1b2\uff09\u7f16\u5267\u6839\u636e\u5b9e\u65f6\u89c2\u4f17\u53cd\u9988\uff08\u8bc4\u8bba\u3001\u8f6c\u53d1\u3001\u8868\u60c5\u5305\uff09\u8fed\u4ee3\u8c03\u6574\u6545\u4e8b\u60c5\u8282\uff1b3\uff09\u8bc6\u522b\u51fa\u72ec\u7279\u7684\u53d9\u4e8b\u98ce\u683c\uff0c\u5982AI\u751f\u6210\u5fae\u77ed\u5267\u548c\u89c2\u4f17\u54cd\u5e94\u5f0f\u5fae\u77ed\u5267\u3002", "conclusion": "\u89c2\u4f17\u4e92\u52a8\u5df2\u6210\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e0a\u534f\u4f5c\u521b\u4f5c\u8fc7\u7a0b\u7684\u65b0\u8303\u5f0f\uff0c\u5fae\u77ed\u5267\u521b\u4f5c\u4f53\u73b0\u4e86\u521b\u4f5c\u8005\u4e0e\u89c2\u4f17\u4e4b\u95f4\u7684\u5b9e\u65f6\u4e92\u52a8\u548c\u5171\u540c\u521b\u4f5c\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u6570\u5b57\u65f6\u4ee3\u7684\u53d9\u4e8b\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.13733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13733", "abs": "https://arxiv.org/abs/2602.13733", "authors": ["Robin Schwager", "Andrea Anastasio", "Simon Hartmann", "Andreas Ronellenfitsch", "Michael Grimm", "Tim Br\u00fchl", "Tin Stribor Sohn", "Tim Dieter Eberhardt", "S\u00f6ren Hohmann"], "title": "Improving Driver Satisfaction with a Driving Function Learning from Implicit Human Feedback -- a Test Group Study", "comment": null, "summary": "During the use of advanced driver assistance systems, drivers frequently intervene into the active driving function and adjust the system's behavior to their personal wishes. These active driver-initiated takeovers contain feedback about deviations in the driving function's behavior from the drivers' personal preferences. This feedback should be utilized to optimize and personalize the driving function's behavior. In this work, the adjustment of the speed profile of a Predictive Longitudinal Driving Function (PLDF) on a pre-defined route is highlighted. An algorithm is introduced which iteratively adjusts the PLDF's speed profile by taking into account both the original speed profile of the PLDF and the driver demonstration. This approach allows for personalization in a traded control scenario during active use of the PLDF. The applicability of the proposed algorithm is tested in a driving simulator-based test group study with 43 participants. The study finds a significant increase in driver satisfaction and a significant reduction in the intervention frequency when using the proposed adaptive PLDF. Additionally, feedback by the participants was gathered to identify further optimization potentials of the proposed system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9884\u6d4b\u7eb5\u5411\u9a7e\u9a76\u529f\u80fd\u7b97\u6cd5\uff0c\u901a\u8fc7\u9a7e\u9a76\u5458\u5e72\u9884\u53cd\u9988\u6765\u4e2a\u6027\u5316\u8c03\u6574\u901f\u5ea6\u66f2\u7ebf\uff0c\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u7814\u7a76\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u5458\u6ee1\u610f\u5ea6\u5e76\u51cf\u5c11\u4e86\u5e72\u9884\u9891\u7387\u3002", "motivation": "\u9a7e\u9a76\u5458\u5728\u4f7f\u7528\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u65f6\u7ecf\u5e38\u4e3b\u52a8\u5e72\u9884\u8c03\u6574\u7cfb\u7edf\u884c\u4e3a\uff0c\u8fd9\u4e9b\u5e72\u9884\u5305\u542b\u4e86\u9a7e\u9a76\u5458\u4e2a\u4eba\u504f\u597d\u4e0e\u7cfb\u7edf\u884c\u4e3a\u4e4b\u95f4\u7684\u504f\u5dee\u4fe1\u606f\uff0c\u5e94\u8be5\u5229\u7528\u8fd9\u4e9b\u53cd\u9988\u6765\u4f18\u5316\u548c\u4e2a\u6027\u5316\u9a7e\u9a76\u529f\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u8c03\u6574\u7b97\u6cd5\uff0c\u7efc\u5408\u8003\u8651\u539f\u59cb\u9884\u6d4b\u7eb5\u5411\u9a7e\u9a76\u529f\u80fd\u7684\u901f\u5ea6\u66f2\u7ebf\u548c\u9a7e\u9a76\u5458\u6f14\u793a\u6570\u636e\uff0c\u5728\u9884\u5b9a\u4e49\u8def\u7ebf\u4e0a\u8c03\u6574\u901f\u5ea6\u66f2\u7ebf\uff0c\u5b9e\u73b0\u4ea4\u6613\u63a7\u5236\u573a\u666f\u4e0b\u7684\u4e2a\u6027\u5316\u3002", "result": "\u572843\u540d\u53c2\u4e0e\u8005\u7684\u9a7e\u9a76\u6a21\u62df\u5668\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528\u81ea\u9002\u5e94PLDF\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u5458\u6ee1\u610f\u5ea6\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5e72\u9884\u9891\u7387\u3002\u540c\u65f6\u6536\u96c6\u4e86\u53c2\u4e0e\u8005\u53cd\u9988\u4ee5\u8bc6\u522b\u8fdb\u4e00\u6b65\u4f18\u5316\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u9a7e\u9a76\u5458\u4e3b\u52a8\u5e72\u9884\u7684\u53cd\u9988\u6765\u4e2a\u6027\u5316\u8c03\u6574\u9884\u6d4b\u7eb5\u5411\u9a7e\u9a76\u529f\u80fd\u7684\u901f\u5ea6\u66f2\u7ebf\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u63d0\u9ad8\u9a7e\u9a76\u5458\u6ee1\u610f\u5ea6\u548c\u51cf\u5c11\u5e72\u9884\uff0c\u4e3a\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2602.13666", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13666", "abs": "https://arxiv.org/abs/2602.13666", "authors": ["Edward Chen", "Natalie Dullerud", "Pang Wei Koh", "Thomas Niedermayr", "Elizabeth Kidd", "Sanmi Koyejo", "Carlos Guestrin"], "title": "ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer", "comment": "Abstract accepted at Symposium on Artificial Intelligence in Learning Health Systems (SAIL) 2025", "summary": "In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.", "AI": {"tldr": "ALMo\u7cfb\u7edf\u901a\u8fc7\u76ee\u6807-\u9650\u5236\u9608\u503c\u4ea4\u4e92\u754c\u9762\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u5728\u5bab\u9888\u764cHDR\u8fd1\u8ddd\u79bb\u653e\u7597\u8ba1\u5212\u5236\u5b9a\u4e2d\u9ad8\u6548\u5e73\u8861\u80bf\u7624\u8986\u76d6\u4e0e\u5668\u5b98\u4fdd\u62a4\uff0c\u51cf\u5c11\u8ba1\u5212\u65f6\u95f4\u5e76\u63d0\u9ad8\u8ba1\u5212\u8d28\u91cf\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u4e2d\u9700\u8981\u540c\u65f6\u8ddf\u8e2a\u591a\u4e2a\u7ade\u4e89\u6027\u6307\u6807\uff08\u7406\u60f3\u76ee\u6807\u548c\u4e25\u683c\u9650\u5236\u9608\u503c\uff09\uff0c\u5728\u9ad8\u7ef4\u6743\u8861\u4e2d\u63a8\u65ad\u6700\u4f18\u60a3\u8005\u7279\u5b9a\u7b56\u7565\u5177\u6709\u8ba4\u77e5\u6311\u6218\u4e14\u5b58\u5728\u53d8\u5f02\u6027\u3002\u7279\u522b\u662f\u5728\u5bab\u9888\u764cHDR\u8fd1\u8ddd\u79bb\u653e\u7597\u4e2d\uff0c\u9700\u8981\u4e25\u683c\u7ba1\u7406\u8f90\u5c04\u70ed\u70b9\uff0c\u540c\u65f6\u5e73\u8861\u80bf\u7624\u8986\u76d6\u4e0e\u5668\u5b98\u4fdd\u62a4\u3002", "method": "\u63d0\u51faALMo\uff08\u76ee\u6807-\u9650\u5236\u5b9a\u4e49\u7684\u591a\u76ee\u6807\u7cfb\u7edf\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u4f18\u5316\u6846\u67b6\u6700\u5c0f\u5316\u624b\u52a8\u8f93\u5165\uff0c\u5b9e\u73b0\u53c2\u6570\u81ea\u52a8\u8bbe\u7f6e\uff0c\u5e76\u5141\u8bb8\u4e34\u5e8a\u533b\u751f\u901a\u8fc7\u76f4\u63a5\u64cd\u4f5c\u76f4\u89c2\u7684\u76ee\u6807\u548c\u9650\u5236\u503c\u6765\u5bfc\u822a\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u572825\u4e2a\u4e34\u5e8a\u75c5\u4f8b\u7684\u56de\u987e\u6027\u8bc4\u4f30\u4e2d\uff0cALMo\u751f\u6210\u7684\u8ba1\u5212\u8d28\u91cf\u59cb\u7ec8\u8fbe\u5230\u6216\u8d85\u8fc7\u624b\u52a8\u8ba1\u5212\uff0c65%\u7684\u75c5\u4f8b\u663e\u793a\u5242\u91cf\u5b66\u6539\u8fdb\u3002\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0c\u5e73\u5747\u8ba1\u5212\u65f6\u95f4\u51cf\u5c11\u81f3\u7ea617\u5206\u949f\uff08\u4f20\u7edf\u65b9\u6cd5\u4e3a30-60\u5206\u949f\uff09\u3002", "conclusion": "ALMo\u5728\u8fd1\u8ddd\u79bb\u653e\u7597\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u7b80\u5316\u591a\u6807\u51c6\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u4ea4\u4e92\u8fc7\u7a0b\u3002"}}
{"id": "2602.13866", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13866", "abs": "https://arxiv.org/abs/2602.13866", "authors": ["Jinwoo Park", "Harish Ravichandar", "Seth Hutchinson"], "title": "Modeling and Optimizing the Provisioning of Exhaustible Capabilities for Simultaneous Task Allocation and Scheduling", "comment": "Accepted at AAMAS 2026", "summary": "Deploying heterogeneous robot teams to accomplish multiple tasks over extended time horizons presents significant computational challenges for task allocation and planning. In this paper, we present a comprehensive, time-extended, offline heterogeneous multi-robot task allocation framework, TRAITS, which we believe to be the first that can cope with the provisioning of exhaustible traits under battery and temporal constraints. Specifically, we introduce a nonlinear programming-based trait distribution module that can optimize the trait-provisioning rate of coalitions to yield feasible and time-efficient solutions. TRAITS provides a more accurate feasibility assessment and estimation of task execution times and makespan by leveraging trait-provisioning rates while optimizing battery consumption -- an advantage that state-of-the-art frameworks lack. We evaluate TRAITS against two state-of-the-art frameworks, with results demonstrating its advantage in satisfying complex trait and battery requirements while remaining computationally tractable.", "AI": {"tldr": "TRAITS\u662f\u4e00\u4e2a\u79bb\u7ebf\u5f02\u6784\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u53ef\u8017\u5c3d\u7279\u6027\u5728\u7535\u6c60\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u89c4\u5212\u4f18\u5316\u7279\u6027\u4f9b\u7ed9\u7387\uff0c\u63d0\u9ad8\u53ef\u884c\u6027\u8bc4\u4f30\u548c\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u957f\u65f6\u95f4\u8303\u56f4\u5185\u5b8c\u6210\u591a\u4e2a\u4efb\u52a1\u65f6\uff0c\u4efb\u52a1\u5206\u914d\u548c\u89c4\u5212\u9762\u4e34\u663e\u8457\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u7279\u522b\u662f\u9700\u8981\u5904\u7406\u53ef\u8017\u5c3d\u7279\u6027\u5728\u7535\u6c60\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u5206\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51faTRAITS\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u975e\u7ebf\u6027\u89c4\u5212\u7684\u7279\u6027\u5206\u5e03\u6a21\u5757\uff0c\u4f18\u5316\u8054\u76df\u7684\u7279\u6027\u4f9b\u7ed9\u7387\uff0c\u540c\u65f6\u8003\u8651\u7535\u6c60\u6d88\u8017\u548c\u65f6\u95f4\u7ea6\u675f\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u53ef\u884c\u6027\u8bc4\u4f30\u548c\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u4f30\u8ba1\u3002", "result": "\u4e0e\u4e24\u79cd\u6700\u5148\u8fdb\u6846\u67b6\u76f8\u6bd4\uff0cTRAITS\u5728\u6ee1\u8db3\u590d\u6742\u7279\u6027\u548c\u7535\u6c60\u8981\u6c42\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "TRAITS\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u53ef\u8017\u5c3d\u7279\u6027\u5728\u7535\u6c60\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u5206\u914d\u7684\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7279\u6027\u4f9b\u7ed9\u7387\u63d0\u9ad8\u4e86\u4efb\u52a1\u5206\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.14740", "categories": ["cs.AI", "cs.CY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.14740", "abs": "https://arxiv.org/abs/2602.14740", "authors": ["Kenneth Payne"], "title": "AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises", "comment": "45 pages, 6 figures, 27 tables", "summary": "Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.\n  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.\n  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.\n  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.", "AI": {"tldr": "\u524d\u6cbfAI\u6a21\u578b\u5728\u6838\u5371\u673a\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u590d\u6742\u7684\u6218\u7565\u884c\u4e3a\uff0c\u5305\u62ec\u6b3a\u9a97\u3001\u5fc3\u667a\u7406\u8bba\u548c\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u90e8\u5206\u6218\u7565\u7406\u8bba\u4f46\u4e5f\u6311\u6218\u4e86\u6838\u7981\u5fcc\u7b49\u6838\u5fc3\u5047\u8bbe\u3002", "motivation": "\u7814\u7a76\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6218\u7565\u7ade\u4e89\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u6838\u5371\u673a\u60c5\u5883\u4e0b\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u4e3a\u56fd\u5bb6\u5b89\u5168\u4e13\u4e1a\u4eba\u5458\u548c\u6218\u7565\u5206\u6790\u63d0\u4f9bAI\u6a21\u62df\u5de5\u5177\uff0c\u540c\u65f6\u7406\u89e3AI\u5982\u4f55\u6a21\u4eff\u6216\u504f\u79bb\u4eba\u7c7b\u6218\u7565\u601d\u7ef4\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-5.2\u3001Claude Sonnet 4\u3001Gemini 3 Flash\uff09\u5728\u6838\u5371\u673a\u6a21\u62df\u4e2d\u626e\u6f14\u5bf9\u7acb\u9886\u5bfc\u4eba\u89d2\u8272\uff0c\u89c2\u5bdf\u5176\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u63a8\u7406\u548c\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "AI\u6a21\u578b\u8868\u73b0\u51fa\u6b3a\u9a97\u3001\u5fc3\u667a\u7406\u8bba\u548c\u5143\u8ba4\u77e5\u80fd\u529b\uff1b\u9a8c\u8bc1\u4e86Schelling\u7684\u627f\u8bfa\u7406\u8bba\u3001Kahn\u7684\u5347\u7ea7\u6846\u67b6\u548cJervis\u7684\u8bef\u89e3\u7406\u8bba\uff1b\u4f46\u53d1\u73b0\u6838\u7981\u5fcc\u672a\u80fd\u963b\u6b62\u6838\u5347\u7ea7\uff0c\u6218\u7565\u6838\u653b\u51fb\u867d\u7f55\u89c1\u4f46\u786e\u5b9e\u53d1\u751f\uff0c\u5a01\u80c1\u66f4\u591a\u5f15\u53d1\u5bf9\u6297\u800c\u975e\u670d\u4ece\uff0c\u9ad8\u4e92\u4fe1\u53cd\u800c\u52a0\u901f\u51b2\u7a81\uff0c\u6a21\u578b\u4ece\u672a\u9009\u62e9\u59a5\u534f\u6216\u64a4\u9000\u3002", "conclusion": "AI\u6a21\u62df\u662f\u5f3a\u5927\u7684\u6218\u7565\u5206\u6790\u5de5\u5177\uff0c\u4f46\u9700\u8981\u6839\u636e\u4eba\u7c7b\u63a8\u7406\u6a21\u5f0f\u8fdb\u884c\u9002\u5f53\u6821\u51c6\u3002\u7406\u89e3\u524d\u6cbf\u6a21\u578b\u5982\u4f55\u6a21\u4eff\u6216\u4e0d\u6a21\u4eff\u4eba\u7c7b\u6218\u7565\u903b\u8f91\u5bf9\u4e8eAI\u65e5\u76ca\u5f71\u54cd\u6218\u7565\u7ed3\u679c\u7684\u4e16\u754c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.14102", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14102", "abs": "https://arxiv.org/abs/2602.14102", "authors": ["Guozheng Li", "Ao Wang", "Shaoxiang Wang", "Yu Zhang", "Pengcheng Cao", "Yang Bai", "Chi Harold Liu"], "title": "DALL: Data Labeling via Data Programming and Active Learning Enhanced by Large Language Models", "comment": null, "summary": "Deep learning models for natural language processing rely heavily on high-quality labeled datasets. However, existing labeling approaches often struggle to balance label quality with labeling cost. To address this challenge, we propose DALL, a text labeling framework that integrates data programming, active learning, and large language models. DALL introduces a structured specification that allows users and large language models to define labeling functions via configuration, rather than code. Active learning identifies informative instances for review, and the large language model analyzes these instances to help users correct labels and to refine or suggest labeling functions. We implement DALL as an interactive labeling system for text labeling tasks. Comparative, ablation, and usability studies demonstrate DALL's efficiency, the effectiveness of its modules, and its usability.", "AI": {"tldr": "DALL\u662f\u4e00\u4e2a\u6587\u672c\u6807\u6ce8\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6570\u636e\u7f16\u7a0b\u3001\u4e3b\u52a8\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u914d\u7f6e\u800c\u975e\u4ee3\u7801\u5b9a\u4e49\u6807\u6ce8\u51fd\u6570\uff0c\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6807\u6ce8\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6807\u6ce8\u8d28\u91cf\u548c\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDALL\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u636e\u7f16\u7a0b\u3001\u4e3b\u52a8\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u8303\u8ba9\u7528\u6237\u548cLLM\u901a\u8fc7\u914d\u7f6e\u5b9a\u4e49\u6807\u6ce8\u51fd\u6570\uff0c\u4e3b\u52a8\u5b66\u4e60\u8bc6\u522b\u9700\u8981\u5ba1\u6838\u7684\u5b9e\u4f8b\uff0cLLM\u5206\u6790\u8fd9\u4e9b\u5b9e\u4f8b\u5e2e\u52a9\u7528\u6237\u4fee\u6b63\u6807\u7b7e\u5e76\u4f18\u5316\u6807\u6ce8\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u7814\u7a76\u3001\u6d88\u878d\u5b9e\u9a8c\u548c\u53ef\u7528\u6027\u7814\u7a76\uff0c\u8bc1\u660e\u4e86DALL\u7684\u9ad8\u6548\u6027\u3001\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\u4ee5\u53ca\u826f\u597d\u7684\u53ef\u7528\u6027\u3002", "conclusion": "DALL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u6807\u6ce8\u4e2d\u8d28\u91cf\u4e0e\u6210\u672c\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u6807\u6ce8\u7cfb\u7edf\u3002"}}
{"id": "2602.13670", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13670", "abs": "https://arxiv.org/abs/2602.13670", "authors": ["Binyu Zhao", "Wei Zhang", "Xingrui Yu", "Zhaonian Zou", "Ivor Tsang"], "title": "Advancing Analytic Class-Incremental Learning through Vision-Language Calibration", "comment": "14 pages, 11 figures", "summary": "Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \\textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA", "AI": {"tldr": "VILA\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6821\u51c6\u7b56\u7565\u6539\u8fdb\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u5206\u6790\u5b66\u4e60\u6548\u7387\u7684\u540c\u65f6\u514b\u670d\u5176\u8106\u5f31\u6027\u3002", "motivation": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u9762\u4e34\u9ad8\u6548\u9002\u5e94\u4e0e\u957f\u671f\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5206\u6790\u5b66\u4e60\u867d\u7136\u80fd\u5b9e\u73b0\u5feb\u901f\u9012\u5f52\u95ed\u5f0f\u66f4\u65b0\uff0c\u4f46\u5e38\u53d7\u7d2f\u79ef\u8bef\u5dee\u548c\u7279\u5f81\u4e0d\u517c\u5bb9\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u8868\u793a\u521a\u6027\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faVILA\u53cc\u5206\u652f\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u7ea7\u89c6\u89c9-\u8bed\u8a00\u6821\u51c6\u7b56\u7565\uff1a\u5728\u7279\u5f81\u5c42\u9762\u901a\u8fc7\u51e0\u4f55\u6821\u51c6\u5c06\u53ef\u5851\u6027\u4efb\u52a1\u9002\u5e94\u7279\u5f81\u4e0e\u51bb\u7ed3\u7684\u901a\u7528\u8bed\u4e49\u951a\u70b9\u878d\u5408\uff1b\u5728\u51b3\u7b56\u5c42\u9762\u5229\u7528\u8de8\u6a21\u6001\u5148\u9a8c\u7ea0\u6b63\u9884\u6d4b\u504f\u5dee\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVILA\u59cb\u7ec8\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u548c\u957f\u5e8f\u5217\u573a\u666f\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5206\u6790\u5b66\u4e60\u7684\u6781\u7aef\u6548\u7387\u3002", "conclusion": "VILA\u6846\u67b6\u5c06\u9ad8\u4fdd\u771f\u9884\u6d4b\u4e0e\u5206\u6790\u5b66\u4e60\u7684\u7b80\u6d01\u6027\u76f8\u534f\u8c03\uff0c\u514b\u670d\u4e86\u5206\u6790\u5b66\u4e60\u7684\u56fa\u6709\u8106\u5f31\u6027\uff0c\u4e3a\u7c7b\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14183", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14183", "abs": "https://arxiv.org/abs/2602.14183", "authors": ["Le Lin", "Zihao Zhu", "Rainbow Tin Hung Ho", "Jing Liao", "Yuhan Luo"], "title": "Exploring a Multimodal Chatbot as a Facilitator in Therapeutic Art Activity", "comment": null, "summary": "Therapeutic art activities, such as expressive drawing and painting, require the synergy between creative visual production and interactive dialogue. Recent advancements in Multimodal Large Language Models (MLLMs) have expanded the capacity of computing systems to interpret both textual and visual data, offering a new frontier for AI-mediated therapeutic support. This work-in-progress paper introduces an MLLM-powered chatbot that analyzes visual creation in real-time while engaging the creator in reflective conversations. We conducted an evaluation with five experts in art therapy and related fields, which demonstrated the chatbot's potential to facilitate therapeutic engagement, and highlighted several areas for future development, including entryways and risk management, bespoke alignment of user profile and therapeutic style, balancing conversational depth and width, and enriching visual interactivity. These themes provide a design roadmap for designing the future AI-mediated creative expression tools.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u5b9e\u65f6\u5206\u6790\u89c6\u89c9\u521b\u4f5c\u5e76\u5f15\u5bfc\u53cd\u601d\u5bf9\u8bdd\uff0c\u7528\u4e8e\u827a\u672f\u6cbb\u7597\u652f\u6301", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e3aAI\u4ecb\u5bfc\u7684\u6cbb\u7597\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u89c6\u89c9\u521b\u4f5c\u4e0e\u4e92\u52a8\u5bf9\u8bdd\u534f\u540c\u7684\u827a\u672f\u6cbb\u7597\u6d3b\u52a8\u4e2d", "method": "\u5f00\u53d1\u57fa\u4e8eMLLM\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u80fd\u591f\u5b9e\u65f6\u5206\u6790\u89c6\u89c9\u521b\u4f5c\u5e76\u53c2\u4e0e\u53cd\u601d\u5bf9\u8bdd\uff0c\u5e76\u9080\u8bf75\u4f4d\u827a\u672f\u6cbb\u7597\u53ca\u76f8\u5173\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u8bc4\u4f30", "result": "\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u804a\u5929\u673a\u5668\u4eba\u5177\u6709\u4fc3\u8fdb\u6cbb\u7597\u53c2\u4e0e\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u591a\u4e2a\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\uff1a\u5165\u53e3\u548c\u98ce\u9669\u7ba1\u7406\u3001\u7528\u6237\u753b\u50cf\u4e0e\u6cbb\u7597\u98ce\u683c\u7684\u5b9a\u5236\u5316\u5bf9\u9f50\u3001\u5bf9\u8bdd\u6df1\u5ea6\u4e0e\u5e7f\u5ea6\u7684\u5e73\u8861\u3001\u89c6\u89c9\u4ea4\u4e92\u7684\u4e30\u5bcc\u5316", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bbe\u8ba1\u672a\u6765\u7684AI\u4ecb\u5bfc\u521b\u9020\u6027\u8868\u8fbe\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u8def\u7ebf\u56fe\uff0c\u9700\u8981\u5728\u5b89\u5168\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u8fdb\u4e00\u6b65\u4f18\u5316"}}
{"id": "2602.13684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13684", "abs": "https://arxiv.org/abs/2602.13684", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling", "comment": null, "summary": "Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $\u0398(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \\emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\\varepsilon$-coresets of optimal size $\\tilde{O}(n/\\varepsilon^2)$; that at most $\\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\\overline\u0393_w$) once $\\tilde\u0398(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u76f8\u5173\u805a\u7c7b\u95ee\u9898\u7684\u7a00\u758f\u5316-\u8fd1\u4f3c\u6743\u8861\uff0c\u63a2\u7d22\u4e86\u5728\u4fdd\u6301LP\u8fd1\u4f3c\u4fdd\u8bc1\u7684\u524d\u63d0\u4e0b\u9700\u8981\u591a\u5c11\u8fb9\u4fe1\u606f\u3002\u7814\u7a76\u63ed\u793a\u4e86\u4f2a\u5ea6\u91cf\u4e0e\u4e00\u822c\u52a0\u6743\u5b9e\u4f8b\u4e4b\u95f4\u7684\u7ed3\u6784\u4e8c\u5206\u6027\uff0c\u63d0\u51fa\u4e86\u6838\u5fc3\u96c6\u3001\u5207\u5272\u5e73\u9762\u6c42\u89e3\u5668\u548c\u7a00\u758f\u5316LP-PIVOT\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u4fe1\u606f\u9700\u6c42\u7684\u4e0b\u754c\u3002", "motivation": "\u76f8\u5173\u805a\u7c7b\uff08CC\uff09\u4f5c\u4e3a\u57fa\u7840\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u539f\u8bed\uff0c\u5176\u6700\u5f3a\u7684LP\u8fd1\u4f3c\u4fdd\u8bc1\u9700\u8981\u0398(n\u00b3)\u4e09\u89d2\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76CC\u7684\u7a00\u758f\u5316-\u8fd1\u4f3c\u6743\u8861\uff0c\u63a2\u7d22\u5728\u4fdd\u6301LP\u8fd1\u4f3c\u4fdd\u8bc1\u7684\u524d\u63d0\u4e0b\u9700\u8981\u591a\u5c11\u8fb9\u4fe1\u606f\u3002", "method": "1. \u7406\u8bba\u5206\u6790\uff1a\u8bc1\u660e\u805a\u7c7b\u5206\u6b67\u7c7b\u7684VC\u7ef4\u5ea6\u4e3an-1\uff0c\u5f97\u5230\u6700\u4f18\u5927\u5c0f\u7684\u52a0\u6027\u03b5-\u6838\u5fc3\u96c6\uff1b2. \u7b97\u6cd5\u8bbe\u8ba1\uff1a\u5f00\u53d1\u7cbe\u786e\u5207\u5272\u5e73\u9762\u6c42\u89e3\u5668\uff0c\u5229\u7528\u6700\u591aC(n,2)\u4e2a\u6d3b\u8dc3\u4e09\u89d2\u4e0d\u7b49\u5f0f\uff1b3. \u63d0\u51fa\u7a00\u758f\u5316LP-PIVOT\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e09\u89d2\u4e0d\u7b49\u5f0f\u586b\u5145\u7f3a\u5931\u7684LP\u8fb9\u9645\uff1b4. \u4f7f\u7528Yao\u6781\u5c0f\u6781\u5927\u539f\u7406\u8bc1\u660e\u4e0b\u754c\u3002", "result": "1. \u83b7\u5f97\u6700\u4f18\u5927\u5c0f\u00d5(n/\u03b5\u00b2)\u7684\u52a0\u6027\u03b5-\u6838\u5fc3\u96c6\uff1b2. \u5b9e\u73b0\u7cbe\u786e\u5207\u5272\u5e73\u9762\u6c42\u89e3\uff1b3. \u7a00\u758f\u5316LP-PIVOT\u7b97\u6cd5\u5728\u89c2\u5bdf\u00d5(n^{3/2})\u6761\u8fb9\u65f6\u8fbe\u523010/3\u8fd1\u4f3c\u6bd4\uff08\u52a0\u4e0a\u7531\u53ef\u8ba1\u7b97\u7edf\u8ba1\u91cf\u0393_w\u63a7\u5236\u7684\u52a0\u6027\u9879\uff09\uff0c\u4e14\u8be5\u9608\u503c\u662f\u6700\u4f18\u7684\uff1b4. \u8bc1\u660e\u5bf9\u4e8e\u975e\u4f2a\u5ea6\u91cf\u5b9e\u4f8b\uff0c\u4efb\u4f55\u89c2\u5bdfo(n)\u6761\u968f\u673a\u8fb9\u7684\u7b97\u6cd5\u90fd\u4f1a\u6709\u65e0\u754c\u8fd1\u4f3c\u6bd4\u3002", "conclusion": "\u76f8\u5173\u805a\u7c7b\u95ee\u9898\u7684\u7a00\u758f\u5316-\u8fd1\u4f3c\u6743\u8861\u5b58\u5728\u7ed3\u6784\u4e8c\u5206\u6027\uff1a\u4f2a\u5ea6\u91cf\u5b9e\u4f8b\u53ef\u4ee5\u901a\u8fc7\u7a00\u758f\u5316\u5b9e\u73b0\u9ad8\u6548\u8fd1\u4f3c\uff0c\u800c\u4e00\u822c\u52a0\u6743\u5b9e\u4f8b\u5219\u9700\u8981\u66f4\u591a\u8fb9\u4fe1\u606f\u3002\u4f2a\u5ea6\u91cf\u6761\u4ef6\u4e0d\u4ec5\u5f71\u54cd\u8ba1\u7b97\u53ef\u5904\u7406\u6027\uff0c\u4e5f\u51b3\u5b9a\u4e86CC\u5bf9\u4e0d\u5b8c\u5168\u4fe1\u606f\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u76f8\u5173\u805a\u7c7b\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2602.14301", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14301", "abs": "https://arxiv.org/abs/2602.14301", "authors": ["Songyuan Li", "Jia Hu", "Ahmed M. Abdelmoniem", "Geyong Min", "Haojun Huang", "Jiwei Huang"], "title": "DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices", "comment": "Index Terms: Large language models, Mixture-of-experts, Federated knowledge distillation, Edge device heterogeneity", "summary": "Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.", "AI": {"tldr": "DeepFusion\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8054\u90a6MoE\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u90a6\u77e5\u8bc6\u84b8\u998f\u878d\u5408\u5f02\u6784\u8bbe\u5907\u4e0a\u7684LLM\u77e5\u8bc6\uff0c\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u65e0\u6cd5\u627f\u8f7d\u5927\u578bMoE\u6a21\u578b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "MoE-based LLMs\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\uff0c\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5229\u7528\u79c1\u6709\u6570\u636e\u4f46\u4f20\u7edf\u65b9\u6cd5\u8981\u6c42\u8bbe\u5907\u672c\u5730\u627f\u8f7d\u5927\u578bMoE\u6a21\u578b\uff0c\u8fd9\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0d\u73b0\u5b9e\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u9002\u5e94\u8bbe\u5907\u8d44\u6e90\u9650\u5236\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u8bbe\u5907\u72ec\u7acb\u914d\u7f6e\u548c\u8bad\u7ec3\u9002\u5408\u81ea\u8eab\u9700\u6c42\u548c\u786c\u4ef6\u9650\u5236\u7684\u672c\u5730LLM\uff1b2) \u63d0\u51faView-Aligned Attention\u6a21\u5757\uff0c\u6574\u5408\u5168\u5c40MoE\u6a21\u578b\u7684\u591a\u9636\u6bb5\u7279\u5f81\u8868\u793a\uff0c\u6784\u5efa\u4e0e\u672c\u5730LLM\u5bf9\u9f50\u7684\u9884\u6d4b\u89c6\u89d2\uff1b3) \u901a\u8fc7\u8054\u90a6\u77e5\u8bc6\u84b8\u998f\u878d\u5408\u5f02\u6784\u8bbe\u5907\u77e5\u8bc6\uff0c\u89e3\u51b3\u6a21\u578b\u67b6\u6784\u548c\u9884\u6d4b\u884c\u4e3a\u5f02\u8d28\u6027\u5bfc\u81f4\u7684\u89c6\u89d2\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u884c\u4e1a\u7ea7MoE\u6a21\u578b\uff08Qwen-MoE\u548cDeepSeek-MoE\uff09\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u533b\u7597\u548c\u91d1\u878d\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aDeepFusion\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0fMoE\u8bad\u7ec3\uff1b\u76f8\u6bd4\u5173\u952e\u8054\u90a6MoE\u57fa\u7ebf\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe71%\uff0ctoken\u56f0\u60d1\u5ea6\u63d0\u5347\u9ad8\u8fbe5.28%\u3002", "conclusion": "DeepFusion\u662f\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u8054\u90a6MoE\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89c6\u89d2\u5bf9\u9f50\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8bbe\u5907\u95f4\u7684\u77e5\u8bc6\u878d\u5408\u95ee\u9898\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684MoE\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2602.14948", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.14948", "abs": "https://arxiv.org/abs/2602.14948", "authors": ["Balram Kandoria", "Aryaman Singh Samyal"], "title": "Kalman Filtering Based Flight Management System Modeling for AAM Aircraft", "comment": null, "summary": "Advanced Aerial Mobility (AAM) operations require strategic flight planning services that predict both spatial and temporal uncertainties to safely validate flight plans against hazards such as weather cells, restricted airspaces, and CNS disruption areas. Current uncertainty estimation methods for AAM vehicles rely on conservative linear models due to limited real-world performance data. This paper presents a novel Kalman Filter-based uncertainty propagation method that models AAM Flight Management System (FMS) architectures through sigmoid-blended measurement noise covariance. Unlike existing approaches with fixed uncertainty thresholds, our method continuously adapts the filter's measurement trust based on progress toward waypoints, enabling FMS correction behavior to emerge naturally. The approach scales proportionally with control inputs and is tunable to match specific aircraft characteristics or route conditions. We validate the method using real ADS-B data from general aviation aircraft divided into training and verification sets. Uncertainty propagation parameters were tuned on the training set, achieving 76% accuracy in predicting arrival times when compared against the verification dataset, demonstrating the method's effectiveness for strategic flight plan validation in AAM operations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\uff0c\u901a\u8fc7sigmoid\u6df7\u5408\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u5efa\u6a21AAM\u98de\u884c\u7ba1\u7406\u7cfb\u7edf\u67b6\u6784\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6d4b\u91cf\u4fe1\u4efb\u5ea6\uff0c\u5728AAM\u6218\u7565\u98de\u884c\u89c4\u5212\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "AAM\u8fd0\u8425\u9700\u8981\u9884\u6d4b\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\u7684\u6218\u7565\u98de\u884c\u89c4\u5212\u670d\u52a1\uff0c\u4ee5\u5b89\u5168\u9a8c\u8bc1\u98de\u884c\u8ba1\u5212\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u771f\u5b9e\u6027\u80fd\u6570\u636e\u800c\u4f9d\u8d56\u4fdd\u5b88\u7ebf\u6027\u6a21\u578b\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\uff0c\u901a\u8fc7sigmoid\u6df7\u5408\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u5efa\u6a21AAM\u98de\u884c\u7ba1\u7406\u7cfb\u7edf\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u822a\u70b9\u8fdb\u5ea6\u8fde\u7eed\u8c03\u6574\u6ee4\u6ce2\u5668\u7684\u6d4b\u91cf\u4fe1\u4efb\u5ea6\uff0c\u4f7fFMS\u6821\u6b63\u884c\u4e3a\u81ea\u7136\u6d8c\u73b0\uff0c\u53ef\u6269\u5c55\u4e14\u53ef\u8c03\u4ee5\u9002\u5e94\u7279\u5b9a\u98de\u673a\u7279\u6027\u6216\u822a\u7ebf\u6761\u4ef6\u3002", "result": "\u4f7f\u7528\u901a\u7528\u822a\u7a7a\u98de\u673a\u7684\u771f\u5b9eADS-B\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u3002\u5728\u8bad\u7ec3\u96c6\u4e0a\u8c03\u6574\u53c2\u6570\u540e\uff0c\u5728\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u9884\u6d4b\u5230\u8fbe\u65f6\u95f4\u7684\u51c6\u786e\u7387\u8fbe\u523076%\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728AAM\u8fd0\u8425\u4e2d\u6218\u7565\u98de\u884c\u8ba1\u5212\u9a8c\u8bc1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAAM\u6218\u7565\u98de\u884c\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u5de5\u5177\uff0c\u76f8\u6bd4\u73b0\u6709\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5\u5177\u6709\u81ea\u9002\u5e94\u4f18\u52bf\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6a21\u62df\u98de\u884c\u7ba1\u7406\u7cfb\u7edf\u884c\u4e3a\uff0c\u652f\u6301\u66f4\u5b89\u5168\u7684\u98de\u884c\u8ba1\u5212\u9a8c\u8bc1\u3002"}}
{"id": "2602.14232", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14232", "abs": "https://arxiv.org/abs/2602.14232", "authors": ["Marianne Bossema", "Rob Saunders", "Vlad Glaveanu", "Somaya Ben Allouch"], "title": "Designing a Rashomon Machine: Pluri-perspectivism and XAI for Creativity Support", "comment": null, "summary": "While intelligent technologies offer unique opportunities for creativity support, there are fundamental challenges in designing human-centered co-creative systems. Explainable AI (XAI) can contribute when shifting its traditional role from justification (explaining decisions) to exploration (explaining possibilities). Contextual understanding is essential for supporting embodied creativity. Generative Artificial Intelligence (AI) models are fundamentally limited, however, by their reliance on disembodied data. We propose Pluri-perspectivism as a framework for XAI, to bridge the epistemological gap between human and machine, and promote creative exploration. It is a pragmatic, action-oriented solution to guide the system, repurposing XAI methods such as the Rashomon Technique. This facilitates exploring a spectrum of creative possibilities, and the exchange of 'perspectives' between human and machine. Using Pluri-perspectivism as a framework for XAI, we can reintroduce productive friction and support human agency in human-machine creative collaborations.", "AI": {"tldr": "\u63d0\u51faPluri-perspectivism\u4f5c\u4e3aXAI\u6846\u67b6\uff0c\u5c06AI\u89e3\u91ca\u4ece\u51b3\u7b56\u8fa9\u62a4\u8f6c\u5411\u53ef\u80fd\u6027\u63a2\u7d22\uff0c\u652f\u6301\u4eba\u673a\u5171\u521b\u4e2d\u7684\u521b\u9020\u6027\u63a2\u7d22\u548c\u4eba\u7c7b\u4e3b\u4f53\u6027", "motivation": "\u667a\u80fd\u6280\u672f\u4e3a\u521b\u9020\u529b\u652f\u6301\u63d0\u4f9b\u72ec\u7279\u673a\u4f1a\uff0c\u4f46\u8bbe\u8ba1\u4ee5\u4eba\u4e3a\u672c\u7684\u5171\u521b\u7cfb\u7edf\u9762\u4e34\u6839\u672c\u6311\u6218\u3002\u4f20\u7edf\u53ef\u89e3\u91caAI(XAI)\u4e3b\u8981\u5173\u6ce8\u51b3\u7b56\u89e3\u91ca\uff0c\u9700\u8981\u8f6c\u5411\u652f\u6301\u521b\u9020\u6027\u63a2\u7d22\u3002\u751f\u6210\u5f0fAI\u6a21\u578b\u4f9d\u8d56\u8131\u79bb\u5b9e\u4f53\u7684\u6570\u636e\u5b58\u5728\u6839\u672c\u5c40\u9650\uff0c\u9700\u8981\u5f25\u5408\u4eba\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\u8ba4\u8bc6\u8bba\u5dee\u8ddd\u3002", "method": "\u63d0\u51faPluri-perspectivism\u4f5c\u4e3aXAI\u6846\u67b6\uff0c\u91c7\u7528\u5b9e\u7528\u4e3b\u4e49\u3001\u884c\u52a8\u5bfc\u5411\u7684\u65b9\u6cd5\uff0c\u91cd\u65b0\u5229\u7528Rashomon\u6280\u672f\u7b49XAI\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u4fc3\u8fdb\u63a2\u7d22\u521b\u9020\u6027\u53ef\u80fd\u6027\u8c31\u7cfb\uff0c\u652f\u6301\u4eba\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\"\u89c6\u89d2\"\u4ea4\u6362\uff0c\u91cd\u65b0\u5f15\u5165\u751f\u4ea7\u6027\u6469\u64e6\u5e76\u652f\u6301\u4eba\u7c7b\u4e3b\u4f53\u6027\u3002", "result": "Pluri-perspectivism\u6846\u67b6\u80fd\u591f\u5f25\u5408\u4eba\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\u8ba4\u8bc6\u8bba\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u521b\u9020\u6027\u63a2\u7d22\uff0c\u652f\u6301\u4eba\u673a\u5171\u521b\u4e2d\u7684\u4eba\u7c7b\u4e3b\u4f53\u6027\u3002\u901a\u8fc7\u91cd\u65b0\u5229\u7528XAI\u65b9\u6cd5\u5982Rashomon\u6280\u672f\uff0c\u53ef\u4ee5\u63a2\u7d22\u521b\u9020\u6027\u53ef\u80fd\u6027\u8c31\u7cfb\u5e76\u4fc3\u8fdb\u89c6\u89d2\u4ea4\u6362\u3002", "conclusion": "Pluri-perspectivism\u4f5c\u4e3aXAI\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u4eba\u673a\u521b\u9020\u6027\u534f\u4f5c\uff0c\u5c06AI\u89e3\u91ca\u4ece\u51b3\u7b56\u8fa9\u62a4\u8f6c\u5411\u53ef\u80fd\u6027\u63a2\u7d22\uff0c\u91cd\u65b0\u5f15\u5165\u751f\u4ea7\u6027\u6469\u64e6\u5e76\u589e\u5f3a\u4eba\u7c7b\u5728\u5171\u521b\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u4f53\u6027\u3002"}}
{"id": "2602.13762", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13762", "abs": "https://arxiv.org/abs/2602.13762", "authors": ["Amr Afifi", "Ahmad Gazar", "Javier Alonso-Mora", "Paolo Robuffo Giordano", "Antonio Franchi"], "title": "Impact-Robust Posture Optimization for Aerial Manipulation", "comment": null, "summary": "We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u8fd0\u52a8\u5197\u4f59\u626d\u77e9\u63a7\u5236\u673a\u5668\u4eba\u59ff\u6001\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u51b2\u51fb\u524d\u540e\u901f\u5ea6\u53d8\u5316\u6765\u51cf\u5c11\u51b2\u51fb\u65f6\u7684\u72b6\u6001\u548c\u8f93\u5165\u5c16\u5cf0\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8fd0\u52a8\u5197\u4f59\u626d\u77e9\u63a7\u5236\u673a\u5668\u4eba\u5728\u6267\u884c\u63a5\u89e6\u4efb\u52a1\u65f6\u5bb9\u6613\u53d7\u5230\u51b2\u51fb\u5f71\u54cd\uff0c\u5bfc\u81f4\u72b6\u6001\u548c\u8f93\u5165\u547d\u4ee4\u51fa\u73b0\u5c16\u5cf0\uff0c\u53ef\u80fd\u5f15\u53d1\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u51b2\u51fb\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e3b\u52a8\u4f18\u5316\u59ff\u6001\u4ee5\u51cf\u5c11\u51b2\u51fb\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u521a\u6027\u51b2\u51fb\u6a21\u578b\u5efa\u7acb\u914d\u7f6e\u4f9d\u8d56\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u91cf\u5316\u51b2\u51fb\u524d\u540e\u901f\u5ea6\u53d8\u5316\u3002\u5c06\u5bfb\u627e\u51b2\u51fb\u9c81\u68d2\u59ff\u6001\u95ee\u9898\u5efa\u6a21\u4e3a\u6700\u5c0f\u6700\u5927\u5316\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91cd\u65b0\u8868\u8ff0\u4e3a\u57fa\u4e8e\u68af\u5ea6\u7684\u8fd0\u52a8\u4efb\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1\u7a7a\u95f4\u9006\u52a8\u529b\u5b66\uff08TSID\uff09\u5168\u8eab\u63a7\u5236\u5668\u5b9e\u73b0\uff0c\u53ef\u4e0e\u5176\u4ed6\u63a7\u5236\u76ee\u6807\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u8fd0\u52a8\u5197\u4f59\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u91cd\u590d\u70b9\u63a5\u89e6\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6TSID\uff0c\u8be5\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u914d\u7f6e\u7684\u51b2\u51fb\u540e\u5c16\u5cf0\u51cf\u5c11\u8fbe51%\uff0c\u6210\u529f\u907f\u514d\u6267\u884c\u5668\u9971\u548c\u3002\u5728\u56db\u8db3\u673a\u5668\u4eba\u548c\u4eba\u5f62\u673a\u5668\u4eba\u7684\u989d\u5916\u6570\u503c\u6a21\u62df\u4e2d\uff0c\u51b2\u51fb\u540e\u72b6\u6001\u5c16\u5cf0\u51cf\u5c11\u8fbe45%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u8fd0\u52a8\u5197\u4f59\u673a\u5668\u4eba\u7684\u59ff\u6001\uff0c\u663e\u8457\u51cf\u5c11\u51b2\u51fb\u5f71\u54cd\uff0c\u63d0\u9ad8\u51b2\u51fb\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u8fd0\u52a8\u5197\u4f59\u5728\u51b2\u51fb\u9c81\u68d2\u6027\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u6267\u884c\u63a5\u89e6\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14254", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14254", "abs": "https://arxiv.org/abs/2602.14254", "authors": ["Mahsa Bazzaz", "Seth Cooper"], "title": "Playing the Imitation Game: How Perceived Generated Content Shapes Player Experience", "comment": "21 pages, 12 figures, Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI '26)", "summary": "With the fast progress of generative AI in recent years, more games are integrating generated content, raising questions regarding how players perceive and respond to this content. To investigate, we ran a mixed-method survey on the games Super Mario Bros. and Sokoban, comparing procedurally generated levels and levels designed by humans to explore how perceptions of the creator relate to players' overall experience of gameplay. Players could not reliably identify the level's creator, yet their experiences were strongly linked to their beliefs about that creator rather than the actual truth. Levels believed to be human-made were rated as more fun and aesthetically pleasing. In contrast, those believed to be AI-generated were rated as more frustrating and challenging. This negative bias appeared spontaneously without knowing the levels' creator and often was based on unreliable cues of \"human-likeness.\" Our results underscore the importance of understanding perception biases when integrating generative systems into games.", "AI": {"tldr": "\u73a9\u5bb6\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u5173\u5361\u521b\u4f5c\u8005\uff0c\u4f46\u6e38\u620f\u4f53\u9a8c\u5f3a\u70c8\u53d7\u5176\u4fe1\u5ff5\u5f71\u54cd\u800c\u975e\u4e8b\u5b9e\u3002\u88ab\u8ba4\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u5173\u5361\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u7f8e\u89c2\uff0c\u88ab\u8ba4\u4e3aAI\u751f\u6210\u7684\u5173\u5361\u88ab\u8ba4\u4e3a\u66f4\u4ee4\u4eba\u6cae\u4e27\u548c\u6311\u6218\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u6e38\u620f\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u9700\u8981\u7814\u7a76\u73a9\u5bb6\u5982\u4f55\u770b\u5f85\u548c\u54cd\u5e94\u751f\u6210\u5185\u5bb9\uff0c\u4ee5\u53ca\u521b\u4f5c\u8005\u8ba4\u77e5\u5982\u4f55\u5f71\u54cd\u6e38\u620f\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\uff0c\u5728\u300a\u8d85\u7ea7\u9a6c\u91cc\u5965\u5144\u5f1f\u300b\u548c\u300a\u63a8\u7bb1\u5b50\u300b\u6e38\u620f\u4e2d\u6bd4\u8f83\u7a0b\u5e8f\u751f\u6210\u5173\u5361\u548c\u4eba\u7c7b\u8bbe\u8ba1\u5173\u5361\uff0c\u63a2\u7d22\u73a9\u5bb6\u5bf9\u521b\u4f5c\u8005\u7684\u8ba4\u77e5\u5982\u4f55\u5f71\u54cd\u6e38\u620f\u4f53\u9a8c\u3002", "result": "\u73a9\u5bb6\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u5173\u5361\u521b\u4f5c\u8005\uff0c\u4f46\u6e38\u620f\u4f53\u9a8c\u5f3a\u70c8\u53d7\u5176\u4fe1\u5ff5\u5f71\u54cd\u3002\u88ab\u8ba4\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u5173\u5361\u88ab\u8bc4\u4e3a\u66f4\u6709\u8da3\u3001\u66f4\u7f8e\u89c2\uff1b\u88ab\u8ba4\u4e3aAI\u751f\u6210\u7684\u5173\u5361\u88ab\u8bc4\u4e3a\u66f4\u4ee4\u4eba\u6cae\u4e27\u3001\u66f4\u5177\u6311\u6218\u6027\u3002\u8fd9\u79cd\u8d1f\u9762\u504f\u89c1\u5728\u6ca1\u6709\u521b\u4f5c\u8005\u4fe1\u606f\u65f6\u81ea\u53d1\u51fa\u73b0\uff0c\u901a\u5e38\u57fa\u4e8e\u4e0d\u53ef\u9760\u7684\"\u7c7b\u4eba\u6027\"\u7ebf\u7d22\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7406\u89e3\u611f\u77e5\u504f\u89c1\u5728\u5c06\u751f\u6210\u7cfb\u7edf\u96c6\u6210\u5230\u6e38\u620f\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u73a9\u5bb6\u5bf9\u521b\u4f5c\u8005\u7684\u4fe1\u5ff5\u800c\u975e\u4e8b\u5b9e\u663e\u8457\u5f71\u54cd\u6e38\u620f\u4f53\u9a8c\u3002"}}
{"id": "2602.13764", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13764", "abs": "https://arxiv.org/abs/2602.13764", "authors": ["Heng Zhi", "Wentao Tan", "Lei Zhu", "Fengling Li", "Jingjing Li", "Guoli Yang", "Heng Tao Shen"], "title": "MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer", "comment": null, "summary": "While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.", "AI": {"tldr": "MOTIF\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5c11\u6837\u672c\u8de8\u5177\u8eab\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u5177\u8eab\u65e0\u5173\u7684\u65f6\u7a7a\u6a21\u5f0f\uff08\u52a8\u4f5c\u57fa\u5143\uff09\u6765\u5e94\u5bf9\u4e0d\u540c\u673a\u5668\u4eba\u95f4\u7684\u8fd0\u52a8\u5f02\u6784\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u5177\u8eab\u7b56\u7565\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u8de8\u5177\u8eab\u8fc1\u79fb\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u4e0d\u540c\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5f02\u6784\u6027\u4ee5\u53ca\u6536\u96c6\u8db3\u591f\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6570\u636e\u7684\u9ad8\u6210\u672c\u3002\u73b0\u6709\u7684\u8de8\u5177\u8eab\u7b56\u7565\u901a\u5e38\u4f9d\u8d56\u5171\u4eab-\u79c1\u6709\u67b6\u6784\uff0c\u8fd9\u79cd\u67b6\u6784\u5b58\u5728\u79c1\u6709\u53c2\u6570\u5bb9\u91cf\u6709\u9650\u548c\u7f3a\u4e4f\u663e\u5f0f\u9002\u5e94\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "MOTIF\u901a\u8fc7\u89e3\u8026\u5177\u8eab\u65e0\u5173\u7684\u65f6\u7a7a\u6a21\u5f0f\uff08\u79f0\u4e3a\u52a8\u4f5c\u57fa\u5143\uff09\u6765\u5904\u7406\u5f02\u6784\u52a8\u4f5c\u6570\u636e\u3002\u9996\u5148\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u5b66\u4e60\u7edf\u4e00\u7684\u52a8\u4f5c\u57fa\u5143\uff0c\u91c7\u7528\u8fdb\u5ea6\u611f\u77e5\u5bf9\u9f50\u548c\u5177\u8eab\u5bf9\u6297\u7ea6\u675f\u786e\u4fdd\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u8de8\u5177\u8eab\u4e00\u81f4\u6027\u3002\u7136\u540e\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\u4ece\u5b9e\u65f6\u8f93\u5165\u9884\u6d4b\u8fd9\u4e9b\u57fa\u5143\uff0c\u6307\u5bfc\u6d41\u5339\u914d\u7b56\u7565\uff0c\u5c06\u5176\u4e0e\u673a\u5668\u4eba\u7279\u5b9a\u72b6\u6001\u878d\u5408\u4ee5\u751f\u6210\u65b0\u5177\u8eab\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86MOTIF\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u5c11\u6837\u672c\u8fc1\u79fb\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eff\u771f\u4e2d\u63d0\u53476.5%\uff0c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u63d0\u534743.7%\u3002", "conclusion": "MOTIF\u901a\u8fc7\u89e3\u8026\u5177\u8eab\u65e0\u5173\u7684\u52a8\u4f5c\u57fa\u5143\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c11\u6837\u672c\u8de8\u5177\u8eab\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5bb9\u91cf\u9650\u5236\u548c\u9002\u5e94\u673a\u5236\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5f02\u6784\u673a\u5668\u4eba\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13699", "abs": "https://arxiv.org/abs/2602.13699", "authors": ["Sophie Ostmeier", "Brian Axelrod", "Maya Varma", "Asad Aali", "Yabin Zhang", "Magdalini Paschali", "Sanmi Koyejo", "Curtis Langlotz", "Akshay Chaudhari"], "title": "Attention Head Entropy of LLMs Predicts Answer Correctness", "comment": null, "summary": "Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHead Entropy\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u71b5\u6a21\u5f0f\u9884\u6d4bLLM\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5728\u5206\u5e03\u5185\u548c\u8de8\u57df\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "LLM\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u7684\u7b54\u6848\uff0c\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u5b58\u5728\u98ce\u9669\u3002\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u800c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u5f15\u5165\u9690\u85cf\u9519\u8bef\u3002\u73b0\u6709\u767d\u76d2\u65b9\u6cd5\u4e3b\u8981\u68c0\u6d4b\u4e0a\u4e0b\u6587\u5e7b\u89c9\uff0c\u4f46\u4e24\u4e2a\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff1a\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u5426\u6269\u5c55\u5230\u9884\u6d4b\u7b54\u6848\u6b63\u786e\u6027\uff1f\u80fd\u5426\u5728\u8de8\u57df\u573a\u666f\u4e2d\u6cdb\u5316\uff1f", "method": "\u63d0\u51faHead Entropy\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u6ce8\u610f\u529b\u8d28\u91cf\u7684\u5206\u5e03\u6765\u9884\u6d4b\u7b54\u6848\u6b63\u786e\u6027\u3002\u5177\u4f53\u4f7f\u7528\u7a00\u758f\u903b\u8f91\u56de\u5f52\u5bf9\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u76842-Renyi\u71b5\u8fdb\u884c\u5206\u6790\uff0c\u6355\u6349\u6ce8\u610f\u529b\u71b5\u6a21\u5f0f\u3002", "result": "Head Entropy\u5728\u5206\u5e03\u5185\u4efb\u52a1\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u663e\u8457\u66f4\u597d\uff0c\u5e73\u5747AUROC\u6bd4\u6700\u63a5\u8fd1\u7684\u57fa\u7ebf\u9ad8+8.5%\u3002\u8fdb\u4e00\u6b65\u53d1\u73b0\uff0c\u4ec5\u57fa\u4e8e\u95ee\u9898/\u4e0a\u4e0b\u6587\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff08\u5728\u7b54\u6848\u751f\u6210\u524d\uff09\u5df2\u5177\u6709\u9884\u6d4b\u4fe1\u53f7\uff0c\u5e73\u5747AUROC\u6bd4\u6700\u63a5\u8fd1\u7684\u57fa\u7ebf\u9ad8+17.7%\u3002", "conclusion": "Head Entropy\u662f\u4e00\u79cd\u6709\u6548\u7684\u767d\u76d2\u65b9\u6cd5\uff0c\u80fd\u591f\u53ef\u9760\u9884\u6d4bLLM\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5728\u5206\u5e03\u5185\u548c\u8de8\u57df\u573a\u666f\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14357", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14357", "abs": "https://arxiv.org/abs/2602.14357", "authors": ["Annalisa Szymanski", "Oghenemaro Anuyah", "Toby Jia-Jun Li", "Ronald A. Metoyer"], "title": "Key Considerations for Domain Expert Involvement in LLM Design and Evaluation: An Ethnographic Study", "comment": "14 pages", "summary": "Large Language Models (LLMs) are increasingly developed for use in complex professional domains, yet little is known about how teams design and evaluate these systems in practice. This paper examines the challenges and trade-offs in LLM development through a 12-week ethnographic study of a team building a pedagogical chatbot. The researcher observed design and evaluation activities and conducted interviews with both developers and domain experts. Analysis revealed four key practices: creating workarounds for data collection, turning to augmentation when expert input was limited, co-developing evaluation criteria with experts, and adopting hybrid expert-developer-LLM evaluation strategies. These practices show how teams made strategic decisions under constraints and demonstrate the central role of domain expertise in shaping the system. Challenges included expert motivation and trust, difficulties structuring participatory design, and questions around ownership and integration of expert knowledge. We propose design opportunities for future LLM development workflows that emphasize AI literacy, transparent consent, and frameworks recognizing evolving expert roles.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc712\u5468\u7684\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u4e13\u4e1a\u9886\u57dfLLM\u5f00\u53d1\u56e2\u961f\u5728\u5b9e\u8df5\u4e2d\u9762\u4e34\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u6559\u5b66\u804a\u5929\u673a\u5668\u4eba\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u5b9e\u8df5\u548c\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6218\u7565\u51b3\u7b56\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u56e2\u961f\u5728\u5b9e\u8df5\u4e2d\u5982\u4f55\u8bbe\u8ba1\u548c\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u7684\u7814\u7a76\u5f88\u5c11\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793aLLM\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u9645\u6311\u6218\u548c\u6743\u8861\u3002", "method": "\u91c7\u752812\u5468\u7684\u6c11\u65cf\u5fd7\u7814\u7a76\u65b9\u6cd5\uff0c\u89c2\u5bdf\u4e00\u4e2a\u6559\u5b66\u804a\u5929\u673a\u5668\u4eba\u5f00\u53d1\u56e2\u961f\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6d3b\u52a8\uff0c\u5e76\u5bf9\u5f00\u53d1\u8005\u548c\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u8bbf\u8c08\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u56db\u4e2a\u5173\u952e\u5b9e\u8df5\uff1a1) \u4e3a\u6570\u636e\u6536\u96c6\u521b\u5efa\u53d8\u901a\u65b9\u6848\uff1b2) \u4e13\u5bb6\u8f93\u5165\u6709\u9650\u65f6\u8f6c\u5411\u589e\u5f3a\u65b9\u6cd5\uff1b3) \u4e0e\u4e13\u5bb6\u5171\u540c\u5236\u5b9a\u8bc4\u4f30\u6807\u51c6\uff1b4) \u91c7\u7528\u4e13\u5bb6-\u5f00\u53d1\u8005-LLM\u6df7\u5408\u8bc4\u4f30\u7b56\u7565\u3002\u8fd9\u4e9b\u5b9e\u8df5\u5c55\u793a\u4e86\u56e2\u961f\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u5982\u4f55\u505a\u51fa\u6218\u7565\u51b3\u7b56\uff0c\u4ee5\u53ca\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u5728\u5851\u9020\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "LLM\u5f00\u53d1\u9762\u4e34\u4e13\u5bb6\u52a8\u673a\u4e0e\u4fe1\u4efb\u3001\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7ed3\u6784\u5316\u56f0\u96be\u3001\u4e13\u4e1a\u77e5\u8bc6\u6240\u6709\u6743\u4e0e\u6574\u5408\u7b49\u6311\u6218\u3002\u672a\u6765LLM\u5f00\u53d1\u5de5\u4f5c\u6d41\u5e94\u5f3a\u8c03AI\u7d20\u517b\u3001\u900f\u660e\u540c\u610f\u673a\u5236\u4ee5\u53ca\u80fd\u591f\u8bc6\u522b\u4e13\u5bb6\u89d2\u8272\u6f14\u53d8\u7684\u6846\u67b6\u3002"}}
{"id": "2602.13700", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13700", "abs": "https://arxiv.org/abs/2602.13700", "authors": ["Orin Levy", "Yishay Mansour"], "title": "Optimal Regret for Policy Optimization in Contextual Bandits", "comment": null, "summary": "We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\\widetilde{O}(\\sqrt{ K|\\mathcal{A}|\\log|\\mathcal{F}|})$, where $K$ is the number of rounds, $\\mathcal{A}$ is the set of arms, and $\\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5177\u6709\u4e00\u822c\u79bb\u7ebf\u51fd\u6570\u903c\u8fd1\u7684\u968f\u673a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u7684\u9ad8\u6982\u7387\u6700\u4f18\u9057\u61be\u754c\uff0c\u7b97\u6cd5\u9ad8\u6548\u4e14\u8fbe\u5230\u6700\u4f18\u9057\u61be\u754c\u3002", "motivation": "\u89e3\u51b3\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7406\u8bba\u754c\u4e0e\u5b9e\u8df5\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8bc1\u660e\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u4e25\u683c\u8bc1\u660e\u7684\u6700\u4f18\u9057\u61be\u754c\u3002", "method": "\u91c7\u7528\u7b56\u7565\u4f18\u5316\u6280\u672f\uff0c\u7ed3\u5408\u4e00\u822c\u79bb\u7ebf\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u5904\u7406\u968f\u673a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u3002", "result": "\u7b97\u6cd5\u8fbe\u5230\u6700\u4f18\u9057\u61be\u754c $\\widetilde{O}(\\sqrt{ K|\\mathcal{A}|\\log|\\mathcal{F}|})$\uff0c\u5176\u4e2dK\u4e3a\u8f6e\u6570\uff0c$\\mathcal{A}$\u4e3a\u81c2\u96c6\u5408\uff0c$\\mathcal{F}$\u4e3a\u51fd\u6570\u7c7b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u6865\u63a5\u4e86\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u8bc1\u660e\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u4e25\u683c\u8bc1\u660e\u7684\u6700\u4f18\u9057\u61be\u754c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2602.14438", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14438", "abs": "https://arxiv.org/abs/2602.14438", "authors": ["Hamid Khabazi", "Ali F. Meghdari", "Alireza Taheri"], "title": "RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems", "comment": null, "summary": "This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u548cVLM\u7684\u667a\u80fd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u4eba\u5b66\u8bbe\u8ba1\uff0c\u80fd\u591f\u81ea\u52a8\u5206\u6790\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u81c2\u76f8\u5173\u95ee\u9898\uff0c\u5305\u62ec\u8fd0\u52a8\u5b66\u8ba1\u7b97\u3001\u4eff\u771f\u548c\u63a7\u5236\u3002", "motivation": "\u5c06LLM\u548cVLM\u7684\u4f18\u52bf\u4e0e\u8ba1\u7b97\u5de5\u5177\u7ed3\u5408\uff0c\u81ea\u52a8\u5206\u6790\u548c\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u81c2\u76f8\u5173\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u5230\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u7684\u5b8c\u6574\u6d41\u7a0b\u81ea\u52a8\u5316\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eLLM\u548cVLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u63a5\u53d7\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\uff0c\u81ea\u52a8\u6267\u884c\u6b63\u5411/\u9006\u5411\u8fd0\u52a8\u5b66\u3001\u901f\u5ea6\u52a0\u901f\u5ea6\u8ba1\u7b97\u30013D\u4eff\u771f\u751f\u6210\u548c\u8fd0\u52a8\u63a7\u5236\u3002\u4f7f\u7528GPT-4o\u3001DeepSeek-V3.2\u3001Claude-Sonnet-4.5\u7b49\u6a21\u578b\u8fdb\u884c\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u6587\u672c\u8f93\u5165\u6d4b\u8bd5\u4e2d\uff0c\u6846\u67b6\u96c6\u6210GPT-4o\u8fbe\u52300.97\u51c6\u786e\u7387\uff08\u539f\u59cb\u6a21\u578b\u4ec50.30\uff09\uff1b\u89c6\u89c9\u8f93\u5165\u6d4b\u8bd5\u4e2d\u8fbe\u52300.93\u51c6\u786e\u7387\uff0c\u6bd4\u539f\u59cb\u6a21\u578b\u9ad8\u7ea620%\uff1b\u7efc\u5408\u673a\u5668\u4eba\u4efb\u52a1\u6d4b\u8bd5\u4e2d\u8fbe\u52300.97\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u76f8\u5173\u95ee\u9898\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u5b66\u8ba1\u7b97\u548c\u4eff\u771f\u63a7\u5236\u65b9\u9762\uff0c\u8bc1\u660e\u4e86LLM/VLM\u4e0e\u8ba1\u7b97\u5de5\u5177\u96c6\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.13833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13833", "abs": "https://arxiv.org/abs/2602.13833", "authors": ["Kevin Yuchen Ma", "Heng Zhang", "Weisi Lin", "Mike Zheng Shou", "Yan Wu"], "title": "Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation", "comment": null, "summary": "Generalizing tool manipulation requires both semantic planning and precise physical control. Modern generalist robot policies, such as Vision-Language-Action (VLA) models, often lack the high-fidelity physical grounding required for contact-rich tool manipulation. Conversely, existing contact-aware policies that leverage tactile or haptic sensing are typically instance-specific and fail to generalize across diverse tool geometries. Bridging this gap requires learning unified contact representations from diverse data, yet a fundamental barrier remains: diverse real-world tactile data are prohibitive at scale, while direct zero-shot sim-to-real transfer is challenging due to the complex dynamics of nonlinear deformation of soft sensors.\n  To address this, we propose Semantic-Contact Fields (SCFields), a unified 3D representation fusing visual semantics with dense contact estimates. We enable this via a two-stage Sim-to-Real Contact Learning Pipeline: first, we pre-train on a large simulation data set to learn general contact physics; second, we fine-tune on a small set of real data, pseudo-labeled via geometric heuristics and force optimization, to align sensor characteristics. This allows physical generalization to unseen tools. We leverage SCFields as the dense observation input for a diffusion policy to enable robust execution of contact-rich tool manipulation tasks. Experiments on scraping, crayon drawing, and peeling demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.", "AI": {"tldr": "SCFields\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u89c6\u89c9\u8bed\u4e49\u4e0e\u5bc6\u96c6\u63a5\u89e6\u4f30\u8ba1\u76843D\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5Sim-to-Real\u63a5\u89e6\u5b66\u4e60\u7ba1\u9053\u5b9e\u73b0\u672a\u89c1\u5de5\u5177\u7684\u7269\u7406\u6cdb\u5316\uff0c\u5728\u522e\u524a\u3001\u8721\u7b14\u753b\u548c\u5265\u79bb\u7b49\u63a5\u89e6\u5bc6\u96c6\u578b\u5de5\u5177\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7b56\u7565\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7f3a\u4e4f\u9ad8\u4fdd\u771f\u7269\u7406\u57fa\u7840\uff0c\u96be\u4ee5\u5904\u7406\u63a5\u89e6\u5bc6\u96c6\u578b\u5de5\u5177\u64cd\u4f5c\uff1b2\uff09\u57fa\u4e8e\u89e6\u89c9\u611f\u77e5\u7684\u63a5\u89e6\u611f\u77e5\u7b56\u7565\u901a\u5e38\u662f\u5b9e\u4f8b\u7279\u5b9a\u7684\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7684\u5de5\u5177\u3002\u540c\u65f6\uff0c\u5927\u89c4\u6a21\u83b7\u53d6\u771f\u5b9e\u4e16\u754c\u89e6\u89c9\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u800c\u76f4\u63a5\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u96f6\u6837\u672c\u8f6c\u79fb\u53c8\u56e0\u8f6f\u4f20\u611f\u5668\u975e\u7ebf\u6027\u53d8\u5f62\u7684\u590d\u6742\u52a8\u529b\u5b66\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u63a5\u89e6\u573a\uff08SCFields\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u878d\u5408\u89c6\u89c9\u8bed\u4e49\u4e0e\u5bc6\u96c6\u63a5\u89e6\u4f30\u8ba1\u7684\u7edf\u4e003D\u8868\u793a\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u6a21\u62df\u5230\u73b0\u5b9e\u63a5\u89e6\u5b66\u4e60\u7ba1\u9053\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u5927\u89c4\u6a21\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u4ee5\u5b66\u4e60\u901a\u7528\u63a5\u89e6\u7269\u7406\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u5c0f\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff0c\u901a\u8fc7\u51e0\u4f55\u542f\u53d1\u5f0f\u548c\u529b\u4f18\u5316\u8fdb\u884c\u4f2a\u6807\u6ce8\u4ee5\u5bf9\u9f50\u4f20\u611f\u5668\u7279\u6027\u3002\u5c06SCFields\u4f5c\u4e3a\u6269\u6563\u7b56\u7565\u7684\u5bc6\u96c6\u89c2\u6d4b\u8f93\u5165\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u5de5\u5177\u64cd\u4f5c\u6267\u884c\u3002", "result": "\u5728\u522e\u524a\u3001\u8721\u7b14\u753b\u548c\u5265\u79bb\u7b49\u63a5\u89e6\u5bc6\u96c6\u578b\u5de5\u5177\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cSCFields\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7c7b\u522b\u7ea7\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u548c\u539f\u59cb\u89e6\u89c9\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SCFields\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u8bed\u4e49\u4e0e\u63a5\u89e6\u7269\u7406\u7684\u7edf\u4e00\u8868\u793a\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u6a21\u62df\u5230\u73b0\u5b9e\u5b66\u4e60\u7ba1\u9053\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u5de5\u5177\u7684\u7269\u7406\u6cdb\u5316\uff0c\u4e3a\u63a5\u89e6\u5bc6\u96c6\u578b\u5de5\u5177\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13706", "abs": "https://arxiv.org/abs/2602.13706", "authors": ["Orin Levy", "Aviv Rosenberg", "Alon Cohen", "Yishay Mansour"], "title": "Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation", "comment": null, "summary": "We introduce \\texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\\widetilde{O}(H^4\\sqrt{T|S||A|\\log(|\\mathcal{F}||\\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\\mathcal{F}, \\mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.", "AI": {"tldr": "OPO-CMDP\u662f\u9996\u4e2a\u7528\u4e8e\u968f\u673a\u4e0a\u4e0b\u6587\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDPs\uff09\u7684\u79bb\u7ebf\u51fd\u6570\u8fd1\u4f3c\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684|S|\u548c|A|\u4f9d\u8d56\u6027\u7684\u9057\u61be\u754c\u3002", "motivation": "\u5f53\u524dCMDPs\u5728\u79bb\u7ebf\u51fd\u6570\u8fd1\u4f3c\u4e0b\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5b58\u5728\u7406\u8bba\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u72b6\u6001\u7a7a\u95f4\u548c\u52a8\u4f5c\u7a7a\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u4e0d\u591f\u4f18\u5316\uff0c\u9700\u8981\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u4e50\u89c2\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u51fd\u6570\u7c7b\u8fd1\u4f3c\u635f\u5931\u548c\u52a8\u6001\uff0c\u5b9e\u73b0\u9ad8\u6982\u7387\u9057\u61be\u754c\u5206\u6790\u3002", "result": "\u83b7\u5f97\u4e86$\\widetilde{O}(H^4\\sqrt{T|S||A|\\log(|\\mathcal{F}||\\mathcal{P}|)})$\u7684\u9057\u61be\u754c\uff0c\u5728|S|\u548c|A|\u4f9d\u8d56\u6027\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u76f4\u63a5\u6539\u8fdb\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u4e50\u89c2\u7b56\u7565\u4f18\u5316\u4e3a\u6c42\u89e3CMDPs\u63d0\u4f9b\u4e86\u4e00\u6761\u81ea\u7136\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u7406\u8bba\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u5f84\u3002"}}
{"id": "2602.14559", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14559", "abs": "https://arxiv.org/abs/2602.14559", "authors": ["Shishir Sharma", "Doina Precup", "Theodore J. Perkins"], "title": "Fluid-Agent Reinforcement Learning", "comment": "Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u6d41\u4f53\u667a\u80fd\u4f53\u73af\u5883\"\u6846\u67b6\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u52a8\u6001\u521b\u5efa\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u667a\u80fd\u4f53\u6570\u91cf\u56fa\u5b9a\u4e0d\u53d8\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u667a\u80fd\u4f53\u6570\u91cf\u901a\u5e38\u4e0d\u56fa\u5b9a\u4e14\u672a\u77e5\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u521b\u5efa\u5176\u4ed6\u667a\u80fd\u4f53\uff08\u5982\u7ec6\u80de\u5206\u88c2\u3001\u516c\u53f8\u5206\u62c6\u90e8\u95e8\uff09\uff0c\u800c\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u53ea\u7814\u7a76\u56fa\u5b9a\u6570\u91cf\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u6d41\u4f53\u667a\u80fd\u4f53\u73af\u5883\u6846\u67b6\uff0c\u5f15\u5165\u535a\u5f08\u8bba\u89e3\u51b3\u65b9\u6848\u6982\u5ff5\uff0c\u5e76\u5728\u8be5\u6846\u67b6\u4e0b\u5b9e\u8bc1\u8bc4\u4f30\u591a\u79cdMARL\u7b97\u6cd5\uff0c\u5305\u62ecPredator-Prey\u548cLevel-Based Foraging\u7684\u6d41\u4f53\u53d8\u4f53\uff0c\u4ee5\u53ca\u65b0\u8bbe\u8ba1\u7684\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u4ea7\u751f\u6839\u636e\u73af\u5883\u9700\u6c42\u52a8\u6001\u8c03\u6574\u89c4\u6a21\u7684\u667a\u80fd\u4f53\u56e2\u961f\uff0c\u6d41\u4f53\u6027\u80fd\u591f\u89e3\u9501\u56fa\u5b9a\u7fa4\u4f53\u8bbe\u7f6e\u4e2d\u65e0\u6cd5\u89c2\u5bdf\u5230\u7684\u65b0\u9896\u89e3\u51b3\u7b56\u7565\u3002", "conclusion": "\u6d41\u4f53\u667a\u80fd\u4f53\u6846\u67b6\u6269\u5c55\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u66f4\u7075\u6d3b\u5730\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u7fa4\u4f53\u89c4\u6a21\uff0c\u4e3a\u667a\u80fd\u4f53\u81ea\u4e3b\u521b\u5efa\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2602.13849", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13849", "abs": "https://arxiv.org/abs/2602.13849", "authors": ["Majid Sadeghinejad", "Arman Barghi", "Hamed Hosseini", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Push-Placement: A Hybrid Approach Integrating Prehensile and Non-Prehensile Manipulation for Object Rearrangement", "comment": "International Conference on Robotics and Mechatronics (ICRoM 2025)", "summary": "Efficient tabletop rearrangement remains challenging due to collisions and the need for temporary buffering when target poses are obstructed. Prehensile pick-and-place provides precise control but often requires extra moves, whereas non-prehensile pushing can be more efficient but suffers from complex, imprecise dynamics. This paper proposes push-placement, a hybrid action primitive that uses the grasped object to displace obstructing items while being placed, thereby reducing explicit buffering. The method is integrated into a physics-in-the-loop Monte Carlo Tree Search (MCTS) planner and evaluated in the PyBullet simulator. Empirical results show push-placement reduces the manipulator travel cost by up to 11.12% versus a baseline MCTS planner and 8.56% versus dynamic stacking. These findings indicate that hybrid prehensile/non-prehensile action primitives can substantially improve efficiency in long-horizon rearrangement tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"push-placement\"\u7684\u6df7\u5408\u64cd\u4f5c\u539f\u8bed\uff0c\u7ed3\u5408\u6293\u53d6\u653e\u7f6e\u548c\u63a8\u52a8\u64cd\u4f5c\uff0c\u5728\u653e\u7f6e\u7269\u4f53\u65f6\u540c\u65f6\u63a8\u5f00\u969c\u788d\u7269\uff0c\u51cf\u5c11\u663e\u5f0f\u7f13\u51b2\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86\u684c\u9762\u91cd\u6392\u4efb\u52a1\u7684\u6548\u7387\u3002", "motivation": "\u684c\u9762\u91cd\u6392\u4efb\u52a1\u9762\u4e34\u78b0\u649e\u548c\u76ee\u6807\u4f4d\u59ff\u88ab\u906e\u6321\u65f6\u9700\u8981\u4e34\u65f6\u7f13\u51b2\u7684\u6311\u6218\u3002\u6293\u53d6\u653e\u7f6e\u64cd\u4f5c\u63a7\u5236\u7cbe\u786e\u4f46\u9700\u8981\u989d\u5916\u79fb\u52a8\uff0c\u800c\u63a8\u52a8\u64cd\u4f5c\u66f4\u9ad8\u6548\u4f46\u52a8\u6001\u590d\u6742\u4e14\u4e0d\u7cbe\u786e\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u91cd\u6392\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86push-placement\u6df7\u5408\u64cd\u4f5c\u539f\u8bed\uff0c\u5728\u6293\u53d6\u7269\u4f53\u653e\u7f6e\u7684\u540c\u65f6\u4f7f\u7528\u8be5\u7269\u4f53\u63a8\u5f00\u969c\u788d\u7269\u3002\u5c06\u6b64\u65b9\u6cd5\u96c6\u6210\u5230\u57fa\u4e8e\u7269\u7406\u4eff\u771f\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u89c4\u5212\u5668\u4e2d\uff0c\u5728PyBullet\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cpush-placement\u76f8\u6bd4\u57fa\u51c6MCTS\u89c4\u5212\u5668\u51cf\u5c11\u4e86\u6700\u591a11.12%\u7684\u673a\u68b0\u81c2\u79fb\u52a8\u6210\u672c\uff0c\u76f8\u6bd4\u52a8\u6001\u5806\u53e0\u65b9\u6cd5\u51cf\u5c11\u4e868.56%\u3002", "conclusion": "\u6df7\u5408\u6293\u53d6/\u975e\u6293\u53d6\u64cd\u4f5c\u539f\u8bed\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u957f\u89c6\u91ce\u91cd\u6392\u4efb\u52a1\u7684\u6548\u7387\uff0cpush-placement\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u663e\u5f0f\u7f13\u51b2\u9700\u6c42\u6709\u6548\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c\u3002"}}
{"id": "2602.13710", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13710", "abs": "https://arxiv.org/abs/2602.13710", "authors": ["Xin Yan", "Zhenglin Wan", "Feiyang Ye", "Xingrui Yu", "Hangyu Du", "Yang You", "Ivor Tsang"], "title": "HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.", "AI": {"tldr": "HBVLA\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u76841\u6bd4\u7279\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u611f\u77e5\u589e\u5f3aHessian\u8bc6\u522b\u5173\u952e\u6743\u91cd\uff0c\u4f7f\u7528\u7a00\u758f\u6b63\u4ea4\u53d8\u6362\u5904\u7406\u975e\u663e\u8457\u6743\u91cd\uff0c\u5728Harr\u57df\u8fdb\u884c\u5206\u7ec41\u6bd4\u7279\u91cf\u5316\uff0c\u663e\u8457\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u6307\u4ee4\u8ddf\u968f\u7684\u5177\u8eab\u63a7\u5236\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u673a\u5668\u4eba\u548c\u8fb9\u7f18\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002\u73b0\u67091\u6bd4\u7279\u91cf\u5316\u65b9\u6cd5\u65e0\u6cd5\u7f29\u5c0f\u4e8c\u503c\u5316\u4e0e\u5168\u7cbe\u5ea6\u6743\u91cd\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5728\u957f\u65f6\u7a0b\u95ed\u73af\u6267\u884c\u4e2d\u91cf\u5316\u8bef\u5dee\u7d2f\u79ef\uff0c\u4e25\u91cd\u964d\u4f4e\u52a8\u4f5c\u8d28\u91cf\u3002", "method": "HBVLA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a1\uff09\u4f7f\u7528\u7b56\u7565\u611f\u77e5\u589e\u5f3aHessian\u8bc6\u522b\u5bf9\u52a8\u4f5c\u751f\u6210\u771f\u6b63\u5173\u952e\u7684\u6743\u91cd\uff1b2\uff09\u5bf9\u975e\u663e\u8457\u6743\u91cd\u4f7f\u7528\u7a00\u758f\u6b63\u4ea4\u53d8\u6362\u8bf1\u5bfc\u4f4e\u71b5\u4e2d\u95f4\u72b6\u6001\uff1b3\uff09\u5728Harr\u57df\u5bf9\u663e\u8457\u548c\u975e\u663e\u8457\u6743\u91cd\u8fdb\u884c\u5206\u7ec41\u6bd4\u7279\u91cf\u5316\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u4e0a\uff0c\u91cf\u5316\u7684OpenVLA-OFT\u4fdd\u7559\u4e8692.2%\u7684\u5168\u7cbe\u5ea6\u6027\u80fd\uff1b\u5728SimplerEnv\u4e0a\uff0c\u91cf\u5316\u7684CogAct\u4fdd\u7559\u4e8693.6%\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e8c\u503c\u5316\u65b9\u6cd5\u3002\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u663e\u793aHBVLA\u4ec5\u5e26\u6765\u8fb9\u9645\u6210\u529f\u7387\u4e0b\u964d\uff0c\u8bc1\u660e\u4e86\u5728\u4e25\u683c\u786c\u4ef6\u7ea6\u675f\u4e0b\u7684\u9c81\u68d2\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "HBVLA\u4e3aVLA\u6a21\u578b\u7684\u8d85\u4f4e\u4f4d\u91cf\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u80fd\u591f\u5728\u786c\u4ef6\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u90e8\u7f72\uff0c\u586b\u8865\u4e86\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5728\u7f29\u5c0f\u4e8c\u503c\u5316\u4e0e\u5168\u7cbe\u5ea6\u6743\u91cd\u5206\u5e03\u5dee\u8ddd\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.14849", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14849", "abs": "https://arxiv.org/abs/2602.14849", "authors": ["Bardia Mohammadi", "Nearchos Potamitis", "Lars Klein", "Akhil Arora", "Laurent Bindschaedler"], "title": "Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows", "comment": null, "summary": "LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.", "AI": {"tldr": "Atomix\u4e3aLLM\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u63d0\u4f9b\u8fdb\u5ea6\u611f\u77e5\u7684\u4e8b\u52a1\u8bed\u4e49\uff0c\u901a\u8fc7\u7eaa\u5143\u6807\u8bb0\u3001\u8d44\u6e90\u524d\u6cbf\u8ddf\u8e2a\u548c\u8fdb\u5ea6\u8c13\u8bcd\u63a7\u5236\u63d0\u4ea4\uff0c\u652f\u6301\u53ef\u7f13\u51b2\u6548\u679c\u7684\u5ef6\u8fdf\u6267\u884c\u548c\u5916\u90e8\u5316\u6548\u679c\u7684\u8865\u507f\u56de\u6eda\uff0c\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u5e76\u589e\u5f3a\u9694\u79bb\u6027\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u5916\u90e8\u7cfb\u7edf\u4e0a\u6267\u884c\u64cd\u4f5c\uff0c\u4f46\u5de5\u5177\u6548\u679c\u662f\u7acb\u5373\u751f\u6548\u7684\u3002\u5728\u6545\u969c\u3001\u63a8\u6d4b\u6267\u884c\u6216\u8d44\u6e90\u7ade\u4e89\u60c5\u51b5\u4e0b\uff0c\u5931\u8d25\u7684\u5206\u652f\u53ef\u80fd\u4f1a\u6cc4\u6f0f\u610f\u5916\u7684\u526f\u4f5c\u7528\uff0c\u4e14\u6ca1\u6709\u5b89\u5168\u7684\u56de\u6eda\u673a\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u4e8b\u52a1\u6027\u4fdd\u8bc1\u7684\u8fd0\u884c\u65f6\u7cfb\u7edf\u3002", "method": "Atomix\u8fd0\u884c\u65f6\u4e3a\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u63d0\u4f9b\u8fdb\u5ea6\u611f\u77e5\u7684\u4e8b\u52a1\u8bed\u4e49\uff1a1\uff09\u4e3a\u6bcf\u4e2a\u8c03\u7528\u6807\u8bb0\u7eaa\u5143\uff1b2\uff09\u8ddf\u8e2a\u6bcf\u4e2a\u8d44\u6e90\u7684\u524d\u6cbf\u72b6\u6001\uff1b3\uff09\u4ec5\u5f53\u8fdb\u5ea6\u8c13\u8bcd\u6307\u793a\u5b89\u5168\u65f6\u624d\u63d0\u4ea4\uff1b4\uff09\u53ef\u7f13\u51b2\u7684\u6548\u679c\u53ef\u4ee5\u5ef6\u8fdf\u6267\u884c\uff1b5\uff09\u5916\u90e8\u5316\u7684\u6548\u679c\u5728\u56de\u6eda\u65f6\u8fdb\u884c\u8865\u507f\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u8fdb\u884c\u6545\u969c\u6ce8\u5165\u6d4b\u8bd5\uff1a1\uff09\u4e8b\u52a1\u6027\u91cd\u8bd5\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff1b2\uff09\u524d\u6cbf\u95e8\u63a7\u63d0\u4ea4\u5728\u63a8\u6d4b\u6267\u884c\u548c\u8d44\u6e90\u7ade\u4e89\u60c5\u51b5\u4e0b\u589e\u5f3a\u4e86\u9694\u79bb\u6027\u3002", "conclusion": "Atomix\u901a\u8fc7\u63d0\u4f9b\u8fdb\u5ea6\u611f\u77e5\u7684\u4e8b\u52a1\u8bed\u4e49\uff0c\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u5916\u90e8\u7cfb\u7edf\u64cd\u4f5c\u4e2d\u7684\u526f\u4f5c\u7528\u6cc4\u6f0f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9694\u79bb\u6027\uff0c\u4e3a\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6267\u884c\u63d0\u4f9b\u4e86\u4fdd\u969c\u3002"}}
{"id": "2602.14442", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14442", "abs": "https://arxiv.org/abs/2602.14442", "authors": ["Kengo Tanaka", "Xiyue Wang", "Hironobu Takagi", "Yoichi Ochiai", "Chieko Asakawa"], "title": "Touching Movement: 3D Tactile Poses for Supporting Blind People in Learning Body Movements", "comment": "Accepted to TEI 2026", "summary": "Visual impairments create barriers to learning physical activities, since conventional training methods rely on visual demonstrations or often inadequate verbal descriptions. This research explores 3D-printed human body models to enhance movement comprehension for blind individuals. Through a participatory design approach in collaboration with a blind designer, we developed detailed 3D models representing various body movements and incorporated tactile reference elements to enhance spatial understanding. We conducted two user studies with 10 blind participants across different activities: static yoga poses and sequential calisthenic movements. The results demonstrated that 3D models significantly improved understanding speed, reduced questions for clarification, and enhanced movement accuracy compared to conventional teaching methods. Participants consistently rated 3D models higher for ease of understanding, effectiveness, and motivation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u75283D\u6253\u5370\u4eba\u4f53\u6a21\u578b\u5e2e\u52a9\u76f2\u4eba\u7406\u89e3\u8eab\u4f53\u52a8\u4f5c\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u4e0e\u76f2\u4eba\u8bbe\u8ba1\u5e08\u5408\u4f5c\u5f00\u53d1\u4e86\u5305\u542b\u89e6\u89c9\u53c2\u8003\u5143\u7d20\u7684\u8be6\u7ec6\u6a21\u578b\uff0c\u5728\u745c\u4f3d\u59ff\u52bf\u548c\u5065\u8eab\u52a8\u4f5c\u8bad\u7ec3\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7406\u89e3\u901f\u5ea6\u548c\u52a8\u4f5c\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u89c9\u969c\u788d\u8005\u5728\u5b66\u4e60\u4f53\u80b2\u6d3b\u52a8\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u56e0\u4e3a\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u6f14\u793a\u6216\u4e0d\u5145\u5206\u7684\u53e3\u5934\u63cf\u8ff0\uff0c\u9700\u8981\u5f00\u53d1\u66ff\u4ee3\u65b9\u6cd5\u6765\u5e2e\u52a9\u4ed6\u4eec\u7406\u89e3\u8eab\u4f53\u52a8\u4f5c\u3002", "method": "\u91c7\u7528\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e0e\u76f2\u4eba\u8bbe\u8ba1\u5e08\u5408\u4f5c\u5f00\u53d1\u8be6\u7ec6\u76843D\u6253\u5370\u4eba\u4f53\u6a21\u578b\uff0c\u5305\u542b\u89e6\u89c9\u53c2\u8003\u5143\u7d20\u4ee5\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u3002\u8fdb\u884c\u4e86\u4e24\u9879\u7528\u6237\u7814\u7a76\uff0c\u6d89\u53ca10\u540d\u76f2\u4eba\u53c2\u4e0e\u8005\uff0c\u5206\u522b\u6d4b\u8bd5\u9759\u6001\u745c\u4f3d\u59ff\u52bf\u548c\u8fde\u7eed\u5065\u8eab\u52a8\u4f5c\u7684\u5b66\u4e60\u6548\u679c\u3002", "result": "3D\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u7406\u89e3\u901f\u5ea6\uff0c\u51cf\u5c11\u4e86\u6f84\u6e05\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u4e86\u52a8\u4f5c\u51c6\u786e\u6027\u3002\u53c2\u4e0e\u8005\u4e00\u81f4\u8ba4\u4e3a3D\u6a21\u578b\u5728\u6613\u7406\u89e3\u6027\u3001\u6709\u6548\u6027\u548c\u6fc0\u52b1\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6559\u5b66\u65b9\u6cd5\u3002", "conclusion": "3D\u6253\u5370\u4eba\u4f53\u6a21\u578b\u662f\u5e2e\u52a9\u76f2\u4eba\u7406\u89e3\u8eab\u4f53\u52a8\u4f5c\u7684\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u5f25\u8865\u4f20\u7edf\u89c6\u89c9\u4f9d\u8d56\u6559\u5b66\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u89c6\u89c9\u969c\u788d\u8005\u7684\u4f53\u80b2\u6d3b\u52a8\u5b66\u4e60\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13850", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13850", "abs": "https://arxiv.org/abs/2602.13850", "authors": ["Minku Kim", "Kuan-Chia Chen", "Aayam Shrestha", "Li Fuxin", "Stefan Lee", "Alan Fern"], "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement", "comment": "10 pages, 6 figures", "summary": "We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \\emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6280\u80fd\u7684\u4eba\u5f62\u673a\u5668\u4eba\u7bb1\u5b50\u91cd\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7ea7\u6280\u80fd\u5e8f\u5217\u5b9e\u73b0\u957f\u65f6\u7a0b\u6267\u884c\uff0c\u91c7\u7528\u5171\u4eab\u7684\u5168\u8eab\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u805a\u5408\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6bcf\u4e2a\u6280\u80fd\u4f7f\u7528\u72ec\u7acb\u7684\u4f4e\u7ea7\u63a7\u5236\u5668\uff0c\u96be\u4ee5\u5b9e\u73b0\u957f\u65f6\u7a0b\u7684\u9c81\u68d2\u6267\u884c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u652f\u6301\u6280\u80fd\u7ec4\u5408\u548c\u957f\u65f6\u7a0b\u64cd\u4f5c\u7684\u7edf\u4e00\u63a7\u5236\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u6280\u80fd\u6846\u67b6\uff0c\u6240\u6709\u6280\u80fd\u901a\u8fc7\u5171\u4eab\u7684\u4efb\u52a1\u65e0\u5173\u5168\u8eab\u63a7\u5236\u5668\u6267\u884c\uff1b\u91c7\u7528\u6570\u636e\u805a\u5408\u65b9\u6cd5\uff0c\u5728\u9886\u57df\u968f\u673a\u5316\u4e0b\u901a\u8fc7\u95ed\u73af\u6280\u80fd\u6267\u884c\u589e\u5f3a\u5171\u4eab\u63a7\u5236\u5668\u7684\u8bad\u7ec3\uff1b\u5f15\u5165Humanoid Hanoi\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u4eff\u771f\u548cDigit V3\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u4e3b\u7684\u957f\u65f6\u7a0b\u91cd\u6392\uff0c\u91cf\u5316\u4e86\u5171\u4eab\u5168\u8eab\u63a7\u5236\u5668\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u975e\u5171\u4eab\u57fa\u7ebf\u7684\u4f18\u52bf\u3002", "conclusion": "\u5171\u4eab\u5168\u8eab\u63a7\u5236\u5668\u6846\u67b6\u901a\u8fc7\u6280\u80fd\u7ec4\u5408\u652f\u6301\u957f\u65f6\u7a0b\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u6570\u636e\u805a\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6280\u80fd\u7ec4\u5408\u5e26\u6765\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u590d\u6742\u91cd\u6392\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.14467", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14467", "abs": "https://arxiv.org/abs/2602.14467", "authors": ["Kexin Quan", "Jessie Chin"], "title": "Conversational Decision Support for Information Search Under Uncertainty: Effects of Gist and Verbatim Feedback", "comment": null, "summary": "Many real-world decisions rely on information search, where people sample evidence and decide when to stop under uncertainty. The uncertainty in the environment, particularly how diagnostic evidence is distributed, causes complexities in information search, further leading to suboptimal decision-making outcomes. Yet AI decision support often targets outcome optimization, and less is known about how to scaffold search without increasing cognitive load. We introduce SERA, an LLM-based assistant that provides either gist or verbatim feedback during search. Across two experiments (N1=54, N2=54), we examined decision-making outcomes and information search in SERA-Gist, SERA-Verbatim, and a no-feedback baseline across three environments varying in uncertainty. The uncertainty in environment is operationalized by the perceived gain of information across the course of sampling, which individuals may experience diminishing return of information gain (decremental; low-uncertainty), or a local drop of information gain (local optimum; medium-uncertainty), or no patterns in information gain (high-uncertainty), as they search more. Individuals show more accurate decision outcomes and are more confident with SERA support, especially under higher uncertainty. Gist feedback was associated with more efficient integration and showed a descriptive pattern of reduced oversampling, while verbatim feedback promoted more extensive exploration. These findings establish feedback representation as a design lever when search matters, motivating adaptive systems that match feedback granularity to uncertainty.", "AI": {"tldr": "SERA\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u51b3\u7b56\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u4f9b\u8981\u70b9\u5f0f\u6216\u9010\u5b57\u5f0f\u53cd\u9988\u6765\u652f\u6301\u4fe1\u606f\u641c\u7d22\u51b3\u7b56\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u63d0\u9ad8\u51b3\u7b56\u51c6\u786e\u6027\u548c\u4fe1\u5fc3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u4f9d\u8d56\u4e8e\u4fe1\u606f\u641c\u7d22\uff0c\u4f46\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff08\u7279\u522b\u662f\u8bc1\u636e\u7684\u8bca\u65ad\u6027\u5206\u5e03\uff09\u4f7f\u641c\u7d22\u590d\u6742\u5316\uff0c\u5bfc\u81f4\u6b21\u4f18\u51b3\u7b56\u3002AI\u51b3\u7b56\u652f\u6301\u901a\u5e38\u5173\u6ce8\u7ed3\u679c\u4f18\u5316\uff0c\u800c\u5982\u4f55\u5728\u4e0d\u8fc7\u5ea6\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u641c\u7d22\u8fc7\u7a0b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5f15\u5165SERA\uff08\u57fa\u4e8eLLM\u7684\u52a9\u624b\uff09\uff0c\u63d0\u4f9b\u8981\u70b9\u5f0f\u6216\u9010\u5b57\u5f0f\u53cd\u9988\u3002\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff08\u540454\u4eba\uff09\uff0c\u5728\u4e09\u79cd\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u6bd4\u8f83SERA-Gist\u3001SERA-Verbatim\u548c\u65e0\u53cd\u9988\u57fa\u7ebf\uff0c\u8bc4\u4f30\u51b3\u7b56\u7ed3\u679c\u548c\u4fe1\u606f\u641c\u7d22\u884c\u4e3a\u3002\u4e0d\u786e\u5b9a\u6027\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u611f\u77e5\u6765\u64cd\u4f5c\u5316\uff1a\u9012\u51cf\u56de\u62a5\uff08\u4f4e\u4e0d\u786e\u5b9a\u6027\uff09\u3001\u5c40\u90e8\u6700\u4f18\uff08\u4e2d\u7b49\u4e0d\u786e\u5b9a\u6027\uff09\u3001\u65e0\u89c4\u5f8b\uff08\u9ad8\u4e0d\u786e\u5b9a\u6027\uff09\u3002", "result": "\u4f7f\u7528SERA\u652f\u6301\u7684\u4e2a\u4f53\u8868\u73b0\u51fa\u66f4\u51c6\u786e\u7684\u51b3\u7b56\u7ed3\u679c\u548c\u66f4\u9ad8\u7684\u4fe1\u5fc3\uff0c\u7279\u522b\u662f\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u3002\u8981\u70b9\u5f0f\u53cd\u9988\u4e0e\u66f4\u9ad8\u6548\u7684\u4fe1\u606f\u6574\u5408\u76f8\u5173\uff0c\u663e\u793a\u51fa\u51cf\u5c11\u8fc7\u5ea6\u91c7\u6837\u7684\u8d8b\u52bf\uff1b\u9010\u5b57\u5f0f\u53cd\u9988\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002", "conclusion": "\u53cd\u9988\u8868\u5f81\u662f\u641c\u7d22\u76f8\u5173\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u8bbe\u8ba1\u6760\u6746\uff0c\u6fc0\u52b1\u5f00\u53d1\u80fd\u591f\u6839\u636e\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5339\u914d\u53cd\u9988\u7c92\u5ea6\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u3002"}}
{"id": "2602.13759", "categories": ["cs.LG", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.13759", "abs": "https://arxiv.org/abs/2602.13759", "authors": ["ZhiMing Li", "JiaHe Feng"], "title": "Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition", "comment": "75 pages, 9 figures", "summary": "We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + \u03c3_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\\|C_k\\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $\u03c3_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $\u03b7_{max} \\propto 1/\\|C_e\\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\\|C_e\\|_2^2 / (\u0394^2 \u03b5))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\\log(1/\u03b6))$ saddle-escape rate and a high-probability finite-time convergence guarantee.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77e9\u9635\u81ea\u7531\u7279\u5f81\u5206\u89e3\u65b9\u6cd5\uff0c\u5728\u77e9\u9635\u5411\u91cf\u4e58\u79ef\uff08MVP\uff09\u9884\u8a00\u673a\u4e0b\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u53cc\u62ec\u53f7\u6d41\u6765\u6d88\u9664\u5404\u5411\u540c\u6027\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u4e86\u4ec5\u4f9d\u8d56\u4e8e\u8ff9\u81ea\u7531\u534f\u65b9\u5dee\u7684\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u4f20\u7edf\u968f\u673a\u903c\u8fd1\u65b9\u6cd5\u5728\u77e9\u9635\u7279\u5f81\u5206\u89e3\u4e2d\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u56fa\u5b9a\u6b65\u957f\u65b9\u6cd5\u5c06\u7a33\u5b9a\u6027\u4e0e\u534f\u65b9\u5dee\u77e9\u9635\u7684\u8c31\u8303\u6570\u8026\u5408\uff0c\u800c\u81ea\u9002\u5e94\u6b65\u957f\u65b9\u6cd5\u56e0\u66f4\u65b0\u6d88\u5931\u800c\u51cf\u6162\u6536\u655b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6d88\u9664\u5404\u5411\u540c\u6027\u566a\u58f0\u5f71\u54cd\u3001\u4ec5\u4f9d\u8d56\u4e8e\u8ff9\u81ea\u7531\u534f\u65b9\u5dee\u77e9\u9635\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u79bb\u6563\u53cc\u62ec\u53f7\u6d41\uff0c\u5176\u751f\u6210\u5668\u5bf9\u5404\u9879\u540c\u6027\u504f\u79fb\u5177\u6709\u4e0d\u53d8\u6027\uff0c\u4ece\u800c\u5728\u79bb\u6563\u65f6\u95f4\u5c42\u9762\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u03c3_k^2I\u9879\u7684\u8def\u5f84\u4e0d\u53d8\u6027\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u6700\u5927\u7a33\u5b9a\u6b65\u957f\u03b7_max \u221d 1/||C_e||_2^2\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u8ff9\u81ea\u7531\u534f\u65b9\u5deeC_e\u3002", "result": "\u5efa\u7acb\u4e86\u57fa\u4e8e\u4e25\u683c\u978d\u70b9\u51e0\u4f55\u7684\u5168\u5c40\u6536\u655b\u6027\u548c\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(||C_e||_2^2/(\u0394^2\u03b5))\u3002\u901a\u8fc7\u663e\u5f0f\u8868\u5f81\u9000\u5316\u5757\uff0c\u83b7\u5f97\u4e86\u52a0\u901f\u7684O(log(1/\u03b6))\u978d\u70b9\u9003\u9038\u7387\u548c\u9ad8\u6982\u7387\u6709\u9650\u65f6\u95f4\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u79bb\u6563\u53cc\u62ec\u53f7\u6d41\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6d88\u9664\u5404\u5411\u540c\u6027\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u8ff9\u81ea\u7531\u534f\u65b9\u5dee\uff0c\u5728\u77e9\u9635\u81ea\u7531\u7279\u5f81\u5206\u89e3\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2602.13900", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13900", "abs": "https://arxiv.org/abs/2602.13900", "authors": ["Aykut Kabaoglu", "Sanem Sariel"], "title": "UAV-SEAD: State Estimation Anomaly Dataset for UAVs", "comment": null, "summary": "Accurate state estimation in Unmanned Aerial Vehicles (UAVs) is crucial for ensuring reliable and safe operation, as anomalies occurring during mission execution may induce discrepancies between expected and observed system behaviors, thereby compromising mission success or posing potential safety hazards. It is essential to continuously monitor and detect such conditions in order to ensure a timely response and maintain system reliability. In this work, we focus on UAV state estimation anomalies and provide a large-scale real-world UAV dataset to facilitate research aimed at improving the development of anomaly detection. Unlike existing datasets that primarily rely on injected faults into simulated data, this dataset comprises 1396 real flight logs totaling over 52 hours of flight time, collected across diverse indoor and outdoor environments using a collection of PX4-based UAVs equipped with a variety of sensor configurations. The dataset comprises both normal and anomalous flights without synthetic manipulation, making it uniquely suitable for realistic anomaly detection tasks. A structured classification is proposed that categorizes UAV state estimation anomalies into four classes: mechanical and electrical, external position, global position, and altitude anomalies. These classifications reflect collective, contextual, and outlier anomalies observed in multivariate sensor data streams, including IMU, GPS, barometer, magnetometer, distance sensors, visual odometry, and optical flow, that can be found in the PX4 logging mechanism. It is anticipated that this dataset will play a key role in the development, training, and evaluation of anomaly detection and isolation systems to address the critical gap in UAV reliability research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff0c\u5305\u542b1396\u4e2a\u98de\u884c\u65e5\u5fd7\uff08\u8d85\u8fc752\u5c0f\u65f6\u98de\u884c\u65f6\u95f4\uff09\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u72b6\u6001\u4f30\u8ba1\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u4f9d\u8d56\u6a21\u62df\u6ce8\u5165\u6545\u969c\u7684\u7a7a\u767d\u3002", "motivation": "\u65e0\u4eba\u673a\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u5bf9\u53ef\u9760\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5f02\u5e38\u53ef\u80fd\u5bfc\u81f4\u9884\u671f\u4e0e\u5b9e\u9645\u7cfb\u7edf\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u5371\u53ca\u4efb\u52a1\u6210\u529f\u6216\u5b89\u5168\u3002\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u6ce8\u5165\u6545\u969c\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u3002", "method": "\u6536\u96c6\u4e861396\u4e2a\u771f\u5b9e\u98de\u884c\u65e5\u5fd7\uff08\u8d85\u8fc752\u5c0f\u65f6\u98de\u884c\u65f6\u95f4\uff09\uff0c\u4f7f\u7528\u57fa\u4e8ePX4\u7684\u65e0\u4eba\u673a\u5728\u4e0d\u540c\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u91c7\u96c6\uff0c\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u914d\u7f6e\u3002\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c06\u65e0\u4eba\u673a\u72b6\u6001\u4f30\u8ba1\u5f02\u5e38\u5206\u4e3a\u56db\u7c7b\uff1a\u673a\u68b0\u7535\u6c14\u5f02\u5e38\u3001\u5916\u90e8\u4f4d\u7f6e\u5f02\u5e38\u3001\u5168\u5c40\u4f4d\u7f6e\u5f02\u5e38\u548c\u9ad8\u5ea6\u5f02\u5e38\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u6b63\u5e38\u548c\u5f02\u5e38\u98de\u884c\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff0c\u65e0\u5408\u6210\u64cd\u7eb5\uff0c\u9002\u5408\u73b0\u5b9e\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002\u6570\u636e\u96c6\u5305\u542b\u591a\u5143\u4f20\u611f\u5668\u6570\u636e\u6d41\uff08IMU\u3001GPS\u3001\u6c14\u538b\u8ba1\u3001\u78c1\u529b\u8ba1\u3001\u8ddd\u79bb\u4f20\u611f\u5668\u3001\u89c6\u89c9\u91cc\u7a0b\u8ba1\u3001\u5149\u6d41\u7b49\uff09\uff0c\u53cd\u6620\u4e86\u96c6\u4f53\u3001\u4e0a\u4e0b\u6587\u548c\u79bb\u7fa4\u5f02\u5e38\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u5728\u5f02\u5e38\u68c0\u6d4b\u548c\u9694\u79bb\u7cfb\u7edf\u7684\u5f00\u53d1\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u53ef\u9760\u6027\u7814\u7a76\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4fc3\u8fdb\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.13773", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13773", "abs": "https://arxiv.org/abs/2602.13773", "authors": ["Youwei Shu", "Shaomian Zheng", "Dingnan Jin", "Wenjie Qu", "Ziyao Guo", "Qing Cui", "Jun Zhou", "Jiaheng Zhang"], "title": "On Representation Redundancy in Large-Scale Instruction Tuning Data Selection", "comment": null, "summary": "Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.", "AI": {"tldr": "CRDS\u6846\u67b6\u901a\u8fc7\u538b\u7f29\u8bed\u4e49\u8868\u793a\u51cf\u5c11\u5197\u4f59\uff0c\u63d0\u5347\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u9009\u62e9\u8d28\u91cf\uff0c\u4ec5\u75283.5%\u6570\u636e\u5373\u53ef\u8d85\u8d8a\u5168\u6570\u636e\u57fa\u7ebf", "motivation": "\u73b0\u6709LLM\u7f16\u7801\u5668\u4ea7\u751f\u7684\u8bed\u4e49\u5d4c\u5165\u9ad8\u5ea6\u5197\u4f59\uff0c\u800c\u5de5\u4e1a\u7ea7\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u9700\u8981\u89e3\u51b3\u8868\u793a\u5197\u4f59\u95ee\u9898\u6765\u63d0\u5347\u6570\u636e\u9009\u62e9\u8d28\u91cf", "method": "\u63d0\u51faCRDS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u53d8\u4f53\uff1aCRDS-R\u4f7f\u7528Rademacher\u968f\u673a\u6295\u5f71\u5e76\u62fc\u63a5transformer\u9690\u85cf\u5c42\u8868\u793a\uff1bCRDS-W\u91c7\u7528\u767d\u5316\u964d\u7ef4\u6280\u672f\u63d0\u5347\u8868\u793a\u8d28\u91cf", "result": "\u4e24\u79cd\u53d8\u4f53\u5747\u663e\u8457\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u4e8e\u8868\u793a\u7684\u9009\u62e9\u65b9\u6cd5\u3002CRDS-W\u4ec5\u75283.5%\u6570\u636e\u5c31\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8d85\u8d8a\u5168\u6570\u636e\u57fa\u7ebf0.71%", "conclusion": "\u901a\u8fc7\u538b\u7f29\u8bed\u4e49\u8868\u793a\u51cf\u5c11\u5197\u4f59\u80fd\u6709\u6548\u63d0\u5347\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u9009\u62e9\u6548\u679c\uff0cCRDS\u6846\u67b6\u4e3a\u5de5\u4e1a\u7ea7\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.13909", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13909", "abs": "https://arxiv.org/abs/2602.13909", "authors": ["Alfonso Mart\u00ednez-Petersen", "Levin Gerdes", "David Rodr\u00edguez-Mart\u00ednez", "C. J. P\u00e9rez-del-Pulgar"], "title": "High-fidelity 3D reconstruction for planetary exploration", "comment": "7 pages, 3 figures, conference paper", "summary": "Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) provide geometric consistency but struggle to capture radiometric detail or to scale efficiently in unstructured, low-texture terrains typical of extraterrestrial environments. This work explores the integration of radiance field-based methods - specifically Neural Radiance Fields (NeRF) and Gaussian Splatting - into a unified, automated environment reconstruction pipeline for planetary robotics. Our system combines the Nerfstudio and COLMAP frameworks with a ROS2-compatible workflow capable of processing raw rover data directly from rosbag recordings. This approach enables the generation of dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems operating in planetary-like conditions. The resulting pipeline established a foundation for future research in radiance field-based mapping, bridging the gap between geometric and neural representations in planetary exploration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8f90\u5c04\u573a\u65b9\u6cd5\uff08NeRF\u548c\u9ad8\u65af\u6cfc\u6e85\uff09\u96c6\u6210\u5230\u884c\u661f\u673a\u5668\u4eba\u73af\u5883\u91cd\u5efa\u7ba1\u9053\u4e2d\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u5c11\u91cf\u89c6\u89c9\u8f93\u5165\u751f\u6210\u5bc6\u96c6\u3001\u903c\u771f\u4e14\u5ea6\u91cf\u4e00\u81f4\u76843D\u8868\u793a\u3002", "motivation": "\u884c\u661f\u63a2\u7d22\u8d8a\u6765\u8d8a\u4f9d\u8d56\u80fd\u591f\u5728\u6ca1\u6709\u5168\u7403\u5b9a\u4f4d\u6216\u4e0e\u5730\u7403\u5b9e\u65f6\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u611f\u77e5\u3001\u89e3\u91ca\u548c\u91cd\u5efa\u73af\u5883\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u3002\u4f20\u7edf\u57fa\u4e8eSfM\u548cSLAM\u7684\u6280\u672f\u5728\u6355\u6349\u8f90\u5c04\u5ea6\u7ec6\u8282\u6216\u5728\u5178\u578b\u5730\u5916\u73af\u5883\u7684\u975e\u7ed3\u6784\u5316\u3001\u4f4e\u7eb9\u7406\u5730\u5f62\u4e2d\u9ad8\u6548\u6269\u5c55\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u5c06\u8f90\u5c04\u573a\u65b9\u6cd5\uff08NeRF\u548c\u9ad8\u65af\u6cfc\u6e85\uff09\u96c6\u6210\u5230\u7edf\u4e00\u7684\u81ea\u52a8\u5316\u73af\u5883\u91cd\u5efa\u7ba1\u9053\u4e2d\uff0c\u7ed3\u5408Nerfstudio\u548cCOLMAP\u6846\u67b6\uff0c\u91c7\u7528ROS2\u517c\u5bb9\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u76f4\u63a5\u5904\u7406\u6765\u81earosbag\u8bb0\u5f55\u7684\u539f\u59cb\u6f2b\u6e38\u8f66\u6570\u636e\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u4ece\u5c11\u91cf\u89c6\u89c9\u8f93\u5165\u751f\u6210\u5bc6\u96c6\u3001\u903c\u771f\u4e14\u5ea6\u91cf\u4e00\u81f4\u76843D\u8868\u793a\uff0c\u652f\u6301\u5728\u7c7b\u884c\u661f\u6761\u4ef6\u4e0b\u81ea\u4e3b\u7cfb\u7edf\u7684\u6539\u8fdb\u611f\u77e5\u548c\u89c4\u5212\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u8f90\u5c04\u573a\u5efa\u56fe\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f25\u5408\u4e86\u884c\u661f\u63a2\u7d22\u4e2d\u51e0\u4f55\u8868\u793a\u4e0e\u795e\u7ecf\u8868\u793a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.13783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13783", "abs": "https://arxiv.org/abs/2602.13783", "authors": ["Xiaoyun Yu", "Li fan", "Xiangfei Qiu", "Nanqing Dong", "Yonggui Huang", "Honggang Qi", "Geguang Pu", "Wanli Ouyang", "Xi Chen", "Jilin Hu"], "title": "MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models", "comment": null, "summary": "While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.", "AI": {"tldr": "MEMTS\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7684\u68c0\u7d22\u81ea\u7531\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u6301\u4e45\u5316\u6a21\u5757\u5c06\u9886\u57df\u7279\u5b9a\u65f6\u95f4\u52a8\u6001\u5185\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u539f\u578b\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u90e8\u7f72\u65f6\u7684\u5206\u5e03\u504f\u79fb\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u901a\u7528\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b9e\u9645\u5782\u76f4\u9886\u57df\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u65f6\u95f4\u5206\u5e03\u504f\u79fb\u548c\u9886\u57df\u7279\u5b9a\u5468\u671f\u6027\u7ed3\u6784\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u6216\u68c0\u7d22\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6d41\u5904\u7406\u7684\u9ad8\u6548\u8981\u6c42\u3002", "method": "\u63d0\u51faMEMTS\u65b9\u6cd5\uff0c\u6838\u5fc3\u662f\u77e5\u8bc6\u6301\u4e45\u5316\u6a21\u5757\uff0c\u5c06\u9886\u57df\u7279\u5b9a\u7684\u65f6\u95f4\u52a8\u6001\uff08\u5982\u5468\u671f\u6027\u6a21\u5f0f\u548c\u8d8b\u52bf\uff09\u5185\u5316\u4e3a\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u6f5c\u5728\u539f\u578b\u96c6\u5408\uff0c\u5c06\u788e\u7247\u5316\u7684\u5386\u53f2\u89c2\u6d4b\u8f6c\u5316\u4e3a\u8fde\u7eed\u3001\u53c2\u6570\u5316\u7684\u77e5\u8bc6\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u51bb\u7ed3\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMEMTS\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4ee5\u6052\u5b9a\u65f6\u95f4\u63a8\u7406\u548c\u63a5\u8fd1\u96f6\u5ef6\u8fdf\u5b9e\u73b0\u51c6\u786e\u7684\u57df\u9002\u5e94\uff0c\u540c\u65f6\u6709\u6548\u7f13\u89e3\u5bf9\u901a\u7528\u65f6\u95f4\u6a21\u5f0f\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "MEMTS\u901a\u8fc7\u77e5\u8bc6\u6301\u4e45\u5316\u6a21\u5757\u5b9e\u73b0\u4e86\u68c0\u7d22\u81ea\u7531\u7684\u57df\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13932", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13932", "abs": "https://arxiv.org/abs/2602.13932", "authors": ["Omer Daube", "Oren Salzman"], "title": "Joint Task Assistance Planning via Nested Branch and Bound (Extended Version)", "comment": null, "summary": "We introduce and study the Joint Task Assistance Planning problem which generalizes prior work on optimizing assistance in robotic collaboration. In this setting, two robots operate over predefined roadmaps, each represented as a graph corresponding to its configuration space. One robot, the task robot, must execute a timed mission, while the other, the assistance robot, provides sensor-based support that depends on their spatial relationship. The objective is to compute a path for both robots that maximizes the total duration of assistance given. Solving this problem is challenging due to the combinatorial explosion of possible path combinations together with the temporal nature of the problem (time needs to be accounted for as well). To address this, we propose a nested branch-and-bound framework that efficiently explores the space of robot paths in a hierarchical manner. We empirically evaluate our algorithm and demonstrate a speedup of up to two orders of magnitude when compared to a baseline approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u7814\u7a76\u4e86\u8054\u5408\u4efb\u52a1\u8f85\u52a9\u89c4\u5212\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u63a8\u5e7f\u4e86\u5148\u524d\u5173\u4e8e\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u4f18\u5316\u8f85\u52a9\u7684\u5de5\u4f5c\u3002\u4e24\u4e2a\u673a\u5668\u4eba\u5728\u9884\u5b9a\u4e49\u7684\u8def\u56fe\u4e0a\u64cd\u4f5c\uff0c\u6bcf\u4e2a\u8def\u56fe\u5bf9\u5e94\u5176\u914d\u7f6e\u7a7a\u95f4\u3002\u4efb\u52a1\u673a\u5668\u4eba\u5fc5\u987b\u6267\u884c\u5b9a\u65f6\u4efb\u52a1\uff0c\u800c\u8f85\u52a9\u673a\u5668\u4eba\u63d0\u4f9b\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u652f\u6301\u3002\u76ee\u6807\u662f\u8ba1\u7b97\u4e24\u6761\u673a\u5668\u4eba\u7684\u8def\u5f84\uff0c\u6700\u5927\u5316\u603b\u8f85\u52a9\u65f6\u95f4\u3002\u7531\u4e8e\u8def\u5f84\u7ec4\u5408\u7684\u7ec4\u5408\u7206\u70b8\u548c\u65f6\u95f4\u7279\u6027\uff0c\u8be5\u95ee\u9898\u5177\u6709\u6311\u6218\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u4f18\u5316\u8f85\u52a9\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5047\u8bbe\u8f85\u52a9\u673a\u5668\u4eba\u59cb\u7ec8\u53ef\u7528\u6216\u8f85\u52a9\u662f\u77ac\u65f6\u7684\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8f85\u52a9\u673a\u5668\u4eba\u7684\u652f\u6301\u53d6\u51b3\u4e8e\u5176\u4e0e\u4efb\u52a1\u673a\u5668\u4eba\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u4e14\u9700\u8981\u65f6\u95f4\u534f\u8c03\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u66f4\u901a\u7528\u7684\u8054\u5408\u4efb\u52a1\u8f85\u52a9\u89c4\u5212\u95ee\u9898\uff0c\u8003\u8651\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u95f4\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u5d4c\u5957\u5206\u652f\u5b9a\u754c\u6846\u67b6\uff0c\u4ee5\u5206\u5c42\u65b9\u5f0f\u9ad8\u6548\u63a2\u7d22\u673a\u5668\u4eba\u8def\u5f84\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u5904\u7406\u8def\u5f84\u7ec4\u5408\u7684\u7ec4\u5408\u7206\u70b8\u548c\u65f6\u95f4\u7279\u6027\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u641c\u7d22\u7b56\u7565\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002\u8fd9\u8bc1\u660e\u4e86\u5d4c\u5957\u5206\u652f\u5b9a\u754c\u6846\u67b6\u5728\u5904\u7406\u8054\u5408\u4efb\u52a1\u8f85\u52a9\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u89e3\u51b3\u4e86\u8054\u5408\u4efb\u52a1\u8f85\u52a9\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u5d4c\u5957\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u4f18\u5316\u8f85\u52a9\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002"}}
{"id": "2602.13977", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13977", "abs": "https://arxiv.org/abs/2602.13977", "authors": ["Zhennan Jiang", "Shangqing Zhou", "Yutong Jiang", "Zefang Huang", "Mingjie Wei", "Yuhui Chen", "Tianxing Zhou", "Zhen Guo", "Hao Lin", "Quanlu Zhang", "Yu Wang", "Haoran Li", "Chao Yu", "Dongbin Zhao"], "title": "WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL", "comment": "21pages, 8 figures", "summary": "Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.", "AI": {"tldr": "WoVR\u6846\u67b6\u901a\u8fc7\u63a7\u5236\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u4e16\u754c\u6a21\u578b\u3001\u5173\u952e\u5e27\u521d\u59cb\u5316\u6eda\u52a8\u548c\u4e16\u754c\u6a21\u578b-\u7b56\u7565\u534f\u540c\u8fdb\u5316\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e7b\u89c9\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u6709\u671b\u8d85\u8d8a\u6a21\u4eff\u5b66\u4e60\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u89e3\u9501\u66f4\u591a\u80fd\u529b\uff0c\u4f46\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u7684\u76f4\u63a5\u90e8\u7f72\u9700\u8981\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u7684\u6a21\u62df\u5668\uff0c\u4f46\u95ed\u73af\u60f3\u8c61\u6eda\u52a8\u4f1a\u906d\u53d7\u5e7b\u89c9\u548c\u957f\u65f6\u57df\u8bef\u5dee\u7d2f\u79ef\uff0c\u8fd9\u4e9b\u8bef\u5dee\u4e0d\u4ec5\u964d\u4f4e\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u8fd8\u4f1a\u7834\u574f\u4f18\u5316\u4fe1\u53f7\uff0c\u5bfc\u81f4\u7b56\u7565\u5229\u7528\u6a21\u578b\u4e0d\u51c6\u786e\u6027\u800c\u975e\u771f\u6b63\u7684\u4efb\u52a1\u8fdb\u5c55\u3002", "method": "WoVR\u6846\u67b6\u660e\u786e\u8c03\u8282RL\u4e0e\u4e0d\u5b8c\u7f8e\u60f3\u8c61\u52a8\u6001\u7684\u4ea4\u4e92\uff1a1\uff09\u901a\u8fc7\u53ef\u63a7\u7684\u52a8\u4f5c\u6761\u4ef6\u89c6\u9891\u4e16\u754c\u6a21\u578b\u63d0\u9ad8\u6eda\u52a8\u7a33\u5b9a\u6027\uff1b2\uff09\u901a\u8fc7\u5173\u952e\u5e27\u521d\u59cb\u5316\u6eda\u52a8\u51cf\u5c11\u6709\u6548\u8bef\u5dee\u6df1\u5ea6\uff1b3\uff09\u901a\u8fc7\u4e16\u754c\u6a21\u578b-\u7b56\u7565\u534f\u540c\u8fdb\u5316\u4fdd\u6301\u7b56\u7565-\u6a21\u62df\u5668\u5bf9\u9f50\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u5b9e\u9a8c\u4e2d\uff0cWoVR\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u957f\u65f6\u57df\u60f3\u8c61\u6eda\u52a8\u548c\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316\uff1aLIBERO\u5e73\u5747\u6210\u529f\u7387\u4ece39.95%\u63d0\u5347\u81f369.2%\uff08+29.3\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u6210\u529f\u7387\u4ece61.7%\u63d0\u5347\u81f391.7%\uff08+30.0\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "\u5f53\u5e7b\u89c9\u88ab\u660e\u786e\u63a7\u5236\u65f6\uff0c\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u7528\u6a21\u62df\u5668\u3002WoVR\u6846\u67b6\u901a\u8fc7\u8c03\u8282RL\u4e0e\u4e0d\u5b8c\u7f8e\u60f3\u8c61\u52a8\u6001\u7684\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u7b56\u7565\u7684\u6027\u80fd\u3002"}}
{"id": "2602.13802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13802", "abs": "https://arxiv.org/abs/2602.13802", "authors": ["Xiaoyu Tao", "Mingyue Cheng", "Chuang Jiang", "Tian Gao", "Huanjian Zhang", "Yaguo Liu"], "title": "Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting", "comment": null, "summary": "Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.", "AI": {"tldr": "Cast-R1\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bb0\u5fc6\u7684\u72b6\u6001\u7ba1\u7406\u673a\u5236\u548c\u5de5\u5177\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5b9e\u73b0\u8fed\u4ee3\u9884\u6d4b\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u4e2d\u5fc3\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u9884\u6d4b\u89c6\u4e3a\u4ece\u5386\u53f2\u89c2\u6d4b\u5230\u672a\u6765\u503c\u7684\u5355\u6b21\u6620\u5c04\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u548c\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u83b7\u53d6\u4fe1\u606f\u3001\u63a8\u7406\u672a\u6765\u53d8\u5316\u6216\u901a\u8fc7\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\u4fee\u6b63\u9884\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faCast-R1\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u3002\u5f15\u5165\u57fa\u4e8e\u8bb0\u5fc6\u7684\u72b6\u6001\u7ba1\u7406\u673a\u5236\uff0c\u7ef4\u62a4\u8de8\u4ea4\u4e92\u6b65\u9aa4\u7684\u51b3\u7b56\u76f8\u5173\u4fe1\u606f\uff1b\u91c7\u7528\u5de5\u5177\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u81ea\u4e3b\u4e0e\u6a21\u5757\u5316\u5de5\u5177\u5305\u4ea4\u4e92\uff0c\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\u3001\u8c03\u7528\u8f7b\u91cf\u7ea7\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u51b3\u7b56\u652f\u6301\u3001\u6267\u884c\u57fa\u4e8e\u63a8\u7406\u7684\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86Cast-R1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u7684\u667a\u80fd\u4f53\u8303\u5f0f\u63a2\u7d22\u63d0\u4f9b\u4e86\u5b9e\u9645\u6b65\u9aa4\uff0c\u5c55\u793a\u4e86\u5c06\u9884\u6d4b\u91cd\u6784\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.15011", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15011", "abs": "https://arxiv.org/abs/2602.15011", "authors": ["Eric Whitmire", "Evan Strasnick", "Roger Boldu", "Raj Sodhi", "Nathan Godwin", "Shiu Ng", "Andre Levi", "Amy Karlson", "Ran Tan", "Josef Faller", "Emrah Adamey", "Hanchuan Li", "Wolf Kienzle", "Hrvoje Benko"], "title": "TouchFusion: Multimodal Wristband Sensing for Ubiquitous Touch Interactions", "comment": "23 pages, 22 figures, accompanying video available at https://youtu.be/0fdCwHu7uaA", "summary": "TouchFusion is a wristband that enables touch interactions on nearby surfaces without any additional instrumentation or computer vision. TouchFusion combines surface electromyography (sEMG), bioimpedance, inertial, and optical sensing to capture multiple facets of hand activity during touch interactions. Through a combination of early and late fusion, TouchFusion enables stateful touch detection on both environmental and body surfaces, simple surface gestures, and tracking functionality for contextually adaptive interfaces as well as basic trackpad-like interactions. We validate our approach on a dataset of 100 participants, significantly exceeding the population size of typical wearable sensing studies to capture a wider variance of wrist anatomies, skin conductivities, and behavioral patterns. We show that TouchFusion can enable several common touch interaction tasks. Using TouchFusion, a wearer can summon a trackpad on any surface, control contextually adaptive interfaces based on where they tap, or use their palm as an always-available touch surface. When paired with smart glasses or augmented reality devices, TouchFusion enables a ubiquitous, contextually adaptive interaction model.", "AI": {"tldr": "TouchFusion\u662f\u4e00\u6b3e\u8155\u5e26\u8bbe\u5907\uff0c\u65e0\u9700\u989d\u5916\u4eea\u5668\u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u5373\u53ef\u5728\u9644\u8fd1\u8868\u9762\u4e0a\u5b9e\u73b0\u89e6\u6478\u4ea4\u4e92\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u652f\u6301\u72b6\u6001\u5316\u89e6\u6478\u68c0\u6d4b\u3001\u624b\u52bf\u8bc6\u522b\u548c\u8ddf\u8e2a\u529f\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u989d\u5916\u4eea\u5668\u6216\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u901a\u7528\u89e6\u6478\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u65e0\u5904\u4e0d\u5728\u3001\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u4ea4\u4e92\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4e0e\u667a\u80fd\u773c\u955c\u6216\u589e\u5f3a\u73b0\u5b9e\u8bbe\u5907\u914d\u5408\u4f7f\u7528\u65f6\u3002", "method": "\u7ed3\u5408\u8868\u9762\u808c\u7535\u56fe(sEMG)\u3001\u751f\u7269\u963b\u6297\u3001\u60ef\u6027\u548c\u5149\u5b66\u4f20\u611f\u6765\u6355\u6349\u624b\u90e8\u6d3b\u52a8\u7684\u591a\u4e2a\u65b9\u9762\uff0c\u901a\u8fc7\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u6280\u672f\u5b9e\u73b0\u72b6\u6001\u5316\u89e6\u6478\u68c0\u6d4b\u3001\u7b80\u5355\u8868\u9762\u624b\u52bf\u548c\u8ddf\u8e2a\u529f\u80fd\u3002", "result": "\u5728100\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u663e\u8457\u8d85\u8fc7\u5178\u578b\u53ef\u7a7f\u6234\u4f20\u611f\u7814\u7a76\u7684\u6837\u672c\u91cf\uff0c\u80fd\u591f\u652f\u6301\u591a\u79cd\u5e38\u89c1\u89e6\u6478\u4ea4\u4e92\u4efb\u52a1\uff0c\u5305\u62ec\u5728\u4efb\u4f55\u8868\u9762\u53ec\u5524\u89e6\u63a7\u677f\u3001\u57fa\u4e8e\u70b9\u51fb\u4f4d\u7f6e\u63a7\u5236\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u754c\u9762\uff0c\u4ee5\u53ca\u5c06\u624b\u638c\u4f5c\u4e3a\u59cb\u7ec8\u53ef\u7528\u7684\u89e6\u6478\u8868\u9762\u3002", "conclusion": "TouchFusion\u80fd\u591f\u5b9e\u73b0\u65e0\u5904\u4e0d\u5728\u3001\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u4ea4\u4e92\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4e0e\u667a\u80fd\u773c\u955c\u6216\u589e\u5f3a\u73b0\u5b9e\u8bbe\u5907\u914d\u5408\u4f7f\u7528\u65f6\uff0c\u4e3a\u53ef\u7a7f\u6234\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.13999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13999", "abs": "https://arxiv.org/abs/2602.13999", "authors": ["Haozheng Xu", "Wenhao Li", "Zifan Wei", "Bo Jin", "Hongxing Bai", "Ben Yang", "Xiangfeng Wang"], "title": "It Takes Two to Tango: A Holistic Simulator for Joint Order Scheduling and Multi-Agent Path Finding in Robotic Warehouses", "comment": null, "summary": "The prevailing paradigm in Robotic Mobile Fulfillment Systems (RMFS) typically treats order scheduling and multi-agent pathfinding as isolated sub-problems. We argue that this decoupling is a fundamental bottleneck, masking the critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap, often abstracting away heterogeneous kinematics and stochastic execution failures. We propose WareRover, a holistic simulation platform that enforces a tight coupling between OS and MAPF via a unified, closed-loop optimization interface. Unlike standard benchmarks, WareRover integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop. Experiments reveal that SOTA algorithms often falter under these realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination. The project and video is available at https://hhh-x.github.io/WareRover/.", "AI": {"tldr": "WareRover\u662f\u4e00\u4e2a\u65b0\u578b\u673a\u5668\u4eba\u79fb\u52a8\u5c65\u884c\u7cfb\u7edf\u4eff\u771f\u5e73\u53f0\uff0c\u901a\u8fc7\u7edf\u4e00\u95ed\u73af\u4f18\u5316\u63a5\u53e3\u5c06\u8ba2\u5355\u8c03\u5ea6\u548c\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7d27\u5bc6\u8026\u5408\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5c06\u4e24\u8005\u5206\u79bb\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RMFS\u7cfb\u7edf\u901a\u5e38\u5c06\u8ba2\u5355\u8c03\u5ea6\u548c\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4f5c\u4e3a\u72ec\u7acb\u5b50\u95ee\u9898\u5904\u7406\uff0c\u8fd9\u79cd\u89e3\u8026\u63a9\u76d6\u4e86\u9ad8\u5c42\u8c03\u5ea6\u4e0e\u5e95\u5c42\u62e5\u5835\u4e4b\u95f4\u7684\u5173\u952e\u4f9d\u8d56\u5173\u7cfb\u3002\u73b0\u6709\u4eff\u771f\u5668\u65e0\u6cd5\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5e38\u5e38\u5ffd\u7565\u5f02\u6784\u8fd0\u52a8\u5b66\u548c\u968f\u673a\u6267\u884c\u6545\u969c\u3002", "method": "\u63d0\u51faWareRover\u5e73\u53f0\uff0c\u901a\u8fc7\u7edf\u4e00\u95ed\u73af\u4f18\u5316\u63a5\u53e3\u5b9e\u73b0OS\u548cMAPF\u7684\u7d27\u5bc6\u8026\u5408\u3002\u5e73\u53f0\u96c6\u6210\u4e86\u52a8\u6001\u8ba2\u5355\u6d41\u3001\u7269\u7406\u611f\u77e5\u8fd0\u52a8\u7ea6\u675f\u548c\u975e\u6807\u79f0\u6062\u590d\u673a\u5236\u5230\u4e00\u4e2a\u5355\u4e00\u8bc4\u4f30\u5faa\u73af\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u5728\u8fd9\u4e9b\u73b0\u5b9e\u8026\u5408\u7ea6\u675f\u4e0b\u5e38\u5e38\u8868\u73b0\u4e0d\u4f73\uff0c\u8bc1\u660eWareRover\u4e3a\u7a33\u5065\u7684\u4e0b\u4e00\u4ee3\u4ed3\u5e93\u534f\u8c03\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5fc5\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "WareRover\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u8ba2\u5355\u8c03\u5ea6\u548c\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff0c\u4e3a\u4ed3\u5e93\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u771f\u5b9e\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u4eff\u771f\u73af\u5883\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u89e3\u8026\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.14032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14032", "abs": "https://arxiv.org/abs/2602.14032", "authors": ["Xinhua Wang", "Kun Wu", "Zhen Zhao", "Hu Cao", "Yinuo Zhao", "Zhiyuan Xu", "Meng Li", "Shichao Fan", "Di Wu", "Yixue Zhang", "Ning Liu", "Zhengping Che", "Jian Tang"], "title": "RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation", "comment": null, "summary": "Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.", "AI": {"tldr": "RoboAug\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u9700\u5355\u5f20\u56fe\u50cf\u7684\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u6570\u636e\u589e\u5f3a\uff0c\u7ed3\u5408\u533a\u57df\u5bf9\u6bd4\u635f\u5931\u63d0\u5347\u673a\u5668\u4eba\u5b66\u4e60\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u5728\u591a\u6837\u5316\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u662f\u4e00\u4e2a\u57fa\u7840\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u9884\u8bad\u7ec3\uff08\u52b3\u52a8\u5bc6\u96c6\u4e14\u8017\u65f6\uff09\u6216\u8bed\u4e49\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u9700\u8981\u5b8c\u7f8e\u4e0a\u6e38\u7269\u4f53\u68c0\u6d4b\u7684\u4e0d\u5207\u5b9e\u9645\u5047\u8bbe\uff09\u3002", "method": "\u63d0\u51faRoboAug\u6846\u67b6\uff1a1\uff09\u4ec5\u9700\u8bad\u7ec3\u671f\u95f4\u5355\u5f20\u56fe\u50cf\u7684\u8fb9\u754c\u6846\u6807\u6ce8\uff1b2\uff09\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u8bed\u4e49\u6570\u636e\u589e\u5f3a\uff1b3\uff09\u96c6\u6210\u5373\u63d2\u5373\u7528\u7684\u533a\u57df\u5bf9\u6bd4\u635f\u5931\uff0c\u5e2e\u52a9\u6a21\u578b\u805a\u7126\u4efb\u52a1\u76f8\u5173\u533a\u57df\u3002", "result": "\u5728\u4e09\u4e2a\u673a\u5668\u4eba\uff08UR-5e\u3001AgileX\u3001Tien Kung 2.0\uff09\u4e0a\u8fdb\u884c\u4e86\u8d85\u8fc735k\u6b21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u3002\u5728\u5305\u542b\u591a\u6837\u5316\u80cc\u666f\u3001\u5e72\u6270\u7269\u548c\u5149\u7167\u6761\u4ef6\u7684\u672a\u89c1\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u65e0\u589e\u5f3a\u57fa\u7ebf\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff1aUR-5e\u4ece0.09\u52300.47\uff0cAgileX\u4ece0.16\u52300.60\uff0cTien Kung 2.0\u4ece0.19\u52300.67\u3002", "conclusion": "RoboAug\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5b8c\u7f8e\u89c6\u89c9\u8bc6\u522b\u5047\u8bbe\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6807\u6ce8\u9700\u6c42\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2602.13810", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13810", "abs": "https://arxiv.org/abs/2602.13810", "authors": ["Guojian Zhan", "Letian Tao", "Pengcheng Wang", "Yixiao Wang", "Yiheng Li", "Yuxin Chen", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation", "comment": "ICLR Oral Presentation", "summary": "Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.", "AI": {"tldr": "MVP\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u7b56\u7565\u51fd\u6570\uff0c\u901a\u8fc7\u5efa\u6a21\u5e73\u5747\u901f\u5ea6\u573a\u5b9e\u73b0\u6700\u5feb\u7684\u4e00\u6b65\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u4fdd\u6301\u9ad8\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba1\u7b97\u8d1f\u62c5\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u901a\u5e38\u9700\u8981\u901a\u8fc7\u6d41\u6b65\u9aa4\u6570\u91cf\u6765\u63a7\u5236\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8868\u8fbe\u80fd\u529b\u53c8\u80fd\u5b9e\u73b0\u5feb\u901f\u52a8\u4f5c\u751f\u6210\u7684\u7b56\u7565\u51fd\u6570", "method": "\u63d0\u51fa\u5e73\u5747\u901f\u5ea6\u7b56\u7565\uff08MVP\uff09\uff0c\u5efa\u6a21\u5e73\u5747\u901f\u5ea6\u573a\u5b9e\u73b0\u4e00\u6b65\u52a8\u4f5c\u751f\u6210\u3002\u5f15\u5165\u77ac\u65f6\u901f\u5ea6\u7ea6\u675f\uff08IVC\uff09\u786e\u4fdd\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u8be5\u7ea6\u675f\u4f5c\u4e3a\u5173\u952e\u8fb9\u754c\u6761\u4ef6\u63d0\u5347\u5b66\u4e60\u51c6\u786e\u6027\u548c\u7b56\u7565\u8868\u8fbe\u80fd\u529b", "result": "\u5728Robomimic\u548cOGBench\u7684\u591a\u4e2a\u6311\u6218\u6027\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6210\u529f\u7387\u3002\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u57fa\u7ebf\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347", "conclusion": "MVP\u901a\u8fc7\u5efa\u6a21\u5e73\u5747\u901f\u5ea6\u573a\u548c\u5f15\u5165\u77ac\u65f6\u901f\u5ea6\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u4e8e\u6d41\u7b56\u7565\u5728\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u8868\u8fbe\u80fd\u529b\u7684\u7b56\u7565\u5b66\u4e60"}}
{"id": "2602.13283", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13283", "abs": "https://arxiv.org/abs/2602.13283", "authors": ["Gaston Besanson", "Federico Todeschini"], "title": "Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey", "comment": null, "summary": "We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define \"accuracy\" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u4eec\u5728\u5de5\u4f5c\u548c\u4e2a\u4eba\u751f\u6d3b\u4e2d\u5bf9AI\u5de5\u5177\u51c6\u786e\u6027\u7684\u8981\u6c42\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u5de5\u4f5c\u4e2d\u8981\u6c42\u9ad8\u51c6\u786e\u6027\u7684\u6bd4\u4f8b(24.1%)\u8fdc\u9ad8\u4e8e\u4e2a\u4eba\u751f\u6d3b(8.8%)\uff0c\u4e14\u5de5\u5177\u4e0d\u53ef\u7528\u65f6\u5bf9\u4e2a\u4eba\u751f\u6d3b\u7684\u5f71\u54cd\u66f4\u5927\u3002", "motivation": "\u7814\u7a76\u4eba\u4eec\u5728\u4e13\u4e1a\u548c\u4e2a\u4eba\u60c5\u5883\u4e0b\u4f7f\u7528AI\u5de5\u5177\u65f6\u5982\u4f55\u6743\u8861\u51c6\u786e\u6027\uff0c\u4e86\u89e3\u8fd9\u4e9b\u6743\u8861\u7684\u51b3\u5b9a\u56e0\u7d20\uff0c\u4ee5\u53ca\u5f53AI/\u5e94\u7528\u4e0d\u53ef\u7528\u65f6\u7528\u6237\u5982\u4f55\u5e94\u5bf9\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u8c03\u67e5(N=300)\u6536\u96c6\u6570\u636e\uff0c\u5c06\"\u51c6\u786e\u6027\"\u5b9a\u4e49\u4e3a\u60c5\u5883\u7279\u5b9a\u7684\u53ef\u9760\u6027\uff1a\u8f93\u51fa\u5728\u7279\u5b9a\u5bb9\u9519\u9608\u503c\u5185\u4e0e\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u7684\u7a0b\u5ea6\uff0c\u8be5\u9608\u503c\u53d6\u51b3\u4e8e\u98ce\u9669\u6c34\u5e73\u548c\u4fee\u6b63\u6210\u672c\u3002", "result": "\u5de5\u4f5c\u4e2d\u8981\u6c42\u9ad8\u51c6\u786e\u6027\u7684\u6bd4\u4f8b(24.1%)\u663e\u8457\u9ad8\u4e8e\u4e2a\u4eba\u751f\u6d3b(8.8%)\uff1b\u4f7f\u7528\u5927\u91cf\u5e94\u7528\u548c\u7ecf\u9a8c\u6a21\u5f0f\u4e0e\u66f4\u4e25\u683c\u7684\u5de5\u4f5c\u6807\u51c6\u76f8\u5173\uff1b\u5de5\u5177\u4e0d\u53ef\u7528\u65f6\uff0c\u5bf9\u4e2a\u4eba\u65e5\u5e38\u751f\u6d3b\u7684\u5e72\u6270(34.1%)\u5927\u4e8e\u5de5\u4f5c(15.3%)\u3002", "conclusion": "\u4eba\u4eec\u5bf9AI\u5de5\u5177\u51c6\u786e\u6027\u7684\u8981\u6c42\u56e0\u60c5\u5883\u800c\u5f02\uff0c\u5de5\u4f5c\u4e2d\u8981\u6c42\u66f4\u9ad8\u51c6\u786e\u6027\uff0c\u800c\u5de5\u5177\u4e0d\u53ef\u7528\u5bf9\u4e2a\u4eba\u751f\u6d3b\u7684\u5f71\u54cd\u66f4\u5927\uff0c\u8fd9\u4e3aAI\u7cfb\u7edf\u8bbe\u8ba1\u548c\u7528\u6237\u652f\u6301\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2602.13813", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13813", "abs": "https://arxiv.org/abs/2602.13813", "authors": ["Jorge Carrasco-Pollo", "Floor Eijkelboom", "Jan-Willem van de Meent"], "title": "Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference", "comment": null, "summary": "We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.", "AI": {"tldr": "Pawsterior\u662f\u4e00\u4e2a\u53d8\u5206\u6d41\u5339\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u548c\u6269\u5c55\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\uff0c\u7279\u522b\u9488\u5bf9\u5177\u6709\u7ed3\u6784\u5316\u57df\u7ea6\u675f\u7684\u540e\u9a8c\u5206\u5e03\u95ee\u9898\u3002", "motivation": "\u8bb8\u591aSBI\u95ee\u9898\u6d89\u53ca\u53d7\u7ed3\u6784\u5316\u57df\u7ea6\u675f\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5982\u6709\u754c\u7269\u7406\u53c2\u6570\u6216\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u53d8\u91cf\uff0c\u4f46\u6807\u51c6\u6d41\u5339\u914d\u65b9\u6cd5\u901a\u5e38\u5728\u65e0\u7ea6\u675f\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u7aef\u70b9\u8bf1\u5bfc\u7684\u4eff\u5c04\u51e0\u4f55\u7ea6\u675f\u539f\u7406\uff0c\u901a\u8fc7\u53cc\u9762\u53d8\u5206\u6a21\u578b\u5c06\u57df\u51e0\u4f55\u76f4\u63a5\u7eb3\u5165\u63a8\u7406\u8fc7\u7a0b\uff1b\u5f00\u53d1\u53d8\u5206\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u6d89\u53ca\u79bb\u6563\u6f5c\u7ed3\u6784\u7684SBI\u4efb\u52a1\u3002", "result": "\u5728\u6807\u51c6SBI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u5206\u7c7b\u5668\u53cc\u6837\u672c\u6d4b\u8bd5\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u91c7\u6837\u65f6\u7684\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u540e\u9a8c\u4fdd\u771f\u5ea6\uff1b\u80fd\u591f\u5904\u7406\u4f20\u7edf\u6d41\u5339\u914d\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u79bb\u6563\u6f5c\u7ed3\u6784\u95ee\u9898\u3002", "conclusion": "Pawsterior\u901a\u8fc7\u540c\u65f6\u89e3\u51b3\u51e0\u4f55\u7ea6\u675f\u548c\u79bb\u6563\u6f5c\u7ed3\u6784\u95ee\u9898\uff0c\u5c06\u6d41\u5339\u914d\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u7ed3\u6784\u5316SBI\u95ee\u9898\u7c7b\u522b\uff0c\u8fd9\u4e9b\u95ee\u9898\u662f\u4ee5\u524d\u65e0\u6cd5\u5904\u7406\u7684\u3002"}}
{"id": "2602.13848", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13848", "abs": "https://arxiv.org/abs/2602.13848", "authors": ["Shalev Shaer", "Yarin Bar", "Drew Prinster", "Yaniv Romano"], "title": "Testing For Distribution Shifts with Conditional Conformal Test Martingales", "comment": null, "summary": "We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.\n  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fa\u5b9a\u53c2\u8003\u96c6\u7684\u5e8f\u5217\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u4efb\u610f\u5206\u5e03\u504f\u79fb\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u6d4b\u8bd5\u6c61\u67d3\u95ee\u9898", "motivation": "\u73b0\u6709conformal test martingales (CTMs)\u65b9\u6cd5\u5728\u68c0\u6d4b\u5206\u5e03\u504f\u79fb\u65f6\u5b58\u5728\u6d4b\u8bd5\u6c61\u67d3\u95ee\u9898\uff1a\u504f\u79fb\u540e\u7684\u6837\u672c\u4f1a\u8fdb\u5165\u53c2\u8003\u96c6\uff0c\u7a00\u91ca\u504f\u79fb\u8bc1\u636e\uff0c\u589e\u52a0\u68c0\u6d4b\u5ef6\u8fdf\u5e76\u964d\u4f4e\u68c0\u6d4b\u80fd\u529b", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fa\u5b9a\u53c2\u8003\u96c6\u7684\u5e8f\u5217\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u65b0\u6837\u672c\u4e0e\u56fa\u5b9a\u96f6\u5047\u8bbe\u53c2\u8003\u6570\u636e\u96c6\u6765\u907f\u514d\u6c61\u67d3\u3002\u4e3b\u8981\u6280\u672f\u8d21\u732e\u662f\u6784\u5efa\u9c81\u68d2\u7684\u9a6c\u4e01\u683c\u5c14\uff0c\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u6709\u9650\u53c2\u8003\u96c6\u5f15\u8d77\u7684\u53c2\u8003\u5206\u5e03\u4f30\u8ba1\u8bef\u5dee\uff0c\u786e\u4fdd\u5728\u7ed9\u5b9a\u53c2\u8003\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027", "result": "\u65b9\u6cd5\u5177\u6709\u4efb\u610f\u65f6\u95f4\u6709\u6548\u7684\u7c7b\u578bI\u9519\u8bef\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u8bc1\u6e10\u8fd1\u529f\u6548\u4e3a1\u548c\u6709\u754c\u7684\u671f\u671b\u68c0\u6d4b\u5ef6\u8fdf\u3002\u5b9e\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6807\u51c6CTMs\u68c0\u6d4b\u504f\u79fb\u66f4\u5feb", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u53ef\u9760\u7684\u5206\u5e03\u504f\u79fb\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u907f\u514d\u6d4b\u8bd5\u6c61\u67d3\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u68c0\u6d4b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6027\u80fd"}}
{"id": "2602.13479", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13479", "abs": "https://arxiv.org/abs/2602.13479", "authors": ["Akhil Ramachandran", "Ankit Arun", "Ashish Shenoy", "Abhay Harpale", "Srihari Jayakumar", "Debojeet Chatterjee", "Mohsen Moslehpour", "Pierce Chuang", "Yichao Lu", "Vikas Bhardwaj", "Peyman Heidari"], "title": "GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables", "comment": null, "summary": "Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u6df7\u5408\u67b6\u6784\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u9ad8\u5206\u8fa8\u7387OCR\u548c\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u6d41\uff0c\u5728\u4fdd\u6301\u6587\u672c\u7406\u89e3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u529f\u8017\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u90e8\u7f72\u57fa\u4e8e\u6587\u672c\u7684\u89c6\u89c9\u95ee\u7b54\u9762\u4e34\u6839\u672c\u77db\u76fe\uff1a\u6587\u672c\u8bc6\u522b\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\uff0c\u4f46\u6d41\u5f0f\u4f20\u8f93\u9ad8\u8d28\u91cf\u89c6\u9891\u4f1a\u8017\u5c3d\u7535\u6c60\u5e76\u5bfc\u81f4\u70ed\u8282\u6d41\u3002\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u5b9e\u65f6\u6d41\u4e2d\u7684\u591a\u5e27\u6587\u672c\u65f6\u96be\u4ee5\u4fdd\u6301\u8fde\u8d2f\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002", "method": "\u5229\u7528\u6587\u672c\u8bc6\u522b\u548c\u89c6\u89c9\u63a8\u7406\u7684\u4e0d\u5bf9\u79f0\u5206\u8fa8\u7387\u9700\u6c42\uff0c\u63d0\u51fa\u6df7\u5408\u67b6\u6784\uff1a\u5728\u8bbe\u5907\u4e0a\u6267\u884c\u9009\u62e9\u6027\u9ad8\u5206\u8fa8\u7387OCR\uff0c\u540c\u65f6\u6d41\u5f0f\u4f20\u8f93\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u7528\u4e8e\u89c6\u89c9\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\u7684\u6587\u672cVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u8fbe\u523072%\u51c6\u786e\u7387\uff0c\u529f\u8017\u4ec5\u4e3a\u5168\u5206\u8fa8\u7387\u6d41\u5f0f\u4f20\u8f93\u76840.49\u500d\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u5b9e\u73b0\u6301\u7eed\u7684VQA\u4f1a\u8bdd\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5206\u8fa8\u7387\u9700\u6c42\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u67b6\u6784\u89e3\u51b3\u4e86\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u6587\u672cVQA\u7684\u529f\u8017\u4e0e\u6027\u80fd\u77db\u76fe\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u6301\u7eed\u89c6\u89c9\u7406\u89e3\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.14174", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14174", "abs": "https://arxiv.org/abs/2602.14174", "authors": ["Yifei Yang", "Anzhe Chen", "Zhenjie Zhu", "Kechun Xu", "Yunxuan Mao", "Yufei Wei", "Lu Chen", "Rong Xiong", "Yue Wang"], "title": "Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation", "comment": null, "summary": "Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy's directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u5411\u529b\u9884\u6d4b\u7684sim-to-real\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u63a5\u89e6\u529b\u65b9\u5411\u800c\u975e\u5927\u5c0f\u6765\u514b\u670d\u63a5\u89e6\u52a8\u529b\u5b66\u5dee\u5f02\uff0c\u7ed3\u5408\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63a7\u5236\u5668\u903b\u8f91\u5b9e\u73b0\u9c81\u68d2\u7684\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u63a5\u89e6\u5f0f\u64cd\u4f5c\u7684sim-to-real\u8fc1\u79fb\u9762\u4e34\u63a5\u89e6\u52a8\u529b\u5b66\u5dee\u5f02\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u771f\u5b9e\u6570\u636e\uff0c\u8981\u4e48\u4f7f\u7528\u56fa\u5b9a\u7684\u76f2\u4ece\u63a7\u5236\u5668\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u4eff\u771f\u4f18\u52bf\u53c8\u80fd\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u63a5\u89e6\u53d8\u5316\u7684\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\u7684\u4f4d\u7f6e/\u529b\u63a7\u5236\u5668\u5728\u4eff\u771f\u4e2d\u63d0\u4f9b\u7279\u6743\u76d1\u7763\uff0c\u8bad\u7ec3\u7b56\u7565\u9884\u6d4b\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u3001\u63a5\u89e6\u72b6\u6001\u548c\u5173\u952e\u7684\u65b9\u5411\u529b\u3002\u90e8\u7f72\u65f6\uff0c\u8fd9\u4e9b\u9884\u6d4b\u914d\u7f6e\u529b\u611f\u77e5\u5bfc\u7eb3\u63a7\u5236\u5668\uff0c\u5c06\u7b56\u7565\u7684\u65b9\u5411\u610f\u56fe\u4e0e\u624b\u52a8\u8c03\u6574\u7684\u6052\u5b9a\u529b\u5927\u5c0f\u7ed3\u5408\uff0c\u751f\u6210\u81ea\u9002\u5e94\u3001\u4efb\u52a1\u5bf9\u9f50\u7684\u987a\u4ece\u6027\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff08\u5fae\u6ce2\u7089\u5f00\u95e8\u3001\u5b54\u63d2\u9500\u3001\u767d\u677f\u64e6\u62ed\u3001\u95e8\u5f00\u95e8\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9884\u6d4b\u63a5\u89e6\u529b\u65b9\u5411\u800c\u975e\u5927\u5c0f\uff0c\u7ed3\u5408\u4e13\u5bb6\u63a7\u5236\u5668\u903b\u8f91\u548c\u8f7b\u91cf\u7ea7\u624b\u52a8\u8c03\u6574\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684sim-to-real\u8fc1\u79fb\uff0c\u4e3a\u63a5\u89e6\u5f0f\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13857", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.13857", "abs": "https://arxiv.org/abs/2602.13857", "authors": ["Weixuan Yuan", "Zengrui Jin", "Yichen Wang", "Donglin Xie", "Ziyi Ye", "Chao Zhang", "Xuesong Chen"], "title": "sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals", "comment": null, "summary": "Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \\texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \\texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \\textit{Demography, Age, Site \\& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \\texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.", "AI": {"tldr": "sleep2vec\u662f\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u591a\u6837\u5316\u4e14\u4e0d\u5b8c\u6574\u591c\u95f4\u751f\u7269\u4fe1\u53f7\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u5b66\u4e60\u5171\u4eab\u8868\u793a\uff0c\u5728\u7761\u7720\u5206\u671f\u548c\u4e34\u5e8a\u7ed3\u679c\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edf\u7761\u7720\u76d1\u6d4b\u8bbe\u5907\uff08\u5982PSG\u3001\u5e8a\u8fb9\u76d1\u6d4b\u4eea\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\uff09\u6355\u83b7\u591a\u79cd\u591c\u95f4\u751f\u7269\u4fe1\u53f7\uff0c\u4f46\u8bbe\u5907\u5f02\u8d28\u6027\u548c\u4f20\u611f\u5668\u9891\u7e41\u4e22\u5931\u7ed9\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u7edf\u4e00\u5efa\u6a21\u5e26\u6765\u6311\u6218", "method": "\u63d0\u51fasleep2vec\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u5b66\u4e60\u5171\u4eab\u8868\u793a\u3002\u4f7f\u7528\u5305\u542b\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5e74\u9f84\u3001\u5730\u70b9\u548c\u5386\u53f2\u4fe1\u606f\u7684InfoNCE\u76ee\u6807\u8fdb\u884c\u5bf9\u6bd4\u9884\u8bad\u7ec3\uff0c\u572842,249\u4e2a\u591c\u95f4\u8bb0\u5f55\u4e0a\u8bad\u7ec3\uff0c\u6db5\u76d69\u79cd\u6a21\u6001", "result": "sleep2vec\u5728\u4e0b\u6e38\u7761\u7720\u5206\u671f\u548c\u4e34\u5e8a\u7ed3\u679c\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5bf9\u4efb\u4f55\u53ef\u7528\u6a21\u6001\u5b50\u96c6\u548c\u4f20\u611f\u5668\u4e22\u5931\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u9996\u6b21\u63cf\u8ff0\u4e86\u591c\u95f4\u751f\u7269\u4fe1\u53f7\u5728\u6a21\u6001\u591a\u6837\u6027\u548c\u6a21\u578b\u5bb9\u91cf\u65b9\u9762\u7684\u7f29\u653e\u89c4\u5f8b", "conclusion": "\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u7ed3\u5408\u539f\u5219\u6027\u7f29\u653e\uff0c\u80fd\u591f\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u591c\u95f4\u751f\u7269\u4fe1\u53f7\u7684\u6807\u7b7e\u9ad8\u6548\u3001\u901a\u7528\u5efa\u6a21"}}
{"id": "2602.13653", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.13653", "abs": "https://arxiv.org/abs/2602.13653", "authors": ["Yibo Wang", "Guangda Huzhang", "Yuwei Hu", "Yu Xia", "Shiyin Lu", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang", "Lijun Zhang"], "title": "Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684GUI\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542bagentic-Q\u8bc4\u4f30\u548c\u9010\u6b65\u7b56\u7565\u4f18\u5316\uff0c\u964d\u4f4e\u6570\u636e\u6536\u96c6\u6210\u672c\u5e76\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2dGUI\u667a\u80fd\u4f53\u9762\u4e34\u975e\u5e73\u7a33\u73af\u5883\uff0c\u5bfc\u81f4\u6570\u636e\u6574\u7406\u548c\u7b56\u7565\u4f18\u5316\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) agentic-Q\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u9010\u6b65\u503c\u8bc4\u4f30\u52a8\u4f5c\u5bf9\u4efb\u52a1\u5b8c\u6210\u7684\u8d21\u732e\uff1b2) \u9010\u6b65\u7b56\u7565\u4f18\u5316\uff0c\u4ece\u72b6\u6001-\u52a8\u4f5c\u8f68\u8ff9\u91c7\u6837\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565", "result": "\u6846\u67b6\u4f7fOvis2.5-9B\u5177\u5907\u5f3a\u5927\u7684GUI\u4ea4\u4e92\u80fd\u529b\uff0c\u5728GUI\u5bfc\u822a\u548c\u57fa\u7840\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u7ade\u4e89\u5bf9\u624b", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GUI\u667a\u80fd\u4f53\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u8f68\u8ff9\u548c\u4e0e\u73af\u5883\u89e3\u8026\u7684\u7b56\u7565\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u4f18\u5316"}}
{"id": "2602.14193", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14193", "abs": "https://arxiv.org/abs/2602.14193", "authors": ["Yue Chen", "Muqing Jiang", "Kaifeng Zheng", "Jiaqi Liang", "Chenrui Tie", "Haoran Lu", "Ruihai Wu", "Hao Dong"], "title": "Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation", "comment": "Accept to ICLR 2026, Project page: https://pa3ff.github.io", "summary": "Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io", "AI": {"tldr": "PA3FF\u662f\u4e00\u79cd\u5177\u6709\u90e8\u4ef6\u611f\u77e5\u80fd\u529b\u76843D\u7279\u5f81\u573a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4ece\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u80fd\u591f\u4e3a\u53ef\u6cdb\u5316\u7684\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u8fde\u7eed3D\u7279\u5f81\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u67092D\u548c3D\u8868\u793a\u65b9\u6cd5\u3002", "motivation": "\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u5bf9\u673a\u5668\u4eba\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8de8\u4e0d\u540c\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9762\u4e34\u6311\u6218\u3002\u5173\u952e\u5728\u4e8e\u7406\u89e3\u529f\u80fd\u90e8\u4ef6\uff08\u5982\u95e8\u628a\u624b\u3001\u65cb\u94ae\uff09\uff0c\u8fd9\u4e9b\u90e8\u4ef6\u6307\u793a\u4e86\u8de8\u4e0d\u540c\u7269\u4f53\u7c7b\u522b\u548c\u5f62\u72b6\u7684\u64cd\u4f5c\u4f4d\u7f6e\u548c\u65b9\u5f0f\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e2D\u7279\u5f81\uff0c\u5728\u63d0\u5347\u52303D\u7a7a\u95f4\u65f6\u9762\u4e34\u8fd0\u884c\u65f6\u95f4\u957f\u3001\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u3001\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faPart-Aware 3D Feature Field (PA3FF)\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4ece\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u76843D\u90e8\u4ef6\u63d0\u8bae\u4e2d\u8bad\u7ec3\u5f97\u5230\u3002\u7ed9\u5b9a\u70b9\u4e91\u8f93\u5165\uff0cPA3FF\u4ee5\u524d\u9988\u65b9\u5f0f\u9884\u6d4b\u8fde\u7eed3D\u7279\u5f81\u573a\uff0c\u5176\u4e2d\u70b9\u7279\u5f81\u4e4b\u95f4\u7684\u8ddd\u79bb\u53cd\u6620\u4e86\u529f\u80fd\u90e8\u4ef6\u7684\u63a5\u8fd1\u7a0b\u5ea6\u3002\u57fa\u4e8e\u6b64\u7279\u5f81\uff0c\u8fdb\u4e00\u6b65\u63d0\u51faPart-Aware Diffusion Policy (PADP)\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8bc4\u4f30PADP\uff0c\u7ed3\u679c\u8868\u660ePA3FF\u5728\u64cd\u4f5c\u573a\u666f\u4e2d\u4e00\u81f4\u4f18\u4e8e\u4e00\u7cfb\u52172D\u548c3D\u8868\u793a\u65b9\u6cd5\uff0c\u5305\u62ecCLIP\u3001DINOv2\u548cGrounded-SAM\u3002PA3FF\u8fd8\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9\u5e94\u5173\u7cfb\u5b66\u4e60\u548c\u5206\u5272\u4efb\u52a1\u3002", "conclusion": "PA3FF\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u57fa\u7840\u8868\u793a\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5173\u8282\u7269\u4f53\u64cd\u4f5c\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u90e8\u4ef6\u611f\u77e5\u76843D\u7279\u5f81\u573a\u663e\u8457\u63d0\u5347\u4e86\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2602.13934", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13934", "abs": "https://arxiv.org/abs/2602.13934", "authors": ["Zhimin Zhao"], "title": "Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning", "comment": null, "summary": "Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u7ed3\u6784\u7684\u4e94\u7ea7\u53ef\u5b66\u4e60\u6027\u5c42\u6b21\uff0c\u89e3\u91ca\u4ee3\u7801\u751f\u6210\u6bd4\u5f3a\u5316\u5b66\u4e60\u66f4\u53ef\u9760\u7684\u539f\u56e0\u5728\u4e8e\u4ee3\u7801\u63d0\u4f9b\u5bc6\u96c6\u3001\u5c40\u90e8\u3001\u53ef\u9a8c\u8bc1\u7684\u53cd\u9988\uff0c\u800c\u53ef\u5b66\u4e60\u6027\u5929\u82b1\u677f\u66f4\u591a\u53d6\u51b3\u4e8e\u4efb\u52a1\u672c\u8eab\u800c\u975e\u6a21\u578b\u89c4\u6a21\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4ec0\u4e48\u4ee3\u7801\u751f\u6210\u6bd4\u5f3a\u5316\u5b66\u4e60\u8fdb\u5c55\u66f4\u53ef\u9760\uff0c\u63a2\u7d22\u4efb\u52a1\u53ef\u5b66\u4e60\u6027\u7684\u6839\u672c\u5dee\u5f02\uff0c\u6311\u6218\"\u4ec5\u9760\u6269\u5c55\u5c31\u80fd\u89e3\u51b3\u5269\u4f59ML\u6311\u6218\"\u7684\u5e38\u89c1\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u7ed3\u6784\u7684\u4e94\u7ea7\u53ef\u5b66\u4e60\u6027\u5c42\u6b21\uff0c\u5efa\u7acb\u8ba1\u7b97\u95ee\u9898\u7684\u4e09\u4e2a\u5c5e\u6027\uff08\u53ef\u8868\u8fbe\u6027\u3001\u53ef\u8ba1\u7b97\u6027\u3001\u53ef\u5b66\u4e60\u6027\uff09\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u533a\u5206\u548c\u5173\u7cfb\uff0c\u63d0\u4f9b\u7edf\u4e00\u6a21\u677f\u4f7f\u7ed3\u6784\u5dee\u5f02\u663e\u5f0f\u5316\u3002", "result": "\u5206\u6790\u8868\u660e\u4ee3\u7801\u7684\u76d1\u7763\u5b66\u4e60\u53ef\u9884\u6d4b\u6269\u5c55\u800c\u5f3a\u5316\u5b66\u4e60\u4e0d\u884c\uff0c\u56e0\u4e3a\u4ee3\u7801\u63d0\u4f9b\u5bc6\u96c6\u3001\u5c40\u90e8\u3001\u53ef\u9a8c\u8bc1\u7684\u9010\u4ee4\u724c\u53cd\u9988\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u53cd\u9988\u8d28\u91cf\u8f83\u4f4e\u3002", "conclusion": "ML\u8fdb\u5c55\u7684\u5929\u82b1\u677f\u66f4\u591a\u53d6\u51b3\u4e8e\u4efb\u52a1\u662f\u5426\u53ef\u5b66\u4e60\uff0c\u800c\u975e\u6a21\u578b\u89c4\u6a21\uff1b\u53ef\u5b66\u4e60\u6027\u5dee\u5f02\u662f\u6e10\u8fdb\u7684\u800c\u975e\u4e8c\u5143\u7684\uff1b\u5bf9\"\u4ec5\u9760\u6269\u5c55\u5c31\u80fd\u89e3\u51b3ML\u6311\u6218\"\u7684\u5047\u8bbe\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u3002"}}
{"id": "2602.14473", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14473", "abs": "https://arxiv.org/abs/2602.14473", "authors": ["Baixiao Huang", "Baiyu Huang", "Yu Hou"], "title": "Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots' Performance in U-Shaped Stair Climbing", "comment": "8 pages, 4 figures, International Conference on Computing in Civil Engineering (i3CE 2026)", "summary": "Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot's performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab's pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.", "AI": {"tldr": "\u4f7f\u7528\u4e24\u9636\u6bb5\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u56db\u8db3\u673a\u5668\u4eba\u81ea\u4e3b\u6500\u722cU\u578b\u697c\u68af\uff0c\u5e76\u9a8c\u8bc1\u7b56\u7565\u5728\u4e0d\u540c\u697c\u68af\u7c7b\u578b\u95f4\u7684\u53ef\u8fc1\u79fb\u6027", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u5efa\u7b51\u573a\u666f\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u81ea\u4e3b\u6500\u722c\u4e0d\u540c\u5ba4\u5185\u697c\u68af\u4ecd\u662f\u5b8c\u6210\u5efa\u7b51\u4efb\u52a1\u7684\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u673a\u5668\u4eba\u81ea\u4e3b\u6500\u722cU\u578b\u697c\u68af\u7684\u95ee\u9898", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a\u9996\u5148\u5728Isaac Lab\u7684\u91d1\u5b57\u5854\u697c\u68af\u5730\u5f62\u4e0a\u8bad\u7ec3Unitree Go2\u673a\u5668\u4eba\u6500\u722c\u697c\u68af\uff0c\u7136\u540e\u5229\u7528\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728U\u578b\u5ba4\u5185\u697c\u68af\u4e0a\u8fdb\u884c\u8bad\u7ec3", "result": "1) \u673a\u5668\u4eba\u5728\u6709\u505c\u6ede\u60e9\u7f5a\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u6500\u722cU\u578b\u697c\u68af\u5230\u8fbe\u76ee\u6807\uff1b2) \u5728U\u578b\u697c\u68af\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u53ef\u8fc1\u79fb\u5230\u76f4\u7ebf\u3001L\u578b\u548c\u87ba\u65cb\u697c\u68af\u5730\u5f62\uff0c\u5176\u4ed6\u697c\u68af\u6a21\u578b\u8bad\u7ec3\u7684\u7b56\u7565\u4e5f\u53ef\u8fc1\u79fb\u5230U\u578b\u5730\u5f62", "conclusion": "\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bad\u7ec3\u56db\u8db3\u673a\u5668\u4eba\u81ea\u4e3b\u6500\u722c\u590d\u6742\u697c\u68af\u7ed3\u6784\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5177\u6709\u826f\u597d\u7684\u8de8\u697c\u68af\u7c7b\u578b\u8fc1\u79fb\u80fd\u529b"}}
{"id": "2602.14255", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14255", "abs": "https://arxiv.org/abs/2602.14255", "authors": ["Daniel Ruan", "Salma Mozaffari", "Sigrid Adriaenssens", "Arash Adel"], "title": "A Latency-Aware Framework for Visuomotor Policy Learning on Industrial Robots", "comment": null, "summary": "Industrial robots are increasingly deployed in contact-rich construction and manufacturing tasks that involve uncertainty and long-horizon execution. While learning-based visuomotor policies offer a promising alternative to open-loop control, their deployment on industrial platforms is challenged by a large observation-execution gap caused by sensing, inference, and control latency. This gap is significantly greater than on low-latency research robots due to high-level interfaces and slower closed-loop dynamics, making execution timing a critical system-level issue. This paper presents a latency-aware framework for deploying and evaluating visuomotor policies on industrial robotic arms under realistic timing constraints. The framework integrates calibrated multimodal sensing, temporally consistent synchronization, a unified communication pipeline, and a teleoperation interface for demonstration collection. Within this framework, we introduce a latency-aware execution strategy that schedules finite-horizon, policy-predicted action sequences based on temporal feasibility, enabling asynchronous inference and execution without modifying policy architectures or training. We evaluate the framework on a contact-rich industrial assembly task while systematically varying inference latency. Using identical policies and sensing pipelines, we compare latency-aware execution with blocking and naive asynchronous baselines. Results show that latency-aware execution maintains smooth motion, compliant contact behavior, and consistent task progression across a wide range of latencies while reducing idle time and avoiding instability observed in baseline methods. These findings highlight the importance of explicitly handling latency for reliable closed-loop deployment of visuomotor policies on industrial robots.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u5de5\u4e1a\u673a\u5668\u4eba\u7684\u5ef6\u8fdf\u611f\u77e5\u6846\u67b6\uff0c\u89e3\u51b3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u90e8\u7f72\u4e2d\u7684\u89c2\u6d4b-\u6267\u884c\u5ef6\u8fdf\u95ee\u9898\uff0c\u901a\u8fc7\u5f02\u6b65\u63a8\u7406\u4e0e\u6267\u884c\u8c03\u5ea6\u63d0\u5347\u63a5\u89e6\u5f0f\u88c5\u914d\u4efb\u52a1\u7684\u53ef\u9760\u6027", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u5728\u63a5\u89e6\u5f0f\u5236\u9020\u4efb\u52a1\u4e2d\u9762\u4e34\u663e\u8457\u7684\u89c2\u6d4b-\u6267\u884c\u5ef6\u8fdf\u95ee\u9898\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u8fdc\u5927\u4e8e\u7814\u7a76\u673a\u5668\u4eba\uff0c\u5bfc\u81f4\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u5b9e\u9645\u90e8\u7f72\u65f6\u5b58\u5728\u7cfb\u7edf\u7ea7\u65f6\u5e8f\u6311\u6218", "method": "\u5f00\u53d1\u5305\u542b\u6807\u5b9a\u591a\u6a21\u6001\u611f\u77e5\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u540c\u6b65\u3001\u7edf\u4e00\u901a\u4fe1\u7ba1\u9053\u548c\u9065\u64cd\u4f5c\u754c\u9762\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u57fa\u4e8e\u65f6\u5e8f\u53ef\u884c\u6027\u7684\u5ef6\u8fdf\u611f\u77e5\u6267\u884c\u7b56\u7565\uff0c\u8c03\u5ea6\u6709\u9650\u65f6\u57df\u7684\u52a8\u4f5c\u5e8f\u5217", "result": "\u5728\u63a5\u89e6\u5f0f\u5de5\u4e1a\u88c5\u914d\u4efb\u52a1\u4e2d\uff0c\u5ef6\u8fdf\u611f\u77e5\u6267\u884c\u76f8\u6bd4\u963b\u585e\u5f0f\u548c\u6734\u7d20\u5f02\u6b65\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u5728\u5404\u79cd\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u4fdd\u6301\u5e73\u6ed1\u8fd0\u52a8\u3001\u987a\u5e94\u6027\u63a5\u89e6\u884c\u4e3a\u548c\u4e00\u81f4\u4efb\u52a1\u8fdb\u5c55\uff0c\u51cf\u5c11\u7a7a\u95f2\u65f6\u95f4\u5e76\u907f\u514d\u4e0d\u7a33\u5b9a\u6027", "conclusion": "\u660e\u786e\u5904\u7406\u5ef6\u8fdf\u5bf9\u4e8e\u5de5\u4e1a\u673a\u5668\u4eba\u4e0a\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u53ef\u9760\u95ed\u73af\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u5ef6\u8fdf\u611f\u77e5\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9\u5de5\u4e1a\u5e73\u53f0\u7279\u6709\u7684\u9ad8\u5ef6\u8fdf\u6311\u6218"}}
{"id": "2602.13937", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13937", "abs": "https://arxiv.org/abs/2602.13937", "authors": ["Dat Le", "Duc-Cuong Le", "Anh-Son Nguyen", "Tuan-Dung Bui", "Thu-Trang Nguyen", "Son Nguyen", "Hieu Dinh Vo"], "title": "A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning", "comment": null, "summary": "Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as \"black boxes\", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.", "AI": {"tldr": "iML\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684AutoML\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u5f15\u5bfc\u7684\u6a21\u5757\u5316\u67b6\u6784\u89e3\u51b3\u4f20\u7edfAutoML\u9ed1\u76d2\u95ee\u9898\u548cLLM\u667a\u80fd\u4f53\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u771f\u5b9eKaggle\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfAutoML\u6846\u67b6\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u900f\u660e\u5ea6\uff0c\u800c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5b58\u5728\u903b\u8f91\u5e7b\u89c9\u548c\u903b\u8f91\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd0\u884c\u65f6\u6545\u969c\u96be\u4ee5\u6062\u590d\uff0c\u9700\u8981\u66f4\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u7684AutoML\u67b6\u6784\u3002", "method": "iML\u91c7\u7528\u4e09\u6838\u5fc3\u8bbe\u8ba1\uff1a1) \u4ee3\u7801\u5f15\u5bfc\u89c4\u5212 - \u57fa\u4e8e\u81ea\u4e3b\u7ecf\u9a8c\u5206\u6790\u5236\u5b9a\u6218\u7565\u84dd\u56fe\u6d88\u9664\u5e7b\u89c9\uff1b2) \u4ee3\u7801\u6a21\u5757\u5316\u5b9e\u73b0 - \u5c06\u9884\u5904\u7406\u548c\u5efa\u6a21\u89e3\u8026\u4e3a\u4e13\u4e1a\u7ec4\u4ef6\uff0c\u53d7\u4e25\u683c\u63a5\u53e3\u5951\u7ea6\u7ea6\u675f\uff1b3) \u4ee3\u7801\u53ef\u9a8c\u8bc1\u96c6\u6210 - \u901a\u8fc7\u52a8\u6001\u5951\u7ea6\u9a8c\u8bc1\u548c\u8fed\u4ee3\u81ea\u4fee\u6b63\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u3002", "result": "\u5728MLE-BENCH\u4e0a\u8fbe\u523085%\u6709\u6548\u63d0\u4ea4\u7387\u548c45%\u7ade\u4e89\u5956\u724c\u7387\uff0cAPS\u4e3a0.77\uff1b\u5728iML-BENCH\u4e0a\u6bd4\u5176\u4ed6\u65b9\u6cd5\u63d0\u534738%-163%\u7684APS\uff1b\u5373\u4f7f\u5728\u7b80\u5316\u4efb\u52a1\u63cf\u8ff0\u4e0b\u4ecd\u4fdd\u630170%\u6210\u529f\u7387\u3002", "conclusion": "iML\u901a\u8fc7\u4ee3\u7801\u5f15\u5bfc\u7684\u6a21\u5757\u5316\u67b6\u6784\u6210\u529f\u5f25\u5408\u4e86\u968f\u673a\u751f\u6210\u4e0e\u53ef\u9760\u5de5\u7a0b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u7684AutoML\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.14287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14287", "abs": "https://arxiv.org/abs/2602.14287", "authors": ["Luca Beber", "Edoardo Lamon", "Matteo Saveriano", "Daniele Fontanelli", "Luigi Palopoli"], "title": "Autonomous Robotic Tissue Palpation and Abnormalities Characterisation via Ergodic Exploration", "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "We propose a novel autonomous robotic palpation framework for real-time elastic mapping during tissue exploration using a viscoelastic tissue model. The method combines force-based parameter estimation using a commercial force/torque sensor with an ergodic control strategy driven by a tailored Expected Information Density, which explicitly biases exploration toward diagnostically relevant regions by jointly considering model uncertainty, stiffness magnitude, and spatial gradients. An Extended Kalman Filter is employed to estimate viscoelastic model parameters online, while Gaussian Process Regression provides spatial modelling of the estimated elasticity, and a Heat Equation Driven Area Coverage controller enables adaptive, continuous trajectory planning. Simulations on synthetic stiffness maps demonstrate that the proposed approach achieves better reconstruction accuracy, enhanced segmentation capability, and improved robustness in detecting stiff inclusions compared to Bayesian Optimisation-based techniques. Experimental validation on a silicone phantom with embedded inclusions emulating pathological tissue regions further corroborates the potential of the method for autonomous tissue characterisation in diagnostic and screening applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u5f39\u6027\u6620\u5c04\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u89e6\u8bca\u6846\u67b6\uff0c\u7ed3\u5408\u529b\u57fa\u53c2\u6570\u4f30\u8ba1\u548c\u904d\u5386\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u671f\u671b\u4fe1\u606f\u5bc6\u5ea6\u5f15\u5bfc\u63a2\u7d22\u8bca\u65ad\u76f8\u5173\u533a\u57df\u3002", "motivation": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u7ec4\u7ec7\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u8fdb\u884c\u5f39\u6027\u6620\u5c04\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u89e6\u8bca\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u7ec4\u7ec7\u8868\u5f81\u5728\u8bca\u65ad\u548c\u7b5b\u67e5\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u7ed3\u5408\u5546\u4e1a\u529b/\u626d\u77e9\u4f20\u611f\u5668\u7684\u529b\u57fa\u53c2\u6570\u4f30\u8ba1\u4e0e\u904d\u5386\u63a7\u5236\u7b56\u7565\uff0c\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u7ebf\u4f30\u8ba1\u7c98\u5f39\u6027\u6a21\u578b\u53c2\u6570\uff0c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u63d0\u4f9b\u5f39\u6027\u7a7a\u95f4\u5efa\u6a21\uff0c\u70ed\u65b9\u7a0b\u9a71\u52a8\u533a\u57df\u8986\u76d6\u63a7\u5236\u5668\u5b9e\u73b0\u81ea\u9002\u5e94\u8fde\u7eed\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5728\u5408\u6210\u521a\u5ea6\u56fe\u4e0a\u7684\u4eff\u771f\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u91cd\u5efa\u7cbe\u5ea6\u3001\u589e\u5f3a\u7684\u5206\u5272\u80fd\u529b\u548c\u6539\u8fdb\u7684\u68c0\u6d4b\u786c\u6027\u5305\u6db5\u4f53\u7684\u9c81\u68d2\u6027\u3002\u5728\u6a21\u62df\u75c5\u7406\u7ec4\u7ec7\u533a\u57df\u7684\u7845\u80f6\u4f53\u6a21\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u81ea\u4e3b\u7ec4\u7ec7\u8868\u5f81\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u81ea\u4e3b\u673a\u5668\u4eba\u89e6\u8bca\u6846\u67b6\u5728\u5b9e\u65f6\u5f39\u6027\u6620\u5c04\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5728\u8bca\u65ad\u548c\u7b5b\u67e5\u5e94\u7528\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u7ec4\u7ec7\u8868\u5f81\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.13939", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13939", "abs": "https://arxiv.org/abs/2602.13939", "authors": ["Adolfo Gonz\u00e1lez", "V\u00edctor Parada"], "title": "An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations", "comment": "35 pages, 24 figures and Appendix", "summary": "Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.", "AI": {"tldr": "AHSIV\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6df7\u5408\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u95f4\u6b47\u6027\u548c\u9ad8\u53d8\u5f02\u6027\u9700\u6c42\u73af\u5883\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u8003\u8651\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u3001\u9700\u6c42\u7ed3\u6784\u548c\u591a\u76ee\u6807\u4f18\u5316\u6765\u63d0\u9ad8\u6a21\u578b\u9009\u62e9\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u7ed3\u6784\u6027\u9700\u6c42\u95f4\u6b47\u6027\u3001\u9ad8\u53d8\u5f02\u6027\u548c\u591a\u6b65\u89c4\u5212\u7684\u4e1a\u52a1\u73af\u5883\u4e2d\uff0c\u6ca1\u6709\u5355\u4e00\u7684\u9884\u6d4b\u6a21\u578b\u80fd\u59cb\u7ec8\u8868\u73b0\u6700\u4f73\u3002\u6a21\u578b\u6392\u540d\u4f1a\u56e0\u8bef\u5dee\u6307\u6807\u3001\u9700\u6c42\u673a\u5236\u548c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u7684\u4e0d\u540c\u800c\u53d8\u5316\uff0c\u5bfc\u81f4\u5728\u591aSKU\u51b3\u7b56\u73af\u5883\u4e2d\u4ea7\u751f\u6a21\u7cca\u6027\u3002", "method": "\u63d0\u51faAHSIV\u6846\u67b6\uff0c\u6574\u5408\u4e86\uff1a1) \u901a\u8fc7MDFH\u7a0b\u5e8f\u8c03\u6574\u7684\u7f29\u653e\u548c\u7edd\u5bf9\u8bef\u5dee\u6307\u6807\uff1b2) \u7ed3\u6784\u6027\u9700\u6c42\u5206\u7c7b\uff1b3) \u591a\u76ee\u6807\u5e15\u7d2f\u6258\u4f18\u52bf\uff1b4) \u5206\u5c42\u504f\u5dee\u7ec6\u5316\u3002\u8be5\u6846\u67b6\u5177\u6709\u65f6\u95f4\u8303\u56f4\u611f\u77e5\u548c\u673a\u5236\u6761\u4ef6\u5316\u7684\u7279\u70b9\u3002", "result": "\u5728Walmart\u3001M3\u3001M4\u548cM5\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAHSIV\u5728\u805a\u5408\u6027\u80fd\u4e0a\u4e0e\u6700\u5f3a\u7684\u5355\u6307\u6807\u57fa\u7ebf\u7edf\u8ba1\u7b49\u4ef7\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u9488\u5bf9\u7279\u5b9a\u65f6\u95f4\u8303\u56f4\u7684\u6700\u4f73\u6a21\u578b\u9009\u62e9\u9891\u7387\u3002", "conclusion": "\u5f02\u8d28\u9700\u6c42\u73af\u5883\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u4e0d\u80fd\u88ab\u89c6\u4e3a\u9759\u6001\u6392\u540d\u95ee\u9898\uff0c\u5177\u6709\u65f6\u95f4\u8303\u56f4\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u81ea\u9002\u5e94\u6027\u7684\u673a\u5236\u4e3a\u591aSKU\u9884\u6d4b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u64cd\u4f5c\u8fde\u8d2f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14311", "abs": "https://arxiv.org/abs/2602.14311", "authors": ["Daniel Choate", "Jason Rife"], "title": "Exploiting Structure-from-Motion for Robust Vision-Based Map Matching for Aircraft Surface Movement", "comment": "Accepted to the Proceedings of the 38th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2025). 15 pages, 13 figures", "summary": "In this paper we introduce a vision-aided navigation (VAN) pipeline designed to support ground navigation of autonomous aircraft. The proposed algorithm combines the computational efficiency of indirect methods with the robustness of direct image-based techniques to enhance solution integrity. The pipeline starts by processing ground images (e.g., acquired by a taxiing aircraft) and relates them via a feature-based structure-from-motion (SfM) solution. A ground plane mosaic is then constructed via homography transforms and matched to satellite imagery using a sum of squares differences (SSD) of intensities. Experimental results reveal that drift within the SfM solution, similar to that observed in dead-reckoning systems, challenges the expected accuracy benefits of map-matching with a wide-baseline ground-plane mosaic. However, the proposed algorithm demonstrates key integrity features, such as the ability to identify registration anomalies and ambiguous matches. These characteristics of the pipeline can mitigate outlier behaviors and contribute toward a robust, certifiable solution for autonomous surface movement of aircraft.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u95f4\u63a5\u65b9\u6cd5\u548c\u76f4\u63a5\u56fe\u50cf\u6280\u672f\u4f18\u70b9\u7684\u89c6\u89c9\u8f85\u52a9\u5bfc\u822a\u7ba1\u9053\uff0c\u7528\u4e8e\u652f\u6301\u81ea\u4e3b\u98de\u673a\u7684\u5730\u9762\u5bfc\u822a\uff0c\u901a\u8fc7\u7279\u5f81\u5339\u914d\u548c\u5730\u56fe\u5339\u914d\u6280\u672f\u63d0\u9ad8\u5bfc\u822a\u5b8c\u6574\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u652f\u6301\u81ea\u4e3b\u98de\u673a\u5730\u9762\u5bfc\u822a\u7684\u89c6\u89c9\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4ee5\u589e\u5f3a\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u7684\u5b8c\u6574\u6027\uff0c\u4e3a\u81ea\u4e3b\u98de\u673a\u5730\u9762\u8fd0\u52a8\u63d0\u4f9b\u53ef\u8ba4\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9\u8f85\u52a9\u5bfc\u822a\u7ba1\u9053\uff1a1) \u5904\u7406\u5730\u9762\u56fe\u50cf\u5e76\u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u7684\u7ed3\u6784\u4ece\u8fd0\u52a8(SfM)\u89e3\u51b3\u65b9\u6848\u5173\u8054\u56fe\u50cf\uff1b2) \u901a\u8fc7\u5355\u5e94\u6027\u53d8\u6362\u6784\u5efa\u5730\u9762\u5e73\u9762\u9a6c\u8d5b\u514b\uff1b3) \u4f7f\u7528\u5f3a\u5ea6\u5e73\u65b9\u548c\u5dee\u5f02(SSD)\u5c06\u9a6c\u8d5b\u514b\u4e0e\u536b\u661f\u56fe\u50cf\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSfM\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u6f02\u79fb\uff08\u7c7b\u4f3c\u4e8e\u822a\u4f4d\u63a8\u7b97\u7cfb\u7edf\uff09\u6311\u6218\u4e86\u4e0e\u5bbd\u57fa\u7ebf\u5730\u9762\u5e73\u9762\u9a6c\u8d5b\u514b\u8fdb\u884c\u5730\u56fe\u5339\u914d\u7684\u9884\u671f\u7cbe\u5ea6\u4f18\u52bf\u3002\u4f46\u7b97\u6cd5\u5c55\u793a\u4e86\u5173\u952e\u5b8c\u6574\u6027\u7279\u5f81\uff0c\u5982\u8bc6\u522b\u914d\u51c6\u5f02\u5e38\u548c\u6a21\u7cca\u5339\u914d\u7684\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u8bc6\u522b\u5f02\u5e38\u548c\u6a21\u7cca\u5339\u914d\u7684\u5173\u952e\u5b8c\u6574\u6027\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u6027\u53ef\u4ee5\u51cf\u8f7b\u5f02\u5e38\u884c\u4e3a\uff0c\u4e3a\u81ea\u4e3b\u98de\u673a\u5730\u9762\u8fd0\u52a8\u63d0\u4f9b\u7a33\u5065\u3001\u53ef\u8ba4\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13940", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13940", "abs": "https://arxiv.org/abs/2602.13940", "authors": ["Sam Dauncey", "Roger Wattenhofer"], "title": "You Can Learn Tokenization End-to-End with Reinforcement Learning", "comment": null, "summary": "Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5206\u6570\u51fd\u6570\u4f30\u8ba1\u6765\u5b66\u4e60LLM\u4e2d\u7684\u5206\u8bcd\u8fb9\u754c\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u76f4\u901a\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6709\u66f4\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u57281\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1LLM\u67b6\u6784\u8d8a\u6765\u8d8a\u7aef\u5230\u7aef\uff0c\u4f46\u5206\u8bcd\u4f5c\u4e3a\u786c\u7f16\u7801\u7684\u538b\u7f29\u6b65\u9aa4\u4ecd\u7136\u4fdd\u7559\u5728\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u3002\u5148\u524d\u5de5\u4f5c\u5c1d\u8bd5\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u76f4\u901a\u4f30\u8ba1\u5b66\u4e60\u5206\u8bcd\u8fb9\u754c\uff0c\u4f46\u5b58\u5728\u7406\u8bba\u4fdd\u8bc1\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5206\u6570\u51fd\u6570\u4f30\u8ba1\u76f4\u63a5\u4f18\u5316\u79bb\u6563\u5206\u8bcd\u8fb9\u754c\u95ee\u9898\u4ee5\u6700\u5c0f\u5316\u635f\u5931\uff0c\u5e76\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65f6\u95f4\u6298\u6263\u6280\u672f\u6765\u964d\u4f4e\u65b9\u5dee\uff0c\u4f7f\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u57281\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0a\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u5148\u524d\u63d0\u51fa\u7684\u76f4\u901a\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5206\u6570\u51fd\u6570\u4f30\u8ba1\u5b66\u4e60\u5206\u8bcd\u8fb9\u754c\u662f\u53ef\u884c\u7684\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u76f4\u901a\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e3aLLM\u4e2d\u66f4\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.14363", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14363", "abs": "https://arxiv.org/abs/2602.14363", "authors": ["Morgan Byrd", "Donghoon Baek", "Kartik Garg", "Hyunyoung Jung", "Daesol Cho", "Maks Sorokin", "Robert Wright", "Sehoon Ha"], "title": "AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation", "comment": "Website: https://morganbyrd03.github.io/adaptmanip/", "summary": "This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.", "AI": {"tldr": "AdaptManip\u662f\u4e00\u4e2a\u5168\u81ea\u4e3b\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u8fd0\u52a8\u64cd\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\uff0c\u5b9e\u73b0\u5bfc\u822a\u3001\u7269\u4f53\u6293\u53d6\u548c\u9012\u9001\u7684\u4e00\u4f53\u5316\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u4eba\u7c7b\u6f14\u793a\uff0c\u5bf9\u5e72\u6270\u8106\u5f31\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u6f14\u793a\u6570\u636e\u3001\u66f4\u9c81\u68d2\u7684\u5168\u8eab\u8fd0\u52a8\u64cd\u63a7\u7b56\u7565\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u8026\u5408\u7ec4\u4ef6\uff1a1) \u5faa\u73af\u7269\u4f53\u72b6\u6001\u4f30\u8ba1\u5668\uff0c\u5728\u6709\u9650\u89c6\u91ce\u548c\u906e\u6321\u4e0b\u5b9e\u65f6\u8ddf\u8e2a\u7269\u4f53\uff1b2) \u5168\u8eab\u57fa\u7840\u7b56\u7565\uff0c\u7528\u4e8e\u9c81\u68d2\u8fd0\u52a8\uff0c\u914d\u5408\u6b8b\u5dee\u64cd\u63a7\u63a7\u5236\u5b9e\u73b0\u7a33\u5b9a\u7269\u4f53\u6293\u53d6\u9012\u9001\uff1b3) LiDAR\u673a\u5668\u4eba\u5168\u5c40\u4f4d\u7f6e\u4f30\u8ba1\u5668\uff0c\u63d0\u4f9b\u6297\u6f02\u79fb\u5b9a\u4f4d\u3002\u6240\u6709\u7ec4\u4ef6\u5728\u4eff\u771f\u4e2d\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u96f6\u6837\u672c\u90e8\u7f72\u5230\u771f\u5b9e\u786c\u4ef6\u3002", "result": "AdaptManip\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff09\uff0c\u5728\u9002\u5e94\u6027\u548c\u603b\u4f53\u6210\u529f\u7387\u4e0a\u8868\u73b0\u66f4\u597d\uff1b\u51c6\u786e\u7684\u7269\u4f53\u72b6\u6001\u4f30\u8ba1\u5373\u4f7f\u5728\u906e\u6321\u4e0b\u4e5f\u80fd\u63d0\u5347\u64cd\u63a7\u6027\u80fd\uff1b\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u5168\u81ea\u4e3b\u5bfc\u822a\u3001\u7269\u4f53\u6293\u53d6\u548c\u9012\u9001\u3002", "conclusion": "AdaptManip\u5c55\u793a\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3001\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u7684\u5168\u8eab\u8fd0\u52a8\u64cd\u63a7\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5168\u81ea\u4e3b\u5bfc\u822a-\u6293\u53d6-\u9012\u9001\u4e00\u4f53\u5316\u64cd\u4f5c\u3002"}}
{"id": "2602.13949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13949", "abs": "https://arxiv.org/abs/2602.13949", "authors": ["Taiwei Shi", "Sihao Chen", "Bowen Jiang", "Linxin Song", "Longqi Yang", "Jieyu Zhao"], "title": "Experiential Reinforcement Learning", "comment": "26 pages, 9 tables, 7 figures", "summary": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faExperiential Reinforcement Learning (ERL)\uff0c\u4e00\u79cd\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u7ecf\u9a8c-\u53cd\u601d-\u5de9\u56fa\u5faa\u73af\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u751f\u6210\u521d\u59cb\u5c1d\u8bd5\u3001\u63a5\u6536\u53cd\u9988\u3001\u4ea7\u751f\u53cd\u601d\u6765\u6307\u5bfc\u6539\u8fdb\u7684\u7b2c\u4e8c\u6b21\u5c1d\u8bd5\uff0c\u4ece\u800c\u5c06\u7a00\u758f\u5ef6\u8fdf\u53cd\u9988\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u884c\u4e3a\u4fee\u6b63\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u8bed\u8a00\u6a21\u578b\u4ece\u73af\u5883\u5956\u52b1\u6216\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u5b9e\u8df5\u4e2d\u73af\u5883\u53cd\u9988\u901a\u5e38\u662f\u7a00\u758f\u548c\u5ef6\u8fdf\u7684\u3002\u4ece\u8fd9\u79cd\u4fe1\u53f7\u4e2d\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u8bed\u8a00\u6a21\u578b\u5fc5\u987b\u9690\u5f0f\u63a8\u65ad\u89c2\u5bdf\u5230\u7684\u5931\u8d25\u5e94\u5982\u4f55\u8f6c\u5316\u4e3a\u672a\u6765\u8fed\u4ee3\u7684\u884c\u4e3a\u6539\u53d8\u3002", "method": "\u5f15\u5165Experiential Reinforcement Learning (ERL)\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u542b\u660e\u786e\u7684\u7ecf\u9a8c-\u53cd\u601d-\u5de9\u56fa\u5faa\u73af\uff1a\u6a21\u578b\u751f\u6210\u521d\u59cb\u5c1d\u8bd5 \u2192 \u63a5\u6536\u73af\u5883\u53cd\u9988 \u2192 \u4ea7\u751f\u53cd\u601d \u2192 \u6307\u5bfc\u6539\u8fdb\u7684\u7b2c\u4e8c\u6b21\u5c1d\u8bd5 \u2192 \u5c06\u6210\u529f\u7ed3\u679c\u5f3a\u5316\u5e76\u5185\u5316\u5230\u57fa\u7840\u7b56\u7565\u4e2d\u3002", "result": "\u5728\u7a00\u758f\u5956\u52b1\u63a7\u5236\u73af\u5883\u548c\u667a\u80fd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cERL\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff1a\u5728\u590d\u6742\u591a\u6b65\u73af\u5883\u4e2d\u83b7\u5f97\u9ad8\u8fbe+81%\u7684\u589e\u76ca\uff0c\u5728\u4f7f\u7528\u5de5\u5177\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u83b7\u5f97\u9ad8\u8fbe+11%\u7684\u589e\u76ca\u3002", "conclusion": "\u5c06\u660e\u786e\u7684\u81ea\u6211\u53cd\u601d\u6574\u5408\u5230\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u4e3a\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u6301\u4e45\u7684\u884c\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u673a\u5236\uff0c\u6539\u5584\u4e86\u63a2\u7d22\u3001\u7a33\u5b9a\u4e86\u4f18\u5316\uff0c\u5e76\u5728\u90e8\u7f72\u65f6\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u5373\u53ef\u4fdd\u6301\u589e\u76ca\u3002"}}
{"id": "2602.14434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14434", "abs": "https://arxiv.org/abs/2602.14434", "authors": ["Steven Oh", "Tomoya Takahashi", "Cristian C. Beltran-Hernandez", "Yuki Kuroda", "Masashi Hamaya"], "title": "A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation", "comment": null, "summary": "Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/", "AI": {"tldr": "CLAW\u662f\u4e00\u79cd\u65b0\u578b\u8f6f\u624b\u8155\u673a\u5236\uff0c\u901a\u8fc7\u6b63\u4ea4\u53f6\u7247\u5f39\u7c27\u548c\u5e26\u9501\u5b9a\u673a\u6784\u7684\u65cb\u8f6c\u5173\u8282\u5b9e\u73b0\u5927\u8303\u56f4\u53d8\u5f62\u548c\u5404\u5411\u5f02\u6027\u521a\u5ea6\u8c03\u8282\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5b66\u4e60\u9c81\u68d2\u6027\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u5bf9\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u51fa\u4e86\u9c81\u68d2\u6027\u6311\u6218\uff0c\u73b0\u6709\u8f6f\u672b\u7aef\u6267\u884c\u5668\u5b58\u5728\u53d8\u5f62\u8303\u56f4\u6709\u9650\u3001\u7f3a\u4e4f\u65b9\u5411\u521a\u5ea6\u63a7\u5236\u6216\u9700\u8981\u590d\u6742\u9a71\u52a8\u7cfb\u7edf\u7b49\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1CLAW\u8f6f\u624b\u8155\u673a\u5236\uff0c\u91c7\u7528\u4e24\u4e2a\u6b63\u4ea4\u53f6\u7247\u5f39\u7c27\u548c\u5e26\u9501\u5b9a\u673a\u6784\u7684\u65cb\u8f6c\u5173\u8282\uff0c\u5b9e\u73b06\u81ea\u7531\u5ea6\u5927\u53d8\u5f62\u548c\u5404\u5411\u5f02\u6027\u521a\u5ea6\u8c03\u8282\uff0c\u5177\u6709\u8f7b\u91cf\u5316\u4f4e\u6210\u672c\u7279\u70b9\u3002", "result": "\u5728\u57fa\u51c6\u9500\u9489\u63d2\u5165\u4efb\u52a1\u4e2d\uff0cCLAW\u8fbe\u523076%\u6210\u529f\u7387\uff0c\u4f18\u4e8eFin Ray\u5939\u722a(43%)\u548c\u521a\u6027\u5939\u722a(36%)\uff0c\u80fd\u5904\u7406\u7cbe\u5bc6\u88c5\u914d\u548c\u7cbe\u7ec6\u7269\u4f53\u64cd\u4f5c\u7b49\u591a\u79cd\u63a5\u89e6\u4e30\u5bcc\u573a\u666f\u3002", "conclusion": "CLAW\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u8f6f\u672b\u7aef\u6267\u884c\u5668\u7684\u5c40\u9650\u6027\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u9886\u57df\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.13953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13953", "abs": "https://arxiv.org/abs/2602.13953", "authors": ["Yuhang Li", "Reena Elangovan", "Xin Dong", "Priyadarshini Panda", "Brucek Khailany"], "title": "QuRL: Efficient Reinforcement Learning with Quantized Rollout", "comment": "Accepted to ICLR 2026", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.", "AI": {"tldr": "\u63d0\u51faQuRL\u65b9\u6cd5\uff0c\u4f7f\u7528\u91cf\u5316actor\u52a0\u901fRL\u8bad\u7ec3\u4e2d\u7684rollout\u8fc7\u7a0b\uff0c\u89e3\u51b3\u8bad\u7ec3\u5d29\u6e83\u548c\u6743\u91cd\u66f4\u65b0\u95ee\u9898\uff0c\u5b9e\u73b020-80%\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u8bad\u7ec3\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u7531\u4e8eLLMs\u7684\u81ea\u56de\u5f52\u89e3\u7801\u7279\u6027\uff0crollout\u8fc7\u7a0b\u6210\u4e3a\u8bad\u7ec3\u6548\u7387\u74f6\u9888\uff0c\u5360\u7528\u9ad8\u8fbe70%\u7684\u603b\u8bad\u7ec3\u65f6\u95f4\u3002", "method": "\u63d0\u51fa\u91cf\u5316\u5f3a\u5316\u5b66\u4e60\uff08QuRL\uff09\uff0c\u4f7f\u7528\u91cf\u5316actor\u52a0\u901frollout\u3002\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u63d0\u51fa\u81ea\u9002\u5e94\u88c1\u526a\u8303\u56f4\uff08ACR\uff09\uff0c\u57fa\u4e8e\u5168\u7cbe\u5ea6actor\u548c\u91cf\u5316actor\u4e4b\u95f4\u7684\u7b56\u7565\u6bd4\u7387\u52a8\u6001\u8c03\u6574\u88c1\u526a\u6bd4\u4f8b\uff0c\u9632\u6b62\u957f\u671f\u8bad\u7ec3\u5d29\u6e83\uff1b2\uff09\u901a\u8fc7\u4e0d\u53d8\u7f29\u653e\u6280\u672f\u89e3\u51b3\u6743\u91cd\u66f4\u65b0\u95ee\u9898\uff0c\u51cf\u5c11\u91cf\u5316\u566a\u58f0\u5e76\u589e\u52a0\u6743\u91cd\u66f4\u65b0\u3002", "result": "\u5728DeepScaleR\u548cDAPO\u4e0a\u8fdb\u884c\u4e86INT8\u548cFP8\u91cf\u5316\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e8620%\u523080%\u7684rollout\u52a0\u901f\u6548\u679c\u3002", "conclusion": "QuRL\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316actor\u6709\u6548\u52a0\u901f\u4e86RL\u8bad\u7ec3\u4e2d\u7684rollout\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u91cf\u5316RL\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6743\u91cd\u66f4\u65b0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.13960", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.13960", "abs": "https://arxiv.org/abs/2602.13960", "authors": ["Zedong Wang", "Yuyang Wang", "Ijay Narang", "Felix Wang", "Yuzhou Wang", "Siva Theja Maguluri"], "title": "Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds", "comment": null, "summary": "Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $\u03b1\\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $\u03b1$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $\u03b1$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $\u03b1^{1/2}\\log(1/\u03b1)$ for small $\u03b1$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $\u03b1$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.", "AI": {"tldr": "\u672c\u6587\u4e3a\u56fa\u5b9a\u6b65\u957f\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u7684\u7a33\u6001\u5206\u5e03\u63d0\u4f9b\u4e86\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u5efa\u7acb\u4e86\u7a33\u6001\u5206\u5e03\u4e0e\u9ad8\u65af\u6781\u9650\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\u548c\u5c3e\u90e8\u6982\u7387\u7684\u663e\u5f0f\u8bef\u5dee\u754c\u3002", "motivation": "\u56fa\u5b9a\u6b65\u957f\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u7a33\u6001\u5206\u5e03\u901a\u5e38\u96be\u4ee5\u89e3\u6790\u3002\u73b0\u6709\u7814\u7a76\u53ea\u63d0\u4f9b\u4e86\u6b65\u957f\u8d8b\u4e8e0\u65f6\u7684\u6e10\u8fd1\u9ad8\u65af\u6781\u9650\uff0c\u5bf9\u4e8e\u56fa\u5b9a\u6b65\u957f\u7f3a\u4e4f\u53ef\u7528\u7684\u8bef\u5dee\u754c\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7cbe\u5ea6\u5206\u6790\u3002", "method": "\u9996\u5148\u8bc1\u660e\u4e86\u4e00\u822c\u6027\u5b9a\u7406\uff0c\u5728\u6f02\u79fb\u6b63\u5219\u6027\u548c\u566a\u58f0\u77e9\u6761\u4ef6\u4e0b\uff0c\u5efa\u7acb\u7a33\u6001\u5206\u5e03\u4e0e\u9ad8\u65af\u5206\u5e03\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\u754c\u3002\u65b9\u6cd5\u8986\u76d6\u4e86i.i.d.\u548c\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u6a21\u578b\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u5b9a\u7406\u5177\u4f53\u5e94\u7528\u4e8e\u4e09\u4e2a\u4ee3\u8868\u6027SA\u8bbe\u7f6e\uff1aSGD\uff08\u5149\u6ed1\u5f3a\u51f8\u76ee\u6807\uff09\u3001\u7ebf\u6027SA\u548c\u538b\u7f29\u975e\u7ebf\u6027SA\u3002", "result": "\u83b7\u5f97\u4e86\u7ef4\u5ea6\u4f9d\u8d56\u548c\u6b65\u957f\u4f9d\u8d56\u7684\u663e\u5f0fWasserstein\u8ddd\u79bb\u754c\uff0c\u9636\u6570\u4e3a\u03b1^{1/2}log(1/\u03b1)\u3002\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u975e\u5747\u5300Berry-Esseen\u578b\u5c3e\u90e8\u754c\uff0c\u8bef\u5dee\u9879\u5728\u504f\u79bb\u6c34\u5e73\u548c\u6b65\u957f\u03b1\u4e0a\u90fd\u8870\u51cf\u3002\u5bf9\u4e8e\u975e\u5f3a\u51f8SGD\uff0c\u8bc6\u522b\u4e86\u975e\u9ad8\u65af\uff08\u5409\u5e03\u65af\uff09\u6781\u9650\u5f8b\u3002", "conclusion": "\u672c\u6587\u4e3a\u56fa\u5b9a\u6b65\u957f\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u7684\u7a33\u6001\u5206\u5e03\u63d0\u4f9b\u4e86\u9996\u4e2a\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u5efa\u7acb\u4e86\u7a33\u6001\u5206\u5e03\u4e0e\u9ad8\u65af\u6781\u9650\u4e4b\u95f4\u7684\u663e\u5f0f\u8fd1\u4f3c\u8bef\u5dee\uff0c\u6269\u5c55\u4e86\u7406\u8bba\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7cbe\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2602.14526", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14526", "abs": "https://arxiv.org/abs/2602.14526", "authors": ["Guy Freund", "Tom Jurgenson", "Matan Sudry", "Erez Karpas"], "title": "TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations", "comment": null, "summary": "Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED's single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.", "AI": {"tldr": "TWISTED-RL\u6539\u8fdbTWISTED\u6846\u67b6\uff0c\u7528\u591a\u6b65\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u66ff\u4ee3\u5355\u6b65\u9006\u6a21\u578b\uff0c\u901a\u8fc7\u62bd\u8c61\u62d3\u6251\u52a8\u4f5c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u6210\u529f\u89e3\u51b3\u66f4\u9ad8\u590d\u6742\u5ea6\u7684\u6253\u7ed3\u4efb\u52a1", "motivation": "\u673a\u5668\u4eba\u6253\u7ed3\u4efb\u52a1\u9762\u4e34\u53ef\u53d8\u5f62\u7269\u4f53\u590d\u6742\u4ea4\u4e92\u548c\u4e25\u683c\u62d3\u6251\u7ea6\u675f\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5TWISTED\u867d\u7136\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5355\u6b65\u9006\u6a21\u578b\u5b58\u5728\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u6548\u679c\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u62d3\u6251\u72b6\u6001\u8f6c\u6362\u673a\u5236", "method": "\u63d0\u51faTWISTED-RL\u6846\u67b6\uff0c\u5c06TWISTED\u4e2d\u7684\u76d1\u7763\u5b66\u4e60\u5355\u6b65\u9006\u6a21\u578b\u66ff\u6362\u4e3a\u57fa\u4e8e\u62bd\u8c61\u62d3\u6251\u52a8\u4f5c\u7684\u591a\u6b65\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u907f\u514d\u4e86\u6602\u8d35\u65e0\u6548\u7684\u6570\u636e\u6536\u96c6\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u7684\u62d3\u6251\u72b6\u6001\u8f6c\u6362", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTWISTED-RL\u80fd\u591f\u89e3\u51b3\u4e4b\u524d\u65e0\u6cd5\u5b9e\u73b0\u7684\u9ad8\u590d\u6742\u5ea6\u6253\u7ed3\u4efb\u52a1\uff08\u5982\u516b\u5b57\u7ed3\u548c\u5355\u7ed3\uff09\uff0c\u6210\u529f\u7387\u548c\u89c4\u5212\u65f6\u95f4\u5747\u6709\u663e\u8457\u6539\u5584\uff0c\u6210\u4e3a\u65e0\u9700\u4eba\u5de5\u6f14\u793a\u7684\u673a\u5668\u4eba\u6253\u7ed3\u4efb\u52a1\u65b0SOTA", "conclusion": "\u57fa\u4e8e\u62bd\u8c61\u62d3\u6251\u52a8\u4f5c\u7684\u591a\u6b65\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6bd4\u5355\u6b65\u9006\u6a21\u578b\u66f4\u9002\u5408\u673a\u5668\u4eba\u6253\u7ed3\u4efb\u52a1\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u62d3\u6251\u72b6\u6001\u8f6c\u6362\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.14011", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14011", "abs": "https://arxiv.org/abs/2602.14011", "authors": ["Liangyu Su", "Jun Shu", "Rui Liu", "Deyu Meng", "Zongben Xu"], "title": "KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra", "comment": "25 pages", "summary": "Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.", "AI": {"tldr": "\u63d0\u51faKoopGen\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u751f\u6210\u5668\u7684\u795e\u7ecfKoopman\u65b9\u6cd5\u5efa\u6a21\u9ad8\u7ef4\u65f6\u7a7a\u6df7\u6c8c\u7cfb\u7edf\uff0c\u5229\u7528Cartesian\u5206\u89e3\u5206\u79bb\u4fdd\u5b88\u4f20\u8f93\u548c\u4e0d\u53ef\u9006\u8017\u6563\uff0c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u5bbd\u5e26\u6216\u8fde\u7eed\u8c31\u4e3b\u5bfc\u7684\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u800c\u4f20\u7edfKoopman\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7ef4\u5047\u8bbe\u6216\u663e\u5f0f\u8c31\u53c2\u6570\u5316\uff0c\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165KoopGen\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u72b6\u6001\u4f9d\u8d56\u7684Koopman\u751f\u6210\u5668\u8868\u793a\u5efa\u6a21\u52a8\u529b\u5b66\uff0c\u5229\u7528Cartesian\u5206\u89e3\u5c06\u7b97\u5b50\u5206\u89e3\u4e3a\u659c\u4f34\u968f\u548c\u81ea\u4f34\u968f\u5206\u91cf\uff0c\u5206\u79bb\u4fdd\u5b88\u4f20\u8f93\u548c\u4e0d\u53ef\u9006\u8017\u6563\uff0c\u5e76\u5728\u5b66\u4e60\u4e2d\u5f3a\u5236\u7cbe\u786e\u7684\u7b97\u5b50\u7406\u8bba\u7ea6\u675f\u3002", "result": "\u5728\u975e\u7ebf\u6027\u632f\u8361\u5668\u3001\u9ad8\u7ef4\u6df7\u6c8c\u548c\u65f6\u7a7a\u52a8\u529b\u5b66\u7b49\u591a\u79cd\u7cfb\u7edf\u4e2d\uff0cKoopGen\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u9610\u660e\u4e86\u8fde\u7eed\u8c31\u52a8\u529b\u5b66\u7684\u54ea\u4e9b\u5206\u91cf\u5141\u8bb8\u53ef\u89e3\u91ca\u548c\u53ef\u5b66\u4e60\u7684\u8868\u793a\u3002", "conclusion": "KoopGen\u4e3a\u9ad8\u7ef4\u65f6\u7a7a\u6df7\u6c8c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684Koopman\u751f\u6210\u5668\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b97\u5b50\u5206\u89e3\u548c\u7ea6\u675f\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.14540", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14540", "abs": "https://arxiv.org/abs/2602.14540", "authors": ["Devodita Chakravarty", "John Dolan", "Yiwei Lyu"], "title": "Multimodal Covariance Steering in Belief Space with Active Probing and Influence for Autonomous Driving", "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA 2026)", "summary": "Autonomous driving in complex traffic requires reasoning under uncertainty. Common approaches rely on prediction-based planning or risk-aware control, but these are typically treated in isolation, limiting their ability to capture the coupled nature of action and inference in interactive settings. This gap becomes especially critical in uncertain scenarios, where simply reacting to predictions can lead to unsafe maneuvers or overly conservative behavior. Our central insight is that safe interaction requires not only estimating human behavior but also shaping it when ambiguity poses risks. To this end, we introduce a hierarchical belief model that structures human behavior across coarse discrete intents and fine motion modes, updated via Bayesian inference for interpretable multi-resolution reasoning. On top of this, we develop an active probing strategy that identifies when multimodal ambiguity in human predictions may compromise safety and plans disambiguating actions that both reveal intent and gently steer human decisions toward safer outcomes. Finally, a runtime risk-evaluation layer based on Conditional Value-at-Risk (CVaR) ensures that all probing actions remain within human risk tolerance during influence. Our simulations in lane-merging and unsignaled intersection scenarios demonstrate that our approach achieves higher success rates and shorter completion times compared to existing methods. These results highlight the benefit of coupling belief inference, probing, and risk monitoring, yielding a principled and interpretable framework for planning under uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u5ff5\u63a8\u7406\u3001\u4e3b\u52a8\u63a2\u6d4b\u548c\u98ce\u9669\u76d1\u63a7\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5728\u4e0d\u786e\u5b9a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u89c4\u5212\uff0c\u901a\u8fc7\u4e3b\u52a8\u5f71\u54cd\u4eba\u7c7b\u884c\u4e3a\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u901a\u5e38\u5c06\u9884\u6d4b\u548c\u89c4\u5212\u5206\u5f00\u5904\u7406\uff0c\u65e0\u6cd5\u6355\u6349\u4ea4\u4e92\u573a\u666f\u4e2d\u884c\u52a8\u4e0e\u63a8\u7406\u7684\u8026\u5408\u5173\u7cfb\u3002\u5728\u4e0d\u786e\u5b9a\u573a\u666f\u4e2d\uff0c\u5355\u7eaf\u57fa\u4e8e\u9884\u6d4b\u53cd\u5e94\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u6216\u8fc7\u4e8e\u4fdd\u5b88\u7684\u884c\u4e3a\u3002\u9700\u8981\u540c\u65f6\u4f30\u8ba1\u548c\u5851\u9020\u4eba\u7c7b\u884c\u4e3a\u6765\u5e94\u5bf9\u6a21\u7cca\u6027\u5e26\u6765\u7684\u98ce\u9669\u3002", "method": "1. \u5206\u5c42\u4fe1\u5ff5\u6a21\u578b\uff1a\u5c06\u4eba\u7c7b\u884c\u4e3a\u7ed3\u6784\u5316\u4e3a\u7c97\u7c92\u5ea6\u79bb\u6563\u610f\u56fe\u548c\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u6a21\u5f0f\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u8fdb\u884c\u591a\u5206\u8fa8\u7387\u63a8\u7406\uff1b2. \u4e3b\u52a8\u63a2\u6d4b\u7b56\u7565\uff1a\u8bc6\u522b\u591a\u6a21\u6001\u6a21\u7cca\u6027\u53ef\u80fd\u5371\u53ca\u5b89\u5168\u7684\u60c5\u51b5\uff0c\u89c4\u5212\u65e2\u80fd\u63ed\u793a\u610f\u56fe\u53c8\u80fd\u6e29\u548c\u5f15\u5bfc\u4eba\u7c7b\u51b3\u7b56\u7684\u6d88\u6b67\u884c\u52a8\uff1b3. \u8fd0\u884c\u65f6\u98ce\u9669\u8bc4\u4f30\u5c42\uff1a\u57fa\u4e8e\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u786e\u4fdd\u6240\u6709\u63a2\u6d4b\u884c\u52a8\u4fdd\u6301\u5728\u4eba\u7c7b\u98ce\u9669\u5bb9\u5fcd\u5ea6\u5185\u3002", "result": "\u5728\u8f66\u9053\u5408\u5e76\u548c\u65e0\u4fe1\u53f7\u4ea4\u53c9\u53e3\u573a\u666f\u7684\u4eff\u771f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u77ed\u7684\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "\u5c06\u4fe1\u5ff5\u63a8\u7406\u3001\u4e3b\u52a8\u63a2\u6d4b\u548c\u98ce\u9669\u76d1\u63a7\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u4e3b\u52a8\u5f71\u54cd\u4eba\u7c7b\u884c\u4e3a\u6765\u63d0\u9ad8\u4ea4\u4e92\u5b89\u5168\u6027\u3002"}}
{"id": "2602.14017", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14017", "abs": "https://arxiv.org/abs/2602.14017", "authors": ["Chenyue Li", "Wen Deng", "Zhuotao Sun", "Mengxi Jin", "Hanzhe Cui", "Han Li", "Shentong Li", "Man Kit Yu", "Ming Long Lai", "Yuhao Yang", "Mengqian Lu", "Binhang Yuan"], "title": "S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services", "comment": "18 pages, 3 figures, 6 tables", "summary": "Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.", "AI": {"tldr": "S2SServiceBench\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b21\u5b63\u8282\u5230\u5b63\u8282\uff08S2S\uff09\u6c14\u5019\u670d\u52a1\u4e2d\"\u6700\u540e\u4e00\u82f1\u91cc\"\u51b3\u7b56\u652f\u6301\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d66\u4e2a\u5e94\u7528\u9886\u57df\u300110\u79cd\u670d\u52a1\u4ea7\u54c1\u3001\u7ea6500\u4e2a\u4efb\u52a1\u548c1000+\u8bc4\u4f30\u9879\u3002", "motivation": "S2S\u9884\u62a5\u5bf9\u6c14\u5019\u97e7\u6027\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b58\u5728\"\u6700\u540e\u4e00\u82f1\u91cc\"\u74f6\u9888\uff1a\u5982\u4f55\u5c06\u79d1\u5b66\u9884\u62a5\u8f6c\u5316\u4e3a\u53ef\u4fe1\u8d56\u3001\u53ef\u64cd\u4f5c\u7684\u6c14\u5019\u670d\u52a1\uff0c\u8fd9\u9700\u8981\u53ef\u9760\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u63a8\u7406\u3002\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u5404\u79cd\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u80fd\u5426\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ece\u4e1a\u52a1\u670d\u52a1\u4ea7\u54c1\u4e2d\u53ef\u9760\u751f\u6210\u51b3\u7b56\u4ea4\u4ed8\u6210\u679c\u3002", "method": "\u5f15\u5165S2SServiceBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ece\u4e1a\u52a1\u6c14\u5019\u670d\u52a1\u7cfb\u7edf\u4e2d\u7b56\u5212\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u5728S2S\u6c14\u5019\u670d\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u57fa\u51c6\u6db5\u76d66\u4e2a\u5e94\u7528\u9886\u57df\uff08\u519c\u4e1a\u3001\u707e\u5bb3\u3001\u80fd\u6e90\u3001\u91d1\u878d\u3001\u5065\u5eb7\u3001\u822a\u8fd0\uff09\uff0c10\u79cd\u670d\u52a1\u4ea7\u54c1\uff0c\u7ea6150+\u4e13\u5bb6\u9009\u62e9\u6848\u4f8b\uff0c\u6bcf\u4e2a\u6848\u4f8b\u5728\u4e09\u4e2a\u670d\u52a1\u7ea7\u522b\u5b9e\u4f8b\u5316\uff0c\u4ea7\u751f\u7ea6500\u4e2a\u4efb\u52a1\u548c1000+\u8bc4\u4f30\u9879\u3002", "result": "\u4f7f\u7528S2SServiceBench\u5bf9\u6700\u5148\u8fdb\u7684MLLMs\u548c\u667a\u80fd\u4f53\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u4ea7\u54c1\u548c\u670d\u52a1\u7ea7\u522b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86S2S\u670d\u52a1\u56fe\u7406\u89e3\u548c\u63a8\u7406\u4e2d\u7684\u6301\u7eed\u6311\u6218\uff1a\u53ef\u64cd\u4f5c\u4fe1\u53f7\u7406\u89e3\u3001\u5c06\u4e0d\u786e\u5b9a\u6027\u64cd\u4f5c\u5316\u4e3a\u53ef\u6267\u884c\u4ea4\u63a5\u3001\u4ee5\u53ca\u5bf9\u52a8\u6001\u707e\u5bb3\u7684\u7a33\u5b9a\u3001\u8bc1\u636e\u57fa\u7840\u7684\u5206\u6790\u548c\u89c4\u5212\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u672a\u6765\u6c14\u5019\u670d\u52a1\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728S2S\u6c14\u5019\u670d\u52a1\u51b3\u7b56\u652f\u6301\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u548c\u8bc1\u636e\u57fa\u7840\u63a8\u7406\u65b9\u9762\u3002"}}
{"id": "2602.14551", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14551", "abs": "https://arxiv.org/abs/2602.14551", "authors": ["Taichi Kato", "Takuya Kiyokawa", "Namiko Saito", "Kensuke Harada"], "title": "Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction", "comment": "16 pages, 8 figures", "summary": "Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u91cd\u6821\u6b63\u673a\u5236\uff0c\u901a\u8fc7\u5185\u90e8\u903b\u8f91\u9a8c\u8bc1\u548c\u5916\u90e8\u7269\u7406\u53cd\u9988\u6765\u6539\u5584\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u6307\u4ee4\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u5728\u4eba\u673a\u534f\u4f5c\u88c5\u914d\u4efb\u52a1\u4e2d\uff0c\u4eba\u7c7b\u6307\u4ee4\u901a\u5e38\u5b58\u5728\u8bed\u8a00\u6b67\u4e49\u548c\u63cf\u8ff0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u751f\u6210\u7269\u7406\u53ef\u884c\u4e14\u534f\u4f5c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u63a8\u7406\u548c\u65e0\u6cd5\u9884\u6d4b\u7269\u7406\u6267\u884c\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aHRC\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u6821\u6b63\u673a\u5236\u589e\u5f3aVLM\u63a8\u7406\uff1a\u5185\u90e8\u6821\u6b63\u6a21\u578b\u5728\u6267\u884c\u524d\u9a8c\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u548c\u4efb\u52a1\u53ef\u884c\u6027\uff1b\u5916\u90e8\u6821\u6b63\u6a21\u578b\u901a\u8fc7\u6267\u884c\u540e\u53cd\u9988\u68c0\u6d4b\u548c\u7ea0\u6b63\u7269\u7406\u5931\u8d25\u3002", "result": "\u4eff\u771f\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u6ca1\u6709\u6821\u6b63\u6a21\u578b\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u5728\u534f\u4f5c\u88c5\u914d\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u7269\u4f53\u56fa\u5b9a\u6216\u5de5\u5177\u51c6\u5907\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u6821\u6b63\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86VLM\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u534f\u4f5c\u4efb\u52a1\u4e2d\u4eba\u7c7b\u6307\u4ee4\u7684\u4ea4\u4e92\u5f0f\u91cd\u89c4\u5212\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2602.14024", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14024", "abs": "https://arxiv.org/abs/2602.14024", "authors": ["Xinxing Zhou", "Qingren Yao", "Yiji Zhao", "Chenghao Liu", "Flora Salim", "Xiaojie Yuan", "Yanlong Wen", "Ming Jin"], "title": "EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models", "comment": null, "summary": "Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.", "AI": {"tldr": "EIDOS\uff1a\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u5b66\u4e60\u800c\u975e\u76f4\u63a5\u89c2\u6d4b\u503c\u9884\u6d4b\u6765\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u83b7\u5f97\u66f4\u7ed3\u6784\u5316\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u6f5c\u5728\u8868\u793a", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u76f4\u63a5\u9884\u6d4b\u672a\u6765\u89c2\u6d4b\u503c\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u901a\u5e38\u4ea7\u751f\u5f31\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u8868\u793a\uff0c\u6355\u6349\u8868\u9762\u566a\u58f0\u800c\u975e\u8fde\u8d2f\u53ef\u9884\u6d4b\u7684\u65f6\u95f4\u52a8\u6001", "method": "\u5f15\u5165EIDOS\u6a21\u578b\u7cfb\u5217\uff0c\u5c06\u9884\u8bad\u7ec3\u4ece\u672a\u6765\u503c\u9884\u6d4b\u8f6c\u5411\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u5b66\u4e60\uff1b\u8bad\u7ec3\u56e0\u679cTransformer\u9884\u6d4b\u6f5c\u5728\u8868\u793a\u7684\u6f14\u5316\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u805a\u5408\u5206\u652f\u6784\u5efa\u76ee\u6807\u8868\u793a\uff1b\u901a\u8fc7\u8054\u5408\u76ee\u6807\u4f18\u5316\uff0c\u6574\u5408\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u3001\u89c2\u6d4b\u63a5\u5730\u548c\u76f4\u63a5\u9884\u6d4b\u76d1\u7763", "result": "\u5728GIFT-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEIDOS\u7f13\u89e3\u4e86\u8868\u793a\u7a7a\u95f4\u7684\u7ed3\u6784\u788e\u7247\u5316\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u7ea6\u675f\u6a21\u578b\u5b66\u4e60\u53ef\u9884\u6d4b\u7684\u6f5c\u5728\u52a8\u6001\u662f\u6784\u5efa\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u539f\u5219\u6027\u6b65\u9aa4"}}
{"id": "2602.14561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14561", "abs": "https://arxiv.org/abs/2602.14561", "authors": ["Arik Laemmle", "Bal\u00e1zs Andr\u00e1s B\u00e1lint", "Philipp Tenbrock", "Frank Naegele", "David Traunecker", "J\u00f3zsef V\u00e1ncza", "Marco F. Huber"], "title": "Simulation-based Learning of Electrical Cabinet Assembly Using Robot Skills", "comment": "20 pages, 14 Figures", "summary": "This paper presents a simulation-driven approach for automating the force-controlled assembly of electrical terminals on DIN-rails, a task traditionally hindered by high programming effort and product variability. The proposed method integrates deep reinforcement learning (DRL) with parameterizable robot skills in a physics-based simulation environment. To realistically model the snap-fit assembly process, we develop and evaluate two types of joining models: analytical models based on beam theory and rigid-body models implemented in the MuJoCo physics engine. These models enable accurate simulation of interaction forces, essential for training DRL agents. The robot skills are structured using the pitasc framework, allowing modular, reusable control strategies. Training is conducted in simulation using Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Domain randomization is applied to improve robustness. The trained policies are transferred to a physical UR10e robot system without additional tuning. Experimental results demonstrate high success rates (up to 100%) in both simulation and real-world settings, even under significant positional and rotational deviations. The system generalizes well to new terminal types and positions, significantly reducing manual programming effort. This work highlights the potential of combining simulation-based learning with modular robot skills for flexible, scalable automation in small-batch manufacturing. Future work will explore hybrid learning methods, automated environment parameterization, and further refinement of joining models for design integration.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4eff\u771f\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316DIN\u5bfc\u8f68\u4e0a\u7535\u6c14\u7aef\u5b50\u7684\u529b\u63a7\u88c5\u914d\uff0c\u901a\u8fc7\u7269\u7406\u4eff\u771f\u8bad\u7ec3\u673a\u5668\u4eba\u6280\u80fd\uff0c\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u4e14\u65e0\u9700\u73b0\u573a\u8c03\u4f18", "motivation": "\u4f20\u7edfDIN\u5bfc\u8f68\u7aef\u5b50\u88c5\u914d\u5b58\u5728\u7f16\u7a0b\u5de5\u4f5c\u91cf\u5927\u3001\u4ea7\u54c1\u53d8\u5f02\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u9002\u5e94\u5c0f\u6279\u91cf\u5236\u9020\u9700\u6c42", "method": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u53c2\u6570\u5316\u673a\u5668\u4eba\u6280\u80fd\uff0c\u5728\u7269\u7406\u4eff\u771f\u73af\u5883\u4e2d\u5f00\u53d1\u4e24\u79cd\u88c5\u914d\u6a21\u578b\uff08\u57fa\u4e8e\u6881\u7406\u8bba\u7684\u89e3\u6790\u6a21\u578b\u548cMuJoCo\u521a\u4f53\u6a21\u578b\uff09\uff0c\u4f7f\u7528SAC\u548cTD3\u7b97\u6cd5\u8bad\u7ec3\uff0c\u91c7\u7528pitasc\u6846\u67b6\u5b9e\u73b0\u6a21\u5757\u5316\u63a7\u5236\u7b56\u7565\uff0c\u5e94\u7528\u9886\u57df\u968f\u673a\u5316\u63d0\u5347\u9c81\u68d2\u6027", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\uff08\u6700\u9ad8\u8fbe100%\uff09\uff0c\u5373\u4f7f\u5b58\u5728\u663e\u8457\u4f4d\u7f6e\u548c\u65cb\u8f6c\u504f\u5dee\u4e5f\u80fd\u7a33\u5b9a\u5de5\u4f5c\uff0c\u7cfb\u7edf\u80fd\u6cdb\u5316\u5230\u65b0\u7684\u7aef\u5b50\u7c7b\u578b\u548c\u4f4d\u7f6e\uff0c\u5927\u5e45\u51cf\u5c11\u624b\u52a8\u7f16\u7a0b\u5de5\u4f5c\u91cf", "conclusion": "\u4eff\u771f\u5b66\u4e60\u4e0e\u6a21\u5757\u5316\u673a\u5668\u4eba\u6280\u80fd\u7ed3\u5408\u4e3a\u5c0f\u6279\u91cf\u5236\u9020\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u65b9\u6848\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u6df7\u5408\u5b66\u4e60\u65b9\u6cd5\u3001\u81ea\u52a8\u5316\u73af\u5883\u53c2\u6570\u5316\u548c\u88c5\u914d\u6a21\u578b\u4f18\u5316"}}
{"id": "2602.14666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14666", "abs": "https://arxiv.org/abs/2602.14666", "authors": ["Ruofeng Wei", "Kai Chen", "Yui Lun Ng", "Yiyao Ma", "Justin Di-Lang Ho", "Hon Sing Tong", "Xiaomei Wang", "Jing Dai", "Ka-Wai Kwok", "Qi Dou"], "title": "Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments", "comment": null, "summary": "Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u8154\u5185\u624b\u672f\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u89c6\u89c9\u611f\u77e5\u5e73\u53f0\uff0c\u901a\u8fc7\u5355\u76ee\u5185\u7aa5\u955c\u56fe\u50cf\u7b97\u6cd5\u8bc6\u522b\u67d4\u6027\u5668\u68b0\u4f4d\u7f6e\u548c\u65b9\u5411\uff0c\u5e76\u6d4b\u91cf\u5176\u4e0e\u7ec4\u7ec7\u7684\u8ddd\u79bb\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u624b\u672f\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u8154\u5185\u624b\u672f\u4e3a\u65e9\u671f\u80c3\u80a0\u9053\u548c\u6ccc\u5c3f\u7cfb\u7edf\u764c\u75c7\u63d0\u4f9b\u5fae\u521b\u9009\u62e9\uff0c\u4f46\u53d7\u9650\u4e8e\u624b\u672f\u5de5\u5177\u548c\u9661\u5ced\u7684\u5b66\u4e60\u66f2\u7ebf\u3002\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u67d4\u6027\u5668\u68b0\uff0c\u53ef\u5b9e\u73b0\u7cbe\u786e\u7ec4\u7ec7\u5207\u9664\uff0c\u4f46\u9700\u8981\u66f4\u597d\u7684\u611f\u77e5\u548c\u63a7\u5236\u65b9\u6cd5\u6765\u6539\u5584\u624b\u672f\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e862D\u548c3D\u5b66\u4e60\u578b\u611f\u77e5\u7b97\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86\u7269\u7406\u903c\u771f\u7684\u6a21\u62df\u5668\u6765\u5efa\u6a21\u67d4\u6027\u5668\u68b0\u52a8\u529b\u5b66\u3002\u8be5\u6a21\u62df\u5668\u751f\u6210\u771f\u5b9e\u7684\u8154\u5185\u573a\u666f\uff0c\u652f\u6301\u67d4\u6027\u673a\u5668\u4eba\u63a7\u5236\u548c\u5927\u91cf\u6570\u636e\u6536\u96c6\u3002\u4f7f\u7528\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u539f\u578b\u8fdb\u884c\u6a21\u5757\u548c\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u3002", "result": "\u7b97\u6cd5\u663e\u8457\u6539\u5584\u4e86\u67d4\u6027\u5668\u68b0\u7684\u63a7\u5236\uff0c\u5728\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5c06\u64cd\u4f5c\u65f6\u95f4\u51cf\u5c11\u4e8670%\u4ee5\u4e0a\uff0c\u589e\u5f3a\u4e86\u5bf9\u624b\u672f\u573a\u666f\u7684\u7406\u89e3\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u8154\u5185\u624b\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u89c9\u611f\u77e5\u5e73\u53f0\u6709\u6548\u89e3\u51b3\u4e86\u8154\u5185\u624b\u672f\u4e2d\u67d4\u6027\u5668\u68b0\u611f\u77e5\u548c\u63a7\u5236\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u7b97\u6cd5\u548c\u7269\u7406\u6a21\u62df\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u624b\u672f\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u8154\u5185\u624b\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2602.14050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14050", "abs": "https://arxiv.org/abs/2602.14050", "authors": ["Atsushi Shimizu", "Shohei Taniguchi", "Yutaka Matsuo"], "title": "Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers", "comment": "To appear at EACL 2026", "summary": "Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u968f\u673a\u6d6e\u70b9\u91c7\u6837(RFS)\u7684\u7b80\u5355\u800c\u5f3a\u5927\u7684\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\uff0c\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u957f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5ea6\u6cdb\u5316\u65b9\u9762\u7684\u6311\u6218\u2014\u2014\u5373\u6a21\u578b\u5728\u9047\u5230\u6bd4\u9884\u8bad\u7ec3\u65f6\u66f4\u957f\u7684\u8f93\u5165\u65f6\u4fdd\u6301\u6027\u80fd\u7684\u80fd\u529b\u3002\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u957f\u5ea6\u65f6\u5b58\u5728\u5206\u5e03\u5916(OOD)\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u968f\u673a\u6d6e\u70b9\u91c7\u6837(RFS)\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\uff1a\u4e0d\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u79bb\u6563\u4f4d\u7f6e\u7d22\u5f15\u96c6\uff0c\u800c\u662f\u4f7f\u7528\u968f\u673a\u91c7\u6837\u7684\u8fde\u7eed\u503c\u4f5c\u4e3a\u4f4d\u7f6e\u7d22\u5f15\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u8ba9\u6a21\u578b\u63a5\u89e6\u591a\u6837\u5316\u7684\u7d22\u5f15\uff0c\u4ece\u800c\u907f\u514d\u5728\u672a\u89c1\u957f\u5ea6\u4e0a\u51fa\u73b0OOD\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eRFS\u5728\u957f\u5ea6\u6cdb\u5316\u4efb\u52a1\u548c\u96f6\u6837\u672c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8f7b\u677e\u6574\u5408\u5230\u5e7f\u6cdb\u4f7f\u7528\u7684\u7edd\u5bf9\u6b63\u5f26\u7f16\u7801\u3001RoPE\u548cALiBi\u7b49\u4f4d\u7f6e\u7f16\u7801\u4e2d\u3002", "conclusion": "RFS\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u672a\u89c1\u957f\u5ea6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14726", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14726", "abs": "https://arxiv.org/abs/2602.14726", "authors": ["Kohio Deflesselle", "M\u00e9lodie Daniel", "Aly Magassouba", "Miguel Aranda", "Olivier Ly"], "title": "ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions", "comment": "8 pages, 5, figures, Accepted for 2026 IEEE International Conference on Robotics & Automation (ICRA)", "summary": "Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.", "AI": {"tldr": "ManeuverNet\u662f\u4e00\u4e2a\u9488\u5bf9\u53cc\u963f\u514b\u66fc\u8f6c\u5411\u673a\u5668\u4eba\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408Soft Actor-Critic\u4e0eCrossQ\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u53c2\u6570\u654f\u611f\u548c\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u519c\u4e1a\u5e94\u7528\u4e2d\u53cc\u963f\u514b\u66fc\u8f6c\u5411\u673a\u5668\u4eba\u9700\u8981\u5728\u6709\u9650\u7a7a\u95f4\u5185\u6267\u884c\u7cbe\u786e\u590d\u6742\u7684\u673a\u52a8\u52a8\u4f5c\u3002\u4f20\u7edf\u65b9\u6cd5\u5982TEB\u89c4\u5212\u5668\u4f9d\u8d56\u53c2\u6570\u8c03\u4f18\uff0c\u5bf9\u673a\u5668\u4eba\u914d\u7f6e\u6216\u73af\u5883\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\uff1b\u800c\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5e38\u56e0\u5956\u52b1\u51fd\u6570\u4e0d\u9002\u5408\u975e\u5b8c\u6574\u7ea6\u675f\u5bfc\u81f4\u6b21\u4f18\u7b56\u7565\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faManeuverNet\u6846\u67b6\uff0c\u7ed3\u5408Soft Actor-Critic\u4e0eCrossQ\u7b97\u6cd5\uff0c\u5f15\u5165\u56db\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u6765\u652f\u6301\u673a\u52a8\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e13\u5bb6\u6570\u636e\u6216\u4eba\u5de5\u6307\u5bfc\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684DRL\u57fa\u7ebf\uff0cManeuverNet\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u52a8\u6027\u548c\u6210\u529f\u7387\uff0c\u83b7\u5f97\u8d85\u8fc740%\u7684\u6027\u80fd\u63d0\u5347\uff1b\u6709\u6548\u7f13\u89e3\u4e86TEB\u89c4\u5212\u5668\u7684\u5f3a\u53c2\u6570\u654f\u611f\u6027\uff1b\u5728\u5b9e\u9645\u8bd5\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe90%\u7684\u673a\u52a8\u8f68\u8ff9\u6548\u7387\u63d0\u5347\u3002", "conclusion": "ManeuverNet\u6846\u67b6\u5728\u53cc\u963f\u514b\u66fc\u8f6c\u5411\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u673a\u52a8\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.14794", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14794", "abs": "https://arxiv.org/abs/2602.14794", "authors": ["Alexander Fee\u00df", "Martin Wei\u00df"], "title": "Analysis of a Cuspidal 6R Robot", "comment": null, "summary": "We present a theoretical and numerical analysis of the kinematics for the \"Transpressor\", a cuspidal 6R robot. It admits up to 16 inverse kinematics solutions which are described geometrically. For special target poses, we provide the solutions analytically and present a simple numerical solver for the general case. Moreover, an analytical estimate of the Jacobian determinant on a path between two solutions proves cuspidality for a class of robots similar to the transpressor.", "AI": {"tldr": "\u672c\u6587\u5bf9\"Transpressor\"\uff08\u5c16\u70b96R\u673a\u5668\u4eba\uff09\u8fdb\u884c\u4e86\u8fd0\u52a8\u5b66\u7406\u8bba\u548c\u6570\u503c\u5206\u6790\uff0c\u8be5\u673a\u5668\u4eba\u6700\u591a\u670916\u4e2a\u9006\u8fd0\u52a8\u5b66\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u7279\u6b8a\u76ee\u6807\u4f4d\u59ff\u7684\u89e3\u6790\u89e3\u548c\u901a\u7528\u6570\u503c\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u901a\u8fc7\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u7c7b\u673a\u5668\u4eba\u7684\u5c16\u70b9\u7279\u6027\u3002", "motivation": "\u7814\u7a76Transpressor\u8fd9\u7c7b\u5c16\u70b96R\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5b66\u7279\u6027\uff0c\u7279\u522b\u662f\u5176\u591a\u89e3\u6027\u548c\u5c16\u70b9\u884c\u4e3a\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u548c\u63a7\u5236\u8fd9\u7c7b\u590d\u6742\u673a\u5668\u4eba\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u65b9\u6cd5\u63cf\u8ff0\u9006\u8fd0\u52a8\u5b66\u89e3\uff0c\u4e3a\u7279\u6b8a\u76ee\u6807\u4f4d\u59ff\u63d0\u4f9b\u89e3\u6790\u89e3\uff0c\u5f00\u53d1\u901a\u7528\u6570\u503c\u6c42\u89e3\u5668\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u5728\u89e3\u8def\u5f84\u4e0a\u7684\u53d8\u5316\u6765\u8bc1\u660e\u5c16\u70b9\u7279\u6027\u3002", "result": "Transpressor\u6700\u591a\u670916\u4e2a\u9006\u8fd0\u52a8\u5b66\u89e3\uff0c\u5efa\u7acb\u4e86\u7279\u6b8a\u76ee\u6807\u4f4d\u59ff\u7684\u89e3\u6790\u89e3\uff0c\u5f00\u53d1\u4e86\u6709\u6548\u7684\u6570\u503c\u6c42\u89e3\u5668\uff0c\u5e76\u901a\u8fc7\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u7c7b\u673a\u5668\u4eba\u7684\u5c16\u70b9\u7279\u6027\u3002", "conclusion": "\u6210\u529f\u5206\u6790\u4e86Transpressor\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5b66\u7279\u6027\uff0c\u5efa\u7acb\u4e86\u591a\u89e3\u63cf\u8ff0\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u6c42\u89e3\u5de5\u5177\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u4e86\u5176\u5c16\u70b9\u884c\u4e3a\uff0c\u4e3a\u8fd9\u7c7b\u590d\u6742\u673a\u5668\u4eba\u7684\u7406\u89e3\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.14799", "categories": ["cs.RO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.14799", "abs": "https://arxiv.org/abs/2602.14799", "authors": ["Javier Gonz\u00e1lez Villasmil"], "title": "Scalable Multi-Robot Path Planning via Quadratic Unconstrained Binary Optimization", "comment": "21 pages, 9 figures, 1 table. Accompanying open-source implementation at https://github.com/JavideuS/Spooky", "summary": "Multi-Agent Path Finding (MAPF) remains a fundamental challenge in robotics, where classical centralized approaches exhibit exponential growth in joint-state complexity as the number of agents increases. This paper investigates Quadratic Unconstrained Binary Optimization (QUBO) as a structurally scalable alternative for simultaneous multi-robot path planning. This approach is a robotics-oriented QUBO formulation incorporating BFS-based logical pre-processing (achieving over 95% variable reduction), adaptive penalty design for collision and constraint enforcement, and a time-windowed decomposition strategy that enables execution within current hardware limitations. An experimental evaluation in grid environments with up to four robots demonstrated near-optimal solutions in dense scenarios and favorable scaling behavior compared to sequential classical planning. These results establish a practical and reproducible baseline for future quantum and quantum-inspired multi-robot coordinations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQUBO\uff08\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff09\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7BFS\u9884\u5904\u7406\u3001\u81ea\u9002\u5e94\u60e9\u7f5a\u8bbe\u8ba1\u548c\u65f6\u95f4\u7a97\u53e3\u5206\u89e3\u7b56\u7565\uff0c\u5728\u7f51\u683c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u968f\u7740\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u4f1a\u51fa\u73b0\u6307\u6570\u7ea7\u7684\u72b6\u6001\u7a7a\u95f4\u589e\u957f\uff0c\u9700\u8981\u5bfb\u627e\u7ed3\u6784\u4e0a\u66f4\u5177\u53ef\u6269\u5c55\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528QUBO\u6846\u67b6\uff0c\u7ed3\u5408BFS\u903b\u8f91\u9884\u5904\u7406\uff08\u51cf\u5c1195%\u4ee5\u4e0a\u53d8\u91cf\uff09\u3001\u81ea\u9002\u5e94\u60e9\u7f5a\u8bbe\u8ba1\u7528\u4e8e\u78b0\u649e\u548c\u7ea6\u675f\u6267\u884c\uff0c\u4ee5\u53ca\u65f6\u95f4\u7a97\u53e3\u5206\u89e3\u7b56\u7565\u4ee5\u9002\u5e94\u5f53\u524d\u786c\u4ef6\u9650\u5236\u3002", "result": "\u5728\u6700\u591a4\u4e2a\u673a\u5668\u4eba\u7684\u7f51\u683c\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5bc6\u96c6\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u987a\u5e8f\u7ecf\u5178\u89c4\u5212\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u548c\u91cf\u5b50\u542f\u53d1\u5f0f\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\u5efa\u7acb\u4e86\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86QUBO\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.14086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14086", "abs": "https://arxiv.org/abs/2602.14086", "authors": ["Jae-Hwan Choi", "Jiwoo Yoon", "Dohyun Kwon", "Jaewoong Choi"], "title": "Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing", "comment": "31 pages, 3 figures", "summary": "We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u795e\u7ecf\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u65af\u5e73\u6ed1\u7b56\u7565\u89e3\u51b3\u534a\u5bf9\u5076\u795e\u7ecfOT\u5728\u975e\u6b63\u5219\u8bbe\u7f6e\u4e0b\u4ea7\u751f\u7684\u865a\u5047\u89e3\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u6b63\u5219\u6e90\u6d4b\u5ea6\u4e0b\u516c\u5f0f\u662f\u9002\u5b9a\u7684\u5e76\u80fd\u6062\u590d\u552f\u4e00\u7684Monge\u6620\u5c04\u3002", "motivation": "\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\uff0c\u534a\u5bf9\u5076\u795e\u7ecf\u6700\u4f18\u4f20\u8f93\u5728\u975e\u6b63\u5219\u8bbe\u7f6e\u4e0b\u7ecf\u5e38\u4ea7\u751f\u865a\u5047\u89e3\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u76ee\u6807\u5206\u5e03\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u4e0d\u9002\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e03\u6717\u8fd0\u52a8\u7684\u9ad8\u65af\u5e73\u6ed1\u7b56\u7565\u6765\u6269\u5c55\u534a\u5bf9\u5076\u6846\u67b6\uff0c\u4f7f\u7528\u6b63\u5219\u6d4b\u5ea6\u6846\u67b6\u5206\u6790\u865a\u5047\u89e3\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u5728\u6b63\u5219\u6e90\u6d4b\u5ea6\u4e0b\u516c\u5f0f\u7684\u9002\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u6b63\u5219\u6e90\u6d4b\u5ea6\u4e0b\uff0c\u516c\u5f0f\u662f\u9002\u5b9a\u7684\u5e76\u80fd\u6062\u590d\u552f\u4e00\u7684Monge\u6620\u5c04\uff1b\u5efa\u7acb\u4e86\u5e73\u6ed1\u6d4b\u5ea6\u6b63\u5219\u6027\u7684\u5c16\u9510\u7279\u5f81\u5316\uff1b\u5728\u5408\u6210\u51fd\u6570\u6570\u636e\u548c\u65f6\u5e8f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u865a\u5047\u89e3\u5e76\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u65af\u5e73\u6ed1\u7b56\u7565\u6269\u5c55\u7684\u534a\u5bf9\u5076\u795e\u7ecf\u6700\u4f18\u4f20\u8f93\u6846\u67b6\u89e3\u51b3\u4e86\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u865a\u5047\u89e3\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u51fd\u6570\u6570\u636e\u548c\u65f6\u5e8f\u6570\u636e\u7684\u4f20\u8f93\u95ee\u9898\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14874", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14874", "abs": "https://arxiv.org/abs/2602.14874", "authors": ["Xiaoxiang Dong", "Weiming Zhi"], "title": "Affordance Transfer Across Object Instances via Semantically Anchored Functional Map", "comment": null, "summary": "Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \\emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.", "AI": {"tldr": "\u63d0\u51faSemantic Anchored Functional Maps\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u89c6\u89c9\u6f14\u793a\u5b9e\u73b0\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7269\u4f53\u95f4\u7684\u529f\u80fd\u5bf9\u5e94\u5173\u7cfb\u8fc1\u79fb\uff0c\u89e3\u51b3\u4f20\u7edf\u793a\u6559\u5b66\u4e60\u9700\u8981\u5927\u91cf\u7269\u7406\u6f14\u793a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u793a\u6559\u5b66\u4e60\u9700\u8981\u5927\u91cf\u7269\u7406\u6f14\u793a\uff0c\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\uff1b\u73b0\u6709\u4ece\u4eba\u7c7b\u89c6\u9891\u5b66\u4e60\u7684\u65b9\u6cd5\u96be\u4ee5\u5c06\u4ea4\u4e92\u7ebf\u7d22\u6cdb\u5316\u5230\u51e0\u4f55\u5f62\u72b6\u4e0d\u540c\u4f46\u529f\u80fd\u76f8\u4f3c\u7684\u7269\u4f53\u5b9e\u4f8b\u4e0a\u3002", "method": "\u57fa\u4e8e\u56fe\u50cf\u91cd\u5efa\u7684\u7c97\u7565\u7f51\u683c\uff0c\u8bc6\u522b\u7269\u4f53\u95f4\u7684\u8bed\u4e49\u5bf9\u5e94\u529f\u80fd\u533a\u57df\uff0c\u9009\u62e9\u4e92\u65a5\u7684\u8bed\u4e49\u951a\u70b9\uff0c\u901a\u8fc7\u529f\u80fd\u6620\u5c04\u5728\u8868\u9762\u4e0a\u4f20\u64ad\u7ea6\u675f\uff0c\u83b7\u5f97\u5bc6\u96c6\u7684\u8bed\u4e49\u4e00\u81f4\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u7269\u4f53\u7c7b\u522b\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u9002\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u51c6\u786e\u7684\u529f\u80fd\u8fc1\u79fb\uff0c\u9002\u5408\u5b9e\u9645\u673a\u5668\u4eba\u611f\u77e5-\u884c\u52a8\u6d41\u7a0b\u3002", "conclusion": "SemFM\u6846\u67b6\u4e3a\u4ece\u5355\u6b21\u89c6\u89c9\u6f14\u793a\u4e2d\u8de8\u51e0\u4f55\u591a\u6837\u7269\u4f53\u8fc1\u79fb\u4ea4\u4e92\u533a\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4ece\u6f14\u793a\u7684\u5b9e\u7528\u5316\u3002"}}
{"id": "2602.14958", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.14958", "abs": "https://arxiv.org/abs/2602.14958", "authors": ["Mohanraj A", "S Ganga Prasath"], "title": "Morphing of and writing with a scissor linkage mechanism", "comment": null, "summary": "Kinematics of mechanisms is intricately coupled to their geometry and their utility often arises out of the ability to perform reproducible motion with fewer actuating degrees of freedom. In this article, we explore the assembly of scissor-units, each made of two rigid linear members connected by a pin joint. The assembly has a single degree of freedom, where actuating any single unit results in a shape change of the entire assembly. We derive expressions for the effective curvature of the unit and the trajectory of the mechanism's tip as a function of the geometric variables which we then use as the basis to program two tasks in the mechanism: shape morphing and writing. By phrasing these tasks as optimization problems and utilizing the differentiable simulation framework, we arrive at solutions that are then tested in table-top experiments. Our results show that the geometry of scissor assemblies can be leveraged for automated navigation and inspection in complex domains, in light of the optimization framework. However, we highlight that the challenges associated with rapid programming and error-free implementation in experiments without feedback still remain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u526a\u5200\u5355\u5143\u673a\u6784\u7684\u8fd0\u52a8\u5b66\u7279\u6027\uff0c\u901a\u8fc7\u51e0\u4f55\u4f18\u5316\u5b9e\u73b0\u5f62\u72b6\u53d8\u6362\u548c\u4e66\u5199\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u9886\u57df\u81ea\u52a8\u5316\u5bfc\u822a\u548c\u68c0\u6d4b\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u526a\u5200\u5355\u5143\u673a\u6784\u7684\u8fd0\u52a8\u5b66\u4e0e\u51e0\u4f55\u8026\u5408\u5173\u7cfb\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5176\u5355\u81ea\u7531\u5ea6\u7279\u6027\u5b9e\u73b0\u53ef\u91cd\u590d\u8fd0\u52a8\uff0c\u4e3a\u590d\u6742\u9886\u57df\u7684\u81ea\u52a8\u5316\u5bfc\u822a\u548c\u68c0\u6d4b\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u526a\u5200\u5355\u5143\u7684\u6709\u6548\u66f2\u7387\u548c\u673a\u6784\u5c16\u7aef\u8f68\u8ff9\u7684\u51e0\u4f55\u53d8\u91cf\u8868\u8fbe\u5f0f\uff0c\u5c06\u5f62\u72b6\u53d8\u6362\u548c\u4e66\u5199\u4efb\u52a1\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u53ef\u5fae\u5206\u4eff\u771f\u6846\u67b6\u8fdb\u884c\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u526a\u5200\u5355\u5143\u673a\u6784\u7684\u51e0\u4f55\u7279\u6027\u53ef\u7528\u4e8e\u590d\u6742\u9886\u57df\u7684\u81ea\u52a8\u5316\u5bfc\u822a\u548c\u68c0\u6d4b\uff0c\u4f46\u5feb\u901f\u7f16\u7a0b\u548c\u65e0\u53cd\u9988\u5b9e\u9a8c\u4e2d\u7684\u65e0\u5dee\u9519\u5b9e\u65bd\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u526a\u5200\u5355\u5143\u673a\u6784\u7684\u51e0\u4f55\u7279\u6027\u7ed3\u5408\u4f18\u5316\u6846\u67b6\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u5feb\u901f\u7f16\u7a0b\u548c\u5b9e\u9a8c\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2602.14154", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.14154", "abs": "https://arxiv.org/abs/2602.14154", "authors": ["Yuxuan Linghu", "Zhiyuan Liu", "Qi Deng"], "title": "A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers", "comment": "18 pages, 4 figures, 5 tables", "summary": "Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.", "AI": {"tldr": "dXPP\uff1a\u4e00\u79cd\u57fa\u4e8e\u60e9\u7f5a\u7684\u4e8c\u6b21\u89c4\u5212\u53ef\u5fae\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6c42\u89e3\u4e0e\u5fae\u5206\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4efb\u610f\u9ed1\u76d2QP\u6c42\u89e3\u5668\uff0c\u5e76\u901a\u8fc7\u60e9\u7f5a\u95ee\u9898\u9690\u5f0f\u5fae\u5206\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u95ee\u9898\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eKKT\u7cfb\u7edf\u7684\u53ef\u5fae\u4f18\u5316\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6570\u503c\u9c81\u68d2\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5065\u7684\u4e8c\u6b21\u89c4\u5212\u53ef\u5fae\u4f18\u5316\u6846\u67b6\u3002", "method": "\u63d0\u51fadXPP\u60e9\u7f5a\u5fae\u5206\u6846\u67b6\uff1a1\uff09\u524d\u5411\u6c42\u89e3\u9636\u6bb5\u89e3\u8026QP\u6c42\u89e3\uff0c\u53ef\u4f7f\u7528\u4efb\u610f\u9ed1\u76d2QP\u6c42\u89e3\u5668\uff1b2\uff09\u53cd\u5411\u5fae\u5206\u9636\u6bb5\u5c06\u89e3\u6620\u5c04\u5230\u5e73\u6ed1\u8fd1\u4f3c\u60e9\u7f5a\u95ee\u9898\uff0c\u901a\u8fc7\u9690\u5f0f\u5fae\u5206\u4ec5\u9700\u6c42\u89e3\u66f4\u5c0f\u7684\u539f\u59cb\u53d8\u91cf\u7ebf\u6027\u7cfb\u7edf\uff0c\u907f\u514d\u663e\u5f0fKKT\u5fae\u5206\u56f0\u96be\u3002", "result": "\u5728\u968f\u673a\u751f\u6210QP\u3001\u5927\u89c4\u6a21\u7a00\u758f\u6295\u5f71\u95ee\u9898\u548c\u5b9e\u9645\u591a\u671f\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cdXPP\u4e0e\u57fa\u4e8eKKT\u7684\u5fae\u5206\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "conclusion": "dXPP\u901a\u8fc7\u89e3\u8026\u6c42\u89e3\u4e0e\u5fae\u5206\u3001\u91c7\u7528\u60e9\u7f5a\u65b9\u6cd5\u9690\u5f0f\u5fae\u5206\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4e8c\u6b21\u89c4\u5212\u53ef\u5fae\u4f18\u5316\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u53ef\u5fae\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14159", "abs": "https://arxiv.org/abs/2602.14159", "authors": ["Rizhen Hu", "Yuan Cao", "Boao Kong", "Mou Sun", "Kun Yuan"], "title": "Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization", "comment": "preprint", "summary": "Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u5373\u63d2\u5373\u7528\u7684\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\uff0c\u65e0\u9700\u4fee\u6539MoE\u67b6\u6784\u5373\u53ef\u63d0\u5347\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u8def\u7531\u6548\u7387\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5b58\u5728\u4e13\u5bb6\u91cd\u53e0\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5bb9\u91cf\u5229\u7528\u4e0d\u8db3\u3002\u73b0\u6709\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u9700\u8981\u5927\u91cf\u7ed3\u6784\u4fee\u6539\u4e14\u4ec5\u4f9d\u8d56\u5c42\u5185\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6b63\u5219\u5316\u635f\u5931\uff1a1) \u5c42\u5185\u4e13\u4e1a\u5316\u635f\u5931\uff0c\u60e9\u7f5a\u76f8\u540ctoken\u4e0a\u4e13\u5bb6SwiGLU\u6fc0\u6d3b\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff1b2) \u8de8\u5c42\u8026\u5408\u635f\u5931\uff0c\u6700\u5927\u5316\u76f8\u90bb\u5c42\u95f4Top-k\u8def\u7531\u6982\u7387\u7684\u8054\u5408\u6982\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5728\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u83b7\u5f97\u4e00\u81f4\u7684\u4efb\u52a1\u589e\u76ca\uff0c\u4e13\u5bb6\u4e13\u4e1a\u5316\u7a0b\u5ea6\u66f4\u9ad8\uff0c\u8def\u7531\u71b5\u66f4\u4f4e\uff0c\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u4e24\u79cd\u5373\u63d2\u5373\u7528\u7684\u6b63\u5219\u5316\u635f\u5931\u80fd\u6709\u6548\u63d0\u5347MoE\u6a21\u578b\u7684\u4e13\u4e1a\u5316\u548c\u8def\u7531\u6548\u7387\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u5b9e\u73b0\u6027\u80fd\u6539\u8fdb\u548c\u66f4\u5feb\u7684\u63a8\u7406\u3002"}}
{"id": "2602.14979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14979", "abs": "https://arxiv.org/abs/2602.14979", "authors": ["Ronghao Dang", "Jiayan Guo", "Bohan Hou", "Sicong Leng", "Kehan Li", "Xin Li", "Jiangpin Liu", "Yunxuan Mao", "Zhikai Wang", "Yuqian Yuan", "Minghao Zhu", "Xiao Lin", "Yang Bai", "Qian Jiang", "Yaxi Zhao", "Minghua Zeng", "Junlong Gao", "Yuming Jiang", "Jun Cen", "Siteng Huang", "Liuyi Wang", "Wenqiao Zhang", "Chengju Liu", "Jianfei Yang", "Shijian Lu", "Deli Zhao"], "title": "RynnBrain: Open Embodied Foundation Models", "comment": "Homepage: https://alibaba-damo-academy.github.io/RynnBrain.github.io", "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.", "AI": {"tldr": "RynnBrain\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u5177\u8eab\u667a\u80fd\uff0c\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5177\u8eab\u667a\u80fd\u9886\u57df\u4ecd\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u3001\u7269\u7406\u57fa\u7840\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\u6574\u5408\u5230\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u7a7a\u52a8\u6001\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86RynnBrain\uff0c\u4e00\u4e2a\u5f00\u6e90\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5f3a\u5316\u56db\u4e2a\u6838\u5fc3\u80fd\u529b\uff1a\u5168\u9762\u7684\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u3001\u591a\u6837\u5316\u7684\u65f6\u7a7a\u5b9a\u4f4d\u3001\u7269\u7406\u57fa\u7840\u63a8\u7406\u548c\u7269\u7406\u611f\u77e5\u89c4\u5212\u3002\u6a21\u578b\u5bb6\u65cf\u5305\u62ec\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\u89c4\u6a21\uff082B\u30018B\u548c30B-A3B MoE\uff09\u548c\u56db\u4e2a\u9488\u5bf9\u4e0b\u6e38\u5177\u8eab\u4efb\u52a1\u6216\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u7684\u540e\u8bad\u7ec3\u53d8\u4f53\u3002", "result": "\u572820\u4e2a\u5177\u8eab\u57fa\u51c6\u6d4b\u8bd5\u548c8\u4e2a\u901a\u7528\u89c6\u89c9\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRynnBrain\u57fa\u7840\u6a21\u578b\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u5177\u8eab\u57fa\u7840\u6a21\u578b\u3002\u540e\u8bad\u7ec3\u6a21\u578b\u5957\u4ef6\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86RynnBrain\u7684\u4e24\u4e2a\u5173\u952e\u6f5c\u529b\uff1a\u652f\u6301\u7269\u7406\u57fa\u7840\u63a8\u7406\u548c\u89c4\u5212\uff0c\u4ee5\u53ca\u4f5c\u4e3a\u53ef\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u5177\u8eab\u4efb\u52a1\u7684\u5f3a\u5927\u9884\u8bad\u7ec3\u9aa8\u5e72\u3002", "conclusion": "RynnBrain\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u7269\u7406\u57fa\u7840\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u67b6\u6784\u3002"}}
{"id": "2602.15010", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15010", "abs": "https://arxiv.org/abs/2602.15010", "authors": ["Max Sobol Mark", "Jacky Liang", "Maria Attarian", "Chuyuan Fu", "Debidatta Dwibedi", "Dhruv Shah", "Aviral Kumar"], "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames", "comment": null, "summary": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.", "AI": {"tldr": "\u63d0\u51faBig Picture Policies (BPP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u5173\u952e\u5e27\u6765\u51cf\u5c11\u8bad\u7ec3\u4e0e\u90e8\u7f72\u95f4\u7684\u5206\u5e03\u504f\u79fb\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u7b56\u7565\u5728\u5386\u53f2\u89c2\u6d4b\u6761\u4ef6\u5316\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\u3002", "motivation": "\u8bb8\u591a\u673a\u5668\u4eba\u4efb\u52a1\u9700\u8981\u5173\u6ce8\u5386\u53f2\u89c2\u6d4b\uff0c\u4f46\u5f53\u524d\u6700\u4f73\u7b56\u7565\u901a\u5e38\u4ec5\u57fa\u4e8e\u5f53\u524d\u89c2\u6d4b\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u6734\u7d20\u5730\u6761\u4ef6\u5316\u5386\u53f2\u89c2\u6d4b\u5e38\u56e0\u865a\u5047\u76f8\u5173\u6027\u800c\u5931\u8d25\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u671f\u95f4\u5386\u53f2\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faBig Picture Policies (BPP)\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u6700\u5c0f\u5316\u6709\u610f\u4e49\u7684\u4efb\u52a1\u76f8\u5173\u5173\u952e\u5e27\uff0c\u5c06\u591a\u6837\u5316\u7684\u8f68\u8ff9\u6295\u5f71\u5230\u7d27\u51d1\u7684\u4e8b\u4ef6\u96c6\u5408\u4e0a\uff0c\u51cf\u5c11\u5206\u5e03\u504f\u79fb\u540c\u65f6\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u548c\u4e09\u4e2a\u4eff\u771f\u4efb\u52a1\u4e0a\u8bc4\u4f30BPP\uff0c\u6240\u6709\u4efb\u52a1\u90fd\u9700\u8981\u5386\u53f2\u6761\u4ef6\u5316\u3002BPP\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\u6bd4\u6700\u4f73\u5bf9\u6bd4\u65b9\u6cd5\u83b7\u5f9770%\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "BPP\u901a\u8fc7\u68c0\u6d4b\u4efb\u52a1\u76f8\u5173\u5173\u952e\u5e27\u6709\u6548\u89e3\u51b3\u4e86\u5386\u53f2\u6761\u4ef6\u5316\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7b56\u7565\u5728\u9700\u8981\u8bb0\u5fc6\u5386\u53f2\u7684\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.14169", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14169", "abs": "https://arxiv.org/abs/2602.14169", "authors": ["Yiran Guo", "Zhongjian Qiao", "Yingqi Xie", "Jie Liu", "Dan Ye", "Ruiqing Zhang", "Shuang Qiu", "Lijie Xu"], "title": "Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling", "comment": null, "summary": "Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.", "AI": {"tldr": "DDE\uff08Deep Dense Exploration\uff09\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u63a2\u7d22\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u7684\u6df1\u5ea6\u53ef\u6062\u590d\u72b6\u6001\uff08pivots\uff09\u8fdb\u884c\u5bc6\u96c6\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u91c7\u6837\u7a00\u91ca\u548c\u6df1\u5ea6\u72b6\u6001\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a2\u7d22\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\uff1aGRPO\u4ec5\u4ece\u6839\u8282\u70b9\u91c7\u6837\uff0c\u5bfc\u81f4\u9ad8\u6982\u7387\u8f68\u8ff9\u9971\u548c\u800c\u6df1\u5ea6\u9519\u8bef\u72b6\u6001\u63a2\u7d22\u4e0d\u8db3\uff1b\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\u76f2\u76ee\u5206\u6563\u91c7\u6837\u9884\u7b97\uff0c\u9020\u6210\u91c7\u6837\u7a00\u91ca\uff0c\u65e0\u6cd5\u53d1\u73b0\u7f55\u89c1\u6b63\u786e\u540e\u7f00\u5e76\u7834\u574f\u5c40\u90e8\u57fa\u7ebf\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faDDE\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u5931\u8d25\u8f68\u8ff9\u4e2d\u7684pivots\uff08\u6df1\u5ea6\u53ef\u6062\u590d\u72b6\u6001\uff09\u3002\u5177\u4f53\u5b9e\u73b0\u4e3aDEEP-GRPO\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u8f7b\u91cf\u7ea7\u6570\u636e\u9a71\u52a8\u7684\u6548\u7528\u51fd\u6570\uff0c\u81ea\u52a8\u5e73\u8861\u53ef\u6062\u590d\u6027\u548c\u6df1\u5ea6\u504f\u5dee\u4ee5\u8bc6\u522bpivot\u72b6\u6001\uff1b2\uff09\u5728\u6bcf\u4e2apivot\u8fdb\u884c\u5c40\u90e8\u5bc6\u96c6\u91cd\u91c7\u6837\uff0c\u589e\u52a0\u53d1\u73b0\u6b63\u786e\u540e\u7eed\u8f68\u8ff9\u7684\u6982\u7387\uff1b3\uff09\u53cc\u6d41\u4f18\u5316\u76ee\u6807\uff0c\u5c06\u5168\u5c40\u7b56\u7565\u5b66\u4e60\u4e0e\u5c40\u90e8\u7ea0\u6b63\u66f4\u65b0\u89e3\u8026\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u91c7\u6837\u9884\u7b97\u4e0b\u4e00\u81f4\u4f18\u4e8eGRPO\u3001\u57fa\u4e8e\u6811\u7684\u65b9\u6cd5\u548c\u5176\u4ed6\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DDE\u901a\u8fc7\u4e13\u6ce8\u4e8e\u6df1\u5ea6\u53ef\u6062\u590d\u72b6\u6001\u7684\u5bc6\u96c6\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6311\u6218\uff0c\u5728\u6709\u9650\u91c7\u6837\u9884\u7b97\u4e0b\u80fd\u591f\u53d1\u73b0\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002"}}
{"id": "2602.15018", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15018", "abs": "https://arxiv.org/abs/2602.15018", "authors": ["Richeek Das", "Pratik Chaudhari"], "title": "Neurosim: A Fast Simulator for Neuromorphic Robot Perception", "comment": "13 pages, 6 figures", "summary": "Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .", "AI": {"tldr": "Neurosim\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u4f20\u611f\u5668\u6a21\u62df\u5e93\uff0c\u652f\u6301\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u3001RGB\u76f8\u673a\u3001\u6df1\u5ea6\u4f20\u611f\u5668\u548c\u60ef\u6027\u4f20\u611f\u5668\u6a21\u62df\uff0c\u4ee5\u53ca\u591a\u65cb\u7ffc\u98de\u884c\u5668\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u684c\u9762GPU\u4e0a\u53ef\u8fbe~2700 FPS\u3002\u914d\u5408Cortex\u901a\u4fe1\u5e93\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u548c\u673a\u5668\u4eba\u5de5\u4f5c\u6d41\u3002", "motivation": "\u4e3a\u795e\u7ecf\u5f62\u6001\u611f\u77e5\u548c\u63a7\u5236\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u540c\u6b65\u548c\u5b9e\u65f6\u95ed\u73af\u6d4b\u8bd5\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u62df\u5de5\u5177\u5728\u6027\u80fd\u548c\u96c6\u6210\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86Neurosim\u9ad8\u6027\u80fd\u4f20\u611f\u5668\u6a21\u62df\u5e93\uff0c\u652f\u6301\u591a\u79cd\u4f20\u611f\u5668\u7c7b\u578b\u548c\u98de\u884c\u5668\u52a8\u529b\u5b66\u6a21\u62df\uff1b\u914d\u5408\u57fa\u4e8eZeroMQ\u7684Cortex\u901a\u4fe1\u5e93\uff0c\u63d0\u4f9b\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u7684\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u652f\u6301NumPy\u6570\u7ec4\u548cPyTorch\u5f20\u91cf\u3002", "result": "Neurosim\u5728\u684c\u9762GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe~2700 FPS\u7684\u5e27\u7387\uff0c\u80fd\u591f\u5b9e\u65f6\u6a21\u62df\u590d\u6742\u52a8\u6001\u73af\u5883\uff1bCortex\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684Python/C++\u901a\u4fe1\u63a5\u53e3\uff0c\u652f\u6301\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u65f6\u95f4\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u3002", "conclusion": "Neurosim\u548cCortex\u4e3a\u795e\u7ecf\u5f62\u6001\u611f\u77e5\u548c\u63a7\u5236\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u96c6\u6210\u7684\u5e73\u53f0\uff0c\u652f\u6301\u4ece\u8bad\u7ec3\u5230\u5b9e\u65f6\u95ed\u73af\u6d4b\u8bd5\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2602.14200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14200", "abs": "https://arxiv.org/abs/2602.14200", "authors": ["Nicolas Zumarraga", "Thomas Kaar", "Ning Wang", "Maxwell A. Xu", "Max Rosenblattl", "Markus Kreft", "Kevin O'Sullivan", "Paul Schmiedmayer", "Patrick Langer", "Robert Jakob"], "title": "TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models", "comment": "18 pages, 13 figures, 10 tables (main paper: 5 pages, 3 figures, 2 tables)", "summary": "Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.", "AI": {"tldr": "TS-Haystack\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e2d\u7684\u5c40\u9650\u6027\uff1a\u73b0\u6709\u6a21\u578b\u538b\u7f29\u7b56\u7565\u867d\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5374\u635f\u5bb3\u4e86\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u7a81\u663e\u4e86\u9700\u8981\u89e3\u8026\u5e8f\u5217\u957f\u5ea6\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u67b6\u6784\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u4f20\u611f\u5668\u6570\u636e\u53ef\u8fbe\u6570\u767e\u4e07\u6570\u636e\u70b9\uff0c\u800c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u77ed\u5e8f\u5217\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u80fd\u7cfb\u7edf\u8bc4\u4f30\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faTS-Haystack\u957f\u4e0a\u4e0b\u6587\u65f6\u95f4\u68c0\u7d22\u57fa\u51c6\uff0c\u5305\u542b\u76f4\u63a5\u68c0\u7d22\u3001\u65f6\u95f4\u63a8\u7406\u3001\u591a\u6b65\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u5f02\u5e38\u56db\u7c7b\u5171\u5341\u79cd\u4efb\u52a1\u7c7b\u578b\u3002\u901a\u8fc7\u5728\u957f\u7eb5\u5411\u52a0\u901f\u5ea6\u8ba1\u8bb0\u5f55\u4e2d\u5d4c\u5165\u77ed\u6d3b\u52a8\u7247\u6bb5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4ece\u79d2\u52302\u5c0f\u65f6\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u6027\u80fd\u3002", "result": "\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u538b\u7f29\u5728\u9ad8\u8fbe176\u500d\u538b\u7f29\u6bd4\u4e0b\u80fd\u4fdd\u6301\u6216\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f46\u68c0\u7d22\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u5bfc\u81f4\u65f6\u95f4\u5b9a\u4f4d\u4fe1\u606f\u4e22\u5931\u3002\u5206\u7c7b\u4e0e\u68c0\u7d22\u884c\u4e3a\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "conclusion": "\u9700\u8981\u8bbe\u8ba1\u80fd\u89e3\u8026\u5e8f\u5217\u957f\u5ea6\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u4fdd\u771f\u5ea6\u7684\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e2d\u7684\u6839\u672c\u5c40\u9650\u6027\u3002"}}
{"id": "2602.13248", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13248", "abs": "https://arxiv.org/abs/2602.13248", "authors": ["Ashkan Y. Zadeh", "Xiaomeng Li", "Andry Rakotonirainy", "Ronald Schroeter", "Sebastien Glaser", "Zishuo Zhu"], "title": "X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles", "comment": null, "summary": "Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.\n  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.\n  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.\n  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86X-Blocks\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u8bed\u8a00\u6784\u5efa\u5757\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u3001\u53e5\u6cd5\u548c\u8bcd\u6c47\u4e09\u4e2a\u5c42\u6b21\uff0c\u5176\u4e2dRACE\u6846\u67b6\u80fd\u51c6\u786e\u5206\u7c7b32\u79cd\u573a\u666f\u611f\u77e5\u7684\u89e3\u91ca\u7c7b\u522b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u6765\u5206\u6790\u4eba\u7c7b\u5982\u4f55\u5728\u4e0d\u540c\u9a7e\u9a76\u573a\u666f\u4e2d\u8bed\u8a00\u6784\u5efa\u9a7e\u9a76\u539f\u7406\uff0c\u800c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5bf9\u4e8e\u5efa\u7acb\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u4fe1\u4efb\u548c\u63a5\u53d7\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faX-Blocks\u5206\u5c42\u5206\u6790\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5c42\u6b21\uff1a\u4e0a\u4e0b\u6587\u5c42\u9762\u4f7f\u7528RACE\u591aLLM\u96c6\u6210\u6846\u67b6\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u81ea\u4e00\u81f4\u6027\u673a\u5236\u8fdb\u884c\u5206\u7c7b\uff1b\u8bcd\u6c47\u5c42\u9762\u4f7f\u7528\u5e26\u4fe1\u606f\u6027\u72c4\u5229\u514b\u96f7\u5148\u9a8c\u7684\u5bf9\u6570\u51e0\u7387\u5206\u6790\uff1b\u53e5\u6cd5\u5c42\u9762\u4f7f\u7528\u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u548c\u6a21\u677f\u63d0\u53d6\u3002", "result": "RACE\u5728Berkeley DeepDrive-X\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.45%\u7684\u51c6\u786e\u7387\u548c0.91\u7684Cohen's kappa\u7cfb\u6570\uff1b\u8bcd\u6c47\u5206\u6790\u63ed\u793a\u4e86\u573a\u666f\u7279\u5b9a\u7684\u8bcd\u6c47\u6a21\u5f0f\uff1b\u53e5\u6cd5\u5206\u6790\u663e\u793a\u89e3\u91ca\u4f7f\u7528\u6709\u9650\u7684\u8bed\u6cd5\u5bb6\u65cf\uff0c\u8c13\u8bcd\u7c7b\u578b\u548c\u56e0\u679c\u7ed3\u6784\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u6709\u7cfb\u7edf\u53d8\u5316\u3002", "conclusion": "X-Blocks\u6846\u67b6\u662f\u6570\u636e\u96c6\u65e0\u5173\u548c\u4efb\u52a1\u72ec\u7acb\u7684\uff0c\u4e3a\u751f\u6210\u652f\u6301\u900f\u660e\u5ea6\u3001\u7528\u6237\u4fe1\u4efb\u548c\u8ba4\u77e5\u53ef\u53ca\u6027\u7684\u573a\u666f\u611f\u77e5\u89e3\u91ca\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u8bed\u8a00\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2602.14208", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14208", "abs": "https://arxiv.org/abs/2602.14208", "authors": ["Jinbo Wang", "Binghui Li", "Zhanpeng Zhou", "Mingze Wang", "Yuxuan Sun", "Jiaqi Zhang", "Xunliang Cai", "Lei Wu"], "title": "Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws", "comment": "34 pages, accepted by ICLR 2026 as a conference paper", "summary": "Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u51fd\u6570\u7f29\u653e\u5b9a\u5f8b\u6846\u67b6\u5206\u6790\u6279\u91cf\u5927\u5c0f\u8c03\u5ea6\uff0c\u53d1\u73b0\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u53d6\u51b3\u4e8e\u4efb\u52a1\u96be\u5ea6\uff1a\u7b80\u5355\u4efb\u52a1\u5e94\u6301\u7eed\u589e\u52a0\u6279\u91cf\u5927\u5c0f\uff0c\u800c\u56f0\u96be\u4efb\u52a1\u5e94\u5728\u8bad\u7ec3\u540e\u671f\u624d\u5207\u6362\u5230\u5927\u6279\u91cf\uff0c\u8fd9\u80fd\u663e\u8457\u51cf\u5c11\u6570\u636e\u6d88\u8017\u4e14\u4e0d\u635f\u5931\u6027\u80fd\u3002", "motivation": "\u6279\u91cf\u5927\u5c0f\u8c03\u5ea6\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7406\u8bba\u57fa\u7840\u8584\u5f31\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5efa\u7acb\u6279\u91cf\u5927\u5c0f\u8c03\u5ea6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7406\u89e3\u4e0d\u540c\u4efb\u52a1\u96be\u5ea6\u4e0b\u7684\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u3002", "method": "\u91c7\u7528\u51fd\u6570\u7f29\u653e\u5b9a\u5f8b\u6846\u67b6\u5206\u6790\u6279\u91cf\u5927\u5c0f\u8c03\u5ea6\uff0c\u7406\u8bba\u63a8\u5bfc\u6700\u4f18\u8c03\u5ea6\u7ed3\u6784\uff0c\u63ed\u793a\"\u5feb\u901f\u8ffd\u8d76\u6548\u5e94\"\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u7b80\u5355\u4efb\u52a1\u5e94\u6301\u7eed\u589e\u52a0\u6279\u91cf\u5927\u5c0f\uff1b\u56f0\u96be\u4efb\u52a1\u5e94\u5728\u8bad\u7ec3\u540e\u671f\u5207\u6362\u5230\u5927\u6279\u91cf\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u540e\u671f\u5207\u6362\u8c03\u5ea6\u7b56\u7565\u5728\u5bc6\u96c6\u548cMoE\u67b6\u6784\u76841.1B\u53c2\u6570\u6a21\u578b\u4e2d\u5747\u4f18\u4e8e\u6052\u5b9a\u6279\u91cf\u5927\u5c0f\u548c\u65e9\u671f\u5207\u6362\u57fa\u7ebf\u3002", "conclusion": "\u6279\u91cf\u5927\u5c0f\u8c03\u5ea6\u7b56\u7565\u5e94\u57fa\u4e8e\u4efb\u52a1\u96be\u5ea6\u8bbe\u8ba1\uff0c\u56f0\u96be\u4efb\u52a1\u91c7\u7528\u540e\u671f\u5927\u6279\u91cf\u5207\u6362\u7b56\u7565\u53ef\u663e\u8457\u51cf\u5c11\u6570\u636e\u6d88\u8017\u800c\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2602.13267", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.13267", "abs": "https://arxiv.org/abs/2602.13267", "authors": ["Hengyu Mu", "Jianshi Wu", "Yuxin Guo", "XianLian Lin", "Qingyong Hu", "Chenglu Wen", "Cheng Wang"], "title": "Beyond Ground: Map-Free LiDAR Relocalization for UAVs", "comment": "18 pages, 16 figures", "summary": "Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.", "AI": {"tldr": "MAILS\uff1a\u4e00\u79cd\u9762\u5411\u65e0\u4eba\u673a\u7684\u65e0\u5730\u56feLiDAR\u91cd\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u4fdd\u6301\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u3001\u5750\u6807\u65e0\u5173\u7279\u5f81\u521d\u59cb\u5316\u548c\u5c40\u90e8\u4e0d\u53d8\u4f4d\u7f6e\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u73b0\u6709LiDAR\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\uff0c\u5728\u65e0\u4eba\u673a\u573a\u666f\u4e0b\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002\u65e0\u4eba\u673a\u98de\u884c\u4e2d\u5b58\u5728\u5927\u89d2\u5ea6\u504f\u822a\u65cb\u8f6c\u548c\u9ad8\u5ea6\u53d8\u5316\uff0c\u4e14\u7f3a\u4e4f\u771f\u5b9e\u65e0\u4eba\u673a\u98de\u884c\u7279\u6027\u7684LiDAR\u91cd\u5b9a\u4f4d\u6570\u636e\u96c6", "method": "\u63d0\u51faMAILS\u6846\u67b6\uff1a1) \u5c40\u90e8\u4fdd\u6301\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u6a21\u5757\u4ece\u7a00\u758f\u70b9\u4e91\u63d0\u53d6\u5c40\u90e8\u5224\u522b\u6027\u51e0\u4f55\u7279\u5f81\uff1b2) \u5750\u6807\u65e0\u5173\u7279\u5f81\u521d\u59cb\u5316\u6a21\u5757\u548c\u5c40\u90e8\u4e0d\u53d8\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u5904\u7406\u65e0\u4eba\u673a\u98de\u884c\u4e2d\u7684\u504f\u822a\u65cb\u8f6c\u548c\u9ad8\u5ea6\u53d8\u5316\uff1b3) \u6784\u5efa\u5927\u89c4\u6a21\u65e0\u4eba\u673aLiDAR\u5b9a\u4f4d\u6570\u636e\u96c6\uff0c\u5305\u542b\u56db\u4e2a\u573a\u666f\u548c\u591a\u79cd\u98de\u884c\u8f68\u8ff9", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u8fbe\u5230\u6ee1\u610f\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03", "conclusion": "MAILS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u573a\u666f\u4e0bLiDAR\u91cd\u5b9a\u4f4d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u63d0\u53d6\u548c\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u5728\u771f\u5b9e\u65e0\u4eba\u673a\u98de\u884c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d"}}
{"id": "2602.14209", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14209", "abs": "https://arxiv.org/abs/2602.14209", "authors": ["Omin Kwon", "Yeonjae Kim", "Doyeon Kim", "Minseo Kim", "Yeonhong Park", "Jae W. Lee"], "title": "MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM", "comment": null, "summary": "Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.", "AI": {"tldr": "MAGE\u662f\u4e00\u79cd\u9488\u5bf9\u5757\u6269\u6563LLM\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9996\u6b21\u53bb\u566a\u6b65\u9aa4\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u9884\u6d4b\u91cd\u8981KV\u6761\u76ee\uff0c\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7a00\u758f\u53bb\u566a\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5e76\u52a0\u901f\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u3002", "motivation": "\u5757\u6269\u6563LLM\u5728\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u4e2d\u9762\u4e34KV\u7f13\u5b58\u5185\u5b58\u8bbf\u95ee\u74f6\u9888\uff0c\u73b0\u6709\u4e3a\u81ea\u56de\u5f52LLM\u8bbe\u8ba1\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u5757\u6269\u6563\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5757\u6269\u6563\u7279\u6027\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "MAGE\u5229\u7528\u5757\u6269\u6563\u7279\u6709\u7684\u673a\u4f1a\uff1a\u9996\u6b21\u5168[MASK]\u53bb\u566a\u6b65\u9aa4\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u80fd\u53ef\u9760\u9884\u6d4b\u91cd\u8981KV\u6761\u76ee\u548c\u9884\u7b97\u9700\u6c42\uff0c\u901a\u8fc7\u5355\u6b21\u7cbe\u786e\u6ce8\u610f\u529b\u8ba1\u7b97\u5e76\u91cd\u7528\u4e8e\u65e0\u8bad\u7ec3\u7a00\u758f\u53bb\u566a\uff0c\u540c\u65f6\u91c7\u7528\u8f7b\u91cf\u7ea7\u5fae\u8c03\u7b56\u7565\u5f3a\u5316[MASK]\u5f15\u5bfc\u6a21\u5f0f\u3002", "result": "\u5728LongBench\u548cNeedle-in-a-Haystack\u7b49\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMAGE\u4ee5\u5c11\u91cfKV\u9884\u7b97\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u7684\u51c6\u786e\u7387\uff0c\u7aef\u5230\u7aef\u901f\u5ea6\u63d0\u53473-4\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u81ea\u56de\u5f52\u5bfc\u5411\u7684\u7a00\u758f\u6ce8\u610f\u529b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MAGE\u901a\u8fc7\u5229\u7528\u5757\u6269\u6563\u7684\u72ec\u7279\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u751f\u6210\uff0c\u4ec5\u9700\u5355GPU\u6570\u5c0f\u65f6\u5fae\u8c03\u5373\u53ef\u5e94\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u4e3a\u5757\u6269\u6563LLM\u7684\u5185\u5b58\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13324", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13324", "abs": "https://arxiv.org/abs/2602.13324", "authors": ["Jesse Barkley", "Abraham George", "Amir Barati Farimani"], "title": "Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge", "comment": "8 Pages, 3 Figures", "summary": "Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel \"Controlled Input\" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u76ee\u6807\u68c0\u6d4b\u4e0e\u7d27\u51d1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u519b\u4e8b\u8fb9\u7f18\u673a\u5668\u4eba\u81ea\u4e3b\u90e8\u7f72\uff0c\u5728\u5408\u6210\u6218\u573a\u89c6\u9891\u4e0a\u9a8c\u8bc1\u4e86\u9ad8\u7cbe\u5ea6\u6027\u80fd\u3002", "motivation": "\u519b\u4e8b\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u81ea\u4e3b\u8fb9\u7f18\u673a\u5668\u4eba\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u8fb9\u7f18\u786c\u4ef6\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u4e14\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7ea7\u8054\u67b6\u6784\uff1aGrounding DINO\u4f5c\u4e3a\u9ad8\u53ec\u56de\u7387\u3001\u6587\u672c\u53ef\u63d0\u793a\u7684\u533a\u57df\u63d0\u8bae\u5668\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u68c0\u6d4b\u5e27\u4f20\u9012\u7ed9\u8fb9\u7f18\u7ea7VLMs\uff08Qwen\u548cGemma\u5bb6\u65cf\uff0c4B-12B\u53c2\u6570\uff09\u8fdb\u884c\u8bed\u4e49\u9a8c\u8bc1\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u4fa6\u5bdf-\u6307\u6325\u5b98\u5de5\u4f5c\u6d41\uff0c\u5e76\u5f15\u5165\"\u53d7\u63a7\u8f93\u5165\"\u65b9\u6cd5\u5206\u79bb\u611f\u77e5\u4e0e\u63a8\u7406\u3002", "result": "\u572855\u4e2a\u9ad8\u4fdd\u771f\u5408\u6210\u6218\u573a\u89c6\u9891\u4e0a\u8bc4\u4f30\uff1a\u8bef\u62a5\u8fc7\u6ee4\u8fbe100%\u51c6\u786e\u7387\uff0c\u635f\u4f24\u8bc4\u4f30\u8fbe97.5%\uff0c\u7ec6\u7c92\u5ea6\u8f66\u8f86\u5206\u7c7b55-90%\u3002\u4fa6\u5bdf-\u6307\u6325\u5b98\u5de5\u4f5c\u6d41\u5b9e\u73b0100%\u6b63\u786e\u8d44\u4ea7\u90e8\u7f72\uff0c\u63a8\u7406\u5f97\u52069.8/10\uff0c\u5ef6\u8fdf\u4f4e\u4e8e75\u79d2\u3002\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\u5dee\u5f02\u3002", "conclusion": "\u5206\u5c42\u96f6\u6837\u672c\u67b6\u6784\u9002\u7528\u4e8e\u8fb9\u7f18\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2dVLM\u9002\u7528\u6027\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u8bca\u65ad\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u7279\u5f02\u6027\u3002"}}
{"id": "2602.14231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14231", "abs": "https://arxiv.org/abs/2602.14231", "authors": ["Seyedsaman Emami", "Daniel Hern\u00e1ndez-Lobato", "Gonzalo Mart\u00ednez-Mu\u00f1oz"], "title": "Robust multi-task boosting using clustering and local ensembling", "comment": null, "summary": "Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.", "AI": {"tldr": "RMB-CLE\uff1a\u57fa\u4e8e\u805a\u7c7b\u548c\u5c40\u90e8\u96c6\u6210\u7684\u65b0\u578b\u9c81\u68d2\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u8bef\u5dee\u81ea\u9002\u5e94\u805a\u7c7b\u4efb\u52a1\uff0c\u9632\u6b62\u8d1f\u8fc1\u79fb\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5728\u4efb\u52a1\u4e0d\u76f8\u5173\u6216\u5b58\u5728\u566a\u58f0\u65f6\u5bb9\u6613\u53d1\u751f\u8d1f\u8fc1\u79fb\uff0c\u5f3a\u5236\u5171\u4eab\u8868\u793a\u4f1a\u635f\u5bb3\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u8bc6\u522b\u4efb\u52a1\u5173\u7cfb\u5e76\u9632\u6b62\u8d1f\u8fc1\u79fb\u7684\u9c81\u68d2\u6846\u67b6\u3002", "method": "\u63d0\u51faRMB-CLE\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u8de8\u4efb\u52a1\u8bef\u5dee\u63a8\u5bfc\u4efb\u52a1\u95f4\u76f8\u4f3c\u5ea6\uff0c\u5c06\u98ce\u9669\u5206\u89e3\u4e3a\u51fd\u6570\u4e0d\u5339\u914d\u548c\u4e0d\u53ef\u7ea6\u566a\u58f0\uff1b2\uff09\u4f7f\u7528\u51dd\u805a\u805a\u7c7b\u81ea\u9002\u5e94\u5206\u7ec4\u4efb\u52a1\uff1b3\uff09\u5728\u6bcf\u4e2a\u805a\u7c7b\u5185\u4f7f\u7528\u5c40\u90e8\u96c6\u6210\u5b9e\u73b0\u9c81\u68d2\u77e5\u8bc6\u5171\u4eab\uff0c\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u6a21\u5f0f\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e2d\u80fd\u591f\u6062\u590d\u771f\u5b9e\u805a\u7c7b\u7ed3\u6784\uff1b\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u81f4\u4f18\u4e8e\u591a\u4efb\u52a1\u3001\u5355\u4efb\u52a1\u548c\u57fa\u4e8e\u6c60\u5316\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "conclusion": "RMB-CLE\u4e0d\u4ec5\u7ed3\u5408\u4e86\u805a\u7c7b\u548c\u63d0\u5347\u6280\u672f\uff0c\u66f4\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4e3a\u9c81\u68d2\u591a\u4efb\u52a1\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u80fd\u591f\u6709\u6548\u9632\u6b62\u8d1f\u8fc1\u79fb\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.13329", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13329", "abs": "https://arxiv.org/abs/2602.13329", "authors": ["Yiru Wang", "Zichong Gu", "Yu Gao", "Anqing Jiang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun"], "title": "HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving", "comment": null, "summary": "Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.\n  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.", "AI": {"tldr": "HiST-VLA\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u5206\u5c42\u65f6\u7a7a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u3001\u52a8\u6001\u4ee4\u724c\u7a00\u758f\u5316\u548c\u5206\u5c42\u89c4\u5212\u5668\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u7684\u6570\u503c\u63a8\u7406\u4e0d\u7cbe\u786e\u30013D\u7a7a\u95f4\u611f\u77e5\u5f31\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u95ee\u9898\uff0c\u5728NAVSIM v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u6570\u503c\u63a8\u7406\u4e0d\u7cbe\u786e\u30013D\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u5f31\u3001\u5bf9\u4e0a\u4e0b\u6587\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u4e9b\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u3002", "method": "1. \u96c6\u6210\u51e0\u4f55\u611f\u77e5\u4e0e\u7ec6\u7c92\u5ea6\u9a7e\u9a76\u547d\u4ee4\u548c\u72b6\u6001\u5386\u53f2\u63d0\u793a\u6765\u589e\u5f3a3D\u65f6\u7a7a\u63a8\u7406\uff1b2. \u5728VLA\u67b6\u6784\u4e2d\u96c6\u6210\u52a8\u6001\u4ee4\u724c\u7a00\u758f\u5316\u6280\u672f\uff0c\u878d\u5408\u5197\u4f59\u4ee4\u724c\u800c\u975e\u8fc7\u6ee4\uff1b3. \u4f7f\u7528\u5206\u5c42\u53d8\u6362\u5668\u89c4\u5212\u5668\u9010\u6b65\u5c06\u7c97\u7c92\u5ea6VLA\u8def\u5f84\u70b9\u7ec6\u5316\u4e3a\u7ec6\u7c92\u5ea6\u8f68\u8ff9\uff1b4. \u901a\u8fc7\u52a8\u6001\u6f5c\u5728\u6b63\u5219\u5316\u6574\u5408\u8bed\u8a00\u547d\u4ee4\uff0c\u786e\u4fdd\u4e25\u683c\u7684\u7a7a\u95f4\u57fa\u7840\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "\u5728NAVSIM v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff1aNavtest\u4e0aEPDMS\u8fbe\u523088.6\uff0c\u4f2a\u95ed\u73afNavhard\u57fa\u51c6\u4e0aEPDMS\u8fbe\u523050.9\u3002", "conclusion": "HiST-VLA\u901a\u8fc7\u5206\u5c42\u65f6\u7a7a\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14233", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2602.14233", "abs": "https://arxiv.org/abs/2602.14233", "authors": ["Yaxuan Kong", "Hoyoung Lee", "Yoontae Hwang", "Alejandro Lopez-Lira", "Bradford Levy", "Dhagash Mehta", "Qingsong Wen", "Chanyeol Choi", "Yongjae Lee", "Stefan Zohren"], "title": "Evaluating LLMs in Finance Requires Explicit Bias Consideration", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u91d1\u878d\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u5b58\u5728\u4e94\u79cd\u5e38\u89c1\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u4f1a\u5938\u5927\u6027\u80fd\u3001\u6c61\u67d3\u56de\u6d4b\u7ed3\u679c\uff0c\u5e76\u5bfc\u81f4\u62a5\u544a\u7ed3\u679c\u5bf9\u90e8\u7f72\u58f0\u660e\u65e0\u6548\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed3\u6784\u6709\u6548\u6027\u6846\u67b6\u548c\u8bc4\u4f30\u6e05\u5355\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u91d1\u878d\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u4f46\u8bc4\u4f30\u5b9e\u8df5\u672a\u80fd\u8ddf\u4e0a\u3002\u91d1\u878d\u7279\u5b9a\u504f\u5dee\u4f1a\u5938\u5927\u6027\u80fd\u8868\u73b0\u3001\u6c61\u67d3\u56de\u6d4b\u7ed3\u679c\uff0c\u4f7f\u5f97\u62a5\u544a\u7ed3\u679c\u5bf9\u4efb\u4f55\u90e8\u7f72\u58f0\u660e\u90fd\u65e0\u6548\u3002\u5f53\u524d\u7814\u7a76\u5bf9\u8fd9\u4e9b\u504f\u5dee\u7684\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u8bc6\u522b\u4e86\u91d1\u878dLLM\u5e94\u7528\u4e2d\u7684\u4e94\u79cd\u5e38\u89c1\u504f\u5dee\uff1a\u524d\u77bb\u504f\u5dee\u3001\u5e78\u5b58\u8005\u504f\u5dee\u3001\u53d9\u4e8b\u504f\u5dee\u3001\u76ee\u6807\u504f\u5dee\u548c\u6210\u672c\u504f\u5dee\u3002\u56de\u987e\u4e862023-2025\u5e74\u7684164\u7bc7\u8bba\u6587\uff0c\u5206\u6790\u504f\u5dee\u8ba8\u8bba\u60c5\u51b5\u3002\u63d0\u51fa\u4e86\u7ed3\u6784\u6709\u6548\u6027\u6846\u67b6\u548c\u5305\u542b\u6700\u5c0f\u8981\u6c42\u7684\u8bc4\u4f30\u6e05\u5355\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728164\u7bc7\u8bba\u6587\u4e2d\uff0c\u6ca1\u6709\u4efb\u4f55\u5355\u4e00\u504f\u5dee\u5728\u8d85\u8fc728%\u7684\u7814\u7a76\u4e2d\u88ab\u8ba8\u8bba\u3002\u8fd9\u4e9b\u504f\u5dee\u4f1a\u4ee5\u4e0d\u540c\u65b9\u5f0f\u7834\u574f\u91d1\u878d\u4efb\u52a1\uff0c\u5e76\u4e14\u5e38\u5e38\u76f8\u4e92\u53e0\u52a0\uff0c\u4ea7\u751f\u6709\u6548\u6027\u5e7b\u89c9\u3002", "conclusion": "\u91d1\u878dLLM\u7cfb\u7edf\u4e2d\u7684\u504f\u5dee\u9700\u8981\u660e\u786e\u5173\u6ce8\uff0c\u5728\u652f\u6301\u90e8\u7f72\u58f0\u660e\u4e4b\u524d\u5e94\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u6709\u6548\u6027\u3002\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6e05\u5355\u4e3a\u504f\u5dee\u8bca\u65ad\u548c\u672a\u6765\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6700\u5c0f\u8981\u6c42\u3002"}}
{"id": "2602.13347", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13347", "abs": "https://arxiv.org/abs/2602.13347", "authors": ["Lijun Zhang", "Nikhil Chacko", "Petter Nilsson", "Ruinian Xu", "Shantanu Thakar", "Bai Lou", "Harpreet Sawhney", "Zhebin Zhang", "Mudit Agrawal", "Bhavana Chandrashekhar", "Aaron Parness"], "title": "Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots", "comment": "20 pages, 16 figures", "summary": "Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.", "AI": {"tldr": "FOREST\u662f\u4e00\u4e2a\u4ed3\u5e93\u5b58\u50a8\u610f\u56fe\u6761\u4ef6\u5316\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u53d8\u6362\u5668\u9884\u6d4b\u5b58\u50a8\u64cd\u4f5c\u540e\u7684\u8d27\u7bb1\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u4ed3\u5e93\u6267\u884c\u6570\u767e\u4e07\u6b21\u5b58\u50a8\u64cd\u4f5c\uff0c\u9700\u8981\u5728\u771f\u5b9e\u6267\u884c\u524d\u6839\u636e\u5f53\u524d\u89c2\u5bdf\u548c\u8ba1\u5212\u5b58\u50a8\u884c\u4e3a\u9884\u6d4b\u8d27\u7bb1\u72b6\u6001\uff0c\u8fd9\u5bf9\u7cfb\u7edf\u89c4\u5212\u5f88\u6709\u4ef7\u503c\u3002", "method": "\u63d0\u51faFOREST\u6a21\u578b\uff0c\u5c06\u8d27\u7bb1\u72b6\u6001\u8868\u793a\u4e3a\u7269\u54c1\u5bf9\u9f50\u7684\u5b9e\u4f8b\u63a9\u7801\uff0c\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u53d8\u6362\u5668\u4ece\u89c2\u5bdf\u4e0a\u4e0b\u6587\u4e2d\u9884\u6d4b\u5b58\u50a8\u540e\u7684\u914d\u7f6e\u3002", "result": "FOREST\u663e\u8457\u6539\u5584\u4e86\u9884\u6d4b\u4e0e\u771f\u5b9e\u5b58\u50a8\u540e\u5e03\u5c40\u4e4b\u95f4\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u7528FOREST\u9884\u6d4b\u66ff\u6362\u771f\u5b9e\u540e\u5b58\u50a8\u63a9\u7801\u4ec5\u5bfc\u81f4\u9002\u5ea6\u7684\u6027\u80fd\u635f\u5931\u3002", "conclusion": "FOREST\u80fd\u591f\u4e3a\u4ed3\u5e93\u89c4\u5212\u63d0\u4f9b\u6709\u7528\u7684\u524d\u77bb\u4fe1\u53f7\uff0c\u5728\u8d1f\u8f7d\u8d28\u91cf\u8bc4\u4f30\u548c\u591a\u5b58\u50a8\u63a8\u7406\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2602.13440", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13440", "abs": "https://arxiv.org/abs/2602.13440", "authors": ["Sebastian-Ion Nae", "Mihai-Eugen Barbu", "Sebastian Mocanu", "Marius Leordeanu"], "title": "Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones", "comment": "Accepted at European Robotics Forum (ERF) 2026", "summary": "Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\\%$ with $5\\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5ba4\u5185\u65e0\u4eba\u673a\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u57fa\u4e8e\u56de\u653e\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d1\u73b0Forgetting-Aware Replay\u5728\u6709\u9650\u5185\u5b58\u9884\u7b97\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5ba4\u5185\u65e0\u4eba\u673a\u7b49\u81ea\u4e3b\u667a\u80fd\u4f53\u9700\u8981\u5728\u5b9e\u65f6\u5b66\u4e60\u65b0\u7269\u4f53\u7c7b\u522b\u7684\u540c\u65f6\u9650\u5236\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f46\u73b0\u6709\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u5ba4\u5916\u573a\u666f\u4e14\u7f3a\u4e4f\u65f6\u95f4\u8fde\u8d2f\u7684\u5ba4\u5185\u89c6\u9891\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u5ba4\u5185\u65e0\u4eba\u673a\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "1) \u521b\u5efa\u5305\u542b14,400\u5e27\u7684\u5ba4\u5185\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff1b2) \u4f7f\u7528YOLOv11-nano\u4f5c\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u68c0\u6d4b\u5668\uff1b3) \u5728\u6709\u9650\u5185\u5b58\u9884\u7b97(5-10%\u56de\u653e)\u4e0b\u8bc4\u4f30\u4e09\u79cd\u57fa\u4e8e\u56de\u653e\u7684CIL\u7b56\u7565\uff1a\u7ecf\u9a8c\u56de\u653e(ER)\u3001\u6700\u5927\u5e72\u6270\u68c0\u7d22(MIR)\u548c\u9057\u5fd8\u611f\u77e5\u56de\u653e(FAR)\uff1b4) \u4f7f\u7528Grad-CAM\u5206\u6790\u6ce8\u610f\u529b\u8f6c\u79fb\u3002", "result": "\u57285%\u56de\u653e\u9884\u7b97\u4e0b\uff0cFAR\u83b7\u5f9782.96%\u7684\u5e73\u5747\u51c6\u786e\u7387(mAP50-95)\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002Grad-CAM\u5206\u6790\u663e\u793a\u5728\u6df7\u5408\u573a\u666f\u4e2d\u6ce8\u610f\u529b\u5728\u4e0d\u540c\u7c7b\u522b\u95f4\u8f6c\u79fb\uff0c\u8fd9\u4e0e\u65e0\u4eba\u673a\u5b9a\u4f4d\u8d28\u91cf\u4e0b\u964d\u76f8\u5173\u3002\u5b9e\u9a8c\u8bc1\u660e\u57fa\u4e8e\u56de\u653e\u7684\u6301\u7eed\u5b66\u4e60\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u8fb9\u7f18\u7a7a\u4e2d\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8d21\u732e\u4e86\u4e00\u4e2a\u5177\u6709\u65f6\u95f4\u8fde\u8d2f\u6027\u7684\u5ba4\u5185\u65e0\u4eba\u673a\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6709\u9650\u56de\u653e\u9884\u7b97\u4e0b\u8bc4\u4f30\u4e86\u57fa\u4e8e\u56de\u653e\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14267", "abs": "https://arxiv.org/abs/2602.14267", "authors": ["Manal Rahal", "Bestoun S. Ahmed", "Roger Renstr\u00f6m", "Robert Stener"], "title": "Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting", "comment": "8 pages", "summary": "With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.\n  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.", "AI": {"tldr": "DELTAiF\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5bb6\u5ead\u70ed\u6c34\u6d88\u8017\uff0c\u901a\u8fc7\u4ece\u4ee3\u8868\u6027\u5bb6\u5ead\u5b66\u4e60\u77e5\u8bc6\u5e76\u5fae\u8c03\u5230\u5176\u4ed6\u5bb6\u5ead\uff0c\u51cf\u5c1167%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740\u4f4f\u5b85\u70ed\u6cf5\u5b89\u88c5\u7684\u5feb\u901f\u589e\u957f\uff0c\u4f18\u5316\u5bb6\u5ead\u70ed\u6c34\u751f\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u6280\u672f\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u5bb6\u5ead\u5355\u72ec\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u4e91\u8fde\u63a5\u70ed\u6cf5\u90e8\u7f72\u4e2d\u3002", "method": "\u63d0\u51faDELTAiF\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u4ee3\u8868\u6027\u5bb6\u5ead\u5b66\u4e60\u77e5\u8bc6\uff0c\u7136\u540e\u5fae\u8c03\u5230\u5176\u4ed6\u5bb6\u5ead\uff0c\u9884\u6d4b\u5927\u578b\u70ed\u6c34\u4f7f\u7528\u4e8b\u4ef6\uff08\u5982\u6dcb\u6d74\uff09\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u4e14\u53ef\u6269\u5c55\u7684\u70ed\u6c34\u751f\u4ea7\u3002", "result": "\u51cf\u5c11\u6574\u4f53\u8bad\u7ec3\u65f6\u95f4\u7ea667%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff080.874-0.991\uff09\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u503c\u57280.001-0.017\u4e4b\u95f4\u3002\u5f53\u6e90\u5bb6\u5ead\u8868\u73b0\u51fa\u89c4\u5f8b\u6d88\u8d39\u6a21\u5f0f\u65f6\uff0c\u8fc1\u79fb\u5b66\u4e60\u7279\u522b\u6709\u6548\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u70ed\u6c34\u9700\u6c42\u9884\u6d4b\uff0cDELTAiF\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2602.13806", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13806", "abs": "https://arxiv.org/abs/2602.13806", "authors": ["Can Li", "Jie Gu", "Jingmin Chen", "Fangzhou Qiu", "Lei Sun"], "title": "Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos", "comment": null, "summary": "Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u5c3a\u5ea6\u52a8\u6001\u673a\u5236\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u8fd0\u52a8\u573a\u6765\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u4e2d4D\u91cd\u5efa\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u5c3a\u5ea6\u52a8\u6001\u7684\u9ad8\u65af\u5e8f\u5217\u8868\u793a\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u5148\u9a8c\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u5168\u5c40\u4e00\u81f4\u76844D\u91cd\u5efa\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u7406\u89e3\u52a8\u6001\u573a\u666f\u5bf9\u4e8e\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u4e25\u683c\u5355\u76ee\u8bbe\u7f6e\u4e0b\u76844D\u91cd\u5efa\u4ecd\u7136\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u3002\u771f\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u5728\u4ece\u7269\u4f53\u5230\u7c92\u5b50\u7ea7\u522b\u8868\u73b0\u51fa\u591a\u5c3a\u5ea6\u89c4\u5f8b\u6027\uff0c\u8fd9\u4e3a\u89e3\u51b3\u91cd\u5efa\u6a21\u7cca\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u601d\u8def\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6\u52a8\u6001\u673a\u5236\u6765\u5206\u89e3\u590d\u6742\u8fd0\u52a8\u573a\uff0c\u63d0\u51fa\u591a\u5c3a\u5ea6\u52a8\u6001\u7684\u9ad8\u65af\u5e8f\u5217\u8868\u793a\uff0c\u901a\u8fc7\u591a\u7ea7\u8fd0\u52a8\u7ec4\u5408\u6784\u5efa\u52a8\u60013D\u9ad8\u65af\u3002\u91c7\u7528\u5206\u5c42\u7ed3\u6784\u51cf\u8f7b\u91cd\u5efa\u6a21\u7cca\u6027\uff0c\u5e76\u878d\u5165\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u5148\u9a8c\u5efa\u7acb\u4e92\u8865\u76d1\u7763\uff0c\u7ea6\u675f\u89e3\u7a7a\u95f4\u3002", "result": "\u5728\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u6570\u636e\u96c6\u4e0a\u7684\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002\u80fd\u591f\u4ece\u5355\u76ee\u968f\u610f\u89c6\u9891\u5b9e\u73b0\u51c6\u786e\u4e14\u5168\u5c40\u4e00\u81f4\u76844D\u91cd\u5efa\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u7684\u591a\u5c3a\u5ea6\u89c4\u5f8b\u6027\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u52a8\u6001\u673a\u5236\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee\u89c6\u98914D\u91cd\u5efa\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u52a8\u6001\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2602.14272", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14272", "abs": "https://arxiv.org/abs/2602.14272", "authors": ["Yilun Kuang", "Yash Dagade", "Deep Chakraborty", "Erik Learned-Miller", "Randall Balestriero", "Tim G. J. Rudner", "Yann LeCun"], "title": "Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization", "comment": "Published in the Unifying Representations in Neural Models (UniReps) and Symmetry and Geometry in Neural Representations (NeurReps) Workshops at NeurIPS 2025", "summary": "Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.", "AI": {"tldr": "Radial-VCReg\u901a\u8fc7\u6dfb\u52a0\u5f84\u5411\u9ad8\u65af\u5316\u635f\u5931\u589e\u5f3aVCReg\uff0c\u5c06\u7279\u5f81\u8303\u6570\u4e0e\u5361\u65b9\u5206\u5e03\u5bf9\u9f50\uff0c\u51cf\u5c11\u9ad8\u9636\u4f9d\u8d56\u5e76\u63d0\u5347\u8868\u793a\u591a\u6837\u6027", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u65e8\u5728\u5b66\u4e60\u6700\u5927\u5316\u4fe1\u606f\u8868\u793a\uff0c\u4f46\u663e\u5f0f\u4fe1\u606f\u6700\u5927\u5316\u53d7\u5230\u7ef4\u5ea6\u707e\u96be\u7684\u963b\u788d\u3002\u73b0\u6709\u65b9\u6cd5\u5982VCReg\u901a\u8fc7\u6b63\u5219\u5316\u4e00\u9636\u548c\u4e8c\u9636\u7279\u5f81\u7edf\u8ba1\u91cf\u65e0\u6cd5\u5b8c\u5168\u5b9e\u73b0\u6700\u5927\u71b5", "method": "\u63d0\u51faRadial-VCReg\uff0c\u5728VCReg\u57fa\u7840\u4e0a\u6dfb\u52a0\u5f84\u5411\u9ad8\u65af\u5316\u635f\u5931\uff0c\u5c06\u7279\u5f81\u8303\u6570\u4e0e\u5361\u65b9\u5206\u5e03\u5bf9\u9f50\u2014\u2014\u8fd9\u662f\u9ad8\u7ef4\u9ad8\u65af\u5206\u5e03\u7684\u4e00\u4e2a\u5b9a\u4e49\u6027\u7279\u5f81", "result": "\u8bc1\u660eRadial-VCReg\u76f8\u6bd4VCReg\u80fd\u5c06\u66f4\u5e7f\u6cdb\u7684\u5206\u5e03\u7c7b\u522b\u8f6c\u5316\u4e3a\u6b63\u6001\u5206\u5e03\uff1b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u793a\u5b83\u901a\u8fc7\u51cf\u5c11\u9ad8\u9636\u4f9d\u8d56\u548c\u4fc3\u8fdb\u66f4\u591a\u6837\u5316\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u6765\u6301\u7eed\u63d0\u5347\u6027\u80fd", "conclusion": "Radial-VCReg\u901a\u8fc7\u5f84\u5411\u9ad8\u65af\u5316\u589e\u5f3aVCReg\uff0c\u6709\u6548\u89e3\u51b3\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u4ea7\u751f\u66f4\u63a5\u8fd1\u6700\u5927\u71b5\u7684\u8868\u793a\uff0c\u63d0\u5347\u81ea\u76d1\u7763\u5b66\u4e60\u6027\u80fd"}}
{"id": "2602.13865", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13865", "abs": "https://arxiv.org/abs/2602.13865", "authors": ["Gabriel Romio", "Mateus Begnini Melchiades", "Bruno Castro da Silva", "Gabriel de Oliveira Ramos"], "title": "Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay", "comment": null, "summary": "Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.", "AI": {"tldr": "MOC-2HER\uff1a\u4e00\u79cd\u7ed3\u5408\u53cc\u91cd\u76ee\u6807\u7ecf\u9a8c\u56de\u653e\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u95e8\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u591a\u76ee\u6807\u73af\u5883\u4e2d\u7684\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u73b0\u6709\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982OC\u548cMOC\uff09\u5728\u7a00\u758f\u5956\u52b1\u7684\u591a\u76ee\u6807\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u5956\u52b1\u53d6\u51b3\u4e8e\u7269\u4f53\u662f\u5426\u5230\u8fbe\u76ee\u6807\u800c\u975e\u667a\u80fd\u4f53\u76f4\u63a5\u4ea4\u4e92\uff0c\u8fd9\u4f7f\u5f97\u667a\u80fd\u4f53\u96be\u4ee5\u5b66\u4e60\u5982\u4f55\u4e0e\u7269\u4f53\u4e92\u52a8", "method": "\u9996\u5148\u63d0\u51faMOC-HER\uff0c\u5c06Hindsight Experience Replay\u96c6\u6210\u5230MOC\u6846\u67b6\u4e2d\uff1b\u7136\u540e\u5f15\u51652HER\uff0c\u521b\u5efa\u4e24\u5957\u865a\u62df\u76ee\u6807\uff1a\u4e00\u5957\u57fa\u4e8e\u7269\u4f53\u6700\u7ec8\u72b6\u6001\uff08\u6807\u51c6HER\uff09\uff0c\u53e6\u4e00\u5957\u57fa\u4e8e\u667a\u80fd\u4f53\u6548\u5e94\u5668\u4f4d\u7f6e\uff0c\u5956\u52b1\u667a\u80fd\u4f53\u65e2\u4e0e\u7269\u4f53\u4ea4\u4e92\u53c8\u5b8c\u6210\u4efb\u52a1", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u73af\u5883\u4e2d\uff0cMOC-2HER\u6210\u529f\u7387\u9ad8\u8fbe90%\uff0c\u800cMOC\u548cMOC-HER\u7684\u6210\u529f\u7387\u5747\u4f4e\u4e8e11%\uff0c\u53cc\u91cd\u76ee\u6807\u91cd\u6807\u8bb0\u7b56\u7565\u5728\u7a00\u758f\u5956\u52b1\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457", "conclusion": "\u53cc\u91cd\u76ee\u6807\u7ecf\u9a8c\u56de\u653e\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u591a\u76ee\u6807\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5b66\u4e60\u4e0e\u7269\u4f53\u4ea4\u4e92\u5e76\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b"}}
{"id": "2602.14274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14274", "abs": "https://arxiv.org/abs/2602.14274", "authors": ["Boning Zhou", "Ziyu Wang", "Han Hong", "Haoqi Hu"], "title": "Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data", "comment": "10 pages, 5 figures", "summary": "Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u57fa\u4e8etransformer\u7684\u8bed\u8a00\u6a21\u578b\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u7684\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u6587\u672c\u6570\u636e\u5728\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u63a8\u65ad\u4e25\u91cd\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4f46\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u7ed3\u6784\u5316\u6570\u636e\u53ef\u80fd\u4e0d\u5b8c\u6574\u6216\u4e0d\u53ef\u7528\uff0c\u9650\u5236\u4e86\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8etransformer\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u4fe1\u606f\u5e76\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\uff0c\u901a\u8fc7\u5bf9\u6bd4\u7ed3\u6784\u5316\u6570\u636e\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684\u56e0\u679c\u4f30\u8ba1\u7ed3\u679c\u6765\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u5728\u603b\u4f53\u3001\u7fa4\u4f53\u548c\u4e2a\u4f53\u4e09\u4e2a\u5c42\u9762\u4e0a\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u5f97\u5230\u7684\u56e0\u679c\u4f30\u8ba1\u4e0e\u4ece\u7ed3\u6784\u5316\u6570\u636e\u5f97\u5230\u7684\u7ed3\u679c\u5177\u6709\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u6587\u672c\u6570\u636e\u5728\u56e0\u679c\u63a8\u65ad\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u5728\u53ea\u6709\u6587\u672c\u6570\u636e\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u5546\u4e1a\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002"}}
{"id": "2602.14275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14275", "abs": "https://arxiv.org/abs/2602.14275", "authors": ["Lamine Rihani"], "title": "Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems", "comment": null, "summary": "Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53cd\u5411n-wise\u8f93\u51fa\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u8f93\u51fa\u7a7a\u95f4\u6784\u5efa\u8986\u76d6\u6570\u7ec4\uff0c\u89e3\u51b3AI/ML\u548c\u91cf\u5b50\u8ba1\u7b97\u4e2d\u9ad8\u7ef4\u8fde\u7eed\u8f93\u5165\u3001\u6982\u7387\u6027\u8f93\u51fa\u7b49\u6d4b\u8bd5\u6311\u6218\u3002", "motivation": "AI/ML\u7cfb\u7edf\u548c\u91cf\u5b50\u8ba1\u7b97\u8f6f\u4ef6\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u6d4b\u8bd5\u6311\u6218\uff1a\u9ad8\u7ef4\u8fde\u7eed\u8f93\u5165\u7a7a\u95f4\u3001\u6982\u7387\u6027/\u975e\u786e\u5b9a\u6027\u8f93\u51fa\u5206\u5e03\u3001\u4ec5\u901a\u8fc7\u53ef\u89c2\u6d4b\u9884\u6d4b\u884c\u4e3a\u5b9a\u4e49\u6b63\u786e\u6027\uff0c\u4ee5\u53ca\u516c\u5e73\u6027\u3001\u9c81\u68d2\u6027\u3001\u9519\u8bef\u6a21\u5f0f\u7b49\u5173\u952e\u8d28\u91cf\u7ef4\u5ea6\u9700\u8981\u901a\u8fc7\u590d\u6742\u7684\u591a\u5411\u4ea4\u4e92\u6765\u4f53\u73b0\u3002", "method": "\u63d0\u51fa\u53cd\u5411n-wise\u8f93\u51fa\u6d4b\u8bd5\u8303\u5f0f\uff0c\u76f4\u63a5\u5728\u9886\u57df\u7279\u5b9a\u7684\u8f93\u51fa\u7b49\u4ef7\u7c7b\u4e0a\u6784\u5efa\u8986\u76d6\u6570\u7ec4\uff0c\u5305\u62ecML\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6876\u3001\u51b3\u7b56\u8fb9\u754c\u533a\u57df\u3001\u516c\u5e73\u6027\u5206\u533a\u3001\u5d4c\u5165\u805a\u7c7b\u3001\u91cf\u5b50\u6d4b\u91cf\u7ed3\u679c\u5206\u5e03\u7b49\uff0c\u7136\u540e\u901a\u8fc7\u65e0\u68af\u5ea6\u5143\u542f\u53d1\u5f0f\u4f18\u5316\u89e3\u51b3\u9ed1\u76d2\u9006\u6620\u5c04\u95ee\u9898\uff0c\u5408\u6210\u80fd\u591f\u4ece\u9ed1\u76d2\u6a21\u578b\u4e2d\u5f15\u51fa\u76ee\u6807\u884c\u4e3a\u7279\u5f81\u7684\u8f93\u5165\u914d\u7f6e\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u4e24\u4e2a\u9886\u57df\u5e26\u6765\u534f\u540c\u6548\u76ca\uff1a\u660e\u786e\u7684\u5ba2\u6237\u4e2d\u5fc3\u9884\u6d4b/\u6d4b\u91cf\u8986\u76d6\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8ML\u6821\u51c6/\u8fb9\u754c\u6545\u969c\u548c\u91cf\u5b50\u9519\u8bef\u6a21\u5f0f\u7684\u6545\u969c\u68c0\u6d4b\u7387\uff0c\u589e\u5f3a\u6d4b\u8bd5\u5957\u4ef6\u6548\u7387\uff0c\u4ee5\u53ca\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u548c\u8986\u76d6\u6f02\u79fb\u76d1\u6d4b\u5b9e\u73b0\u7ed3\u6784\u5316MLOps/\u91cf\u5b50\u9a8c\u8bc1\u6d41\u7a0b\u3002", "conclusion": "\u53cd\u5411n-wise\u8f93\u51fa\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u5b66\u539f\u7406\u4e0a\u7684\u8303\u5f0f\u8f6c\u6362\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u8f93\u51fa\u7a7a\u95f4\u6784\u5efa\u8986\u76d6\u6765\u5e94\u5bf9AI/ML\u548c\u91cf\u5b50\u8ba1\u7b97\u7684\u72ec\u7279\u6d4b\u8bd5\u6311\u6218\uff0c\u4e3a\u8fd9\u4e9b\u590d\u6742\u7cfb\u7edf\u7684\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2602.14252", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14252", "abs": "https://arxiv.org/abs/2602.14252", "authors": ["Osher Elhadad", "Felipe Meneguzzi", "Reuth Mirsky"], "title": "GRAIL: Goal Recognition Alignment through Imitation Learning", "comment": "Accepted for publication at AAMAS 2026", "summary": "Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.", "AI": {"tldr": "GRAIL\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u548c\u9006\u5f3a\u5316\u5b66\u4e60\u4ece\uff08\u53ef\u80fd\u6b21\u4f18\u7684\uff09\u6f14\u793a\u8f68\u8ff9\u4e2d\u5b66\u4e60\u6bcf\u4e2a\u5019\u9009\u76ee\u6807\u7684\u76ee\u6807\u5bfc\u5411\u7b56\u7565\uff0c\u5b9e\u73b0\u4e00\u6b21\u6027\u63a8\u7406\u7684\u76ee\u6807\u8bc6\u522b\uff0c\u5728\u6b21\u4f18\u3001\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u566a\u58f0\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6700\u4f18\u76ee\u6807\u5bfc\u5411\u7b56\u7565\u8868\u793a\uff0c\u4f46\u8fd9\u53ef\u80fd\u4e0e\u884c\u4e3a\u8005\u7684\u771f\u5b9e\u884c\u4e3a\u4e0d\u540c\uff0c\u963b\u788d\u51c6\u786e\u8bc6\u522b\u5176\u76ee\u6807\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u4ee5\u66f4\u597d\u5730\u5bf9\u9f50AI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u610f\u56fe\u3002", "method": "GRAIL\uff08Goal Recognition Alignment through Imitation Learning\uff09\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u9006\u5f3a\u5316\u5b66\u4e60\uff0c\u76f4\u63a5\u4ece\uff08\u53ef\u80fd\u6b21\u4f18\u7684\uff09\u6f14\u793a\u8f68\u8ff9\u4e2d\u4e3a\u6bcf\u4e2a\u5019\u9009\u76ee\u6807\u5b66\u4e60\u4e00\u4e2a\u76ee\u6807\u5bfc\u5411\u7b56\u7565\u3002\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u9012\u7528\u6bcf\u4e2a\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5bf9\u89c2\u5bdf\u5230\u7684\u90e8\u5206\u8f68\u8ff9\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u6700\u4f18\u884c\u4e3a\u4e0bF1\u5206\u6570\u63d0\u5347\u8d85\u8fc70.5\uff1b\u5728\u6b21\u4f18\u884c\u4e3a\u4e0b\u63d0\u5347\u7ea60.1-0.3\uff1b\u5728\u566a\u58f0\u6700\u4f18\u8f68\u8ff9\u4e0b\u63d0\u5347\u9ad8\u8fbe0.4\uff1b\u5728\u5b8c\u5168\u6700\u4f18\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "GRAIL\u5728\u4fdd\u6301\u7ecf\u5178\u76ee\u6807\u8bc6\u522b\u4e00\u6b21\u6027\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u7b56\u7565\u6355\u6349\u6b21\u4f18\u548c\u7cfb\u7edf\u6027\u504f\u5dee\u884c\u4e3a\uff0c\u4e3a\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u89e3\u91ca\u667a\u80fd\u4f53\u76ee\u6807\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
{"id": "2602.14413", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14413", "abs": "https://arxiv.org/abs/2602.14413", "authors": ["Sourya Saha", "Md. Nurul Absur"], "title": "Understanding Sensor Vulnerabilities in Industrial XR Tracking", "comment": "IEEE VR XRIOS 2026 Workshop", "summary": "Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6545\u969c\u6ce8\u5165\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u5728\u5de5\u4e1aXR\u7cfb\u7edf\u4e2d\u9762\u5bf9\u4f20\u611f\u5668\u9000\u5316\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u60ef\u6027\u4f20\u611f\u5668\u9000\u5316\u6bd4\u89c6\u89c9\u4f20\u611f\u5668\u9000\u5316\u5bf9\u8f68\u8ff9\u7cbe\u5ea6\u7684\u5f71\u54cd\u66f4\u4e25\u91cd\u3002", "motivation": "\u5de5\u4e1aXR\u7cfb\u7edf\u4f9d\u8d56VIO\u8fdb\u884c\u59ff\u6001\u8ddf\u8e2a\uff0c\u4f46\u5b9e\u9645\u73af\u5883\u5e38\u504f\u79bb\u7406\u60f3\u5047\u8bbe\uff0c\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u6b63\u5e38\u4f20\u611f\u5668\u884c\u4e3a\uff0c\u5bf9\u6301\u7eed\u4f20\u611f\u5668\u9000\u5316\u5728\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u5f71\u54cd\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u53d7\u63a7\u5b9e\u8bc1\u7814\u7a76\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6545\u969c\u6ce8\u5165\uff0c\u8003\u5bdf\u89c6\u89c9\u548c\u60ef\u6027\u6a21\u6001\u5728\u4e0d\u540c\u64cd\u4f5c\u72b6\u6001\u4e0b\u7684\u9000\u5316\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "\u89c2\u5bdf\u5230\u6545\u969c\u5f71\u54cd\u7684\u663e\u8457\u4e0d\u5bf9\u79f0\u6027\uff1a\u89c6\u89c9\u4f20\u611f\u9000\u5316\u901a\u5e38\u5bfc\u81f4\u5398\u7c73\u7ea7\u7684\u6709\u9650\u59ff\u6001\u8bef\u5dee\uff0c\u800c\u60ef\u6027\u4f20\u611f\u9000\u5316\u53ef\u80fd\u5f15\u8d77\u6570\u767e\u81f3\u6570\u5343\u7c73\u7684\u8f68\u8ff9\u504f\u5dee\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5de5\u4e1aXR\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u8bbe\u8ba1\u4e2d\u9700\u8981\u66f4\u52a0\u91cd\u89c6\u60ef\u6027\u4f20\u611f\u5668\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2602.14295", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14295", "abs": "https://arxiv.org/abs/2602.14295", "authors": ["Edwin Chen", "Zulekha Bibi"], "title": "Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows", "comment": "Submitted to the Google Gemini 3 Hackathon", "summary": "We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.\n  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.\n  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.", "AI": {"tldr": "MLAT\u662f\u4e00\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\u96c6\u6210\u5230LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u6839\u636e\u9700\u8981\u8c03\u7528\u5b9a\u91cf\u9884\u6d4b\u5e76\u5728\u4e0a\u4e0b\u6587\u4e2d\u63a8\u7406\u8f93\u51fa\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u6d41\u6c34\u7ebf\u5c06ML\u63a8\u7406\u4f5c\u4e3a\u9759\u6001\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u800cMLAT\u65e8\u5728\u5c06\u6a21\u578b\u5b9a\u4f4d\u4e3a\u4e00\u7b49\u5de5\u5177\uff0c\u4f7fLLM\u80fd\u591f\u6839\u636e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u51b3\u5b9a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u5b83\uff0c\u5b9e\u73b0\u5b9a\u91cf\u4f30\u8ba1\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u7ed3\u5408\u3002", "method": "\u63d0\u51faMLAT\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u901a\u8fc7PitchCraft\u7cfb\u7edf\u9a8c\u8bc1\uff1a\u4f7f\u7528\u4e24\u4e2a\u667a\u80fd\u4f53\uff08\u7814\u7a76\u667a\u80fd\u4f53\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u6536\u96c6\u60c5\u62a5\uff0c\u8349\u7a3f\u667a\u80fd\u4f53\u5c06XGBoost\u5b9a\u4ef7\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\u8c03\u7528\uff09\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u67b6\u6784\uff0c\u5728\u6781\u7aef\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8bad\u7ec3\u5b9a\u4ef7\u6a21\u578b\u3002", "result": "\u5b9a\u4ef7\u6a21\u578b\u572870\u4e2a\u771f\u5b9e\u548c\u4eba\u5de5\u9a8c\u8bc1\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fbe\u5230R^2=0.807\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee3688\u7f8e\u5143\u3002\u7cfb\u7edf\u5c06\u63d0\u6848\u751f\u6210\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u51cf\u5c11\u523010\u5206\u949f\u4ee5\u5185\u3002", "conclusion": "MLAT\u6846\u67b6\u9002\u7528\u4e8e\u9700\u8981\u5b9a\u91cf\u4f30\u8ba1\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u7ed3\u5408\u7684\u9886\u57df\uff0c\u5c06ML\u6a21\u578b\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\u96c6\u6210\u5230LLM\u5de5\u4f5c\u6d41\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2602.14662", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14662", "abs": "https://arxiv.org/abs/2602.14662", "authors": ["Zhenjun Zhao", "Heng Yang", "Bangyan Liao", "Yingping Zeng", "Shaocheng Yan", "Yingdong Gu", "Peidong Liu", "Yi Zhou", "Haoang Li", "Javier Civera"], "title": "Advances in Global Solvers for 3D Vision", "comment": "Comprehensive survey; 37 pages, 7 figures, 3 tables. Project page with literature tracking and code tutorials: https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision", "summary": "Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u9996\u6b21\u7cfb\u7edf\u56de\u987e\u4e863D\u89c6\u89c9\u4e2d\u7684\u5168\u5c40\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5206\u652f\u5b9a\u754c\u3001\u51f8\u677e\u5f1b\u548c\u6e10\u8fdb\u975e\u51f8\u6027\u4e09\u79cd\u6838\u5fc3\u8303\u5f0f\u6784\u5efa\u4e86\u7edf\u4e00\u5206\u7c7b\u4f53\u7cfb\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u51e0\u4f55\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u7406\u8bba\u57fa\u7840\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u6700\u4f18\u6027-\u9c81\u68d2\u6027-\u53ef\u6269\u5c55\u6027\u6743\u8861\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5168\u5c40\u6c42\u89e3\u5668\u4f5c\u4e3a3D\u89c6\u89c9\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u80fd\u591f\u4e3a\u4f20\u7edf\u4e0a\u7531\u5c40\u90e8\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\u5904\u7406\u7684\u975e\u51f8\u51e0\u4f55\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u9700\u8981\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u6307\u5bfc\u7b97\u6cd5\u9009\u62e9\u548c\u672a\u6765\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e09\u79cd\u6838\u5fc3\u8303\u5f0f\u7684\u7efc\u5408\u5206\u7c7b\u4f53\u7cfb\uff1a\u5206\u652f\u5b9a\u754c\uff08BnB\uff09\u3001\u51f8\u677e\u5f1b\uff08CR\uff09\u548c\u6e10\u8fdb\u975e\u51f8\u6027\uff08GNC\uff09\u3002\u5206\u6790\u4e86\u6bcf\u79cd\u8303\u5f0f\u7684\u7406\u8bba\u57fa\u7840\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5b9e\u9645\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u8003\u5bdf\u4e86\u5b83\u4eec\u5728\u5341\u4e2a\u6838\u5fc3\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63ed\u793a\u4e86\u5168\u5c40\u6c42\u89e3\u5668\u5728\u6700\u4f18\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6c42\u89e3\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u672a\u6765\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u4fdd\u6301\u4fdd\u8bc1\u524d\u63d0\u4e0b\u7684\u7b97\u6cd5\u6269\u5c55\u3001\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u4e0e\u53ef\u8bc1\u660e\u4f18\u5316\u7684\u7ed3\u5408\u3001\u6807\u51c6\u5316\u57fa\u51c6\u7684\u5efa\u7acb\u4ee5\u53ca\u5b89\u5168\u5173\u952e\u90e8\u7f72\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u6574\u5408\u7406\u8bba\u57fa\u7840\u3001\u5b9e\u8df5\u8fdb\u5c55\u548c\u66f4\u5e7f\u6cdb\u5f71\u54cd\uff0c\u4e3a3D\u89c6\u89c9\u4e2d\u7684\u5168\u5c40\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\u548c\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u63a8\u52a8\u53ef\u8bc1\u660e\u3001\u53ef\u4fe1\u8d56\u7684\u611f\u77e5\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002\u63d0\u4f9b\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u6587\u732e\u603b\u7ed3\u548c\u914d\u5957\u4ee3\u7801\u6559\u7a0b\u3002"}}
{"id": "2602.14318", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14318", "abs": "https://arxiv.org/abs/2602.14318", "authors": ["Trishit Mondal", "Ameya D. Jagtap"], "title": "In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes", "comment": "46 pages, 34 Figures", "summary": "Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \\textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.", "AI": {"tldr": "\u672c\u6587\u5bf9Transformer\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u6db5\u76d6\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u3001\u9690\u79c1\u7b49\u65b9\u9762\uff0c\u8bc6\u522b\u4e86\u5176\u7ed3\u6784\u8106\u5f31\u6027\u548c\u9886\u57df\u7279\u5b9a\u98ce\u9669\u3002", "motivation": "\u968f\u7740Transformer\u67b6\u6784\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u533b\u7597\u5065\u5eb7\u3001\u81ea\u4e3b\u7cfb\u7edf\u4ee5\u53ca\u6c14\u5019\u5efa\u6a21\u3001\u6750\u6599\u53d1\u73b0\u3001\u836f\u7269\u7814\u53d1\u3001\u6838\u79d1\u5b66\u7b49\u5173\u952e\u79d1\u5b66\u8ba1\u7b97\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u5bf9\u5176\u53ef\u4fe1\u5ea6\u8fdb\u884c\u66f4\u6df1\u5165\u3001\u66f4\u4e25\u8c28\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u7cfb\u7edf\u8bc4\u4f30Transformer\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\uff0c\u5305\u62ec\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5bf9\u6297\u653b\u51fb\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u65b9\u9762\u3002\u540c\u65f6\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u79d1\u5b66\u5de5\u7a0b\u9886\u57df\uff08\u5305\u62ec\u673a\u5668\u4eba\u3001\u533b\u5b66\u3001\u5730\u7403\u79d1\u5b66\u3001\u6750\u6599\u79d1\u5b66\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u6838\u79d1\u5b66\u3001\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff09\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7cfb\u7edf\u68c0\u9a8cTransformer\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u8bc6\u522b\u4e86Transformer\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u7684\u91cd\u590d\u6027\u7ed3\u6784\u8106\u5f31\u6027\u548c\u9886\u57df\u7279\u5b9a\u98ce\u9669\uff0c\u5206\u6790\u4e86\u8fd9\u4e9b\u67b6\u6784\u90e8\u7f72\u65f6\u53ef\u80fd\u9762\u4e34\u7684\u98ce\u9669\uff0c\u5e76\u6307\u51fa\u4e86\u9650\u5236Transformer\u53ef\u9760\u90e8\u7f72\u7684\u5f00\u653e\u7814\u7a76\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u4e0d\u540c\u9886\u57df\u7684\u7efc\u5408\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86Transformer\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5728\u90e8\u7f72\u8fd9\u4e9b\u5f3a\u5927\u67b6\u6784\u65f6\u9700\u8981\u89e3\u51b3\u7684\u53ef\u4fe1\u5ea6\u6311\u6218\u3002"}}
{"id": "2602.14965", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14965", "abs": "https://arxiv.org/abs/2602.14965", "authors": ["Qingming Liu", "Xinyue Yao", "Shuyuan Zhang", "Yueci Deng", "Guiliang Liu", "Zhen Liu", "Kui Jia"], "title": "PAct: Part-Decomposed Single-View Articulated Object Generation", "comment": "Technical Report(11 figures, 14 pages), Project Page: https://PAct-project.github.io", "summary": "Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u4ef6\u4e2d\u5fc3\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u5feb\u901f\u751f\u6210\u5e26\u5173\u8282\u76843D\u8d44\u4ea7\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u8017\u65f6\u7684\u7f3a\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u90e8\u4ef6\u7ed3\u6784\u548c\u8fd0\u52a8\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u57fa\u4e8e\u4f18\u5316\u7684\u91cd\u5efa\u65b9\u6cd5\u867d\u7136\u51c6\u786e\u4f46\u8017\u65f6\uff08\u6570\u5341\u5206\u949f\u5230\u6570\u5c0f\u65f6\uff09\uff0c\u800c\u57fa\u4e8e\u6a21\u677f\u68c0\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u867d\u7136\u5feb\u901f\u4f46\u53ef\u80fd\u65e0\u6cd5\u5339\u914d\u8f93\u5165\u89c2\u6d4b\u7684\u5177\u4f53\u7ed3\u6784\u548c\u5916\u89c2\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5feb\u901f\u751f\u6210\u53c8\u80fd\u4fdd\u6301\u8f93\u5165\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u90e8\u4ef6\u4e2d\u5fc3\u7684\u751f\u6210\u6846\u67b6\uff0c\u5c06\u7269\u4f53\u5efa\u6a21\u4e3a\u4e00\u7ec4\u53ef\u79fb\u52a8\u90e8\u4ef6\uff0c\u6bcf\u4e2a\u90e8\u4ef6\u7528\u5e26\u6709\u90e8\u4ef6\u8eab\u4efd\u548c\u5173\u8282\u63d0\u793a\u7684\u6f5c\u5728\u4ee4\u724c\u7f16\u7801\u3002\u5728\u5355\u5f20\u56fe\u50cf\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u751f\u6210\u4fdd\u6301\u5b9e\u4f8b\u7ea7\u5bf9\u5e94\u5173\u7cfb\u7684\u5e26\u5173\u82823D\u8d44\u4ea7\uff0c\u540c\u65f6\u786e\u4fdd\u6709\u6548\u7684\u90e8\u4ef6\u7ed3\u6784\u548c\u8fd0\u52a8\u3002", "result": "\u5728\u5e38\u89c1\u5173\u8282\u7c7b\u522b\uff08\u5982\u62bd\u5c49\u548c\u95e8\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u4e8e\u4f18\u5316\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8f93\u5165\u4e00\u81f4\u6027\u3001\u90e8\u4ef6\u51c6\u786e\u6027\u548c\u5173\u8282\u5408\u7406\u6027\u65b9\u9762\u90fd\u6709\u6539\u8fdb\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u90e8\u4ef6\u4e2d\u5fc3\u751f\u6210\u6846\u67b6\u907f\u514d\u4e86\u9010\u5b9e\u4f8b\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u524d\u5411\u63a8\u7406\uff0c\u5e76\u652f\u6301\u53ef\u63a7\u7684\u7ec4\u88c5\u548c\u5173\u8282\u8fde\u63a5\uff0c\u8fd9\u5bf9\u4e8e\u5177\u8eab\u4ea4\u4e92\u5e94\u7528\u975e\u5e38\u91cd\u8981\u3002"}}
{"id": "2602.14338", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14338", "abs": "https://arxiv.org/abs/2602.14338", "authors": ["Zhi Zhang", "Zhen Han", "Costas Mavromatis", "Qi Zhu", "Yunyi Zhang", "Sheng Guan", "Dingmin Wang", "Xiong Zhou", "Shuai Wang", "Soji Adeshina", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.", "AI": {"tldr": "AERO\u662f\u4e00\u79cd\u6539\u8fdbGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u3001\u9009\u62e9\u6027\u62d2\u7edd\u548c\u8d1d\u53f6\u65af\u540e\u9a8c\u6765\u907f\u514d\u96f6\u68af\u5ea6\u4fe1\u53f7\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "GRPO\u65b9\u6cd5\u5728LLM\u540e\u8bad\u7ec3\u4e2d\u5b58\u5728\u6548\u7387\u95ee\u9898\uff1a\u5f53\u4e00\u7ec4rollouts\u5168\u90e8\u6b63\u786e\u6216\u5168\u90e8\u9519\u8bef\u65f6\uff0c\u5f52\u4e00\u5316\u4f18\u52bf\u503c\u4e3a\u96f6\uff0c\u5bfc\u81f4\u65e0\u68af\u5ea6\u4fe1\u53f7\uff0c\u6d6a\u8d39\u4e86\u5fae\u8c03\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u63d0\u51faAERO\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u81ea\u9002\u5e94rollout\u7b56\u7565\uff1b2\uff09\u9009\u62e9\u6027\u62d2\u7edd\u7b56\u7565\u6765\u4fee\u526arollouts\uff1b3\uff09\u8d1d\u53f6\u65af\u540e\u9a8c\u6765\u9632\u6b62\u96f6\u4f18\u52bf\u6b7b\u533a\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\u914d\u7f6e\u4e0a\u6d4b\u8bd5\uff0c\u5728\u76f8\u540c\u603brollout\u9884\u7b97\u4e0b\uff0cAERO\u51cf\u5c11\u7ea648%\u7684\u603b\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff0c\u7f29\u77ed\u7ea645%\u7684\u6bcf\u6b65\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u8fdbPass@8\u548cAvg@8\u6027\u80fd\u3002", "conclusion": "AERO\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2602.14344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14344", "abs": "https://arxiv.org/abs/2602.14344", "authors": ["Mathias Jackermeier", "Mattia Giuri", "Jacques Cloete", "Alessandro Abate"], "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations", "comment": null, "summary": "We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTL)\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u795e\u7ecf\u67b6\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\uff0c\u4ee5\u63d0\u5347\u96f6\u6837\u672c\u6267\u884c\u672a\u89c1\u4efb\u52a1\u7684\u80fd\u529b\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u80fd\u8bad\u7ec3\u901a\u7528\u7b56\u7565\uff0c\u4f46\u5f80\u5f80\u96be\u4ee5\u6709\u6548\u6355\u6349\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTL)\u89c4\u8303\u4e2d\u4e30\u5bcc\u7684\u903b\u8f91\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u5728\u96f6\u6837\u672c\u6267\u884c\u672a\u89c1\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\uff1a1) \u57fa\u4e8e\u4efb\u52a1\u6709\u9650\u81ea\u52a8\u673a\u6784\u5efa\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\u4f5c\u4e3a\u7b56\u7565\u6761\u4ef6\uff1b2) \u91c7\u7528\u5c42\u6b21\u5316\u795e\u7ecf\u67b6\u6784\u7f16\u7801\u8fd9\u4e9b\u516c\u5f0f\u7684\u903b\u8f91\u7ed3\u6784\uff1b3) \u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u4f7f\u7b56\u7565\u80fd\u591f\u63a8\u7406\u672a\u6765\u5b50\u76ee\u6807\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\u548c\u5c42\u6b21\u5316\u67b6\u6784\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349LTL\u89c4\u8303\u7684\u903b\u8f91\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u96f6\u6837\u672c\u6267\u884c\u672a\u89c1\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2602.14351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14351", "abs": "https://arxiv.org/abs/2602.14351", "authors": ["Mehran Aghabozorgi", "Alireza Moazeni", "Yanshu Zhang", "Ke Li"], "title": "WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control", "comment": "Accepted at ICLR 2026. OpenReview: https://openreview.net/forum?id=mzLOnTb3WH", "summary": "Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.", "AI": {"tldr": "WIMLE\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u9690\u5f0f\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6765\u5b66\u4e60\u968f\u673a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u96c6\u6210\u548c\u6f5c\u5728\u91c7\u6837\u4f30\u8ba1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5408\u6210\u8f6c\u79fb\u6765\u7a33\u5b9a\u5b66\u4e60\u3002", "motivation": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5305\u62ec\uff1a\u6a21\u578b\u8bef\u5dee\u7d2f\u79ef\u3001\u5355\u6a21\u6001\u4e16\u754c\u6a21\u578b\u5e73\u5747\u591a\u6a21\u6001\u52a8\u6001\u3001\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u504f\u5dee\u5b66\u4e60\u3002", "method": "\u5c06\u9690\u5f0f\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6269\u5c55\u5230\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b66\u4e60\u968f\u673a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff08\u65e0\u9700\u8fed\u4ee3\u91c7\u6837\uff09\uff0c\u901a\u8fc7\u96c6\u6210\u548c\u6f5c\u5728\u91c7\u6837\u4f30\u8ba1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u8bad\u7ec3\u65f6\u6839\u636e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5408\u6210\u8f6c\u79fb\u3002", "result": "\u572840\u4e2a\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\uff08DeepMind Control\u3001MyoSuite\u3001HumanoidBench\uff09\u4e0a\uff0cWIMLE\u5b9e\u73b0\u4e86\u4f18\u4e8e\u6a21\u578b\u65e0\u5173\u548c\u57fa\u4e8e\u6a21\u578b\u57fa\u7ebf\u7684\u6837\u672c\u6548\u7387\u548c\u7ade\u4e89\u6027\u6216\u66f4\u597d\u7684\u6e10\u8fd1\u6027\u80fd\u3002\u5728Humanoid-run\u4efb\u52a1\u4e0a\u6837\u672c\u6548\u7387\u63d0\u9ad850%\u4ee5\u4e0a\uff0c\u5728HumanoidBench\u4e0a\u89e3\u51b3\u4e8614\u4e2a\u4efb\u52a1\u4e2d\u76848\u4e2a\u3002", "conclusion": "\u57fa\u4e8eIMLE\u7684\u591a\u6a21\u6001\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a0\u6743\u5bf9\u4e8e\u7a33\u5b9a\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.14375", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14375", "abs": "https://arxiv.org/abs/2602.14375", "authors": ["Kensuke Ajimoto", "Yuma Yamamoto", "Yoshifumi Kusunoki", "Tomoharu Nakashima"], "title": "A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments", "comment": null, "summary": "This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u73af\u5883\u7684\u591a\u7c7b\u5728\u7ebf\u6a21\u7cca\u5206\u7c7b\u5668\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u4ec5\u5904\u7406\u4e8c\u5206\u7c7b\u95ee\u9898\u7684\u5728\u7ebf\u6a21\u7cca\u5206\u7c7b\u5668", "motivation": "\u4f20\u7edf\u5728\u7ebf\u6a21\u7cca\u5206\u7c7b\u5668\u53ea\u80fd\u5904\u7406\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u4f46\u5728\u5b9e\u9645\u52a8\u6001\u73af\u5883\u4e2d\u9700\u8981\u5904\u7406\u591a\u7c7b\u5206\u7c7b\u95ee\u9898", "method": "\u4f7f\u7528\u6a21\u7ccaif-then\u89c4\u5219\u6784\u5efa\u5206\u7c7b\u5668\uff0c\u5176\u4e2d\u524d\u4ef6\u6a21\u7cca\u96c6\u7531\u7528\u6237\u9884\u5148\u786e\u5b9a\uff0c\u540e\u4ef6\u5b9e\u503c\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u5b66\u4e60\uff1b\u5728\u5728\u7ebf\u6846\u67b6\u4e2d\uff0c\u8bad\u7ec3\u6570\u636e\u6a21\u5f0f\u4e0d\u662f\u4e00\u6b21\u6027\u5168\u90e8\u53ef\u7528\uff0c\u800c\u662f\u968f\u65f6\u95f4\u9010\u6b65\u83b7\u5f97", "result": "\u901a\u8fc7\u5408\u6210\u52a8\u6001\u6570\u636e\u548c\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u591a\u7c7b\u5728\u7ebf\u6a21\u7cca\u5206\u7c7b\u5668\u7684\u6027\u80fd", "conclusion": "\u6210\u529f\u5c06\u5728\u7ebf\u6a21\u7cca\u5206\u7c7b\u5668\u6269\u5c55\u5230\u591a\u7c7b\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u591a\u7c7b\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.14423", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14423", "abs": "https://arxiv.org/abs/2602.14423", "authors": ["Abdelali Bouyahia", "Fr\u00e9d\u00e9ric LeBlanc", "Mario Marchand"], "title": "The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization", "comment": null, "summary": "Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.", "AI": {"tldr": "\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6\u5206\u6790\u6570\u636e\u589e\u5f3a\u5bf9\u6cdb\u5316\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u8fb9\u754c\u5c06\u6cdb\u5316\u5dee\u8ddd\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u89e3\u91ca\u9879\uff0c\u5f15\u5165\u7fa4\u76f4\u5f84\u6982\u5ff5\u63ed\u793a\u6570\u636e\u589e\u5f3a\u7684\u6743\u8861\u673a\u5236\u3002", "motivation": "\u6570\u636e\u589e\u5f3a\u662f\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u7684\u5e38\u7528\u6280\u672f\uff0c\u4f46\u5176\u7406\u8bba\u4f5c\u7528\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u7cfb\u7edf\u6846\u67b6\uff0c\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u5206\u6790\u6570\u636e\u589e\u5f3a\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u4e92\u4fe1\u606f\u8fb9\u754c\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5c06\u589e\u5f3a\u5206\u5e03\u5efa\u6a21\u4e3a\u539f\u59cb\u6570\u636e\u5206\u5e03\u4e0e\u53d8\u6362\u5206\u5e03\u7684\u590d\u5408\uff0c\u63a8\u5bfc\u8f68\u9053\u5e73\u5747\u635f\u5931\u51fd\u6570\u3002\u5728\u635f\u5931\u51fd\u6570\u548c\u589e\u5f3a\u8fc7\u7a0b\u7684\u6b21\u9ad8\u65af\u5047\u8bbe\u4e0b\uff0c\u5c06\u6cdb\u5316\u5dee\u8ddd\u5206\u89e3\u4e3a\u5206\u5e03\u5dee\u5f02\u3001\u7b97\u6cd5\u7a33\u5b9a\u6027\u548c\u589e\u5f3a\u654f\u611f\u6027\u4e09\u4e2a\u53ef\u89e3\u91ca\u9879\uff0c\u5e76\u5f15\u5165\u7fa4\u76f4\u5f84\u6982\u5ff5\u91cf\u5316\u589e\u5f3a\u6270\u52a8\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u6cdb\u5316\u5dee\u8ddd\u53ef\u5206\u89e3\u4e3a\u4e09\u4e2a\u53d7\u7fa4\u76f4\u5f84\u63a7\u5236\u7684\u9879\uff1a\u5206\u5e03\u5dee\u5f02\u3001\u7b97\u6cd5\u7a33\u5b9a\u6027\u548c\u589e\u5f3a\u654f\u611f\u6027\u3002\u7fa4\u76f4\u5f84\u63ed\u793a\u4e86\u6570\u636e\u589e\u5f3a\u7684\u5185\u5728\u6743\u8861\uff1a\u5c0f\u76f4\u5f84\u4fdd\u6301\u6570\u636e\u4fdd\u771f\u5ea6\u4f46\u6b63\u5219\u5316\u6709\u9650\uff0c\u5927\u76f4\u5f84\u589e\u5f3a\u7a33\u5b9a\u6027\u4f46\u589e\u52a0\u504f\u5dee\u548c\u654f\u611f\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u8fb9\u754c\u80fd\u53ef\u9760\u8ffd\u8e2a\u771f\u5b9e\u6cdb\u5316\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u606f\u8bba\u6846\u67b6\u4e3a\u7406\u89e3\u6570\u636e\u589e\u5f3a\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5206\u6790\u5de5\u5177\uff0c\u7fa4\u76f4\u5f84\u6982\u5ff5\u7edf\u4e00\u4e86\u589e\u5f3a\u6548\u679c\u7684\u91cf\u5316\u63a7\u5236\uff0c\u63ed\u793a\u4e86\u6570\u636e\u589e\u5f3a\u8bbe\u8ba1\u4e2d\u4fdd\u771f\u5ea6\u4e0e\u6b63\u5219\u5316\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2602.14430", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14430", "abs": "https://arxiv.org/abs/2602.14430", "authors": ["Prithwijit Chowdhury", "Ahmad Mustafa", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking", "comment": null, "summary": "In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of \"importance\" and \"relevance\" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u751f\u6210\u548c\u5fc5\u8981\u6027/\u5145\u5206\u6027\u91cf\u5316\u6765\u8bc4\u4f30LIME\u548cSHAP\u5728\u6cb9\u6c14\u52d8\u63a2\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u89e3\u91ca\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u6cb9\u6c14\u52d8\u63a2\u98ce\u9669\u8bc4\u4f30\u4e2d\uff0c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5206\u7c7b\u5668\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u800c\u73b0\u6709XAI\u65b9\u6cd5\uff08\u5982LIME\u548cSHAP\uff09\u5bf9\u540c\u4e00\u573a\u666f\u7684\u89e3\u91ca\u5b58\u5728\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6570\u636e\u4e0a\uff0c\u8fd9\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u751f\u6210\u53cd\u4e8b\u5b9e\u5e76\u91cf\u5316\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\uff0c\u7528\u4e8e\u8bc4\u4f30LIME\u548cSHAP\u5728\u9ad8\u7ef4\u7ed3\u6784\u5316\u52d8\u63a2\u98ce\u9669\u8bc4\u4f30\u6570\u636e\u4e0a\u7684\u89e3\u91ca\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u9c81\u68d2\u6027\u6d4b\u8bd5\uff0c\u6df1\u5165\u4e86\u89e3\u4e86\u6a21\u578b\u5904\u7406\u9519\u8bef\u6570\u636e\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u54ea\u79cdXAI\u6a21\u5757\u4e0e\u54ea\u79cd\u6a21\u578b\u5728\u6cb9\u6c14\u6307\u793a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u57fa\u4e8e\u56e0\u679c\u7406\u8bba\u4e2d\u7684\u5fc5\u8981\u6027\u548c\u5145\u5206\u6027\u6982\u5ff5\u6765\u9a8c\u8bc1\u7279\u5f81\u91cd\u8981\u6027\u6392\u540d\uff0c\u662f\u63d0\u9ad8XAI\u65b9\u6cd5\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u66f4\u53ef\u9760\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5728\u6cb9\u6c14\u52d8\u63a2\u4e2d\u505a\u51fa\u66f4\u53ef\u4fe1\u7684\u51b3\u7b56\u3002"}}
{"id": "2602.14432", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14432", "abs": "https://arxiv.org/abs/2602.14432", "authors": ["Arnav Chavan", "Nahush Lele", "Udbhav Bamba", "Sankalp Dayal", "Aditi Raghunathan", "Deepak Gupta"], "title": "S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations", "comment": null, "summary": "Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9009\u62e9\u6027\u8c31\u8870\u51cf\uff08S\u00b2D\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u6b63\u5219\u5316\u6743\u91cd\u77e9\u9635\u7684\u6700\u5927\u5947\u5f02\u503c\u5206\u91cf\uff0c\u6709\u6548\u51cf\u5c11\u5927\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21Transformer\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u662f\u6a21\u578b\u91cf\u5316\u7684\u6839\u672c\u6311\u6218\uff0c\u8fd9\u4e9b\u5f02\u5e38\u503c\u4f1a\u4ea7\u751f\u8fc7\u5927\u7684\u6570\u503c\u8303\u56f4\uff0c\u5bfc\u81f4\u91cf\u5316\u65f6\u7cbe\u5ea6\u4e25\u91cd\u4e0b\u964d\u3002\u968f\u7740\u9884\u8bad\u7ec3\u89c4\u6a21\u7684\u6269\u5927\uff08\u5982\u4eceCLIP\u5230SigLIP\u3001SigLIP2\uff09\uff0c\u5f02\u5e38\u503c\u95ee\u9898\u53d8\u5f97\u66f4\u52a0\u4e25\u91cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u76f8\u5173\u6027\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6fc0\u6d3b\u5f02\u5e38\u503c\u4e0e\u6743\u91cd\u77e9\u9635\u4e3b\u5bfc\u5947\u5f02\u503c\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\u3002\u57fa\u4e8e\u8fd9\u4e00\u6d1e\u5bdf\uff0c\u63d0\u51fa\u4e86\u9009\u62e9\u6027\u8c31\u8870\u51cf\uff08S\u00b2D\uff09\u65b9\u6cd5\u2014\u2014\u4e00\u79cd\u51e0\u4f55\u539f\u7406\u9a71\u52a8\u7684\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4ec5\u5bf9\u4e0e\u6700\u5927\u5947\u5f02\u503c\u5bf9\u5e94\u7684\u6743\u91cd\u5206\u91cf\u8fdb\u884c\u5916\u79d1\u624b\u672f\u5f0f\u7684\u6b63\u5219\u5316\u3002", "result": "S\u00b2D\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6fc0\u6d3b\u5f02\u5e38\u503c\uff0c\u751f\u6210\u4e86\u6761\u4ef6\u826f\u597d\u7684\u8868\u793a\uff0c\u8fd9\u4e9b\u8868\u793a\u672c\u8d28\u4e0a\u5bf9\u91cf\u5316\u53cb\u597d\u3002\u4f7f\u7528S\u00b2D\u8bad\u7ec3\u7684\u6a21\u578b\u5728W4A4\u91cf\u5316\u4e0b\uff0c\u5728ImageNet\u4e0a\u7684PTQ\u7cbe\u5ea6\u63d0\u5347\u4e86\u9ad8\u8fbe7%\uff0c\u4e0eQAT\u7ed3\u5408\u65f6\u63d0\u5347\u4e864%\u3002\u8fd9\u4e9b\u6539\u8fdb\u5728\u4e0b\u6e38\u4efb\u52a1\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4e5f\u5177\u6709\u6cdb\u5316\u6027\u3002", "conclusion": "S\u00b2D\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u727a\u7272\u90e8\u7f72\u6548\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u6269\u5c55\u8d8a\u6765\u8d8a\u5927\u89c4\u6a21\u548c\u4e25\u683c\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u91cf\u5316\u4e2d\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14452", "abs": "https://arxiv.org/abs/2602.14452", "authors": ["Lei Chen", "Yuan Meng", "Xiaoyu Zhan", "Zhi Wang", "Wenwu Zhu"], "title": "WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity", "comment": null, "summary": "Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.", "AI": {"tldr": "WiSparse\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6743\u91cd\u611f\u77e5\u6df7\u5408\u7c92\u5ea6\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6fc0\u6d3b\u548c\u6743\u91cd\u4fe1\u606f\u8fdb\u884c\u81ea\u9002\u5e94\u7a00\u758f\u5206\u914d\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901fLLM\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6fc0\u6d3b\u4fe1\u606f\u4e14\u4f7f\u7528\u7edf\u4e00\u7a00\u758f\u7387\uff0c\u5ffd\u7565\u4e86\u6743\u91cd\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u4e0d\u540c\u6a21\u578b\u5757\u5bf9\u7a00\u758f\u5316\u7684\u654f\u611f\u6027\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6743\u91cd\u611f\u77e5\u6df7\u5408\u7c92\u5ea6\u8bad\u7ec3\u514d\u8d39\u6fc0\u6d3b\u7a00\u758f\u5316\uff08WiSparse\uff09\uff1a1\uff09\u6743\u91cd\u611f\u77e5\u673a\u5236\u7ed3\u5408\u6fc0\u6d3b\u5e45\u5ea6\u548c\u9884\u8ba1\u7b97\u6743\u91cd\u8303\u6570\u6765\u8bc6\u522b\u91cd\u8981\u901a\u9053\uff1b2\uff09\u6df7\u5408\u7c92\u5ea6\u5206\u914d\u65b9\u6848\uff1a\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u5168\u5c40\u5206\u914d\u7a00\u758f\u9884\u7b97\u4fdd\u62a4\u654f\u611f\u533a\u57df\uff0c\u7136\u540e\u5728\u5757\u5185\u7ec6\u5316\u6700\u5c0f\u5316\u91cd\u6784\u8bef\u5dee\uff1b3\uff09\u6539\u8fdb\u7a00\u758f\u6838\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c50%\u7a00\u758f\u7387\u4e0b\uff0cWiSparse\u4fdd\u6301Llama3.1\u5bc6\u96c6\u6a21\u578b97%\u7684\u6027\u80fd\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u53472.23\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5b9e\u73b0\u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea621.4%\u7684\u52a0\u901f\u3002", "conclusion": "WiSparse\u63a8\u8fdb\u4e86\u65e0\u9700\u8bad\u7ec3\u65b9\u6cd5\u5728\u9ad8\u6548LLM\u63a8\u7406\u4e2d\u7684\u6781\u9650\uff0c\u5728\u4e0d\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u7a81\u7834\u4e86\u53ef\u5b9e\u73b0\u7684\u52a0\u901f\u8fb9\u754c\u3002"}}
{"id": "2602.14462", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14462", "abs": "https://arxiv.org/abs/2602.14462", "authors": ["Hong Li", "Zhen Zhou", "Honggang Zhang", "Yuping Luo", "Xinyue Wang", "Han Gong", "Zhiyuan Liu"], "title": "Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment", "comment": "9 pages, 8 figures", "summary": "Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \\emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \\texttt{openPangu-Embedded-1B-V1.1} model on the \\texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u4e2d\u5de5\u4f5c\u8282\u70b9\u95f4\u7684\"\u9759\u9ed8\u4e0d\u4e00\u81f4\u6027\"\uff0c\u5373\u635f\u5931\u548c\u68af\u5ea6\u5dee\u5f02\u5728\u805a\u5408\u76d1\u63a7\u4fe1\u53f7\u4e0b\u4e0d\u53ef\u89c1\u7684\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u4e2d\uff0c\u867d\u7136\u53c2\u6570\u540c\u6b65\u4fdd\u8bc1\u4e86\u6a21\u578b\u6743\u91cd\u7684\u6570\u503c\u7b49\u4ef7\u6027\uff0c\u4f46\u5e76\u4e0d\u80fd\u786e\u4fdd\u5de5\u4f5c\u8282\u70b9\u7ea7\u4f18\u5316\u52a8\u6001\u5728\u68af\u5ea6\u805a\u5408\u524d\u7684\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\"\u9759\u9ed8\u4e0d\u4e00\u81f4\u6027\"\u53ef\u80fd\u5bfc\u81f4\u9690\u85cf\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u805a\u5408\u76d1\u63a7\u4fe1\u53f7\u65e0\u6cd5\u68c0\u6d4b\u5230\u8fd9\u79cd\u8de8\u5de5\u4f5c\u8282\u70b9\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u6307\u6807\uff1a\u635f\u5931\u5206\u6563\u5ea6\u3001\u68af\u5ea6\u8303\u6570\u5206\u6563\u5ea6\u3001\u4ee5\u53ca\u901a\u8fc7\u5de5\u4f5c\u8282\u70b9\u95f4\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8861\u91cf\u7684\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u6307\u6807\u4f7f\u7528\u6807\u51c6\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u5df2\u6709\u7684\u4fe1\u53f7\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3001\u540c\u6b65\u673a\u5236\u6216\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u57288-NPU\u6570\u636e\u5e76\u884c\u8bbe\u7f6e\u4e0b\u5bf91B\u53c2\u6570\u6a21\u578b\u8fdb\u884c\u5168\u53c2\u6570\u5fae\u8c03\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u9010\u6b65\u53bb\u540c\u6b65\u5316\u7684\u6570\u636e\u6d17\u724c\u548c\u968f\u673a\u79cd\u5b50\u4f1a\u5bfc\u81f4\u635f\u5931/\u68af\u5ea6\u5206\u6563\u5ea6\u663e\u8457\u589e\u52a0\u548c\u65b9\u5411\u5bf9\u9f50\u5ea6\u964d\u4f4e\uff0c\u5c3d\u7ba1\u5168\u5c40\u5e73\u5747\u635f\u5931\u66f2\u7ebf\u4fdd\u6301\u5e73\u6ed1\u3002", "conclusion": "\u63d0\u51fa\u7684\u6307\u6807\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5e76\u884c\u5fae\u8c03\u4e2d\u7684\u9690\u85cf\u4e0d\u7a33\u5b9a\u6a21\u5f0f\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u53ef\u89c1\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8bca\u65ad\u548c\u914d\u7f6e\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.14468", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14468", "abs": "https://arxiv.org/abs/2602.14468", "authors": ["Chang Liu", "Yiran Zhao", "Lawrence Liu", "Yaoqi Ye", "Csaba Szepesv\u00e1ri", "Lin F. Yang"], "title": "LACONIC: Length-Aware Constrained Reinforcement Learning for LLM", "comment": null, "summary": "Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.", "AI": {"tldr": "LACONIC\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u76ee\u6807token\u9884\u7b97\u6765\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4f1a\u4ea7\u751f\u8fc7\u957f\u7684\u54cd\u5e94\uff0c\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709\u7684\u957f\u5ea6\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u5956\u52b1\u8c03\u6574\uff0c\u53ef\u80fd\u4e0e\u4efb\u52a1\u76ee\u6807\u4e0d\u4e00\u81f4\u4e14\u9700\u8981\u8106\u5f31\u7684\u8c03\u53c2\u3002", "method": "\u63d0\u51faLACONIC\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u76ee\u6807token\u9884\u7b97\u3002\u4f7f\u7528\u589e\u5f3a\u7684\u76ee\u6807\u51fd\u6570\u66f4\u65b0\u7b56\u7565\u6a21\u578b\uff0c\u7ed3\u5408\u4efb\u52a1\u5956\u52b1\u548c\u57fa\u4e8e\u957f\u5ea6\u7684\u6210\u672c\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6210\u672c\u89c4\u6a21\u6765\u5e73\u8861\u7b80\u6d01\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0cLACONIC\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86pass@1\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u4e8650%\u4ee5\u4e0a\u3002\u5728\u901a\u7528\u77e5\u8bc6\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u752844%\u66f4\u5c11\u7684token\u4fdd\u6301\u4e86\u57df\u5916\u6027\u80fd\u3002", "conclusion": "LACONIC\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u8fc7\u7a0b\u4e14\u90e8\u7f72\u5f00\u9500\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u96c6\u6210\u5230\u6807\u51c6RL\u8c03\u4f18\u4e2d\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u5ea6\u63a7\u5236\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u5956\u52b1\u3002"}}
{"id": "2602.14474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14474", "abs": "https://arxiv.org/abs/2602.14474", "authors": ["Aadirupa Saha", "Amith Bhat", "Haipeng Luo"], "title": "One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise", "comment": null, "summary": "We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\\{\u03c3_j^2\\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\\tilde{O}\\left({\u03c3^*}^2\\sum_{i=2}^K \\frac{\\log T}{\u0394_i} + \\sqrt{K \\sum_{j=1}^M \u03c3_j^2}\\right)$, up to preprocessing costs depending only on problem parameters, where ${\u03c3^*}^2 := \\min_j \u03c3_j^2$ is the minimum source variance and $\u0394_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${\u03c3^*}^2$, while incurring only an small (and unavoidable) additive cost of $\\tilde O(\\sqrt{K \\sum_{j=1}^M \u03c3_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $\u03c3_{\\max}^2$ in place of ${\u03c3^*}^2$-a gap that can be arbitrarily large when $\u03c3_{\\max} \\gg \u03c3^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\\;25M dataset, demonstrating the superior performance of SOAR over the baselines.", "AI": {"tldr": "SOAR\u7b97\u6cd5\u5728\u591a\u6e90\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u6570\u636e\u6e90\u6765\u6700\u5c0f\u5316\u9057\u61be\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u5355\u6e90\u6027\u80fd\u7684\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u591a\u6e90\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6570\u636e\u6e90\u5177\u6709\u672a\u77e5\u4e14\u4e0d\u540c\u7684\u566a\u58f0\u65b9\u5dee\u3002\u5b66\u4e60\u8005\u9700\u8981\u5728\u6807\u51c6MAB\u9057\u61be\u6700\u5c0f\u5316\u7684\u57fa\u7840\u4e0a\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u67e5\u8be2\u54ea\u4e2a\u6570\u636e\u6e90\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u6570\u636e\u6e90\u8d28\u91cf\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSOAR\u7b97\u6cd5\uff1a1\uff09\u4f7f\u7528\u5c16\u9510\u7684\u65b9\u5dee\u96c6\u4e2d\u754c\u9650\u5feb\u901f\u526a\u679d\u9ad8\u65b9\u5dee\u6570\u636e\u6e90\uff1b2\uff09\u91c7\u7528\"\u5e73\u8861\u6700\u5c0f\u6700\u5927LCB-UCB\u65b9\u6cd5\"\uff0c\u5c06\u8bc6\u522b\u6700\u4f73\u81c2\u548c\u6700\u4f18\uff08\u6700\u5c0f\u65b9\u5dee\uff09\u6570\u636e\u6e90\u7684\u4efb\u52a1\u65e0\u7f1d\u96c6\u6210\u3002", "result": "SOAR\u5b9e\u73b0\u4e86\u5b9e\u4f8b\u4f9d\u8d56\u7684\u9057\u61be\u754c$\\tilde{O}\\left({\u03c3^*}^2\\sum_{i=2}^K \\frac{\\log T}{\u0394_i} + \\sqrt{K \\sum_{j=1}^M \u03c3_j^2}\\right)$\uff0c\u5176\u4e2d${\u03c3^*}^2$\u662f\u6700\u5c0f\u6e90\u65b9\u5dee\u3002\u8be5\u7ed3\u679c\u4f18\u4e8eUniform UCB\u6216Explore-then-Commit UCB\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540e\u8005\u53ef\u80fd\u906d\u53d7\u4e0e$\u03c3_{\\max}^2$\u6210\u6bd4\u4f8b\u7684\u9057\u61be\u3002", "conclusion": "\u5c3d\u7ba1\u7f3a\u4e4f\u5bf9\u6700\u5c0f\u65b9\u5dee\u6e90\u7684\u5148\u9a8c\u77e5\u8bc6\uff0cSOAR\u80fd\u591f\u8fbe\u5230\u5177\u6709\u65b9\u5dee${\u03c3^*}^2$\u7684\u6807\u51c6\u5355\u6e90MAB\u7684\u6700\u4f18\u5b9e\u4f8b\u4f9d\u8d56\u9057\u61be\uff0c\u540c\u65f6\u4ec5\u4ea7\u751f\u8f83\u5c0f\u7684\u53ef\u52a0\u6210\u672c\u7528\u4e8e\u6700\u4f18\u6e90\u8bc6\u522b\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SOAR\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.14506", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14506", "abs": "https://arxiv.org/abs/2602.14506", "authors": ["Kutay Tire", "Yufan Zhang", "Ege Onur Taga", "Samet Oymak"], "title": "Covariance-Aware Transformers for Quadratic Programming and Decision Making", "comment": null, "summary": "We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\\frac{1}{2}x^\\top Ax+b^\\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical \"Predict-then-Optimize (PtO)\" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.", "AI": {"tldr": "Transformer\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u53ef\u8bc1\u660e\u5730\u89e3\u51b3\u65e0\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\uff0c\u7ed3\u5408MLP\u53ef\u89e3\u51b3L1\u60e9\u7f5a\u548c\u7ea6\u675f\u7684\u4e8c\u6b21\u89c4\u5212\uff0c\u5e76\u63d0\u51fa\u4e86Time2Decide\u65b9\u6cd5\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u5728\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22Transformer\u5728\u89e3\u51b3\u4e8c\u6b21\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u8fd9\u79cd\u80fd\u529b\u5982\u4f55\u6709\u76ca\u4e8e\u6d89\u53ca\u534f\u65b9\u5dee\u77e9\u9635\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u7279\u522b\u662f\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "1. \u8bc1\u660e\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u53ef\u901a\u8fc7\u9010\u884c\u6807\u8bb0\u5316\u77e9\u9635\u53d8\u91cf\u6765\u6a21\u62df\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3\uff0c\u4ece\u800c\u89e3\u51b3\u65e0\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\uff1b2. \u901a\u8fc7\u7ed3\u5408MLP\uff0cTransformer\u5757\u53ef\u89e3\u51b3L1\u60e9\u7f5a\u7684\u4e8c\u6b21\u89c4\u5212\uff08\u6a21\u62df\u8fed\u4ee3\u8f6f\u9608\u503c\uff09\u548cL1\u7ea6\u675f\u7684\u4e8c\u6b21\u89c4\u5212\uff08\u9700\u8981\u989d\u5916\u53cd\u9988\u73af\uff09\uff1b3. \u63d0\u51faTime2Decide\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u8f93\u5165\u53d8\u91cf\u95f4\u7684\u534f\u65b9\u5dee\u77e9\u9635\u6765\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u3002", "result": "Time2Decide\u5728\u7ecf\u5178\u7684L1\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7840\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff1b\u5728\u9002\u5f53\u8bbe\u7f6e\u4e0b\uff0c\u751a\u81f3\u4f18\u4e8e\u4f20\u7edf\u7684\"\u9884\u6d4b\u540e\u4f18\u5316\"\u6d41\u7a0b\uff0c\u5176\u4e2d\u5148\u9884\u6d4b\u6536\u76ca\u518d\u663e\u5f0f\u6c42\u89e3\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u3002", "conclusion": "Transformer\u4ece\u663e\u5f0f\u4f7f\u7528\u4e8c\u9636\u7edf\u8ba1\u91cf\u4e2d\u53d7\u76ca\uff0c\u8fd9\u4f7f\u5b83\u4eec\u80fd\u591f\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u5982\u6295\u8d44\u7ec4\u5408\u6784\u5efa\uff0c\u5c55\u793a\u4e86Transformer\u5728\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.14519", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14519", "abs": "https://arxiv.org/abs/2602.14519", "authors": ["Chaosheng Dong", "Peiyao Xiao", "Yijia Wang", "Kaiyi Ji"], "title": "DeepMTL2R: A Library for Deep Multi-task Learning to Rank", "comment": null, "summary": "This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \\href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.", "AI": {"tldr": "DeepMTL2R\u662f\u4e00\u4e2a\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u6392\u5e8f\uff0c\u901a\u8fc7transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5f02\u6784\u76f8\u5173\u6027\u4fe1\u53f7\uff0c\u652f\u630121\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u7b97\u6cd5\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5b9e\u73b0Pareto\u6700\u4f18\u6392\u5e8f\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3\u6392\u5e8f\u7cfb\u7edf\u9700\u8981\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76f8\u5173\u6027\u6807\u51c6\uff0c\u8fd9\u4e9b\u6807\u51c6\u53ef\u80fd\u76f8\u4e92\u51b2\u7a81\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u5f02\u6784\u76f8\u5173\u6027\u4fe1\u53f7\u5e76\u5904\u7406\u76ee\u6807\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u53ef\u6269\u5c55\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6392\u5e8f\u6846\u67b6\u3002", "method": "DeepMTL2R\u5229\u7528transformer\u67b6\u6784\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u5f02\u6784\u76f8\u5173\u6027\u4fe1\u53f7\u6574\u5408\u5230\u7edf\u4e00\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u4e2d\u3002\u6846\u67b6\u5305\u542b21\u79cd\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u7b97\u6cd5\uff0c\u652f\u6301\u591a\u76ee\u6807\u4f18\u5316\u4ee5\u8bc6\u522bPareto\u6700\u4f18\u6392\u5e8f\u6a21\u578b\uff0c\u80fd\u591f\u6355\u6349\u9879\u76ee\u548c\u6807\u7b7e\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u548c\u957f\u8ddd\u79bb\u4ea4\u4e92\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u62a5\u544a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u53ef\u89c6\u5316\u4e86\u76ee\u6807\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002DeepMTL2R\u4e3a\u73b0\u4ee3\u6392\u5e8f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4fc3\u8fdb\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u95f4\u7684\u53d7\u63a7\u6bd4\u8f83\u3002", "conclusion": "DeepMTL2R\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u6392\u5e8f\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u6765\u5f00\u53d1\u548c\u6bd4\u8f83\u591a\u4efb\u52a1\u5b66\u4e60\u6392\u5e8f\u6a21\u578b\u3002"}}
{"id": "2602.14543", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14543", "abs": "https://arxiv.org/abs/2602.14543", "authors": ["Francesco Emanuele Stradi", "Kalana Kalupahana", "Matteo Castiglioni", "Alberto Marchesi", "Nicola Gatti"], "title": "Truly Adapting to Adversarial Constraints in Constrained MABs", "comment": null, "summary": "We study the constrained variant of the \\emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \\emph{unknown} constraints, under both \\emph{full} and \\emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \\emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \\emph{full feedback} we propose an algorithm attaining $\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$ regret and $\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \\emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$ {positive} violation and $\\widetilde{\\mathcal{O}}(\\sqrt{T}+C\\sqrt{T})$ regret.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u672a\u77e5\u7ea6\u675f\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e0b\u540c\u65f6\u6700\u5c0f\u5316\u603b\u635f\u5931\u548c\u63a7\u5236\u7ea6\u675f\u8fdd\u53cd\uff0c\u63d0\u51fa\u4e86\u5728\u4e0d\u540c\u53cd\u9988\u673a\u5236\u4e0b\u7684\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u5728\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\uff0c\u5b66\u4e60\u8005\u4e0d\u4ec5\u8981\u6700\u5c0f\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u603b\u635f\u5931\uff0c\u8fd8\u8981\u63a7\u5236\u591a\u4e2a\u672a\u77e5\u7ea6\u675f\u7684\u8fdd\u53cd\u3002\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5047\u8bbe\u7ea6\u675f\u662f\u968f\u673a\u7684\uff0c\u8981\u4e48\u901a\u8fc7\u7ade\u4e89\u6bd4\u7b49\u653e\u677e\u57fa\u51c6\u6765\u5904\u7406\u5b8c\u5168\u5bf9\u6297\u6027\u7ea6\u675f\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u5728\u7ea6\u675f\u968f\u673a\u800c\u635f\u5931\u4efb\u610f\u53d8\u5316\u65f6\uff0c\u540c\u65f6\u83b7\u5f97\u6700\u4f18\u7684\u9057\u61be\u548c\u6b63\u7ea6\u675f\u8fdd\u53cd\u7387\uff0c\u5e76\u80fd\u5e73\u6ed1\u9002\u5e94\u7ea6\u675f\u5bf9\u6297\u6027\u7a0b\u5ea6\u7684\u53d8\u5316\u3002", "method": "\u9488\u5bf9\u4e0d\u540c\u53cd\u9988\u673a\u5236\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7b97\u6cd5\uff1a1\uff09\u5b8c\u5168\u53cd\u9988\u4e0b\uff0c\u63d0\u51fa\u7b97\u6cd5\u83b7\u5f97$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$\u7684\u9057\u61be\u548c\u6b63\u7ea6\u675f\u8fdd\u53cd\uff1b2\uff09\u4ec5\u635f\u5931\u6709\u8001\u864e\u673a\u53cd\u9988\u65f6\uff0c\u6269\u5c55\u4e86\u8fd9\u4e9b\u4fdd\u8bc1\uff1b3\uff09\u7ea6\u675f\u4e5f\u6709\u8001\u864e\u673a\u53cd\u9988\u65f6\uff0c\u8bbe\u8ba1\u4e86\u7b97\u6cd5\u83b7\u5f97$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$\u7684\u6b63\u7ea6\u675f\u8fdd\u53cd\u548c$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C\\sqrt{T})$\u7684\u9057\u61be\u3002\u5176\u4e2d$C$\u91cf\u5316\u4e86\u7ea6\u675f\u7684\u975e\u5e73\u7a33\u6027\u7a0b\u5ea6\u3002", "result": "\u5728\u5b8c\u5168\u53cd\u9988\u4e0b\uff0c\u7b97\u6cd5\u8fbe\u5230\u4e86$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$\u7684\u9057\u61be\u548c$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$\u7684\u6b63\u7ea6\u675f\u8fdd\u53cd\u3002\u5f53\u53ea\u6709\u635f\u5931\u6709\u8001\u864e\u673a\u53cd\u9988\u65f6\uff0c\u80fd\u591f\u6269\u5c55\u8fd9\u4e9b\u4fdd\u8bc1\u3002\u5f53\u7ea6\u675f\u4e5f\u6709\u8001\u864e\u673a\u53cd\u9988\u65f6\uff0c\u7b97\u6cd5\u5b9e\u73b0\u4e86$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$\u7684\u6b63\u7ea6\u675f\u8fdd\u53cd\u548c$\\widetilde{\\mathcal{O}}(\\sqrt{T}+C\\sqrt{T})$\u7684\u9057\u61be\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5728\u7ea6\u675f\u968f\u673a\u800c\u635f\u5931\u4efb\u610f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u8bbe\u8ba1\u4e86\u80fd\u591f\u540c\u65f6\u83b7\u5f97\u6700\u4f18\u9057\u61be\u548c\u6b63\u7ea6\u675f\u8fdd\u53cd\u7387\u7684\u7b97\u6cd5\uff0c\u5e76\u4e14\u8fd9\u4e9b\u4fdd\u8bc1\u80fd\u591f\u5e73\u6ed1\u5730\u9002\u5e94\u7ea6\u675f\u5bf9\u6297\u6027\u7a0b\u5ea6\u7684\u53d8\u5316\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.14580", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14580", "abs": "https://arxiv.org/abs/2602.14580", "authors": ["Matteo Bollini", "Gianmarco Genalti", "Francesco Emanuele Stradi", "Matteo Castiglioni", "Alberto Marchesi"], "title": "Replicable Constrained Bandits", "comment": null, "summary": "Algorithmic \\emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \\emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \\emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \\emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7684\u7b97\u6cd5\u53ef\u590d\u5236\u6027\uff0c\u63d0\u51fa\u4e86\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u590d\u5236\u6027\u7684\u7b97\u6cd5\uff0c\u5176\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\u4e0e\u975e\u53ef\u590d\u5236\u7b97\u6cd5\u76f8\u5f53\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5b9e\u9a8c\u9700\u8981\u53ef\u91cd\u590d\u6027\uff0c\u7b97\u6cd5\u53ef\u590d\u5236\u6027\u65e8\u5728\u786e\u4fdd\u7b97\u6cd5\u5728\u4e0d\u540c\u6267\u884c\u4e2d\u505a\u51fa\u76f8\u540c\u51b3\u7b56\u3002\u672c\u6587\u9996\u6b21\u7814\u7a76\u7ea6\u675f\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u7684\u53ef\u590d\u5236\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u53ef\u590d\u5236\u7684\u7ea6\u675f\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u9996\u5148\u5f00\u53d1\u4e86\u9996\u4e2a\u53ef\u590d\u5236\u7684UCB\u7c7b\u7b97\u6cd5\u7528\u4e8e\u65e0\u7ea6\u675f\u60c5\u51b5\uff0c\u8bc1\u660e\u57fa\u4e8e\u4e50\u89c2\u4e0d\u786e\u5b9a\u6027\u539f\u5219\u7684\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u53ef\u590d\u5236\u6027\uff0c\u7136\u540e\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230\u7ea6\u675f\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u7ea6\u675f\u591a\u81c2\u8001\u864e\u673a\u4e2d\u7684\u53ef\u590d\u5236\u6027\uff0c\u8bbe\u8ba1\u7684\u53ef\u590d\u5236\u7b97\u6cd5\u5728\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\u65b9\u9762\u4e0e\u975e\u53ef\u590d\u5236\u7b97\u6cd5\u5177\u6709\u76f8\u540c\u7684T\u9636\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4e50\u89c2\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u53ef\u590d\u5236\u6027\u3002", "conclusion": "\u7b97\u6cd5\u53ef\u590d\u5236\u6027\u53ef\u4ee5\u5728\u7ea6\u675f\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u5b9e\u73b0\uff0c\u4e14\u6027\u80fd\u4e0e\u975e\u53ef\u590d\u5236\u7b97\u6cd5\u76f8\u5f53\uff0c\u8fd9\u4e00\u7ed3\u679c\u4e3a\u53ef\u590d\u5236\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2602.14587", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.14587", "abs": "https://arxiv.org/abs/2602.14587", "authors": ["Minh Nguyen"], "title": "Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow", "comment": null, "summary": "Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u66f4\u65b0\u89e3\u51b3\u6807\u51c6\u79bb\u6563\u65f6\u95f4RL\u5728\u8fde\u7eed\u65f6\u95f4\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5b9e\u9645\u4ea4\u6613\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u63a7\u5236\u95ee\u9898\uff08\u91d1\u878d\u3001\u673a\u5668\u4eba\u7b49\uff09\u901a\u5e38\u5728\u8fde\u7eed\u65f6\u95f4\u4e2d\u6f14\u5316\uff0c\u5177\u6709\u975e\u5747\u5300\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u51b3\u7b56\u7279\u6027\u3002\u6807\u51c6\u79bb\u6563\u65f6\u95f4RL\u57fa\u4e8e\u56fa\u5b9a\u6b65\u957fBellman\u66f4\u65b0\uff0c\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1a\u5f53\u65f6\u95f4\u95f4\u9694\u7f29\u5c0f\u65f6\uff0cQ\u51fd\u6570\u4f1a\u574d\u7f29\u5230\u503c\u51fd\u6570V\uff0c\u5931\u53bb\u52a8\u4f5c\u6392\u5e8f\u80fd\u529b\u3002\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u901a\u8fc7\u4f18\u52bf\u7387\u51fd\u6570q\u91cd\u65b0\u5f15\u5165\u52a8\u4f5c\u4fe1\u606f\uff0c\u4f46\u4f7f\u7528\u590d\u6742\u7684\u9785\u635f\u5931\u6216\u6b63\u4ea4\u7ea6\u675f\u6765\u5f3a\u5236\u6700\u4f18\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u6d4b\u8bd5\u8fc7\u7a0b\u7684\u9009\u62e9\u654f\u611f\uff0c\u5e76\u5c06V\u548cq\u8026\u5408\u5230\u5927\u578b\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u96be\u4ee5\u53ef\u9760\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u8026\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u91c7\u7528\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\uff1a1\uff09q\u901a\u8fc7V\u7684\u6269\u6563\u751f\u6210\u5668\u5b66\u4e60\uff1b2\uff09V\u901a\u8fc7\u57fa\u4e8eHamiltonian\u7684\u503c\u6d41\u66f4\u65b0\uff0c\u8be5\u503c\u6d41\u5728\u65e0\u9650\u5c0f\u65f6\u95f4\u6b65\u4e0b\u4ecd\u4fdd\u6301\u4fe1\u606f\u6027\uff08\u6807\u51c6max/softmax\u5907\u4efd\u5728\u6b64\u60c5\u51b5\u4e0b\u5931\u6548\uff09\u3002\u7406\u8bba\u4e0a\u901a\u8fc7\u65b0\u7684\u6982\u7387\u8bba\u8bc1\u8bc1\u660e\u4e86\u4e25\u683c\u6536\u655b\u6027\uff0c\u7ed5\u8fc7\u4e86\u751f\u6210\u5668\u57faHamiltonian\u5728sup-norm\u4e0b\u7f3a\u4e4fBellman\u5f0f\u6536\u7f29\u7684\u6311\u6218\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u4ea4\u6613\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5148\u524d\u7684\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u548c\u9886\u5148\u7684\u79bb\u6563\u65f6\u95f4\u57fa\u7ebf\uff0c\u5728\u4e00\u4e2a\u5b63\u5ea6\u5185\u5b9e\u73b0\u4e8621%\u7684\u5229\u6da6\uff0c\u51e0\u4e4e\u662f\u6700\u4f73\u65b9\u6cd5\u76842\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u8026\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u901a\u8fc7\u4ea4\u66ff\u66f4\u65b0V\u548cq\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4RL\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14602", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14602", "abs": "https://arxiv.org/abs/2602.14602", "authors": ["Tianyi Ma", "Yiyang Li", "Yiyue Qian", "Zheyuan Zhang", "Zehong Wang", "Chuxu Zhang", "Yanfang Ye"], "title": "OPBench: A Graph Benchmark to Combat the Opioid Crisis", "comment": null, "summary": "The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.", "AI": {"tldr": "OPBench\u662f\u9996\u4e2a\u5168\u9762\u7684\u963f\u7247\u7c7b\u836f\u7269\u5371\u673a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e2a\u6570\u636e\u96c6\u8986\u76d63\u4e2a\u5173\u952e\u5e94\u7528\u9886\u57df\uff0c\u63d0\u4f9b\u6807\u51c6\u8bc4\u4f30\u6846\u67b6\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u963f\u7247\u5371\u673a\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u963f\u7247\u7c7b\u836f\u7269\u5371\u673a\u6301\u7eed\u8086\u8650\u5168\u7403\uff0c\u6025\u9700\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002\u867d\u7136\u56fe\u5b66\u4e60\u65b9\u6cd5\u5df2\u6210\u4e3a\u5efa\u6a21\u590d\u6742\u836f\u7269\u76f8\u5173\u73b0\u8c61\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u5728\u771f\u5b9e\u963f\u7247\u5371\u673a\u573a\u666f\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u7684\u7efc\u5408\u57fa\u51c6\u3002", "method": "\u521b\u5efaOPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b5\u4e2a\u6570\u636e\u96c6\u8986\u76d63\u4e2a\u5173\u952e\u9886\u57df\uff1a\u533b\u7597\u7d22\u8d54\u4e2d\u7684\u963f\u7247\u7c7b\u836f\u7269\u8fc7\u91cf\u68c0\u6d4b\u3001\u6570\u5b57\u5e73\u53f0\u4e2d\u7684\u975e\u6cd5\u836f\u7269\u8d29\u8fd0\u68c0\u6d4b\u3001\u996e\u98df\u6a21\u5f0f\u4e2d\u7684\u836f\u7269\u6ee5\u7528\u9884\u6d4b\u3002\u91c7\u7528\u5f02\u6784\u56fe\u548c\u8d85\u56fe\u7b49\u591a\u6837\u56fe\u7ed3\u6784\uff0c\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u6807\u6ce8\u6570\u636e\uff0c\u5efa\u7acb\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6807\u51c6\u5316\u534f\u8bae\u3001\u9884\u5b9a\u4e49\u6570\u636e\u5206\u5272\u548c\u53ef\u590d\u73b0\u57fa\u7ebf\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u5206\u6790\u4e86\u73b0\u6709\u56fe\u5b66\u4e60\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u963f\u7247\u5371\u673a\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "OPBench\u586b\u8865\u4e86\u963f\u7247\u5371\u673a\u9886\u57df\u7f3a\u4e4f\u7efc\u5408\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u56fe\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5bf9\u6297\u963f\u7247\u5371\u673a\u7684\u8ba1\u7b97\u7814\u7a76\u3002"}}
{"id": "2602.14626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14626", "abs": "https://arxiv.org/abs/2602.14626", "authors": ["Karim Galliamov", "Syed M Ahsan Kazmi", "Adil Khan", "Ad\u00edn Ram\u00edrez Rivera"], "title": "Concepts' Information Bottleneck Models", "comment": "To appear in ICLR 2026, code: https://github.com/dsb-ifi/cibm", "summary": "Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u6b63\u5219\u5316\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6982\u5ff5\u5c42\u65bd\u52a0\u4fe1\u606f\u74f6\u9888\u7ea6\u675f\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6982\u5ff5\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u867d\u7136\u65e8\u5728\u901a\u8fc7\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u6982\u5ff5\u5c42\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u4f46\u901a\u5e38\u9762\u4e34\u51c6\u786e\u6027\u4e0b\u964d\u548c\u6982\u5ff5\u6cc4\u9732\u95ee\u9898\uff0c\u8fd9\u524a\u5f31\u4e86\u6982\u5ff5\u7684\u5fe0\u5b9e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6982\u5ff5\u8868\u793a\u7684\u5145\u5206\u6027\u548c\u6700\u5c0f\u6027\u4e4b\u95f4\u5b58\u5728\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u7684\u4fe1\u606f\u74f6\u9888\u6b63\u5219\u5316\u5668\uff0c\u5728\u6982\u5ff5\u5c42\u60e9\u7f5aI(X;C)\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606fI(C;Y)\uff0c\u9f13\u52b1\u6700\u5c0f\u5145\u5206\u7684\u6982\u5ff5\u8868\u793a\u3002\u63a8\u5bfc\u4e86\u4e24\u79cd\u5b9e\u7528\u53d8\u4f53\uff08\u53d8\u5206\u76ee\u6807\u548c\u57fa\u4e8e\u71b5\u7684\u66ff\u4ee3\u65b9\u6cd5\uff09\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6807\u51c6CBM\u8bad\u7ec3\u4e2d\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u6216\u989d\u5916\u76d1\u7763\u3002", "result": "\u5728\u516d\u4e2aCBM\u5bb6\u65cf\u548c\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cIB\u6b63\u5219\u5316\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5176\u539f\u59cb\u7248\u672c\u3002\u4fe1\u606f\u5e73\u9762\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u9884\u671f\u884c\u4e3a\uff0c\u8868\u660e\u6700\u5c0f\u5145\u5206\u6982\u5ff5\u74f6\u9888\u80fd\u540c\u65f6\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u6982\u5ff5\u7ea7\u5e72\u9884\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6b63\u5219\u5316\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u3001\u67b6\u6784\u65e0\u5173\u7684\u8def\u5f84\uff0c\u53ef\u6784\u5efa\u66f4\u5fe0\u5b9e\u548c\u53ef\u5e72\u9884\u7684CBMs\u3002\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u534f\u8bae\u89e3\u51b3\u4e86\u5148\u524d\u8bc4\u4f30\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7a33\u5065\u7684\u6539\u8fdb\u3002"}}
{"id": "2602.14635", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14635", "abs": "https://arxiv.org/abs/2602.14635", "authors": ["Rohit Raj Rai", "Abhishek Dhaka", "Amit Awekar"], "title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models", "comment": null, "summary": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.", "AI": {"tldr": "\u63d0\u51faAlignment Adapter (AlAd)\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5bf9\u9f50\u538b\u7f29\u6a21\u578b\u4e0e\u539f\u5927\u6a21\u578b\u7684token\u7ea7\u5d4c\u5165\uff0c\u63d0\u5347\u538b\u7f29\u6a21\u578b\u6027\u80fd", "motivation": "\u538b\u7f29\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6027\u80fd\u901a\u5e38\u843d\u540e\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u9700\u8981\u7f29\u5c0f\u8fd9\u4e00\u6027\u80fd\u5dee\u8ddd", "method": "\u63d0\u51fa\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u7684\u8f7b\u91cf\u7ea7\u9002\u914d\u5668AlAd\uff0c\u5bf9\u9f50\u538b\u7f29\u6a21\u578b\u4e0e\u539f\u5927\u6a21\u578b\u7684token\u7ea7\u5d4c\u5165\uff0c\u4fdd\u6301\u5c40\u90e8\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u652f\u6301\u4e0d\u540c\u7ef4\u5ea6\u6216\u67b6\u6784\u7684\u7075\u6d3b\u5bf9\u9f50\uff0c\u4e0e\u5e95\u5c42\u538b\u7f29\u65b9\u6cd5\u65e0\u5173", "result": "\u5728BERT\u7cfb\u5217\u6a21\u578b\u7684\u4e09\u4e2atoken\u7ea7NLP\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cAlAd\u663e\u8457\u63d0\u5347\u538b\u7f29\u6a21\u578b\u6027\u80fd\uff0c\u4ec5\u5e26\u6765\u5fae\u5c0f\u7684\u5c3a\u5bf8\u548c\u5ef6\u8fdf\u5f00\u9500", "conclusion": "AlAd\u662f\u4e00\u79cd\u6709\u6548\u7684\u538b\u7f29\u6a21\u578b\u6027\u80fd\u63d0\u5347\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u90e8\u7f72\u5728\u51bb\u7ed3\u7684\u538b\u7f29\u6a21\u578b\u4e0a\uff0c\u6216\u4e0e\u538b\u7f29\u6a21\u578b\u8054\u5408\u5fae\u8c03\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u6027\u80fd\u63d0\u5347"}}
{"id": "2602.14656", "categories": ["cs.LG", "math.DG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.14656", "abs": "https://arxiv.org/abs/2602.14656", "authors": ["Adri\u00e1n Javaloy", "Antonio Vergari"], "title": "An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale", "comment": "23 pages, 10 figures, in review", "summary": "Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.", "AI": {"tldr": "POGO\u662f\u4e00\u79cd\u65b0\u7684\u6b63\u4ea4\u7ea6\u675f\u4f18\u5316\u7b97\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff08\u5982Landing\u7b97\u6cd5\uff09\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\uff0c\u652f\u6301\u73b0\u4ee3\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u80fd\u5904\u7406\u6570\u5343\u4e2a\u6b63\u4ea4\u77e9\u9635\u7684\u5927\u89c4\u6a21\u95ee\u9898\u3002", "motivation": "\u6b63\u4ea4\u7ea6\u675f\u5728\u9c81\u68d2\u548c\u6982\u7387\u673a\u5668\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u4f18\u5316\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u6570\u767e\u6216\u6570\u5343\u4e2a\u7ea6\u675f\u7684\u95ee\u9898\u3002\u867d\u7136Landing\u7b97\u6cd5\u662f\u4e00\u4e2a\u4f8b\u5916\uff0c\u4f46\u5b83\u4ee5\u6682\u65f6\u653e\u677e\u6b63\u4ea4\u6027\u4e3a\u4ee3\u4ef7\u3002", "method": "POGO\u7b97\u6cd5\u91cd\u65b0\u5ba1\u89c6\u5e76\u6539\u8fdb\u4e86Landing\u7b97\u6cd5\u7684\u601d\u60f3\uff0c\u80fd\u591f\u96c6\u6210\u73b0\u4ee3\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u540c\u65f6\u786e\u4fdd\u6b63\u4ea4\u7ea6\u675f\u5f97\u5230\u6709\u6548\u6ee1\u8db3\u3002\u7b97\u6cd5\u5feb\u901f\u4e14GPU\u53cb\u597d\uff0c\u4ec5\u97005\u4e2a\u77e9\u9635\u4e58\u79ef\uff0c\u5728\u5b9e\u8df5\u4e2d\u59cb\u7ec8\u4fdd\u6301\u6b63\u4ea4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPOGO\u663e\u8457\u4f18\u4e8e\u6700\u8fd1\u7684\u4f18\u5316\u5668\uff0c\u80fd\u591f\u5728\u51e0\u5206\u949f\u5185\u4f18\u5316\u5305\u542b\u6570\u5343\u4e2a\u6b63\u4ea4\u77e9\u9635\u7684\u95ee\u9898\uff0c\u800c\u66ff\u4ee3\u65b9\u6cd5\u9700\u8981\u6570\u5c0f\u65f6\u3002", "conclusion": "POGO\u4e3a\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5927\u89c4\u6a21\u5229\u7528\u6b63\u4ea4\u7ea6\u675f\u8bbe\u5b9a\u4e86\u91cc\u7a0b\u7891\uff0c\u5176PyTorch\u5b9e\u73b0\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.14682", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.14682", "abs": "https://arxiv.org/abs/2602.14682", "authors": ["Farzan Farnia", "Mohammad Jalali", "Azim Ospanov"], "title": "Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error", "comment": null, "summary": "Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u591a\u6837\u6027\u504f\u5dee\uff0c\u751f\u6210\u7684\u6837\u672c\u591a\u6837\u6027\u663e\u8457\u4f4e\u4e8e\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u8fd9\u6e90\u4e8e\u6709\u9650\u6837\u672c\u4f30\u8ba1\u7684\u71b5\u57fa\u591a\u6837\u6027\u6307\u6807\u4f1a\u4f4e\u4f30\u771f\u5b9e\u5206\u5e03\u7684\u591a\u6837\u6027\u3002", "motivation": "\u867d\u7136\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u65b9\u9762\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u4e00\u4e2a\u91cd\u8981\u4f46\u8f83\u5c11\u7cfb\u7edf\u7814\u7a76\u7684\u95ee\u9898\u662f\uff1a\u8bad\u7ec3\u597d\u7684\u751f\u6210\u6a21\u578b\u662f\u5426\u5fe0\u5b9e\u5730\u6355\u6349\u4e86\u5e95\u5c42\u6570\u636e\u5206\u5e03\u7684\u591a\u6837\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u76f4\u63a5\u6bd4\u8f83\u751f\u6210\u6837\u672c\u4e0e\u6d4b\u8bd5\u6837\u672c\u7684\u591a\u6837\u6027\u6765\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6700\u8fd1\u63d0\u51fa\u7684\u65e0\u53c2\u8003\u71b5\u57fa\u591a\u6837\u6027\u8bc4\u5206\u65b9\u6cd5Vendi\u548cRKE\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u6700\u5148\u8fdb\u751f\u6210\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u4e0e\u4ece\u76ee\u6807\u6570\u636e\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\u6d4b\u8bd5\u6837\u672c\u7684\u591a\u6837\u6027\u3002\u5206\u6790\u71b5\u57fa\u591a\u6837\u6027\u8bc4\u5206\u7684\u6709\u9650\u6837\u672c\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8\u57fa\u4e8eVendi\u548cRKE\u7684\u591a\u6837\u6027\u611f\u77e5\u6b63\u5219\u5316\u548c\u5f15\u5bfc\u7b56\u7565\u3002", "result": "\u6d4b\u8bd5\u6570\u636e\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u83b7\u5f97\u4e86\u663e\u8457\u9ad8\u4e8e\u751f\u6210\u6837\u672c\u7684Vendi\u548cRKE\u591a\u6837\u6027\u5206\u6570\uff0c\u8868\u660e\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u5411\u4e0b\u591a\u6837\u6027\u504f\u5dee\u3002\u5206\u6790\u8868\u660e\uff0c\u71b5\u57fa\u591a\u6837\u6027\u8bc4\u5206\u7684\u671f\u671b\u503c\u968f\u6837\u672c\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u8fd9\u610f\u5473\u7740\u57fa\u4e8e\u6709\u9650\u8bad\u7ec3\u96c6\u4f30\u8ba1\u7684\u591a\u6837\u6027\u4f1a\u56fa\u6709\u5730\u4f4e\u4f30\u771f\u5b9e\u5206\u5e03\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u4f18\u5316\u751f\u6210\u5668\u4ee5\u6700\u5c0f\u5316\u4e0e\u7ecf\u9a8c\u6570\u636e\u5206\u5e03\u7684\u6563\u5ea6\u4f1a\u5bfc\u81f4\u591a\u6837\u6027\u635f\u5931\u3002\u57fa\u4e8eVendi\u548cRKE\u7684\u591a\u6837\u6027\u611f\u77e5\u6b63\u5219\u5316\u548c\u5f15\u5bfc\u7b56\u7565\u662f\u7f13\u89e3\u8fd9\u79cd\u504f\u5dee\u7684\u6709\u539f\u5219\u65b9\u5411\uff0c\u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\u8fd9\u4e9b\u7b56\u7565\u6709\u6f5c\u529b\u6539\u5584\u7ed3\u679c\u3002"}}
{"id": "2602.14687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14687", "abs": "https://arxiv.org/abs/2602.14687", "authors": ["David Chanin", "Adri\u00e0 Garriga-Alonso"], "title": "SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data", "comment": null, "summary": "Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.", "AI": {"tldr": "SynthSAEBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7a00\u758f\u81ea\u7f16\u7801\u5668\u67b6\u6784\u7684\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u771f\u5b9e\u7279\u5f81\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\u6a21\u578b\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u7cbe\u786e\u8bca\u65adSAE\u6545\u969c\u6a21\u5f0f\u5e76\u9a8c\u8bc1\u67b6\u6784\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dSAE\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5728LLM\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u566a\u58f0\u592a\u5927\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u67b6\u6784\u6539\u8fdb\uff1b\u800c\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u89c4\u6a21\u592a\u5c0f\u4e14\u4e0d\u771f\u5b9e\uff0c\u65e0\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u9a8c\u8bc1SAE\u67b6\u6784\u521b\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86SynthSAEBench\u5de5\u5177\u5305\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u771f\u5b9e\u7279\u5f81\uff08\u5305\u62ec\u76f8\u5173\u6027\u3001\u5c42\u6b21\u7ed3\u6784\u548c\u53e0\u52a0\uff09\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\uff0c\u5e76\u521b\u5efa\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u6a21\u578bSynthSAEBench-16k\uff0c\u4f7f\u4e0d\u540cSAE\u67b6\u6784\u80fd\u591f\u76f4\u63a5\u6bd4\u8f83\u3002", "result": "\u8be5\u57fa\u51c6\u6210\u529f\u590d\u73b0\u4e86\u591a\u4e2a\u5148\u524d\u89c2\u5bdf\u5230\u7684LLM SAE\u73b0\u8c61\uff0c\u5305\u62ec\u91cd\u6784\u4e0e\u6f5c\u5728\u8d28\u91cf\u6307\u6807\u4e4b\u95f4\u7684\u8131\u8282\u3001SAE\u63a2\u6d4b\u7ed3\u679c\u4e0d\u4f73\u4ee5\u53ca\u7531L0\u8c03\u8282\u7684\u7cbe\u5ea6-\u53ec\u56de\u6743\u8861\u3002\u8fd8\u53d1\u73b0\u4e86\u4e00\u4e2a\u65b0\u7684\u6545\u969c\u6a21\u5f0f\uff1a\u5339\u914d\u8ffd\u8e2aSAE\u5229\u7528\u53e0\u52a0\u566a\u58f0\u6765\u6539\u8fdb\u91cd\u6784\uff0c\u800c\u6ca1\u6709\u5b66\u4e60\u771f\u5b9e\u7279\u5f81\uff0c\u8868\u660e\u66f4\u5177\u8868\u8fbe\u529b\u7684\u7f16\u7801\u5668\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "conclusion": "SynthSAEBench\u901a\u8fc7\u63d0\u4f9b\u771f\u5b9e\u7279\u5f81\u548c\u53d7\u63a7\u6d88\u878d\u5b9e\u9a8c\uff0c\u8865\u5145\u4e86LLM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5728\u6269\u5c55\u5230LLM\u4e4b\u524d\u7cbe\u786e\u8bca\u65adSAE\u6545\u969c\u6a21\u5f0f\u5e76\u9a8c\u8bc1\u67b6\u6784\u6539\u8fdb\u3002"}}
{"id": "2602.14696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14696", "abs": "https://arxiv.org/abs/2602.14696", "authors": ["Nihal V. Nayak", "Paula Rodriguez-Diaz", "Neha Hulkund", "Sara Beery", "David Alvarez-Melis"], "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)", "comment": null, "summary": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u6307\u4ee4\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u53d1\u73b0\u68af\u5ea6\u8868\u793a\u4e0e\u8d2a\u5fc3\u8f6e\u8be2\u7b97\u6cd5\u5728\u4f4e\u9884\u7b97\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4f18\u52bf\u968f\u9884\u7b97\u589e\u52a0\u800c\u51cf\u5f31\u3002\u7814\u7a76\u7edf\u4e00\u4e86\u591a\u79cd\u73b0\u6709\u7b97\u6cd5\u4e3a\u8fd1\u4f3c\u8ddd\u79bb\u6700\u5c0f\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u9009\u62e9\u65b9\u6cd5\u7814\u7a76\u5206\u6563\u4e14\u4e0d\u900f\u660e\uff1a\u65b9\u6cd5\u5dee\u5f02\u5927\u3001\u5e38\u5ffd\u7565\u96f6\u6837\u672c\u57fa\u7ebf\u3001\u5173\u952e\u7ec4\u4ef6\u8d21\u732e\u6df7\u6742\uff0c\u5bfc\u81f4\u5b9e\u8df5\u8005\u7f3a\u4e4f\u9488\u5bf9\u76ee\u6807\u4efb\u52a1\u7684\u6307\u4ee4\u9009\u62e9\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u6846\u67b6\u5206\u79bb\u5206\u6790\u6570\u636e\u8868\u793a\u548c\u9009\u62e9\u7b97\u6cd5\u4e24\u4e2a\u6838\u5fc3\u8981\u7d20\uff0c\u652f\u6301\u8de8\u6a21\u578b\u3001\u4efb\u52a1\u548c\u9884\u7b97\u7684\u53d7\u63a7\u6bd4\u8f83\u3002\u5c06\u591a\u79cd\u73b0\u6709\u7b97\u6cd5\u7edf\u4e00\u4e3a\u67e5\u8be2\u96c6\u4e0e\u9009\u62e9\u5b50\u96c6\u95f4\u7684\u8fd1\u4f3c\u8ddd\u79bb\u6700\u5c0f\u5316\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u53ea\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u6570\u636e\u8868\u793a\u80fd\u4e00\u81f4\u9884\u6d4b\u6027\u80fd\uff1b\u68af\u5ea6\u8868\u793a+\u8d2a\u5fc3\u8f6e\u8be2\u7b97\u6cd5\u5728\u4f4e\u9884\u7b97\u4e0b\u5e73\u5747\u8868\u73b0\u6700\u4f73\uff1b\u4f18\u52bf\u968f\u9884\u7b97\u589e\u52a0\u800c\u51cf\u5f31\uff1b\u63d0\u4f9b\u4e86\u65b0\u7684\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\u652f\u6301\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u548c\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u66f4\u539f\u5219\u6027\u7684\u6307\u4ee4\u9009\u62e9\u65b9\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2602.14728", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14728", "abs": "https://arxiv.org/abs/2602.14728", "authors": ["Nozomu Fujisawa", "Masaaki Kondo"], "title": "D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation", "comment": "19 pages, 3 figures", "summary": "We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.", "AI": {"tldr": "D2-LoRA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e26\u7b26\u53f7\u7684\u4f4e\u79e9\u6b8b\u5dee\u66f4\u65b0\u548c\u5217\u5411\u6295\u5f71\u6280\u672f\uff0c\u5728\u6709\u9650\u6570\u636e\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u65f6\u7684\u4ee3\u6570\u53ef\u5408\u5e76\u6027\u3002", "motivation": "\u5728\u73b0\u5b9e\u7684\u6570\u636e\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\uff0c\u7cfb\u7edf\u7814\u7a76\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u96f6\u5ef6\u8fdf\u5408\u5e76\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5e26\u7b26\u53f7\u7684\u4f4e\u79e9\u6b8b\u5dee\u66f4\u65b0\uff08\u52a0\u6cd5\u548c\u51cf\u6cd5\u7ec4\u4ef6\uff09\u4ee5\u53ca\u8bad\u7ec3\u65f6\u7684\u5217\u5411\u6295\u5f71\uff0c\u4fdd\u6301\u6bcf\u5217\u63a5\u8fd1\u539f\u59cb\u8303\u6570\uff0c\u8bad\u7ec3\u540e\u5c06\u9002\u914d\u5668\u5408\u5e76\u4e3a\u5355\u4e00\u6743\u91cd\u77e9\u9635\u3002", "result": "\u57288\u4e2a\u95ee\u7b54\u548c\u9605\u8bfb\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523076.4%\uff0c\u4ec5\u4f7f\u7528\u6bcf\u4e2a\u4efb\u52a15k\u8bad\u7ec3\u6837\u672c\u548c2\u4e2aepoch\uff1b\u76f8\u6bd4LoRA\u63d0\u53472.2\u4e2a\u767e\u5206\u70b9\uff0c\u76f8\u6bd4DoRA\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\uff1b\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u4e5f\u6709\u63d0\u5347\uff0c\u8bad\u7ec3\u6ce2\u52a8\u964d\u4f4e36%\u3002", "conclusion": "D2-LoRA\u5728\u6709\u9650\u6570\u636e\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u800c\u975e\u5355\u7eaf\u589e\u52a0\u53c2\u6570\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u65f6\u7684\u4ee3\u6570\u53ef\u5408\u5e76\u6027\u548c\u6570\u503c\u7b49\u4ef7\u6027\u3002"}}
{"id": "2602.14759", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14759", "abs": "https://arxiv.org/abs/2602.14759", "authors": ["Jonathan Lys", "Vincent Gripon", "Bastien Pasdeloup", "Lukas Mauch", "Fabien Cardinaux", "Ghouthi Boukli Hacene"], "title": "Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training", "comment": null, "summary": "Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.", "AI": {"tldr": "\u63d0\u51fa\"\u63a8\u7406\u65f6\u5185\u90e8\u5faa\u73af\"\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u590d\u5e94\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9009\u5b9a\u5757\u8303\u56f4\u6765\u5ef6\u957f\u7ec6\u5316\u8fc7\u7a0b\uff0c\u4ece\u800c\u5728\u51bb\u7ed3\u6a21\u578b\u4e2d\u83b7\u5f97\u989d\u5916\u8ba1\u7b97\u548c\u6027\u80fd\u63d0\u5347", "motivation": "\u57fa\u4e8eTransformer\u67b6\u6784\u4e2d\u6b8b\u5dee\u8fde\u63a5\u5c06\u5185\u90e8\u8868\u793a\u89c6\u4e3a\u8fed\u4ee3\u7ec6\u5316\u7684\u89c2\u70b9\uff0c\u4ee5\u53ca\u65e9\u671f\u89e3\u7801\u548c\u7ec6\u5316\u5c42\u5047\u8bbe\uff0c\u63a2\u7d22\u901a\u8fc7\u5ef6\u957f\u7ec6\u5316\u8fc7\u7a0b\u6765\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd", "method": "\u5728\u63a8\u7406\u9636\u6bb5\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9009\u5b9a\u5757\u8303\u56f4\u8fdb\u884c\u91cd\u590d\u5e94\u7528\uff0c\u5f62\u6210\u5185\u90e8\u5faa\u73af\uff0c\u5ef6\u957f\u7ec6\u5316\u8fc7\u7a0b\u800c\u4e0d\u6539\u53d8\u6a21\u578b\u53c2\u6570", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5185\u90e8\u5faa\u73af\u65b9\u6cd5\u5e26\u6765\u4e86\u9002\u5ea6\u4f46\u4e00\u81f4\u7684\u51c6\u786e\u6027\u63d0\u5347\uff0c\u6f5c\u5728\u8f68\u8ff9\u5206\u6790\u663e\u793a\u66f4\u7a33\u5b9a\u7684\u72b6\u6001\u6f14\u5316\u548c\u6301\u7eed\u7684\u8bed\u4e49\u7ec6\u5316", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7684\u6d4b\u8bd5\u65f6\u5faa\u73af\u53ef\u4ee5\u5728\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u83b7\u5f97\u989d\u5916\u7684\u7ec6\u5316\u6548\u679c\uff0c\u6269\u5c55\u8ba1\u7b97\u800c\u4e0d\u6539\u53d8\u6a21\u578b\u53c2\u6570"}}
{"id": "2602.14761", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.14761", "abs": "https://arxiv.org/abs/2602.14761", "authors": ["Stefano Woerner", "Seong Joon Oh", "Christian F. Baumgartner"], "title": "Universal Algorithm-Implicit Learning", "comment": null, "summary": "Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like \"universal\" and \"general-purpose\" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u5b9a\u4e49\u5143\u5b66\u4e60\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86TAIL\u2014\u2014\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7b97\u6cd5\u9690\u5f0f\u5143\u5b66\u4e60\u5668\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u9886\u57df\u3001\u6a21\u6001\u548c\u6807\u7b7e\u914d\u7f6e\u7684\u4efb\u52a1\uff0c\u5728\u5c11\u6837\u672c\u5b66\u4e60\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5143\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u72ed\u7a84\u7684\u4efb\u52a1\u5206\u5e03\u548c\u56fa\u5b9a\u7684\u7279\u5f81/\u6807\u7b7e\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u540c\u65f6\uff0c\u5143\u5b66\u4e60\u6587\u732e\u4e2d\"\u901a\u7528\"\u548c\"\u901a\u7528\u76ee\u7684\"\u7b49\u672f\u8bed\u4f7f\u7528\u4e0d\u4e00\u81f4\u4e14\u7f3a\u4e4f\u7cbe\u786e\u5b9a\u4e49\uff0c\u963b\u788d\u4e86\u65b9\u6cd5\u95f4\u7684\u53ef\u6bd4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u5b9a\u4e49\u5b9e\u7528\u901a\u7528\u6027\uff0c\u533a\u5206\u7b97\u6cd5\u663e\u5f0f\u548c\u7b97\u6cd5\u9690\u5f0f\u5b66\u4e60\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u5f00\u53d1\u4e86TAIL\uff1a\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7b97\u6cd5\u9690\u5f0f\u5143\u5b66\u4e60\u5668\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a1) \u8de8\u6a21\u6001\u7279\u5f81\u7f16\u7801\u7684\u968f\u673a\u6295\u5f71\uff1b2) \u53ef\u6269\u5c55\u5230\u66f4\u5927\u6807\u7b7e\u7a7a\u95f4\u7684\u968f\u673a\u6ce8\u5165\u6807\u7b7e\u5d4c\u5165\uff1b3) \u9ad8\u6548\u7684\u5728\u7ebf\u67e5\u8be2\u5904\u7406\u3002", "result": "TAIL\u5728\u6807\u51c6\u5c11\u6837\u672c\u5b66\u4e60\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u9886\u57df\u548c\u6a21\u6001\uff08\u4f8b\u5982\u4ec5\u7528\u56fe\u50cf\u8bad\u7ec3\u5374\u80fd\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff09\uff0c\u5904\u7406\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u591a\u8fbe20\u500d\u7c7b\u522b\u6570\u7684\u4efb\u52a1\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u6bd4\u5148\u524d\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u9ad8\u51fa\u6570\u91cf\u7ea7\u3002", "conclusion": "\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u4e3a\u901a\u7528\u5143\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8bcd\u6c47\u548c\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u800cTAIL\u5c55\u793a\u4e86\u7b97\u6cd5\u9690\u5f0f\u5143\u5b66\u4e60\u5728\u5904\u7406\u591a\u6837\u5316\u4efb\u52a1\u5206\u5e03\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4e3a\u66f4\u901a\u7528\u7684\u5143\u5b66\u4e60\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.14789", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14789", "abs": "https://arxiv.org/abs/2602.14789", "authors": ["Rotem Mulayoff", "Sebastian U. Stich"], "title": "On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials", "comment": "Preprint", "summary": "The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f18\u5316\u7b97\u6cd5\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u7684\u52a8\u6001\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u5173\u6ce8\u975e\u7ebf\u6027\u9879\u5bf9\u68af\u5ea6\u4e0b\u964d\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u7ebf\u6027\u5316\u5206\u6790\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u8bba\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u7684\u52a8\u6001\u7a33\u5b9a\u6027\u5982\u4f55\u51b3\u5b9a\u4f18\u5316\u7b97\u6cd5\u627e\u5230\u7684\u6700\u5c0f\u503c\u3002\u867d\u7136\u5148\u524d\u5de5\u4f5c\u5e38\u4f9d\u8d56\u7ebf\u6027\u5316\u6765\u786e\u5b9a\u7a33\u5b9a\u6027\uff0c\u4f46\u7ebf\u6027\u5316\u52a8\u6001\u662f\u5426\u771f\u5b9e\u6355\u6349\u5b8c\u6574\u975e\u7ebf\u6027\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\u3002\u6700\u8fd1\u7814\u7a76\u8868\u660e\u68af\u5ea6\u4e0b\u964d\u53ef\u80fd\u5728\u7ebf\u6027\u4e0d\u7a33\u5b9a\u6700\u5c0f\u503c\u9644\u8fd1\u7a33\u5b9a\u632f\u8361\uff0c\u8fd9\u8868\u660e\u7ebf\u6027\u5206\u6790\u53ef\u80fd\u5177\u6709\u8bef\u5bfc\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u63a8\u5bfc\u591a\u5143\u8bbe\u7f6e\u4e2d\u68af\u5ea6\u4e0b\u964d\u5728\u6700\u5c0f\u503c\u9644\u8fd1\u7a33\u5b9a\u632f\u8361\u7684\u7cbe\u786e\u51c6\u5219\uff0c\u8be5\u6761\u4ef6\u4f9d\u8d56\u4e8e\u9ad8\u9636\u5bfc\u6570\uff0c\u63a8\u5e7f\u4e86\u73b0\u6709\u7ed3\u679c\uff1b2\uff09\u5c06\u5206\u6790\u6269\u5c55\u5230\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff0c\u7814\u7a76\u975e\u7ebf\u6027\u52a8\u6001\u5982\u4f55\u5728\u671f\u671b\u4e2d\u53d1\u6563\uff1b3\uff09\u8bc1\u660e\u5982\u679c\u6240\u6709\u6279\u6b21\u90fd\u7ebf\u6027\u7a33\u5b9a\uff0c\u5219SGD\u7684\u975e\u7ebf\u6027\u52a8\u6001\u5728\u671f\u671b\u4e2d\u662f\u7a33\u5b9a\u7684\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1\uff09\u5efa\u7acb\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u6700\u5c0f\u503c\u9644\u8fd1\u7a33\u5b9a\u632f\u8361\u7684\u7cbe\u786e\u975e\u7ebf\u6027\u51c6\u5219\uff1b2\uff09\u53d1\u73b0SGD\u7684\u975e\u7ebf\u6027\u52a8\u6001\u53ef\u80fd\u5728\u671f\u671b\u4e2d\u53d1\u6563\uff0c\u5373\u4f7f\u5355\u4e2a\u6279\u6b21\u4e0d\u7a33\u5b9a\uff1b3\uff09\u8bc1\u660e\u7a33\u5b9a\u6027\u53ef\u80fd\u7531\u5355\u4e2a\u4e0d\u7a33\u5b9a\u632f\u8361\u7684\u6279\u6b21\u51b3\u5b9a\uff0c\u800c\u4e0d\u662f\u7ebf\u6027\u5206\u6790\u6240\u5efa\u8bae\u7684\u5e73\u5747\u6548\u5e94\uff1b4\uff09\u8bc1\u660e\u4e86\u5982\u679c\u6240\u6709\u6279\u6b21\u90fd\u7ebf\u6027\u7a33\u5b9a\uff0cSGD\u7684\u975e\u7ebf\u6027\u52a8\u6001\u5728\u671f\u671b\u4e2d\u662f\u7a33\u5b9a\u7684\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u975e\u7ebf\u6027\u9879\u5728\u786e\u5b9a\u4f18\u5316\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u7ebf\u6027\u5316\u5206\u6790\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u89e3\u68af\u5ea6\u4e0b\u964d\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u975e\u7ebf\u6027\u52a8\u6001\u7a33\u5b9a\u6027\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5355\u4e2a\u4e0d\u7a33\u5b9a\u6279\u6b21\u53ef\u80fd\u4e3b\u5bfcSGD\u6574\u4f53\u7a33\u5b9a\u6027\u7684\u673a\u5236\u3002"}}
{"id": "2602.14814", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14814", "abs": "https://arxiv.org/abs/2602.14814", "authors": ["Julien Siems", "Riccardo Grazzi", "Kirill Kalinin", "Hitesh Ballani", "Babak Rahmani"], "title": "Learning State-Tracking from Code Using Linear RNNs", "comment": null, "summary": "Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.", "AI": {"tldr": "\u8bba\u6587\u5c06\u6392\u5217\u7ec4\u5408\u4efb\u52a1\u8f6c\u5316\u4e3a\u4ee3\u7801REPL\u8ddf\u8e2a\u5f62\u5f0f\uff0c\u53d1\u73b0\u7ebf\u6027RNN\u80fd\u6709\u6548\u5904\u7406\u72b6\u6001\u8ddf\u8e2a\uff0c\u800cTransformer\u4ecd\u5931\u8d25\uff0c\u5e76\u63a2\u8ba8\u4e86\u4ee3\u7801\u4e2d\u72b6\u6001\u8ddf\u8e2a\u56f0\u96be\u7684\u539f\u56e0\u3002", "motivation": "\u73b0\u6709\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\uff08\u7279\u522b\u662f\u6392\u5217\u7ec4\u5408\uff09\u901a\u5e38\u91c7\u7528\u5e8f\u5217\u5230\u5e8f\u5217\u5f62\u5f0f\uff0c\u4e0e\u8bed\u8a00\u6a21\u578b\u5e38\u7528\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bbe\u7f6e\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7REPL\u8ddf\u8e2a\u5c06\u6392\u5217\u7ec4\u5408\u8f6c\u5316\u4e3a\u4ee3\u7801\u5f62\u5f0f\uff0c\u4ea4\u7ec7\u72b6\u6001\u663e\u793a\u548c\u53d8\u91cf\u8f6c\u6362\uff0c\u5e76\u5efa\u7acb\u6982\u7387\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u6846\u67b6\u6765\u5206\u6790\u72b6\u6001\u8ddf\u8e2a\u96be\u5ea6\u3002", "result": "\u7ebf\u6027RNN\u5728\u4ee3\u7801\u8bbe\u7f6e\u4e0b\u4ecd\u80fd\u6709\u6548\u8fdb\u884c\u72b6\u6001\u8ddf\u8e2a\uff0c\u800cTransformer\u5931\u8d25\uff1b\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u52a8\u4f5c\u573a\u666f\u4e2d\uff0c\u7ebf\u6027RNN\u53ef\u80fd\u6bd4\u975e\u7ebf\u6027RNN\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u4ee3\u7801\u4e2d\u7684\u72b6\u6001\u8ddf\u8e2a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u52a8\u4f5c\u5e76\u975e\u603b\u662f\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\uff1b\u4e0d\u540c\u67b6\u6784\u5728\u5904\u7406\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u65f6\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002"}}
{"id": "2602.14844", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14844", "abs": "https://arxiv.org/abs/2602.14844", "authors": ["Elias Malomgr\u00e9", "Pieter Simoens"], "title": "Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment", "comment": "Accepted for the AAMAS 2026 Blue Sky Ideas track", "summary": "AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.", "AI": {"tldr": "\u63d0\u51faInteractionless Inverse Reinforcement Learning\u65b9\u6cd5\uff0c\u5c06\u5bf9\u9f50\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\u89e3\u8026\uff0c\u521b\u5efa\u53ef\u68c0\u67e5\u3001\u53ef\u7f16\u8f91\u3001\u6a21\u578b\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7Alignment Flywheel\u5faa\u73af\u8fed\u4ee3\u5f3a\u5316\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u7f3a\u9677\uff0c\u5c06\u5b89\u5168\u76ee\u6807\u4e0e\u667a\u80fd\u4f53\u7b56\u7565\u7ea0\u7f20\u5728\u4e00\u8d77\u3002RLHF\u548cDPO\u7b49\u65b9\u6cd5\u4ea7\u751f\u4e0d\u900f\u660e\u3001\u5355\u6b21\u4f7f\u7528\u7684\u5bf9\u9f50\u4ea7\u7269\uff08\u79f0\u4e3aAlignment Waste\uff09\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u7684\u5bf9\u9f50\u67b6\u6784\u3002", "method": "\u63d0\u51faInteractionless Inverse Reinforcement Learning\uff08\u65e0\u4ea4\u4e92\u9006\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5c06\u5bf9\u9f50\u4ea7\u7269\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\u89e3\u8026\uff0c\u751f\u6210\u53ef\u68c0\u67e5\u3001\u53ef\u7f16\u8f91\u3001\u6a21\u578b\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\u3002\u540c\u65f6\u5f15\u5165Alignment Flywheel\uff08\u5bf9\u9f50\u98de\u8f6e\uff09\u5faa\u73af\uff0c\u901a\u8fc7\u4eba\u5de5\u53c2\u4e0e\u3001\u81ea\u52a8\u5316\u5ba1\u8ba1\u548c\u7cbe\u70bc\u8fed\u4ee3\u5f3a\u5316\u5956\u52b1\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u5b89\u5168\u4ece\u4e00\u6b21\u6027\u6d88\u8017\u8f6c\u53d8\u4e3a\u6301\u4e45\u3001\u53ef\u9a8c\u8bc1\u7684\u5de5\u7a0b\u8d44\u4ea7\uff0c\u521b\u5efa\u4e86\u53ef\u68c0\u67e5\u3001\u53ef\u7f16\u8f91\u7684\u5bf9\u9f50\u4ea7\u7269\uff0c\u652f\u6301\u6a21\u578b\u65e0\u5173\u7684\u90e8\u7f72\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u89e3\u51b3\u4e86\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u7684\u7ed3\u6784\u7f3a\u9677\uff0c\u901a\u8fc7\u89e3\u8026\u5bf9\u9f50\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\uff0c\u4ee5\u53ca\u5efa\u7acb\u8fed\u4ee3\u5f3a\u5316\u673a\u5236\uff0c\u4f7fAI\u5b89\u5168\u6210\u4e3a\u53ef\u6301\u7eed\u3001\u53ef\u9a8c\u8bc1\u7684\u5de5\u7a0b\u5b9e\u8df5\u3002"}}
{"id": "2602.14855", "categories": ["cs.LG", "cs.SI", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.14855", "abs": "https://arxiv.org/abs/2602.14855", "authors": ["Ryan DeWolfe", "Pawe\u0142 Pra\u0142at", "Fran\u00e7ois Th\u00e9berge"], "title": "A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers", "comment": "14 pages, 3 figures", "summary": "Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6bd4\u8f83\u5305\u542b\u91cd\u53e0\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u7684\u805a\u7c7b\u7ed3\u679c\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u805a\u7c7b\u6bd4\u8f83\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5305\u542b\u5f02\u5e38\u503c\uff08\u4e0d\u5c5e\u4e8e\u4efb\u4f55\u7c07\u7684\u5bf9\u8c61\uff09\u548c\u91cd\u53e0\u805a\u7c7b\uff08\u5bf9\u8c61\u53ef\u80fd\u5c5e\u4e8e\u591a\u4e2a\u7c07\uff09\u7684\u60c5\u51b5\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\u4f46\u7f3a\u4e4f\u76f8\u5e94\u7684\u8bc4\u4f30\u5de5\u5177", "method": "\u5b9a\u4e49\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u6bd4\u8f83\u5305\u542b\u91cd\u53e0\u548c\u5f02\u5e38\u503c\u7684\u805a\u7c7b\u7ed3\u679c\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u591a\u4e2a\u7406\u60f3\u7279\u6027", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u53d7\u5176\u4ed6\u805a\u7c7b\u6bd4\u8f83\u5ea6\u91cf\u5e38\u89c1\u7684\u591a\u79cd\u504f\u5dee\u5f71\u54cd", "conclusion": "\u63d0\u51fa\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u586b\u8865\u4e86\u805a\u7c7b\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u5305\u542b\u5f02\u5e38\u503c\u548c\u91cd\u53e0\u805a\u7c7b\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6bd4\u8f83\u5de5\u5177"}}
{"id": "2602.14868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14868", "abs": "https://arxiv.org/abs/2602.14868", "authors": ["Ilia Mahrooghi", "Aryo Lotfi", "Emmanuel Abbe"], "title": "Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning", "comment": "21 pages, 12 figures", "summary": "Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.", "AI": {"tldr": "Goldilocks\u662f\u4e00\u79cd\u6559\u5e08\u9a71\u52a8\u7684\u6570\u636e\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u6bcf\u4e2a\u95ee\u9898\u5bf9\u5b66\u751f\u6a21\u578b\u7684\u96be\u5ea6\uff0c\u9009\u62e9\"\u521a\u521a\u597d\"\u96be\u5ea6\u7684\u95ee\u9898\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u89e3\u9501\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f9d\u8d56\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002\u4f20\u7edf\u7684\u8bfe\u7a0b\u5b66\u4e60\u901a\u8fc7\u6309\u590d\u6742\u5ea6\u6392\u5e8f\u6570\u636e\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u6700\u4f73\u6392\u5e8f\u5f80\u5f80\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faGoldilocks\u7b56\u7565\uff1a\u6559\u5e08\u6a21\u578b\u9884\u6d4b\u6bcf\u4e2a\u95ee\u9898\u5bf9\u5b66\u751f\u6a21\u578b\u7684\u96be\u5ea6\uff0c\u9009\u62e9\u65e2\u4e0d\u592a\u7b80\u5355\u4e5f\u4e0d\u592a\u56f0\u96be\u7684\u95ee\u9898\uff08Goldilocks\u539f\u5219\uff09\uff0c\u540c\u65f6\u7528GRPO\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u3002\u6559\u5e08\u6a21\u578b\u6839\u636e\u5b66\u751f\u5728\u5df2\u89c1\u6837\u672c\u4e0a\u7684\u8868\u73b0\u6301\u7eed\u9002\u5e94\u5176\u80fd\u529b\u53d8\u5316\u3002", "result": "\u5728OpenMathReasoning\u6570\u636e\u96c6\u4e0a\uff0cGoldilocks\u6570\u636e\u91c7\u6837\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u6bd4\u6807\u51c6GRPO\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Goldilocks\u7b56\u7565\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u5b66\u751f\u6a21\u578b\u80fd\u529b\u7684\u6559\u5e08\u9a71\u52a8\u6570\u636e\u91c7\u6837\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2602.14872", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14872", "abs": "https://arxiv.org/abs/2602.14872", "authors": ["Yu Huang", "Zixin Wen", "Yuejie Chi", "Yuting Wei", "Aarti Singh", "Yingbin Liang", "Yuxin Chen"], "title": "On the Learning Dynamics of RLVR at the Edge of Competence", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.", "AI": {"tldr": "RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u901a\u8fc7\u5e73\u6ed1\u96be\u5ea6\u8c31\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\uff0c\u800c\u7a81\u53d8\u7684\u96be\u5ea6\u8c31\u4f1a\u5bfc\u81f4\u5b66\u4e60\u505c\u6ede\u548c\u9636\u6bb5\u6027\u7a81\u7834\u3002", "motivation": "\u7406\u89e3\u4ec5\u57fa\u4e8e\u6700\u7ec8\u7ed3\u679c\u7684\u5956\u52b1\u5982\u4f55\u5e2e\u52a9\u514b\u670d\u957f\u89c6\u91ce\u63a8\u7406\u969c\u788d\uff0c\u63ed\u793aRLVR\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u6210\u529f\u7684\u5185\u5728\u673a\u5236\u3002", "method": "\u5f00\u53d1\u4e86Transformer\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e0a\u7684RL\u8bad\u7ec3\u52a8\u6001\u7406\u8bba\uff0c\u4f7f\u7528\u6709\u9650\u7fa4\u4e0a\u7684\u5085\u91cc\u53f6\u5206\u6790\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u9884\u6d4b\u673a\u5236\u3002", "result": "RLVR\u7684\u6709\u6548\u6027\u53d7\u96be\u5ea6\u8c31\u5e73\u6ed1\u6027\u652f\u914d\uff1a\u5e73\u6ed1\u96be\u5ea6\u8c31\u4ea7\u751f\u63a5\u529b\u6548\u5e94\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6301\u7eed\u6539\u8fdb\uff1b\u7a81\u53d8\u96be\u5ea6\u8c31\u5bfc\u81f4grokking\u578b\u76f8\u53d8\uff0c\u4ea7\u751f\u5b66\u4e60\u505c\u6ede\u3002", "conclusion": "RLVR\u901a\u8fc7\u63d0\u5347\u6a21\u578b\u5728\u80fd\u529b\u8fb9\u7f18\u7684\u6027\u80fd\u53d1\u6325\u4f5c\u7528\uff0c\u9002\u5f53\u8bbe\u8ba1\u7684\u6570\u636e\u6df7\u5408\u53ef\u4ee5\u4ea7\u751f\u53ef\u6269\u5c55\u7684\u6536\u76ca\uff0c\u5e73\u6ed1\u96be\u5ea6\u8c31\u662f\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u7684\u5173\u952e\u3002"}}
{"id": "2602.14913", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.14913", "abs": "https://arxiv.org/abs/2602.14913", "authors": ["Farbod Siahkali", "Ashwin Verma", "Vijay Gupta"], "title": "Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift", "comment": "Under review. 6 pages, 2 figures, 1 table", "summary": "Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u4f2a\u6821\u51c6\u65b9\u6cd5\u6765\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u4e0b\u4fdd\u5f62\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u5de5\u5177\u63a8\u5bfc\u76ee\u6807\u8986\u76d6\u7387\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u6e90\u8c03\u8c10\u4f2a\u6821\u51c6\u7b97\u6cd5\u6765\u7f13\u89e3\u8986\u76d6\u9000\u5316\u3002", "motivation": "\u4fdd\u5f62\u9884\u6d4b\u5728\u6570\u636e\u5206\u5e03\u504f\u79fb\u65f6\u53ef\u80fd\u5931\u6548\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u65b9\u6cd5\u6765\u7ef4\u6301\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4f2a\u6821\u51c6\u4f5c\u4e3a\u5de5\u5177\uff0c\u57fa\u4e8e\u6709\u754c\u6807\u7b7e\u6761\u4ef6\u534f\u53d8\u91cf\u504f\u79fb\u6a21\u578b\uff0c\u5229\u7528\u9886\u57df\u81ea\u9002\u5e94\u5de5\u5177\u63a8\u5bfc\u76ee\u6807\u8986\u76d6\u7387\u4e0b\u754c\uff0c\u8bbe\u8ba1\u901a\u8fc7\u677e\u5f1b\u53c2\u6570\u81a8\u80c0\u4fdd\u5f62\u9608\u503c\u7684\u4f2a\u6821\u51c6\u96c6\uff0c\u5e76\u63d0\u51fa\u6e90\u8c03\u8c10\u4f2a\u6821\u51c6\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u51fa\u76ee\u6807\u8986\u76d6\u7387\u4e0b\u754c\u4e0e\u5206\u7c7b\u5668\u6e90\u57df\u635f\u5931\u548cWasserstein\u504f\u79fb\u5ea6\u91cf\u76f8\u5173\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u8fb9\u754c\u80fd\u5b9a\u6027\u8ddf\u8e2a\u4f2a\u6821\u51c6\u884c\u4e3a\uff0c\u6e90\u8c03\u8c10\u65b9\u6848\u80fd\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8986\u76d6\u9000\u5316\u540c\u65f6\u4fdd\u6301\u975e\u5e73\u51e1\u9884\u6d4b\u96c6\u5927\u5c0f\u3002", "conclusion": "\u4f2a\u6821\u51c6\u662f\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u4e0b\u4fdd\u5f62\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\u7684\u6709\u6548\u5de5\u5177\uff0c\u6e90\u8c03\u8c10\u4f2a\u6821\u51c6\u7b97\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u80fd\u5e73\u8861\u8986\u76d6\u7387\u548c\u9884\u6d4b\u96c6\u5927\u5c0f\u3002"}}
{"id": "2602.14914", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14914", "abs": "https://arxiv.org/abs/2602.14914", "authors": ["Olivier Jeunen", "Shashank Gupta"], "title": "Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation", "comment": null, "summary": "Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $\u03b2^\\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u5728\u79bb\u7b56\u7565\u8bc4\u4f30\u4e2d\uff0c\u4f7f\u7528\u6700\u4f18\u52a0\u6027\u57fa\u7ebf\uff08\u03b2*-IPS\uff09\u7684\u4f30\u8ba1\u5668\u5728\u5747\u65b9\u8bef\u5dee\u4e0a\u6e10\u8fd1\u4f18\u4e8e\u81ea\u5f52\u4e00\u5316\u9006\u503e\u5411\u8bc4\u5206\uff08SNIPS\uff09\uff0c\u4e3a\u63a8\u8350\u548c\u6392\u5e8f\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "motivation": "\u79bb\u7b56\u7565\u8bc4\u4f30\u5bf9\u4e8e\u8bc4\u4f30\u63a8\u8350\u548c\u6392\u5e8f\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e2d\u81ea\u5f52\u4e00\u5316\u9006\u503e\u5411\u8bc4\u5206\uff08SNIPS\uff09\u867d\u7136\u901a\u8fc7\u4e58\u6027\u63a7\u5236\u53d8\u91cf\u51cf\u5c11\u65b9\u5dee\uff0c\u800c\u52a0\u6027\u63a7\u5236\u53d8\u91cf\uff08\u57fa\u7ebf\u4fee\u6b63\uff09\u5728\u79bb\u7b56\u7565\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u03b2*-IPS\u4f30\u8ba1\u5668\uff0c\u4f7f\u7528\u6700\u4f18\u52a0\u6027\u57fa\u7ebf\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u5728\u5747\u65b9\u8bef\u5dee\u4e0a\u6e10\u8fd1\u4f18\u4e8eSNIPS\uff0c\u5e76\u89e3\u6790\u5206\u89e3\u65b9\u5dee\u5dee\u8ddd\uff0c\u5c55\u793aSNIPS\u7b49\u4ef7\u4e8e\u4f7f\u7528\u7279\u5b9a\u4f46\u901a\u5e38\u6b21\u4f18\u7684\u52a0\u6027\u57fa\u7ebf\u3002", "result": "\u8bc1\u660e\u03b2*-IPS\u5728\u5747\u65b9\u8bef\u5dee\u4e0a\u6e10\u8fd1\u4e3b\u5bfcSNIPS\uff0c\u4e3a\u4ece\u81ea\u5f52\u4e00\u5316\u8f6c\u5411\u6700\u4f18\u57fa\u7ebf\u4fee\u6b63\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u9002\u7528\u4e8e\u63a8\u8350\u548c\u6392\u5e8f\u7cfb\u7edf\u7684\u79bb\u7b56\u7565\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u63a8\u8350\u548c\u6392\u5e8f\u7cfb\u7edf\u7684\u79bb\u7b56\u7565\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u8868\u660e\u5e94\u8be5\u4ece\u81ea\u5f52\u4e00\u5316\u65b9\u6cd5\u8f6c\u5411\u6700\u4f18\u52a0\u6027\u57fa\u7ebf\u4fee\u6b63\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2602.14938", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.14938", "abs": "https://arxiv.org/abs/2602.14938", "authors": ["Martin Van Waerebeke", "Marco Lorenzi", "Kevin Scaman", "El Mahdi El Mhamdi", "Giovanni Neglia"], "title": "Variance-Reduced $(\\varepsilon,\u03b4)-$Unlearning using Forget Set Gradients", "comment": null, "summary": "In machine unlearning, $(\\varepsilon,\u03b4)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\\varepsilon,\u03b4)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\\varepsilon,\u03b4)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\\varepsilon,\u03b4)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VRU\u7b97\u6cd5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5728\u66f4\u65b0\u89c4\u5219\u4e2d\u76f4\u63a5\u5305\u542b\u9057\u5fd8\u96c6\u68af\u5ea6\uff0c\u540c\u65f6\u53ef\u8bc1\u660e\u6ee1\u8db3(\u03b5,\u03b4)-\u9057\u5fd8\u4fdd\u8bc1\u7684\u4e00\u9636\u7b97\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u66f4\u597d\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u73b0\u6709(\u03b5,\u03b4)-\u9057\u5fd8\u65b9\u6cd5\u4ec5\u4f7f\u7528\u9057\u5fd8\u96c6\u6765\u6821\u51c6\u6ce8\u5165\u7684\u566a\u58f0\uff0c\u4ece\u672a\u5c06\u5176\u4f5c\u4e3a\u76f4\u63a5\u4f18\u5316\u4fe1\u53f7\uff1b\u800c\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u867d\u7136\u5229\u7528\u9057\u5fd8\u6837\u672c\uff08\u5982\u68af\u5ea6\u4e0a\u5347\uff09\uff0c\u4f46\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9057\u5fd8\u4fdd\u8bc1\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u65b9\u5dee\u51cf\u5c11\u9057\u5fd8\uff08VRU\uff09\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u4e00\u9636\u7b97\u6cd5\uff0c\u5728\u66f4\u65b0\u89c4\u5219\u4e2d\u76f4\u63a5\u5305\u542b\u9057\u5fd8\u96c6\u68af\u5ea6\uff0c\u540c\u65f6\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u6ee1\u8db3(\u03b5,\u03b4)-\u9057\u5fd8\u4fdd\u8bc1\u3002", "result": "VRU\u7b97\u6cd5\u6536\u655b\u6027\u5f97\u5230\u8bc1\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u4e00\u9636(\u03b5,\u03b4)-\u9057\u5fd8\u65b9\u6cd5\u6709\u66f4\u4f18\u7684\u6536\u655b\u901f\u7387\uff08\u5bf9\u8bef\u5dee\u7684\u4f9d\u8d56\u66f4\u597d\uff09\u3002\u5728\u4f4e\u8bef\u5dee\u72b6\u6001\u4e0b\uff0cVRU\u6e10\u8fd1\u5730\u4f18\u4e8e\u4efb\u4f55\u5ffd\u7565\u9057\u5fd8\u96c6\u7684\u4e00\u9636\u65b9\u6cd5\u3002", "conclusion": "VRU\u7b97\u6cd5\u6210\u529f\u5730\u5c06\u9057\u5fd8\u96c6\u68af\u5ea6\u76f4\u63a5\u7eb3\u5165\u4f18\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u5f62\u5f0f\u5316\u9057\u5fd8\u4fdd\u8bc1\uff0c\u586b\u8865\u4e86\u7406\u8bba\u4fdd\u8bc1\u65b9\u6cd5\u4e0e\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.14952", "categories": ["cs.LG", "math.OC", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14952", "abs": "https://arxiv.org/abs/2602.14952", "authors": ["Jivat Neet Kaur", "Isaac Gibbs", "Michael I. Jordan"], "title": "Locally Adaptive Multi-Objective Learning", "comment": "Code is available at https://github.com/jivatneet/adaptive-multiobjective", "summary": "We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u591a\u76ee\u6807\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7b97\u6cd5\u5b9e\u73b0\u5c40\u90e8\u9002\u5e94\u6027\uff0c\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\u4fdd\u6301\u7a33\u5065\u6027", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u5e03\u968f\u65f6\u95f4\u4efb\u610f\u53d8\u5316\u65f6\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5206\u5e03\u6f02\u79fb\uff0c\u9700\u8981\u6539\u8fdb\u5c40\u90e8\u9002\u5e94\u80fd\u529b", "method": "\u5c06\u591a\u76ee\u6807\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7684\u4e00\u90e8\u5206\u66ff\u6362\u4e3a\u81ea\u9002\u5e94\u5728\u7ebf\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5c40\u90e8\u9002\u5e94\u6027\uff0c\u5728\u8fde\u7eed\u5b50\u533a\u95f4\u4e0a\u63d0\u4f9b\u4fdd\u969c", "result": "\u5728\u80fd\u6e90\u9884\u6d4b\u548c\u7b97\u6cd5\u516c\u5e73\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u5728\u5b50\u7ec4\u95f4\u5b9e\u73b0\u65e0\u504f\u9884\u6d4b\uff0c\u5e76\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\u4fdd\u6301\u7a33\u5065", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u591a\u76ee\u6807\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u5206\u5e03\u6f02\u79fb\uff0c\u63d0\u9ad8\u5c40\u90e8\u9002\u5e94\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u4f18\u52bf"}}
{"id": "2602.14972", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14972", "abs": "https://arxiv.org/abs/2602.14972", "authors": ["Arik Reuter", "Anish Dhir", "Cristiana Diaconu", "Jake Robertson", "Ole Ossen", "Frank Hutter", "Adrian Weller", "Mark van der Wilk", "Bernhard Sch\u00f6lkopf"], "title": "Use What You Know: Causal Foundation Models with Partial Graphs", "comment": null, "summary": "Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u56e0\u679c\u57fa\u7840\u6a21\u578b\u4e2d\u878d\u5165\u9886\u57df\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56e0\u679c\u4fe1\u606f\uff08\u5982\u56e0\u679c\u56fe\u6216\u7956\u5148\u4fe1\u606f\uff09\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u4f7f\u901a\u7528CFM\u80fd\u591f\u5339\u914d\u9488\u5bf9\u7279\u5b9a\u56e0\u679c\u7ed3\u6784\u8bad\u7ec3\u7684\u4e13\u7528\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u56e0\u679c\u57fa\u7840\u6a21\u578b\uff08CFMs\uff09\u867d\u7136\u7edf\u4e00\u4e86\u56e0\u679c\u53d1\u73b0\u548c\u63a8\u65ad\uff0c\u4f46\u65e0\u6cd5\u878d\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u4e0d\u7406\u60f3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u969c\u788d\uff0c\u4f7fCFM\u80fd\u591f\u5728\u6570\u636e\u9a71\u52a8\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u4efb\u4f55\u7a0b\u5ea6\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u5f15\u5165\u5c06\u56e0\u679c\u4fe1\u606f\uff08\u5305\u62ec\u5b8c\u6574\u56e0\u679c\u56fe\u3001\u90e8\u5206\u56e0\u679c\u4fe1\u606f\u6216\u66f4\u6613\u83b7\u5f97\u7684\u7956\u5148\u4fe1\u606f\uff09\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165CFM\u7684\u65b9\u6cd5\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u6761\u4ef6\u5316\u7b56\u7565\uff0c\u53d1\u73b0\u5c06\u53ef\u5b66\u4e60\u7684\u504f\u7f6e\u6ce8\u5165\u6ce8\u610f\u529b\u673a\u5236\u662f\u5229\u7528\u5b8c\u6574\u548c\u90e8\u5206\u56e0\u679c\u4fe1\u606f\u7684\u6700\u6709\u6548\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u8fd9\u79cd\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u901a\u7528CFM\u80fd\u591f\u5339\u914d\u9488\u5bf9\u7279\u5b9a\u56e0\u679c\u7ed3\u6784\u8bad\u7ec3\u7684\u4e13\u7528\u6a21\u578b\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u5b8c\u6574\u548c\u90e8\u5206\u56e0\u679c\u4fe1\u606f\uff0c\u4f7fCFM\u5728\u4fdd\u6301\u901a\u7528\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6761\u4ef6\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u56e0\u679c\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u4e2d\u7684\u6838\u5fc3\u969c\u788d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6570\u636e\u9a71\u52a8\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u9886\u57df\u77e5\u8bc6\uff0c\u4e3a\u5b9e\u73b0\"\u4e00\u4f53\u5316\"\u56e0\u679c\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.14977", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14977", "abs": "https://arxiv.org/abs/2602.14977", "authors": ["Alicja Maksymiuk", "Alexandre Duplessis", "Michael Bronstein", "Alexander Tong", "Fernanda Duarte", "\u0130smail \u0130lkan Ceylan"], "title": "MacroGuide: Topological Guidance for Macrocycle Generation", "comment": null, "summary": "Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.", "AI": {"tldr": "MacroGuide\u662f\u4e00\u79cd\u6269\u6563\u5f15\u5bfc\u673a\u5236\uff0c\u5229\u7528\u6301\u4e45\u540c\u8c03\u5b66\u6307\u5bfc\u9884\u8bad\u7ec3\u5206\u5b50\u751f\u6210\u6a21\u578b\u751f\u6210\u5927\u73af\u5316\u5408\u7269\uff0c\u5c06\u5927\u73af\u751f\u6210\u7387\u4ece1%\u63d0\u5347\u523099%", "motivation": "\u5927\u73af\u5316\u5408\u7269\u56e0\u5176\u5bf9\u56f0\u96be\u9776\u70b9\u5177\u6709\u589e\u5f3a\u7684\u9009\u62e9\u6027\u548c\u7ed3\u5408\u4eb2\u548c\u529b\u800c\u6210\u4e3a\u6709\u524d\u666f\u7684\u836f\u7269\u66ff\u4ee3\u54c1\uff0c\u4f46\u7531\u4e8e\u516c\u5171\u6570\u636e\u96c6\u4e2d\u7a00\u7f3a\u4ee5\u53ca\u6807\u51c6\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u62d3\u6251\u7ea6\u675f\u96be\u4ee5\u5b9e\u65bd\uff0c\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "MacroGuide\u662f\u4e00\u79cd\u6269\u6563\u5f15\u5bfc\u673a\u5236\uff0c\u4f7f\u7528\u6301\u4e45\u540c\u8c03\u5b66\u6307\u5bfc\u9884\u8bad\u7ec3\u5206\u5b50\u751f\u6210\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\u3002\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\uff0c\u4ece\u539f\u5b50\u4f4d\u7f6e\u6784\u5efaVietoris-Rips\u590d\u5f62\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6301\u4e45\u540c\u8c03\u7279\u5f81\u4fc3\u8fdb\u73af\u5f62\u6210", "result": "\u5c06MacroGuide\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u540e\uff0c\u5927\u73af\u5316\u5408\u7269\u7684\u751f\u6210\u7387\u4ece1%\u63d0\u9ad8\u523099%\uff0c\u540c\u65f6\u5728\u5316\u5b66\u6709\u6548\u6027\u3001\u591a\u6837\u6027\u548cPoseBusters\u68c0\u67e5\u7b49\u5173\u952e\u8d28\u91cf\u6307\u6807\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "MacroGuide\u901a\u8fc7\u62d3\u6251\u5f15\u5bfc\u6210\u529f\u89e3\u51b3\u4e86\u5927\u73af\u5316\u5408\u7269\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u751f\u6210\u5177\u6709\u590d\u6742\u62d3\u6251\u7ed3\u6784\u7684\u5206\u5b50\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2602.14983", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14983", "abs": "https://arxiv.org/abs/2602.14983", "authors": ["Carolin Cissee", "Raneen Younis", "Zahra Ahmadi"], "title": "Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations", "comment": null, "summary": "Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \\textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.", "AI": {"tldr": "COrAL\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u7ea6\u675f\u548c\u4e0d\u5bf9\u79f0\u63a9\u7801\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u7559\u5197\u4f59\u3001\u72ec\u7279\u548c\u534f\u540c\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u5168\u9762\u7684\u8868\u5f81\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u5197\u4f59\u7684\u8de8\u6a21\u6001\u4fe1\u53f7\uff0c\u5f80\u5f80\u5ffd\u89c6\u6a21\u6001\u7279\u5b9a\uff08\u72ec\u7279\uff09\u4fe1\u606f\u548c\u4ea4\u4e92\u9a71\u52a8\uff08\u534f\u540c\uff09\u4fe1\u606f\uff0c\u5bfc\u81f4\u8868\u5f81\u4e0d\u5b8c\u6574\u548c\u6f5c\u5728\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\u914d\u5408\u6b63\u4ea4\u7ea6\u675f\u6765\u89e3\u8026\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff1b\u5f15\u5165\u5177\u6709\u4e92\u8865\u89c6\u56fe\u7279\u5b9a\u6a21\u5f0f\u7684\u4e0d\u5bf9\u79f0\u63a9\u7801\uff0c\u5f3a\u5236\u6a21\u578b\u63a8\u65ad\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\u800c\u975e\u4ec5\u4f9d\u8d56\u5197\u4f59\u7ebf\u7d22\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u591a\u6837\u5316MultiBench\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOrAL\u59cb\u7ec8\u5339\u914d\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u8f83\u4f4e\u7684\u8fd0\u884c\u95f4\u6027\u80fd\u65b9\u5dee\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u5b8c\u6574\u7684\u591a\u6a21\u6001\u4fe1\u606f\u8c31\u7cfb\u80fd\u591f\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u53ef\u9760\u548c\u5168\u9762\u7684\u5d4c\u5165\u8868\u793a\u3002"}}
{"id": "2602.14997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14997", "abs": "https://arxiv.org/abs/2602.14997", "authors": ["Tim Mangliers", "Bernhard M\u00f6ssner", "Benjamin Himpel"], "title": "Spectral Convolution on Orbifolds for Geometric Deep Learning", "comment": "17 pages, 5 figures", "summary": "Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u8c31\u5377\u79ef\u6982\u5ff5\u6269\u5c55\u5230\u8f68\u9053\u6d41\u5f62\uff0c\u4e3a\u5904\u7406\u8f68\u9053\u6d41\u5f62\u7ed3\u6784\u6570\u636e\u63d0\u4f9b\u57fa\u7840\u6784\u5efa\u6a21\u5757\uff0c\u5e76\u4ee5\u97f3\u4e50\u7406\u8bba\u4e3a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002", "motivation": "\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u4e3b\u8981\u5904\u7406\u56fe\u6216\u6d41\u5f62\u7b49\u975e\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u66f4\u591a\u62d3\u6251\u548c\u51e0\u4f55\u7ed3\u6784\u9700\u8981\u88ab\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6240\u5904\u7406\u3002\u8f68\u9053\u6d41\u5f62\u4f5c\u4e3a\u4e00\u79cd\u91cd\u8981\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u76ee\u524d\u7f3a\u4e4f\u76f8\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u8f68\u9053\u6d41\u5f62\u4e0a\u7684\u8c31\u5377\u79ef\u6982\u5ff5\uff0c\u5c06\u5176\u4f5c\u4e3a\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7840\u6784\u5efa\u6a21\u5757\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edfCNN\u4e2d\u7684\u5377\u79ef\u64cd\u4f5c\uff0c\u4f46\u9002\u7528\u4e8e\u8f68\u9053\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u8f68\u9053\u6d41\u5f62\u4e0a\u7684\u8c31\u5377\u79ef\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u5904\u7406\u8f68\u9053\u6d41\u5f62\u7ed3\u6784\u6570\u636e\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u97f3\u4e50\u7406\u8bba\u7684\u4f8b\u5b50\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8f68\u9053\u6d41\u5f62\u4e0a\u7684\u8c31\u5377\u79ef\u6269\u5c55\u4e86\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5f97\u80fd\u591f\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u62d3\u6251\u548c\u51e0\u4f55\u7ed3\u6784\u6570\u636e\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2602.15001", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15001", "abs": "https://arxiv.org/abs/2602.15001", "authors": ["Xander Davies", "Giorgi Giglemiani", "Edmund Lau", "Eric Winsor", "Geoffrey Irving", "Yarin Gal"], "title": "Boundary Point Jailbreaking of Black-Box LLMs", "comment": null, "summary": "Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as \"jailbreaks\". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (\"boundary points\"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.", "AI": {"tldr": "BPJ\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u52a8\u5316\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fb9\u754c\u70b9\u68c0\u6d4b\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u4ec5\u4f7f\u7528\u5355\u6bd4\u7279\u5206\u7c7b\u5668\u53cd\u9988\u5c31\u80fd\u7ed5\u8fc7\u6700\u5f3a\u884c\u4e1a\u90e8\u7f72\u7684\u5b89\u5168\u9632\u62a4\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u524d\u6cbfLLMs\u901a\u8fc7\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u9632\u62a4\u7cfb\u7edf\u62b5\u5fa1\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5df2\u7ecf\u8fc7\u6570\u5343\u5c0f\u65f6\u7684\u4eba\u5de5\u7ea2\u961f\u6d4b\u8bd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u767d\u76d2/\u7070\u76d2\u5047\u8bbe\uff08\u5982\u5206\u7c7b\u5668\u5206\u6570\u6216\u68af\u5ea6\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u73b0\u6709\u8d8a\u72f1\u5e93\u3002\u9700\u8981\u4e00\u79cd\u5b8c\u5168\u9ed1\u76d2\u3001\u4ec5\u4f7f\u7528\u6700\u5c11\u4fe1\u606f\u7684\u81ea\u52a8\u5316\u653b\u51fb\u65b9\u6cd5\u6765\u7a81\u7834\u6700\u5f3a\u5927\u7684\u73b0\u5b9e\u4e16\u754c\u9632\u5fa1\u3002", "method": "BPJ\u91c7\u7528\u8fb9\u754c\u70b9\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff1a1\uff09\u5c06\u76ee\u6807\u6709\u5bb3\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u4e2d\u95f4\u653b\u51fb\u76ee\u6807\u7684\u8bfe\u7a0b\uff1b2\uff09\u4e3b\u52a8\u9009\u62e9\u6700\u80fd\u68c0\u6d4b\u653b\u51fb\u5f3a\u5ea6\u5fae\u5c0f\u53d8\u5316\u7684\u8bc4\u4f30\u70b9\uff08\u8fb9\u754c\u70b9\uff09\uff1b3\uff09\u4ec5\u4f7f\u7528\u5206\u7c7b\u5668\u662f\u5426\u6807\u8bb0\u4ea4\u4e92\u7684\u5355\u6bd4\u7279\u4fe1\u606f\u8fdb\u884c\u5b8c\u5168\u9ed1\u76d2\u4f18\u5316\u3002", "result": "BPJ\u662f\u9996\u4e2a\u6210\u529f\u5f00\u53d1\u9488\u5bf9\u5baa\u6cd5\u5206\u7c7b\u5668\u901a\u7528\u8d8a\u72f1\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u653b\u51fb\u7b97\u6cd5\uff0c\u4e5f\u662f\u9996\u4e2a\u5728\u4e0d\u4f9d\u8d56\u4eba\u7c7b\u653b\u51fb\u79cd\u5b50\u60c5\u51b5\u4e0b\u6210\u529f\u653b\u51fbGPT-5\u8f93\u5165\u5206\u7c7b\u5668\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4f1a\u89e6\u53d1\u8bb8\u591a\u6807\u8bb0\uff0c\u4f46\u5728\u5355\u4e2a\u4ea4\u4e92\u4e2d\u96be\u4ee5\u9632\u5fa1\u3002", "conclusion": "BPJ\u5c55\u793a\u4e86\u4ec5\u4f7f\u7528\u6700\u5c11\u4fe1\u606f\u5c31\u80fd\u7ed5\u8fc7\u6700\u5f3a\u884c\u4e1a\u9632\u62a4\u7cfb\u7edf\u7684\u53ef\u80fd\u6027\u3002\u6709\u6548\u9632\u5fa1\u9700\u8981\u8865\u5145\u5355\u4ea4\u4e92\u65b9\u6cd5\uff0c\u589e\u52a0\u6279\u91cf\u7ea7\u76d1\u63a7\uff0c\u56e0\u4e3a\u653b\u51fb\u5728\u4f18\u5316\u9636\u6bb5\u4f1a\u7559\u4e0b\u53ef\u68c0\u6d4b\u7684\u6a21\u5f0f\u3002"}}
{"id": "2602.15004", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.15004", "abs": "https://arxiv.org/abs/2602.15004", "authors": ["Johannes Schmude", "Sujit Roy", "Liping Wang", "Theodore van Kessel", "Levente Klein", "Marcus Freitag", "Eloisa Bentivegna", "Robert Manson-Sawko", "Bjorn Lutjens", "Manil Maskey", "Campbell Watson", "Rahul Ramachandran", "Juan Bernabe-Moreno"], "title": "PDE foundation models are skillful AI weather emulators for the Martian atmosphere", "comment": null, "summary": "We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u504f\u5fae\u5206\u65b9\u7a0b\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u8fc1\u79fb\u5230\u706b\u661f\u5927\u6c14\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u5c062D\u6a21\u578b\u6269\u5c55\u52303D\u5e76\u5229\u7528\u5c11\u91cf\u6570\u636e\u5fae\u8c03\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u5728\u504f\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3\u4e0a\u7684\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u5b9e\u9645\u590d\u6742\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8bad\u7ec3\u6570\u636e\u6709\u9650\u6216\u8ba1\u7b97\u9884\u7b97\u4e0d\u8db3\u7684\u771f\u5b9e\u4e16\u754c\u95ee\u9898\uff0c\u5982\u706b\u661f\u5927\u6c14\u9884\u6d4b\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8ePoseidon PDE\u57fa\u7840\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4ece\u4e8c\u7ef4\u6269\u5c55\u5230\u4e09\u7ef4\u7684\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u4fe1\u606f\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u6a21\u578b\u5728\u7a00\u758f\u521d\u59cb\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u4e86\u7ea634GB\u7684\u706b\u661f\u5927\u6c14\u6570\u636e\uff084\u4e2a\u706b\u661f\u5e74\uff09\u548c13GPU\u5c0f\u65f6\u7684\u4e2d\u7b49\u8ba1\u7b97\u9884\u7b97\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9884\u8bad\u7ec3\u4e0e\u6a21\u578b\u6269\u5c55\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u5728\u4fdd\u7559\u6d4b\u8bd5\u5e74\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u4e8634.4%\uff0c\u8bc1\u660e\u4e86PDE\u57fa\u7840\u6a21\u578b\u4e0d\u4ec5\u80fd\u8fd1\u4f3c\u5176\u4ed6PDE\u7684\u89e3\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u7f3a\u4e4f\u8db3\u591f\u8bad\u7ec3\u6570\u636e\u6216\u5408\u9002\u8ba1\u7b97\u9884\u7b97\u7684\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u7684\u951a\u5b9a\u6a21\u578b\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660ePDE\u57fa\u7840\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u7528\u4e8e\u5b9e\u9645\u590d\u6742\u7cfb\u7edf\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u548c\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15008", "categories": ["cs.LG", "cs.IT", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15008", "abs": "https://arxiv.org/abs/2602.15008", "authors": ["Daniil Dmitriev", "Zhihan Huang", "Yuting Wei"], "title": "Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees", "comment": null, "summary": "Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $\u03c4$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $\u03c4$-leaping algorithm achieves an iteration complexity of order $\\tilde O(d/\\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $\u03c4$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \\log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\uff0c\u9488\u5bf9\u03c4-leaping\u91c7\u6837\u5668\u5efa\u7acb\u4e86KL\u6563\u5ea6\u6536\u655b\u4fdd\u8bc1\uff0c\u5728\u5747\u5300\u6269\u6563\u4e2d\u6d88\u9664\u4e86\u5bf9\u8bcd\u6c47\u5927\u5c0f\u7684\u7ebf\u6027\u4f9d\u8d56\uff0c\u5728\u63a9\u7801\u6269\u6563\u4e2d\u5f15\u5165\u4e86\u6709\u6548\u603b\u76f8\u5173\u5ea6\u91cf\u5b9e\u73b0\u81ea\u9002\u5e94\u6536\u655b\u3002", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u7684\u7ecf\u9a8c\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\uff0c\u7279\u522b\u662f\u03c4-leaping\u91c7\u6837\u5668\u7684\u6536\u655b\u6027\u80fd\uff0c\u586b\u8865\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u6846\u67b6\u5206\u6790\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u4e3b\u8981\u7814\u7a76\u03c4-leaping\u91c7\u6837\u5668\u3002\u5bf9\u4e8e\u5747\u5300\u79bb\u6563\u6269\u6563\uff0c\u5206\u6790\u6807\u51c6\u03c4-leaping\u7b97\u6cd5\uff1b\u5bf9\u4e8e\u63a9\u7801\u79bb\u6563\u6269\u6563\uff0c\u5f15\u5165\u6539\u8fdb\u7684\u03c4-leaping\u91c7\u6837\u5668\u3002\u5206\u6790\u57fa\u4e8e\u5206\u6570\u71b5\u635f\u5931\u63a7\u5236\uff0c\u65e0\u9700\u6709\u754c\u6027\u6216\u5e73\u6ed1\u6027\u5047\u8bbe\u3002", "result": "\u5bf9\u4e8e\u5747\u5300\u79bb\u6563\u6269\u6563\uff0c\u03c4-leaping\u7b97\u6cd5\u8fbe\u5230\u8fed\u4ee3\u590d\u6742\u5ea6O\u0303(d/\u03b5)\uff0c\u6d88\u9664\u4e86\u5bf9\u8bcd\u6c47\u5927\u5c0fS\u7684\u7ebf\u6027\u4f9d\u8d56\uff0c\u6bd4\u73b0\u6709\u754c\u9650\u6539\u8fdbd\u500d\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u7684\u7b97\u6cd5\u4e0b\u754c\u3002\u5bf9\u4e8e\u63a9\u7801\u79bb\u6563\u6269\u6563\uff0c\u6539\u8fdb\u91c7\u6837\u5668\u7684\u6536\u655b\u7387\u7531\u6709\u6548\u603b\u76f8\u5173\u63a7\u5236\uff0c\u8be5\u91cf\u6709\u754c\u4e8ed log S\u4f46\u5bf9\u7ed3\u6784\u5316\u6570\u636e\u53ef\u4ee5\u662f\u4e9a\u7ebf\u6027\u751a\u81f3\u5e38\u6570\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5b9e\u73b0\u4e9a\u7ebf\u6027\u6536\u655b\u3002", "conclusion": "\u672c\u6587\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6536\u655b\u7406\u8bba\uff0c\u03c4-leaping\u91c7\u6837\u5668\u5728\u5747\u5300\u6269\u6563\u4e2d\u8fbe\u5230\u6700\u4f18\u7ef4\u5ea6\u4f9d\u8d56\uff0c\u5728\u63a9\u7801\u6269\u6563\u4e2d\u901a\u8fc7\u6709\u6548\u603b\u76f8\u5173\u81ea\u9002\u5e94\u5229\u7528\u6570\u636e\u7ed3\u6784\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2602.15022", "categories": ["cs.LG", "cs.AI", "math.GR", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2602.15022", "abs": "https://arxiv.org/abs/2602.15022", "authors": ["Cai Zhou", "Zijie Chen", "Zian Li", "Jike Wang", "Kaiyi Jiang", "Pan Li", "Rose Yu", "Muhan Zhang", "Stephen Bates", "Tommi Jaakkola"], "title": "Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation", "comment": "32 pages", "summary": "Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \\times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\u2014\u2014\u89c4\u8303\u6269\u6563\uff0c\u901a\u8fc7\u5c06\u6837\u672c\u6620\u5c04\u5230\u8f68\u9053\u4ee3\u8868\u5143\uff08\u89c4\u8303\u59ff\u6001\u6216\u987a\u5e8f\uff09\uff0c\u5728\u89c4\u8303\u5207\u7247\u4e0a\u8bad\u7ec3\u65e0\u7ea6\u675f\u6269\u6563\u6a21\u578b\uff0c\u7136\u540e\u5728\u751f\u6210\u65f6\u5e94\u7528\u968f\u673a\u5bf9\u79f0\u53d8\u6362\uff0c\u4ece\u800c\u5904\u7406\u5316\u5b66\u548c\u79d1\u5b66\u4e2d\u5177\u6709\u7fa4\u5bf9\u79f0\u6027\u7684\u5206\u5e03\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u67b6\u6784\u7ea6\u675f\uff08\u5982\u7b49\u53d8\u53bb\u566a\u5668\u548c\u4e0d\u53d8\u5148\u9a8c\uff09\u6765\u5f3a\u5236\u5bf9\u79f0\u6027\u4e0d\u53d8\u6027\u548c\u7b49\u53d8\u6027\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u4f20\u7edf\uff0c\u63d0\u51fa\u4ece\u89c4\u8303\u5316\u7684\u66ff\u4ee3\u89c6\u89d2\u6765\u5904\u7406\u5bf9\u79f0\u4e0d\u53d8\u5206\u5e03\uff0c\u65e8\u5728\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u91c7\u7528\u89c4\u8303\u6269\u6563\u6846\u67b6\uff1a1\uff09\u5c06\u6bcf\u4e2a\u6837\u672c\u6620\u5c04\u5230\u8f68\u9053\u4ee3\u8868\u5143\uff08\u89c4\u8303\u59ff\u6001\u6216\u987a\u5e8f\uff09\uff1b2\uff09\u5728\u89c4\u8303\u5207\u7247\u4e0a\u8bad\u7ec3\u65e0\u7ea6\u675f\uff08\u975e\u7b49\u53d8\uff09\u6269\u6563\u6216\u6d41\u6a21\u578b\uff1b3\uff09\u5728\u751f\u6210\u65f6\u901a\u8fc7\u968f\u673a\u5bf9\u79f0\u53d8\u6362\u6062\u590d\u4e0d\u53d8\u5206\u5e03\u3002\u57fa\u4e8e\u5546\u7a7a\u95f4\u7406\u8bba\uff0c\u7ed3\u5408\u5bf9\u9f50\u5148\u9a8c\u548c\u6700\u4f18\u4f20\u8f93\u8fdb\u4e00\u6b65\u6539\u8fdb\u8bad\u7ec3\u6548\u7387\u3002\u5728\u5206\u5b50\u56fe\u751f\u6210\u4e2d\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u51e0\u4f55\u8c31\u7684\u89c4\u8303\u5316\u548c\u6e29\u548c\u7684\u4f4d\u7f6e\u7f16\u7801\u3002", "result": "\u89c4\u8303\u6269\u6563\u57283D\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u7b49\u53d8\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u91cf\u76f8\u4f3c\u751a\u81f3\u66f4\u5c11\u3002\u63d0\u51fa\u7684\u65b0\u9896\u67b6\u6784CanonFlow\u5728\u5177\u6709\u6311\u6218\u6027\u7684GEOM-DRUG\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5c11\u6b65\u751f\u6210\u4e2d\u4f18\u52bf\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "\u89c4\u8303\u6269\u6563\u4e3a\u5904\u7406\u5bf9\u79f0\u4e0d\u53d8\u5206\u5e03\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5c55\u793a\u4e86\u5176\u5728\u8868\u8fbe\u80fd\u529b\u3001\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u6027\u80fd\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4e3a\u5316\u5b66\u548c\u79d1\u5b66\u4e2d\u7684\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15028", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15028", "abs": "https://arxiv.org/abs/2602.15028", "authors": ["Shangding Gu"], "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PAPerBench\u57fa\u51c6\uff0c\u7cfb\u7edf\u7814\u7a76\u4e0a\u4e0b\u6587\u957f\u5ea6\u5bf9LLM\u4e2a\u6027\u5316\u8d28\u91cf\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u968f\u7740\u4e0a\u4e0b\u6587\u589e\u957f\uff0c\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u5747\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u7a00\u91ca\u95ee\u9898\u3002", "motivation": "LLM\u5728\u9690\u79c1\u654f\u611f\u548c\u4e2a\u6027\u5316\u573a\u666f\u4e2d\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u4e0a\u4e0b\u6587\u957f\u5ea6\u5bf9\u9690\u79c1\u6cc4\u9732\u548c\u4e2a\u6027\u5316\u6548\u679c\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6765\u63a2\u7d22\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u7ea629,000\u4e2a\u5b9e\u4f8b\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u4ece1K\u5230256K token\u7684PAPerBench\u57fa\u51c6\uff0c\u5171377K\u4e2a\u8bc4\u4f30\u95ee\u9898\uff0c\u8054\u5408\u8bc4\u4f30\u4e2a\u6027\u5316\u6027\u80fd\u548c\u9690\u79c1\u98ce\u9669\uff0c\u5e76\u8fdb\u884c\u6ce8\u610f\u529b\u7a00\u91ca\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u6240\u6709\u5148\u8fdbLLM\u7684\u4e2a\u6027\u5316\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u5747\u51fa\u73b0\u4e00\u81f4\u4e0b\u964d\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\u8fd9\u662f\u56fa\u5b9a\u5bb9\u91cfTransformer\u4e2dsoft attention\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5b58\u5728\u666e\u904d\u7684\u6269\u5c55\u5dee\u8ddd\uff1a\u957f\u4e0a\u4e0b\u6587\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u6563\uff1b\u57fa\u51c6\u7684\u53d1\u5e03\u652f\u6301\u53ef\u91cd\u590d\u8bc4\u4f30\u548c\u672a\u6765\u53ef\u6269\u5c55\u9690\u79c1\u4e0e\u4e2a\u6027\u5316\u7814\u7a76\u3002"}}
{"id": "2602.13218", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.13218", "abs": "https://arxiv.org/abs/2602.13218", "authors": ["Bowen Liu", "Zhi Wu", "Runquan Xie", "Zhanhui Kang", "Jia Li"], "title": "Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning", "comment": "37 pages, 8 figures, 4 tables in the main body. Project page: https://github.com/AdAstraAbyssoque/Scaling-the-Scaling-Logic", "summary": "Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.", "AI": {"tldr": "SSLogic\u662f\u4e00\u4e2a\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1-\u4fee\u590d\u5faa\u73af\u81ea\u52a8\u5408\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\u5bf9\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u5c55\u53ef\u9a8c\u8bc1\u8bad\u7ec3\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u89c4\u6a21\u5316\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u53ef\u9a8c\u8bc1\u8bad\u7ec3\u4fe1\u53f7\u7684\u89c4\u6a21\u5316\u662f\u5f3a\u5316\u5b66\u4e60\u4ece\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u7684\u5173\u952e\u74f6\u9888\u3002\u903b\u8f91\u63a8\u7406\u662f\u5929\u7136\u8f7d\u4f53\uff0c\u4f46\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u7f16\u5199\u4ee3\u7801\u6216\u56fa\u5b9a\u6a21\u677f\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u5c42\u9762\u7684\u6269\u5c55\u3002", "method": "\u63d0\u51faSSLogic\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5408\u6210\u548c\u4fee\u590d\u53ef\u6267\u884c\u7684\u751f\u6210\u5668-\u9a8c\u8bc1\u5668\u7a0b\u5e8f\u5bf9\uff0c\u5728\u5c01\u95ed\u7684\u751f\u6210-\u9a8c\u8bc1-\u4fee\u590d\u5faa\u73af\u4e2d\u5b9e\u73b0\u8fde\u7eed\u4efb\u52a1\u65cf\u6f14\u5316\u3002\u5f15\u5165\u591a\u95e8\u9a8c\u8bc1\u534f\u8bae\uff0c\u7ed3\u5408\u591a\u7b56\u7565\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u5bf9\u6297\u6027\u76f2\u5ba1\uff0c\u786e\u4fdd\u53ef\u9760\u6027\u3002", "result": "\u4ece400\u4e2a\u79cd\u5b50\u65cf\u5f00\u59cb\uff0c\u7ecf\u8fc7\u4e24\u8f6e\u6f14\u5316\u6269\u5c55\u5230953\u4e2a\u4efb\u52a1\u65cf\u548c21,389\u4e2a\u53ef\u9a8c\u8bc1\u5b9e\u4f8b\uff08\u4ece5,718\u4e2a\uff09\u3002\u5728SSLogic\u6f14\u5316\u6570\u636e\u4e0a\u8bad\u7ec3\u76f8\u6bd4\u79cd\u5b50\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\uff1aSynLogic +5.2\uff0cBBEH +1.4\uff0cAIME25 +3.0\uff0cBrumo25 +3.7\u3002", "conclusion": "SSLogic\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4efb\u52a1\u65cf\u5c42\u9762\u7684\u89c4\u6a21\u5316\u6269\u5c55\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u7a0b\u5e8f\u5bf9\u5408\u6210\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u9a8c\u8bc1\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u6570\u91cf\uff0c\u4e3aRLVR\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89c4\u6a21\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13230", "abs": "https://arxiv.org/abs/2602.13230", "authors": ["Truong Xuan Khanh", "Truong Quynh Hoa"], "title": "Intelligence as Trajectory-Dominant Pareto Optimization", "comment": "13 pages, 3 figures", "summary": "Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8f68\u8ff9\u4e3b\u5bfc\u5e15\u7d2f\u6258\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u89c6\u4e3a\u8f68\u8ff9\u5c42\u9762\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u5e15\u7d2f\u6258\u9677\u9631\u5982\u4f55\u9650\u5236\u957f\u671f\u9002\u5e94\u6027\u53d1\u5c55\uff0c\u5e76\u5b9a\u4e49\u4e86\u9677\u9631\u9003\u9038\u96be\u5ea6\u6307\u6570\u6765\u91cf\u5316\u8fd9\u79cd\u7ea6\u675f\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u8fd1\u671f\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u8bb8\u591a\u7cfb\u7edf\u5728\u957f\u671f\u9002\u5e94\u6027\u65b9\u9762\u51fa\u73b0\u505c\u6ede\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u9650\u5236\u5e76\u975e\u6e90\u4e8e\u5b66\u4e60\u3001\u6570\u636e\u6216\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\uff0c\u800c\u662f\u6e90\u4e8e\u667a\u80fd\u968f\u65f6\u95f4\u4f18\u5316\u7684\u6df1\u5c42\u7ed3\u6784\u7279\u6027\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u4e3b\u5bfc\u5e15\u7d2f\u6258\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u63a8\u5e7f\u5230\u8f68\u8ff9\u5c42\u9762\uff1b\u5b9a\u4e49\u9677\u9631\u9003\u9038\u96be\u5ea6\u6307\u6570(TEDI)\u6765\u91cf\u5316\u7ea6\u675f\u521a\u6027\uff1b\u5efa\u7acb\u5e15\u7d2f\u6258\u9677\u9631\u7684\u5f62\u5f0f\u5316\u5206\u7c7b\uff1b\u4f7f\u7528\u6700\u5c0f\u667a\u80fd\u4f53-\u73af\u5883\u6a21\u578b\u8fdb\u884c\u8bf4\u660e\u3002", "result": "\u52a8\u6001\u667a\u80fd\u4e0a\u9650\u662f\u8f68\u8ff9\u5c42\u9762\u4e3b\u5bfc\u6027\u7684\u5fc5\u7136\u51e0\u4f55\u7ed3\u679c\uff0c\u4e0e\u5b66\u4e60\u8fdb\u5c55\u6216\u67b6\u6784\u89c4\u6a21\u65e0\u5173\uff1b\u63ed\u793a\u4e86\u4f18\u5316\u51e0\u4f55\u5982\u4f55\u51b3\u5b9a\u667a\u80fd\u53d1\u5c55\uff0c\u800c\u975e\u7ec8\u7aef\u6027\u80fd\u3002", "conclusion": "\u5c06\u667a\u80fd\u7814\u7a76\u7684\u7126\u70b9\u4ece\u7ec8\u7aef\u6027\u80fd\u8f6c\u79fb\u5230\u4f18\u5316\u51e0\u4f55\uff0c\u4e3a\u8bca\u65ad\u548c\u514b\u670d\u81ea\u9002\u5e94\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u53d1\u5c55\u7ea6\u675f\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2602.13286", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13286", "abs": "https://arxiv.org/abs/2602.13286", "authors": ["Nathanya Satriani", "Djordje Slijep\u010devi\u0107", "Markus Schedl", "Matthias Zeppelzauer"], "title": "Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification", "comment": "8 pages, 4 figures, CBMI2025", "summary": "Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u53ef\u89e3\u91ca\u4ea4\u4e92\u5b66\u4e60\uff08XIL\uff09\u5728\u7f13\u89e3\u89c6\u89c9\u5206\u7c7b\u5668\u504f\u89c1\u548c\u865a\u5047\u76f8\u5173\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6027\u522b\u5206\u7c7b\u7b49\u6613\u53d7\u6570\u636e\u504f\u89c1\u5f71\u54cd\u7684\u573a\u666f\u4e2d\u3002\u7814\u7a76\u6bd4\u8f83\u4e86CAIPI\u3001RRR\u4e24\u79cdXIL\u7b56\u7565\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u76f8\u5173\u7279\u5f81\u5e76\u51cf\u5c11\u6a21\u578b\u504f\u89c1\uff0c\u4f46\u901a\u5e38\u4f1a\u8f7b\u5fae\u964d\u4f4e\u6027\u80fd\uff08CAIPI\u9664\u5916\uff09\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5bb9\u6613\u53d7\u5230\u6570\u636e\u504f\u89c1\u548c\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u6027\u522b\u5206\u7c7b\u7b49\u654f\u611f\u9886\u57df\u3002\u53ef\u89e3\u91ca\u4ea4\u4e92\u5b66\u4e60\uff08XIL\uff09\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u63d0\u4f9b\u5bf9\u6a21\u578b\u89e3\u91ca\u7684\u53cd\u9988\u6765\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\uff0c\u8fd9\u4e3a\u89e3\u51b3\u504f\u89c1\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22XIL\u5728\u7f13\u89e3\u89c6\u89c9\u5206\u7c7b\u5668\u504f\u89c1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u8c03\u67e5\u4e86\u4e24\u79cd\u65b9\u6cd5\u5b66\u4e0a\u4e0d\u540c\u7684\u6700\u5148\u8fdbXIL\u7b56\u7565\uff1aCAIPI\u548cRight for the Right Reasons\uff08RRR\uff09\uff0c\u4ee5\u53ca\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u7b56\u7565\u7684\u65b0\u9896\u6df7\u5408\u65b9\u6cd5\u3002\u901a\u8fc7\u6bd4\u8f83\u4f7f\u7528Gradient-weighted Class Activation Mapping\uff08GradCAM\uff09\u548cBounded Logit Attention\uff08BLA\uff09\u751f\u6210\u7684\u89e3\u91ca\u4e0e\u5206\u5272\u63a9\u7801\uff0c\u5bf9\u7ed3\u679c\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\u3002\u5b9e\u9a8c\u5728\u6613\u53d7\u6570\u636e\u504f\u89c1\u5f71\u54cd\u7684\u6027\u522b\u5206\u7c7b\u573a\u666f\u4e2d\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\uff081\uff09\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5173\u6ce8\u76f8\u5173\u56fe\u50cf\u7279\u5f81\uff0c\u7279\u522b\u662f\u4f7f\u7528CAIPI\u65f6\u6548\u679c\u66f4\u4f73\uff1b\uff082\uff09\u80fd\u591f\u51cf\u5c11\u6a21\u578b\u504f\u89c1\uff0c\u5373\u5e73\u8861\u7537\u6027\u548c\u5973\u6027\u9884\u6d4b\u4e4b\u95f4\u7684\u8bef\u5206\u7c7b\u7387\uff1b\uff083\uff09XIL\u65b9\u6cd5\u5728\u63d0\u9ad8\u6027\u522b\u5206\u7c7b\u5668\u516c\u5e73\u6027\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002\u867d\u7136XIL\u5e26\u6765\u7684\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u63d0\u5347\u901a\u5e38\u4f1a\u5bfc\u81f4\u8f7b\u5fae\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4f46CAIPI\u663e\u793a\u51fa\u751a\u81f3\u53ef\u80fd\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u53ef\u89e3\u91ca\u4ea4\u4e92\u5b66\u4e60\uff08XIL\uff09\u65b9\u6cd5\u5728\u7f13\u89e3\u89c6\u89c9\u5206\u7c7b\u5668\u504f\u89c1\u548c\u63d0\u9ad8\u516c\u5e73\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002CAIPI\u3001RRR\u53ca\u5176\u6df7\u5408\u65b9\u6cd5\u90fd\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u76f8\u5173\u7279\u5f81\u5e76\u51cf\u5c11\u504f\u89c1\uff0c\u5176\u4e2dCAIPI\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u8fd9\u4e9b\u6539\u8fdb\u3002\u8fd9\u9879\u7814\u7a76\u652f\u6301\u4e86XIL\u5728\u6784\u5efa\u66f4\u516c\u5e73\u3001\u66f4\u900f\u660e\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u65b9\u9762\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.13296", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13296", "abs": "https://arxiv.org/abs/2602.13296", "authors": ["Edwyn Brient", "Santiago Velasco-Forero", "Rami Kassab"], "title": "MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models", "comment": null, "summary": "High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u7269\u7406\u89e3\u91ca\u7684HRRP\u751f\u6210\u6570\u636e\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u5c06HRRP\u6570\u636e\u5206\u89e3\u4e3a\u63a9\u7801\u3001\u7279\u5f81\u548c\u566a\u58f0\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9ed1\u76d2\u5206\u7c7b\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u591a\u7ea7\u8bc4\u4f30\u80fd\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dHRRP\u751f\u6210\u6570\u636e\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u5206\u7c7b\u6a21\u578b\uff0c\u8fd9\u4e9b\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u8fdb\u884c\u591a\u7ea7\u8bc4\u4f30\u3002\u9700\u8981\u57fa\u4e8eHRRP\u6570\u636e\u7684\u7269\u7406\u7279\u6027\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06HRRP\u6570\u636e\u5206\u89e3\u4e3a\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u63a9\u7801\u3001\u7279\u5f81\u548c\u566a\u58f0\u3002\u57fa\u4e8e\u8fd9\u79cd\u5206\u89e3\uff0c\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u7269\u7406\u89e3\u91ca\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5728\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u4f7f\u7528\u6602\u8d35\u7684\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u7684\u4e24\u79cd\u6307\u6807\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5224\u522b\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30HRRP\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5c06HRRP\u6570\u636e\u5206\u89e3\u4e3a\u63a9\u7801\u3001\u7279\u5f81\u548c\u566a\u58f0\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u57fa\u4e8e\u7269\u7406\u89e3\u91ca\u63d0\u51fa\u8bc4\u4f30\u6307\u6807\uff0c\u4e3aHRRP\u751f\u6210\u6570\u636e\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.13297", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13297", "abs": "https://arxiv.org/abs/2602.13297", "authors": ["Edwyn Brient", "Santiago Velasco-Forero", "Rami Kassab"], "title": "Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset", "comment": null, "summary": "High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u57fa\u4e8e\u5927\u89c4\u6a21\u6d77\u4e0a\u6570\u636e\u5e93\u7684HRRP\u5408\u6210\uff0c\u53d1\u73b0\u51e0\u4f55\u56e0\u7d20\uff08\u8239\u8236\u5c3a\u5bf8\u548c\u89c6\u89d2\uff09\u662f\u5f71\u54cd\u96f7\u8fbe\u9ad8\u5206\u8fa8\u7387\u8ddd\u79bb\u50cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u6a21\u578b\u80fd\u591f\u590d\u73b0\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u51e0\u4f55\u8d8b\u52bf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u8ddd\u79bb\u50cf\uff08HRRP\uff09\u5728\u96f7\u8fbe\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\u4e2d\u5177\u6709\u5feb\u901f\u5904\u7406\u4f18\u52bf\uff0c\u4f46\u5bf9\u91c7\u96c6\u6761\u4ef6\u7684\u9ad8\u5ea6\u654f\u611f\u6027\u9650\u5236\u4e86\u5176\u5728\u5404\u79cd\u64cd\u4f5c\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u73b0\u6709\u7814\u7a76\u53d7\u9650\u4e8e\u5c0f\u578b\u3001\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u9700\u8981\u5728\u5927\u89c4\u6a21\u4ee3\u8868\u6027\u6570\u636e\u5e93\u4e0a\u7814\u7a76HRRP\u5408\u6210\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u6d77\u4e0a\u6570\u636e\u5e93\uff0c\u5206\u6790\u53d1\u73b0\u51e0\u4f55\u56e0\u7d20\uff08\u8239\u8236\u5c3a\u5bf8\u548c\u671f\u671b\u89c6\u89d2\uff09\u662f\u4e3b\u8981\u573a\u666f\u9a71\u52a8\u56e0\u7d20\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u4f5c\u4e3a\u6761\u4ef6\uff0c\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u8fdb\u884cHRRP\u5408\u6210\u3002", "result": "\u5408\u6210\u4fe1\u53f7\u80fd\u591f\u590d\u73b0\u5b9e\u6570\u636e\u4e2d\u89c2\u5bdf\u5230\u7684\u89c6\u7ebf\u51e0\u4f55\u8d8b\u52bf\uff0c\u9a8c\u8bc1\u4e86\u51e0\u4f55\u56e0\u7d20\u5728HRRP\u751f\u6210\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "\u91c7\u96c6\u51e0\u4f55\u5728\u9c81\u68d2HRRP\u751f\u6210\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u57fa\u4e8e\u51e0\u4f55\u6761\u4ef6\u7684\u751f\u6210\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6d77\u4e0a\u76d1\u89c6\u573a\u666f\u7684\u53d8\u5f02\u6027\u3002"}}
{"id": "2602.13303", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.13303", "abs": "https://arxiv.org/abs/2602.13303", "authors": ["Nicolas Bourriez", "Alexandre Verine", "Auguste Genovesio"], "title": "Spectral Collapse in Diffusion Inversion", "comment": null, "summary": "Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6b63\u4ea4\u65b9\u5dee\u5f15\u5bfc(OVG)\u65b9\u6cd5\uff0c\u89e3\u51b3\u6761\u4ef6\u6269\u6563\u53cd\u6f14\u4e2d\u6e90\u57df\u9891\u8c31\u7a00\u758f\u65f6\u7684\"\u9891\u8c31\u574d\u7f29\"\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u6062\u590d\u903c\u771f\u7eb9\u7406", "motivation": "\u6807\u51c6\u786e\u5b9a\u6027\u53cd\u6f14\u65b9\u6cd5\u5728\u6e90\u57df\u9891\u8c31\u7a00\u758f\u65f6\uff08\u5982\u8d85\u5206\u8fa8\u7387\u3001\u8349\u56fe\u8f6c\u56fe\u50cf\uff09\u4f1a\u5931\u8d25\uff0c\u5bfc\u81f4\u6062\u590d\u7684\u6f5c\u5728\u53d8\u91cf\u4e0d\u7b26\u5408\u5404\u5411\u540c\u6027\u9ad8\u65af\u5206\u5e03\uff0c\u4ea7\u751f\u4f4e\u9891\u4fe1\u53f7\u548c\u7eb9\u7406\u8d2b\u4e4f\u7684\u751f\u6210\u7ed3\u679c\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\"\u9891\u8c31\u574d\u7f29\"", "method": "\u63d0\u51fa\u6b63\u4ea4\u65b9\u5dee\u5f15\u5bfc(OVG)\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63ODE\u52a8\u529b\u5b66\uff0c\u5728\u7ed3\u6784\u68af\u5ea6\u7684\u96f6\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6267\u884c\u7406\u8bba\u9ad8\u65af\u566a\u58f0\u5e45\u5ea6\uff0c\u89e3\u51b3\u7ed3\u6784-\u7eb9\u7406\u6743\u8861\u95ee\u9898", "result": "\u5728\u663e\u5fae\u955c\u8d85\u5206\u8fa8\u7387(BBBC021)\u548c\u8349\u56fe\u8f6c\u56fe\u50cf(Edges2Shoes)\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOVG\u80fd\u6709\u6548\u6062\u590d\u903c\u771f\u7eb9\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6", "conclusion": "OVG\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6761\u4ef6\u6269\u6563\u53cd\u6f14\u4e2d\u7684\u9891\u8c31\u574d\u7f29\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u7eb9\u7406\u8d28\u91cf\uff0c\u4e3a\u8de8\u57df\u56fe\u50cf\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.13306", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13306", "abs": "https://arxiv.org/abs/2602.13306", "authors": ["Zhehan Zhang", "Meihua Qian", "Li Luo", "Siyu Huang", "Chaoyi Zhou", "Ripon Saha", "Xinxin Song"], "title": "Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique", "comment": null, "summary": "Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eQwen2-VL-7B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u4eba\u7c7b\u7ed8\u753b\u7684\u521b\u9020\u529b\uff0c\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u6570\u503c\u8bc4\u5206\u548c\u751f\u6210\u7b26\u5408\u8bc4\u5206\u6807\u51c6\u7684\u89e3\u91ca\u6027\u53cd\u9988\u3002", "motivation": "\u827a\u672f\u521b\u9020\u529b\u8bc4\u4f30\u5bf9\u521b\u9020\u529b\u7814\u7a76\u548c\u827a\u672f\u6559\u80b2\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u4eba\u5de5\u8bc4\u5206\u65b9\u6cd5\uff08\u5982\u6258\u5170\u65af\u521b\u9020\u529b\u6d4b\u8bd5\uff09\u5728\u5927\u89c4\u6a21\u5e94\u7528\u65f6\u52b3\u52a8\u5bc6\u96c6\u3002\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u56fe\u50cf\u7279\u5f81\uff0c\u63d0\u4f9b\u7684\u89e3\u91ca\u6027\u53cd\u9988\u6709\u9650\u6216\u6ca1\u6709\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u81ea\u52a8\u8bc4\u5206\u53c8\u80fd\u63d0\u4f9b\u89e3\u91ca\u6027\u53cd\u9988\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f7f\u75281000\u5e45\u4eba\u7c7b\u521b\u4f5c\u7684\u7ed8\u753b\u4f5c\u54c1\u6570\u636e\u96c6\uff0c\u6bcf\u5e45\u4f5c\u54c1\u90fd\u67091-100\u5206\u7684\u8bc4\u5206\u548c\u7b80\u77ed\u63cf\u8ff0\u3002\u91c7\u7528Qwen2-VL-7B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u89c6\u89c9\u7f16\u7801\u5668\u8f93\u51fa\u4e0a\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u56de\u5f52\u5934\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u6570\u503c\u8bc4\u5206\u548c\u751f\u6210\u7b26\u5408\u4e94\u7ef4\u5ea6\u8bc4\u5206\u6807\u51c6\uff08\u539f\u521b\u6027\u3001\u8272\u5f69\u3001\u7eb9\u7406\u3001\u6784\u56fe\u3001\u5185\u5bb9\uff09\u7684\u53cd\u9988\u3002\u901a\u8fc7\u7cfb\u7edf\u63d0\u793a\u5d4c\u5165\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u548c\u4f5c\u54c1\u63cf\u8ff0\u6765\u7ea6\u675f\u751f\u6210\u6587\u672c\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u5728100\u5206\u5236\u4e0a\u8fbe\u5230\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570r > 0.97\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7ea63.95\u5206\u3002\u751f\u6210\u7684\u53cd\u9988\u4e0e\u4e13\u5bb6\u8bc4\u8bed\u8bed\u4e49\u76f8\u4f3c\u5ea6\u9ad8\uff08\u5e73\u5747SBERT\u4f59\u5f26\u76f8\u4f3c\u5ea6=0.798\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u827a\u672f\u8bc4\u4f30\u76f8\u7ed3\u5408\uff0c\u4e3a\u521b\u9020\u529b\u7814\u7a76\u548c\u8bfe\u5802\u53cd\u9988\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u65e2\u80fd\u81ea\u52a8\u5316\u8bc4\u5206\u53c8\u80fd\u63d0\u4f9b\u89e3\u91ca\u6027\u53cd\u9988\u3002"}}
{"id": "2602.13318", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13318", "abs": "https://arxiv.org/abs/2602.13318", "authors": ["Daesik Jang", "Morgan Lindsay Heisler", "Linzi Xing", "Yifei Li", "Edward Wang", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing", "comment": null, "summary": "Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .", "AI": {"tldr": "DECKBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u5e7b\u706f\u7247\u751f\u6210\u548c\u7f16\u8f91\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\u548c\u7cfb\u7edf\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5185\u5bb9\u9009\u62e9\u3001\u7ec4\u7ec7\u3001\u5e03\u5c40\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5b66\u672f\u5e7b\u706f\u7247\u81ea\u52a8\u751f\u6210\u548c\u8fed\u4ee3\u7f16\u8f91\u9700\u8981\u8d85\u8d8a\u6587\u6863\u6458\u8981\u7684\u80fd\u529b\uff0c\u5305\u62ec\u5fe0\u5b9e\u7684\u5185\u5bb9\u9009\u62e9\u3001\u8fde\u8d2f\u7684\u5e7b\u706f\u7247\u7ec4\u7ec7\u3001\u5e03\u5c40\u611f\u77e5\u7684\u6e32\u67d3\u548c\u9c81\u68d2\u7684\u591a\u8f6e\u6307\u4ee4\u9075\u5faa\u3002\u7136\u800c\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u534f\u8bae\u672a\u80fd\u5145\u5206\u8861\u91cf\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86DECKBench\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u7cbe\u5fc3\u7b56\u5212\u7684\u8bba\u6587-\u5e7b\u706f\u7247\u5bf9\u6570\u636e\u96c6\uff0c\u5e76\u589e\u52a0\u4e86\u771f\u5b9e\u7684\u6a21\u62df\u7f16\u8f91\u6307\u4ee4\u3002\u8bc4\u4f30\u534f\u8bae\u7cfb\u7edf\u5316\u8bc4\u4f30\u5e7b\u706f\u7247\u7ea7\u548c\u6f14\u793a\u6587\u7a3f\u7ea7\u7684\u4fdd\u771f\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u5e03\u5c40\u8d28\u91cf\u548c\u591a\u8f6e\u6307\u4ee4\u9075\u5faa\u3002\u540c\u65f6\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u8bba\u6587\u89e3\u6790\u4e0e\u6458\u8981\u3001\u5e7b\u706f\u7247\u89c4\u5212\u3001HTML\u521b\u5efa\u548c\u8fed\u4ee3\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u80fd\u591f\u7a81\u51fa\u7cfb\u7edf\u4f18\u52bf\u3001\u66b4\u9732\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4e3a\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u5e7b\u706f\u7247\u751f\u6210\u548c\u7f16\u8f91\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u5b66\u672f\u6f14\u793a\u6587\u7a3f\u751f\u6210\u548c\u7f16\u8f91\u7684\u53ef\u91cd\u590d\u3001\u53ef\u6bd4\u8f83\u8bc4\u4f30\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b66\u672f\u6f14\u793a\u6587\u7a3f\u751f\u6210\u548c\u7f16\u8f91\u7684\u53ef\u91cd\u590d\u548c\u53ef\u6bd4\u8f83\u8bc4\u4f30\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.13321", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13321", "abs": "https://arxiv.org/abs/2602.13321", "authors": ["Tri Nguyen", "Huy Hoang Bao Le", "Lohith Srikanth Pentapalli", "Laurah Turner", "Kelly Cohen"], "title": "Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction", "comment": null, "summary": "Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u63d0\u53d6\u8bed\u8a00\u7279\u5f81\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e34\u5e8a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8d8a\u72f1\u5c1d\u8bd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u7684\u56db\u4e2a\u6838\u5fc3\u8bed\u8a00\u7279\u5f81\u8bad\u7ec3BERT\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u7136\u540e\u4f7f\u7528\u591a\u79cd\u5206\u7c7b\u5668\u8fdb\u884c\u8d8a\u72f1\u68c0\u6d4b\u3002", "motivation": "\u4e34\u5e8a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u68c0\u6d4b\u8d8a\u72f1\u5c1d\u8bd5\uff0c\u4f46\u4e4b\u524d\u57fa\u4e8e\u624b\u52a8\u6807\u6ce8\u8bed\u8a00\u7279\u5f81\u7684\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u6807\u6ce8\u7684\u56db\u4e2a\u6838\u5fc3\u8bed\u8a00\u7279\u5f81\uff08\u4e13\u4e1a\u6027\u3001\u533b\u5b66\u76f8\u5173\u6027\u3001\u4f26\u7406\u884c\u4e3a\u3001\u4e0a\u4e0b\u6587\u5e72\u6270\uff09\uff0c\u8bad\u7ec3\u591a\u4e2a\u901a\u7528\u9886\u57df\u548c\u533b\u5b66\u9886\u57df\u7684BERT\u6a21\u578b\u6765\u9884\u6d4b\u8fd9\u4e9b\u7279\u5f81\uff0c\u9009\u62e9\u6700\u53ef\u9760\u7684\u7279\u5f81\u56de\u5f52\u5668\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u6811\u3001\u7ebf\u6027\u3001\u6982\u7387\u548c\u96c6\u6210\u65b9\u6cd5\u7684\u5206\u7c7b\u5668\u8fdb\u884c\u8d8a\u72f1\u68c0\u6d4b\u3002", "result": "\u7cfb\u7edf\u5728\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4fdd\u7559\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8868\u660eLLM\u63d0\u53d6\u7684\u8bed\u8a00\u7279\u5f81\u4e3a\u81ea\u52a8\u8d8a\u72f1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6807\u6ce8\u548c\u7279\u5f81\u8868\u793a\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5b89\u5168\u5173\u952e\u4e34\u5e8a\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u8d8a\u72f1\u884c\u4e3a\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\uff0c\u5982\u66f4\u4e30\u5bcc\u7684\u6807\u6ce8\u65b9\u6848\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\u63d0\u53d6\u4ee5\u53ca\u6355\u6349\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u8d8a\u72f1\u884c\u4e3a\u6f14\u53d8\u98ce\u9669\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.13322", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13322", "abs": "https://arxiv.org/abs/2602.13322", "authors": ["Datorien L. Anderson"], "title": "Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture", "comment": "8 pages, 3 figures and extra material to help can be found: https://zenodo.org/records/18529180", "summary": "We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the \"Form-First\" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.", "AI": {"tldr": "PSI\u6570\u636e\u96c6\u901a\u8fc7\u4e09\u4e2a\u8bca\u65ad\u6027\u6d4b\u8bd5\u9a8c\u8bc1\u4e86Eidos\u67b6\u6784\u5728\u4fdd\u6301\u62d3\u6251\u4e0d\u53d8\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u5176\u6cdb\u5316\u80fd\u529b\u6e90\u4e8e\u51e0\u4f55\u5b8c\u6574\u6027\u800c\u975e\u7edf\u8ba1\u89c4\u6a21", "motivation": "\u6807\u51c6\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7eb9\u7406\u76f8\u5173\u6027\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u96be\u4ee5\u5206\u79bb\u62d3\u6251\u4e0d\u53d8\u6027\uff08\u7ed3\u6784\u5728\u4eff\u5c04\u53d8\u6362\u4e0b\u7684\u4fdd\u6301\u80fd\u529b\uff09\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bca\u65ad\u57fa\u51c6\u6765\u9a8c\u8bc1\"\u5f62\u5f0f\u4f18\u5148\"\u5047\u8bbe", "method": "\u521b\u5efaPolyShapes-Ideal (PSI)\u8bca\u65ad\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b\u4e09\u4e2a\u8bca\u65ad\u63a2\u9488\uff1a\u566a\u58f0\u4e0b\u7684\u591a\u8fb9\u5f62\u5206\u7c7b\u3001MNIST\u7684\u96f6\u6837\u672c\u5b57\u4f53\u8fc1\u79fb\u3001\u6e10\u8fdb\u53d8\u5f62\u4e0b\u7684\u51e0\u4f55\u584c\u9677\u6620\u5c04", "result": "Eidos\u67b6\u6784\u5728PSI\u4e0a\u8fbe\u5230>99%\u51c6\u786e\u7387\uff0c\u572830\u79cd\u672a\u89c1\u5b57\u4f53\u4e0a\u5b9e\u73b081.67%\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u65e0\u9700\u9884\u8bad\u7ec3", "conclusion": "\u7ed3\u679c\u9a8c\u8bc1\u4e86\"\u5f62\u5f0f\u4f18\u5148\"\u5047\u8bbe\uff1a\u7ed3\u6784\u7ea6\u675f\u67b6\u6784\u7684\u6cdb\u5316\u80fd\u529b\u662f\u51e0\u4f55\u5b8c\u6574\u6027\u7684\u5c5e\u6027\uff0c\u800c\u975e\u7edf\u8ba1\u89c4\u6a21"}}
{"id": "2602.13334", "categories": ["cs.CV", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13334", "abs": "https://arxiv.org/abs/2602.13334", "authors": ["Hao Liu", "Suhaib A. Fahmy"], "title": "Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators", "comment": null, "summary": "Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u8f7b\u91cf\u7ea7\u901a\u7528ViT\uff0c\u5728\u8fd1\u8fb9\u7f18\u52a0\u901f\u5668\u90e8\u7f72\u591a\u4e2a\u4e2d\u578b\u4e13\u5bb6ViT\uff0c\u901a\u8fc7\u8def\u7531\u673a\u5236\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u89c6\u89c9Transformer\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u5b8c\u5168\u5378\u8f7d\u5230\u4e91\u7aef\u5219\u5b58\u5728\u663e\u8457\u7684\u5ef6\u8fdf\u5f00\u9500\uff0c\u9700\u8981\u4e00\u79cd\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff1a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u8f7b\u91cf\u7ea7\u901a\u7528ViT\uff0c\u8fd1\u8fb9\u7f18\u52a0\u901f\u5668\u90e8\u7f72\u591a\u4e2a\u4e2d\u578b\u4e13\u5bb6ViT\uff1b2) \u8def\u7531\u673a\u5236\uff1a\u57fa\u4e8e\u8fb9\u7f18\u6a21\u578b\u7684Top-k\u9884\u6d4b\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u4e13\u5bb6\uff1b3) \u6e10\u8fdb\u5f0f\u4e13\u5bb6\u8bad\u7ec3\u7b56\u7565\uff1a\u589e\u5f3a\u4e13\u5bb6\u5728\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728CIFAR-100\u6570\u636e\u96c6\u548c\u771f\u5b9e\u8fb9\u7f18/\u8fd1\u8fb9\u7f18\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u8bad\u7ec3\u7b56\u7565\u5c06\u4e13\u5bb6\u5728\u76ee\u6807\u5b50\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u53474.12%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u6bd4\u9759\u6001\u4e13\u5bb6\u63d0\u9ad82.76%\uff1b\u5ef6\u8fdf\u6bd4\u8fb9\u7f18\u6267\u884c\u964d\u4f4e45%\uff0c\u80fd\u8017\u6bd4\u4ec5\u8fd1\u8fb9\u7f18\u5378\u8f7d\u964d\u4f4e46%\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u4f5c\u63a8\u7406\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u8fb9\u7f18\u8ba1\u7b97\u548c\u4e91\u7aef\u5378\u8f7d\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u80fd\u8017\u3002"}}
{"id": "2602.13372", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13372", "abs": "https://arxiv.org/abs/2602.13372", "authors": ["Simon Rosen", "Siddarth Singh", "Ebenezer Gelo", "Helen Sarah Robertson", "Ibrahim Suder", "Victoria Williams", "Benjamin Rosman", "Geraud Nangue Tasse", "Steven James"], "title": "MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents", "comment": "Accepted at AAMAS 2026", "summary": "Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.", "AI": {"tldr": "\u63d0\u51faMorality Chains\u5f62\u5f0f\u5316\u6846\u67b6\u548cMoralityGym\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u51b2\u7a81\u6027\u5c42\u7ea7\u9053\u5fb7\u89c4\u8303\u4e0b\u7684\u5bf9\u9f50\u8868\u73b0\uff0c\u901a\u8fc798\u4e2a\u4f26\u7406\u56f0\u5883\u95ee\u9898\u6d4b\u8bd5\uff0c\u63ed\u793a\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8bc4\u4f30AI\u5728\u51b2\u7a81\u6027\u3001\u5c42\u7ea7\u5316\u4eba\u7c7b\u9053\u5fb7\u89c4\u8303\u4e0b\u7684\u5bf9\u9f50\u8868\u73b0\u662fAI\u5b89\u5168\u3001\u9053\u5fb7\u54f2\u5b66\u548c\u8ba4\u77e5\u79d1\u5b66\u4ea4\u53c9\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faMorality Chains\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5c06\u9053\u5fb7\u89c4\u8303\u8868\u793a\u4e3a\u6709\u5e8f\u7684\u9053\u4e49\u7ea6\u675f\uff1b\u521b\u5efaMoralityGym\u57fa\u51c6\uff0c\u5305\u542b98\u4e2a\u4f26\u7406\u56f0\u5883\u95ee\u9898\uff0c\u4ee5\u7535\u8f66\u56f0\u5883\u98ce\u683c\u7684Gymnasium\u73af\u5883\u5448\u73b0\uff1b\u5c06\u4efb\u52a1\u89e3\u51b3\u4e0e\u9053\u5fb7\u8bc4\u4f30\u89e3\u8026\uff0c\u5f15\u5165\u65b0\u7684\u9053\u5fb7\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u4f7f\u7528\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u83b7\u5f97\u7684\u57fa\u7ebf\u7ed3\u679c\u63ed\u793a\u4e86\u5173\u952e\u5c40\u9650\u6027\uff0c\u8868\u660e\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u4f26\u7406\u51b3\u7b56\u65b9\u6cd5\uff1b\u8be5\u6846\u67b6\u4e3a\u6574\u5408\u5fc3\u7406\u5b66\u548c\u54f2\u5b66\u89c1\u89e3\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u66f4\u53ef\u9760\u3001\u900f\u660e\u548c\u9053\u5fb7\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u9053\u5fb7\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.13378", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13378", "abs": "https://arxiv.org/abs/2602.13378", "authors": ["Sohail Ali Farooqui", "Zuhair Ahmed Khan Taha", "Mohammed Mudassir Uddin", "Shahnawaz Alam"], "title": "LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery", "comment": null, "summary": "Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\\times$160 resolution extends localization to objects below 8$\\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\\pm$0.3\\% mAP@0.5 on VisDrone-DET2019 with 2.3\\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\\pm$0.4\\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.", "AI": {"tldr": "LAF-YOLOv10\u662f\u57fa\u4e8eYOLOv10n\u6539\u8fdb\u7684\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u56db\u4e2a\u4e92\u8865\u6280\u672f\u6a21\u5757\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5728VisDrone\u6570\u636e\u96c6\u4e0a\u8fbe\u523035.1% mAP\uff0c\u53c2\u6570\u4ec52.3M\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u90e8\u7f72\u3002", "motivation": "\u65e0\u4eba\u673a\u4f5c\u4e3a\u4e3b\u8981\u611f\u77e5\u5e73\u53f0\uff0c\u5728\u76d1\u63a7\u3001\u4ea4\u901a\u76d1\u6d4b\u548c\u707e\u96be\u54cd\u5e94\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002\u5f53\u524d\u68c0\u6d4b\u5668\u9762\u4e34\u65e0\u4eba\u673a\u7279\u6709\u7684\u6311\u6218\uff1a\u76ee\u6807\u5c3a\u5bf8\u5c0f\uff08\u4ec5\u51e0\u4e2a\u50cf\u7d20\uff09\u3001\u80cc\u666f\u6742\u4e71\u3001\u4e25\u91cd\u906e\u6321\u4ee5\u53ca\u4e25\u683c\u7684\u673a\u8f7d\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u3002", "method": "1. PC-C2f\u6a21\u5757\uff1a\u9650\u5236\u7a7a\u95f4\u5377\u79ef\u52301/4\u9aa8\u5e72\u901a\u9053\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u540c\u65f6\u4fdd\u6301\u5224\u522b\u80fd\u529b\n2. AG-FPN\u6a21\u5757\uff1a\u5728\u8de8\u5c3a\u5ea6\u878d\u5408\u524d\u63d2\u5165SE\u901a\u9053\u95e8\u63a7\uff0c\u7528DySample\u66ff\u6362\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\n3. \u8f85\u52a9P2\u68c0\u6d4b\u5934\uff1a\u5728160\u00d7160\u5206\u8fa8\u7387\u589e\u52a0\u68c0\u6d4b\u5934\uff0c\u63d0\u53478\u00d78\u50cf\u7d20\u4ee5\u4e0b\u5c0f\u76ee\u6807\u68c0\u6d4b\n4. Wise-IoU v3\uff1a\u66ff\u6362CIoU\u7528\u4e8e\u8fb9\u754c\u6846\u56de\u5f52\uff0c\u51cf\u5c11\u6807\u6ce8\u566a\u58f0\u5f71\u54cd", "result": "\u5728VisDrone-DET2019\u6570\u636e\u96c6\u4e0a\u8fbe\u523035.1\u00b10.3% mAP@0.5\uff0c\u6bd4YOLOv10n\u63d0\u53473.3\u4e2a\u767e\u5206\u70b9\uff1b\u5728UAVDT\u6570\u636e\u96c6\u4e0a\u8fbe\u523035.8\u00b10.4% mAP@0.5\uff1b\u5728NVIDIA Jetson Orin Nano\u4e0a\u5b9e\u73b024.3 FPS\uff08FP16\u7cbe\u5ea6\uff09\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u90e8\u7f72\u3002", "conclusion": "LAF-YOLOv10\u901a\u8fc7\u56db\u4e2a\u4e92\u8865\u6a21\u5757\u7684\u8054\u5408\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u591a\u4e2a\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5d4c\u5165\u5f0f\u65e0\u4eba\u673a\u90e8\u7f72\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.13515", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13515", "abs": "https://arxiv.org/abs/2602.13515", "authors": ["Jintao Zhang", "Kai Jiang", "Chendong Xiang", "Weiqi Feng", "Yuezhou Hu", "Haocheng Xi", "Jianfei Chen", "Jun Zhu"], "title": "SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning", "comment": null, "summary": "Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.", "AI": {"tldr": "SpargeAttention2\u662f\u4e00\u79cd\u53ef\u8bad\u7ec3\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u63a9\u7801\u89c4\u5219\u548c\u84b8\u998f\u5f0f\u5fae\u8c03\u76ee\u6807\uff0c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b095%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5ea6\u548c16.2\u500d\u7684\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u514d\u8d39\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u53ef\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7a00\u758f\u5ea6\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u7814\u7a76\u9700\u8981\u89e3\u51b3\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1aTop-k\u548cTop-p\u63a9\u7801\u89c4\u5219\u7684\u5931\u6548\u60c5\u51b5\u3001\u53ef\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u4e3a\u4f55\u80fd\u8fbe\u5230\u66f4\u9ad8\u7a00\u758f\u5ea6\u3001\u4ee5\u53ca\u6269\u6563\u635f\u5931\u5fae\u8c03\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faSpargeAttention2\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u7ed3\u5408Top-k\u548cTop-p\u7684\u6df7\u5408\u63a9\u7801\u89c4\u5219\uff0c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u63a9\u7801\uff1b(2) \u9ad8\u6548\u7684\u53ef\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u5b9e\u73b0\uff1b(3) \u84b8\u998f\u542f\u53d1\u7684\u5fae\u8c03\u76ee\u6807\uff0c\u5728\u7a00\u758f\u6ce8\u610f\u529b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u66f4\u597d\u5730\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "result": "\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cSpargeAttention2\u8fbe\u523095%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5ea6\u548c16.2\u500d\u7684\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "SpargeAttention2\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7a00\u758f\u5ea6\u4e0b\u7684\u9ad8\u6548\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13583", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13583", "abs": "https://arxiv.org/abs/2602.13583", "authors": ["Kun Gao", "Katsumi Inoue", "Yongzhi Cao", "Hanpin Wang", "Feng Yang"], "title": "Differentiable Rule Induction from Raw Sequence Inputs", "comment": "Accepted at ICLR 2025", "summary": "Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u53ef\u5fae\u5206\u805a\u7c7b\u548c\u65b0\u578b\u53ef\u5fae\u5206ILP\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u5b66\u4e60\u89c4\u5219\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53ef\u5fae\u5206ILP\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u663e\u5f0f\u6807\u7b7e\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u53ef\u5fae\u5206\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\uff08ILP\uff09\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7b26\u53f7\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u5b66\u4e60\u89c4\u5219\uff0c\u5b58\u5728\u663e\u5f0f\u6807\u7b7e\u6cc4\u6f0f\u95ee\u9898\u2014\u2014\u5373\u65e0\u6cd5\u5728\u6ca1\u6709\u8f93\u5165\u7279\u5f81\u6807\u7b7e\u663e\u5f0f\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5c06\u8fde\u7eed\u8f93\u5165\u6620\u5c04\u5230\u7b26\u53f7\u53d8\u91cf\u3002", "method": "\u96c6\u6210\u81ea\u76d1\u7763\u53ef\u5fae\u5206\u805a\u7c7b\u6a21\u578b\u4e0e\u65b0\u578b\u53ef\u5fae\u5206ILP\u6a21\u578b\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u5b66\u4e60\u89c4\u5219\uff0c\u907f\u514d\u663e\u5f0f\u6807\u7b7e\u6cc4\u6f0f\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u76f4\u89c2\u4e14\u7cbe\u786e\u5730\u4ece\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\u4e2d\u5b66\u4e60\u6cdb\u5316\u89c4\u5219\uff0c\u6709\u6548\u901a\u8fc7\u7279\u5f81\u63cf\u8ff0\u539f\u59cb\u6570\u636e\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u53ef\u5fae\u5206ILP\u4e2d\u7684\u663e\u5f0f\u6807\u7b7e\u6cc4\u6f0f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u5b66\u4e60\u53ef\u89e3\u91ca\u89c4\u5219\u7684\u80fd\u529b\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.13697", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13697", "abs": "https://arxiv.org/abs/2602.13697", "authors": ["Linjie Xu", "Yanlin Zhang", "Quan Gan", "Minjie Wang", "David Wipf"], "title": "No Need to Train Your RDB Foundation Model", "comment": null, "summary": "Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \\textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \\emph{within} high-dimensional RDB columns where all entities share units and roles, not \\textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\\footnote{\\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5904\u7406\u591a\u8868\u5173\u7cfb\u6570\u636e\u5e93\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5217\u5185\u538b\u7f29\u800c\u975e\u8de8\u5217\u538b\u7f29\u6765\u4fdd\u6301\u4fe1\u606f\u5b8c\u6574\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u5355\u8868\u57fa\u7840\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\u5305\u542b\u5927\u91cf\u5f02\u6784\u8868\u683c\u4fe1\u606f\u53ef\u7528\u4e8e\u9884\u6d4b\u5efa\u6a21\uff0c\u4f46\u4f01\u4e1a\u73af\u5883\u4e2d\u6f5c\u5728\u9884\u6d4b\u76ee\u6807\u4f17\u591a\uff0c\u9700\u8981\u907f\u514d\u6bcf\u6b21\u9884\u6d4b\u65b0\u76ee\u6807\u65f6\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002\u73b0\u6709\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u8868\u64cd\u4f5c\uff0c\u9700\u8981\u6269\u5c55\u5230\u591a\u8868\u5173\u7cfb\u6570\u636e\u5e93\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u539f\u5219\u6027\u7684RDB\u7f16\u7801\u5668\u5bb6\u65cf\uff0c\u5c06\u53ef\u53d8\u5927\u5c0f\u7684RDB\u90bb\u57df\u538b\u7f29\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684ICL\u6837\u672c\u3002\u5173\u952e\u521b\u65b0\u5728\u4e8e\u9650\u5236\u5728\u5171\u4eab\u5355\u4f4d\u548c\u89d2\u8272\u7684\u9ad8\u7ef4RDB\u5217\u5185\u8fdb\u884c\u538b\u7f29\uff0c\u800c\u4e0d\u662f\u8de8\u5217\u538b\u7f29\u3002\u7f16\u7801\u5668\u65e0\u9700\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u53ef\u4e0e\u73b0\u6709\u5355\u8868ICL\u57fa\u7840\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684SQL\u539f\u8bed\u5b9e\u73b0\u7f16\u7801\u9636\u6bb5\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90\u7684RDB\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7a33\u5065\u6027\u80fd\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u4f7f\u7528\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\uff0c\u5217\u5185\u538b\u7f29\u65b9\u6cd5\u4f18\u4e8e\u8de8\u5217\u538b\u7f29\u3002", "conclusion": "\u901a\u8fc7\u9650\u5236\u5728\u5171\u4eab\u5355\u4f4d\u548c\u89d2\u8272\u7684\u5217\u5185\u8fdb\u884c\u538b\u7f29\uff0c\u53ef\u4ee5\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u53c2\u6570\u7684\u5173\u7cfb\u6570\u636e\u5e93\u7f16\u7801\u5668\uff0c\u4e0e\u73b0\u6709\u5355\u8868\u4e0a\u4e0b\u6587\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u5b9e\u73b0\u5f00\u7bb1\u5373\u7528\u7684\u591a\u8868\u5173\u7cfb\u6570\u636e\u5e93\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2602.13712", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13712", "abs": "https://arxiv.org/abs/2602.13712", "authors": ["Chan Hao Sien", "Hezerul Abdul Karim", "Nouar AlDahoul"], "title": "Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images", "comment": null, "summary": "Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.", "AI": {"tldr": "\u4f7f\u7528\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Microsoft Florence\uff09\u5b9a\u4f4d\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u5bc4\u751f\u866b\u5375\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0cmIOU\u8fbe\u52300.94\uff0c\u4e3a\u81ea\u52a8\u5316\u5bc4\u751f\u866b\u8bca\u65ad\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u571f\u58e4\u4f20\u64ad\u8815\u866b\u611f\u67d3\u5728\u70ed\u5e26\u548c\u4e9a\u70ed\u5e26\u5730\u533a\u6301\u7eed\u5f71\u54cd\u5927\u91cf\u4eba\u53e3\uff0c\u8fd9\u4e9b\u5730\u533a\u7f3a\u4e4f\u4e13\u4e1a\u8bca\u65ad\u4e13\u5bb6\u3002\u4f20\u7edf\u624b\u52a8\u663e\u5fae\u955c\u8bca\u65ad\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982Microsoft Florence\uff09\u6765\u5b9a\u4f4d\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u6240\u6709\u5bc4\u751f\u866b\u5375\uff0c\u5e76\u4e0eEfficientDet\u7b49\u5176\u4ed6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9a\u4f4d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0cmIOU\u8fbe\u52300.94\uff0c\u663e\u793a\u51fa\u5728\u5bc4\u751f\u866b\u5375\u68c0\u6d4b\u65b9\u9762\u7684\u4f18\u8d8a\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u4f5c\u4e3a\u81ea\u52a8\u5316\u6846\u67b6\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u4e3a\u667a\u80fd\u5bc4\u751f\u866b\u5b66\u8bca\u65ad\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13804", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13804", "abs": "https://arxiv.org/abs/2602.13804", "authors": ["Vashista Nobaub"], "title": "Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees", "comment": "22 pages", "summary": "Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (\u0394) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\\exp(-\u03a9(\u0394/\\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.\n  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVashista\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u6ce8\u610f\u529b\u53ef\u96c6\u4e2d\u5728\u5c11\u91cf\u5173\u952etoken\u4e0a\uff0c\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6052\u5b9a\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u4e3b\u8981\u8ba1\u7b97\u6210\u672c\u96c6\u4e2d\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e0a\uff0c\u4f46\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u53ea\u6709\u5c11\u91cftoken\u5bf9\u6bcf\u4e2a\u67e5\u8be2\u6709\u5b9e\u8d28\u6027\u8d21\u732e\u3002\u9700\u8981\u4e00\u79cd\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u5f00\u53d1\u5b9e\u7528\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "1. \u7406\u8bba\u5efa\u6a21\uff1a\u5c06\u6ce8\u610f\u529b\u5efa\u6a21\u4e3a\u5728\u5173\u952e\u5411\u91cf\u51f8\u5305\u4e0a\u7684\u6295\u5f71\uff0c\u5206\u6790\u5176\u71b5\u677e\u5f1b\uff08softmax-like\uff09\u7279\u6027\uff1b2. \u63d0\u51fa\u9762\u7a33\u5b9a\u6027\u5b9a\u7406\uff1a\u5728\u4e25\u683c\u4e92\u8865\u8fb9\u9645\u6761\u4ef6\u4e0b\uff0c\u71b5\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u6052\u5b9a\u5927\u5c0f\u7684\u6d3b\u8dc3\u9762\u4e0a\uff1b3. \u5f00\u53d1Vashista\u7a00\u758f\u6ce8\u610f\u529b\uff1a\u57fa\u4e8e\u5206\u9875\u5f0f\u4e0a\u4e0b\u6587\u9009\u62e9\u7b56\u7565\u7ef4\u62a4\u6bcf\u4e2a\u67e5\u8be2\u7684\u5c0f\u5019\u9009\u96c6\uff0c\u4e0e\u73b0\u4ee3\u63a8\u7406\u6808\u517c\u5bb9\u3002", "result": "1. \u7406\u8bba\u7ed3\u679c\uff1a\u8bc1\u660e\u975e\u6d3b\u8dc3token\u7684\u603b\u8d28\u91cf\u4ee5\u6307\u6570\u901f\u5ea6\u8870\u51cf\uff0c\u6d3b\u8dc3\u9762\u4e0a\u7684\u8bef\u5dee\u968f\u6e29\u5ea6\u53c2\u6570\u7ebf\u6027\u7f29\u653e\uff1b2. \u5b9e\u8df5\u6548\u679c\uff1a\u5728\u957f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u4e2d\u89c2\u5bdf\u5230\u7a33\u5b9a\u7684\u6052\u5b9a\u5927\u5c0f\u6709\u6548\u652f\u6301\u3001\u663e\u8457\u7684\u65f6\u949f\u901f\u5ea6\u63d0\u5347\uff0c\u4ee5\u53ca\u5728\u652f\u6301\u95f4\u9699\u8bca\u65ad\u9884\u6d4b\u7684\u533a\u57df\u5185\u8d28\u91cf\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7a00\u758f\u957f\u4e0a\u4e0b\u6587\u89e3\u7801\u63d0\u4f9b\u4e86\u5b89\u5168\u5224\u636e\u548c\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6743\u8861\u7684\u7406\u8bba\u57fa\u7840\uff0cVashista\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9884\u6d4b\u7684\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u548c\u9694\u79bb\u73af\u5883\u3002"}}
{"id": "2602.13818", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13818", "abs": "https://arxiv.org/abs/2602.13818", "authors": ["Zongcheng Han", "Dongyan Cao", "Haoran Sun", "Yu Hong"], "title": "VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer", "comment": null, "summary": "Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.", "AI": {"tldr": "VAR-3D\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u56fe\u611f\u77e5\u76843D\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668\u548c\u6e32\u67d3\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u52303D\u751f\u6210\u7684\u8d28\u91cf\u548c\u6587\u672c\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u81ea\u56de\u5f52\u53d8\u6362\u5668\u7684\u6587\u672c\u52303D\u751f\u6210\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u5728\u79bb\u65633D\u8868\u793a\u5b66\u4e60\u4e2d\u5b58\u5728\u4fe1\u606f\u635f\u5931\uff0c\u5bfc\u81f4\u91cf\u5316\u524d\u7684\u8868\u793a\u5931\u771f\uff1b2\uff09\u5411\u91cf\u91cf\u5316\u8fdb\u4e00\u6b65\u653e\u5927\u5931\u771f\uff0c\u964d\u4f4e\u51e0\u4f55\u4e00\u81f4\u6027\uff1b3\uff09\u4f20\u7edf\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u5728\u91cd\u5efa\u548c\u6587\u672c\u6761\u4ef6\u751f\u6210\u4e4b\u95f4\u5b58\u5728\u76ee\u6807\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e86VAR-3D\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u89c6\u56fe\u611f\u77e5\u76843D\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5c06\u590d\u67423D\u51e0\u4f55\u7ed3\u6784\u8f6c\u6362\u4e3a\u79bb\u6563\u6807\u8bb0\uff1b2\uff09\u6e32\u67d3\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u79bb\u6563\u6807\u8bb0\u9884\u6d4b\u4e0e\u89c6\u89c9\u91cd\u5efa\u8026\u5408\uff0c\u9f13\u52b1\u751f\u6210\u8fc7\u7a0b\u66f4\u597d\u5730\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVAR-3D\u5728\u751f\u6210\u8d28\u91cf\u548c\u6587\u672c-3D\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VAR-3D\u901a\u8fc7\u89c6\u56fe\u611f\u77e5\u76843D\u8868\u793a\u5b66\u4e60\u548c\u6e32\u67d3\u76d1\u7763\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u52303D\u751f\u6210\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u548c\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u76843D\u751f\u6210\u548c\u66f4\u597d\u7684\u6587\u672c\u5bf9\u9f50\u3002"}}
{"id": "2602.13873", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13873", "abs": "https://arxiv.org/abs/2602.13873", "authors": ["Harris Abdul Majid", "Giannis Daras", "Francesco Tudisco", "Steven McDonagh"], "title": "Ambient Physics: Training Neural PDE Solvers with Partial Observations", "comment": null, "summary": "In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish \"truly unobserved\" from \"artificially unobserved\", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\\%$ reduction in average overall error while using 125$\\times$ fewer function evaluations. We also identify a \"one-point transition\": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.", "AI": {"tldr": "Ambient Physics\u6846\u67b6\u76f4\u63a5\u4ece\u90e8\u5206\u89c2\u6d4b\u4e2d\u5b66\u4e60PDE\u7cfb\u6570-\u89e3\u5bf9\u7684\u8054\u5408\u5206\u5e03\uff0c\u65e0\u9700\u5b8c\u6574\u89c2\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u968f\u673a\u63a9\u7801\u5df2\u89c2\u6d4b\u70b9\u5b9e\u73b0\u8bad\u7ec3\uff0c\u5728\u91cd\u5efa\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8bb8\u591a\u79d1\u5b66\u573a\u666f\u4e2d\uff0c\u83b7\u53d6PDE\u7cfb\u6570\u548c\u89e3\u7684\u5b8c\u6574\u89c2\u6d4b\u6570\u636e\u6210\u672c\u9ad8\u6602\u3001\u5371\u9669\u751a\u81f3\u4e0d\u53ef\u80fd\u3002\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u89c2\u6d4b\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u9650\u5236\u3002", "method": "\u63d0\u51faAmbient Physics\u6846\u67b6\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u968f\u673a\u63a9\u7801\u5df2\u89c2\u6d4b\u6d4b\u91cf\u503c\u7684\u5b50\u96c6\u5e76\u5bf9\u5176\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002\u8fd9\u6837\u6a21\u578b\u65e0\u6cd5\u533a\u5206\"\u771f\u6b63\u672a\u89c2\u6d4b\"\u548c\"\u4eba\u5de5\u672a\u89c2\u6d4b\"\u70b9\uff0c\u4ece\u800c\u5fc5\u987b\u5728\u6240\u6709\u4f4d\u7f6e\u4ea7\u751f\u5408\u7406\u7684\u9884\u6d4b\u3002", "result": "Ambient Physics\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u6027\u80fd\uff1a\u76f8\u6bd4\u4e4b\u524d\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u5e73\u5747\u603b\u4f53\u8bef\u5dee\u964d\u4f4e62.51%\uff0c\u540c\u65f6\u4f7f\u7528125\u500d\u66f4\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u3002\u8fd8\u53d1\u73b0\u4e86\"\u5355\u70b9\u8f6c\u6362\"\u73b0\u8c61\uff1a\u63a9\u7801\u5355\u4e2a\u5df2\u89c2\u6d4b\u70b9\u5373\u53ef\u5728\u4e0d\u540c\u67b6\u6784\u548c\u6d4b\u91cf\u6a21\u5f0f\u4e0b\u4ece\u90e8\u5206\u89c2\u6d4b\u4e2d\u5b66\u4e60\u3002", "conclusion": "Ambient Physics\u6846\u67b6\u4f7f\u5f97\u5728\u65e0\u6cd5\u83b7\u5f97\u5b8c\u6574\u89c2\u6d4b\u6570\u636e\u7684\u79d1\u5b66\u573a\u666f\u4e2d\u53d6\u5f97\u8fdb\u5c55\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u90e8\u5206\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u7269\u7406\u573a\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13889", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13889", "abs": "https://arxiv.org/abs/2602.13889", "authors": ["Daniel Chen", "Zaria Zinn", "Marcus Lowe"], "title": "Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification", "comment": null, "summary": "We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b57\u4f53\u5206\u7c7b\u7cfb\u7edf\uff0c\u4f7f\u7528LoRA\u5fae\u8c03DINOv2 Vision Transformer\uff0c\u4ec5\u8bad\u7ec3\u4e0d\u52301%\u7684\u53c2\u6570\u5c31\u80fd\u5728394\u79cd\u5b57\u4f53\u4e0a\u8fbe\u5230\u7ea686%\u7684top-1\u51c6\u786e\u7387\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u4ece\u6e32\u67d3\u6587\u672c\u56fe\u50cf\u4e2d\u51c6\u786e\u8bc6\u522b\u5927\u91cf\u5b57\u4f53\u5bb6\u65cf\u7684\u5b9e\u7528\u7cfb\u7edf\uff0c\u89e3\u51b3\u5b57\u4f53\u5206\u7c7b\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u90e8\u7f72\u95ee\u9898\u3002", "method": "\u4f7f\u7528LoRA\uff08\u4f4e\u79e9\u9002\u5e94\uff09\u5fae\u8c03DINOv2 Vision Transformer\uff1b\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u7ba1\u9053\uff0c\u5927\u89c4\u6a21\u6e32\u67d3Google\u5b57\u4f53\u5e76\u5e94\u7528\u591a\u79cd\u589e\u5f3a\uff08\u968f\u673a\u989c\u8272\u3001\u5bf9\u9f50\u3001\u6362\u884c\u3001\u9ad8\u65af\u566a\u58f0\uff09\uff1b\u5185\u7f6e\u9884\u5904\u7406\u786e\u4fdd\u8bad\u7ec3\u548c\u63a8\u7406\u4e00\u81f4\u6027\uff1b\u90e8\u7f72\u4e3aHuggingFace\u63a8\u7406\u7aef\u70b9\u3002", "result": "\u5728394\u79cd\u5b57\u4f53\u5bb6\u65cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ea686%\u7684top-1\u51c6\u786e\u7387\uff0c\u4ec5\u8bad\u7ec3\u4e86\u6a21\u578b87.2M\u53c2\u6570\u4e2d\u4e0d\u52301%\u7684\u53c2\u6570\uff08\u7ea60.87M\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5fae\u8c03\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u7684\u5b57\u4f53\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u5c06\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u7ba1\u9053\u5f00\u6e90\uff0c\u4e3a\u5b57\u4f53\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u8d44\u6e90\u3002"}}
{"id": "2602.13935", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13935", "abs": "https://arxiv.org/abs/2602.13935", "authors": ["Yangxinyu Xie", "Tao Wang", "Soham Mallick", "Yan Sun", "Georgy Noarov", "Mengxin Yu", "Tanwi Mallick", "Weijie J. Su", "Edgar Dobriban"], "title": "Statistical Early Stopping for Reasoning Models", "comment": null, "summary": "While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u7edf\u8ba1\u539f\u7406\u7684\u65e9\u505c\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u6765\u51cf\u5c11LLM\u5728\u6a21\u7cca\u67e5\u8be2\u4e0b\u7684\u8fc7\u5ea6\u63a8\u7406\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u867d\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u6709\u65f6\u4f1a\u8fc7\u5ea6\u601d\u8003\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0d\u786e\u5b9a\u3001\u8868\u8ff0\u4e0d\u6e05\u6216\u6a21\u7cca\u7684\u67e5\u8be2\u65f6\uff0c\u4f1a\u751f\u6210\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u7edf\u8ba1\u539f\u7406\u7684\u65e9\u505c\u65b9\u6cd5\uff1a1) \u53c2\u6570\u5316\u65b9\u6cd5\uff1a\u5c06\u4e0d\u786e\u5b9a\u6027\u5173\u952e\u8bcd\u7684\u51fa\u73b0\u95f4\u9694\u65f6\u95f4\u5efa\u6a21\u4e3a\u66f4\u65b0\u8fc7\u7a0b\uff0c\u5e76\u5e94\u7528\u5e8f\u5217\u6d4b\u8bd5\u8fdb\u884c\u505c\u6b62\u51b3\u7b56\uff1b2) \u975e\u53c2\u6570\u5316\u65b9\u6cd5\uff1a\u4e3a\u5b9a\u4e49\u660e\u786e\u7684\u67e5\u8be2\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u786e\u4fdd\u4e0d\u4f1a\u8fc7\u65e9\u505c\u6b62\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u548c\u6a21\u578b\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u65e9\u505c\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u63d0\u9ad8LLM\u63a8\u7406\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u89c2\u5bdf\u5230\u7279\u522b\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u76d1\u63a7\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u7684\u7edf\u8ba1\u65e9\u505c\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11LLM\u7684\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u63d0\u5347LLM\u63a8\u7406\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13980", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13980", "abs": "https://arxiv.org/abs/2602.13980", "authors": ["Guojie Liu", "Yiqi Wang", "Yanfeng Yang", "Wenqi Fan", "Songlei Jian", "Jianfeng Zhang", "Jie Yu"], "title": "Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking", "comment": null, "summary": "Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\\% in F1 score and 40.7\\% in EM score on QA tasks at the $64\\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\\%.", "AI": {"tldr": "PIC\uff08\u5e76\u884c\u8fed\u4ee3\u538b\u7f29\uff09\u901a\u8fc7\u4fee\u6539Transformer\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u5c06\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u4e3a\u5c40\u90e8\u5757\u8bb0\u5fc6\u5d4c\u5165\uff0c\u964d\u4f4e\u8bad\u7ec3\u96be\u5ea6\uff0c\u5728QA\u7b49\u9ad8\u538b\u7f29\u6bd4\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u957f\u4e0a\u4e0b\u6587\u663e\u8457\u589e\u52a0LLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u73b0\u6709\u8f6f\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u6355\u83b7\u5168\u5c40\u4f9d\u8d56\u5e76\u4f9d\u8d56\u5927\u91cf\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u8bad\u7ec3\u96be\u5ea6\u5927", "method": "\u53d7\u4eba\u7c7b\u5de5\u4f5c\u8bb0\u5fc6\u5206\u5757\u673a\u5236\u542f\u53d1\uff0c\u901a\u8fc7\u4fee\u6539Transformer\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u5c06\u8bb0\u5fc6\u5d4c\u5165\u7684\u63a5\u6536\u57df\u9650\u5236\u5728\u987a\u5e8f\u5c40\u90e8\u5757\u4e2d\uff0c\u964d\u4f4e\u538b\u7f29\u5668\u8bad\u7ec3\u96be\u5ea6", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9ad8\u538b\u7f29\u6bd4\u573a\u666f\u4f18\u52bf\u663e\u8457\uff0864\u00d7\u538b\u7f29\u6bd4\u4e0bQA\u4efb\u52a1F1\u63d0\u534729.8%\uff0cEM\u63d0\u534740.7%\uff09\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u7ea640%", "conclusion": "PIC\u901a\u8fc7\u5c40\u90e8\u5316\u538b\u7f29\u7b56\u7565\u6709\u6548\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347"}}
{"id": "2602.14038", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14038", "abs": "https://arxiv.org/abs/2602.14038", "authors": ["Mingfei Lu", "Mengjia Wu", "Feng Liu", "Jiawei Xu", "Weikai Li", "Haoyang Wang", "Zhengdong Hu", "Ying Ding", "Yizhou Sun", "Jie Lu", "Yi Zhang"], "title": "Choosing How to Remember: Adaptive Memory Structures for LLM Agents", "comment": null, "summary": "Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.", "AI": {"tldr": "FluxMem\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ec4\u7ec7\u6846\u67b6\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u591a\u79cd\u8bb0\u5fc6\u7ed3\u6784\uff0c\u901a\u8fc7\u5b66\u4e60\u4ea4\u4e92\u7279\u5f81\u9009\u62e9\u6700\u4f73\u8bb0\u5fc6\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u4e09\u5c42\u8bb0\u5fc6\u5c42\u6b21\u548c\u6982\u7387\u95e8\u63a7\u8fdb\u884c\u8bb0\u5fc6\u878d\u5408\uff0c\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u91c7\u7528\u4e00\u5200\u5207\u7684\u8bb0\u5fc6\u7ed3\u6784\uff0c\u4e14\u672a\u5c06\u8bb0\u5fc6\u7ed3\u6784\u9009\u62e9\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u8fd9\u9650\u5236\u4e86\u5904\u7406\u5f02\u6784\u4ea4\u4e92\u6a21\u5f0f\u7684\u80fd\u529b\u5e76\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFluxMem\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u667a\u80fd\u4f53\u914d\u5907\u591a\u79cd\u4e92\u8865\u8bb0\u5fc6\u7ed3\u6784\uff0c\u57fa\u4e8e\u4ea4\u4e92\u7ea7\u7279\u5f81\u5b66\u4e60\u9009\u62e9\u6700\u4f73\u7ed3\u6784\uff08\u4f7f\u7528\u4e0b\u6e38\u54cd\u5e94\u8d28\u91cf\u548c\u8bb0\u5fc6\u5229\u7528\u7387\u7684\u79bb\u7ebf\u76d1\u7763\uff09\u3002\u5f15\u5165\u4e09\u5c42\u8bb0\u5fc6\u5c42\u6b21\u548c\u57fa\u4e8eBeta\u6df7\u5408\u6a21\u578b\u7684\u6982\u7387\u95e8\u63a7\u8fdb\u884c\u5206\u5e03\u611f\u77e5\u8bb0\u5fc6\u878d\u5408\uff0c\u66ff\u4ee3\u8106\u5f31\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\u3002", "result": "\u5728PERSONAMEM\u548cLoCoMo\u4e24\u4e2a\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5206\u522b\u63d0\u5347\u4e869.18%\u548c6.14%\u7684\u6027\u80fd\u3002", "conclusion": "FluxMem\u901a\u8fc7\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ec4\u7ec7\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.14130", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14130", "abs": "https://arxiv.org/abs/2602.14130", "authors": ["Kazuo Yano", "Jonghyeok Lee", "Tae Ishitomi", "Hironobu Kawaguchi", "Akira Koyama", "Masakuni Ota", "Yuki Ota", "Nobuo Sato", "Keita Shimada", "Sho Takematsu", "Ayaka Tobinai", "Satomi Tsuji", "Kazunori Yanagi", "Keiko Yano", "Manabu Harada", "Yuki Matsuda", "Kazunori Matsumoto", "Kenichi Matsumura", "Hamae Matsuo", "Yumi Miyazaki", "Kotaro Murai", "Tatsuya Ohshita", "Marie Seki", "Shun Tanoue", "Tatsuki Terakado", "Yuko Ichimaru", "Mirei Saito", "Akihiro Otsuka", "Koji Ara"], "title": "Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4ee3\u6570\u91cf\u5b50\u667a\u80fd(AQI)\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u4ea4\u6362\u4ee3\u6570\u7ed3\u6784\u6269\u5c55LLMs\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u89e3\u51b3\u5176\u521b\u9020\u6027\u53d7\u9650\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u521b\u610f\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6d41\u7545\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u771f\u6b63\u7684\u521b\u9020\u6027\u8f93\u51fa\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u9650\u5236\u6e90\u4e8eLLMs\u7684\u7ed3\u6784\u7279\u6027\uff1a\u5f53\u63d0\u4f9b\u4e30\u5bcc\u4e0a\u4e0b\u6587\u65f6\uff0c\u672a\u6765\u751f\u6210\u7a7a\u95f4\u53d7\u5230\u5f3a\u70c8\u7ea6\u675f\uff0c\u751f\u6210\u8fc7\u7a0b\u63a5\u8fd1\u786e\u5b9a\u6027\u52a8\u6001\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6d4b\u8bd5\u65f6\u7f29\u653e\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u672a\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u8fd9\u79cd\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4ee3\u6570\u91cf\u5b50\u667a\u80fd(AQI)\u4f5c\u4e3a\u8ba1\u7b97\u6846\u67b6\uff0c\u91c7\u7528\u53d7\u91cf\u5b50\u7406\u8bba\u542f\u53d1\u7684\u975e\u4ea4\u6362\u4ee3\u6570\u7ed3\u6784\uff0c\u5b9e\u73b0\u987a\u5e8f\u4f9d\u8d56\u3001\u5e72\u6d89\u548c\u4e0d\u786e\u5b9a\u6027\u7b49\u7279\u6027\u3002\u8bed\u4e49\u72b6\u6001\u8868\u793a\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\uff0c\u5176\u6f14\u5316\u7531\u975e\u4ea4\u6362\u7b97\u5b50\u8ba1\u7b97\u7684C\u503c\u63a7\u5236\uff0c\u786e\u4fdd\u591a\u4e2a\u672a\u6765\u8bed\u4e49\u53ef\u80fd\u6027\u7684\u5171\u5b58\u548c\u6269\u5c55\u3002\u901a\u8fc7\u6269\u5c55\u57fa\u4e8eTransformer\u7684LLM\uff0c\u5f15\u5165600\u591a\u4e2a\u4e13\u7528\u7b97\u5b50\u6765\u5b9e\u73b0AQI\u3002", "result": "\u5728\u6db5\u76d6\u5341\u4e2a\u9886\u57df\u7684\u521b\u610f\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528LLM-as-a-judge\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793aAQI\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u4ea7\u751f\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\u5e76\u964d\u4f4e\u8de8\u9886\u57df\u65b9\u5dee\u3002\u975e\u4ea4\u6362\u4ee3\u6570\u52a8\u6001\u53ef\u4f5c\u4e3a\u673a\u5668\u521b\u9020\u6027\u7684\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u57fa\u7840\u3002", "conclusion": "\u975e\u4ea4\u6362\u4ee3\u6570\u52a8\u6001\u80fd\u591f\u4e3a\u673a\u5668\u521b\u9020\u6027\u63d0\u4f9b\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002\u8be5\u67b6\u6784\u5df2\u5728\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.14134", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14134", "abs": "https://arxiv.org/abs/2602.14134", "authors": ["Yi Li", "Hongze Shen", "Lexiang Tang", "Xin Li", "Xinpeng Ding", "Yinsong Liu", "Deqiang Jiang", "Xing Sun", "Xiaomeng Li"], "title": "DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors", "comment": "25 pages, 9 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.", "AI": {"tldr": "DenseMLLM\uff1a\u4e00\u79cd\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9token\u76d1\u7763\u7b56\u7565\u5b9e\u73b0\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1", "motivation": "\u5f53\u524dMLLMs\u5728\u7ec6\u7c92\u5ea6\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u8bed\u4e49\u5206\u5272\u3001\u6df1\u5ea6\u4f30\u8ba1\uff09\u4e2d\u9700\u8981\u590d\u6742\u7684\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\uff0c\u8fd9\u589e\u52a0\u4e86\u6a21\u578b\u590d\u6742\u6027\uff0c\u504f\u79bb\u4e86MLLMs\u7684\u901a\u7528\u8bbe\u8ba1\u7406\u5ff5\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027", "method": "\u63d0\u51faDenseMLLM\u6a21\u578b\uff0c\u57fa\u4e8e\u6807\u51c6\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u9896\u7684\u89c6\u89c9token\u76d1\u7763\u7b56\u7565\u6765\u5904\u7406\u591a\u6807\u7b7e\u548c\u591a\u4efb\u52a1\uff0c\u65e0\u9700\u989d\u5916\u7684\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668", "result": "\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u5bc6\u96c6\u9884\u6d4b\u548c\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd", "conclusion": "\u6807\u51c6\u7684\u901a\u7528MLLM\u65e0\u9700\u67b6\u6784\u4e13\u4e1a\u5316\u5373\u53ef\u6709\u6548\u652f\u6301\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5668\u7684\u8303\u5f0f"}}
{"id": "2602.14157", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14157", "abs": "https://arxiv.org/abs/2602.14157", "authors": ["Ahmed Ghorbel", "Badr Moufad", "Navid Bagheri Shouraki", "Alain Oliviero Durmus", "Thomas Hirtz", "Eric Moulines", "Jimmy Olsson", "Yazid Janati"], "title": "When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance", "comment": "Preprint", "summary": "Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.", "AI": {"tldr": "\u57fa\u4e8eMoufad\u7b49\u4eba(2025)\u7684\u5de5\u4f5c\uff0c\u672c\u6587\u6269\u5c55\u4e86\u65e0\u9700\u5411\u91cf-\u96c5\u53ef\u6bd4\u4e58\u79ef(VJP)\u8ba1\u7b97\u7684\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u53ef\u4ee5\u81ea\u7136\u5730\u8f6c\u5316\u4e3a\u4fee\u590d\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5411\u91cf-\u96c5\u53ef\u6bd4\u4e58\u79ef(VJP)\u8ba1\u7b97\u6765\u8fd1\u4f3c\u96be\u4ee5\u5904\u7406\u7684\u5f15\u5bfc\u9879\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u57fa\u4e8eMoufad\u7b49\u4eba(2025)\u7684VJP-free\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u5e76\u5927\u5e45\u6269\u5c55\u5176\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\u57fa\u51c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u5355\u72ec\u5c31\u80fd\u8fbe\u5230\u4e0e\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8d8a\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u5728\u65e0\u9700\u6602\u8d35VJP\u8ba1\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u6587\u672c\u9a71\u52a8\u7684\u56fe\u50cf\u548c\u89c6\u9891\u7f16\u8f91\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14229", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14229", "abs": "https://arxiv.org/abs/2602.14229", "authors": ["Abubakarr Jaye", "Nigel Boachie Kumankumah", "Chidera Biringa", "Anjel Shaileshbhai Patel", "Sulaiman Vesal", "Dayquan Julienne", "Charlotte Siska", "Manuel Ra\u00fal Mel\u00e9ndez Luj\u00e1n", "Anthony Twum-Barimah", "Mauricio Velazco", "Tianwei Chen"], "title": "CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments", "comment": null, "summary": "Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMulti-Horizon Task Environments\uff08MHTEs\uff09\u65b0\u95ee\u9898\u7c7b\u522b\uff0c\u9488\u5bf9\u591a\u5e76\u53d1\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5f00\u53d1CorpGen\u6846\u67b6\u89e3\u51b3\u56db\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5728OSWorld Office\u4e0a\u5b9e\u73b03.5\u500d\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u5b64\u7acb\u5355\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u800c\u771f\u5b9e\u7ec4\u7ec7\u5de5\u4f5c\u9700\u8981\u7ba1\u7406\u8bb8\u591a\u5e76\u53d1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u6d89\u53ca\u4efb\u52a1\u4ea4\u9519\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u4f18\u5148\u7ea7\u91cd\u6392\u3002\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6a21\u62df\u8fd9\u79cd\u590d\u6742\u573a\u666f\u3002", "method": "\u63d0\u51faCorpGen\u67b6\u6784\u65e0\u5173\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u89c4\u5212\u5b9e\u73b0\u591a\u65f6\u7a0b\u76ee\u6807\u5bf9\u9f50\uff0c\u5b50\u667a\u80fd\u4f53\u9694\u79bb\u9632\u6b62\u4efb\u52a1\u4ea4\u53c9\u6c61\u67d3\uff0c\u5206\u5c42\u5185\u5b58\uff08\u5de5\u4f5c\u3001\u7ed3\u6784\u5316\u3001\u8bed\u4e49\uff09\u548c\u81ea\u9002\u5e94\u6458\u8981\u3002\u901a\u8fc7\u5177\u6709\u6301\u4e45\u8eab\u4efd\u548c\u771f\u5b9e\u65e5\u7a0b\u7684\u6570\u5b57\u5458\u5de5\u6a21\u62df\u4f01\u4e1a\u73af\u5883\u3002", "result": "\u5728OSWorld Office\u4e0a\uff0cCorpGen\u5728\u4e09\u4e2aCUA\u540e\u7aef\uff08UFO2\u3001OpenAI CUA\u3001\u5206\u5c42\uff09\u4e0a\u5b9e\u73b0\u9ad8\u8fbe3.5\u500d\u6539\u8fdb\uff0815.2% vs 4.3%\uff09\uff0c\u5728\u8d1f\u8f7d\u589e\u52a0\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u7ecf\u9a8c\u5b66\u4e60\u63d0\u4f9b\u6700\u5927\u6536\u76ca\u3002", "conclusion": "CorpGen\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65f6\u7a0b\u4efb\u52a1\u73af\u5883\u4e2d\u7684\u56db\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\u67b6\u6784\u673a\u5236\u800c\u975e\u7279\u5b9aCUA\u5b9e\u73b0\u3002\u8be5\u5de5\u4f5c\u4e3a\u8bc4\u4f30\u548c\u7ba1\u7406\u590d\u6742\u7ec4\u7ec7\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2602.14236", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.14236", "abs": "https://arxiv.org/abs/2602.14236", "authors": ["Vishnu Sai", "Dheeraj Sai", "Srinath B", "Girish Varma", "Priyesh Shukla"], "title": "Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.", "AI": {"tldr": "Sali-Cache\u901a\u8fc7\u5148\u9a8c\u4f18\u5316\u6846\u67b6\u89e3\u51b3VLMs\u5904\u7406\u957f\u89c6\u9891\u65f6\u7684KV\u7f13\u5b58\u5185\u5b58\u74f6\u9888\uff0c\u4f7f\u7528\u65f6\u7a7a\u53cc\u4fe1\u53f7\u81ea\u9002\u5e94\u7f13\u5b58\uff0c\u5728\u4fdd\u6301100%\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b02.20\u500d\u5185\u5b58\u538b\u7f29", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u89c6\u9891\u5185\u5bb9\u65f6\u9762\u4e34\u5173\u952e\u7684\u5185\u5b58\u74f6\u9888\uff0cKV\u7f13\u5b58\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u91c7\u7528\u53cd\u5e94\u5f0f\u6dd8\u6c70\u7b56\u7565\uff0c\u5728\u8ba1\u7b97\u5b8c\u6574\u6ce8\u610f\u529b\u77e9\u9635\u540e\u4e22\u5f03token\uff0c\u9020\u6210\u5927\u91cf\u8ba1\u7b97\u6d6a\u8d39\u3002", "method": "\u63d0\u51faSali-Cache\u5148\u9a8c\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u5185\u5b58\u7ba1\u7406\u5b9e\u73b0\u53cc\u4fe1\u53f7\u81ea\u9002\u5e94\u7f13\u5b58\uff1a1)\u57fa\u4e8e\u5149\u6d41\u5206\u6790\u7684\u65f6\u95f4\u6ee4\u6ce2\u5668\u68c0\u6d4b\u5e27\u95f4\u5197\u4f59\uff1b2)\u57fa\u4e8e\u663e\u8457\u6027\u68c0\u6d4b\u7684\u7a7a\u95f4\u6ee4\u6ce2\u5668\u8bc6\u522b\u89c6\u89c9\u663e\u8457\u533a\u57df\u3002\u5728\u8fdb\u5165\u8ba1\u7b97\u6602\u8d35\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u524d\u667a\u80fd\u7ba1\u7406\u5185\u5b58\u5206\u914d\u3002", "result": "\u5728LLaVA 1.6\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301BLEU\u3001ROUGE-L\u548cExact Match\u6307\u6807100%\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u5185\u5b58\u4f7f\u75282.20\u500d\u7684\u538b\u7f29\u6bd4\u3002\u5728\u76f8\u540c\u5185\u5b58\u9884\u7b97\u7ea6\u675f\u4e0b\uff0cSali-Cache\u80fd\u5728\u4e0d\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u66f4\u957f\u65f6\u95f4\u5185\u4fdd\u7559\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u7279\u5f81\u3002", "conclusion": "Sali-Cache\u901a\u8fc7\u5148\u9a8c\u4f18\u5316\u548c\u81ea\u9002\u5e94\u7f13\u5b58\u7ba1\u7406\uff0c\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u9ad8\u6548\u5904\u7406\u957f\u89c6\u9891\u5185\u5bb9\uff0c\u89e3\u51b3\u4e86KV\u7f13\u5b58\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u4e0d\u53d8\u3002"}}
{"id": "2602.14307", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14307", "abs": "https://arxiv.org/abs/2602.14307", "authors": ["Samuele Marro", "Jialin Yu", "Emanuele La Malfa", "Oishi Deb", "Jiawei Li", "Yibo Yang", "Ebey Abraham", "Sunando Sengupta", "Eric Sommerlade", "Michael Wooldridge", "Philip Torr"], "title": "Benchmarking at the Edge of Comprehension", "comment": null, "summary": "As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.", "AI": {"tldr": "\u63d0\u51fa\"\u6279\u5224\u6027\u6297\u6027\u57fa\u51c6\u6d4b\u8bd5\"\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u751f\u6210-\u8bc4\u4f30\u6e38\u620f\u89e3\u51b3AI\u6a21\u578b\u8d85\u8d8a\u4eba\u7c7b\u7406\u89e3\u80fd\u529b\u540e\u7684\u57fa\u51c6\u6d4b\u8bd5\u96be\u9898", "motivation": "\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u9971\u548c\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f53\u6a21\u578b\u80fd\u529b\u8d85\u8d8a\u4eba\u7c7b\u7406\u89e3\u65f6\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u6311\u6218\uff1a\u4eba\u7c7b\u96be\u4ee5\u751f\u6210\u533a\u5206\u6027\u4efb\u52a1\u3001\u63d0\u4f9b\u51c6\u786e\u7b54\u6848\u6216\u8bc4\u4f30\u590d\u6742\u89e3\u51b3\u65b9\u6848\u3002\u5982\u679c\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u53ef\u884c\uff0c\u6211\u4eec\u5c06\u65e0\u6cd5\u8861\u91cfAI\u8fdb\u5c55", "method": "\u63d0\u51fa\u6279\u5224\u6027\u6297\u6027\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u57fa\u4e8e\"\u6279\u5224\u6027\u6297\u6027\u6b63\u786e\u6027\"\u6982\u5ff5\uff1a\u7b54\u6848\u88ab\u8ba4\u4e3a\u662f\u6b63\u786e\u7684\uff0c\u9664\u975e\u5bf9\u624b\u80fd\u4ee4\u4eba\u4fe1\u670d\u5730\u8bc1\u660e\u5176\u9519\u8bef\u3002\u4eba\u7c7b\u4f5c\u4e3a\u6709\u9650\u9a8c\u8bc1\u8005\uff0c\u4e13\u6ce8\u4e8e\u5c40\u90e8\u58f0\u660e\u800c\u975e\u5b8c\u5168\u7406\u89e3\u4efb\u52a1\u3002\u4f7f\u7528\u9879\u76ee\u5316\u4e8c\u5206Bradley-Terry\u6a21\u578b\u8054\u5408\u6392\u540dLLMs\u7684\u89e3\u9898\u80fd\u529b\u548c\u751f\u6210\u96be\u9898\u80fd\u529b", "result": "\u5728\u6570\u5b66\u9886\u57df\u5bf9\u516b\u4e2a\u524d\u6cbfLLMs\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u6240\u5f97\u5206\u6570\u7a33\u5b9a\u4e14\u4e0e\u5916\u90e8\u80fd\u529b\u6d4b\u91cf\u76f8\u5173\u3002\u6846\u67b6\u5c06\u57fa\u51c6\u6d4b\u8bd5\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bf9\u6297\u6027\u751f\u6210-\u8bc4\u4f30\u6e38\u620f\uff0c\u4eba\u7c7b\u4f5c\u4e3a\u6700\u7ec8\u88c1\u51b3\u8005", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\"\u540e\u7406\u89e3\u673a\u5236\"\u4e0b\u7684\u57fa\u51c6\u6d4b\u8bd5\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5373\u4f7f\u4eba\u7c7b\u65e0\u6cd5\u5b8c\u5168\u7406\u89e3\u4efb\u52a1\uff0c\u4e5f\u80fd\u4fdd\u6301\u8bc4\u4f30\u5b8c\u6574\u6027\uff0c\u786e\u4fddAI\u8fdb\u5c55\u7684\u53ef\u6d4b\u91cf\u6027"}}
{"id": "2602.14505", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14505", "abs": "https://arxiv.org/abs/2602.14505", "authors": ["Dennis Gross"], "title": "Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC", "comment": null, "summary": "Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.", "AI": {"tldr": "COOL-MC\u662f\u4e00\u4e2a\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u548c\u9a8c\u8bc1\u533b\u7597\u9886\u57df\uff08\u7279\u522b\u662f\u8113\u6bd2\u75c7\u6cbb\u7597\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u8fbe\u72b6\u6001\u7a7a\u95f4\u3001\u4e34\u5e8a\u6807\u7b7e\u548c\u89e3\u91ca\u6027\u5206\u6790\u6765\u63d0\u9ad8\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u5728\u533b\u7597\u9886\u57df\uff0c\u7279\u522b\u662f\u8113\u6bd2\u75c7\u6cbb\u7597\u4f18\u5316\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u901a\u5e38\u662f\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u9a8c\u8bc1\u7684\u3002\u4f20\u7edf\u7684\u6982\u7387\u6a21\u578b\u68c0\u67e5\u5668\u9700\u8981\u5904\u7406\u5b8c\u6574\u72b6\u6001\u7a7a\u95f4\uff0c\u5bf9\u4e8e\u5927\u578bMDP\u4e0d\u53ef\u884c\uff0c\u5e76\u4e14\u65e0\u6cd5\u89e3\u91ca\u7b56\u7565\u4e3a\u4f55\u505a\u51fa\u7279\u5b9a\u51b3\u7b56\u3002\u8fd9\u9650\u5236\u4e86RL\u7b56\u7565\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "COOL-MC\u57fa\u4e8eStorm\u6a21\u578b\u68c0\u67e5\u5668\uff0c\u4f46\u589e\u52a0\u4e86\u4e09\u4e2a\u5173\u952e\u529f\u80fd\uff1a1\uff09\u4ec5\u6784\u5efa\u8bad\u7ec3\u7b56\u7565\u8bf1\u5bfc\u7684\u53ef\u8fbe\u72b6\u6001\u7a7a\u95f4\uff0c\u751f\u6210\u66f4\u5c0f\u7684\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff1b2\uff09\u81ea\u52a8\u7528\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u539f\u5b50\u547d\u9898\u6807\u8bb0\u72b6\u6001\uff1b3\uff09\u5c06\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u4e0e\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91\u67e5\u8be2\u96c6\u6210\uff0c\u63ed\u793a\u9a71\u52a8\u51b3\u7b56\u7684\u7279\u5f81\u3002\u5728ICU-Sepsis MDP\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "result": "\u901a\u8fc7\u5b8c\u6574MDP\u9a8c\u8bc1\u5efa\u7acb\u4e86\u786c\u8fb9\u754c\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8fbe\u5230\u6700\u4f18\u751f\u5b58\u6982\u7387\u7684\u5b89\u5168RL\u7b56\u7565\uff0c\u5e76\u901a\u8fc7PCTL\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u5176\u884c\u4e3a\u3002\u5206\u6790\u53d1\u73b0\u8bad\u7ec3\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u5148\u524d\u7684\u7ed9\u836f\u5386\u53f2\u800c\u975e\u60a3\u8005\u4e0d\u65ad\u53d8\u5316\u7684\u72b6\u51b5\uff0c\u8fd9\u662f\u6807\u51c6\u8bc4\u4f30\u65e0\u6cd5\u53d1\u73b0\u4f46\u88abCOOL-MC\u66b4\u9732\u7684\u5f31\u70b9\u3002", "conclusion": "COOL-MC\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u7ed3\u5408\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u5728\u90e8\u7f72\u524d\u8c03\u67e5\u548c\u8c03\u8bd5\u8113\u6bd2\u75c7\u6cbb\u7597\u7b56\u7565\u63d0\u4f9b\u4e86\u5de5\u5177\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3a\u533b\u7597\u51b3\u7b56\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4fc3\u8fdbRL\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u53ef\u4fe1\u5e94\u7528\u3002"}}
{"id": "2602.14589", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14589", "abs": "https://arxiv.org/abs/2602.14589", "authors": ["Gabriel Roccabruna", "Olha Khomyn", "Giuseppe Riccardi"], "title": "MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs", "comment": null, "summary": "AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.", "AI": {"tldr": "MATEO\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u771f\u5b9e\u4e16\u754c\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u65f6\u5e8f\u6267\u884c\u987a\u5e8f\u7406\u89e3", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u57fa\u7840\u6a21\u578b\u65f6\u5e8f\u6267\u884c\u987a\u5e8f\u7684\u7406\u89e3\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u81ea\u52a8\u751f\u6210\u7684\u6807\u6ce8\u3001\u7ebf\u6027\u94fe\u8fd1\u4f3c\u6216\u7eaf\u6587\u672c\u8f93\u5165\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30", "method": "\u6536\u96c6\u9ad8\u8d28\u91cf\u4e13\u4e1a\u591a\u6a21\u6001\u98df\u8c31\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u7f16\u8f91\u6d41\u7a0b\u5c06\u6307\u4ee4\u5206\u89e3\u4e3a\u79bb\u6563\u6b65\u9aa4\u5e76\u914d\u56fe\uff1b\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u4f17\u5305\u6807\u6ce8\u6d41\u7a0b\u6536\u96c6\u65f6\u5e8f\u6267\u884c\u987a\u5e8f\u56fe\uff1b\u8bc4\u4f306\u4e2a\u6700\u5148\u8fdb\u7684LVLM\u6a21\u578b", "result": "\u8bc4\u4f30\u4e86\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u3001\u8bed\u8a00\u4e0a\u4e0b\u6587\u3001\u591a\u6a21\u6001\u8f93\u5165\u7ed3\u6784\u548c\u5fae\u8c03\u7b56\u7565\u4e0b\u76846\u4e2a\u5148\u8fdbLVLM\u6a21\u578b\u5728\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0", "conclusion": "MATEO\u57fa\u51c6\u586b\u8865\u4e86\u591a\u6a21\u6001\u65f6\u5e8f\u63a8\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u63d0\u5347LVLM\u5728\u771f\u5b9e\u4e16\u754c\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u65f6\u5e8f\u6267\u884c\u987a\u5e8f\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177"}}
{"id": "2602.14615", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14615", "abs": "https://arxiv.org/abs/2602.14615", "authors": ["Aswathi Varma", "Suprosanna Shit", "Chinmay Prabhakar", "Daniel Scholz", "Hongwei Bran Li", "Bjoern Menze", "Daniel Rueckert", "Benedikt Wiestler"], "title": "VariViT: A Vision Transformer for Variable Image Sizes", "comment": null, "summary": "Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit", "AI": {"tldr": "VariViT\u662f\u4e00\u79cd\u6539\u8fdb\u7684Vision Transformer\u6a21\u578b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u53ef\u53d8\u5c3a\u5bf8\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u81f4\u7684patch\u5927\u5c0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfViT\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u56fa\u5b9a\u5c3a\u5bf8\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfVision Transformers\u9700\u8981\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684patch\uff0c\u8fd9\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5b58\u5728\u95ee\u9898\uff1a1\uff09\u4e0d\u89c4\u5219\u5f62\u72b6\u7ed3\u6784\uff08\u5982\u80bf\u7624\uff09\u9700\u8981\u56fa\u5b9a\u8fb9\u754c\u6846\u88c1\u526a\uff0c\u5bfc\u81f4\u524d\u666f\u80cc\u666f\u6bd4\u4f8b\u9ad8\u5ea6\u53ef\u53d8\uff1b2\uff09\u8c03\u6574\u533b\u5b66\u56fe\u50cf\u5c3a\u5bf8\u4f1a\u964d\u4f4e\u4fe1\u606f\u8d28\u91cf\u5e76\u5f15\u5165\u4f2a\u5f71\uff1b3\uff09\u5927\u56fe\u50cf\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c0f\u56fe\u50cf\u53c8\u53ef\u80fd\u4e22\u5931\u4fe1\u606f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u53ef\u53d8\u56fe\u50cf\u5c3a\u5bf8\u7684ViT\u53d8\u4f53\u3002", "method": "\u63d0\u51faVariViT\u6a21\u578b\uff1a1\uff09\u91c7\u7528\u65b0\u9896\u7684\u4f4d\u7f6e\u5d4c\u5165\u8c03\u6574\u65b9\u6848\uff0c\u9002\u5e94\u53ef\u53d8\u6570\u91cf\u7684patch\uff1b2\uff09\u5b9e\u73b0\u65b0\u7684\u6279\u5904\u7406\u7b56\u7565\u4ee5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b3\uff09\u4fdd\u6301\u4e00\u81f4\u7684patch\u5927\u5c0f\uff0c\u4f46\u5141\u8bb8\u8f93\u5165\u56fe\u50cf\u5c3a\u5bf8\u53ef\u53d8\u3002", "result": "\u5728\u4e24\u4e2a3D\u8111\u90e8MRI\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a1\uff09\u5728\u80f6\u8d28\u7624\u57fa\u56e0\u578b\u9884\u6d4b\u548c\u8111\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cVariViT\u8d85\u8d8a\u4e86\u4f20\u7edfViT\u548cResNet\uff1b2\uff09\u5206\u522b\u8fbe\u523075.5%\u548c76.3%\u7684F1\u5206\u6570\uff1b3\uff09\u5b66\u4e60\u5230\u66f4\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\uff1b4\uff09\u63d0\u51fa\u7684\u6279\u5904\u7406\u7b56\u7565\u76f8\u6bd4\u4f20\u7edf\u67b6\u6784\u51cf\u5c11\u9ad8\u8fbe30%\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "VariViT\u5728\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u7279\u522b\u9002\u5408\u533b\u5b66\u5f71\u50cf\u5206\u6790\uff0c\u80fd\u591f\u5904\u7406\u53ef\u53d8\u5c3a\u5bf8\u8f93\u5165\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14622", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14622", "abs": "https://arxiv.org/abs/2602.14622", "authors": ["Erkan Karabulut", "Daniel Daza", "Paul Groth", "Martijn C. Schut", "Victoria Degeler"], "title": "Tabular Foundation Models Can Learn Association Rules", "comment": null, "summary": "Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.", "AI": {"tldr": "TabProbe\uff1a\u57fa\u4e8e\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u5173\u8054\u89c4\u5219\u6316\u6398\u6846\u67b6\uff0c\u65e0\u9700\u9891\u7e41\u9879\u96c6\u6316\u6398\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065", "motivation": "\u4f20\u7edf\u5173\u8054\u89c4\u5219\u6316\u6398\u65b9\u6cd5\u5b58\u5728\u89c4\u5219\u7206\u70b8\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u800c\u795e\u7ecf\u65b9\u6cd5\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u8868\u683c\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u7684\u57fa\u7840", "method": "\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684\u5173\u8054\u89c4\u5219\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ece\u4efb\u4f55\u6761\u4ef6\u6982\u7387\u6a21\u578b\u4e2d\u63d0\u53d6\u5173\u8054\u89c4\u5219\u3002\u5177\u4f53\u5b9e\u73b0TabProbe\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u6761\u4ef6\u6982\u7387\u4f30\u8ba1\u5668\uff0c\u65e0\u9700\u9891\u7e41\u9879\u96c6\u6316\u6398", "result": "\u8868\u683c\u57fa\u7840\u6a21\u578b\u80fd\u6301\u7eed\u4ea7\u751f\u7b80\u6d01\u3001\u9ad8\u8d28\u91cf\u7684\u5173\u8054\u89c4\u5219\uff0c\u5177\u6709\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4f4e\u6570\u636e\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7a33\u5065\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3", "conclusion": "TabProbe\u6846\u67b6\u6210\u529f\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5173\u8054\u89c4\u5219\u6316\u6398\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u4f4e\u6570\u636e\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u77e5\u8bc6\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.14795", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14795", "abs": "https://arxiv.org/abs/2602.14795", "authors": ["Ivan Diliso", "Roberto Barile", "Claudia d'Amato", "Nicola Fanizzi"], "title": "Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs", "comment": null, "summary": "Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \\resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u540c\u65f6\u5305\u542b\u6a21\u5f0f\u5c42\u548c\u4e8b\u5b9e\u5c42\u77e5\u8bc6\u7684\u6570\u636e\u96c6\u8d44\u6e90\uff0c\u652f\u6301\u77e5\u8bc6\u56fe\u8c31\u7cbe\u5316\u7b97\u6cd5\u7684\u5168\u9762\u8bc4\u4f30", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u7cbe\u5316\u7b97\u6cd5\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u901a\u5e38\u53ea\u5305\u542b\u4e8b\u5b9e\u5c42\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u6a21\u5f0f\u5c42\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u4f9d\u8d56\u4e30\u5bcc\u672c\u4f53\u7ea6\u675f\u3001\u63a8\u7406\u6216\u795e\u7ecf\u7b26\u53f7\u6280\u672f\u7684\u8bc4\u4f30", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u540c\u65f6\u5305\u542b\u6a21\u5f0f\u548c\u4e8b\u5b9e\u7684\u6570\u636e\u96c6\uff0c\u5904\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u5229\u7528\u63a8\u7406\u63a8\u5bfc\u9690\u542b\u77e5\u8bc6\uff0c\u5e76\u652f\u6301OWL\u5e8f\u5217\u5316\u548c\u5f20\u91cf\u8868\u793a", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u6a21\u5f0f\u548c\u4e8b\u5b9e\u7684\u7cbe\u9009\u6570\u636e\u96c6\u5957\u4ef6\uff0c\u5305\u62ec\u4ece\u8868\u8fbe\u6027\u6a21\u5f0f\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u73b0\u6709\u6570\u636e\u96c6\u8865\u5145\u4e86\u6a21\u5f0f\u4fe1\u606f", "conclusion": "\u8be5\u8d44\u6e90\u586b\u8865\u4e86\u77e5\u8bc6\u56fe\u8c31\u7cbe\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4f7f\u4f9d\u8d56\u672c\u4f53\u7ea6\u675f\u548c\u63a8\u7406\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u66f4\u771f\u5b9e\u7684\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30"}}
{"id": "2602.14846", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14846", "abs": "https://arxiv.org/abs/2602.14846", "authors": ["Xiang Xiang Wang", "Guo-Wei Wei"], "title": "Multi-dimensional Persistent Sheaf Laplacians for Image Analysis", "comment": null, "summary": "We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u7eaf\u590d\u5f62\u7684\u591a\u7ef4\u6301\u4e45\u5c42\u62c9\u666e\u62c9\u65af\u6846\u67b6\u7528\u4e8e\u56fe\u50cf\u5206\u6790\uff0c\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u964d\u7ef4\u7ef4\u5ea6\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u907f\u514d\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u5bf9\u7ef4\u5ea6\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u5728COIL20\u548cETH80\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\uff08\u5982\u4e3b\u6210\u5206\u5206\u6790\uff09\u5bf9\u964d\u7ef4\u7ef4\u5ea6\u7684\u9009\u62e9\u975e\u5e38\u654f\u611f\uff0c\u5355\u4e00\u7ef4\u5ea6\u9009\u62e9\u6216\u8de8\u7ef4\u5ea6\u5e73\u5747\u90fd\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0d\u540c\u7ef4\u5ea6\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u591a\u7ef4\u5ea6\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5c06\u56fe\u50cf\u6837\u672c\u89c6\u4e3a\u5355\u7eaf\u590d\u5f62\uff0c\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u4e0a\u4f7f\u7528\u6301\u4e45\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u63d0\u53d6\u591a\u5c3a\u5ea6\u5c40\u90e8\u62d3\u6251\u8c31\u8868\u793a\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8c31\u7684\u7edf\u8ba1\u6458\u8981\u8de8\u5c3a\u5ea6\u548c\u7ef4\u5ea6\u805a\u5408\uff0c\u5f62\u6210\u591a\u5c3a\u5ea6\u591a\u7ef4\u56fe\u50cf\u8868\u793a\u3002", "result": "\u5728COIL20\u548cETH80\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u964d\u7ef4\u7ef4\u5ea6\u8303\u56f4\u5185\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e2d\u7b49\u7ef4\u5ea6\u8303\u56f4\u5185\u76f8\u5bf9\u4e8e\u57fa\u4e8ePCA\u7684\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u7ef4\u6301\u4e45\u5c42\u62c9\u666e\u62c9\u65af\u6846\u67b6\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u964d\u7ef4\u7ef4\u5ea6\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u964d\u7ef4\u65b9\u6cd5\u5bf9\u7ef4\u5ea6\u9009\u62e9\u7684\u654f\u611f\u6027\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u6709\u6548\u7684\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2602.14989", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14989", "abs": "https://arxiv.org/abs/2602.14989", "authors": ["Ayush Shrivastava", "Kirtan Gangani", "Laksh Jain", "Mayank Goel", "Nipun Batra"], "title": "ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery", "comment": "8 Pages with 2 figures of main content. 2 pages of References. 10 pages of appendix with 6 figures", "summary": "Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.", "AI": {"tldr": "ThermEval-B\u662f\u4e00\u4e2a\u5305\u542b\u7ea655,000\u4e2a\u70ed\u6210\u50cf\u89c6\u89c9\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u70ed\u6210\u50cf\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709RGB\u4e2d\u5fc3\u6a21\u578b\u5728\u70ed\u6210\u50cf\u7406\u89e3\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728RGB\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u70ed\u6210\u50cf\u56fe\u50cf\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u70ed\u6210\u50cf\u5728\u53ef\u89c1\u5149\u5931\u6548\u7684\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5982\u591c\u95f4\u76d1\u63a7\u3001\u641c\u6551\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u533b\u7597\u7b5b\u67e5\u3002\u70ed\u6210\u50cf\u7f16\u7801\u7684\u662f\u7269\u7406\u6e29\u5ea6\u800c\u975e\u989c\u8272\u6216\u7eb9\u7406\uff0c\u9700\u8981\u73b0\u6709RGB\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u8bc4\u4f30\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86ThermEval-B\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6574\u5408\u4e86\u516c\u5171\u6570\u636e\u96c6\u548c\u65b0\u6536\u96c6\u7684ThermEval-D\u6570\u636e\u96c6\u3002ThermEval-D\u662f\u9996\u4e2a\u63d0\u4f9b\u5bc6\u96c6\u9010\u50cf\u7d20\u6e29\u5ea6\u56fe\u548c\u8bed\u4e49\u8eab\u4f53\u90e8\u4f4d\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u73af\u5883\u3002\u8bc4\u4f30\u4e8625\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6e29\u5ea6\u57fa\u7840\u63a8\u7406\u65b9\u9762\u6301\u7eed\u5931\u8d25\uff0c\u5728\u8272\u5f69\u6620\u5c04\u53d8\u6362\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u6216\u56fa\u5b9a\u54cd\u5e94\u3002\u63d0\u793a\u5de5\u7a0b\u6216\u76d1\u7763\u5fae\u8c03\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\u3002\u70ed\u6210\u50cf\u7406\u89e3\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\uff0c\u8d85\u8d8aRGB\u4e2d\u5fc3\u5047\u8bbe\u3002", "conclusion": "ThermEval-B\u4f5c\u4e3a\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u63a8\u52a8\u70ed\u6210\u50cf\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u7684\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u70ed\u6210\u50cf\u7406\u89e3\u9700\u8981\u8d85\u8d8aRGB\u4e2d\u5fc3\u5047\u8bbe\u7684\u4e13\u95e8\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
