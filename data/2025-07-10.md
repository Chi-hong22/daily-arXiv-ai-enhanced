<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.SD](#cs.SD) [Total: 12]
- [cs.GR](#cs.GR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种结合CLIP感知损失模块和课程对比正则化的水下图像增强方法，显著提升了图像的感知质量和内容恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水下图像增强方法忽视人类感知且缺乏解空间约束，导致增强图像感知质量下降或内容恢复不佳。

Method: 利用CLIP模型的视觉语义特征提取能力构建感知损失模块，并结合课程对比正则化增强约束。

Result: 实验表明，该方法在视觉质量和泛化能力上优于现有技术。

Conclusion: 结合CLIP感知和课程对比正则化的方法有效提升了水下图像增强的感知质量和内容恢复效果。

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [2] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: SPARC框架通过全局TopK稀疏机制和跨重建损失，学习跨模型和模态的统一潜在空间，显著提升概念对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因模型独立表示导致的概念空间不兼容问题，实现跨模型解释性。

Method: 引入全局TopK稀疏机制和跨重建损失，强制不同模型在相同概念上激活相同潜在维度。

Result: 在Open Images上，SPARC的Jaccard相似度达0.80，比之前方法提升三倍以上。

Conclusion: SPARC实现了跨模型和模态的概念对齐，支持直接比较不同架构的概念表示，并具有实际应用潜力。

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [3] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: BayesSDF提出了一种新的概率框架，用于量化神经隐式SDF模型中的不确定性，解决了现有方法的计算效率低和几何不一致问题。


<details>
  <summary>Details</summary>
Motivation: 科学模拟应用（如森林中的流体流动）需要精确的表面几何和不确定性量化，而现有方法缺乏几何整合。

Method: BayesSDF利用拉普拉斯近似和基于Hessian的度量，高效地量化局部表面不稳定性。

Result: 实验表明，BayesSDF在校准和几何一致性上优于现有方法，提供了可操作的不确定性度量。

Conclusion: BayesSDF为不确定性感知的3D场景重建、模拟和机器人决策奠定了坚实基础。

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [4] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 论文提出LIRA框架，通过结合语义增强特征提取器和交错局部视觉耦合，解决多模态模型在分割和理解任务中的不准确和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在分割和理解任务中存在不准确分割和幻觉理解的局限性，主要源于视觉理解能力弱和缺乏细粒度感知。

Method: LIRA框架包含两个关键组件：语义增强特征提取器（SEFE）和交错局部视觉耦合（ILVC），分别提升分割准确性和减少幻觉理解。

Result: 实验表明LIRA在分割和理解任务中达到最先进性能。

Conclusion: LIRA通过结合视觉理解和分割的互补关系，有效解决了多模态模型的局限性。

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [5] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: FIFA是一个统一的视频多模态大语言模型（VideoMLLMs）忠实性评估框架，通过提取描述性事实、建模语义依赖关系，并使用VideoQA模型验证，同时提出后校正工具改进幻觉内容。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于单一任务且无法评估开放自由形式回答中的幻觉问题，因此需要更全面的评估框架。

Method: 提出FIFA框架，提取描述性事实，构建时空语义依赖图，并通过VideoQA模型验证；同时引入后校正工具修正幻觉内容。

Result: FIFA比现有评估方法更接近人类判断，后校正工具有效提高了文本和视频生成的事实一致性。

Conclusion: FIFA和后校正工具为VideoMLLMs的忠实性评估和内容修正提供了有效解决方案。

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [6] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: 本文综述了离线手写文本识别（HTR）中的数据增强与生成技术，探讨了传统方法与深度学习方法（如GANs、扩散模型和基于Transformer的方法）的优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 离线HTR系统在历史文档数字化等领域至关重要，但标注数据稀缺限制了其性能，尤其是对低资源语言和复杂脚本。

Method: 采用PRISMA方法系统筛选了1,302篇文献，最终纳入848篇，分析了传统和深度学习的数据增强与生成技术。

Result: 综述了现有数据集、评估指标和先进方法，揭示了研究空白，如脚本真实性和数据稀缺问题。

Conclusion: 提出了未来研究方向，以推动手写文本生成技术在多样语言和风格中的应用。

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [7] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: 论文提出了一种名为CCPDA的数据增强方法，用于改善深度学习多类分割模型的训练效果，特别是在火灾类别的分割上表现突出。


<details>
  <summary>Details</summary>
Motivation: 在野外火灾科学领域，获取和标注图像用于训练分割模型成本高昂且公开数据集稀缺，因此需要一种有效的数据增强方法。

Method: CCPDA方法包括三个步骤：识别源图像中的火灾簇、应用中心化技术聚焦火灾核心区域、将优化的火灾簇粘贴到目标图像上。

Result: 通过数值分析和多目标优化比较，CCPDA显著提升了火灾类别的分割性能，优于其他增强方法。

Conclusion: CCPDA有效缓解了小规模手动标注数据集的训练难题，尤其在火灾类别分割上表现优异。

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [8] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: AR2通过对齐干净和损坏图像的类激活图（CAMs）提升预训练CNN的鲁棒性，无需架构修改，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在常见损坏（如噪声、模糊等）下性能显著下降，限制了其在实际应用中的可靠性。

Method: AR2采用注意力引导修复策略，通过CAM对齐和标准微调交替迭代，增强模型对损坏的鲁棒性。

Result: AR2在CIFAR-10-C等基准测试中优于现有方法，平衡了干净数据准确性和损坏鲁棒性。

Conclusion: AR2为提升模型在多样化损坏环境中的可靠性提供了可扩展的解决方案。

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [9] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: 该论文提出了首个专门用于水下多鱼跟踪的数据集MFT25，并开发了一种名为SU-T的跟踪框架，通过实验验证其在复杂水下环境中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 水下多目标跟踪技术在海洋生态和水产养殖中具有重要意义，但目前研究不足，缺乏专门的数据集和方法。

Method: 提出了MFT25数据集，包含多样化的水下视频序列和标注；开发了SU-T框架，结合Unscented Kalman Filter和FishIoU匹配方法。

Result: SU-T在MFT25上表现优异，HOTA为34.1，IDF1为44.6，揭示了水下与陆地跟踪的差异。

Conclusion: MFT25和SU-T为水下跟踪研究提供了重要基础，推动了海洋生物学和水产养殖等领域的应用。

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [10] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: SImpHAR框架通过模拟生物阻抗信号和两阶段训练策略，显著提升了基于阻抗的人体活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 生物阻抗传感在人体活动识别中潜力巨大，但缺乏标记数据限制了其应用。

Method: 提出模拟管道生成逼真的生物阻抗信号，并采用两阶段训练策略，无需对齐标签的合成数据。

Result: 在多个数据集上表现优于现有方法，准确率和F1分数分别提升22.3%和21.8%。

Conclusion: 模拟驱动的数据增强和模块化训练为基于阻抗的活动识别提供了新思路。

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [11] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: 提出了一种基于多阶段Transformer架构的PCL-Former模型，用于时间动作定位任务，并在多个数据集上取得优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 受Transformer和多阶段架构在视频识别和目标检测领域的成功启发，探索其在时间动作定位任务中的应用。

Method: 设计了PCL-Former，包含三个专用Transformer模块：Proposal-Former（候选片段识别）、Classification-Former（动作分类）和Localization-Former（时间边界预测）。

Result: 在THUMOS14、ActivityNet-1.3和HACS数据集上分别提升2.8%、1.2%和4.8%，优于现有方法。

Conclusion: PCL-Former通过多阶段Transformer架构有效解决了时间动作定位任务，性能显著提升。

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [12] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: THOR是一种实时自适应时空RGB帧采样方法，利用热感技术捕捉手部活动，显著减少数据量和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决穿戴式相机连续处理RGB图像的高能耗、大数据量、隐私问题和计算资源需求。

Method: 结合低分辨率热感数据动态调整RGB采样率，并利用热感线索定位感兴趣区域（手部活动）。

Result: 仅使用3%的原始RGB数据，实现了与完整视频相当的识别准确率（F1-score 95%）。

Conclusion: THOR为穿戴式相机实时监测手部活动提供了更实用的解决方案。

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [13] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: 提出了一种基于事件自动编码器的高效事件数据压缩与重建方法，显著提升了事件相机的物体检测性能，同时降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统帧式视觉系统在动态环境中存在运动模糊、高延迟和数据冗余问题，事件相机虽能解决这些问题，但其稀疏和噪声事件流对物体检测提出了挑战。

Method: 采用卷积编码的事件自动编码器架构，结合自适应阈值选择和轻量级分类器，优化事件数据的空间和时间特征保留。

Result: 在SEFD数据集上，模型精度与YOLO-v4相当，但参数减少35.5倍；嵌入式平台上帧率可达8-44.8 FPS，性能优于现有技术87.84倍。

Conclusion: 该方法显著提升了事件相机的实时性能，适用于低功耗、高速的边缘计算应用。

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [14] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-RTS通过结合数据高效的强化学习和视频自适应测试时间缩放策略，显著提高了视频推理能力的数据效率，无需大规模监督微调。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习和大型语言模型的视频推理方法依赖大规模监督微调和长链思维标注，成本高且难以扩展。

Method: 跳过资源密集的监督微调步骤，采用纯强化学习训练和基于输出的奖励，结合稀疏到密集的视频测试时间缩放策略。

Result: 在多个视频推理基准测试中，Video-RTS仅使用3.6%的训练样本，平均准确率提升2.4%，在Video-Holmes和MMVU上分别提升4.2%和2.6%。

Conclusion: Video-RTS通过纯强化学习和自适应视频测试时间缩放的互补优势，实现了强大的推理性能。

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [15] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: 提出了一种名为Mask6D的新型预训练策略，通过结合2D-3D对应图和可见掩码图，提升了单目RGB图像在复杂或遮挡条件下的6D物体姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前姿态估计网络在2D特征提取上表现不足，尤其在目标被遮挡或场景复杂时，RGB信息有限。

Method: 引入2D-3D对应图和可见掩码图作为额外模态信息，设计基于重建的预训练模型和物体聚焦的损失函数。

Result: 实验表明，该方法优于现有的端到端姿态估计方法。

Conclusion: Mask6D通过预训练策略有效提升了姿态估计的鲁棒性，尤其在复杂场景中。

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [16] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: 提出了一种双边协作框架（BC-HOI），用于开放词汇的HOI检测，通过注意力偏差引导（ABG）和LLM监督引导（LSG）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖视觉语言模型（VLM）生成的特征，但这些特征过于粗粒度，不适合检测任务。

Method: 提出BC-HOI框架，包含ABG（引导VLM生成细粒度特征）和LSG（利用LLM提供细粒度监督）。

Result: 在HICO-DET和V-COCO基准测试中表现优异。

Conclusion: BC-HOI框架有效解决了开放词汇HOI检测中的特征粒度问题。

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [17] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 这篇论文提出了一种系统分类法，将交通场景中的关键元素分为异常和正常但关键的实体，并分析了35个视觉驱动任务和73个数据集，为研究者提供了全面的资源优化和研究方向指导。


<details>
  <summary>Details</summary>
Motivation: 通过视觉传感器和计算机视觉算法的进步，提升交通场景的分析能力，促进道路安全。

Method: 提出一种分类法，将交通实体分为异常和正常但关键的两大类，涵盖10个类别和20个子类，并分析相关任务和数据集。

Result: 建立了统一的分类框架，分析了35个视觉驱动任务和73个数据集，提供了资源优化和标准统一的信息。

Conclusion: 论文总结了现有研究的不足，提出了潜在解决方案，为快速发展的领域提供了全面的综述和研究方向。

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [18] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为KSCU的新方法，通过针对扩散模型的关键步骤进行概念遗忘，平衡了遗忘效果与生成能力的保留。


<details>
  <summary>Details</summary>
Motivation: 现有的概念遗忘方法难以平衡遗忘效果与生成能力的保留，导致扩散模型的安全风险无法有效解决。

Method: KSCU方法利用扩散模型的逐步采样特性，专注于对最终结果影响最大的关键步骤，仅在这些步骤上微调模型。

Result: 实验表明，KSCU能有效防止生成不良图像，同时更好地保留模型的生成能力。

Conclusion: KSCU为扩散模型的安全使用提供了一种高效且平衡的解决方案。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [19] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: 该论文提出了一种完整的流程，将英语语音转换为流畅的3D手语动画，结合语音识别、文本翻译和动作生成技术。


<details>
  <summary>Details</summary>
Motivation: 帮助聋哑和听力障碍人群更轻松地沟通，填补了从英语语音到手语动画转换的研究空白。

Method: 使用Whisper进行语音转文本，MarianMT模型翻译为ASL gloss，结合Word2Vec和FastText优化翻译，并通过3D关键点系统生成动画。

Result: 模型表现优异，BLEU分数达到0.7714和0.8923，并创建了Sign3D-WLASL和BookGlossCorpus-CG数据集。

Conclusion: 该研究提供了一个从语音到3D手语动画的完整框架，为聋哑人群的沟通提供了新工具。

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [20] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: ILNet提出了一种多智能体轨迹预测方法，结合逆向学习注意力机制和动态锚点选择模块，显著提升了复杂交互场景下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 受人类驾驶员动态调整驾驶决策的启发，解决现有方法在时空协调和适应性方面的不足。

Method: 采用逆向学习注意力机制（IL Attention）建模交互意图，并引入动态锚点选择模块（DAS）提取关键点。

Result: 在INTERACTION和Argoverse数据集上达到最优性能，尤其在复杂交互场景中表现突出。

Conclusion: ILNet通过动态建模交互意图和关键点选择，显著提升了轨迹预测的准确性和多模态分布能力。

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [21] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种模型无关的主动学习方法，用于优化野生动物相机陷阱数据的标注和检测模型训练，仅需30%的数据即可达到或超越完整数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 野生动物相机陷阱数据量大，标注和训练成本高，现有主动学习方法需要完全访问模型，限制了应用。

Method: 结合样本在对象和图像层面的不确定性和多样性，提出模型无关的主动学习样本选择方法。

Result: 在基准动物数据集上验证，仅用30%的训练数据，检测器性能达到或超过完整数据集。

Conclusion: 该方法显著减少数据标注需求，提升野生动物监测和保护的自动化效率。

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [22] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: ToBo是一种自监督学习框架，通过压缩场景为瓶颈标记并预测后续场景，学习动态场景的紧凑表示。


<details>
  <summary>Details</summary>
Motivation: 动态场景的紧凑和时间感知表示对视觉跟踪和机器人操作等任务至关重要。

Method: ToBo通过压缩步骤将参考场景编码为瓶颈标记，在扩展步骤中利用少量目标补丁预测目标场景。

Result: 在视频标签传播和机器人操作等任务中，ToBo表现优于基线方法，并在真实环境中验证了其鲁棒性。

Conclusion: ToBo能有效学习动态场景的时序依赖，适用于不同规模的模型。

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [23] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Concept-TRAK的新方法，用于解决扩散模型在图像生成中的版权和透明度问题，通过概念级归因分析提供更细粒度的贡献识别。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其广泛使用引发了版权和透明度的担忧。现有方法无法识别特定元素（如风格或对象）的贡献，因此需要更细粒度的归因方法。

Method: Concept-TRAK通过两种创新扩展了影响函数：(1) 基于扩散后验采样的训练损失重新表述；(2) 强调语义相关性的概念感知奖励函数。

Result: 在AbC基准测试中，Concept-TRAK显著优于现有方法，并通过案例研究展示了其在版权保护、内容安全和生成AI治理中的实用性。

Conclusion: Concept-TRAK为生成AI的负责任开发和治理提供了可操作的见解，填补了概念级归因的空白。

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [24] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: 提出了一种基于分布差异的相似性函数（DSF），通过将多视图表示为分布并测量分布间的差异，显式捕捉多视图的联合结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注成对关系，未能有效建模多视图的联合结构。

Method: 提出DSF，将多视图表示为分布并计算分布间的差异作为相似性度量。

Result: 实验表明DSF在kNN分类和线性评估等任务中表现优异，且效率更高。

Conclusion: DSF无需温度超参数即可有效工作，并在理论和实验上优于余弦相似性。

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [25] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: 提出了一种新的损失函数EBT，通过将像素分为边缘、边界和纹理三类，并赋予不同的监督权重，改进了传统的WBCE损失函数，提升了边缘检测的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的WBCE损失函数对所有非边缘像素一视同仁，忽略了边缘附近的结构差异，导致预测模糊。

Method: 提出EBT损失函数，将像素分为边缘、边界和纹理三类，并为每类分配不同的监督权重。

Result: 在多个基准测试中，EBT损失函数在定量和感知上均优于WBCE。

Conclusion: EBT损失函数不仅性能优越，且超参数统一，易于部署，具有实际应用价值。

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [26] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST是一种通过时序片段Banzhaf交互的新型运动扩散模型，专注于解决从罕见语言提示生成人类运动的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在粗粒度匹配和忽略语义线索方面存在问题，MOST通过利用细粒度片段关系来解决这些问题。

Method: MOST采用时序片段Banzhaf交互量化文本-运动一致性，并结合运动提示模块生成语义一致的运动。

Result: MOST在文本到运动的检索和生成任务中表现优异，尤其在罕见提示上效果显著。

Conclusion: MOST通过细粒度交互和消除冗余，显著提升了文本到运动生成的质量和一致性。

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [27] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: MK-Pose是一种多模态关键点学习框架，结合RGB图像、点云和类别文本描述，通过自监督关键点检测和图增强特征融合，显著提升类别级物体姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在物体遮挡和跨实例、跨类别泛化能力上的不足，适用于仓库自动化和制造等应用。

Method: 提出MK-Pose框架，整合RGB、点云和文本数据，采用自监督关键点检测、注意力查询生成、软热图匹配和图关系建模，并设计图增强特征融合模块。

Result: 在CAMERA25、REAL275和HouseCat6D数据集上表现优于现有方法，IoU和平均精度显著提升。

Conclusion: MK-Pose通过多模态融合和图增强特征，显著提升了类别级物体姿态估计的准确性和泛化能力。

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [28] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 提出了一种自适应边缘对比学习方法AMContrast3D，用于点云的3D语义分割，通过模糊估计框架优化模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法对模糊区域的特征处理不足，且人工标注的模糊点标签不可靠，导致模型性能受限。

Method: 设计了AMContrast3D和AMContrast3D++，结合对比学习和模糊预测模块，自适应调整目标函数。

Result: 在S3DIS和ScanNet数据集上验证了方法的有效性，提升了分割性能和鲁棒性。

Conclusion: 通过自适应处理模糊点，显著提升了3D语义分割的准确性和鲁棒性。

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [29] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: StixelNExT++是一种用于单目感知系统的新型场景表示方法，通过聚类3D Stixel单元增强对象分割，实现高压缩率并支持点云和鸟瞰图表示。


<details>
  <summary>Details</summary>
Motivation: 改进现有的Stixel表示方法，提升场景信息的压缩效率和适应性，以支持自动驾驶系统的实时感知需求。

Method: 基于轻量级神经网络，利用LiDAR生成的地面真值进行训练，实时推断3D Stixels并聚类。

Result: 在Waymo数据集上，30米范围内表现出竞争力，单帧计算时间低至10毫秒。

Conclusion: StixelNExT++在实时性和性能上具有潜力，适用于自动驾驶的集体感知任务。

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [30] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出了一种双摄像头系统（DCS）和曝光自适应融合网络（EAFNet）来解决HDR视频重建中的闪烁问题，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: HDR视频重建中，交替曝光方法导致的曝光波动常引起闪烁问题，需要更稳定的解决方案。

Method: 采用双摄像头系统（DCS），一个捕捉稳定参考序列，另一个补充信息；设计EAFNet网络，包含预对齐子网络、非对称跨特征融合子网络和重建子网络。

Result: 实验表明，该方法在不同数据集上达到最优性能，验证了DCS在HDR视频重建中的潜力。

Conclusion: DCS和EAFNet有效解决了HDR视频的闪烁问题，具有广泛应用前景。

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [31] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: SpatialReasoner框架通过结合大语言模型（LLM）和视觉属性增强的分层特征场，提升了开放词汇3D视觉定位的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言查询中处理空间关系（如“椅子上的书”）时表现不佳，主要因为对语言和3D场景中的空间关系推理不足。

Method: SpatialReasoner通过微调LLM捕捉空间关系，并构建视觉属性增强的分层特征场，结合CLIP特征和SAM提取的掩码，实现目标实例的定位。

Result: 实验表明，该框架能无缝集成到不同神经表示中，显著提升3D视觉定位性能，并增强空间推理能力。

Conclusion: SpatialReasoner为解决开放词汇3D视觉定位中的空间关系问题提供了有效方案。

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [32] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: CMDCL提出了一种跨模态双因果学习方法，通过文本和视觉因果干预解决长期动作识别中的跨模态偏差和视觉干扰。


<details>
  <summary>Details</summary>
Motivation: 长期动作识别（LTAR）因时间跨度长、动作关联复杂和视觉干扰而具有挑战性，现有方法依赖统计相关性而非因果机制，且缺乏跨模态因果建模。

Method: CMDCL通过结构因果模型揭示视频与标签文本间的因果关系，分别通过文本因果干预和视觉因果干预消除跨模态偏差和视觉干扰。

Result: 在Charades、Breakfast和COIN三个基准测试中验证了模型的有效性。

Conclusion: CMDCL通过双因果干预提升了长期动作识别的鲁棒性，解决了跨模态偏差和视觉干扰问题。

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [33] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: Percep360是一种用于自动驾驶的全景生成方法，通过局部场景扩散方法和概率提示方法实现高质量、可控的全景数据生成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要全景感知，但现有数据采集和标注过程复杂且耗时，现有生成模型无法实现高质量可控生成。

Method: 提出局部场景扩散方法（LSDM）解决信息丢失问题，概率提示方法（PPM）实现可控生成。

Result: 生成图像在无参考质量指标上优于原始拼接图像，并提升下游感知模型性能。

Conclusion: Percep360为自动驾驶提供了高质量、可控的全景数据生成解决方案。

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [34] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Omni-Fuse的新型空间-光谱全融合网络，用于高光谱图像分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学高光谱成像（MHSI）在疾病诊断中具有潜力，但其高维度和光谱冗余特性使得空间和光谱信息的有效融合具有挑战性。

Method: 设计了跨维度特征融合操作，包括双向注意力机制、光谱引导的空间查询选择和两阶段跨维度解码器。

Result: 在两个高光谱图像数据集上的实验表明，该方法在DSC指标上比现有方法提高了5.73%。

Conclusion: Omni-Fuse通过高效的跨维度特征融合，显著提升了高光谱图像的分割性能。

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [35] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 提出了一种基于视点依赖投影（VDP）的点云分割方法，通过动态适应空间几何变化生成高效3D到2D映射，解决了传统方法中投影多样性和计算效率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有投影方法依赖预定义参数，无法适应不同视点变化，导致投影多样性不足和计算冗余。

Method: 设计了VDP框架，通过数据驱动生成投影，并结合颜色正则化优化特征提取。

Result: 在S3DIS和ScanNet基准测试中取得了竞争性结果，计算成本低。

Conclusion: PointVDP提供了一种资源高效的语义理解解决方案。

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [36] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: EXAONE Path 2.0提出了一种病理学基础模型，通过直接使用切片级监督学习补丁级表示，解决了现有自监督学习方法在生物标志物预测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在数字病理学中可能忽略复杂的领域特定特征，且数据效率低。

Method: EXAONE Path 2.0通过直接使用切片级监督学习补丁级表示，仅需37k WSIs进行训练。

Result: 在10项生物标志物预测任务中达到最先进的平均性能，表现出卓越的数据效率。

Conclusion: EXAONE Path 2.0为病理学提供了一种高效且性能优越的基础模型。

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [37] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 该论文提出了一种从稀疏标注中学习密集预测任务的方法，特别针对医学图像中的关键点定位问题，并设计了一种新的损失函数（Crag and Tail loss）以提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 医学领域中，密集标注（如像素级标注）成本高昂且难以获取，尤其是在新任务中。因此，研究如何从少量稀疏标注中学习密集预测任务具有重要意义。

Method: 将问题建模为稀疏热图回归，并提出Crag and Tail损失函数，以有效利用稀疏标注并减少假阴性或漏标注的影响。

Result: 通过大量实验验证，该方法在2D腹腔镜视频帧的癌变关键点定位任务中表现出色，能够实现准确的密集定位。

Conclusion: 该方法在稀疏标注场景下具有潜力，能够推动医学图像分析领域的研究进展。

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [38] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: ClipGS是一个支持裁剪平面的高斯样条框架，用于医学体积数据的交互式电影化渲染，解决了高计算成本和低渲染速度的问题。


<details>
  <summary>Details</summary>
Motivation: 医学体积数据的可视化对诊断和手术规划至关重要，但现有方法因高计算成本和低渲染速度难以满足交互需求。

Method: 提出ClipGS框架，包括可学习的截断方案和自适应调整模型，动态调整高斯原语的可见性和变形。

Result: 在五个医学数据上验证，平均PSNR为36.635，渲染速度为156 FPS，模型大小为16.1 MB，优于现有方法。

Conclusion: ClipGS在渲染质量和效率上显著提升，适用于医学数据的交互式可视化。

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [39] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: Diff$^2$I2P 是一种基于扩散先验的全微分图像到点云配准框架，通过控制侧分数蒸馏和可变形对应调整模块，显著提升了跨模态特征学习和配准效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过度量学习实现跨模态特征对齐，但忽略了图像与点云数据的固有模态差异，导致跨模态对应关系不准确。

Method: 提出 Diff$^2$I2P 框架，利用深度条件扩散模型的先验知识，结合控制侧分数蒸馏（CSD）和可变形对应调整（DCT）模块，实现微分化的对应关系估计和变换优化。

Result: 在 7-Scenes 基准测试中，Diff$^2$I2P 显著优于现有方法，配准召回率提升超过 7%。

Conclusion: Diff$^2$I2P 通过扩散先验和微分优化设计，有效解决了跨模态配准问题，为图像到点云配准提供了新思路。

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [40] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: 本文提出了一种名为CDR-CA的新任务，通过多源DPP方法优化多属性多样性，以适应不同应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注图像外观多样性，而多样性需求因应用场景不同而异，限制了其应用范围。

Method: 提出多源DPP（MS-DPP），扩展DPP至多源，并引入切线归一化以反映上下文。

Result: 实验证明该方法有效，代码已公开。

Conclusion: CDR-CA和MS-DPP为多属性多样性优化提供了灵活且高效的解决方案。

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [41] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种新的梯度管理技术SPGD，用于解决扩散模型在图像恢复中梯度不稳定问题，显著提升了生成稳定性和恢复性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复中表现出色，但现有方法中先验和似然梯度方向的冲突及似然梯度的波动未被充分研究，导致生成过程不稳定。

Method: 提出SPGD技术，包括渐进式似然预热策略和自适应方向动量平滑，以缓解梯度冲突和减少波动。

Result: 实验表明SPGD在多种恢复任务中显著提升稳定性，取得了定量和视觉上的最佳性能。

Conclusion: SPGD通过梯度管理有效解决了扩散模型中的不稳定问题，为图像恢复提供了新思路。

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [42] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian是一种无需训练的3D高斯压缩方法，结合混合精度量化和属性判别剪枝，实现高压缩比和快速部署。


<details>
  <summary>Details</summary>
Motivation: 大规模3D模型的需求增长需要高效压缩以减少内存和计算成本，现有方法缺乏灵活性且需要重新训练。

Method: 采用混合精度量化和属性判别剪枝，无需重新训练即可适应不同压缩目标。

Result: FlexGaussian实现高达96.4%的压缩率，渲染质量损失小于1 dB PSNR，速度比现有方法快1.7-2.1倍。

Conclusion: FlexGaussian是一种灵活且高效的3D高斯压缩方法，适用于移动设备，代码即将开源。

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [43] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: QUANet提出了一种新的数量导向文本提示和视觉-文本数量对齐损失，以增强模型的数量感知能力，并通过双流自适应计数解码器提升密度图预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本提示中使用对象类别信息，但不足以训练模型准确区分计数任务中的对象数量。

Method: 引入数量导向文本提示和视觉-文本数量对齐损失；设计双流自适应计数解码器（Transformer流、CNN流和T2C适配器）；提出跨流数量排序损失。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等基准测试中表现出强大的零样本类无关计数泛化能力。

Conclusion: QUANet通过数量感知和双流解码器设计，显著提升了文本提示对象计数的性能。

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [44] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: STG-Mamba是一种新颖的时空图Mamba模型，用于音乐引导的舞蹈视频合成任务，通过音乐到骨架和骨架到视频的两步映射实现。


<details>
  <summary>Details</summary>
Motivation: 解决音乐到舞蹈视频的转换问题，捕捉关节在时空维度的依赖关系。

Method: 1. 音乐到骨架转换：引入时空图Mamba块；2. 骨架到视频转换：提出自监督正则化网络。

Result: 实验表明STG-Mamba显著优于现有方法。

Conclusion: STG-Mamba在音乐引导舞蹈视频合成任务中表现优异。

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [45] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于分层预训练策略的手语翻译方法，结合伪注释和对比视频-语言对齐，提高了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视觉与文本表示之间的差异问题，同时避免依赖注释的负担。

Method: 采用分层预训练策略，从帧、片段和视频级别提取特征，并与伪注释和口语句子对齐。

Result: 实验显示，该方法在BLEU-4和ROUGE分数上有所提升，同时保持高效性。

Conclusion: 提出的方法在手语翻译中有效，无需依赖注释，且性能优于现有方法。

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [46] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: 提出了一种结合视觉适配器、提示学习和部分最优传输（POT）与对比学习（CL）的新方法，用于提升CLIP在医学图像异常检测中的适应性。


<details>
  <summary>Details</summary>
Motivation: 医学异常检测面临成像模态多样、解剖变异大和标记数据有限等挑战，需要一种更灵活的方法。

Method: 采用多提示学习与局部特征对齐（通过POT），结合对比学习增强类内凝聚和类间分离。

Result: 在少样本、零样本和跨数据集场景中取得了最先进的结果，无需合成数据或记忆库。

Conclusion: 该方法显著提升了医学图像异常检测的性能，尤其在数据有限的情况下表现优异。

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [47] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: RPFNet是一种基于残差先验和频率感知的图像融合网络，通过双分支框架和频域卷积实现高效全局特征建模，显著提升融合图像质量。


<details>
  <summary>Details</summary>
Motivation: 图像融合需要整合多模态互补信息，但现有方法在空间域建模长程依赖时计算成本高，且缺乏真实标签增加了特征捕获难度。

Method: RPFNet采用双分支框架：残差先验模块（RPM）提取模态差异信息，频域融合模块（FDFM）通过频域卷积实现全局特征建模，交叉促进模块（CPM）增强局部与全局特征协同。训练中引入辅助解码器和显著性结构损失。

Result: 实验表明RPFNet能有效整合判别性特征，增强纹理细节和显著目标，提升高级视觉任务性能。

Conclusion: RPFNet通过频域建模和残差先验驱动，解决了图像融合中的计算效率和特征互补问题，具有实际应用潜力。

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [48] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: 论文提出了首个半导体晶圆切割过程的公开数据集CHDL，并设计了新型预测模型DIFFUMA，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 高精度工业场景（如半导体制造）缺乏专用数据集，阻碍了复杂过程建模与预测的研究。

Method: 构建CHDL数据集，并提出DIFFUMA模型，结合Mamba模块和扩散模块以捕捉全局时序和局部细节。

Result: DIFFUMA在CHDL数据集上MSE降低39%，SSIM提升至0.988，且在自然现象数据上表现优异。

Conclusion: 研究不仅提供了SOTA模型，还为工业AI领域贡献了宝贵的数据资源。

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [49] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: 提出了一种基于提示复杂度的自适应缓存方法（PCA），通过动态调整重用阈值提升视频生成速度，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 固定频率的缓存机制在复杂场景中会显著降低质量，而手动调整阈值效率低下且缺乏鲁棒性。

Method: 提出PCA缓存，利用输入提示的语义信息估计场景复杂度，动态调整重用阈值；改进TeaCache的输入-输出关系建模；引入动态CFGCache机制。

Result: 实验显示方法显著加速（如Wan2.1模型提速2.79倍），同时保持高视觉保真度。

Conclusion: PCA缓存和动态CFGCache机制有效解决了视频生成中的速度瓶颈问题，兼顾效率与质量。

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [50] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种局部和全局双粒度身份关联机制，显著提升了文本到人物图像的匹配精度。


<details>
  <summary>Details</summary>
Motivation: 减少模型对大规模人工标注样本的依赖，解决现有方法难以预测复杂一对多身份关系的问题。

Method: 局部层面显式建立跨模态身份关系，全局层面构建动态跨模态身份关联网络，并结合信息不对称样本对构建和一致性学习。

Result: 实验结果表明，该方法显著提升了跨模态匹配的准确性。

Conclusion: 该方法为文本到人物图像匹配提供了一种高效实用的解决方案。

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [51] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: 研究开发了高性能的OCR系统，用于濒危语言满文的识别，通过微调开源视觉语言模型，显著提升了真实历史文档的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 满文作为濒危语言，对早期现代东亚历史研究至关重要，但缺乏有效的OCR系统处理真实历史文档。

Method: 研究通过参数高效训练，在6万张合成的满文单词图像上微调了三个开源视觉语言模型（LLaMA-3.2-11B、Qwen2.5-VL-7B、Qwen2.5-VL-3B）。

Result: LLaMA-3.2-11B在合成数据上表现优异（98.3%单词准确率），在真实手写文档上保持93.1%准确率，显著优于传统方法（CRNN基线在真实文档上仅72.5%）。

Conclusion: 该研究为濒危语言OCR提供了可迁移的框架，降低了技术和财务门槛，助力历史学家和语言学家处理历史档案。

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [52] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: FOLC-Net框架通过轻量级联邦优化架构提升MRI多视角诊断性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有SOTA模型在处理MRI多视角（如轴向、冠状、矢状面）时的性能下降问题。

Method: 提出FOLC-Net，结合MRFO优化、全局模型克隆和ConvNeXt，参数仅1.217百万，存储需求0.9MB。

Result: 在矢状面准确率达92.44%，优于现有方法（88.37%和88.95%），多视角下表现稳健。

Conclusion: FOLC-Net为分散式医疗图像分析提供了更可靠、适应性更强的解决方案。

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [53] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 论文提出了一种从空中视角生成合成热图像的新方法，解决了热成像数据稀缺的问题，并在目标检测任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于热成像数据收集成本高且复杂，限制了深度学习模型的发展，因此需要一种方法来生成合成热图像以扩展数据集。

Method: 通过将任意对象类别集成到现有热背景中，控制其位置、比例和方向，并与背景视角对齐，生成合成热图像。

Result: 在HIT-UAV和MONET数据集中新增了无人机和动物类别，目标检测任务表现优异，热检测器优于可见光训练的模型。

Conclusion: 合成热图像方法有效扩展了数据集，验证了热检测器的优越性，强调了复制空中视角的重要性。

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [54] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: 论文提出GreenHyperSpectra数据集，用于解决植物性状预测中标签稀缺和领域转移问题，通过半监督和自监督方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 植物性状（如叶片碳含量和质量）对生物多样性和气候变化研究至关重要，但传统采样方法难以覆盖生态尺度，机器学习结合遥感数据成为解决方案。

Method: 使用GreenHyperSpectra数据集，采用半监督和自监督方法预训练多输出回归模型，评估框架包括分布内和分布外场景。

Result: 模型性能优于现有监督基线，显著提升了光谱表征学习能力。

Conclusion: GreenHyperSpectra为植物功能性状评估与表征学习的交叉研究提供了方法论框架和数据支持。

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [55] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 提出一种轻量级框架，利用2D全身骨架作为辅助条件，结合扩散模型和骨架-音频特征融合，生成高质量、音频同步的说话者视频，并发布首个公开数据集CSG-405。


<details>
  <summary>Details</summary>
Motivation: 解决语音-手势视频生成中音频与视觉内容的一对多映射问题，以及数据集稀缺和计算需求高的挑战。

Method: 使用2D骨架作为中间条件，结合扩散模型和骨架-音频特征融合，生成骨架动作，再通过现有人体视频生成模型合成视频。

Result: 方法在视觉质量和同步性上优于现有技术，并能泛化到不同说话者和场景。

Conclusion: 提出的框架和数据集为语音-手势视频生成研究提供了高效且可扩展的解决方案。

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [56] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的颜色空间HVI和网络HVI-CIDNet+，用于低光图像增强，解决了现有方法的颜色偏差和噪声问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于sRGB和HSV颜色空间的低光图像增强方法存在颜色偏差和噪声问题，需要一种更有效的解决方案。

Method: 提出HVI颜色空间，结合HV颜色图和可学习强度，设计HVI-CIDNet+网络，利用预训练视觉语言模型提取上下文知识，并通过Prior-guided Attention Block和Region Refinement Block优化内容恢复和颜色校正。

Result: 在10个基准数据集上，HVI-CIDNet+优于现有最先进方法。

Conclusion: HVI颜色空间和HVI-CIDNet+网络有效解决了低光图像增强中的颜色偏差和噪声问题，显著提升了性能。

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [57] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: 提出了一种结合符号回归和轨迹引导的视频生成框架，以提升视频生成的物理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏物理对齐，无法准确模拟真实世界动态。

Method: 通过提取运动轨迹、符号回归和轨迹引导的视频生成，无需微调现有模型。

Result: 在经典力学场景中成功恢复真实运动方程，并提升了生成视频的物理对齐。

Conclusion: 该方法显著提高了视频生成的物理准确性，优于基线方法。

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [58] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: 提出一种基于Vision Transformer的端到端弱监督语义分割方法，利用多[CLS]令牌的注意力图生成伪分割掩码，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统弱监督语义分割方法依赖外部模块的问题，提升自注意力图的可解释性和类分配的准确性。

Method: 训练稀疏ViT，使用多[CLS]令牌和随机掩码策略，聚合自注意力图生成伪分割掩码。

Result: 在多个标准数据集上表现优异，生成的伪掩码可用于训练分割模型，接近全监督模型性能。

Conclusion: 该方法显著减少了对细粒度标注数据的依赖，为弱监督语义分割提供了高效解决方案。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [59] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: IAP是一种新的对抗性补丁攻击框架，通过感知感知定位和扰动优化方案生成高度不可见的对抗性补丁。


<details>
  <summary>Details</summary>
Motivation: 现有方法在针对性攻击场景中表现不佳或生成的补丁不连贯，容易被人类或自动防御系统检测到。

Method: IAP结合类定位和敏感度图选择补丁位置，并使用感知感知正则化对抗损失和颜色恒常性梯度更新规则优化扰动。

Result: IAP在多种基准测试和模型架构中表现优异，攻击成功率高且补丁隐蔽性显著优于现有基线。

Conclusion: IAP不仅对人类高度不可见，还能有效绕过多种先进的补丁防御系统。

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [60] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Blümel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: 长期生物特征评估显示，个体间的日间分数波动比长期波动更显著，强调长期测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估长期生物特征数据的变化，为生物特征分析提供更可靠的基础。

Method: 使用先进的人脸识别算法，分析400多名参与者的238,000多组生物特征数据。

Result: 个体间的日间分数波动比长期波动更显著。

Conclusion: 长期测试对生物特征分析至关重要，为未来研究奠定基础。

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [61] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出了一种名为SemRaFiner的方法，用于稀疏雷达点云的全景分割，提升场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 相机和LiDAR在恶劣天气下表现受限且无法提供运动信息，雷达传感器虽能克服这些限制，但数据稀疏且噪声多。

Method: SemRaFiner方法优化了稀疏雷达点云的特征提取，并提出了改进的训练流程，包括数据增强。

Result: 实验表明，该方法在雷达全景分割任务上优于现有技术。

Conclusion: SemRaFiner通过优化特征提取和训练流程，显著提升了雷达点云的全景分割性能。

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [62] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: 论文提出了一种自适应部分发现和学习方法（APL），通过共享可学习部分查询和DINO部分先验生成一致的对象部分及其对应关系，无需额外标注。提出了一种新的全最小对比损失，以学习更具区分性和泛化性的部分表示。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法仅依赖DINO CLS令牌的全局表示，导致区分性和泛化性之间的固有权衡。APL旨在通过自适应部分学习解决这一问题。

Method: 使用共享可学习部分查询和DINO部分先验生成一致对象部分及其对应关系，提出全最小对比损失学习部分表示。

Result: APL显著提升了细粒度数据集上的性能，并能轻松集成到不同GCD框架中。

Conclusion: APL通过自适应部分学习和全最小对比损失，有效平衡了区分性和泛化性，提升了GCD任务的性能。

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [63] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 论文提出了一个多属性中国书法字符数据集（MCCD），填补了现有数据集稀缺且缺乏属性信息的空白，并展示了其在多种研究任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 中国书法字符的属性信息（如风格、朝代和书法家）具有重要文化和历史价值，但现有数据集稀缺且缺乏属性标注，限制了深入研究。

Method: 构建了包含7,765类329,715个字符图像的MCCD数据集，并提取了基于字体风格、朝代和书法家的三个子集，进行了单任务和多任务识别实验。

Result: 实验表明，书法字符的笔画结构复杂性和属性间的相互作用显著增加了准确识别的难度。

Conclusion: MCCD填补了详细书法数据集的空白，为书法研究和多领域发展提供了宝贵资源。

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [64] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,César Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hernández,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度学习与聚类算法的方法，通过卫星图像识别棕榈树分布，揭示古代人类管理区域的范围及其生态影响。


<details>
  <summary>Details</summary>
Motivation: 研究古代人类对热带森林的长期影响，尤其是高分辨率尺度下的管理痕迹，目前仍具挑战性。

Method: 使用深度学习模型从卫星图像中识别棕榈树，并通过聚类算法分析棕榈树簇，进而估计古代管理区域。

Result: 棕榈树在考古遗址附近显著密集，且管理区域可能比考古证据显示的大两个数量级。

Conclusion: 研究表明，前哥伦布时期人类通过植被管理留下了持久生态足迹，为人工智能与生态考古数据结合提供了新思路。

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [65] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: CheXPO通过结合置信度-相似性联合挖掘和反事实推理，优化医学视觉语言模型，减少幻觉问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型（VLMs）易产生幻觉，影响可靠性。偏好优化虽有效，但面临数据分布不均、专家标注成本高等挑战。

Method: 提出CheXPO策略：1）合成多任务胸部X光视觉指令数据集进行监督微调；2）通过置信度分析和相似性检索扩展困难样本；3）利用反事实推理提供细粒度偏好。

Result: 仅用5%的SFT样本，CheXPO实现8.93%相对性能提升，达到SOTA水平。

Conclusion: CheXPO为放射学应用提供可扩展、可解释的解决方案。

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [66] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 该研究提出了一种深度学习的可变形图像配准方法（ProRSeg），用于多领域MR-MR配准，并在前列腺癌患者的MR引导自适应放疗中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 在MR引导自适应放疗（MRgART）中，准确的图像配准对于轮廓传播和剂量累积至关重要，因此需要一种能够适应不同领域（如3T和1.5T MR）的配准方法。

Method: 研究使用262对3T MR模拟扫描训练了ProRSeg方法，采用加权分割一致性损失，并在相同领域、跨领域和混合领域数据集上测试了其性能。

Result: ProRSeg在膀胱配准中表现出跨领域的泛化能力（DSC 0.88-0.86），而在直肠和CTV上性能依赖于领域。剂量累积结果显示83.3%的患者满足CTV覆盖和膀胱保护约束。

Conclusion: ProRSeg在多领域MR-MR配准中表现合理，初步验证了其在评估治疗合规性方面的可行性。

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [67] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttopää,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: 论文介绍了MassID45数据集，结合分子和成像数据，用于训练自动分类器处理批量昆虫样本，推动生态和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 昆虫多样性研究面临种群下降和分类效率低的问题，需要结合DNA条形码和高分辨率成像技术提升分类效率。

Method: 使用MassID45数据集，结合AI辅助工具，对批量昆虫样本进行分割和分类标注。

Result: 数据集包含17,000多个标本的分割掩码和分类标签，结合DNA条形码和成像数据，提升了分类效率。

Conclusion: MassID45数据集为昆虫群落的大规模快速表征提供了新方法，推动了生态和机器学习领域的创新。

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [68] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: FreeTTA是一种无需训练且通用的测试时适应方法，通过在线EM算法利用视觉语言模型的零样本预测作为先验，显著提升跨域和分布外场景的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在实际应用中因域偏移和分布变化而受限，传统测试时适应方法依赖昂贵训练或不现实假设。

Method: 提出FreeTTA，通过在线EM算法利用零样本预测作为先验，迭代更新测试样本的后验概率和参数。

Result: 在15个数据集的跨域和分布外实验中，FreeTTA显著优于现有方法。

Conclusion: FreeTTA首次显式建模测试数据分布，无需同时访问样本，为测试时适应提供了灵活且高效的解决方案。

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [69] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: 论文提出了一种名为DenoiseCP-Net的多任务架构，用于恶劣天气下基于LiDAR的集体感知，通过降噪和物体检测减少通信开销和延迟。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知系统在恶劣天气下易受传感器性能下降的影响，集体感知虽能缓解这一问题，但相关研究较少。

Method: 提出DenoiseCP-Net，将体素级降噪和物体检测集成到稀疏卷积骨干网络中，避免冗余计算。

Result: 在模拟的恶劣天气条件下，DenoiseCP-Net实现了近乎完美的降噪效果，减少23.6%的带宽需求，同时保持检测精度并降低延迟。

Conclusion: DenoiseCP-Net为恶劣天气下的集体感知提供了高效解决方案，显著提升了系统性能。

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [70] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MCA-RG的知识驱动框架，通过将视觉特征与医学概念对齐，提升放射学报告生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在放射学报告生成中存在病理和解剖特征与文本描述映射不准确的问题，阻碍了临床应用。

Method: MCA-RG利用病理和解剖概念库对齐视觉特征，采用对比学习和匹配损失优化特征，并通过特征门控机制过滤低质量特征。

Result: 在MIMIC-CXR和CheXpert Plus基准测试中，MCA-RG表现优异。

Conclusion: MCA-RG通过知识驱动的方法有效提升了放射学报告生成的准确性和临床实用性。

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [71] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 提出了一种用于非小细胞肺癌（NSCLC）免疫治疗患者生存预测的多模态特征融合框架，结合3D CT图像和临床数据，通过跨模态掩码学习策略提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: NSCLC患者免疫治疗的精准预后对个性化治疗至关重要，但缺乏大规模数据集和有效的多模态特征融合方法。

Method: 构建了一个包含3D CT图像和临床记录的数据集，并提出了一种跨模态掩码学习框架，包括Slice-Depth Transformer和基于图的Transformer，用于多模态特征融合。

Result: 该方法在多模态整合中表现优异，超越了现有方法，为NSCLC预后模型设定了新基准。

Conclusion: 该框架显著提升了NSCLC生存预测的准确性，为个性化治疗提供了有力工具。

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [72] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: 论文提出了一种名为D2I的框架，通过规则奖励提升多模态大语言模型（MLLMs）的理解和推理能力，无需额外标注和复杂奖励，并在评估时转换为直觉推理风格。


<details>
  <summary>Details</summary>
Motivation: 多模态推理研究中模态对齐和训练成本问题尚未充分解决，现有方法依赖额外标注和规则奖励，增加了成本和限制了扩展性。

Method: D2I框架在训练时通过规则奖励设置深思熟虑的推理策略以增强模态对齐，评估时转换为直觉推理风格，隐式反映模型能力。

Result: D2I在领域内和领域外基准测试中均优于基线方法。

Conclusion: D2I展示了规则奖励在提升MLLMs可迁移推理能力中的作用，并为训练时推理深度与测试时响应灵活性的解耦提供了方向。

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [73] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: GNN-ViTCap框架通过动态聚类和注意力机制处理WSI冗余问题，结合GNN和语言模型实现分类和描述生成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决WSI分类和描述生成中的冗余问题和未知位置挑战，提升病理诊断的准确性。

Method: 使用视觉特征提取器生成嵌入，动态聚类去除冗余，GNN捕捉上下文，结合语言模型生成描述。

Result: 在BreakHis和PatchGastric数据集上，分类F1为0.934，AUC为0.963；描述BLEU-4为0.811，METEOR为0.569。

Conclusion: GNN-ViTCap为病理诊断提供了高效可靠的解决方案，性能优于现有方法。

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [74] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: 提出一种轻量级方法，利用预训练的病理基础模型特征嵌入，直接从H&E染色组织学图像预测细胞组成，避免昂贵的空间转录组学。


<details>
  <summary>Details</summary>
Motivation: 数字病理学和深度学习的快速发展为病理基础模型的出现提供了条件，这些模型有望解决多种疾病条件下的病理问题。同时，空间转录组学技术为从H&E染色图像中获取更细粒度的基因表达数据提供了可能。

Method: 通过预训练的病理基础模型提取信息丰富的特征嵌入，训练轻量级多层感知机（MLP）回归器，预测细胞类型组成。

Result: 该方法在预测细胞组成方面表现出竞争力，显著降低了计算复杂度。

Conclusion: 该方法为从组织学图像中高效预测细胞组成提供了一种轻量级且训练高效的解决方案。

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [75] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: MST-Distill提出了一种新颖的跨模态知识蒸馏框架，通过混合专家教师模型和动态路由网络解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在跨模态场景中因数据和统计异质性而表现不佳，无法充分利用跨模态教师模型的互补先验知识。

Method: 采用多样化的跨模态和多模态教师模型集成，结合实例级路由网络和掩码模块，动态适应蒸馏过程。

Result: 在五个多模态数据集上的实验表明，MST-Distill显著优于现有最先进的跨模态知识蒸馏方法。

Conclusion: MST-Distill通过动态路由和掩码模块有效解决了知识漂移和路径选择问题，提升了跨模态知识蒸馏的效果。

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [76] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: 论文提出了一种基于OCR的流水线，用于从发票中高效提取表格数据，结合Tesseract OCR和自定义后处理逻辑，显著提升了数据提取的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 发票格式多样且噪声较多，传统方法难以高效提取结构化表格数据，因此需要一种优化的解决方案。

Method: 系统采用动态预处理、表格边界检测和行列映射，结合Tesseract OCR和自定义后处理逻辑。

Result: 流水线显著提高了数据提取的准确性和一致性，适用于自动化财务流程和数字存档等实际场景。

Conclusion: 该方法为发票表格提取提供了一种高效且可靠的解决方案，具有实际应用价值。

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [77] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: 论文研究了如何通过整合上下文元数据（如GPS、时间戳和食物信息）提升大型多模态模型（LMMs）在营养分析中的性能，并引入了新数据集ACETADA。实验表明，元数据集成能显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估专有模型（如GPT-4），而忽略了其他LMMs的潜力，且上下文元数据与推理修饰符的交互影响尚未充分探索。

Method: 研究通过整合GPS、时间戳和食物信息等元数据，结合多种推理修饰符（如Chain-of-Thought），评估了八种LMMs的性能。

Result: 实验显示，元数据集成能显著降低营养值预测的平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。

Conclusion: 上下文感知的LMMs在营养分析中具有显著潜力，未来可进一步优化模型性能。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [78] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: 提出了一种基于几何深度学习的框架，用于高效预测CAD网格的Laplace-Beltrami（LB）谱，显著节省计算时间且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法（FEM）计算LB谱复杂度高，不适用于需要快速频繁处理大型网格的CAD机械零件数据库或质量控制应用。

Method: 采用图神经网络架构，利用丰富的网格特征（如高斯曲率、平均曲率和主曲率）预测LB谱。

Result: 实验表明，该方法将LB谱计算时间减少约5倍，同时保持竞争性精度。

Conclusion: 证明了LB谱的可学习性，并为重复性提供了公开数据集。

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [79] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet是一个深度学习框架，通过将标尺读数统一为关键点检测问题，实现像素到真实世界尺寸的准确转换。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中像素测量转换为真实尺寸的挑战限制了生物医学、法医学等领域的进展。

Method: RulerNet通过几何级数参数表示标尺，采用抗畸变标注和训练策略，结合合成数据增强训练多样性。

Result: RulerNet在多样化标尺和成像条件下表现出高准确性和实时性。

Conclusion: RulerNet是一个通用测量工具，可与其他视觉组件集成，实现自动化、尺度感知的分析。

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [80] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: 论文提出了一种新的自动评估指标L-VQAScore，用于解决文本到图像生成模型中实体-属性语义评估的局限性，特别是在时尚领域。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的T2I评估方法在评估复杂实体-属性语义时存在局限性，尤其是属性混淆问题。

Method: 通过结合视觉定位和VQA探测，提出L-VQAScore指标，专注于单个实体的跨模态对齐评估。

Result: 在新构建的数据集上，L-VQAScore在与人评估的相关性上优于现有方法，能更精细地捕捉实体-属性关联。

Conclusion: L-VQAScore是一种可靠且可扩展的替代主观评估的方法。

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [81] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本描述生成多样化自然人体运动序列的方法，并引入了最大的运动数据集MotionMillion和评估基准MotionMillion-Eval，实现了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到运动生成方法在零样本泛化能力和数据集规模上存在局限，且缺乏全面的评估框架。

Method: 开发了高效标注流程，构建了MotionMillion数据集和MotionMillion-Eval基准，并采用可扩展架构训练了7B参数的模型。

Result: 模型在零样本和复杂组合运动上表现出强泛化能力。

Conclusion: 该研究为零样本人体运动生成迈出了重要一步，代码已开源。

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [82] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: 提出Vision-Language-Vision（VLV）自编码框架，通过利用预训练组件减少数据需求，实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言模型需要大量高质量图像-文本对和计算资源，成本高昂。

Method: 结合预训练视觉编码器、T2I扩散模型解码器和LLM，通过信息瓶颈和知识蒸馏优化语言表示空间。

Result: 构建了与GPT-4o和Gemini 2.0 Flash相当的先进描述模型，训练成本低于1000美元。

Conclusion: VLV框架显著降低了数据需求和计算成本，为高效视觉-语言模型提供了新思路。

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [83] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent是一个统一的超分辨率通用系统，能将任何图像提升至4K分辨率，甚至更高。通过三个核心组件（分析、感知代理和恢复代理），系统能从极低分辨率输入生成高质量的4K输出，并在多个领域实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率图像恢复问题，尤其是严重退化的图像，提供一种通用的、高质量的解决方案，并推动视觉自主代理的研究。

Method: 系统包含三个模块：Profiling（定制化分析）、Perception Agent（基于视觉语言模型和图像质量评估制定恢复计划）、Restoration Agent（执行计划并优化输出）。

Result: 在11个任务类别和26个基准测试中表现优异，涵盖自然图像、肖像、AI生成内容、医学影像等，并在感知和保真度指标上领先。

Conclusion: 4KAgent为低层次视觉任务建立了新的代理范式，有望推动视觉自主代理的广泛研究和创新。

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [84] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 该论文探讨了使用预训练的文本到图像扩散模型作为视觉编码器，以弥补CLIP在细粒度细节捕捉上的不足，并提出了一种融合策略以提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: CLIP作为视觉编码器在捕捉细粒度细节方面存在不足，影响了图像问答的性能。

Method: 研究扩散模型的内部表征，利用其语义丰富性和图像-文本对齐能力，并结合CLIP特征进行融合。

Result: 扩散模型在视觉理解任务中表现出色，尤其在需要空间和组合推理的任务中。

Conclusion: 扩散模型作为视觉编码器具有潜力，特别是在需要细粒度视觉理解的任务中。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction](https://arxiv.org/abs/2507.06346)
*Li Zhou,Elvan Ceyhan*

Main category: cs.RO

TL;DR: 论文研究了资源受限的随机消歧路径（RDP）问题，提出了一种结合拉格朗日松弛和两阶段顶点消除（TPVE）的新算法框架COLOGR，解决了在不确定障碍物环境中的路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 研究在资源受限和不确定障碍物环境中的路径规划问题，旨在优化路径选择并满足全局消歧预算。

Method: 将问题建模为带权约束的最短路径问题（WCSPP），提出COLOGR算法框架，结合拉格朗日松弛和TPVE方法，剪枝不可行和次优路径。

Result: COLOGR在多种障碍物密度、传感器精度和风险模型下表现出色，优于贪婪基线并接近离线最优基准。

Conclusion: COLOGR框架在随机网络设计、移动规划和不确定性下的约束决策中具有广泛应用前景。

Abstract: We study a resource-constrained variant of the Random Disambiguation Path
(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,
in which a navigating agent must reach a target in a spatial environment
populated with uncertain obstacles. Each ambiguous obstacle may be
disambiguated at a (possibly) heterogeneous resource cost, subject to a global
disambiguation budget. We formulate this constrained planning problem as a
Weight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs
that incorporate probabilistic blockage and traversal penalties. To solve it,
we propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation
with a two-phase vertex elimination (TPVE) procedure. The method prunes
infeasible and suboptimal paths while provably preserving the optimal solution,
and leverages dual bounds to guide efficient search. We establish correctness,
feasibility guarantees, and surrogate optimality under mild assumptions. Our
analysis also demonstrates that COLOGR frequently achieves zero duality gap and
offers improved computational complexity over prior constrained path-planning
methods. Extensive simulation experiments validate the algorithm's robustness
across varying obstacle densities, sensor accuracies, and risk models,
consistently outperforming greedy baselines and approaching offline-optimal
benchmarks. The proposed framework is broadly applicable to stochastic network
design, mobility planning, and constrained decision-making under uncertainty.

</details>


### [86] [Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System](https://arxiv.org/abs/2507.06397)
*Michalis Chatzispyrou,Luke Horgan,Hyunkil Hwang,Harish Sathishchandra,Monika Roznere,Alberto Quattrini Li,Philippos Mordohai,Ioannis Rekleitis*

Main category: cs.RO

TL;DR: 本文提出了一种利用低成本运动相机和潜水电脑绘制水下洞穴地图的框架，并通过全局优化生成3D地图。


<details>
  <summary>Details</summary>
Motivation: 水下洞穴对淡水资源管理、水下考古和水文地质学至关重要，精确绘制其轮廓和尺寸有助于更好地理解这一水下领域。

Method: 使用运动相机和潜水电脑估计相机轨迹和稀疏点云，结合SVIn2和COLMAP进行全局优化，生成密集3D重建。

Result: 成功绘制了洞穴的一维缩略图和部分区域的密集3D重建，并通过手动测量验证了方法的有效性。

Conclusion: 低成本运动相机结合全局优化框架能够有效绘制水下洞穴地图，并生成高精度3D模型。

Abstract: This paper presents a framework for mapping underwater caves. Underwater
caves are crucial for fresh water resource management, underwater archaeology,
and hydrogeology. Mapping the cave's outline and dimensions, as well as
creating photorealistic 3D maps, is critical for enabling a better
understanding of this underwater domain. In this paper, we present the mapping
of an underwater cave segment (the catacombs) of the Devil's Eye cave system at
Ginnie Springs, FL. We utilized a set of inexpensive action cameras in
conjunction with a dive computer to estimate the trajectories of the cameras
together with a sparse point cloud. The resulting reconstructions are utilized
to produce a one-dimensional retract of the cave passages in the form of the
average trajectory together with the boundaries (top, bottom, left, and right).
The use of the dive computer enables the observability of the z-dimension in
addition to the roll and pitch in a visual/inertial framework (SVIn2). In
addition, the keyframes generated by SVIn2 together with the estimated camera
poses for select areas are used as input to a global optimization (bundle
adjustment) framework -- COLMAP -- in order to produce a dense reconstruction
of those areas. The same cave segment is manually surveyed using the MNemo V2
instrument, providing an additional set of measurements validating the proposed
approach. It is worth noting that with the use of action cameras, the primary
components of a cave map can be constructed. Furthermore, with the utilization
of a global optimization framework guided by the results of VI-SLAM package
SVIn2, photorealistic dense 3D representations of selected areas can be
reconstructed.

</details>


### [87] [Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction](https://arxiv.org/abs/2507.06404)
*Matteo Tiezzi,Tommaso Apicella,Carlos Cardenas-Perez,Giovanni Fregonese,Stefano Dafarra,Pietro Morerio,Daniele Pucci,Alessio Del Bue*

Main category: cs.RO

TL;DR: 提出了一种基于轨迹性能的通用评估框架NeME，用于比较模仿学习方法在复杂人机交互任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于成功率指标难以复现且无法捕捉机器人运动轨迹的复杂性，评估人形机器人性能具有挑战性。

Method: 设计了NeME（神经元评估器），通过深度学习模型对机器人关节轨迹进行分类，作为比较控制策略性能的元评估器。

Result: 实验表明，该方法比基线更符合机器人的实际成功率，提供了可复现且系统的评估手段。

Conclusion: NeME为复杂人机交互任务中的多模态模仿学习方法提供了有效的性能比较工具。

Abstract: Evaluating and comparing the performance of autonomous Humanoid Robots is
challenging, as success rate metrics are difficult to reproduce and fail to
capture the complexity of robot movement trajectories, critical in Human-Robot
Interaction and Collaboration (HRIC). To address these challenges, we propose a
general evaluation framework that measures the quality of Imitation Learning
(IL) methods by focusing on trajectory performance. We devise the Neural Meta
Evaluator (NeME), a deep learning model trained to classify actions from robot
joint trajectories. NeME serves as a meta-evaluator to compare the performance
of robot control policies, enabling policy evaluation without requiring human
involvement in the loop. We validate our framework on ergoCub, a humanoid
robot, using teleoperation data and comparing IL methods tailored to the
available platform. The experimental results indicate that our method is more
aligned with the success rate obtained on the robot than baselines, offering a
reproducible, systematic, and insightful means for comparing the performance of
multimodal imitation learning approaches in complex HRI tasks.

</details>


### [88] [Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion](https://arxiv.org/abs/2507.06426)
*Devin Crowley,Whitney G. Cole,Christina M. Hospodar,Ruiting Shen,Karen E. Adolph,Alan Fern*

Main category: cs.RO

TL;DR: 论文探讨了如何通过系统化的训练方案和细粒度评估方法研究机器人控制器的学习行为，借鉴了发展心理学中对婴儿行走的研究方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人控制器的训练和评估方法较为粗糙，缺乏对训练方案影响的深入理解，而发展心理学中对婴儿的研究方法提供了更精细的评估手段。

Method: 采用强化学习设计系统化的训练方案，并在模拟环境中测试仿人机器人Cassie的行为，类比婴儿行走实验。

Result: 揭示了不同训练方案对行为的影响，并比较了机器人Cassie与婴儿在学习行走过程中的行为发展。

Conclusion: 跨学科的婴儿-机器人研究方法为未来系统化测试训练对复杂机器人行为发展的影响提供了启发。

Abstract: Typically, learned robot controllers are trained via relatively unsystematic
regimens and evaluated with coarse-grained outcome measures such as average
cumulative reward. The typical approach is useful to compare learning
algorithms but provides limited insight into the effects of different training
regimens and little understanding about the richness and complexity of learned
behaviors. Likewise, human infants and other animals are "trained" via
unsystematic regimens, but in contrast, developmental psychologists evaluate
their performance in highly-controlled experiments with fine-grained measures
such as success, speed of walking, and prospective adjustments. However, the
study of learned behavior in human infants is limited by the practical
constraints of training and testing babies. Here, we present a case study that
applies methods from developmental psychology to study the learned behavior of
the simulated bipedal robot Cassie. Following research on infant walking, we
systematically designed reinforcement learning training regimens and tested the
resulting controllers in simulated environments analogous to those used for
babies--but without the practical constraints. Results reveal new insights into
the behavioral impact of different training regimens and the development of
Cassie's learned behaviors relative to infants who are learning to walk. This
interdisciplinary baby-robot approach provides inspiration for future research
designed to systematically test effects of training on the development of
complex learned robot behaviors.

</details>


### [89] [Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies](https://arxiv.org/abs/2507.06519)
*Yuhan Liu,Xinyu Zhang,Haonan Chang,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的框架，用于解决高精度重复插入任务（RIT），结合了失败预测模块以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决RIT任务中毫米级精度和长时间重复性能的挑战，特别是因螺母旋转和摩擦引入的复杂性。

Method: 采用sim-to-real框架，结合强化学习插入策略和失败预测模块，利用6D姿态跟踪实现精确操作。

Result: 在仿真和真实环境中均实现高成功率，并能长时间保持性能。

Conclusion: 该方法显著提升了RIT任务的鲁棒性和可转移性。

Abstract: This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where
a robot must repeatedly perform high-precision insertions, such as screwing a
nut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving
millimeter-level accuracy and maintaining consistent performance over multiple
repetitions, particularly when factors like nut rotation and friction introduce
additional complexity. We propose a sim-to-real framework that integrates a
reinforcement learning-based insertion policy with a failure forecasting
module. By representing the wrench's pose in the nut's coordinate frame rather
than the robot's frame, our approach significantly enhances sim-to-real
transferability. The insertion policy, trained in simulation, leverages
real-time 6D pose tracking to execute precise alignment, insertion, and
rotation maneuvers. Simultaneously, a neural network predicts potential
execution failures, triggering a simple recovery mechanism that lifts the
wrench and retries the insertion. Extensive experiments in both simulated and
real-world environments demonstrate that our method not only achieves a high
one-time success rate but also robustly maintains performance over long-horizon
repetitive tasks.

</details>


### [90] [KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing](https://arxiv.org/abs/2507.06562)
*Keita Yoneda,Kento Kawaharazuka,Temma Suzuki,Takahiro Hattori,Kei Okada*

Main category: cs.RO

TL;DR: 研究开发了具有腰部关节的四足机器人KLEIYN，通过强化学习实现垂直运动（如烟囱攀爬），并引入接触引导课程学习（CGCL）方法，显著提升了攀爬速度和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管四足机器人在平坦地形上的运动控制已取得进展，但在具有显著高度变化的崎岖地形中实现稳定垂直运动仍是一个挑战。

Method: 开发了带有腰部关节的四足机器人KLEIYN，并采用强化学习结合CGCL方法训练其垂直运动能力。

Result: KLEIYN能以150 mm/s的平均速度攀爬800-1000 mm宽的墙壁，速度是传统机器人的50倍，且腰部关节显著提升了在狭窄墙壁上的跟踪能力。

Conclusion: 研究表明，腰部关节和CGCL方法能有效扩展四足机器人的运动能力，尤其在垂直运动任务中表现优异。

Abstract: In recent years, advancements in hardware have enabled quadruped robots to
operate with high power and speed, while robust locomotion control using
reinforcement learning (RL) has also been realized. As a result, expectations
are rising for the automation of tasks such as material transport and
exploration in unknown environments. However, autonomous locomotion in rough
terrains with significant height variations requires vertical movement, and
robots capable of performing such movements stably, along with their control
methods, have not yet been fully established. In this study, we developed the
quadruped robot KLEIYN, which features a waist joint, and aimed to expand
quadruped locomotion by enabling chimney climbing through RL. To facilitate the
learning of vertical motion, we introduced Contact-Guided Curriculum Learning
(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to
1000 mm in width at an average speed of 150 mm/s, 50 times faster than
conventional robots. Furthermore, we demonstrated that the introduction of a
waist joint improves climbing performance, particularly enhancing tracking
ability on narrow walls.

</details>


### [91] [Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments](https://arxiv.org/abs/2507.06750)
*Tohid Kargar Tasooji,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 提出了一种分布式容错协同定位框架，通过自适应事件触发通信策略提升对抗环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失或通信受限的多机器人系统中，传统定位方法易受对抗攻击（如传感器操纵和通信干扰）影响，需要提高鲁棒性。

Method: 采用自适应事件触发通信策略，动态调整通信阈值，确保传感器或通信故障时的最优性能。

Result: 算法在对抗环境中表现出更高的定位精度和通信效率，实验验证了其优越性。

Conclusion: 该框架提升了多机器人系统的可扩展性、可靠性和容错性，适用于现实世界中的挑战性环境。

Abstract: In multi-robot systems (MRS), cooperative localization is a crucial task for
enhancing system robustness and scalability, especially in GPS-denied or
communication-limited environments. However, adversarial attacks, such as
sensor manipulation, and communication jamming, pose significant challenges to
the performance of traditional localization methods. In this paper, we propose
a novel distributed fault-tolerant cooperative localization framework to
enhance resilience against sensor and communication disruptions in adversarial
environments. We introduce an adaptive event-triggered communication strategy
that dynamically adjusts communication thresholds based on real-time sensing
and communication quality. This strategy ensures optimal performance even in
the presence of sensor degradation or communication failure. Furthermore, we
conduct a rigorous analysis of the convergence and stability properties of the
proposed algorithm, demonstrating its resilience against bounded adversarial
zones and maintaining accurate state estimation. Robotarium-based experiment
results show that our proposed algorithm significantly outperforms traditional
methods in terms of localization accuracy and communication efficiency,
particularly in adversarial settings. Our approach offers improved scalability,
reliability, and fault tolerance for MRS, making it suitable for large-scale
deployments in real-world, challenging environments.

</details>


### [92] [SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments](https://arxiv.org/abs/2507.06564)
*Tianshun Li,Tianyi Huai,Zhen Li,Yichun Gao,Haoang Li,Xinhu Zheng*

Main category: cs.RO

TL;DR: SkyVLN框架结合视觉语言导航（VLN）和非线性模型预测控制（NMPC），提升无人机在复杂城市环境中的自主导航能力。


<details>
  <summary>Details</summary>
Motivation: 无人机在复杂城市环境中的自主导航需求日益增长，传统方法难以应对动态3D空间和模糊指令的挑战。

Method: SkyVLN利用大型语言模型（LLM）解析自然语言指令和视觉观察，结合细粒度空间语言化器和历史路径记忆机制，并集成NMPC模块进行动态避障。

Result: 实验表明，SkyVLN显著提高了导航成功率和效率，尤其在陌生环境中表现优异。

Conclusion: SkyVLN为无人机在复杂城市环境中的自主导航提供了高效、鲁棒的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across
various sectors, driven by their mobility and adaptability. This paper
introduces SkyVLN, a novel framework integrating vision-and-language navigation
(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in
complex urban environments. Unlike traditional navigation methods, SkyVLN
leverages Large Language Models (LLMs) to interpret natural language
instructions and visual observations, enabling UAVs to navigate through dynamic
3D spaces with improved accuracy and robustness. We present a multimodal
navigation agent equipped with a fine-grained spatial verbalizer and a history
path memory mechanism. These components allow the UAV to disambiguate spatial
contexts, handle ambiguous instructions, and backtrack when necessary. The
framework also incorporates an NMPC module for dynamic obstacle avoidance,
ensuring precise trajectory tracking and collision prevention. To validate our
approach, we developed a high-fidelity 3D urban simulation environment using
AirSim, featuring realistic imagery and dynamic urban elements. Extensive
experiments demonstrate that SkyVLN significantly improves navigation success
rates and efficiency, particularly in new and unseen environments.

</details>


### [93] [AI Space Cortex: An Experimental System for Future Era Space Exploration](https://arxiv.org/abs/2507.06574)
*Thomas Touma,Ersin Daş,Erica Tevere,Martin Feather,Ksenia Kolcio,Maurice Prather,Alberto Candela,Ashish Goel,Erik Kramer,Hari Nayar,Lorraine Fesq,Joel W. Burdick*

Main category: cs.RO

TL;DR: REASIMO项目旨在为NASA的COLDTech计划开发AI辅助的自主系统，以应对海洋世界任务中的通信延迟和恶劣环境挑战。


<details>
  <summary>Details</summary>
Motivation: 海洋世界任务（如欧罗巴和恩塞拉多斯）面临通信延迟、有限能源和辐射等挑战，传统安全模式无法满足需求，需自主解决异常。

Method: 结合AI技术和预训练行为，开发智能框架，实现异常检测与恢复，并在NASA实验室测试自主采样操作。

Result: 成功展示了AI辅助自主系统在模拟海洋世界表面条件下的性能。

Conclusion: REASIMO框架为未来海洋世界任务提供了高效、自主的解决方案，减少对地球通信的依赖。

Abstract: Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)
effort contributes to NASA's Concepts for Ocean worlds Life Detection
Technology (COLDTech) program, which explores science platform technologies for
ocean worlds such as Europa and Enceladus. Ocean world missions pose
significant operational challenges. These include long communication lags,
limited power, and lifetime limitations caused by radiation damage and hostile
conditions. Given these operational limitations, onboard autonomy will be vital
for future Ocean world missions. Besides the management of nominal lander
operations, onboard autonomy must react appropriately in the event of
anomalies. Traditional spacecraft rely on a transition into 'safe-mode' in
which non-essential components and subsystems are powered off to preserve
safety and maintain communication with Earth. For a severely time-limited Ocean
world mission, resolutions to these anomalies that can be executed without
Earth-in-the-loop communication and associated delays are paramount for
completion of the mission objectives and science goals. To address these
challenges, the REASIMO effort aims to demonstrate a robust level of
AI-assisted autonomy for such missions, including the ability to detect and
recover from anomalies, and to perform missions based on pre-trained behaviors
rather than hard-coded, predetermined logic like all prior space missions. We
developed an AI-assisted, personality-driven, intelligent framework for control
of an Ocean world mission by combining a mix of advanced technologies. To
demonstrate the capabilities of the framework, we perform tests of autonomous
sampling operations on a lander-manipulator testbed at the NASA Jet Propulsion
Laboratory, approximating possible surface conditions such a mission might
encounter.

</details>


### [94] [Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration](https://arxiv.org/abs/2507.06605)
*Xinyu Wu*

Main category: cs.RO

TL;DR: Episodic RRT (ERRT) 是一种结合深度强化学习的混合规划框架，显著提升了传统RRT在复杂或高维空间中的效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的运动规划器（如RRT）在高维或复杂环境中效率低下，主要依赖随机采样。

Method: ERRT 使用深度强化学习生成多步“探索片段”，取代随机采样，实现有向搜索。

Result: 在2D、3D和6D环境中，ERRT 表现优于传统RRT，成功率达98%，速度提升107倍，碰撞检测减少99.6%。

Conclusion: ERRT 及其变体显著提升了运动规划的效率和性能，尤其在高维环境中表现突出。

Abstract: Classical sampling-based motion planners like the RRTs suffer from
inefficiencies, particularly in cluttered or high-dimensional spaces, due to
their reliance on undirected, random sampling. This paper introduces the
Episodic RRT, a novel hybrid planning framework that replaces the primitive of
a random point with a learned, multi-step "exploratory episode" generated by a
Deep Reinforcement Learning agent. By making the DRL agent the engine of
exploration, ERRT transforms the search process from a diffuse, volumetric
expansion into a directed, branch-like growth. This paradigm shift yields key
advantages: it counters the curse of dimensionality with focused exploration,
minimizes expensive collision checks by proactively proposing locally valid
paths, and improves connectivity by generating inherently connected path
segments. We demonstrate through extensive empirical evaluation across 2D, 3D,
and 6D environments that ERRT and its variants consistently and significantly
outperform their classical counterparts. In a challenging 6D robotic arm
scenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to
107x faster, reduces collision checks by over 99.6%, and finds initial paths
that are nearly 50% shorter. Furthermore, its asymptotically optimal variant,
ERRT*, demonstrates vastly superior anytime performance, refining solutions to
near-optimality up to 29x faster than standard RRT* in 3D environments. Code:
https://xinyuwuu.github.io/Episodic_RRT/.

</details>


### [95] [Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic](https://arxiv.org/abs/2507.06625)
*Shizhe Cai,Jayadeep Jacob,Zeya Yin,Fabio Ramos*

Main category: cs.RO

TL;DR: Q-STAC结合贝叶斯MPC与演员-评论家强化学习，通过约束Stein变分梯度下降，提升样本效率并确保安全性。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在连续控制任务中数据需求大、长时规划难和安全性不足的问题，同时弥补MPC的局部最优和成本函数设计复杂的缺陷。

Method: 整合贝叶斯MPC与演员-评论家强化学习，利用约束SVGD优化控制序列，直接以学习到的Q值为目标。

Result: 在2D导航和机器人操作任务中，Q-STAC表现出更高的样本效率、鲁棒性和最优性。

Conclusion: Q-STAC成功结合了两种方法的优势，实现了高效、安全和最优的控制。

Abstract: Deep reinforcement learning has shown remarkable success in continuous
control tasks, yet often requires extensive training data, struggles with
complex, long-horizon planning, and fails to maintain safety constraints during
operation. Meanwhile, Model Predictive Control (MPC) offers explainability and
constraint satisfaction, but typically yields only locally optimal solutions
and demands careful cost function design. This paper introduces the Q-guided
STein variational model predictive Actor-Critic (Q-STAC), a novel framework
that bridges these approaches by integrating Bayesian MPC with actor-critic
reinforcement learning through constrained Stein Variational Gradient Descent
(SVGD). Our method optimizes control sequences directly using learned Q-values
as objectives, eliminating the need for explicit cost function design while
leveraging known system dynamics to enhance sample efficiency and ensure
control signals remain within safe boundaries. Extensive experiments on 2D
navigation and robotic manipulation tasks demonstrate that Q-STAC achieves
superior sample efficiency, robustness, and optimality compared to
state-of-the-art algorithms, while maintaining the high expressiveness of
policy distributions. Experiment videos are available on our website:
https://sites.google.com/view/q-stac

</details>


### [96] [Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs](https://arxiv.org/abs/2507.06690)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Hongyin Zhang,Donglin Wang,Shiyu Zhao*

Main category: cs.RO

TL;DR: 提出了一种分层方法，通过技能图解决多任务多智能体强化学习中的无关任务和知识转移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理无关任务且知识转移能力有限，需要更高效的多任务学习方案。

Method: 采用分层结构，高层使用技能图，低层采用标准MARL算法。

Result: 实验表明，该方法优于最新的分层MAPPO算法。

Conclusion: 该方法扩展了多任务学习的范围，并提升了知识转移能力。

Abstract: Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained
attention for its potential to enhance MARL's adaptability across multiple
tasks. However, it is challenging for existing multi-task learning methods to
handle complex problems, as they are unable to handle unrelated tasks and
possess limited knowledge transfer capabilities. In this paper, we propose a
hierarchical approach that efficiently addresses these challenges. The
high-level module utilizes a skill graph, while the low-level module employs a
standard MARL algorithm. Our approach offers two contributions. First, we
consider the MT-MARL problem in the context of unrelated tasks, expanding the
scope of MTRL. Second, the skill graph is used as the upper layer of the
standard hierarchical approach, with training independent of the lower layer,
effectively handling unrelated tasks and enhancing knowledge transfer
capabilities. Extensive experiments are conducted to validate these advantages
and demonstrate that the proposed method outperforms the latest hierarchical
MAPPO algorithms. Videos and code are available at
https://github.com/WindyLab/MT-MARL-SG

</details>


### [97] [Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction](https://arxiv.org/abs/2507.06700)
*Pranav Pandey,Ramviyas Parasuraman,Prashant Doshi*

Main category: cs.RO

TL;DR: 论文提出了一种参数化通用安全模型，通过个性化参数ρ结合物理安全和主观安全感知，研究了情感状态、信任和机器人行为对安全感知的影响。


<details>
  <summary>Details</summary>
Motivation: 传统安全模型仅依赖传感器数据，忽略了主观安全感知，因此需要一种结合物理和心理因素的安全模型。

Method: 通过模拟救援场景的人体实验，分析情感状态、信任和机器人行为对安全感知的影响，并引入参数ρ捕捉个体差异。

Result: ρ能有效反映个体差异，可预测和一致的机器人行为及积极情感状态显著提升安全感知，用户可聚类为少数类型。

Conclusion: 研究强调了自适应、以人为本的安全模型的重要性，为安全关键领域的人机交互提供了更可信的解决方案。

Abstract: Ensuring safety in human-robot interaction (HRI) is essential to foster user
trust and enable the broader adoption of robotic systems. Traditional safety
models primarily rely on sensor-based measures, such as relative distance and
velocity, to assess physical safety. However, these models often fail to
capture subjective safety perceptions, which are shaped by individual traits
and contextual factors. In this paper, we introduce and analyze a parameterized
general safety model that bridges the gap between physical and perceived safety
by incorporating a personalization parameter, $\rho$, into the safety
measurement framework to account for individual differences in safety
perception. Through a series of hypothesis-driven human-subject studies in a
simulated rescue scenario, we investigate how emotional state, trust, and robot
behavior influence perceived safety. Our results show that $\rho$ effectively
captures meaningful individual differences, driven by affective responses,
trust in task consistency, and clustering into distinct user types.
Specifically, our findings confirm that predictable and consistent robot
behavior as well as the elicitation of positive emotional states, significantly
enhance perceived safety. Moreover, responses cluster into a small number of
user types, supporting adaptive personalization based on shared safety models.
Notably, participant role significantly shapes safety perception, and repeated
exposure reduces perceived safety for participants in the casualty role,
emphasizing the impact of physical interaction and experiential change. These
findings highlight the importance of adaptive, human-centered safety models
that integrate both psychological and behavioral dimensions, offering a pathway
toward more trustworthy and effective HRI in safety-critical domains.

</details>


### [98] [Spatial-Temporal Aware Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2507.06710)
*Zhenyang Liu,Yikai Wang,Kuanning Wang,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: 提出了一种名为4D Diffusion Policy (DP4)的新方法，通过引入时空感知改进视觉模仿学习，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模仿学习方法依赖历史轨迹的行为克隆，缺乏对3D空间和4D时空关系的捕捉能力，限制了实际应用效果。

Method: DP4利用动态高斯世界模型从交互环境中学习3D空间和4D时空感知，通过单视角RGB-D观测构建当前3D场景并预测未来3D场景，优化轨迹生成。

Result: 在17个模拟任务和3个真实机器人任务中，DP4显著优于基线方法，模拟任务成功率平均提升16.4%、14%和6.45%，真实任务成功率提升8.6%。

Conclusion: DP4通过引入时空感知，显著提升了视觉模仿学习的性能，适用于复杂任务的实际部署。

Abstract: Visual imitation learning is effective for robots to learn versatile tasks.
However, many existing methods rely on behavior cloning with supervised
historical trajectories, limiting their 3D spatial and 4D spatiotemporal
awareness. Consequently, these methods struggle to capture the 3D structures
and 4D spatiotemporal relationships necessary for real-world deployment. In
this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation
learning method that incorporates spatiotemporal awareness into diffusion-based
policies. Unlike traditional approaches that rely on trajectory cloning, DP4
leverages a dynamic Gaussian world model to guide the learning of 3D spatial
and 4D spatiotemporal perceptions from interactive environments. Our method
constructs the current 3D scene from a single-view RGB-D observation and
predicts the future 3D scene, optimizing trajectory generation by explicitly
modeling both spatial and temporal dependencies. Extensive experiments across
17 simulation tasks with 173 variants and 3 real-world robotic tasks
demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,
improving the average simulation task success rate by 16.4% (Adroit), 14%
(DexArt), and 6.45% (RLBench), and the average real-world robotic task success
rate by 8.6%.

</details>


### [99] [LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/abs/2507.06747)
*Daojie Peng,Jiahang Cao,Qiang Zhang,Jun Ma*

Main category: cs.RO

TL;DR: LOVON是一个结合大型语言模型（LLMs）和开放词汇视觉检测模型的新框架，用于动态非结构化环境中的长距离物体导航。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在开放世界环境中执行长时程任务时，难以有效整合物体检测和高级任务规划的挑战。

Method: 集成LLMs进行分层任务规划，结合开放词汇视觉检测模型，并设计视觉稳定化（如拉普拉斯方差滤波）和功能执行逻辑。

Result: 成功完成涉及实时检测、搜索和导航的长序列任务，并在不同腿式机器人上验证了兼容性和即插即用特性。

Conclusion: LOVON在动态非结构化环境中表现出色，具备自主导航、任务适应和稳健任务完成的能力。

Abstract: Object navigation in open-world environments remains a formidable and
pervasive challenge for robotic systems, particularly when it comes to
executing long-horizon tasks that require both open-world object detection and
high-level task planning. Traditional methods often struggle to integrate these
components effectively, and this limits their capability to deal with complex,
long-range navigation missions. In this paper, we propose LOVON, a novel
framework that integrates large language models (LLMs) for hierarchical task
planning with open-vocabulary visual detection models, tailored for effective
long-range object navigation in dynamic, unstructured environments. To tackle
real-world challenges including visual jittering, blind zones, and temporary
target loss, we design dedicated solutions such as Laplacian Variance Filtering
for visual stabilization. We also develop a functional execution logic for the
robot that guarantees LOVON's capabilities in autonomous navigation, task
adaptation, and robust task completion. Extensive evaluations demonstrate the
successful completion of long-sequence tasks involving real-time detection,
search, and navigation toward open-vocabulary dynamic targets. Furthermore,
real-world experiments across different legged robots (Unitree Go2, B2, and
H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

</details>


### [100] [Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance](https://arxiv.org/abs/2507.06787)
*Sean Smith,Emmanuel Witrant,Ya-Jun Pan*

Main category: cs.RO

TL;DR: 提出了一种基于流函数的导航控制系统，用于避障，结合了涡板方法和模型预测控制，适用于复杂环境。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂、部分观测环境中避障的问题，特别是处理快速移动障碍物的挑战。

Method: 结合涡板方法（VPM）和模型预测控制器（MPC），利用高阶控制屏障函数（HOCBF）优化避障轨迹。

Result: 在仿真和实际实验中验证了系统的有效性，能够实时避障。

Conclusion: 该系统为复杂环境中的实时避障提供了有效解决方案。

Abstract: This article presents a novel stream function-based navigational control
system for obstacle avoidance, where obstacles are represented as
two-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The
approach leverages the vortex panel method (VPM) and incorporates safety
margins to control the stream function and flow properties around virtual
surfaces, enabling navigation in complex, partially observed environments using
real-time sensing. To address the limitations of the VPM in managing relative
distance and avoiding rapidly accelerating obstacles at close proximity, the
system integrates a model predictive controller (MPC) based on higher-order
control barrier functions (HOCBF). This integration incorporates VPM trajectory
generation, state estimation, and constraint handling into a receding-horizon
optimization problem. The 2D rigid surfaces are enclosed using minimum bounding
ellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts
obstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid
avoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone
Gazebo simulator and real-time experiments involving a COEX Clover quadcopter
equipped with a 360 degree LiDAR sensor.

</details>


### [101] [Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand](https://arxiv.org/abs/2507.06822)
*Wei Xu,Yanchao Zhao,Weichao Guo,Xinjun Sheng*

Main category: cs.RO

TL;DR: 提出了一种分层目标条件强化学习框架，用于提升仿人机器人手操作关节工具的能力，实验成功率达70.8%。


<details>
  <summary>Details</summary>
Motivation: 研究关节工具的动态形状变化对机器人操作的挑战，探索如何通过强化学习提升操作能力。

Method: 采用分层策略：低层策略控制工具配置，高层策略定义目标状态；结合点云编码器和启发式策略提升训练效率。

Result: 实验表明，机器人能有效操作类似镊子的工具，成功抓取不同形状和大小的物体。

Conclusion: 强化学习在提升机器人操作关节工具方面具有潜力。

Abstract: Manipulating articulated tools, such as tweezers or scissors, has rarely been
explored in previous research. Unlike rigid tools, articulated tools change
their shape dynamically, creating unique challenges for dexterous robotic
hands. In this work, we present a hierarchical, goal-conditioned reinforcement
learning (GCRL) framework to improve the manipulation capabilities of
anthropomorphic robotic hands using articulated tools. Our framework comprises
two policy layers: (1) a low-level policy that enables the dexterous hand to
manipulate the tool into various configurations for objects of different sizes,
and (2) a high-level policy that defines the tool's goal state and controls the
robotic arm for object-picking tasks. We employ an encoder, trained on
synthetic pointclouds, to estimate the tool's affordance states--specifically,
how different tool configurations (e.g., tweezer opening angles) enable
grasping of objects of varying sizes--from input point clouds, thereby enabling
precise tool manipulation. We also utilize a privilege-informed heuristic
policy to generate replay buffer, improving the training efficiency of the
high-level policy. We validate our approach through real-world experiments,
showing that the robot can effectively manipulate a tweezer-like tool to grasp
objects of diverse shapes and sizes with a 70.8 % success rate. This study
highlights the potential of RL to advance dexterous robotic manipulation of
articulated tools.

</details>


### [102] [Friction Estimation for In-Hand Planar Motion](https://arxiv.org/abs/2507.06824)
*Gabriel Arslan Waltersson,Yiannis Karayiannidis*

Main category: cs.RO

TL;DR: 提出了一种在线估计平行夹持器滑动操作中接触特性的方法，包括静摩擦、库仑摩擦和接触半径，并通过仿真和实验验证。


<details>
  <summary>Details</summary>
Motivation: 在滑动操作中准确估计接触特性对机器人控制至关重要，但现有方法难以处理快速滑移-粘滞动态。

Method: 基于触觉测量的接触力和滑动速度，估计摩擦和接触半径，并提出启发式方法处理滑移-粘滞动态。

Result: 方法在仿真和实际实验中均得到验证，能够有效估计接触特性。

Conclusion: 该方法为滑动操作中的接触特性估计提供了实用解决方案，尤其适用于动态变化场景。

Abstract: This paper presents a method for online estimation of contact properties
during in-hand sliding manipulation with a parallel gripper. We estimate the
static and Coulomb friction as well as the contact radius from tactile
measurements of contact forces and sliding velocities. The method is validated
in both simulation and real-world experiments. Furthermore, we propose a
heuristic to deal with fast slip-stick dynamics which can adversely affect the
estimation.

</details>


### [103] [Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems](https://arxiv.org/abs/2507.06884)
*Dong Bi,Yongqi Zhao,Zhengguo Gu,Tomislav Mihalj,Jia Hu,Arno Eichberger*

Main category: cs.RO

TL;DR: 提出了一种全栈工具链，用于从真实数据自动生成场景并通过基于CarMaker、ROS和Apollo的协同仿真平台进行高效验证。


<details>
  <summary>Details</summary>
Motivation: 解决现有仿真工具链在快速自动场景生成与支持高级自动驾驶能力的仿真环境集成方面的不足。

Method: 开发全栈工具链，结合真实数据集自动生成场景，并通过CarMaker、ROS和Apollo的协同仿真平台验证。

Result: 仿真结果证明了工具链的有效性。

Conclusion: 该工具链为加速自动驾驶系统部署提供了有效解决方案。

Abstract: Virtual testing has emerged as an effective approach to accelerate the
deployment of automated driving systems. Nevertheless, existing simulation
toolchains encounter difficulties in integrating rapid, automated scenario
generation with simulation environments supporting advanced automated driving
capabilities. To address this limitation, a full-stack toolchain is presented,
enabling automatic scenario generation from real-world datasets and efficient
validation through a co-simulation platform based on CarMaker, ROS, and Apollo.
The simulation results demonstrate the effectiveness of the proposed toolchain.
A demonstration video showcasing the toolchain is available at the provided
link: https://youtu.be/taJw_-CmSiY.

</details>


### [104] [ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation](https://arxiv.org/abs/2507.06905)
*Wandong Sun,Luying Feng,Baoshi Cao,Yang Liu,Yaochu Jin,Zongwu Xie*

Main category: cs.RO

TL;DR: 论文提出了一种统一的人形机器人运动与操作控制框架（ULC），通过单一策略实现全身协调控制，优于传统分层方法。


<details>
  <summary>Details</summary>
Motivation: 现有分层控制方法限制了子系统间的协调性，而人类表现出的是全身统一控制。论文旨在探索单一策略是否能实现高性能的运动与操作控制。

Method: 提出ULC框架，采用序列技能学习、残差动作建模、命令多项式插值等技术，实现端到端的全身控制。

Result: 在Unitree G1机器人上验证，ULC在跟踪精度、工作空间覆盖和抗干扰能力上优于基线方法。

Conclusion: 单一策略的ULC框架证明了统一控制的可行性，且性能不逊于分层方法，适用于复杂任务。

Abstract: Loco-Manipulation for humanoid robots aims to enable robots to integrate
mobility with upper-body tracking capabilities. Most existing approaches adopt
hierarchical architectures that decompose control into isolated upper-body
(manipulation) and lower-body (locomotion) policies. While this decomposition
reduces training complexity, it inherently limits coordination between
subsystems and contradicts the unified whole-body control exhibited by humans.
We demonstrate that a single unified policy can achieve a combination of
tracking accuracy, large workspace, and robustness for humanoid
loco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a
single-policy framework that simultaneously tracks root velocity, root height,
torso rotation, and dual-arm joint positions in an end-to-end manner, proving
the feasibility of unified control without sacrificing performance. We achieve
this unified control through key technologies: sequence skill acquisition for
progressive learning complexity, residual action modeling for fine-grained
control adjustments, command polynomial interpolation for smooth motion
transitions, random delay release for robustness to deploy variations, load
randomization for generalization to external disturbances, and
center-of-gravity tracking for providing explicit policy gradients to maintain
stability. We validate our method on the Unitree G1 humanoid robot with 3-DOF
(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better
tracking performance to disentangled methods and demonstrating larger workspace
coverage. The unified dual-arm tracking enables precise manipulation under
external loads while maintaining coordinated whole-body control for complex
loco-manipulation tasks.

</details>


### [105] [Bounomodes: the grazing ox algorithm for exploration of clustered anomalies](https://arxiv.org/abs/2507.06960)
*Samuel Matloob,Ayan Dutta,O. Patrick Kreidl,Swapnonel Roy,Ladislau Bölöni*

Main category: cs.RO

TL;DR: 论文提出了一种名为“bounom=odes”的算法，结合均匀覆盖和异常区域针对性探索，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统IPP算法采用均匀覆盖模式，但在异常集群场景下效果不佳，需优先探索异常区域。

Method: 结合均匀采样（boustrophedon）和针对性探索（通过深度强化学习学习异常分布）。

Result: 实验表明，该方法优于多个基线算法。

Conclusion: bounom=odes算法在异常集群场景下更高效。

Abstract: A common class of algorithms for informative path planning (IPP) follows
boustrophedon ("as the ox turns") patterns, which aim to achieve uniform area
coverage. However, IPP is often applied in scenarios where anomalies, such as
plant diseases, pollution, or hurricane damage, appear in clusters. In such
cases, prioritizing the exploration of anomalous regions over uniform coverage
is beneficial. This work introduces a class of algorithms referred to as
bounom\=odes ("as the ox grazes"), which alternates between uniform
boustrophedon sampling and targeted exploration of detected anomaly clusters.
While uniform sampling can be designed using geometric principles, close
exploration of clusters depends on the spatial distribution of anomalies and
must be learned. In our implementation, the close exploration behavior is
learned using deep reinforcement learning algorithms. Experimental evaluations
demonstrate that the proposed approach outperforms several established
baselines.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [106] [Super Kawaii Vocalics: Amplifying the "Cute" Factor in Computer Voice](https://arxiv.org/abs/2507.06235)
*Yuto Mandai,Katie Seaborn,Tomoyasu Nakano,Xin Sun,Yijia Wang,Jun Kato*

Main category: cs.HC

TL;DR: 研究探索了声音中的‘可爱’（kawaii）元素及其操纵方法，通过实验验证了声音频率对可爱感知的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注视觉上的‘可爱’，而声音方面的研究较少，本文旨在填补这一空白。

Method: 采用四阶段研究（N=512），分析文本转语音和游戏角色声音的频率调整对可爱感知的影响。

Result: 发现特定声音的频率调整存在‘可爱甜点’，但效果有限且受声音类型限制。

Conclusion: 初步验证了可爱声音模型，并提出了一种操纵计算机声音可爱感知的基本方法。

Abstract: "Kawaii" is the Japanese concept of cute, which carries sociocultural
connotations related to social identities and emotional responses. Yet,
virtually all work to date has focused on the visual side of kawaii, including
in studies of computer agents and social robots. In pursuit of formalizing the
new science of kawaii vocalics, we explored what elements of voice relate to
kawaii and how they might be manipulated, manually and automatically. We
conducted a four-phase study (grand N = 512) with two varieties of computer
voices: text-to-speech (TTS) and game character voices. We found kawaii "sweet
spots" through manipulation of fundamental and formant frequencies, but only
for certain voices and to a certain extent. Findings also suggest a ceiling
effect for the kawaii vocalics of certain voices. We offer empirical validation
of the preliminary kawaii vocalics model and an elementary method for
manipulating kawaii perceptions of computer voice.

</details>


### [107] [Ragged Blocks: Rendering Structured Text with Style](https://arxiv.org/abs/2507.06460)
*Sam Cohen,Ravi Chugh*

Main category: cs.HC

TL;DR: 论文提出了一种新的文本布局算法，生成不规则多边形（ragged blocks）以嵌套装饰文本，同时最小化对排版布局的干扰。


<details>
  <summary>Details</summary>
Motivation: 当前文本可视化仅限于扁平化或嵌套盒子，限制了信息的丰富展示。

Method: 提出一种布局算法，生成不规则多边形（ragged blocks），支持嵌套文本的紧凑渲染。

Result: 算法生成的布局比传统矩形块更紧凑，并通过场景展示展示了未来应用的潜力。

Conclusion: 该算法为文本可视化提供了更丰富的装饰选项，可能推动代码编辑器和文档渲染GUI的改进。

Abstract: Whether it be source code in a programming language, prose in natural
language, or otherwise, text is highly structured. Currently, text
visualizations are confined either to _flat, line-based_ decorations, which can
convey only limited information about textual structure, or _nested boxes_,
which convey structure but often destroy the typographic layout of the
underlying text. We hypothesize that the lack of rich styling options limits
the kinds of information that are displayed alongside text, wherever it may be
displayed.
  In this paper, we show that it is possible to achieve arbitrarily nested
decorations while minimally disturbing the underlying typographic layout.
Specifically, we present a layout algorithm that generates _ragged blocks_, or
_rocks_, which are rectilinear polygons that allow nested text to be compactly
rendered even when styled with borders and padding.
  We evaluate our layout algorithm in two ways. First, on a benchmark suite
comprising representative source code files in multiple programming languages,
we show that the (ragged block) layouts produced by our algorithm are
substantially more compact than the (rectangular block) layouts produced by
conventional techniques, when uniformly styling every element in the syntax
tree with borders and padding. Second, through a small gallery of usage
scenarios, we demonstrate how future code editors, word processors, and other
document-rendering GUIs might convey rich semantic information through
domain-specific styling of ragged blocks.

</details>


### [108] [Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents](https://arxiv.org/abs/2507.06483)
*Zackary Rackauckas,Julia Hirschberg*

Main category: cs.HC

TL;DR: 研究探讨了动漫风格语音代理在多模态语言学习环境中如何影响用户互动，发现代理的设计（如声音、人设和语言风格）显著影响用户体验和学习动机。


<details>
  <summary>Details</summary>
Motivation: 探索动漫风格语音代理在语言学习中的效果，特别是代理的个性化设计如何影响用户互动和学习行为。

Method: 采用混合方法评估54名参与者与基于大语言模型和文本转语音合成的动漫风格代理的互动，分析用户参与度、感知可用性、情感反应和学习行为。

Result: 代理的设计（声音、人设和语言风格）显著影响用户体验、学习动机和策略，尤其在语言水平和文化背景不同的用户中表现明显。

Conclusion: 研究为设计更具吸引力和社会响应性的代理提供了指导，并深化了对情感化、文化风格化代理在人际互动中作用的理解。

Abstract: This study investigates how stylized, voiced agents shape user interaction in
a multimodal language learning environment. We conducted a mixed-methods
evaluation of 54 participants interacting with anime-inspired characters
powered by large language models and expressive text-to-speech synthesis. These
agents responded in Japanese character language, offering users asynchronous,
semi-structured conversation in varying speech styles and emotional tones. We
analyzed user engagement patterns, perceived usability, emotional responses,
and learning behaviors, with particular attention to how agent stylization
influenced interaction across language proficiency levels and cultural
backgrounds. Our findings reveal that agent design, especially voice, persona,
and linguistic style, substantially affected user experience, motivation, and
strategy. This work contributes to the understanding of affective, culturally
stylized agents in human-agent interaction and offers guidance for designing
more engaging, socially responsive systems.

</details>


### [109] [Towards Designing Social Interventions for Online Climate Change Denialism Discussions](https://arxiv.org/abs/2507.06561)
*Ruican zhong,Shruti Phadke,Beth Goldberg,Tanushree Mitra*

Main category: cs.HC

TL;DR: 研究提出了一种利用内部语言框架在Reddit上对抗气候变化否认阴谋论的新方法，结合人工和生成AI技术发布干预信息，发现证据导向的中立语言能促进积极讨论。


<details>
  <summary>Details</summary>
Motivation: 随着阴谋论盛行，研究如何在在线社区中推动基于证据和科学的讨论变得至关重要。

Method: 结合手动和生成AI方法，设计干预信息并通过透明标记的机器人账户在Reddit上发布。

Result: 证据导向的中立语言促进了气候变化否认者的积极讨论，支持者也更积极参与并提供额外证据。

Conclusion: 研究为社交媒体干预提供了宝贵见解，并指导未来相关研究。

Abstract: As conspiracy theories gain traction, it has become crucial to research
effective intervention strategies that can foster evidence and science-based
discussions in conspiracy theory communities online. This study presents a
novel framework using insider language to contest conspiracy theory ideology in
climate change denialism on Reddit. Focusing on discussions in two Reddit
communities, our research investigates reactions to pro-social and
evidence-based intervention messages for two cohorts of users: climate change
deniers and climate change supporters. Specifically, we combine manual and
generative AI-based methods to craft intervention messages and deploy the
interventions as replies on Reddit posts and comments through transparently
labeled bot accounts. On the one hand, we find that evidence-based
interventions with neutral language foster positive engagement, encouraging
open discussions among believers of climate change denialism. On the other,
climate change supporters respond positively, actively participating and
presenting additional evidence. Our study contributes valuable insights into
the process and challenges of automatically delivering interventions in
conspiracy theory communities on social media, and helps inform future research
on social media interventions.

</details>


### [110] [Smartphone Exergames with Real-Time Markerless Motion Capture: Challenges and Trade-offs](https://arxiv.org/abs/2507.06669)
*Mathieu Phosanarack,Laura Wallard,Sophie Lepreux,Christophe Kolski,Eugénie Avril*

Main category: cs.HC

TL;DR: 智能手机无标记动作捕捉技术为健康与康复应用提供了低成本解决方案，但仍需解决实时性与准确性平衡等挑战。


<details>
  <summary>Details</summary>
Motivation: 通过智能手机摄像头实现无标记动作捕捉，降低传统系统的硬件成本，提升可及性。

Method: 开发一款结合实时动作捕捉的移动应用，利用AI姿态估计技术。

Result: 技术面临实时性与准确性的平衡问题，需优化AI模型和用户交互设计。

Conclusion: 未来研究应优化AI模型、整合自适应游戏化，以扩展智能手机运动游戏的应用范围。

Abstract: Markerless Motion Capture (MoCap) using smartphone cameras is a promising
approach to making exergames more accessible and cost-effective for health and
rehabilitation. Unlike traditional systems requiring specialized hardware,
recent advancements in AI-powered pose estimation enable movement tracking
using only a mobile device. For an upcoming study, a mobile application with
real-time exergames including markerless motion capture is being developed.
However, implementing such technology introduces key challenges, including
balancing accuracy and real-time responsiveness, ensuring proper user
interaction. Future research should explore optimizing AI models for realtime
performance, integrating adaptive gamification, and refining user-centered
design principles. By overcoming these challenges, smartphone-based exergames
could become powerful tools for engaging users in physical activity and
rehabilitation, extending their benefits to a broader audience.

</details>


### [111] [Effects of task difficulty and music expertise in virtual reality: Observations of cognitive load and task accuracy in a rhythm exergame](https://arxiv.org/abs/2507.06691)
*Kyla Ellahiyoun,Emma Jane Pretty,Renan Guarese,Marcel Takac,Haytham Fayek,Fabio Zambetta*

Main category: cs.HC

TL;DR: 研究探讨了音乐训练、认知负荷（CL）与虚拟现实（VR）游戏Beat Saber中任务准确性的关系，发现音乐训练提高任务准确性但不直接减少主观CL。


<details>
  <summary>Details</summary>
Motivation: 探索音乐训练如何影响VR游戏中的认知负荷和任务表现。

Method: 32名参与者在三种难度下玩Beat Saber，填写问卷并测量生理数据，使用回归分析。

Result: 音乐训练显著预测任务准确性，但不影响主观CL；游戏经验和难度影响主观CL。

Conclusion: 音乐训练提升任务表现，未来研究可考虑其他分组方法和变量（如心流和自我效能）。

Abstract: This study explores the relationship between musical training, cognitive load
(CL), and task accuracy within the virtual reality (VR) exergame Beat Saber
across increasing levels of difficulty. Participants (N=32) completed a series
of post-task questionnaires after playing the game under three task difficulty
levels while having their physiological data measured by an Emotibit. Using
regression analyses, we found that task difficulty and gaming experience
significantly predicted subjective CL, whereas musical training did not.
However, musical training significantly predicted higher task accuracy, along
with lower subjective CL, increased gaming experience, and greater
physiological arousal. These results suggest that musical training enhances
task-specific performance but does not directly reduce subjective CL. Future
research should consider alternative methods of grouping musical expertise and
the additional predictability of flow and self-efficacy.

</details>


### [112] [Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool](https://arxiv.org/abs/2507.06734)
*Milena Pustet,Elisabeth Steffen,Helena Mihaljević,Grischa Stanjek,Yannis Illies*

Main category: cs.HC

TL;DR: 探讨公民社会组织（CSOs）在AI辅助开源监测工具开发中的角色，以对抗Telegram上的反民主运动。


<details>
  <summary>Details</summary>
Motivation: 平台减少内容审核投入，CSOs需更主动参与技术工具开发，而非被动使用。

Method: 与CSO合作开发AI辅助开源监测工具，整合AI模型与社交媒体监控基础设施。

Result: 目前工作进展中，旨在实现CSOs在工具开发中的实质性参与。

Conclusion: CSOs应成为技术开发的合作伙伴，确保工具符合实际需求与价值观。

Abstract: The role of civil society organizations (CSOs) in monitoring harmful online
content is increasingly crucial, especially as platform providers reduce their
investment in content moderation. AI tools can assist in detecting and
monitoring harmful content at scale. However, few open-source tools offer
seamless integration of AI models and social media monitoring infrastructures.
Given their thematic expertise and contextual understanding of harmful content,
CSOs should be active partners in co-developing technological tools, providing
feedback, helping to improve models, and ensuring alignment with stakeholder
needs and values, rather than as passive 'consumers'. However, collaborations
between the open source community, academia, and civil society remain rare, and
research on harmful content seldom translates into practical tools usable by
civil society actors. This work in progress explores how CSOs can be
meaningfully involved in an AI-assisted open-source monitoring tool of
anti-democratic movements on Telegram, which we are currently developing in
collaboration with CSO stakeholders.

</details>


### [113] [Combining Human-centred Explainability and Explainable AI](https://arxiv.org/abs/2507.06751)
*Janin Koch,Vitor Fortes Rey*

Main category: cs.HC

TL;DR: 本文探讨了以人为中心的解释性（HCx）与可解释人工智能（xAI）之间的差异，提出了结合两者的新代数机器学习方法，并展望了未来合作的机会。


<details>
  <summary>Details</summary>
Motivation: 研究HCx与xAI之间的差异，探索两者结合的可能性，以推动更符合人类需求的解释性AI发展。

Method: 通过讨论当前HCx和xAI的理论，提出一种新的代数机器学习方法作为结合案例。

Result: 初步展示了结合HCx与xAI的代数机器学习方法，并发现了合作潜力。

Conclusion: 呼吁与HCxAI社区进一步讨论设计机会，推动HCx与xAI的融合。

Abstract: This position paper looks at differences between the current understandings
of human-centered explainability and explainability AI. We discuss current
ideas in both fields, as well as the differences and opportunities we
discovered. As an example of combining both, we will present preliminary work
on a new algebraic machine learning approach. We are excited to continue
discussing design opportunities for human-centered explainability (HCx) and xAI
with the broader HCxAI community.

</details>


### [114] [Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding](https://arxiv.org/abs/2507.06779)
*Martin Wimpff,Jan Zerfowski,Bin Yang*

Main category: cs.HC

TL;DR: 提出了一种名为RAP的无参数方法，用于解决深度学习在实时脑机接口中的三大挑战：离线到在线的转换、计算复杂性和数据需求。


<details>
  <summary>Details</summary>
Motivation: 深度学习在离线脑机接口中表现优异，但在实时应用中面临三大挑战：离线模型的在线转换困难、计算复杂性高以及数据需求大。

Method: 引入RAP方法，通过修改现有离线模型的池化层以适应在线解码需求，并利用源自由域适应减少数据需求。

Result: RAP在实时脑机接口中表现稳健高效，保护隐私、减少校准需求，并支持协同适应系统。

Conclusion: RAP为在线脑机接口的深度学习应用提供了可行框架，推动了用户中心化高性能脑机接口的发展。

Abstract: Despite the growing success of deep learning (DL) in offline brain-computer
interfaces (BCIs), its adoption in real-time applications remains limited due
to three primary challenges. First, most DL solutions are designed for offline
decoding, making the transition to online decoding unclear. Second, the use of
sliding windows in online decoding substantially increases computational
complexity. Third, DL models typically require large amounts of training data,
which are often scarce in BCI applications. To address these challenges and
enable real-time, cross-subject decoding without subject-specific calibration,
we introduce realtime adaptive pooling (RAP), a novel parameter-free method.
RAP seamlessly modifies the pooling layers of existing offline DL models to
meet online decoding requirements. It also reduces computational complexity
during training by jointly decoding consecutive sliding windows. To further
alleviate data requirements, our method leverages source-free domain
adaptation, enabling privacy-preserving adaptation across varying amounts of
target data. Our results demonstrate that RAP provides a robust and efficient
framework for real-time BCI applications. It preserves privacy, reduces
calibration demands, and supports co-adaptive BCI systems, paving the way for
broader adoption of DL in online BCIs. These findings lay a strong foundation
for developing user-centered, high-performance BCIs that facilitate immediate
feedback and user learning.

</details>


### [115] [Toward Neurodivergent-Aware Productivity: A Systems and AI-Based Human-in-the-Loop Framework for ADHD-Affected Professionals](https://arxiv.org/abs/2507.06864)
*Raghavendra Deshmukh*

Main category: cs.HC

TL;DR: 本文提出了一种结合系统思维、人机交互设计、AI/ML和隐私优先自适应代理的框架，以支持ADHD用户在数字工作环境中的注意力管理。


<details>
  <summary>Details</summary>
Motivation: IT和知识密集型行业对注意力管理要求高，而ADHD用户面临时间盲区、数字干扰等挑战，现有工具无法满足其需求。

Method: 采用系统思维和人机交互设计，结合AI/ML技术，通过设备端机器学习感知用户行为，提供非干扰性的提示和支持。

Result: 开发了一个可复制的模型，用于高干扰工作环境中的自适应包容性支持工具。

Conclusion: 该框架为ADHD用户提供了有效的注意力管理支持，同时保护隐私，适用于高干扰的数字工作环境。

Abstract: Digital work environments in IT and knowledge-based sectors demand high
levels of attention management, task juggling, and self-regulation. For adults
with ADHD, these settings often amplify challenges such as time blindness,
digital distraction, emotional reactivity, and executive dysfunction. These
individuals prefer low-touch, easy-to-use interventions for daily tasks.
Conventional productivity tools often fail to support the cognitive variability
and overload experienced by neurodivergent professionals. This paper presents a
framework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and
privacy-first adaptive agents to support ADHD-affected users. The assistant
senses tab usage, application focus, and inactivity using on-device ML. These
cues are used to infer attention states and deliver nudges, reflective prompts,
or accountability-based presence (body doubling) that aid regulation without
disruption. Technically grounded in AI, the approach views attention as shaped
by dynamic feedback loops. The result is a replicable model for adaptive,
inclusive support tools in high-distraction work environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [116] [A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes](https://arxiv.org/abs/2507.06278)
*Kemboi Cheruiyot,Nickson Kiprotich,Vyacheslav Kungurtsev,Kennedy Mugo,Vivian Mwirigi,Marvin Ngesa*

Main category: cs.MA

TL;DR: 本文综述了多智能体交互的三种拓扑结构：联邦强化学习、去中心化强化学习和非合作强化学习，分析了其结构、理论保证及数值性能的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着对自主智能体研究的兴趣增加，多智能体交互的复杂场景变得重要，需要系统化分类与分析。

Method: 通过文献综述，将多智能体交互分为三类（联邦RL、去中心化RL、非合作RL），并分析其结构、理论及数值性能。

Result: 总结了三种交互拓扑的最新研究进展，包括理论保证和数值性能的局限性。

Conclusion: 本文为多智能体交互研究提供了系统化框架，并指出了未来研究方向。

Abstract: The increasing interest in research and innovation towards the development of
autonomous agents presents a number of complex yet important scenarios of
multiple AI Agents interacting with each other in an environment. The
particular setting can be understood as exhibiting three possibly topologies of
interaction - centrally coordinated cooperation, ad-hoc interaction and
cooperation, and settings with noncooperative incentive structures. This
article presents a comprehensive survey of all three domains, defined under the
formalism of Federal Reinforcement Learning (RL), Decentralized RL, and
Noncooperative RL, respectively. Highlighting the structural similarities and
distinctions, we review the state of the art in these subjects, primarily
explored and developed only recently in the literature. We include the
formulations as well as known theoretical guarantees and highlights and
limitations of numerical performance.

</details>


### [117] [Learning To Communicate Over An Unknown Shared Network](https://arxiv.org/abs/2507.06499)
*Shivangi Agarwal,Adi Asija,Sanjit K. Kaul,Arani Bhattacharya,Saket Anand*

Main category: cs.MA

TL;DR: 论文提出了一种名为QNet的深度强化学习模型，用于解决多智能体共享无线网络时的通信决策问题，并通过仿真到现实的框架验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在多智能体共享无线网络的场景中，智能体无法感知网络资源的动态变化，需要一种能适应不同网络配置的通信策略。

Method: 提出QNet模型，采用仿真到现实的训练框架，通过随机化仿真参数覆盖多种网络条件，并设计了相应的学习算法。

Result: 在WiFi和蜂窝网络实验中验证了QNet的有效性，其性能优于其他策略，适用于从低竞争到高竞争的网络环境。

Conclusion: QNet能够泛化到不同网络配置，通过仿真到现实的训练框架实现了高效的通信决策。

Abstract: As robots (edge-devices, agents) find uses in an increasing number of
settings and edge-cloud resources become pervasive, wireless networks will
often be shared by flows of data traffic that result from communication between
agents and corresponding edge-cloud. In such settings, agent communicating with
the edge-cloud is unaware of state of network resource, which evolves in
response to not just agent's own communication at any given time but also to
communication by other agents, which stays unknown to the agent. We address
challenge of an agent learning a policy that allows it to decide whether or not
to communicate with its cloud node, using limited feedback it obtains from its
own attempts to communicate, to optimize its utility. The policy generalizes
well to any number of other agents sharing the network and must not be trained
for any particular network configuration. Our proposed policy is a DRL model
Query Net (QNet) that we train using a proposed simulation-to-real framework.
Our simulation model has just one parameter and is agnostic to specific
configurations of any wireless network. It allows training an agent's policy
over a wide range of outcomes that an agent's communication with its edge-cloud
node may face when using a shared network, by suitably randomizing the
simulation parameter. We propose a learning algorithm that addresses challenges
observed in training QNet. We validate our simulation-to-real driven approach
through experiments conducted on real wireless networks including WiFi and
cellular. We compare QNet with other policies to demonstrate its efficacy. WiFi
experiments involved as few as five agents, resulting in barely any contention
for the network, to as many as fifty agents, resulting in severe contention.
The cellular experiments spanned a broad range of network conditions, with
baseline RTT ranging from a low of 0.07 second to a high of 0.83 second.

</details>


### [118] [Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration](https://arxiv.org/abs/2507.06520)
*Xinyuan Song,Zeyu Wang,Siyi Wu,Tianyu Shi,Lynn Ai*

Main category: cs.MA

TL;DR: Gradientsys是一个多智能体调度框架，通过MCP协议和动态规划循环协调多样化AI代理，支持并行执行和透明监控，实验显示其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体任务调度中的异构性、并行性和透明性问题。

Method: 采用MCP协议和ReAct动态规划循环，支持混合同步/异步执行，具备重试和重规划机制。

Result: 在GAIA基准测试中，任务成功率更高，延迟和API成本更低。

Conclusion: Gradientsys展示了LLM驱动的多智能体协调在性能和效率上的优势。

Abstract: We present Gradientsys, a next-generation multi-agent scheduling framework
that coordinates diverse specialized AI agents using a typed Model-Context
Protocol (MCP) and a ReAct-based dynamic planning loop. At its core,
Gradientsys employs an LLM-powered scheduler for intelligent one-to-many task
dispatch, enabling parallel execution of heterogeneous agents such as PDF
parsers, web search modules, GUI controllers, and web builders. The framework
supports hybrid synchronous/asynchronous execution, respects agent capacity
constraints, and incorporates a robust retry-and-replan mechanism to handle
failures gracefully. To promote transparency and trust, Gradientsys includes an
observability layer streaming real-time agent activity and intermediate
reasoning via Server-Sent Events (SSE). We offer an architectural overview and
evaluate Gradientsys against existing frameworks in terms of extensibility,
scheduling topology, tool reusability, parallelism, and observability.
Experiments on the GAIA general-assistant benchmark show that Gradientsys
achieves higher task success rates with reduced latency and lower API costs
compared to a MinionS-style baseline, demonstrating the strength of its
LLM-driven multi-agent orchestration.

</details>


### [119] [Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments](https://arxiv.org/abs/2507.07074)
*Farhaan Ebadulla,Dharini Hindlatti,Srinivaasan NS,Apoorva VH,Ayman Aftab*

Main category: cs.MA

TL;DR: 提出了一种基于图的协调复杂度指标，用于多智能体强化学习中的任务排序和课程设计，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在任务排序和课程设计方面面临挑战，尤其是合作协调场景，缺乏有效的任务复杂度指标。

Method: 通过整合智能体依赖熵、空间干扰模式和目标重叠分析，提出了一种图基协调复杂度指标，并在MADDPG框架下验证。

Result: 复杂度指标与随机智能体性能评估的难度相关性高达0.952（p < 0.001），在MultiWalker和Simple Spread环境中分别实现了56倍性能提升和系统性任务进展。

Conclusion: 该指标为多智能体课程设计提供了有效工具，并揭示了协调紧密度对课程学习效果的影响。

Abstract: Multi-agent reinforcement learning (MARL) faces significant challenges in
task sequencing and curriculum design, particularly for cooperative
coordination scenarios. While curriculum learning has demonstrated success in
single-agent domains, principled approaches for multi-agent coordination remain
limited due to the absence of validated task complexity metrics. This approach
presents a graph-based coordination complexity metric that integrates agent
dependency entropy, spatial interference patterns, and goal overlap analysis to
predict task difficulty in multi-agent environments. The complexity metric
achieves strong empirical validation with rho = 0.952 correlation (p < 0.001)
between predicted complexity and empirical difficulty determined by random
agent performance evaluation. This approach evaluates the curriculum learning
framework using MADDPG across two distinct coordination environments: achieving
56x performance improvement in tight coordination tasks (MultiWalker) and
demonstrating systematic task progression in cooperative navigation (Simple
Spread). Through systematic analysis, coordination tightness emerges as a
predictor of curriculum learning effectiveness, where environments requiring
strict agent interdependence benefit substantially from structured progression.
This approach provides a validated complexity metric for multi-agent curriculum
design and establishes empirical guidelines for multi-robot coordination
applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [120] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: 提出了一种名为HADES-NN的新方法，通过神经网络平滑近似不连续信号，用于非自治微分方程的参数估计。


<details>
  <summary>Details</summary>
Motivation: 非自治微分方程在建模受外部信号影响的系统时很重要，但当信号突变时，参数估计变得困难。

Method: HADES-NN分两阶段：1）用神经网络平滑近似不连续信号；2）用平滑信号估计模型参数。

Result: HADES-NN在多种应用中（如昼夜节律系统、酵母交配响应）提供了高精度参数估计。

Conclusion: HADES-NN扩展了可拟合实际测量数据的模型系统范围。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [121] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 论文提出了一种基于二元随机单元的前向-前向算法，旨在减少神经网络训练中的能耗，并通过硬件优化实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习因规模扩大和能耗增加而面临能源问题，反向传播算法在硬件加速器上存在挑战，需要替代方案。

Method: 采用二元随机单元的前向-前向算法，将矩阵乘法转化为索引操作，并结合p-bits实现高效硬件计算。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上表现接近实值前向-前向算法，能耗降低约一个数量级。

Conclusion: 提出的算法在保持性能的同时显著降低了能耗，为节能机器学习提供了可行方案。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [122] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS是一种基于强化学习的自适应脑深部刺激框架，通过预测奖励模型和Gumbel Softmax探索，提高了样本效率和硬件兼容性。


<details>
  <summary>Details</summary>
Motivation: 传统开环DBS缺乏适应性和个性化，而现有RL方法存在样本复杂度高和硬件限制问题。

Method: SEA-DBS结合预测奖励模型和Gumbel Softmax探索，优化了样本效率和探索稳定性。

Result: 在帕金森病基底节模拟中，SEA-DBS表现出更快的收敛速度、更强的病理β波抑制能力和FP16量化适应性。

Conclusion: SEA-DBS为实时、资源受限的神经调控提供了一种实用且有效的RL解决方案。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [123] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux是一个新颖的深度学习框架，用于通过符号回归从标准辛平面上的向量场中识别哈密顿函数。


<details>
  <summary>Details</summary>
Motivation: 自动化哈密顿力学中的符号表达式发现。

Method: 采用混合CNN-LSTM架构，训练和验证基于新开发的哈密顿向量场数据集。

Result: 模型能准确恢复符号表达式。

Conclusion: SymFlux在哈密顿力学自动化发现中具有显著效果。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [124] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 论文提出DecoyDB数据集和定制化GCL框架，用于蛋白质-配体复合物的自监督学习，显著提升预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-配体复合物结合亲和力预测在药物发现中至关重要，但现有数据集规模小且质量不高，限制了进展。

Method: 提出DecoyDB数据集，包含高质量复合物和多样化的诱饵结构，并设计定制化GCL框架进行预训练和微调。

Result: 实验表明，基于DecoyDB预训练的模型在精度、标签效率和泛化性上表现优异。

Conclusion: DecoyDB和定制化GCL框架填补了领域空白，为蛋白质-配体复合物预测提供了有效解决方案。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [125] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 研究了深度线性卷积网络梯度流的几何性质，发现参数空间的梯度流可以表示为函数空间的黎曼梯度流，且不依赖于初始化条件。


<details>
  <summary>Details</summary>
Motivation: 探索线性卷积网络在梯度流下的几何特性，特别是与初始化无关的黎曼梯度流表示。

Method: 分析线性卷积网络的梯度流，证明其在参数空间可以表示为函数空间的黎曼梯度流，适用于多维卷积。

Result: 对于D≥2的卷积或D=1且步长大于1的情况，梯度流可表示为黎曼梯度流，且黎曼度量依赖于初始化。

Conclusion: 线性卷积网络的梯度流具有几何特性，可表示为黎曼梯度流，为理解其优化行为提供了新视角。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [126] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 研究发现，在去中心化学习中，将通信集中在训练后期能显著提升全局泛化性能，且仅需最后一步全局合并即可达到中心化训练的效果。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习的性能常受限于点对点通信，研究如何优化通信调度以提升性能。

Method: 通过实验和理论分析，研究通信时间与频率对去中心化训练的影响，并重新解释本地模型差异的作用。

Result: 集中通信于后期训练显著提升泛化性能，且最后一步全局合并可匹配中心化训练效果。

Conclusion: 去中心化学习在数据异构和有限通信下仍能表现良好，挑战了传统认知，并为模型合并和损失景观提供新见解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [127] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成全连接层权重和压缩卷积层权重，显著减少内存需求，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络存储大量权重的高内存需求问题。

Method: 使用PCA降维和轻量级SVR模型预测权重，结合敏感性分析压缩CNN权重。

Result: 实现53倍全连接层压缩和28倍AlexNet压缩，准确率损失仅1-2%。

Conclusion: WINGs显著降低内存需求，适用于资源受限的边缘计算场景。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [128] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 论文提出了一种梯度流分解方法，用于理解非线性循环模型中的学习机制，并通过实验和理论验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度下降及其变体在训练循环动态系统（如RNNs、Neural ODEs和GRUs）中表现优异，但目前缺乏理论工具来严格理解这些模型中的学习机制。

Method: 将梯度流分解为两个算子（参数算子K和线性化流传播子P）的乘积，并分析了它们在训练中的作用。

Result: 分解揭示了低维潜在动态的成因，并展示了多任务训练中目标对齐的测量方法。

Conclusion: 该研究为理解非线性循环模型中的梯度下降学习提供了新的理论工具和实验支持。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [129] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出了一种模仿学习方法，用于学习符合专家轨迹约束的最大熵策略，通过KL散度连接性能与策略差异，并利用对偶梯度下降优化目标。


<details>
  <summary>Details</summary>
Motivation: 研究如何在模仿学习中结合约束条件，同时最大化策略的熵，以实现更灵活和通用的行为学习。

Method: 基于KL散度的性能边界，将对偶梯度下降应用于优化学习目标，结合强化学习和约束条件。

Result: 实验表明，该方法能有效学习符合多种约束的策略，并能适应不同行为模态和泛化能力。

Conclusion: 该方法在约束遵守和策略多样性方面表现出色，适用于复杂任务和多约束场景。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [130] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 论文研究了无线心电图（ECG）系统的信号完整性保护，比较了CNN、ResNet和混合Transformer-CNN模型在篡改检测中的性能，以及Siamese网络在身份验证中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统在健康监测和身份验证中的普及，保护信号免受篡改变得至关重要。

Method: 研究采用六种篡改策略模拟真实攻击，将一维ECG信号通过连续小波变换（CWT）转换为二维时频表示，并使用54名受试者的数据进行模型训练和评估。

Result: 在高强度篡改场景下，CNN、FeatCNN-TranCNN等模型准确率超过99.5%；在细微篡改中，FeatCNN-TranCNN平均准确率达98%。身份验证中，混合CNN-Transformer Siamese模型实现100%准确率。

Conclusion: 混合Transformer-CNN模型在ECG篡改检测和身份验证中表现出色，尤其是Siamese网络在身份验证中达到完美性能。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [131] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 论文提出了一种名为DICE的方法，用于在去中心化网络中估计数据影响力传播，解决了激励不足的问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习虽能分散计算负载，但缺乏公平的激励机制，阻碍了参与积极性。

Method: 设计了DICE方法，通过理论推导近似影响力传播，考虑数据、通信拓扑和损失曲率。

Result: DICE为选择合适合作者和识别恶意行为提供了基础。

Conclusion: DICE是首个在去中心化环境中估计数据影响力传播的方法，具有实际应用潜力。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [132] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare是一个基于领域适应的深度学习框架，用于预测ICU中罕见疾病的临床结果，解决了数据稀缺和异质性问题，并在多个预测任务中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: ICU中罕见疾病因数据稀缺和异质性而未被充分服务，需要开发新方法以支持临床决策。

Method: 通过自监督预训练学习条件无关表示，并利用条件知识图选择性适应临床相似条件的知识。

Result: 在五个临床预测任务中，KnowRare优于现有模型和ICU评分系统，并展示了灵活性和泛化能力。

Conclusion: KnowRare是一个强大且实用的解决方案，可支持ICU中罕见疾病的临床决策。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [133] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: eegFloss是一个开源Python包，通过eegUsability模型检测睡眠EEG记录中的伪影，提高睡眠研究的准确性。


<details>
  <summary>Details</summary>
Motivation: EEG信号在睡眠研究中易受伪影干扰，导致自动睡眠分期错误，影响研究结果。

Method: 开发eegUsability模型，基于人工标记的EEG数据进行训练和评估，并集成到eegFloss工具中。

Result: eegUsability表现优异（F1-score约0.85，Cohen's kappa 0.78），能高效识别可用EEG数据。

Conclusion: eegFloss通过解决伪影问题，提升了睡眠研究的精确性和可靠性。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [134] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 本文探讨了可解释性作为预测模型在分布外（OOD）数据上行为的工具的潜力与挑战，通过分析Transformer模型的注意力模式与OOD泛化能力的关系。


<details>
  <summary>Details</summary>
Motivation: 研究可解释性是否能预测模型在未见数据上的行为，填补现有研究主要关注干预机制而忽略预测OOD行为的空白。

Method: 在合成分类任务上独立训练数百个Transformer模型，分析其注意力模式与OOD泛化能力的相关性。

Result: 发现简单的可解释性工具（如注意力层次模式）可以预测OOD性能，尤其是当注意力呈现层次结构时，模型倾向于在OOD数据上表现出层次泛化。

Conclusion: 研究为可解释性在预测模型未见行为方面的应用提供了概念验证，鼓励进一步探索。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [135] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: FedPhD是一种新颖的联邦学习方法，用于高效训练扩散模型（DMs），通过分层联邦学习和同质性感知模型聚合解决数据异质性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在联邦学习环境中训练时面临数据异质性和高通信成本的挑战，现有研究对此关注不足。

Method: FedPhD采用分层联邦学习框架，结合同质性感知模型聚合和选择策略，并通过分布式结构化剪枝提升计算效率和降低存储需求。

Result: 实验表明，FedPhD在FID分数上表现优异，通信成本降低88%，计算和通信资源仅需56%，FID提升至少34%。

Conclusion: FedPhD为联邦学习中扩散模型的高效训练提供了有效解决方案，显著提升了性能和资源利用率。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [136] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 该论文提出了首个自动化框架，用于为蛋白质语言模型（PLM）中的每个神经元标注基于生物学的自然语言描述，并开发了一种新的神经元激活引导方法，用于生成具有目标特性的蛋白质。


<details>
  <summary>Details</summary>
Motivation: 理解PLM内部神经元的生物学意义，并利用这些信息指导蛋白质设计。

Method: 引入自动化框架标注神经元，开发神经元激活引导的蛋白质生成方法。

Result: 揭示了神经元对多种生物化学和结构特性的选择性敏感，并成功生成具有目标特性的蛋白质。

Conclusion: 该方法不仅揭示了PLM的神经元分布规律，还为蛋白质设计提供了新工具。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [137] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 论文提出了一种名为SignSoftSGD（S3）的新型优化器，通过改进Adam的机制，解决了其梯度波动和损失尖峰的问题，并在理论和实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: Adam在深度神经网络训练中表现优异，但其成功机制和局限性尚未充分研究。本文旨在揭示Adam的有效性源于其与SignSGD的相似性，同时解决其因更新缩放失控导致的损失尖峰问题。

Method: 提出SignSoftSGD（S3），采用三方面创新：1）使用灵活的p阶动量替代传统二阶动量；2）统一分子和分母的指数移动平均系数以减少损失尖峰；3）引入等效的Nesterov加速梯度模块。

Result: 理论证明S3在非凸随机优化中达到最优收敛率O(1/T^{1/4})。实验显示S3收敛更快、性能更优，且在高学习率下仍稳定。

Conclusion: S3在效率和最终任务性能上均优于AdamW，验证了其作为优化器的有效性。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [138] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: FMSP利用基础模型的能力改进自博弈算法，解决传统自博弈的局限性，提出三种方法并在实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈算法（SP）容易陷入局部最优且缺乏多样性，FMSP通过基础模型的代码生成能力和广泛知识克服这些问题。

Method: 提出三种FMSP方法：vFMSP（持续优化策略）、NSSP（忽略性能的策略多样性）和QDSP（结合多样性和质量）。在Car Tag和Gandalf实验中验证。

Result: FMSP在Car Tag中超越人工策略，在Gandalf中成功突破LLM防御并自动修复漏洞。

Conclusion: FMSP为自博弈算法开辟了新方向，提升了策略发现的多样性和质量。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [139] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 论文提出了一种双视图图表示学习方法MimbFD，用于解决基于GNN的欺诈检测中拓扑和类别不平衡问题，通过拓扑消息可达性和局部混淆去偏模块优化节点表示，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在欺诈检测中因局部交互和类别不平衡导致全局拓扑信息传播不均，节点信息易被掩盖，影响下游任务表现。

Method: 提出MimbFD方法，包含拓扑消息可达性模块（穿透欺诈伪装）和局部混淆去偏模块（平衡类别影响），优化节点表示。

Result: 在三个公开欺诈数据集上的实验表明，MimbFD在欺诈检测中表现优异。

Conclusion: MimbFD通过双视图设计有效缓解了消息不平衡问题，提升了欺诈检测性能。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [140] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedDifRC的新方法，利用扩散模型解决联邦学习中的数据异构性问题，通过文本驱动的对比学习和噪声驱动的一致性正则化提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性导致模型收敛和性能问题，需要一种新方法来缓解这一问题。

Method: 引入扩散模型，提出FedDifRC方法，结合文本驱动的对比学习和噪声驱动的一致性正则化。

Result: 实验验证了FedDifRC的有效性，并展示了关键组件的高效性。

Conclusion: FedDifRC通过扩散模型的指导成功缓解了数据异构性问题，提升了联邦学习的性能。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [141] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: MoFE-Time是一种创新的时间序列预测模型，通过结合时间和频域特征，在预训练-微调范式中显著提升了复杂时间序列的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预训练-微调范式中未能同时建模时间和频域特征，导致对复杂时间序列的预测性能不佳。

Method: 提出MoFE-Time模型，在注意力模块后引入时间和频率单元作为专家，利用MoE路由机制构建多维稀疏表示。

Result: 在六个公开基准测试中，MoFE-Time达到了新的最优性能，MSE和MAE分别降低了6.95%和6.02%。在私有数据集NEV-sales上也表现出色。

Conclusion: MoFE-Time通过结合时间和频域特征，在理论和实际应用中均表现出卓越的性能。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [142] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 提出了一种新的单调后校准方法，通过约束优化确保概率输出的排序，同时保持表达能力、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常产生错误校准的概率估计，导致预测过度自信，现有方法无法同时保证单调性和表达能力。

Method: 采用线性参数化的约束校准映射，通过约束优化问题确保单调性。

Result: 在多个数据集和模型上表现优于现有校准方法，同时数据与计算效率高。

Conclusion: 提出的方法在保持单调性的同时，实现了高性能和高效性。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [143] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: 提出了一种名为AdaDPIGU的差分隐私SGD框架，通过重要性梯度更新和自适应剪枝机制，在高维设置下提升性能，同时满足差分隐私和收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法在高维设置下因噪声增加导致性能下降，需改进以兼顾隐私和模型性能。

Method: 预训练阶段使用差分隐私高斯机制估计参数重要性；梯度更新阶段剪枝低重要性坐标并引入自适应裁剪机制。

Result: 在MNIST和CIFAR-10上验证了有效性，隐私预算下性能接近甚至超过非隐私模型。

Conclusion: AdaDPIGU通过自适应稀疏化同时提升隐私性和实用性，适用于高维深度学习任务。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [144] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: 论文提出CircuitGCL，一种图对比学习框架，通过表示散射和标签重平衡解决AMS电路图表示学习中的挑战，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决AMS电路图表示学习中数据稀缺、标签分布不平衡和电路多样性带来的挑战。

Method: 提出CircuitGCL框架，结合自监督学习（拓扑不变节点嵌入）和标签重平衡损失（平衡MSE和bsmCE）。

Result: 在TSMC 28nm AMS设计上，CircuitGCL在边回归和节点分类任务中优于现有方法，性能显著提升。

Conclusion: CircuitGCL通过表示散射和标签重平衡，实现了鲁棒且可迁移的电路图表示学习。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [145] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯优化方法，通过联合学习最优模型和非近视采集策略，直接优化多步遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法依赖手工设计的采集函数和代理模型，且通常是近视的。本文旨在解决这些问题。

Method: 使用高斯过程集成生成模拟轨迹，训练决策变换器直接学习选择查询点，采用离线密集训练和在线稀疏学习。

Result: 在合成和真实基准测试中表现优于基线，实现了更低的简单遗憾和更鲁棒的探索。

Conclusion: 提出的方法在贝叶斯优化中表现出色，尤其在复杂或噪声环境中更具优势。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [146] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGPS是一种用于AMS电路中寄生效应预测的小样本学习方法，通过异构图表示电路网表，结合小跳采样和图Transformer，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于集成电路设计数据稀缺，训练深度学习模型用于AMS设计受到限制，因此需要一种高效的小样本学习方法。

Method: 将电路网表表示为异构图，采用小跳采样技术生成子图，使用混合图Transformer学习子图嵌入，并结合低成本位置编码。

Result: CircuitGPS在耦合存在性预测上提升至少20%，电容估计的MAE降低至少0.067，并展示了强大的零样本学习能力。

Conclusion: CircuitGPS为AMS电路设计提供了一种高效且可扩展的解决方案，同时为图表示学习提供了有价值的见解。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [147] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的2阶段模型，用于在SRAM电路的预布局阶段准确预测寄生效应，显著提升了仿真效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决SRAM电路中预布局与后布局仿真之间的寄生效应差异问题，减少设计迭代次数。

Method: 结合图神经网络（GNN）分类器和多层感知器（MLP）回归器，采用Focal Loss处理类别不平衡，并集成子电路信息以抽象层次结构。

Result: 在4个实际SRAM设计中，误差最大减少19倍，仿真速度提升高达598倍。

Conclusion: 该方法有效预测寄生效应，显著提升设计效率和仿真速度。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [148] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 提出了一种基于异构图注意力网络的方法，用于多域电力系统状态预测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源和分布式能源的引入而变得复杂，需要可靠的短期状态预测以确保稳定运行。

Method: 使用异构图注意力网络（Heterogeneous Graph Attention Networks）建模同质和异质传感器关系，整合液压和电气两个物理域的数据。

Result: 实验表明，该方法在归一化均方根误差上平均优于传统基线35.5%。

Conclusion: 该方法在多域、多速率电力系统状态预测中表现出色，具有实际应用潜力。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [149] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: LoRAM是一种基于更新幅度的初始化方案，通过优化幅度调节提升LoRA性能，匹配或超越谱初始化方法，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: 解决谱初始化方法在计算和存储上的额外开销问题，同时提升LoRA的性能和收敛性。

Method: 提出LoRAM，一种基于幅度的初始化方案，利用确定性正交基和预训练权重幅度模拟谱增益。

Result: LoRAM在实验中表现优异，匹配或超越谱初始化方法，同时保持高效性。

Conclusion: LoRAM通过优化幅度调节，提供了一种高效且性能优越的LoRA初始化方案。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [150] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种分布式推理方法，通过优化边缘服务器上的专家缓存以减少MoE模型的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在边缘设备上存储负担重的问题，通过分散专家网络实现分布式推理。

Method: 针对Top-$K$专家选择策略，设计了贪婪算法和动态规划方法，优化专家缓存以减少延迟。

Result: 模拟实验表明，该方法显著降低了推理延迟。

Conclusion: 提出的方法有效解决了MoE模型在边缘设备上的存储和延迟问题。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [151] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于样本中心的RLVR框架LPPO，通过前缀引导采样和学习进度加权优化小规模高质量示范数据的使用，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效利用少量高质量示范数据，而非单纯扩大数据规模，以提升强化学习在语言模型中的推理能力。

Method: 提出前缀引导采样和学习进度加权两种技术，前者利用专家示范的部分解前缀引导策略，后者动态调整样本权重以促进学习。

Result: 在数学推理基准测试中，LPPO方法表现优于基线，收敛更快且性能上限更高。

Conclusion: LPPO框架通过优化样本利用方式，显著提升了模型性能，为小规模高质量数据的高效使用提供了新思路。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [152] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 论文提出了一种基于“预测信息增益”的方法，用于探索环境中信息量最大的区域，并通过强化学习找到高效的探索策略。


<details>
  <summary>Details</summary>
Motivation: 在无法获得明确环境动态模型的情况下，如何通过探索学习可控动态模型。

Method: 使用“预测信息增益”作为信息度量，结合强化学习方法寻找高效的探索策略。

Result: 该方法优于几种短视探索方法，能够可靠地估计环境中的可控动态。

Conclusion: 通过信息增益驱动的探索策略，可以有效地学习环境动态模型。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [153] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的通用化框架，用于解决现代无线接入网络中的资源管理问题，通过图注意力网络和分布式训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代无线接入网络环境动态且异构，传统基于规则的资源管理算法性能不足，而强化学习在泛化性上面临挑战。

Method: 采用图注意力网络编码拓扑和节点属性，结合域随机化和分布式数据生成，集中训练以适应多样化网络条件。

Result: 在多个5G基准测试中，该策略显著提升了吞吐量和频谱效率，尤其在高速移动场景下表现更优。

Conclusion: 该框架为AI原生的6G无线接入网络提供了一种通用化强化学习解决方案。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [154] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过多β值的VAE和扩散模型，平衡解纠缠与生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统β-VAE中解纠缠与重建质量之间的权衡问题。

Method: 训练单VAE，结合新损失函数控制潜在表示信息，引入非线性扩散模型平滑过渡不同β值的表示。

Result: 实现了高质量生成与解纠缠，支持无输入图像的样本生成，潜在空间平滑过渡。

Conclusion: 新框架有效平衡了解纠缠与生成质量，支持多样本生成与潜在空间操作。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [155] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为CTPG的新框架，通过跨任务策略指导加速技能学习，并引入两种门控机制提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注参数共享，但忽略了利用已掌握任务的策略直接指导未掌握任务。

Method: CTPG为每个任务训练一个指导策略，从所有任务的控制策略中选择行为策略，并引入两种门控机制优化学习效率。

Result: 实验表明，CTPG与现有参数共享方法结合显著提升了性能。

Conclusion: CTPG是一种通用框架，能有效利用跨任务相似性加速学习。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [156] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 论文提出SAD-DPSGD方法，通过线性衰减机制优化噪声和裁剪阈值，解决医疗图像分类中数据泄漏问题，在HAM10000数据集上表现优于Auto-DPSGD。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分类中数据泄漏问题严重，传统方法在小型不平衡数据集（如HAM10000）上效果不佳，导致模型陷入次优解。

Method: 提出SAD-DPSGD，采用线性衰减机制动态调整噪声和裁剪阈值，初始阶段分配更多隐私预算和高裁剪阈值。

Result: 在HAM10000数据集上，SAD-DPSGD比Auto-DPSGD准确率提高2.15%（ε=3.0，δ=10^-3）。

Conclusion: SAD-DPSGD有效解决了不平衡数据集上的隐私保护问题，提升了模型性能。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [157] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD是一个通用的异常检测框架，通过利用标记数据集训练单一模型，避免了传统方法中繁琐的超参数调优和模型训练，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法需要针对不同数据集进行超参数调优和模型训练，过程繁琐且成本高。UniOD旨在通过通用框架解决这一问题。

Method: UniOD将数据集转换为多个图，生成一致的节点特征，并将异常检测任务转化为节点分类问题，从而泛化到未见过的领域。

Result: 在15个基准数据集上与15种先进基线方法对比，UniOD表现出色。

Conclusion: UniOD避免了模型选择和超参数调优的麻烦，降低了计算成本，并有效利用了历史数据集的知识，提升了实际应用的便利性和准确性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [158] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: GO-Skill是一种离线多任务强化学习方法，通过目标导向的技能提取和分层策略学习，提升任务间的知识共享和性能。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习面临任务间知识共享的挑战，受人类学习启发，提出GO-Skill以提取和利用可重用技能。

Method: 通过目标导向技能提取和向量量化构建离散技能库，引入技能增强阶段优化技能，并采用分层策略学习动态协调技能。

Result: 在MetaWorld机器人操作任务上的实验验证了GO-Skill的有效性和通用性。

Conclusion: GO-Skill通过技能抽象和分层策略学习，显著提升了离线多任务强化学习的性能。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [159] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种基于网格数据结构的过拟合检测与预防方法，通过拉普拉斯算子二阶导数优化超参数，减少模型振荡。


<details>
  <summary>Details</summary>
Motivation: 解决回归问题中数据过拟合的问题，特别是在网格数据结构中，通过拉普拉斯算子计算熵来优化模型。

Method: 在原始训练网格上计算训练数据的导数作为真实标签，在交错网格上计算训练数据的导数以识别振荡，利用拉普拉斯算子导数损失优化超参数。

Result: 通过最小化模型熵减少了不必要的振荡，无需从训练数据中分离测试点。

Conclusion: 该方法有效减少了过拟合，并通过拉普拉斯算子在交错网格上的应用作为替代测试指标。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [160] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了一种新的个体治疗效果估计算法，结合多头注意力机制和线性正交正则化器，通过软分解预处理变量并消除选择偏差，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中估计个体治疗效果是因果推断的核心问题，现有方法在精确分解变量方面存在不足。

Method: 提出了一种结合多头注意力机制和线性正交正则化器的算法，通过软分解预处理变量，并使用重要性采样重加权技术消除选择偏差。

Result: 在公开的半合成和真实数据集上的实验表明，该算法优于现有方法。

Conclusion: 该算法在个体治疗效果估计中表现出色，为因果推断提供了新思路。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [161] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文探讨如何通过机器学习和联邦学习改进模糊系统，以处理不确定性，并讨论了潜在的限制和改进空间。


<details>
  <summary>Details</summary>
Motivation: 模糊系统能处理不确定性，但仍有改进空间。机器学习和联邦学习的新技术可为其提供灵感。

Method: 结合机器学习和联邦学习的方法，如更新模糊规则，以优化模糊系统。

Result: 提出了改进模糊系统的潜在方法，但需进一步研究验证其效果。

Conclusion: 改进模糊系统的潜力存在，但需更多研究以确定其实际效果。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [162] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文提出了一种改进的模仿学习观察方法（IfO），通过利用无动作演示数据，避免了传统方法对动作标签或奖励函数的需求，并研究了更复杂的数据分布。


<details>
  <summary>Details</summary>
Motivation: 当前IfO研究多集中于理想化的双峰数据分布，限制了结果的实用性。本文旨在探索更复杂的数据分布，并提出一种方法以实现迭代式的自我改进模仿学习。

Method: 该方法将基于强化学习的模仿学习应用于无动作演示数据，通过价值函数在专家与非专家数据间传递信息。

Result: 通过全面评估，本文揭示了不同数据分布与算法适用性之间的关系，并指出了现有方法的局限性。

Conclusion: 研究结果为开发更鲁棒和实用的IfO技术提供了有价值的见解，为实现可扩展的行为学习铺平了道路。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [163] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 提出了一种基于自适应物理信息神经网络（PINN-Obs）的非线性系统状态估计方法，优于传统模型观测器。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的状态估计在部分和噪声测量下具有挑战性，传统方法需要显式系统变换或线性化。

Method: 直接整合系统动力学和传感器数据到物理信息学习过程中，自适应学习最优增益矩阵。

Result: 理论分析证明收敛性，数值模拟验证其在多种非线性系统中的优越性。

Conclusion: PINN-Obs在准确性、鲁棒性和适应性上优于现有观测器设计。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [164] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: MAD框架结合物理定律与数据驱动学习，通过生成物理嵌入的合成数据，解决微分方程学习中的数据依赖和效率-精度权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要昂贵标注数据，而模型驱动方法存在效率-精度权衡，MAD旨在消除这些限制。

Method: 利用微分方程的内在数学结构生成物理嵌入的解析解和合成数据，实现高效算子学习。

Result: 在2D参数问题中展示了MAD的泛化能力和高效性/准确性。

Conclusion: MAD有望成为科学计算中物理信息机器智能的通用范式。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [165] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种参数化激活函数（Leaky Exponential Linear Unit）用于改进多维非线性数据回归，通过平滑性和梯度特性提升神经网络性能，并提出新的扩散损失度量评估过拟合。


<details>
  <summary>Details</summary>
Motivation: 非线性激活函数对学习非线性数据集至关重要，但现有平滑但梯度消失的激活函数（如ELU、SiLU）性能有限，非平滑激活函数（如RELU、Leaky-RELU）会导致模型不连续。

Method: 提出平滑且梯度非零的Leaky Exponential Linear Unit激活函数，并设计扩散损失度量评估模型过拟合。

Result: Leaky Exponential Linear Unit在性能上优于现有激活函数，扩散损失度量有效评估了模型过拟合。

Conclusion: 平滑且梯度非零的激活函数能显著提升神经网络性能，扩散损失度量为模型评估提供了新工具。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [166] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑泛化界，避免了复杂的信息论项，通过轨迹稳定性框架将泛化误差与拓扑数据分析和算法稳定性联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑泛化界依赖复杂的信息论项，难以应用于实际算法（如ADAM），因此需要提出更易解释且实用的泛化界。

Method: 引入轨迹稳定性框架，扩展假设集稳定性概念，证明泛化误差可由轨迹的拓扑复杂性和算法稳定性参数上界控制。

Result: 实验表明，拓扑数据项在泛化界中起重要作用，尤其在训练样本增加时，解释了拓扑泛化界的实证成功。

Conclusion: 论文提出的轨迹稳定性框架为拓扑泛化界提供了更实用和可解释的理论基础。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [167] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 本文提出了一种新型语音分词器，同时编码语言和声学信息，显著提升了语音表示保真度，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于残差向量量化（RVQ）的方法常忽略关键声学特征，本文旨在解决这一问题。

Method: 提出一种同时编码语言和声学信息的先进方法，保留韵律和情感内容。

Result: 实验证明该方法在语音编码、语音转换、情感识别和多模态语言建模中有效，无需额外训练。

Conclusion: 该方法具有广泛适用性，有望成为AI驱动语音处理的关键工具。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [168] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR提出了一种无需架构修改或损失函数变化的联邦学习方法，通过利用训练信号实现高效模型聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法需要修改架构或损失函数，LIVAR旨在利用现有训练信号实现高效聚合。

Method: LIVAR采用方差加权分类器聚合和基于SHAP分析的LoRA合并技术。

Result: LIVAR在多个基准测试中达到最优性能，且与现有FL方法无缝集成。

Conclusion: LIVAR证明仅通过现有训练信号即可实现高效模型聚合，为联邦学习提供了新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [169] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文对原型模型（如ProtoPNet、ProtoPool和PIPNet）进行了深入分析，提出了一套全面的评估指标，包括新指标，以增强模型可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 原型模型在可解释人工智能（XAI）和可解释机器学习中具有重要意义，但缺乏全面的评估方法。

Method: 应用多种数据集（细粒度分类、非独立同分布设置和多标签分类）和标准及新提出的指标评估原型模型。

Result: 通过实验对比了原型模型在不同数据集上的性能，并提供了开源代码库以方便应用和扩展。

Conclusion: 研究为原型模型的评估提供了新工具和方法，促进了XAI领域的发展。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [170] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 提出了一种名为HeLo的多模态情感分布学习框架，旨在挖掘多模态数据的异质性和互补信息，以及混合基本情感之间的标签相关性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分布学习方法在多模态异质性挖掘和标签相关性利用方面存在不足。

Method: 采用交叉注意力融合生理数据，设计基于最优传输的异质性挖掘模块，并引入可学习的标签嵌入和相关性矩阵。

Result: 在两个公开数据集上验证了方法的优越性。

Conclusion: HeLo框架在多模态情感分布学习中表现出色。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [171] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 介绍了一个基于Generals.io的实时策略游戏环境，兼容Gymnasium和PettingZoo，支持高性能运行，并训练了一个顶级水平的参考代理。


<details>
  <summary>Details</summary>
Motivation: 为多智能体强化学习研究提供一个易用且具有挑战性的平台。

Method: 结合监督预训练和自我对弈训练参考代理，并采用基于潜在奖励塑造和记忆特征加速学习。

Result: 参考代理在36小时内达到1v1人类排行榜前0.003%的水平。

Conclusion: 提供了一个模块化的RTS基准和竞争性基线代理，推动了多智能体强化学习研究。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [172] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种结合迭代方法和路径条件化的方法，以提升高斯过程在大规模数据下的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在经典框架下难以适应大规模数据和现代并行计算硬件，因此需要改进其可扩展性。

Method: 通过结合迭代方法和路径条件化，将昂贵计算转化为线性方程组的求解，利用迭代线性系统求解器实现。

Result: 显著降低了内存需求，适用于更大规模数据，并将矩阵乘法作为主要计算操作，适合现代硬件。

Conclusion: 该方法为高斯过程在现代大规模计算环境中的应用提供了有效途径。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [173] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra是一种基于扩散模型的生成框架，直接从多模态光谱数据推断2D和3D分子结构，解决了传统方法和现有机器学习方法在分子结构解析中的局限性。


<details>
  <summary>Details</summary>
Motivation: 分子结构解析是化学中的基础问题，传统方法依赖专家解释且缺乏扩展性，现有机器学习方法受限于有限库和忽略3D几何。

Method: DiffSpectra采用扩散模型，结合SE(3)-equivariant架构和基于Transformer的光谱编码器，实现多模态光谱数据的条件生成。

Result: 实验显示，DiffSpectra在结构解析中表现出色，top-1准确率16.01%，top-20准确率96.86%。

Conclusion: DiffSpectra首次统一了多模态光谱推理和2D/3D生成建模，为分子结构解析提供了有效解决方案。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [174] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究了在线情境下决策者与上下文带背包的强盗问题（BwK）的交互，设计了实现次线性遗憾的算法。


<details>
  <summary>Details</summary>
Motivation: 解决动态定价和首次价格拍卖等应用中资源分配的非平稳性和上下文多样性问题。

Method: 设计了一种在线算法，假设访问一个置信边界预言机，克服了无界状态空间的技术挑战。

Result: 算法在T次情节中实现了次线性遗憾，并在某些设置中提供了改进的遗憾界限。

Conclusion: 框架为上下文BwK文献提供了新的视角，特别是在未标记特征数据的情况下。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [175] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种联邦因果发现方法，通过高阶累积量在水平和垂直联邦设置下学习因果结构，解决了数据隐私和变量不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果结构学习方法主要针对水平联邦设置，但在实际中，不同客户端可能包含不同变量，导致虚假因果关系。

Method: 通过聚合客户端的高阶累积量信息构建全局估计，递归识别源因果，生成全局因果强度矩阵。

Result: 实验表明，该方法在合成和真实数据上表现优异，能重建因果图并估计因果强度系数。

Conclusion: 该方法在水平和垂直联邦设置下均有效，解决了变量不完整和隐私保护问题。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [176] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: 论文提出ReMix方法，通过混合策略近端策略梯度、KL凸约束和策略重生，使PPO和GRPO等RFT方法能利用离策略数据，显著降低训练成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RFT方法多为同策略RL，未能充分利用历史数据，导致计算和时间成本高昂，限制了经济高效的扩展。

Method: ReMix包含混合策略近端策略梯度、KL凸约束和策略重生三部分，提升训练效率并平衡稳定性与灵活性。

Result: 在多个数学推理基准测试中，ReMix以极低训练成本（30x至450x降低）达到SOTA性能，如1.5B模型Pass@1准确率52.10%。

Conclusion: ReMix通过离策略数据利用和高效训练策略，显著提升了RL在LLM中的应用效率，同时揭示了离策略差异的潜在影响。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [177] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的动态滑动窗口优化方法（RL-Window），用于处理多维数据流，适应动态变化如概念漂移或突发模式。


<details>
  <summary>Details</summary>
Motivation: 多维数据流（如IoT、金融市场）具有高速度、无界性和复杂维度间依赖关系，固定窗口难以适应动态变化。

Method: 将窗口大小选择建模为强化学习问题，使用Dueling DQN和优先经验回放处理非平稳性和高维性。

Result: 在多个基准数据集上，RL-Window在分类准确性、漂移鲁棒性和计算效率上优于现有方法（如ADWIN、CNN-Adaptive）。

Conclusion: RL-Window具有适应性和稳定性，适用于实时应用。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [178] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 提出了一种N版本机器学习框架（NVML），通过安全感知加权软投票机制提升交通标志识别系统在对抗攻击下的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中交通标志识别易受对抗攻击影响，威胁驾驶安全，需提升其鲁棒性。

Method: 采用NVML框架，结合FMEA评估安全风险，动态分配权重给集成输出，测试了三种投票机制对FGSM和PGD攻击的鲁棒性。

Result: 实验表明，NVML显著提升了对抗条件下的系统鲁棒性和安全性。

Conclusion: NVML框架有效增强了交通标志识别系统的安全性，适用于对抗环境。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [179] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 该论文提出了一种评估基础模型是否真正捕获深层结构的技术，发现模型在适应新任务时可能无法发展出与底层世界模型一致的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型是否通过序列预测真正理解深层领域结构，类似于开普勒预测行星运动后牛顿力学的发现。

Method: 开发了一种称为归纳偏差探针的技术，通过生成合成数据集并测试模型适应性来评估其归纳偏差是否与世界模型一致。

Result: 基础模型在训练任务上表现优异，但在适应新任务时未能发展出与底层世界模型一致的归纳偏差，尤其是在轨道轨迹任务中未能应用牛顿力学。

Conclusion: 基础模型可能仅发展出任务特定的启发式方法，而未能实现泛化能力。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [180] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 论文研究了在噪声数据下，物理信息神经网络（PINNs）逼近偏微分方程解的条件，证明了网络规模的下界，并验证了实际应用中的可行性。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，数据通常带有噪声，但目前对PINNs在噪声条件下有效性的理解有限。

Method: 通过理论分析证明了网络规模的下界，并通过实验验证了PINNs在噪声条件下可以达到低于监督数据方差的经验风险。

Result: 证明了网络规模需满足$d_N\log d_N\gtrsim N_s \eta^2$，且实验验证了PINNs在HJB PDE中的应用。

Conclusion: 研究为理解噪声条件下训练PINNs的参数需求奠定了基础。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [181] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [182] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 论文提出两种新的损失函数（MV-InfoNCE和MV-DHEL），解决了多视图对比学习中存在的四个关键问题，并在实验中验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习方法在处理多视图数据时存在优化冲突、视图间交互不足、对齐-均匀性耦合等问题，无法充分利用多视图的优势。

Method: 提出MV-InfoNCE和MV-DHEL两种损失函数，前者同时建模所有视图交互，后者解耦对齐与均匀性并支持多模态扩展。

Result: 在ImageNet1K等数据集上，新方法显著优于现有多视图方法，且能有效利用更多视图避免维度塌缩。

Conclusion: 新方法为多视图对比学习提供了理论支持和实际优势，尤其在多模态场景下表现出色。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [183] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed是一个生成多表时间序列电子健康记录（EHR）数据的框架，通过文本表示和压缩技术，无需复杂预处理即可捕捉复杂结构和时间动态。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和监管限制，真实EHR数据难以共享和利用，需要生成合成数据。现有方法通常仅生成专家选择的特征，无法模拟原始EHR的复杂性。

Method: RawMed使用文本表示和压缩技术，生成多表时间序列数据，并提出新的评估框架，涵盖分布相似性、表间关系、时间动态和隐私。

Result: 在两个开源EHR数据集上验证，RawMed在保真度和实用性上优于基线模型。

Conclusion: RawMed是首个能生成接近原始EHR的合成数据框架，为医疗研究提供了新工具。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [184] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在高斯数据分布去模糊任务中的准确性，通过计算Wasserstein距离比较理论解与扩散模型解之间的差异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为贝叶斯逆问题的先验，因其灵活性和高方差受到关注，但其性能尚不明确。

Method: 在约束的高斯数据分布下，计算扩散模型采样器分布与理想解分布之间的Wasserstein距离。

Result: 研究结果允许比较文献中不同算法的性能。

Conclusion: 扩散模型在特定任务中的准确性可以通过理论分析进行评估，为算法比较提供了依据。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [185] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 研究在资源受限的智能电表上进行边缘端模型训练的可行性，通过光伏功率预测案例验证了两种机器学习模型和低精度训练方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 推动电网边缘智能化和设备端训练的概念，以提升智能电表的自主性和效率。

Method: 介绍了设备端训练的技术准备步骤，并设计了混合和降低精度的训练方案以适应资源限制。

Result: 实验证明，通过现有先进计量基础设施，经济高效地实现电网边缘智能是可行的。

Conclusion: 资源受限设备上的边缘端模型训练具有实际应用潜力，为电网智能化提供了新思路。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [186] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME是一种新型的MSA设计模型，利用预训练蛋白质语言模型的进化嵌入，提升低同源性和孤儿蛋白质的结构预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有折叠模型对多序列比对（MSA）的依赖问题，特别是在低同源性和孤儿蛋白质中MSA信息稀疏或缺失的情况。

Method: PLAME结合预训练表示增强进化信息，采用保守性-多样性损失提升生成质量，并提出新的MSA筛选方法和序列质量评估指标。

Result: 在AlphaFold2和AlphaFold3基准测试中，PLAME在折叠增强和序列质量评估方面达到最先进性能。

Conclusion: PLAME不仅提升了预测性能，还可作为适配器实现AlphaFold2级精度与ESMFold的推理速度。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [187] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）在资源受限的边缘设备上实现高效自监督学习（SSL），通过定制策略减少资源消耗达4倍。


<details>
  <summary>Details</summary>
Motivation: 解决CL和SSL在边缘设备上因数据量大和计算资源需求高而难以部署的问题。

Method: 分析不同SSL技术在有限计算、数据和能源预算下的适应性，评估其在资源受限环境中学习鲁棒表示的有效性。

Result: 定制SSL策略在保持性能的同时，资源消耗减少高达4倍。

Conclusion: SSL技术适用于边缘设备，能实现高效节能学习。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [188] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成嵌入的语义缓存方法，通过结合多个嵌入模型提升LLM系统中语义相似性检测的效果。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存框架依赖单一嵌入模型，无法充分捕捉真实查询中的多样语义关系。

Method: 采用集成嵌入方法，通过训练的元编码器结合多个嵌入模型，并在QQP数据集上评估性能。

Result: 集成方法实现了92%的缓存命中率，同时保持85%的非等效查询拒绝准确率。

Conclusion: 集成嵌入方法显著优于单一模型，能更有效区分语义相似与不相似的查询，提升缓存性能并减少计算开销。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [189] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: DCE框架通过频率感知专家组和动态专家选择器解决DIL中的类不平衡和跨域分布偏移问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: DIL在非平稳环境中面临类内不平衡和跨域分布偏移的挑战，导致模型性能下降。

Method: 提出DCE框架，包括频率感知专家组和动态专家选择器，分别处理类内不平衡和跨域知识迁移。

Result: 在四个基准数据集上，DCE表现出最先进的性能。

Conclusion: DCE有效解决了DIL中的关键挑战，为类不平衡和跨域学习提供了新思路。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [190] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究发现小批量训练（甚至批量大小为1）在语言模型预训练和微调中表现稳定，且性能优于大批量训练。提出了Adam超参数调整规则，并建议避免梯度累积。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为小批量训练不稳定，但本研究挑战这一观点，探索小批量训练的潜力。

Method: 重新评估小批量训练，提出Adam超参数调整规则，并测试不同批量大小的性能。

Result: 小批量训练稳定、对超参数选择更鲁棒、性能更优，且支持SGD稳定训练。

Conclusion: 建议选择小批量训练并调整超参数，避免不必要的梯度累积。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [191] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究发现，组合泛化能力由数据多样性而非数据规模驱动，线性分解的表征结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨当代视觉模型是否具备组合理解能力，以及数据规模和多样性对组合泛化的影响。

Method: 通过控制实验系统变化数据规模、概念多样性和组合覆盖率，分析预训练模型（DINO、CLIP）的表现。

Result: 组合泛化能力依赖于数据多样性，线性分解的表征结构能实现高效泛化。预训练模型表现高于随机但仍有不足。

Conclusion: 应更注重构建多样性数据集，并关注支持高效组合学习的表征结构。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [192] [MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing](https://arxiv.org/abs/2507.06329)
*Michael Clemens,Ana Marasović*

Main category: cs.SD

TL;DR: MixAssist是一个音频-语言数据集，用于捕捉音乐制作中的协作对话，旨在支持AI助手在音乐混音中的创造性过程。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究多关注自动化或生成，忽视了协作与教学维度，MixAssist填补了这一空白，为业余音乐制作人提供支持。

Method: 构建MixAssist数据集，包含431个音频对话，用于训练和评估音频-语言模型，如Qwen-Audio。

Result: Qwen-Audio在MixAssist上表现优异，生成更相关、有帮助的混音建议。

Conclusion: MixAssist为开发支持音乐混音创造性过程的AI助手提供了重要资源。

Abstract: While AI presents significant potential for enhancing music mixing and
mastering workflows, current research predominantly emphasizes end-to-end
automation or generation, often overlooking the collaborative and instructional
dimensions vital for co-creative processes. This gap leaves artists,
particularly amateurs seeking to develop expertise, underserved. To bridge
this, we introduce MixAssist, a novel audio-language dataset capturing the
situated, multi-turn dialogue between expert and amateur music producers during
collaborative mixing sessions. Comprising 431 audio-grounded conversational
turns derived from 7 in-depth sessions involving 12 producers, MixAssist
provides a unique resource for training and evaluating audio-language models
that can comprehend and respond to the complexities of real-world music
production dialogues. Our evaluations, including automated LLM-as-a-judge
assessments and human expert comparisons, demonstrate that fine-tuning models
such as Qwen-Audio on MixAssist can yield promising results, with Qwen
significantly outperforming other tested models in generating helpful,
contextually relevant mixing advice. By focusing on co-creative instruction
grounded in audio context, MixAssist enables the development of intelligent AI
assistants designed to support and augment the creative process in music
mixing.

</details>


### [193] [IMPACT: Industrial Machine Perception via Acoustic Cognitive Transformer](https://arxiv.org/abs/2507.06481)
*Changheon Han,Yuseop Sim,Hoin Jung,Jiho Lee,Hojun Lee,Yun Seok Kang,Sucheol Woo,Garam Kim,Hyung Wook Park,Martin Byung-Guk Jun*

Main category: cs.SD

TL;DR: 论文提出了DINOS数据集和IMPACT模型，用于工业机器声音分析，解决了现有方法泛化性差和数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 工业机器的声音信号对异常检测和预测性维护很有价值，但现有方法泛化性差且缺乏大规模数据集。

Method: 提出了DINOS数据集和IMPACT模型，后者通过自监督学习联合优化全局和局部损失。

Result: IMPACT在30个下游任务中优于现有模型，尤其在24个任务中表现更优。

Conclusion: DINOS和IMPACT为工业声音分析提供了新的基准和高效解决方案。

Abstract: Acoustic signals from industrial machines offer valuable insights for anomaly
detection, predictive maintenance, and operational efficiency enhancement.
However, existing task-specific, supervised learning methods often scale poorly
and fail to generalize across diverse industrial scenarios, whose acoustic
characteristics are distinct from general audio. Furthermore, the scarcity of
accessible, large-scale datasets and pretrained models tailored for industrial
audio impedes community-driven research and benchmarking. To address these
challenges, we introduce DINOS (Diverse INdustrial Operation Sounds), a
large-scale open-access dataset. DINOS comprises over 74,149 audio samples
(exceeding 1,093 hours) collected from various industrial acoustic scenarios.
We also present IMPACT (Industrial Machine Perception via Acoustic Cognitive
Transformer), a novel foundation model for industrial machine sound analysis.
IMPACT is pretrained on DINOS in a self-supervised manner. By jointly
optimizing utterance and frame-level losses, it captures both global semantics
and fine-grained temporal structures. This makes its representations suitable
for efficient fine-tuning on various industrial downstream tasks with minimal
labeled data. Comprehensive benchmarking across 30 distinct downstream tasks
(spanning four machine types) demonstrates that IMPACT outperforms existing
models on 24 tasks, establishing its superior effectiveness and robustness,
while providing a new performance benchmark for future research.

</details>


### [194] [STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation](https://arxiv.org/abs/2507.06670)
*Wenxiang Guo,Yu Zhang,Changhao Pan,Zhiyuan Zhu,Ruiqi Li,Zhetao Chen,Wenhao Xu,Fei Wu,Zhou Zhao*

Main category: cs.SD

TL;DR: STARS是一个统一的框架，首次同时解决歌唱转录、对齐和风格标注问题，提供多级注释，显著提升歌唱数据集的可扩展性和可控性。


<details>
  <summary>Details</summary>
Motivation: 高质量歌唱数据集的手动标注成本高昂，现有自动标注方法仅解决部分问题，需要一种全面的解决方案。

Method: STARS采用分层声学特征处理，结合非自回归局部声学编码器，实现多级注释，包括音素对齐、音符转录、演唱技巧识别和风格特征。

Result: 实验证明STARS在多个评估维度上优于现有方法，且使用其标注数据的SVS模型在自然性和风格控制上表现更佳。

Conclusion: STARS不仅解决了歌唱数据集的可扩展性问题，还为可控歌唱合成提供了新方法。

Abstract: Recent breakthroughs in singing voice synthesis (SVS) have heightened the
demand for high-quality annotated datasets, yet manual annotation remains
prohibitively labor-intensive and resource-intensive. Existing automatic
singing annotation (ASA) methods, however, primarily tackle isolated aspects of
the annotation pipeline. To address this fundamental challenge, we present
STARS, which is, to our knowledge, the first unified framework that
simultaneously addresses singing transcription, alignment, and refined style
annotation. Our framework delivers comprehensive multi-level annotations
encompassing: (1) precise phoneme-audio alignment, (2) robust note
transcription and temporal localization, (3) expressive vocal technique
identification, and (4) global stylistic characterization including emotion and
pace. The proposed architecture employs hierarchical acoustic feature
processing across frame, word, phoneme, note, and sentence levels. The novel
non-autoregressive local acoustic encoders enable structured hierarchical
representation learning. Experimental validation confirms the framework's
superior performance across multiple evaluation dimensions compared to existing
annotation approaches. Furthermore, applications in SVS training demonstrate
that models utilizing STARS-annotated data achieve significantly enhanced
perceptual naturalness and precise style control. This work not only overcomes
critical scalability challenges in the creation of singing datasets but also
pioneers new methodologies for controllable singing voice synthesis. Audio
samples are available at https://gwx314.github.io/stars-demo/.

</details>


### [195] [Exploring State-Space-Model based Language Model in Music Generation](https://arxiv.org/abs/2507.06674)
*Wei-Jaw Lee,Fang-Chih Hsieh,Xuanjun Chen,Fang-Duo Tsai,Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: 本文探索了基于Mamba的架构在文本到音乐生成中的潜力，发现单层码本可以捕捉音乐语义信息，并展示了SiMBA在有限资源下比Transformer更高效的性能。


<details>
  <summary>Details</summary>
Motivation: 探索Mamba架构在文本到音乐生成中的应用潜力，并验证其效率与表现。

Method: 采用离散的RVQ令牌作为建模表示，将SiMBA从编码器调整为解码器，并与Transformer解码器进行对比。

Result: SiMBA在有限资源下收敛更快，生成结果更接近真实数据。

Conclusion: SSMs在高效且富有表现力的文本到音乐生成中具有潜力。

Abstract: The recent surge in State Space Models (SSMs), particularly the emergence of
Mamba, has established them as strong alternatives or complementary modules to
Transformers across diverse domains. In this work, we aim to explore the
potential of Mamba-based architectures for text-to-music generation. We adopt
discrete tokens of Residual Vector Quantization (RVQ) as the modeling
representation and empirically find that a single-layer codebook can capture
semantic information in music. Motivated by this observation, we focus on
modeling a single-codebook representation and adapt SiMBA, originally designed
as a Mamba-based encoder, to function as a decoder for sequence modeling. We
compare its performance against a standard Transformer-based decoder. Our
results suggest that, under limited-resource settings, SiMBA achieves much
faster convergence and generates outputs closer to the ground truth. This
demonstrates the promise of SSMs for efficient and expressive text-to-music
generation. We put audio examples on Github.

</details>


### [196] [Constraint Optimized Multichannel Mixer-limiter Design](https://arxiv.org/abs/2507.06769)
*Yuancheng Luo,Dmitriy Yamkovoy,Guillermo Garcia*

Main category: cs.SD

TL;DR: 提出了一种耦合混音器-限幅器-包络设计，通过线性约束二次规划降低失真，并展示了高效实时处理的折衷方案。


<details>
  <summary>Details</summary>
Motivation: 传统多通道音频混音器和限幅器设计因高计算复杂度和运行时成本而解耦，本文旨在解决这一问题。

Method: 采用线性约束二次规划，优化多通道增益变量，并引入非对称常数重叠加窗优化、目标函数近似及变量和约束减少方法。

Result: 实验表明耦合设计减少了失真，同时需权衡计算效率以实现实时处理。

Conclusion: 耦合设计在多通道音频处理中有效降低失真，但需在计算效率上做出折衷。

Abstract: Multichannel audio mixer and limiter designs are conventionally decoupled for
content reproduction over loudspeaker arrays due to high computational
complexity and run-time costs. We propose a coupled mixer-limiter-envelope
design formulated as an efficient linear-constrained quadratic program that
minimizes a distortion objective over multichannel gain variables subject to
sample mixture constraints. Novel methods for asymmetric constant overlap-add
window optimization, objective function approximation, variable and constraint
reduction are presented. Experiments demonstrate distortion reduction of the
coupled design, and computational trade-offs required for efficient real-time
processing.

</details>


### [197] [Revealing the Hidden Temporal Structure of HubertSoft Embeddings based on the Russian Phonetic Corpus](https://arxiv.org/abs/2507.06794)
*Anastasia Ananeva,Anton Tomilov,Marina Volkova*

Main category: cs.SD

TL;DR: 研究探讨了HuBERTSoft模型在音素边界处的嵌入是否捕捉相邻音素的身份和顺序，发现其能有效编码音素过渡信息。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督学习模型在提取语音信息方面表现出色，但其是否保留时间结构（如音素边界处的嵌入是否反映相邻音素）尚不明确。

Method: 使用CORPRES俄语语音语料库，标记20毫秒嵌入窗口的音素三元组（开始、中间、结束），训练神经网络预测位置，并评估时间敏感性。

Result: 结果显示边界处的嵌入能捕捉音素身份和顺序，边界准确性尤其高，且模型编码了发音细节和协同发音效应。

Conclusion: 研究增进了对自监督学习语音表示内部结构的理解，表明其在音系分析和细粒度转录任务中的潜力。

Abstract: Self-supervised learning (SSL) models such as Wav2Vec 2.0 and HuBERT have
shown remarkable success in extracting phonetic information from raw audio
without labelled data. While prior work has demonstrated that SSL embeddings
encode phonetic features at the frame level, it remains unclear whether these
models preserve temporal structure, specifically, whether embeddings at phoneme
boundaries reflect the identity and order of adjacent phonemes. This study
investigates the extent to which boundary-sensitive embeddings from HubertSoft,
a soft-clustering variant of HuBERT, encode phoneme transitions. Using the
CORPRES Russian speech corpus, we labelled 20 ms embedding windows with
triplets of phonemes corresponding to their start, centre, and end segments. A
neural network was trained to predict these positions separately, and multiple
evaluation metrics, such as ordered, unordered accuracy and a flexible centre
accuracy, were used to assess temporal sensitivity. Results show that
embeddings extracted at phoneme boundaries capture both phoneme identity and
temporal order, with especially high accuracy at segment boundaries. Confusion
patterns further suggest that the model encodes articulatory detail and
coarticulatory effects. These findings contribute to our understanding of the
internal structure of SSL speech representations and their potential for
phonological analysis and fine-grained transcription tasks.

</details>


### [198] [Data-Balanced Curriculum Learning for Audio Question Answering](https://arxiv.org/abs/2507.06815)
*Gijs Wijngaard,Elia Formisano,Michele Esposito,Michel Dumontier*

Main category: cs.SD

TL;DR: 论文提出了一种结合课程学习和统计数据平衡的方法，用于解决音频问答模型的数据不平衡和训练不稳定性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前音频问答模型面临数据集不平衡和训练动态不稳定的问题，需要一种有效的方法来提升模型性能。

Method: 通过语言模型标注问题难度，采用课程学习从易到难训练；统计过滤过表示的音频类别，并使用引导解码约束输出格式。

Result: 在DCASE 2025训练集和五个公开数据集上，数据优化使准确率提升11.7%，达到64.2%。

Conclusion: 结合课程学习和数据平衡的方法有效提升了音频问答模型的性能，解决了数据不平衡和训练不稳定的问题。

Abstract: Audio question answering (AQA) requires models to understand acoustic content
and perform complex reasoning. Current models struggle with dataset imbalances
and unstable training dynamics. This work combines curriculum learning with
statistical data balancing to address these challenges. The method labels
question difficulty using language models, then trains progressively from easy
to hard examples. Statistical filtering removes overrepresented audio
categories, and guided decoding constrains outputs to valid multiple-choice
formats. Experiments on the DCASE 2025 training set and five additional public
datasets show that data curation improves accuracy by 11.7% over baseline
models, achieving 64.2% on the DCASE 2025 benchmark.

</details>


### [199] [Physics-Informed Direction-Aware Neural Acoustic Fields](https://arxiv.org/abs/2507.06826)
*Yoshiki Masuyama,François G. Germain,Gordon Wichern,Christopher Ick,Jonathan Le Roux*

Main category: cs.SD

TL;DR: 该论文提出了一种基于物理信息的神经网络（PINN）用于建模一阶Ambisonic（FOA）房间脉冲响应（RIRs），通过结合神经网络的强大建模能力和声波传播的物理原理，扩展了PINN框架以处理FOA RIRs。


<details>
  <summary>Details</summary>
Motivation: 在房间声学中，FOA RIRs不仅提供空间特性，还广泛应用于沉浸式音频生成。然而，传统的PINN方法主要针对全向麦克风测量的声压，因此需要扩展以支持FOA RIRs。

Method: 论文推导了两种基于粒子速度与FOA（X, Y, Z）通道对应关系的物理先验，通过偏导数关联预测的W通道与其他通道，并施加物理可行的关系。

Result: 实验表明，与未使用物理先验的神经网络相比，所提方法在建模FOA RIRs方面更有效。

Conclusion: 该研究成功扩展了PINN框架以建模FOA RIRs，并通过物理先验提升了性能，为沉浸式音频生成提供了新工具。

Abstract: This paper presents a physics-informed neural network (PINN) for modeling
first-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have
demonstrated promising performance in sound field interpolation by combining
the powerful modeling capability of neural networks and the physical principles
of sound propagation. In room acoustics, PINNs have typically been trained to
represent the sound pressure measured by omnidirectional microphones where the
wave equation or its frequency-domain counterpart, i.e., the Helmholtz
equation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial
characteristics and are useful for immersive audio generation with a wide range
of applications. In this paper, we extend the PINN framework to model FOA RIRs.
We derive two physics-informed priors for FOA RIRs based on the correspondence
between the particle velocity and the (X, Y, Z)-channels of FOA. These priors
associate the predicted W-channel and other channels through their partial
derivatives and impose the physically feasible relationship on the four
channels. Our experiments confirm the effectiveness of the proposed method
compared with a neural network without the physics-informed prior.

</details>


### [200] [Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation](https://arxiv.org/abs/2507.07043)
*Haris Khan,Shumaila Asif,Hassan Nasir*

Main category: cs.SD

TL;DR: AI在助听器中的应用从传统放大转向智能音频处理，综述了选择性噪声消除技术的进展、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何改进助听器的噪声消除功能，以提升用户体验和听力辅助效果。

Method: 系统综述了深度学习架构、硬件部署策略、临床验证及用户中心设计。

Result: 最新模型在噪声消除性能上显著提升（18.3 dB SI-SDR改进），并实现实时处理（<10 ms）。

Conclusion: 未来需关注轻量化模型、持续学习和临床转化，以解决实际部署中的挑战。

Abstract: The integration of artificial intelligence into hearing assistance marks a
paradigm shift from traditional amplification-based systems to intelligent,
context-aware audio processing. This systematic literature review evaluates
advances in AI-driven selective noise cancellation (SNC) for hearing aids,
highlighting technological evolution, implementation challenges, and future
research directions. We synthesize findings across deep learning architectures,
hardware deployment strategies, clinical validation studies, and user-centric
design. The review traces progress from early machine learning models to
state-of-the-art deep networks, including Convolutional Recurrent Networks for
real-time inference and Transformer-based architectures for high-accuracy
separation. Key findings include significant gains over traditional methods,
with recent models achieving up to 18.3 dB SI-SDR improvement on
noisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and
promising clinical outcomes. Yet, challenges remain in bridging lab-grade
models with real-world deployment - particularly around power constraints,
environmental variability, and personalization. Identified research gaps
include hardware-software co-design, standardized evaluation protocols, and
regulatory considerations for AI-enhanced hearing devices. Future work must
prioritize lightweight models, continual learning, contextual-based
classification and clinical translation to realize transformative hearing
solutions for millions globally.

</details>


### [201] [A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering](https://arxiv.org/abs/2507.07046)
*Shahana Yasmin Chowdhury,Bithi Banik,Md Tamjidul Hoque,Shreya Banerjee*

Main category: cs.SD

TL;DR: 提出DCRF-BiLSTM模型用于语音情感识别，在五个数据集上表现优异，综合准确率达93.76%。


<details>
  <summary>Details</summary>
Motivation: 提升语音情感识别在HCI和AI领域的应用效果，验证模型在多样化数据集上的泛化能力。

Method: 使用DCRF-BiLSTM模型识别七种情感，并在五个数据集（RAVDESS、TESS、SAVEE、EmoDB、Crema-D）上训练和测试。

Result: 在单个数据集上准确率高达97.83%至100%，综合数据集上达98.82%，首次在五个数据集上综合评估，整体准确率93.76%。

Conclusion: DCRF-BiLSTM模型具有鲁棒性和泛化能力，适用于多样化数据集。

Abstract: Nowadays, speech emotion recognition (SER) plays a vital role in the field of
human-computer interaction (HCI) and the evolution of artificial intelligence
(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:
neutral, happy, sad, angry, fear, disgust, and surprise, which are trained on
five datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).
The model achieves high accuracy on individual datasets, including 97.83% on
RAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS
and EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,
outperforming previously reported results. To our knowledge, no existing study
has evaluated a single SER model across all five benchmark datasets (i.e.,
R+T+S+C+E) simultaneously. In our work, we introduce this comprehensive
combination and achieve a remarkable overall accuracy of 93.76%. These results
confirm the robustness and generalizability of our DCRF-BiLSTM framework across
diverse datasets.

</details>


### [202] [Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification](https://arxiv.org/abs/2507.07058)
*Martin Sondermann,Pinar Bisgin,Niklas Tschorn,Anja Burmann,Christoph M. Friedrich*

Main category: cs.SD

TL;DR: 本文比较了四种模型（两种CNN和两种BEATs变换器）在心音图分类中的表现，发现CNN性能更优，但变换器在效率上有潜力。


<details>
  <summary>Details</summary>
Motivation: 探索不同模型和归一化方法对心音图自动分类的影响，为临床诊断提供指导。

Method: 使用PhysioNet2022数据集，比较两种CNN和两种BEATs变换器在固定长度和心跳周期归一化下的表现。

Result: CNN固定长度窗口AUROC为79.5%，心跳周期归一化为75.4%；BEATs变换器固定长度窗口为65.7%，心跳周期归一化为70.1%。

Conclusion: CNN性能更优，但变换器在开发效率上有优势，需平衡准确性和计算效率。

Abstract: The automated classification of phonocardiogram (PCG) recordings represents a
substantial advancement in cardiovascular diagnostics. This paper presents a
systematic comparison of four distinct models for heart murmur detection: two
specialized convolutional neural networks (CNNs) and two zero-shot universal
audio transformers (BEATs), evaluated using fixed-length and heart cycle
normalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart
cycle normalization method tailored to individual cardiac rhythms is
introduced. The findings indicate the following AUROC values: the CNN model
with fixed-length windowing achieves 79.5%, the CNN model with heart cycle
normalization scores 75.4%, the BEATs transformer with fixed-length windowing
achieves 65.7%, and the BEATs transformer with heart cycle normalization
results in 70.1%.
  The findings indicate that physiological signal constraints, especially those
introduced by different normalization strategies, have a substantial impact on
model performance. The research provides evidence-based guidelines for
architecture selection in clinical settings, emphasizing the need for a balance
between accuracy and computational efficiency. Although specialized CNNs
demonstrate superior performance overall, the zero-shot transformer models may
offer promising efficiency advantages during development, such as faster
training and evaluation cycles, despite their lower classification accuracy.
These findings highlight the potential of automated classification systems to
enhance cardiac diagnostics and improve patient care.

</details>


### [203] [Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach](https://arxiv.org/abs/2507.07066)
*Adrian S. Roman,Iran R. Roman,Juan P. Bello*

Main category: cs.SD

TL;DR: 论文提出了一种自监督的Latent Acoustic Mapping (LAM)模型，结合了传统方法的可解释性和深度学习的效率，用于声源定位。


<details>
  <summary>Details</summary>
Motivation: 传统波束形成方法计算量大且对声学变化敏感，而深度学习需要大量标注数据且缺乏可解释性。两者在多样声学环境和阵列配置中泛化能力有限。

Method: LAM是一种自监督框架，生成高分辨率声学图，适应不同声学条件和麦克风阵列。

Result: 在LOCATA和STARSS基准测试中，LAM的定位性能与现有监督方法相当或更优，其声学图还可提升监督模型的准确性。

Conclusion: LAM为自适应高性能声源定位系统提供了潜在解决方案。

Abstract: Acoustic mapping techniques have long been used in spatial audio processing
for direction of arrival estimation (DoAE). Traditional beamforming methods for
acoustic mapping, while interpretable, often rely on iterative solvers that can
be computationally intensive and sensitive to acoustic variability. On the
other hand, recent supervised deep learning approaches offer feedforward speed
and robustness but require large labeled datasets and lack interpretability.
Despite their strengths, both methods struggle to consistently generalize
across diverse acoustic setups and array configurations, limiting their broader
applicability. We introduce the Latent Acoustic Mapping (LAM) model, a
self-supervised framework that bridges the interpretability of traditional
methods with the adaptability and efficiency of deep learning methods. LAM
generates high-resolution acoustic maps, adapts to varying acoustic conditions,
and operates efficiently across different microphone arrays. We assess its
robustness on DoAE using the LOCATA and STARSS benchmarks. LAM achieves
comparable or superior localization performance to existing supervised methods.
Additionally, we show that LAM's acoustic maps can serve as effective features
for supervised models, further enhancing DoAE accuracy and underscoring its
potential to advance adaptive, high-performance sound localization systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [204] [3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds](https://arxiv.org/abs/2507.06484)
*Fan-Yun Sun,Shengguang Wu,Christian Jacobsen,Thomas Yim,Haoming Zou,Alex Zook,Shangru Li,Yu-Hsin Chou,Ethem Can,Xunlei Wu,Clemens Eppner,Valts Blukis,Jonathan Tremblay,Jiajun Wu,Stan Birchfield,Nick Haber*

Main category: cs.GR

TL;DR: 提出了一种可扩展的方法3D-Generalist，利用视觉语言模型（VLMs）生成高质量的3D环境，用于训练基础模型，并在下游任务中表现优于人工合成数据。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏基于3D世界的数据，提升模型的空间推理能力具有挑战性。人工创建3D环境耗时费力。

Method: 将3D环境构建转化为序列决策问题，使用VLMs作为策略生成布局、材质、光照等，并通过自改进微调优化生成质量。

Result: 3D-Generalist能生成仿真就绪的3D环境，其生成的数据预训练视觉基础模型后，在下游任务中表现优于人工合成数据。

Conclusion: 3D-Generalist提供了一种高效且可扩展的3D环境生成方法，为模型训练提供了高质量数据。

Abstract: Despite large-scale pretraining endowing models with language and vision
reasoning capabilities, improving their spatial reasoning capability remains
challenging due to the lack of data grounded in the 3D world. While it is
possible for humans to manually create immersive and interactive worlds through
3D graphics, as seen in applications such as VR, gaming, and robotics, this
process remains highly labor-intensive. In this paper, we propose a scalable
method for generating high-quality 3D environments that can serve as training
data for foundation models. We recast 3D environment building as a sequential
decision-making problem, employing Vision-Language-Models (VLMs) as policies
that output actions to jointly craft a 3D environment's layout, materials,
lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to
generate more prompt-aligned 3D environments via self-improvement fine-tuning.
We demonstrate the effectiveness of 3D-Generalist and the proposed training
strategy in generating simulation-ready 3D environments. Furthermore, we
demonstrate its quality and scalability in synthetic data generation by
pretraining a vision foundation model on the generated data. After fine-tuning
the pre-trained model on downstream tasks, we show that it surpasses models
pre-trained on meticulously human-crafted synthetic data and approaches results
achieved with real data orders of magnitude larger.

</details>


### [205] [Assessing Learned Models for Phase-only Hologram Compression](https://arxiv.org/abs/2507.06646)
*Zicong Peng,Yicheng Zhan,Josef Spjut,Kaan Akşit*

Main category: cs.GR

TL;DR: 评估了四种基于INR和VAE结构的模型在压缩相位全息图时的性能，发现SIREN表现最佳，而预训练的VAE模型TAESD效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究不同学习模型在压缩相位全息图任务中的表现，以优化全息显示的效果。

Method: 使用四种模型（MLP、SIREN、FilmSIREN和TAESD）进行实验，比较其压缩能力和重建质量。

Result: SIREN以4.9k参数实现40%压缩且重建质量高（PSNR=34.54dB），TAESD表现不佳。

Conclusion: INR模型在相位全息图压缩中更有效，预训练VAE模型需任务特定优化。

Abstract: We evaluate the performance of four common learned models utilizing INR and
VAE structures for compressing phase-only holograms in holographic displays.
The evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as
the representative VAE model. Our experiments reveal that a pretrained image
VAE, TAESD, with 2.2M parameters struggles with phase-only hologram
compression, revealing the need for task-specific adaptations. Among the INRs,
SIREN with 4.9k parameters achieves %40 compression with high quality in the
reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the
effectiveness of INRs and identify the limitations of pretrained image
compression VAEs for hologram compression task.

</details>


### [206] [Better frame rates or better visuals? An early report of Esports player practice in Dota 2](https://arxiv.org/abs/2507.06790)
*Arjun Madhusudan,Benjamin Watson*

Main category: cs.GR

TL;DR: 研究探讨了Dota 2玩家如何通过降低视觉质量以提高游戏性能，发现玩家普遍关闭VSYNC以减少延迟，但可能导致画面撕裂。


<details>
  <summary>Details</summary>
Motivation: 了解玩家在视觉质量和性能之间的实际权衡行为，填补研究空白。

Method: 通过调查收集Dota 2玩家的游戏配置数据，并询问其意图。

Result: 玩家确实会降低视觉细节，尤其是关闭VSYNC，且其意图与配置行为一致。

Conclusion: 研究为未来更严谨的调查奠定了基础，有助于新玩家适应游戏，并为开发者优化低视觉配置提供依据。

Abstract: Esports athletes often reduce visual quality to improve latency and frame
rate, and increase their in-game performance. Little research has examined the
effects of this visuo-spatial tradeoff on performance, but we could find no
work studying how players manage this tradeoff in practice. This paper is an
initial examination of this question in the game Dota 2. First, we gather the
game configuration data of Dota 2 players in a small survey. We learn that
players do limit visual detail, particularly by turning off VSYNC, which
removes rendering/display synchronization delay but permits visual "tearing".
Second, we survey the intent of those same players with a few subjective
questions. Player intent matches configuration practice. While our sampling of
Dota 2 players may not be representative, our survey does reveal suggestive
trends that lay the groundwork for future, more rigorous and larger surveys.
Such surveys can help new players adapt to the game more quickly, encourage
researchers to investigate the relative importance of temporal and visual
detail, and justify design effort by developers in "low visual" game
configurations.

</details>


### [207] [Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting](https://arxiv.org/abs/2507.07000)
*Wijayathunga W. M. R. D. B*

Main category: cs.GR

TL;DR: 提出了一种将网格表示与3D高斯泼溅结合的新框架，以增强非刚性3D模型变形能力。


<details>
  <summary>Details</summary>
Motivation: 传统高斯泼溅虽能实现快速实时辐射场渲染，但其后编辑功能和对大规模非刚性变形的支持有限。

Method: 通过将高斯核直接嵌入显式网格表面，利用网格的拓扑和几何先验指导直观编辑操作（如移动、缩放、旋转）和复杂变形（如弯曲、拉伸）。

Result: 实现了更灵活的3D内容创作流程。

Conclusion: 该框架为虚拟现实、角色动画和交互设计等应用提供了更强大的工具。

Abstract: We propose a novel framework that enhances non-rigid 3D model deformations by
bridging mesh representations with 3D Gaussian splatting. While traditional
Gaussian splatting delivers fast, real-time radiance-field rendering, its
post-editing capabilities and support for large-scale, non-rigid deformations
remain limited. Our method addresses these challenges by embedding Gaussian
kernels directly onto explicit mesh surfaces. This allows the mesh's inherent
topological and geometric priors to guide intuitive editing operations -- such
as moving, scaling, and rotating individual 3D components -- and enables
complex deformations like bending and stretching. This work paves the way for
more flexible 3D content-creation workflows in applications spanning virtual
reality, character animation, and interactive design.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [208] [Revisiting Chien-Hrones-Reswick Method for an Analytical Solution](https://arxiv.org/abs/2507.06352)
*Senol Gulgonul*

Main category: eess.SY

TL;DR: 提出了一种基于Lambert W函数的PI控制器调谐方法，用于一阶时滞系统，实现精确极点配置和解析增益计算。


<details>
  <summary>Details</summary>
Motivation: 解决传统调谐方法在理论分析与实际应用之间的差距，提供更精确的调谐规则。

Method: 利用Lambert W函数进行极点配置，推导出PI增益的解析表达式，并确定临界条件以实现无超调或可控超调的响应。

Result: 方法在无超调和可控超调情况下与Chien-Hrones-Reswick经验规则高度一致。

Conclusion: 该方法为FOTD系统的PI控制器调谐提供了理论支持，并验证了其与经验规则的一致性。

Abstract: This study presents an analytical method for tuning PI controllers in
First-Order with Time Delay (FOTD) systems, leveraging the Lambert W function.
The Lambert W function enables exact pole placement, yielding analytical
expressions for PI gains. The proposed approach identifies a critical condition
that achieves a step response without overshoot with minimum settling time,
while also providing explicit tuning rules for systems where controlled
overshoot is specified. The method demonstrates strong agreement with
established empirical Chien-Hrones-Reswick tuning rules for both
non-overshooting and overshooting cases, bridging the gap between theoretical
analysis and empirical results.

</details>


### [209] [Forex Trading Robot Using Fuzzy Logic](https://arxiv.org/abs/2507.06383)
*Mustafa Shabani,Alireza Nasiri,Hassan Nafardi*

Main category: eess.SY

TL;DR: 提出了一种基于模糊逻辑的短期外汇交易系统，通过模糊Mamdani系统改进传统技术指标，提高了交易准确性。


<details>
  <summary>Details</summary>
Motivation: 传统技术指标（如RSI、CCI和Stochastic）因市场变化而表现不佳，需改进以提升交易效果。

Method: 为每个指标引入模糊Mamdani系统，并通过投票机制组合结果设计交易机器人。

Result: 相比其他三种方法，盈利能力显著提升，并计算了净利、总利和最大资本缩减。

Conclusion: 模糊系统显著提高了外汇短期交易的盈利能力。

Abstract: In this study, we propose a fuzzy system for conducting short-term
transactions in the forex market. The system is designed to enhance common
strategies in the forex market using fuzzy logic, thereby improving the
accuracy of transactions. Traditionally, technical strategies based on
oscillator indicators have relied on predefined ranges for indicators such as
Relative Strength Index (RSI), Commodity Channel Indicator (CCI), and
Stochastic to determine entry points for trades. However, the use of these
classic indicators has yielded suboptimal results due to the changing nature of
the market over time. In our proposed approach, instead of employing classical
indicators, we introduce a fuzzy Mamdani system for each indicator. The results
obtained from these systems are then combined through voting to design a
trading robot. Our findings demonstrate a considerable increase in the
profitability factor compared to three other methods. Additionally, net profit,
gross profit, and maximum capital reduction are calculated and compared across
all approaches.

</details>


### [210] [How Complex is a Complex Network? Insights from Linear Systems Theory](https://arxiv.org/abs/2507.06389)
*Giacomo Baggio,Marco Fabris*

Main category: eess.SY

TL;DR: 本文利用线性系统理论提出了一种网络系统复杂性的度量方法，基于McMillan度概念，分析了网络拓扑和节点动态异质性对复杂性的影响。


<details>
  <summary>Details</summary>
Motivation: 研究网络系统的复杂性度量，以理解网络结构和节点动态异质性对系统整体复杂性的影响。

Method: 通过局部过滤节点动态效应，提出基于McMillan度的复杂性指标，分析网络拓扑和节点异质性对指标的影响。

Result: 发现复杂性指标几乎对所有连接权重相同，且依赖于网络子图的匹配数，表明网络结构和组件多样性共同影响复杂性。

Conclusion: 提出的复杂性指标能有效量化网络系统的复杂性，揭示了网络结构和节点动态异质性的联合作用。

Abstract: This paper leverages linear systems theory to propose a principled measure of
complexity for network systems. We focus on a network of first-order scalar
linear systems interconnected through a directed graph. By locally filtering
out the effect of nodal dynamics in the interconnected system, we propose a new
quantitative index of network complexity rooted in the notion of McMillan
degree of a linear system. First, we show that network systems with the same
interconnection structure share the same complexity index for almost all
choices of their interconnection weights. Then, we investigate the dependence
of the proposed index on the topology of the network and the pattern of
heterogeneity of the nodal dynamics. Specifically, we find that the index
depends on the matching number of subgraphs identified by nodal dynamics of
different nature, highlighting the joint impact of network architecture and
component diversity on overall system complexity.

</details>


### [211] [VoI-aware Scheduling Schemes for Multi-Agent Formation Control](https://arxiv.org/abs/2507.06392)
*Federico Chiariotti,Marco Fabris*

Main category: eess.SY

TL;DR: 论文提出了一种基于6G网络的目标导向框架，结合控制、协作定位和通信调度，用于一阶编队跟踪，通过轻量级调度策略提升编队质量。


<details>
  <summary>Details</summary>
Motivation: 现有编队控制方法多假设理想通信条件，而实际通信受限，因此需要一种结合通信调度的框架。

Method: 使用6G网络三角定位估计位置，基于AoI和VoI设计三种无信号调度策略。

Result: 仿真显示该方法能保持精确编队且无额外通信开销，最坏情况下编队精度提升20%。

Conclusion: 提出的框架在通信受限条件下有效提升编队控制性能。

Abstract: Formation control allows agents to maintain geometric patterns using local
information, but most existing methods assume ideal communication. This paper
introduces a goal-oriented framework combining control, cooperative
positioning, and communication scheduling for first-order formation tracking.
Each agent estimates its position using 6G network-based triangulation, and the
scheduling of information updates is governed by Age of Information (AoI) and
Value of Information (VoI) metrics. We design three lightweight, signaling-free
scheduling policies and assess their impact on formation quality. Simulation
results demonstrate the effectiveness of the proposed approach in maintaining
accurate formations with no additional communication overhead, showing that
worst-case formation adherence increases by 20%.

</details>


### [212] [An AI-Driven Thermal-Fluid Testbed for Advanced Small Modular Reactors: Integration of Digital Twin and Large Language Models](https://arxiv.org/abs/2507.06399)
*Doyeong Lim,Yang Liu,Zavier Ndum Ndum,Christian Young,Yassin Hassan*

Main category: eess.SY

TL;DR: 该论文介绍了一个多用途AI驱动的热流体测试平台，用于推进小型模块化反应堆技术，结合物理实验与计算智能。


<details>
  <summary>Details</summary>
Motivation: 通过整合AI与热流体科学，加速下一代核系统的创新与部署。

Method: 测试平台结合了数字孪生和GRU神经网络，实现实时预测与控制，并通过大型语言模型提供操作建议。

Result: GRU模型的温度预测均方根误差为1.42 K，验证了平台的高保真度。

Conclusion: 该工作展示了AI在建模、控制和操作支持中的潜力，为核系统创新提供了集成研究环境。

Abstract: This paper presents a multipurpose artificial intelligence (AI)-driven
thermal-fluid testbed designed to advance Small Modular Reactor technologies by
seamlessly integrating physical experimentation with advanced computational
intelligence. The platform uniquely combines a versatile three-loop
thermal-fluid facility with a high-fidelity digital twin and sophisticated AI
frameworks for real-time prediction, control, and operational assistance.
Methodologically, the testbed's digital twin, built upon the System Analysis
Module code, is coupled with a Gated Recurrent Unit (GRU) neural network. This
machine learning model, trained on experimental data, enables
faster-than-real-time simulation, providing predictive insights into the
system's dynamic behavior. The practical application of this AI integration is
showcased through case studies. An AI-driven control framework where the GRU
model accurately forecasts future system states and the corresponding control
actions required to meet operational demands. Furthermore, an intelligent
assistant, powered by a large language model, translates complex sensor data
and simulation outputs into natural language, offering operators actionable
analysis and safety recommendations. Comprehensive validation against
experimental transients confirms the platform's high fidelity, with the GRU
model achieving a temperature prediction root mean square error of 1.42 K. This
work establishes an integrated research environment at the intersection of AI
and thermal-fluid science, showcasing how AI-driven methodologies in modeling,
control, and operator support can accelerate the innovation and deployment of
next-generation nuclear systems.

</details>


### [213] [Voltage Regulation in Distribution Systems with Data Center Loads](https://arxiv.org/abs/2507.06416)
*Yize Chen,Baosen Zhang*

Main category: eess.SY

TL;DR: 论文提出了一种动态电压控制方案，利用数据中心的负载调节能力解决因高功率需求引起的电压问题。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型和AI计算的快速发展，大规模数据中心的功率和能源轨迹引发了担忧，尤其是电压问题。

Method: 通过动态电压和频率调节（DVFS）方案，利用本地电压测量调整数据中心的功率注入，以分布式方式维持安全电压。

Result: 使用真实大语言模型（LLM）推理负载的模拟验证了该方案的有效性。

Conclusion: 该动态电压控制方案能有效应对数据中心集成挑战，相关数据和方案已开源。

Abstract: Recent boom in foundation models and AI computing have raised growing
concerns on the power and energy trajectories of large-scale data centers. This
paper focuses on the voltage issues caused by volatile and intensity of data
center power demand, which also aligns with recent observations of more
frequent voltage disturbances in power grids. To address these data center
integration challenges, we propose a dynamic voltage control scheme by
harnessing data center's load regulation capabilities. By taking local voltage
measurements and adjusting power injections at each data center buses through
the dynamic voltage and frequency scaling (DVFS) scheme, we are able to
maintain safe voltage magnitude in a distributed fashion with higher data
center computing load. Simulations using real large language model (LLM)
inference load validate the effectiveness of our proposed mechanism. Both the
LLM power data and proposed control scheme are open sourced.

</details>


### [214] [Experience-Centric Resource Management in ISAC Networks: A Digital Agent-Assisted Approach](https://arxiv.org/abs/2507.06436)
*Xinyu Huang,Yixiao Zhang,Yingying Pei,Jianzhe Xue,Xuemin Shen*

Main category: eess.SY

TL;DR: 提出了一种数字代理辅助的资源管理方案，用于提升ISAC网络中用户的体验质量（QoE）。通过用户状态预测、QoS因素选择和QoE拟合模型，结合CRB模型和双层优化算法，显著提升了用户QoE。


<details>
  <summary>Details</summary>
Motivation: 提升ISAC网络中用户的综合体验质量（QoE），结合服务质量（QoS）、用户行为动态和环境复杂性。

Method: 提出数字代理模块，包括用户状态预测、QoS因素选择和QoE拟合模型；利用CRB模型量化资源分配对感知精度的影响；开发双层数据模型驱动算法（深度强化学习和凸优化）。

Result: 仿真结果表明，该方案在用户QoE方面优于基准方案。

Conclusion: 数字代理辅助的资源管理方案能有效提升ISAC网络的用户QoE，具有实际应用潜力。

Abstract: In this paper, we propose a digital agent (DA)-assisted resource management
scheme for enhanced user quality of experience (QoE) in integrated sensing and
communication (ISAC) networks. Particularly, user QoE is a comprehensive metric
that integrates quality of service (QoS), user behavioral dynamics, and
environmental complexity. The novel DA module includes a user status prediction
model, a QoS factor selection model, and a QoE fitting model, which analyzes
historical user status data to construct and update user-specific QoE models.
Users are clustered into different groups based on their QoE models. A
Cram\'er-Rao bound (CRB) model is utilized to quantify the impact of allocated
communication resources on sensing accuracy. A joint optimization problem of
communication and computing resource management is formulated to maximize
long-term user QoE while satisfying CRB and resource constraints. A two-layer
data-model-driven algorithm is developed to solve the formulated problem, where
the top layer utilizes an advanced deep reinforcement learning algorithm to
make group-level decisions, and the bottom layer uses convex optimization
techniques to make user-level decisions. Simulation results based on a
real-world dataset demonstrate that the proposed DA-assisted resource
management scheme outperforms benchmark schemes in terms of user QoE.

</details>


### [215] [VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic](https://arxiv.org/abs/2507.06441)
*Shanting Wang,Panagiotis Typaldos,Chenjun Li,Andreas A. Malikopoulos*

Main category: eess.SY

TL;DR: VisioPath结合视觉语言模型和模型预测控制，实现动态交通环境中的安全自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 解决动态交通环境中安全自动驾驶的挑战，结合AI感知与最优控制。

Method: 利用鸟瞰视频处理和零样本VLM获取车辆信息，构建椭圆避碰势场，嵌入事件触发的MPC循环。

Result: 在SUMO模拟中表现优于传统MPC方法。

Conclusion: VisioPath为复杂交通系统的安全轨迹规划提供了重要进展。

Abstract: In this paper, we introduce VisioPath, a novel framework combining
vision-language models (VLMs) with model predictive control (MPC) to enable
safe autonomous driving in dynamic traffic environments. The proposed approach
leverages a bird's-eye view video processing pipeline and zero-shot VLM
capabilities to obtain structured information about surrounding vehicles,
including their positions, dimensions, and velocities. Using this rich
perception output, we construct elliptical collision-avoidance potential fields
around other traffic participants, which are seamlessly integrated into a
finite-horizon optimal control problem for trajectory planning. The resulting
trajectory optimization is solved via differential dynamic programming with an
adaptive regularization scheme and is embedded in an event-triggered MPC loop.
To ensure collision-free motion, a safety verification layer is incorporated in
the framework that provides an assessment of potential unsafe trajectories.
Extensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that
VisioPath outperforms conventional MPC baselines across multiple metrics. By
combining modern AI-driven perception with the rigorous foundation of optimal
control, VisioPath represents a significant step forward in safe trajectory
planning for complex traffic systems.

</details>


### [216] [On Regular Regressors in Adaptive Control](https://arxiv.org/abs/2507.06446)
*Erick Mejia Uzeda,Mireille E. Broucke*

Main category: eess.SY

TL;DR: 论文提出了一种解决自适应控制中回归器激励问题的正则性概念，并展示了其在新自适应控制问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决自适应控制中回归器激励（PE）行为不稳定的问题，避免激励从无到有的不合理现象。

Method: 引入正则回归器的概念，证明其激励始终局限于子空间的PE分解定理，并给出几何表征。

Result: 提出了新的自适应控制问题，展示了正则回归器在控制中的潜力。

Conclusion: 正则回归器的概念为自适应控制提供了新的理论基础和应用方向。

Abstract: This paper addresses a shortcoming in adaptive control, that the property of
a regressor being persistently exciting (PE) is not well-behaved. One can
construct regressors that upend the commonsense notion that excitation should
not be created out of nothing. To amend the situation, a notion of regularity
of regressors is needed. We are naturally led to a broad class of regular
regressors that enjoy the property that their excitation is always confined to
a subspace, a foundational result called the PE decomposition. A geometric
characterization of regressor excitation opens up new avenues for adaptive
control, as we demonstrate by formulating a number of new adaptive control
problems.

</details>


### [217] [Dual State-space Fidelity Blade (D-STAB): A Novel Stealthy Cyber-physical Attack Paradigm](https://arxiv.org/abs/2507.06492)
*Jiajun Shen,Hao Tu,Fengjun Li,Morteza Hashemi,Di Wu,Huazhen Fang*

Main category: eess.SY

TL;DR: 提出了一种新型网络物理攻击范式D-STAB，利用高保真与低保真物理模型之间的信息不对称性，攻击核心网络物理组件的固件。


<details>
  <summary>Details</summary>
Motivation: 针对网络物理系统中高保真与低保真模型间的信息不对称性，探索一种新型攻击表面。

Method: 通过设计基于高保真状态空间信息的精确对抗约束，诱导高保真状态偏离，而低保真观测无法检测。

Result: 在电池管理系统（BMS）的最优充电任务中验证了D-STAB的有效性。

Conclusion: D-STAB展示了利用模型保真度差异的新型攻击方式，对网络物理系统安全提出新挑战。

Abstract: This paper presents a novel cyber-physical attack paradigm, termed the Dual
State-Space Fidelity Blade (D-STAB), which targets the firmware of core
cyber-physical components as a new class of attack surfaces. The D-STAB attack
exploits the information asymmetry caused by the fidelity gap between
high-fidelity and low-fidelity physical models in cyber-physical systems. By
designing precise adversarial constraints based on high-fidelity state-space
information, the attack induces deviations in high-fidelity states that remain
undetected by defenders relying on low-fidelity observations. The effectiveness
of D-STAB is demonstrated through a case study in cyber-physical battery
systems, specifically in an optimal charging task governed by a Battery
Management System (BMS).

</details>


### [218] [Effects of Net Metering Policies on Distributed Energy Resource Valuation and Operation](https://arxiv.org/abs/2507.06595)
*Lane D. Smith,Daniel S. Kirschen*

Main category: eess.SY

TL;DR: 本文探讨了不同净计量政策对商业消费者的影响，发现最新政策更有利于太阳能与储能结合的消费者，而非仅依赖太阳能的消费者。


<details>
  <summary>Details</summary>
Motivation: 研究净计量政策变化对商业消费者及分布式能源资源的影响，以应对太阳能技术成熟和政策调整带来的挑战。

Method: 分析不同净计量政策对商业消费者的经济影响，特别关注太阳能与储能结合的效果。

Result: 最新净计量政策对仅依赖太阳能的消费者益处较少，而对太阳能与储能结合的消费者更有利。灵活需求的出口价格价值有限。

Conclusion: 政策调整需更平衡地支持太阳能和储能结合，同时提升灵活需求的经济价值。

Abstract: Net energy metering has been a successful policy for increasing solar
generation installations and reducing the costs of photovoltaic arrays for
consumers. However, increased maturity of solar technologies and concerns over
cost shifts created by net energy metering have recently caused the policy to
change its incentives. What once favored behind-the-meter solar generation now
is focused on compensating flexible operation. This paper explores the impacts
that different net energy metering policies have on commercial consumers with
various distributed energy resources. We show that the newest iteration of net
energy metering is less beneficial for consumers with only solar generation and
instead favors those that pair energy storage with solar. Though shiftable
flexible demand offers consumers the ability to operate flexibly, the export
prices offered by the latest net energy metering policy provide limited value
to flexible demand.

</details>


### [219] [The Small Phase Condition is Necessary for Symmetric Systems](https://arxiv.org/abs/2507.06617)
*Xiaokan Yang,Wei Chen,Li Qiu*

Main category: eess.SY

TL;DR: 论文证明了小相位条件对于对称互联系统的反馈稳定性是充分且必要的，填补了相位条件在对称系统中的必要性空白。


<details>
  <summary>Details</summary>
Motivation: 对称系统在多种应用中广泛存在，但相位条件在反馈稳定性中的必要性尚未明确。

Method: 通过分析复对称半扇形矩阵的广义扇形分解，证明其变换矩阵可为实数。

Result: 小相位条件对对称系统是充分且必要的，并探讨了非对称系统的必要性。

Conclusion: 研究填补了相位条件在对称系统中的必要性空白，并为非对称系统提供了新见解。

Abstract: In this paper, we show that the small phase condition is both sufficient and
necessary to ensure the feedback stability when the interconnected systems are
symmetric. Such symmetric systems arise in diverse applications. The key lies
in that, for a complex symmetric and semi-sectorial matrix, the transformation
matrix in its generalized sectorial decomposition can be taken to be real. Such
a result fills the gap of phase based necessary condition for the feedback
stability of symmetric systems, and serves as a counterpart of the necessity
result for small gain condition. Moreover, we explore the necessity of small
phase condition for general asymmetric systems. Some insightful results are
presented, which help to clarify the main challenge in the general case.

</details>


### [220] [Coordinated Fast Frequency Regulation in Dynamic Virtual Power Plants via Disturbance Estimation](https://arxiv.org/abs/2507.06713)
*Saif Ahmad,Seifeddine Ben Elghali,Hafiz Ahmed*

Main category: eess.SY

TL;DR: 论文提出了一种结合局部控制与分布式补偿的分层架构，用于解决动态虚拟电厂中频率调节的容量限制问题。


<details>
  <summary>Details</summary>
Motivation: 动态虚拟电厂（DVPPs）在频率调节方面面临存储容量有限的挑战，需要一种高效且可扩展的解决方案。

Method: 采用基于扰动估计的分散局部控制与分布式不平衡补偿相结合的分层架构。

Result: 在含高比例可再生能源的4总线系统中验证了该方法的有效性。

Conclusion: 该方法为未来可再生能源为主的电网提供了可扩展且高效的频率调节方案。

Abstract: In the context of dynamic virtual power plants (DVPPs), the integration of
frequency containment reserve (FCR) and fast frequency control (FFC) enabled
via local compensation of power imbalance represents a significant advancement
in decentralized frequency regulation. However, they still have to cope with
the limited power and energy capacities associated with commonly available
storage solutions. This work combines a disturbance estimation based
decentralized local control with distributed imbalance compensation in the
event of local shortfall. The layered architecture facilitates fast local
corrections in power setpoints while enabling coordination between neighbouring
DVPP nodes to leverage the aggregated capacity, ensuring scalable and efficient
operation suitable for renewable-heavy future grids. The proposed approach is
validated on an illustrative 4-bus system with a high percentage of renewables.

</details>


### [221] [Techno-economic analysis of decarbonized backup power systems using scenario-based stochastic optimization](https://arxiv.org/abs/2507.06736)
*Jonas Schweiger,Ruaridh Macdonald*

Main category: eess.SY

TL;DR: 该研究评估了多种清洁备用电源系统（BPSs）以替代传统柴油发电机，提出了基于场景的随机优化框架，比较了27种技术的成本效益和环境效益。研究发现，氨发电机和氢燃料电池结合铁空气电池是经济有效的脱碳解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着对电力中断、电网可靠性和脱碳需求的关注增加，研究旨在评估清洁备用电源系统以替代传统柴油发电机。

Method: 采用基于场景的随机优化框架，结合实际负载曲线和停电概率，评估27种技术的成本效益和环境效益。

Result: 研究发现，氨发电机和氢燃料电池结合铁空气电池是经济有效的脱碳解决方案，同时揭示了燃料替代对排放和成本的重要性。

Conclusion: 研究结果为优化清洁备用电源系统设计提供了有价值的参考，帮助经济高效地满足多种设施类型和脱碳目标。

Abstract: In the context of growing concerns about power disruptions, grid reliability
and the need for decarbonization, this study evaluates a broad range of clean
backup power systems (BPSs) to replace traditional emergency diesel generators.
A scenario-based stochastic optimization framework using actual load profiles
and outage probabilities is proposed to assess the most promising options from
a pool of 27 technologies. This framework allows a comparison of
cost-effectiveness and environmental impact of individual technologies and
hybrid BPSs across various scenarios. The results highlight the trade-off
between total annual system cost and emissions. Significant emission reductions
can be achieved at moderate cost increases but deep decarbonization levels
incur higher costs. Primary and secondary batteries are included in optimal
clean fuel-based systems across all decarbonization levels, combining
cost-effective power delivery and long-term storage benefits. The findings
highlight the often-overlooked importance of fuel replacement on both emissions
and costs. Among the assessed technologies, ammonia generators and hydrogen
fuel cells combined with secondary iron-air batteries emerge as cost-effective
solutions for achieving decarbonization goals. To ensure a broad range of
applicability, the study outlines the impact of emergency fuel purchases,
varying demand patterns and demand response options on the optimal BPS. The
research findings are valuable for optimizing the design of clean BPSs to
economically meet the needs of many facility types and decarbonization targets.

</details>


### [222] [Optimisation of Electrolyser Operation: Integrating External Heat](https://arxiv.org/abs/2507.06796)
*Matthias Derez,Alexander Hoogsteyn,Erik Delarue*

Main category: eess.SY

TL;DR: 论文提出了一种整合外部热源到电解槽的方法，以减少生产碳中性氢气所需的电力需求。


<details>
  <summary>Details</summary>
Motivation: 通过整合外部热源，降低氢气生产的电力需求，提高效率和盈利能力。

Method: 基于分段线性近似电化学方程，内建启动成本和直接热整合的详细模型。

Result: 分析了低温和高温热整合对固体氧化物和质子交换膜电解技术效率和盈利能力的影响。

Conclusion: 整合外部热源可显著提高氢气生产的效率和经济效益。

Abstract: Integrating external heat into electrolysers can reduce the electrical power
demand for carbon-neutral hydrogen production. Efficient operation requires
detailed models that incorporate heat availability and its effect on startup
costs. This paper advances existing operational models by endogenously
modelling startup costs and direct heat integration, based on a piecewise
linear approximation of the electrochemical equations. We analyse the impact of
low- and high-temperature heat integration on the efficiency and profitability
of hydrogen production for solid oxide and proton exchange membrane
electrolysis technologies.

</details>


### [223] [A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis](https://arxiv.org/abs/2507.06890)
*Yifan Wang*

Main category: eess.SY

TL;DR: 提出了一种基于单传感器的分数阶记忆增强攻击诊断方案（FO-MADS），用于智能微电网的低延迟故障定位和网络攻击检测。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法依赖昂贵的多点仪器或严格的建模假设，难以在单传感器约束下实现。

Method: 通过联合应用Caputo和Grünwald-Letnikov导数构建双分数阶特征库，并采用两阶段分层分类器和渐进记忆重放对抗训练（PMR-AT）。

Result: 在四种攻击场景下，诊断准确率分别为96.6%（偏置）、94.0%（噪声）、92.8%（数据替换）和95.7%（重放），无攻击条件下为96.7%。

Conclusion: FO-MADS是一种成本低、易部署的方案，显著提升了智能微电网的网络安全韧性。

Abstract: Cyber-attacks jeopardize the safe operation of smart microgrids. At the same
time, existing diagnostic methods either depend on expensive multi-point
instrumentation or stringent modelling assumptions that are untenable under
single-sensor constraints. This paper proposes a Fractional-Order
Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency
fault localisation and cyber-attack detection using only one VPQ
(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual
fractional-order feature library by jointly applying Caputo and
Gr\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and
slow drifts in the VPQ signal. A two-stage hierarchical classifier then
pinpoints the affected inverter and isolates the faulty IGBT switch,
effectively alleviating class imbalance. Robustness is further strengthened
through Progressive Memory-Replay Adversarial Training (PMR-AT), whose
attack-aware loss is dynamically re-weighted via Online Hard Example Mining
(OHEM) to prioritise the most challenging samples. Experiments on a
four-inverter microgrid testbed comprising 1 normal and 24 fault classes under
four attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0
% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining
96.7 % under attack-free conditions. These results establish FO-MADS as a
cost-effective and readily deployable solution that markedly enhances the
cyber-physical resilience of smart microgrids.

</details>


### [224] [A nonlinear dead-time compensation method for path tracking control](https://arxiv.org/abs/2507.06935)
*Karin Festl,Michael Stolz*

Main category: eess.SY

TL;DR: 论文提出了一种补偿自动驾驶车辆路径跟踪控制中死区时间的方法，通过非线性预测模型提高控制性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆控制系统中存在死区时间（如传感器处理和机械响应延迟），影响实时性和安全性，需有效补偿。

Method: 采用类似Smith预测器的非线性预测模型，结合车辆运动学特性，避免数值积分或优化，实现快速执行。

Result: 仿真测试表明，该方法在各种控制器和干扰（包括死区时间不确定性）下均能提升控制性能。

Conclusion: 提出的死区时间补偿方法有效，适用于实际自动驾驶系统。

Abstract: In the realm of autonomous vehicle technologies and advanced driver
assistance systems, precise and reliable path tracking controllers are vital
for safe and efficient navigation. However the presence of dead time in the
vehicle control systems poses a challenge to real-world systems. Input and
output delays are caused by factors like sensor processing and mechanical
response and can range up to a few hundred milliseconds. This chapter addresses
the problem of dead time in path tracking control and proposes a method to
compensate the dead time. The proposed solution involves a nonlinear prediction
model, in a structure similar to the Smith predictor, but incorporating the
kinematic behavior of the vehicle plant system. The implementation avoids
numeric integration or optimization, enabling a fast execution. Simulation
tests with various controllers and disturbances, including dead-time
uncertainty, demonstrate the efficacy of the dead-time compensation method.
Results indicate improved control performance in all tested scenarios.

</details>


### [225] [Device-to-Device Communication in 5G/6G: Architectural Foundations and Convergence with Enabling Technologies](https://arxiv.org/abs/2507.06946)
*Mohammad Reza Fasihi,Brian L. Mark*

Main category: eess.SY

TL;DR: 本文综述了设备间（D2D）通信在5G/6G网络中的潜力，探讨了其与先进蜂窝技术的集成、标准化进展、核心挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 满足5G/6G网络对高效、低延迟和邻近服务的需求，D2D通信提供了直接设备间通信的解决方案。

Method: 通过综述D2D通信的架构基础及其与5G/6G关键技术的集成，分析标准化进展和核心挑战。

Result: D2D通信能提升频谱效率和能源效率，降低延迟，支持邻近服务，但集成过程中存在挑战。

Conclusion: D2D通信在下一代无线网络中具有巨大潜力，需进一步研究以解决集成挑战并实现其全部价值。

Abstract: Device-to-Device (D2D) communication is a promising solution to meet the
growing demands of 5G and future 6G networks by enabling direct communication
between user devices. It enhances spectral efficiency (SE) and energy
efficiency (EE), reduces latency, and supports proximity-based services. As
wireless systems evolve toward 5G and 6G paradigms, the integration of D2D with
advanced cellular technologies introduces new opportunities and challenges.
This survey paper reviews the architectural foundations of D2D communication
and explores its integration with key 5G/6G enabling technologies. We review
standardization efforts, analyze core challenges, and highlight future research
directions to unlock the full potential of D2D in next-generation wireless
networks.

</details>
