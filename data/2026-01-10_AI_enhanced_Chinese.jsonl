{"id": "2601.03323", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.03323", "abs": "https://arxiv.org/abs/2601.03323", "authors": ["Oran Duan", "Yinghua Shen", "Yingzhu Lv", "Luyang Jie", "Yaxin Liu", "Qiong Wu"], "title": "Listen to Rhythm, Choose Movements: Autoregressive Multimodal Dance Generation via Diffusion and Mamba with Decoupled Dance Dataset", "comment": "12 pages, 13 figures", "summary": "Advances in generative models and sequence learning have greatly promoted research in dance motion generation, yet current methods still suffer from coarse semantic control and poor coherence in long sequences. In this work, we present Listen to Rhythm, Choose Movements (LRCM), a multimodal-guided diffusion framework supporting both diverse input modalities and autoregressive dance motion generation. We explore a feature decoupling paradigm for dance datasets and generalize it to the Motorica Dance dataset, separating motion capture data, audio rhythm, and professionally annotated global and local text descriptions. Our diffusion architecture integrates an audio-latent Conformer and a text-latent Cross-Conformer, and incorporates a Motion Temporal Mamba Module (MTMM) to enable smooth, long-duration autoregressive synthesis. Experimental results indicate that LRCM delivers strong performance in both functional capability and quantitative metrics, demonstrating notable potential in multimodal input scenarios and extended sequence generation. We will release the full codebase, dataset, and pretrained models publicly upon acceptance.", "AI": {"tldr": "LRCM\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u652f\u6301\u591a\u6837\u5316\u8f93\u5165\u6a21\u6001\u548c\u81ea\u56de\u5f52\u821e\u8e48\u52a8\u4f5c\u751f\u6210\uff0c\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u548cMotion Temporal Mamba\u6a21\u5757\u5b9e\u73b0\u957f\u5e8f\u5217\u7684\u5e73\u6ed1\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u821e\u8e48\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u63a7\u5236\u7c97\u7cd9\u548c\u957f\u5e8f\u5217\u8fde\u8d2f\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u5e76\u5b9e\u73b0\u9ad8\u8d28\u91cf\u957f\u5e8f\u5217\u751f\u6210\u7684\u65b0\u6846\u67b6\u3002", "method": "\u63d0\u51faLRCM\u591a\u6a21\u6001\u5f15\u5bfc\u6269\u6563\u6846\u67b6\uff0c\u91c7\u7528\u7279\u5f81\u89e3\u8026\u8303\u5f0f\u5206\u79bb\u8fd0\u52a8\u6355\u6349\u6570\u636e\u3001\u97f3\u9891\u8282\u594f\u548c\u4e13\u4e1a\u6807\u6ce8\u7684\u5168\u5c40/\u5c40\u90e8\u6587\u672c\u63cf\u8ff0\uff0c\u96c6\u6210\u97f3\u9891\u6f5c\u5728Conformer\u548c\u6587\u672c\u6f5c\u5728Cross-Conformer\uff0c\u5e76\u5f15\u5165Motion Temporal Mamba\u6a21\u5757\u5b9e\u73b0\u81ea\u56de\u5f52\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLRCM\u5728\u529f\u80fd\u80fd\u529b\u548c\u91cf\u5316\u6307\u6807\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5728\u591a\u6a21\u6001\u8f93\u5165\u573a\u666f\u548c\u6269\u5c55\u5e8f\u5217\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "LRCM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u821e\u8e48\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u63a7\u5236\u548c\u957f\u5e8f\u5217\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u821e\u8e48\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
