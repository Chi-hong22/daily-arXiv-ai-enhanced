<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [A MARL-based Approach for Easing MAS Organization Engineering](https://arxiv.org/abs/2506.05437)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Louis-Marie Traonouez,Paul Théron*

Main category: cs.MA

TL;DR: 论文提出了一种名为AOMEA的方法，通过结合多智能体强化学习和组织模型，辅助设计多智能体系统的组织结构，以解决复杂部署环境中的高成本和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在复杂分布式问题中表现优异，但其设计效率高度依赖部署环境的知识。复杂和低可读性的环境导致现有方法成本高或存在安全隐患。

Method: 提出AOMEA方法，结合多智能体强化学习（MARL）和组织模型，生成相关组织规范以辅助MAS设计。

Result: AOMEA能够为MAS工程提供有效的组织规范建议，降低设计复杂性和成本。

Conclusion: AOMEA是一种创新的方法，能够有效解决MAS设计中的复杂性和安全性问题。

Abstract: Multi-Agent Systems (MAS) have been successfully applied in industry for
their ability to address complex, distributed problems, especially in IoT-based
systems. Their efficiency in achieving given objectives and meeting design
requirements is strongly dependent on the MAS organization during the
engineering process of an application-specific MAS. To design a MAS that can
achieve given goals, available methods rely on the designer's knowledge of the
deployment environment. However, high complexity and low readability in some
deployment environments make the application of these methods to be costly or
raise safety concerns. In order to ease the MAS organization design regarding
those concerns, we introduce an original Assisted MAS Organization Engineering
Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement
Learning (MARL) process with an organizational model to suggest relevant
organizational specifications to help in MAS engineering.

</details>


### [2] [Sequence Modeling for N-Agent Ad Hoc Teamwork](https://arxiv.org/abs/2506.05527)
*Caroline Wang,Di Yang Shi,Elad Liebman,Ishan Durugkar,Arrasy Rahman,Peter Stone*

Main category: cs.MA

TL;DR: 论文提出了一种基于Transformer的集中式方法MAT-NAHT，用于解决N-agent ad hoc teamwork问题，优于现有独立学习方法POAM。


<details>
  <summary>Details</summary>
Motivation: 现有独立学习方法POAM无法充分捕捉多智能体间的动态协作关系，而Transformer在处理变长序列和多任务学习中的有效性为改进提供了思路。

Method: 采用集中式Transformer架构，整合所有受控智能体的历史观察和动作，以应对部分可观测环境中的未知队友。

Result: 在StarCraft II任务中，MAT-NAHT表现出更高的样本效率和泛化能力，优于POAM。

Conclusion: MAT-NAHT通过Transformer的序列建模能力，有效解决了多智能体动态协作问题，无需额外建模目标。

Abstract: N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent
reinforcement learning, where controlled subteams of varying sizes must
dynamically collaborate with varying numbers and types of unknown teammates
without pre-coordination. The existing learning algorithm (POAM) considers only
independent learning for its flexibility in dealing with a changing number of
agents. However, independent learning fails to fully capture the inter-agent
dynamics essential for effective collaboration. Based on our observation that
transformers deal effectively with sequences with varying lengths and have been
shown to be highly effective for a variety of machine learning problems, this
work introduces a centralized, transformer-based method for N-agent ad hoc
teamwork. Our proposed approach incorporates historical observations and
actions of all controlled agents, enabling optimal responses to diverse and
unseen teammates in partially observable environments. Empirical evaluation on
a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving
superior sample efficiency and generalization, without auxiliary agent-modeling
objectives.

</details>


### [3] [Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars](https://arxiv.org/abs/2506.05555)
*Oliver Slumbers,Joel Z. Leibo,Marco A. Janssen*

Main category: cs.MA

TL;DR: 论文探讨了生成式AI（如LLM）是否能够替代人类参与者，用于大规模集体风险社会困境（CRSD）实验，以解决传统方法在多样性和规模上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统CRSD研究方法需要大规模人类实验，但难以保证足够的统计功效和社会人口多样性。生成式AI可能提供一种补充方案。

Method: 通过使用大型语言模型（LLM）替代人类参与者，构建可扩展的实证框架，验证其在大规模CRSD实验中的可行性和多样性表现。

Result: 论文聚焦于LLM在复杂CRSD（如Port of Mars）中的有效性，以验证其是否能模拟人类实验的多样性和规模。

Conclusion: 生成式AI有望成为CRSD研究的有力补充工具，但需进一步验证其在大规模复杂场景中的表现。

Abstract: Collective risk social dilemmas (CRSD) highlight a trade-off between
individual preferences and the need for all to contribute toward achieving a
group objective. Problems such as climate change are in this category, and so
it is critical to understand their social underpinnings. However, rigorous CRSD
methodology often demands large-scale human experiments but it is difficult to
guarantee sufficient power and heterogeneity over socio-demographic factors.
Generative AI offers a potential complementary approach to address thisproblem.
By replacing human participants with large language models (LLM), it allows for
a scalable empirical framework. This paper focuses on the validity of this
approach and whether it is feasible to represent a large-scale human-like
experiment with sufficient diversity using LLM. In particular, where previous
literature has focused on political surveys, virtual towns and classical
game-theoretic examples, we focus on a complex CRSD used in the institutional
economics and sustainability literature known as Port of Mars

</details>


### [4] [Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game](https://arxiv.org/abs/2506.06032)
*Edward Hughes,Tina O. Zhu,Martin J. Chadwick,Raphael Koster,Antonio García Castañeda,Charles Beattie,Thore Graepel,Matthew M. Botvinick,Joel Z. Leibo*

Main category: cs.MA

TL;DR: 多智能体强化学习可用于模拟复杂社会行为，但缺乏实验支持。本文通过实验验证其在公共物品博弈中的有效性，发现人类和AI在可识别身份时表现更好。


<details>
  <summary>Details</summary>
Motivation: 验证多智能体强化学习在复杂社会行为建模中的实用性，并通过实验填补理论与现实的差距。

Method: 在公共物品博弈（Clean Up）中，对比人类和AI在可识别身份与匿名条件下的表现，并开发新的声誉合作模型。

Result: 人类和AI在可识别身份时表现更优，且均通过轮流合作解决问题。

Conclusion: 多智能体强化学习能有效模拟复杂社会行为，尤其在身份识别和声誉管理方面。

Abstract: Multi-agent reinforcement learning algorithms are useful for simulating
social behavior in settings that are too complex for other theoretical
approaches like game theory. However, they have not yet been empirically
supported by laboratory experiments with real human participants. In this work
we demonstrate how multi-agent reinforcement learning can model group behavior
in a spatially and temporally complex public good provision game called Clean
Up. We show that human groups succeed in Clean Up when they can see who is who
and track reputations over time but fail under conditions of anonymity. A new
multi-agent reinforcement learning model of reputation-based cooperation
demonstrates the same difference between identifiable and anonymous conditions.
Furthermore, both human groups and artificial agent groups solve the problem
via turn-taking despite other options being available. Our results highlight
the benefits of using multi-agent reinforcement learning to model human social
behavior in complex environments.

</details>
