{"id": "2506.05437", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.05437", "abs": "https://arxiv.org/abs/2506.05437", "authors": ["Julien Soulé", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Théron"], "title": "A MARL-based Approach for Easing MAS Organization Engineering", "comment": null, "summary": "Multi-Agent Systems (MAS) have been successfully applied in industry for\ntheir ability to address complex, distributed problems, especially in IoT-based\nsystems. Their efficiency in achieving given objectives and meeting design\nrequirements is strongly dependent on the MAS organization during the\nengineering process of an application-specific MAS. To design a MAS that can\nachieve given goals, available methods rely on the designer's knowledge of the\ndeployment environment. However, high complexity and low readability in some\ndeployment environments make the application of these methods to be costly or\nraise safety concerns. In order to ease the MAS organization design regarding\nthose concerns, we introduce an original Assisted MAS Organization Engineering\nApproach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement\nLearning (MARL) process with an organizational model to suggest relevant\norganizational specifications to help in MAS engineering."}
{"id": "2506.05527", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.05527", "abs": "https://arxiv.org/abs/2506.05527", "authors": ["Caroline Wang", "Di Yang Shi", "Elad Liebman", "Ishan Durugkar", "Arrasy Rahman", "Peter Stone"], "title": "Sequence Modeling for N-Agent Ad Hoc Teamwork", "comment": null, "summary": "N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent\nreinforcement learning, where controlled subteams of varying sizes must\ndynamically collaborate with varying numbers and types of unknown teammates\nwithout pre-coordination. The existing learning algorithm (POAM) considers only\nindependent learning for its flexibility in dealing with a changing number of\nagents. However, independent learning fails to fully capture the inter-agent\ndynamics essential for effective collaboration. Based on our observation that\ntransformers deal effectively with sequences with varying lengths and have been\nshown to be highly effective for a variety of machine learning problems, this\nwork introduces a centralized, transformer-based method for N-agent ad hoc\nteamwork. Our proposed approach incorporates historical observations and\nactions of all controlled agents, enabling optimal responses to diverse and\nunseen teammates in partially observable environments. Empirical evaluation on\na StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving\nsuperior sample efficiency and generalization, without auxiliary agent-modeling\nobjectives."}
{"id": "2506.05555", "categories": ["cs.MA", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.05555", "abs": "https://arxiv.org/abs/2506.05555", "authors": ["Oliver Slumbers", "Joel Z. Leibo", "Marco A. Janssen"], "title": "Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars", "comment": null, "summary": "Collective risk social dilemmas (CRSD) highlight a trade-off between\nindividual preferences and the need for all to contribute toward achieving a\ngroup objective. Problems such as climate change are in this category, and so\nit is critical to understand their social underpinnings. However, rigorous CRSD\nmethodology often demands large-scale human experiments but it is difficult to\nguarantee sufficient power and heterogeneity over socio-demographic factors.\nGenerative AI offers a potential complementary approach to address thisproblem.\nBy replacing human participants with large language models (LLM), it allows for\na scalable empirical framework. This paper focuses on the validity of this\napproach and whether it is feasible to represent a large-scale human-like\nexperiment with sufficient diversity using LLM. In particular, where previous\nliterature has focused on political surveys, virtual towns and classical\ngame-theoretic examples, we focus on a complex CRSD used in the institutional\neconomics and sustainability literature known as Port of Mars"}
{"id": "2506.06032", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.06032", "abs": "https://arxiv.org/abs/2506.06032", "authors": ["Edward Hughes", "Tina O. Zhu", "Martin J. Chadwick", "Raphael Koster", "Antonio García Castañeda", "Charles Beattie", "Thore Graepel", "Matthew M. Botvinick", "Joel Z. Leibo"], "title": "Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game", "comment": "45 pages, 29 figures. arXiv admin note: substantial text overlap with\n  arXiv:2103.04982", "summary": "Multi-agent reinforcement learning algorithms are useful for simulating\nsocial behavior in settings that are too complex for other theoretical\napproaches like game theory. However, they have not yet been empirically\nsupported by laboratory experiments with real human participants. In this work\nwe demonstrate how multi-agent reinforcement learning can model group behavior\nin a spatially and temporally complex public good provision game called Clean\nUp. We show that human groups succeed in Clean Up when they can see who is who\nand track reputations over time but fail under conditions of anonymity. A new\nmulti-agent reinforcement learning model of reputation-based cooperation\ndemonstrates the same difference between identifiable and anonymous conditions.\nFurthermore, both human groups and artificial agent groups solve the problem\nvia turn-taking despite other options being available. Our results highlight\nthe benefits of using multi-agent reinforcement learning to model human social\nbehavior in complex environments."}
{"id": "2506.05437", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.05437", "abs": "https://arxiv.org/abs/2506.05437", "authors": ["Julien Soulé", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Théron"], "title": "A MARL-based Approach for Easing MAS Organization Engineering", "comment": null, "summary": "Multi-Agent Systems (MAS) have been successfully applied in industry for\ntheir ability to address complex, distributed problems, especially in IoT-based\nsystems. Their efficiency in achieving given objectives and meeting design\nrequirements is strongly dependent on the MAS organization during the\nengineering process of an application-specific MAS. To design a MAS that can\nachieve given goals, available methods rely on the designer's knowledge of the\ndeployment environment. However, high complexity and low readability in some\ndeployment environments make the application of these methods to be costly or\nraise safety concerns. In order to ease the MAS organization design regarding\nthose concerns, we introduce an original Assisted MAS Organization Engineering\nApproach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement\nLearning (MARL) process with an organizational model to suggest relevant\norganizational specifications to help in MAS engineering."}
{"id": "2506.05527", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.05527", "abs": "https://arxiv.org/abs/2506.05527", "authors": ["Caroline Wang", "Di Yang Shi", "Elad Liebman", "Ishan Durugkar", "Arrasy Rahman", "Peter Stone"], "title": "Sequence Modeling for N-Agent Ad Hoc Teamwork", "comment": null, "summary": "N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent\nreinforcement learning, where controlled subteams of varying sizes must\ndynamically collaborate with varying numbers and types of unknown teammates\nwithout pre-coordination. The existing learning algorithm (POAM) considers only\nindependent learning for its flexibility in dealing with a changing number of\nagents. However, independent learning fails to fully capture the inter-agent\ndynamics essential for effective collaboration. Based on our observation that\ntransformers deal effectively with sequences with varying lengths and have been\nshown to be highly effective for a variety of machine learning problems, this\nwork introduces a centralized, transformer-based method for N-agent ad hoc\nteamwork. Our proposed approach incorporates historical observations and\nactions of all controlled agents, enabling optimal responses to diverse and\nunseen teammates in partially observable environments. Empirical evaluation on\na StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving\nsuperior sample efficiency and generalization, without auxiliary agent-modeling\nobjectives."}
{"id": "2506.05555", "categories": ["cs.MA", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.05555", "abs": "https://arxiv.org/abs/2506.05555", "authors": ["Oliver Slumbers", "Joel Z. Leibo", "Marco A. Janssen"], "title": "Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars", "comment": null, "summary": "Collective risk social dilemmas (CRSD) highlight a trade-off between\nindividual preferences and the need for all to contribute toward achieving a\ngroup objective. Problems such as climate change are in this category, and so\nit is critical to understand their social underpinnings. However, rigorous CRSD\nmethodology often demands large-scale human experiments but it is difficult to\nguarantee sufficient power and heterogeneity over socio-demographic factors.\nGenerative AI offers a potential complementary approach to address thisproblem.\nBy replacing human participants with large language models (LLM), it allows for\na scalable empirical framework. This paper focuses on the validity of this\napproach and whether it is feasible to represent a large-scale human-like\nexperiment with sufficient diversity using LLM. In particular, where previous\nliterature has focused on political surveys, virtual towns and classical\ngame-theoretic examples, we focus on a complex CRSD used in the institutional\neconomics and sustainability literature known as Port of Mars"}
{"id": "2506.06032", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.06032", "abs": "https://arxiv.org/abs/2506.06032", "authors": ["Edward Hughes", "Tina O. Zhu", "Martin J. Chadwick", "Raphael Koster", "Antonio García Castañeda", "Charles Beattie", "Thore Graepel", "Matthew M. Botvinick", "Joel Z. Leibo"], "title": "Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game", "comment": "45 pages, 29 figures. arXiv admin note: substantial text overlap with\n  arXiv:2103.04982", "summary": "Multi-agent reinforcement learning algorithms are useful for simulating\nsocial behavior in settings that are too complex for other theoretical\napproaches like game theory. However, they have not yet been empirically\nsupported by laboratory experiments with real human participants. In this work\nwe demonstrate how multi-agent reinforcement learning can model group behavior\nin a spatially and temporally complex public good provision game called Clean\nUp. We show that human groups succeed in Clean Up when they can see who is who\nand track reputations over time but fail under conditions of anonymity. A new\nmulti-agent reinforcement learning model of reputation-based cooperation\ndemonstrates the same difference between identifiable and anonymous conditions.\nFurthermore, both human groups and artificial agent groups solve the problem\nvia turn-taking despite other options being available. Our results highlight\nthe benefits of using multi-agent reinforcement learning to model human social\nbehavior in complex environments."}
{"id": "2506.05437", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.05437", "abs": "https://arxiv.org/abs/2506.05437", "authors": ["Julien Soulé", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Théron"], "title": "A MARL-based Approach for Easing MAS Organization Engineering", "comment": null, "summary": "Multi-Agent Systems (MAS) have been successfully applied in industry for\ntheir ability to address complex, distributed problems, especially in IoT-based\nsystems. Their efficiency in achieving given objectives and meeting design\nrequirements is strongly dependent on the MAS organization during the\nengineering process of an application-specific MAS. To design a MAS that can\nachieve given goals, available methods rely on the designer's knowledge of the\ndeployment environment. However, high complexity and low readability in some\ndeployment environments make the application of these methods to be costly or\nraise safety concerns. In order to ease the MAS organization design regarding\nthose concerns, we introduce an original Assisted MAS Organization Engineering\nApproach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement\nLearning (MARL) process with an organizational model to suggest relevant\norganizational specifications to help in MAS engineering."}
{"id": "2506.05527", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.05527", "abs": "https://arxiv.org/abs/2506.05527", "authors": ["Caroline Wang", "Di Yang Shi", "Elad Liebman", "Ishan Durugkar", "Arrasy Rahman", "Peter Stone"], "title": "Sequence Modeling for N-Agent Ad Hoc Teamwork", "comment": null, "summary": "N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent\nreinforcement learning, where controlled subteams of varying sizes must\ndynamically collaborate with varying numbers and types of unknown teammates\nwithout pre-coordination. The existing learning algorithm (POAM) considers only\nindependent learning for its flexibility in dealing with a changing number of\nagents. However, independent learning fails to fully capture the inter-agent\ndynamics essential for effective collaboration. Based on our observation that\ntransformers deal effectively with sequences with varying lengths and have been\nshown to be highly effective for a variety of machine learning problems, this\nwork introduces a centralized, transformer-based method for N-agent ad hoc\nteamwork. Our proposed approach incorporates historical observations and\nactions of all controlled agents, enabling optimal responses to diverse and\nunseen teammates in partially observable environments. Empirical evaluation on\na StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving\nsuperior sample efficiency and generalization, without auxiliary agent-modeling\nobjectives."}
{"id": "2506.05555", "categories": ["cs.MA", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.05555", "abs": "https://arxiv.org/abs/2506.05555", "authors": ["Oliver Slumbers", "Joel Z. Leibo", "Marco A. Janssen"], "title": "Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars", "comment": null, "summary": "Collective risk social dilemmas (CRSD) highlight a trade-off between\nindividual preferences and the need for all to contribute toward achieving a\ngroup objective. Problems such as climate change are in this category, and so\nit is critical to understand their social underpinnings. However, rigorous CRSD\nmethodology often demands large-scale human experiments but it is difficult to\nguarantee sufficient power and heterogeneity over socio-demographic factors.\nGenerative AI offers a potential complementary approach to address thisproblem.\nBy replacing human participants with large language models (LLM), it allows for\na scalable empirical framework. This paper focuses on the validity of this\napproach and whether it is feasible to represent a large-scale human-like\nexperiment with sufficient diversity using LLM. In particular, where previous\nliterature has focused on political surveys, virtual towns and classical\ngame-theoretic examples, we focus on a complex CRSD used in the institutional\neconomics and sustainability literature known as Port of Mars"}
{"id": "2506.06032", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.06032", "abs": "https://arxiv.org/abs/2506.06032", "authors": ["Edward Hughes", "Tina O. Zhu", "Martin J. Chadwick", "Raphael Koster", "Antonio García Castañeda", "Charles Beattie", "Thore Graepel", "Matthew M. Botvinick", "Joel Z. Leibo"], "title": "Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game", "comment": "45 pages, 29 figures. arXiv admin note: substantial text overlap with\n  arXiv:2103.04982", "summary": "Multi-agent reinforcement learning algorithms are useful for simulating\nsocial behavior in settings that are too complex for other theoretical\napproaches like game theory. However, they have not yet been empirically\nsupported by laboratory experiments with real human participants. In this work\nwe demonstrate how multi-agent reinforcement learning can model group behavior\nin a spatially and temporally complex public good provision game called Clean\nUp. We show that human groups succeed in Clean Up when they can see who is who\nand track reputations over time but fail under conditions of anonymity. A new\nmulti-agent reinforcement learning model of reputation-based cooperation\ndemonstrates the same difference between identifiable and anonymous conditions.\nFurthermore, both human groups and artificial agent groups solve the problem\nvia turn-taking despite other options being available. Our results highlight\nthe benefits of using multi-agent reinforcement learning to model human social\nbehavior in complex environments."}
