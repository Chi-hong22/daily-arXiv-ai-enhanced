<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 24]
- [cs.CV](#cs.CV) [Total: 98]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SD](#cs.SD) [Total: 10]
- [eess.SY](#eess.SY) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production](https://arxiv.org/abs/2506.08039)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: AI Maglev Conveyor系统结合磁悬浮技术和AI，提升制造效率，降低成本，支持可持续生产。


<details>
  <summary>Details</summary>
Motivation: 现代制造业需要高效、快速和精确的生产方式，传统系统存在摩擦和维护成本高的问题。

Method: 结合磁悬浮技术和AI，实现无摩擦运输、实时监控和自适应控制。

Result: 系统减少维护成本、提高效率、优化资源使用，并支持灵活生产。

Conclusion: AI Maglev Conveyor是高效、灵活且经济的生产解决方案，适用于多种行业。

Abstract: Efficiency, speed, and precision are essential in modern manufacturing. AI
Maglev Conveyor system, combining magnetic levitation (maglev) technology with
artificial intelligence (AI), revolutionizes automated production processes.
This system reduces maintenance costs and downtime by eliminating friction,
enhancing operational efficiency. It transports goods swiftly with minimal
energy consumption, optimizing resource use and supporting sustainability. AI
integration enables real-time monitoring and adaptive control, allowing
businesses to respond to production demand fluctuations and streamline supply
chain operations.
  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse
product types and sizes for flexible manufacturing without extensive
reconfiguration. AI algorithms optimize routing, reduce cycle times, and
improve throughput, creating an agile production line adaptable to market
changes.
  This applied research paper introduces the Maglev Conveyor system, featuring
an electromagnetic controller and multiple movers to enhance automation. It
offers cost savings as an alternative to setups using six-axis robots or linear
motors, with precise adjustments for robotic arm loading. Operating at high
speeds minimizes treatment time for delicate components while maintaining
precision. Its adaptable design accommodates various materials, facilitating
integration of processing stations alongside electronic product assembly.
Positioned between linear-axis and robotic systems in cost, the Maglev Conveyor
is ideal for flat parts requiring minimal travel, transforming production
efficiency across industries. It explores its technical advantages,
flexibility, cost reductions, and overall benefits.

</details>


### [2] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.RO

TL;DR: Agentic UAVs结合感知、决策、记忆和协作规划，超越传统无人机，具备目标驱动行为和自主性。本文探讨其架构、技术、应用领域及挑战，并提出未来发展路线。


<details>
  <summary>Details</summary>
Motivation: 研究Agentic UAVs的动机在于其通过Agentic AI技术实现更高级的自主性和适应性，为复杂环境提供解决方案。

Method: 通过分析架构组件、技术对比和应用案例，全面探讨Agentic UAVs的特点与优势。

Result: 研究展示了Agentic UAVs在多个高影响力领域的应用潜力，并识别了技术、法规和数据可靠性等挑战。

Conclusion: 本文为Agentic UAVs的未来发展、部署和治理提供了基础框架，强调了其在社会和工业领域的广泛价值。

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [3] [Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards](https://arxiv.org/abs/2506.08061)
*Ali Abedi,Fernando Cladera,Mohsen Farajijalal,Reza Ehsani*

Main category: cs.RO

TL;DR: 本文提出了一种基于移动LiDAR数据的实时系统，用于估计单棵树冠体积，适用于结构多样的果园环境。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态扫描或假设果园结构均匀，无法适应多样化的田间几何形状。本文旨在开发一种适应性强的系统。

Method: 通过LiDAR-惯性里程计、自适应分割和几何重建的集成流程，结合DBSCAN和谱聚类的混合聚类策略。

Result: 在两种商业果园（开心果和杏仁）中测试，分割成功率达93%（开心果）和80%（杏仁），与无人机估计结果一致。

Conclusion: 该系统为结构多样的果园环境提供了可扩展、非侵入式的树木监测方法。

Abstract: We present a real-time system for per-tree canopy volume estimation using
mobile LiDAR data collected during routine robotic navigation. Unlike prior
approaches that rely on static scans or assume uniform orchard structures, our
method adapts to varying field geometries via an integrated pipeline of
LiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.
We evaluate the system across two commercial orchards, one pistachio orchard
with regular spacing and one almond orchard with dense, overlapping crowns. A
hybrid clustering strategy combining DBSCAN and spectral clustering enables
robust per-tree segmentation, achieving 93% success in pistachio and 80% in
almond, with strong agreement to drone derived canopy volume estimates. This
work advances scalable, non-intrusive tree monitoring for structurally diverse
orchard environments.

</details>


### [4] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Main category: cs.RO

TL;DR: 论文提出CALL方法，通过生成AI的世界模型和潜在表示解决多智能体强化学习中的部分可观测性和非平稳性问题，减少通信开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在复杂高维环境（如自动驾驶）中面临部分可观测性和非平稳性问题，信息共享虽常用但存在通信开销和可扩展性问题。

Method: 提出CALL方法，利用世界模型的潜在表示进行轻量级通信，智能体通过自我中心学习和信息共享丰富世界模型，提升预测和规划能力。

Result: 实验在CARLA平台上验证了CALL在局部轨迹规划任务中的性能提升，并量化了信息共享对预测准确性和性能差距的影响。

Conclusion: CALL通过轻量级信息共享和世界模型的泛化能力，有效解决了MARL中的挑战，提升了任务性能。

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [5] [TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation](https://arxiv.org/abs/2506.08291)
*Won Kyung Do,Matthew Strong,Aiden Swann,Boshu Lei,Monroe Kennedy III*

Main category: cs.RO

TL;DR: TensorTouch通过结合有限元分析和深度学习，从光学触觉传感器中提取全面的接触信息，解决了多触点操作中的挑战。


<details>
  <summary>Details</summary>
Motivation: 多触点操作（如从地面捏硬币或操作交织物体）对机器人系统仍具挑战性，需要高分辨率触觉传感。

Method: TensorTouch整合有限元分析和深度学习，提取应力张量、变形场和力分布等像素级接触信息。

Result: 实验验证显示，TensorTouch在选择性抓取两根绳子时成功率90%，支持大变形触觉传感。

Conclusion: TensorTouch为机器人系统提供了新的高精度触觉操作能力。

Abstract: Advanced dexterous manipulation involving multiple simultaneous contacts
across different surfaces, like pinching coins from ground or manipulating
intertwined objects, remains challenging for robotic systems. Such tasks exceed
the capabilities of vision and proprioception alone, requiring high-resolution
tactile sensing with calibrated physical metrics. Raw optical tactile sensor
images, while information-rich, lack interpretability and cross-sensor
transferability, limiting their real-world utility. TensorTouch addresses this
challenge by integrating finite element analysis with deep learning to extract
comprehensive contact information from optical tactile sensors, including
stress tensors, deformation fields, and force distributions at pixel-level
resolution. The TensorTouch framework achieves sub-millimeter position accuracy
and precise force estimation while supporting large sensor deformations crucial
for manipulating soft objects. Experimental validation demonstrates 90% success
in selectively grasping one of two strings based on detected motion, enabling
new contact-rich manipulation capabilities previously inaccessible to robotic
systems.

</details>


### [6] [HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation](https://arxiv.org/abs/2506.08296)
*Hongjun Wu,Heng Zhang,Pengsong Zhang,Jin Wang,Cong Wang*

Main category: cs.RO

TL;DR: HiBerNAC是一种基于神经科学启发的分层多智能体框架，用于解决复杂机器人操作任务中的记忆、协调和规划问题，显著提升了任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态视觉-语言-动作（VLA）模型在复杂操作任务中仍受限于记忆、多智能体协调和动态规划能力，需要新的解决方案。

Method: 结合多模态VLA规划与神经启发的反射机制和多智能体协作，设计分层决策框架HiBerNAC。

Result: 实验显示HiBerNAC将长时任务完成时间减少23%，并在多路径任务中实现12-31%的成功率，优于现有VLA模型。

Conclusion: HiBerNAC为生物认知与机器人学习机制的结合提供了初步证据，展示了神经启发方法在复杂任务中的潜力。

Abstract: Recent advances in multimodal vision-language-action (VLA) models have
revolutionized traditional robot learning, enabling systems to interpret
vision, language, and action in unified frameworks for complex task planning.
However, mastering complex manipulation tasks remains an open challenge,
constrained by limitations in persistent contextual memory, multi-agent
coordination under uncertainty, and dynamic long-horizon planning across
variable sequences. To address this challenge, we propose \textbf{HiBerNAC}, a
\textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic
\textbf{N}eural \textbf{A}gent \textbf{C}ollective, inspired by breakthroughs
in neuroscience, particularly in neural circuit mechanisms and hierarchical
decision-making. Our framework combines: (1) multimodal VLA planning and
reasoning with (2) neuro-inspired reflection and multi-agent mechanisms,
specifically designed for complex robotic manipulation tasks. By leveraging
neuro-inspired functional modules with decentralized multi-agent collaboration,
our approach enables robust and enhanced real-time execution of complex
manipulation tasks. In addition, the agentic system exhibits scalable
collective intelligence via dynamic agent specialization, adapting its
coordination strategy to variable task horizons and complexity. Through
extensive experiments on complex manipulation tasks compared with
state-of-the-art VLA models, we demonstrate that \textbf{HiBerNAC} reduces
average long-horizon task completion time by 23\%, and achieves non-zero
success rates (12\textendash 31\%) on multi-path tasks where prior
state-of-the-art VLA models consistently fail. These results provide indicative
evidence for bridging biological cognition and robotic learning mechanisms.

</details>


### [7] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: 提出了一种名为Re4MPC的多模型运动规划方法，结合非线性模型预测控制（NMPC）和深度强化学习（DRL），以提高计算效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的高自由度机器人运动规划方法计算成本高，难以适用于实际场景。

Method: 通过DRL学习策略，动态选择NMPC的模型、成本和约束条件，并结合数学框架将NMPC集成到DRL中。

Result: 实验表明，Re4MPC比传统NMPC基线方法计算效率更高，且末端执行器目标达成率更高。

Conclusion: Re4MPC为高自由度机器人提供了一种高效的运动规划解决方案。

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [8] [Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots](https://arxiv.org/abs/2506.08416)
*Bolin Li,Linwei Sun,Xuecong Huang,Yuzhi Jiang,Lijun Zhu*

Main category: cs.RO

TL;DR: 提出了一种基于奖励组合的周期性双足步态学习方法，结合实时步态规划器，用于人形机器人。


<details>
  <summary>Details</summary>
Motivation: 通过结合动态规划和强化学习，减少学习时间并提高运动性能。

Method: 1. 设计了一个结合动态的步态规划器，将3D模型解耦为2D模型并近似为混合倒立摆（H-LIP）；2. 在强化学习框架中设计了三种奖励函数，形成奖励组合。

Result: 提出的方法减少了学习时间并提升了运动性能，通过实例和性能对比验证了有效性。

Conclusion: 该方法通过奖励组合和实时步态规划，成功实现了周期性双足步态学习，具有高效性和实用性。

Abstract: This paper presents a periodic bipedal gait learning method using reward
composition, integrated with a real-time gait planner for humanoid robots.
First, we introduce a novel gait planner that incorporates dynamics to design
the desired joint trajectory. In the gait design process, the 3D robot model is
decoupled into two 2D models, which are then approximated as hybrid inverted
pendulums (H-LIP) for trajectory planning. The gait planner operates in
parallel in real time within the robot's learning environment. Second, based on
this gait planner, we design three effective reward functions within a
reinforcement learning framework, forming a reward composition to achieve
periodic bipedal gait. This reward composition reduces the robot's learning
time and enhances locomotion performance. Finally, a gait design example and
performance comparison are presented to demonstrate the effectiveness of the
proposed method.

</details>


### [9] [Attention-based Learning for 3D Informative Path Planning](https://arxiv.org/abs/2506.08434)
*Rui Zhao,Xingjian Zhang,Yuhong Cao,Yizhuo Wang,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出了一种基于注意力的深度强化学习方法，用于解决3D空间中的自适应信息路径规划问题，通过动态调整无人机位置以平衡感知范围和精度。


<details>
  <summary>Details</summary>
Motivation: 解决自适应信息路径规划问题，使无人机在时间和距离约束下最大化信息收集，同时适应新获取的传感器数据。

Method: 利用注意力机制捕捉全局空间依赖关系，构建上下文信念表示，优化短期和长期搜索目标。

Result: 在有限预算内显著降低环境不确定性，优于现有规划器，并能泛化到不同规模的环境。

Conclusion: 该方法在平衡探索与利用方面表现优异，具有广泛的实际应用潜力。

Abstract: In this work, we propose an attention-based deep reinforcement learning
approach to address the adaptive informative path planning (IPP) problem in 3D
space, where an aerial robot equipped with a downward-facing sensor must
dynamically adjust its 3D position to balance sensing footprint and accuracy,
and finally obtain a high-quality belief of an underlying field of interest
over a given domain (e.g., presence of specific plants, hazardous gas,
geological structures, etc.). In adaptive IPP tasks, the agent is tasked with
maximizing information collected under time/distance constraints, continuously
adapting its path based on newly acquired sensor data. To this end, we leverage
attention mechanisms for their strong ability to capture global spatial
dependencies across large action spaces, allowing the agent to learn an
implicit estimation of environmental transitions. Our model builds a contextual
belief representation over the entire domain, guiding sequential movement
decisions that optimize both short- and long-term search objectives.
Comparative evaluations against state-of-the-art planners demonstrate that our
approach significantly reduces environmental uncertainty within constrained
budgets, thus allowing the agent to effectively balance exploration and
exploitation. We further show our model generalizes well to environments of
varying sizes, highlighting its potential for many real-world applications.

</details>


### [10] [TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization](https://arxiv.org/abs/2506.08440)
*Zengjue Chen,Runliang Niu,He Kong,Qi Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为TGRPO的方法，通过结合步级和轨迹级优势信号，改进了GRPO的优势估计，适用于VLA模型的在线强化学习训练。实验表明，TGRPO在多个任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在新环境中仍需任务特定的微调，且依赖静态轨迹数据集，无法利用实时交互反馈。强化学习提供了一种闭环交互的替代方案。

Method: 提出了TGRPO方法，融合步级和轨迹级优势信号，改进GRPO的优势估计，适用于VLA模型的在线强化学习训练。

Result: 在十个操作任务的实验中，TGRPO表现优于基线方法，能生成更鲁棒和高效的政策。

Conclusion: TGRPO方法在VLA模型的在线强化学习中表现出色，为任务特定微调提供了更优的解决方案。

Abstract: Recent advances in Vision-Language-Action (VLA) model have demonstrated
strong generalization capabilities across diverse scenes, tasks, and robotic
platforms when pretrained at large-scale datasets. However, these models still
require task-specific fine-tuning in novel environments, a process that relies
almost exclusively on supervised fine-tuning (SFT) using static trajectory
datasets. Such approaches neither allow robot to interact with environment nor
do they leverage feedback from live execution. Also, their success is
critically dependent on the size and quality of the collected trajectories.
Reinforcement learning (RL) offers a promising alternative by enabling
closed-loop interaction and aligning learned policies directly with task
objectives. In this work, we draw inspiration from the ideas of GRPO and
propose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.
By fusing step-level and trajectory-level advantage signals, this method
improves GRPO's group-level advantage estimation, thereby making the algorithm
more suitable for online reinforcement learning training of VLA. Experimental
results on ten manipulation tasks from the libero-object benchmark demonstrate
that TGRPO consistently outperforms various baseline methods, capable of
generating more robust and efficient policies across multiple tested scenarios.
Our source codes are available at: https://github.com/hahans/TGRPO

</details>


### [11] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 利用去噪扩散模型生成自动驾驶系统的潜在故障案例，无需外部数据集，适用于交通路口的安全验证。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统安全验证中高风险、高成本及故障案例罕见且多样的问题。

Method: 训练去噪扩散模型，根据初始交通状态生成潜在故障案例。

Result: 在四路交叉口实验中，模型能生成多样化且真实的故障样本。

Conclusion: 该模型无需外部数据，计算资源需求低，适用于交通路口的安全验证。

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [12] [Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication](https://arxiv.org/abs/2506.08890)
*Tauhid Tanjim,Promise Ekpo,Huajie Cao,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.RO

TL;DR: 研究比较了机器人急救车（RCC）的言语与非言语沟通方式对医护人员工作负荷和态度的影响，发现言语沟通显著降低心理需求和努力。


<details>
  <summary>Details</summary>
Motivation: 医护人员在急救场景中快速获取医疗用品时面临挑战，机器人急救车可能通过沟通方式优化这一过程，但缺乏对实际环境中有效沟通方式的探索。

Method: 采用被试间实验，比较RCC的言语与非言语沟通方式与传统急救车在复苏场景中的效果，评估工作负荷和态度。

Result: 言语沟通显著降低心理需求和努力，但机器人合作时的挫败感略高于传统急救车。

Conclusion: 研究为高风险环境中人机协作提供了有价值的见解，尤其是言语沟通的优化潜力。

Abstract: Healthcare workers (HCWs) encounter challenges in hospitals, such as
retrieving medical supplies quickly from crash carts, which could potentially
result in medical errors and delays in patient care. Robotic crash carts (RCCs)
have shown promise in assisting healthcare teams during medical tasks through
guided object searches and task reminders. Limited exploration has been done to
determine what communication modalities are most effective and least disruptive
to patient care in real-world settings. To address this gap, we conducted a
between-subjects experiment comparing the RCC's verbal and non-verbal
communication of object search with a standard crash cart in resuscitation
scenarios to understand the impact of robot communication on workload and
attitudes toward using robots in the workplace. Our findings indicate that
verbal communication significantly reduced mental demand and effort compared to
visual cues and with a traditional crash cart. Although frustration levels were
slightly higher during collaborations with the robot compared to a traditional
cart, these research insights provide valuable implications for human-robot
teamwork in high-stakes environments.

</details>


### [13] [Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot](https://arxiv.org/abs/2506.08578)
*Boyang Chen,Xizhe Zang,Chao Song,Yue Zhang,Xuehe Zhang,Jie Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种针对ESVC脚的机器人状态估计方法，通过噪声分析和自适应状态估计器提高了行走效率和精度。


<details>
  <summary>Details</summary>
Motivation: ESVC脚虽然提高了机器人行走的能量效率，但由于支撑腿倾斜导致接触模型误差放大，增加了状态估计的难度。

Method: 研究通过物理实验分析噪声影响，开发噪声-时间回归模型，并提出两阶段自适应状态估计器（预估计和后估计）。

Result: 实验表明，该方法比EKF和自适应EKF更精确且收敛更快。

Conclusion: 提出的自适应状态估计器有效解决了ESVC脚机器人行走中的噪声问题，提升了状态估计性能。

Abstract: The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design
inspired by the rollover shape of the human foot, significantly enhances the
energy efficiency of the robot walking gait. However, due to the tilt of the
supporting leg, the error of the contact model are amplified, making robot
state estimation more challenging. Therefore, this paper focuses on the noise
analysis and state estimation for robot walking with the ESVC foot. First,
through physical robot experiments, we investigate the effect of the ESVC foot
on robot measurement noise and process noise. and a noise-time regression model
using sliding window strategy is developed. Then, a hierarchical adaptive state
estimator for biped robots with the ESVC foot is proposed. The state estimator
consists of two stages: pre-estimation and post-estimation. In the
pre-estimation stage, a data fusion-based estimation is employed to process the
sensory data. During post-estimation, the acceleration of center of mass is
first estimated, and then the noise covariance matrices are adjusted based on
the regression model. Following that, an EKF(Extended Kalman Filter) based
approach is applied to estimate the centroid state during robot walking.
Physical experiments demonstrate that the proposed adaptive state estimator for
biped robot walking with the ESVC foot not only provides higher precision than
both EKF and Adaptive EKF, but also converges faster under varying noise
conditions.

</details>


### [14] [Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators](https://arxiv.org/abs/2506.08639)
*Amir Hossein Barjini,Seyed Adel Alizadeh Kolagar,Sadeq Yaqubi,Jouni Mattila*

Main category: cs.RO

TL;DR: 提出了一种结合深度强化学习（DRL）和非线性偏微分方程（PDE）控制器的柔性机械臂运动规划与控制框架，通过优化轨迹减少振动。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注控制，而忽略了期望轨迹对端点振动的影响，因此需要一种综合方法来优化轨迹并确保稳定性。

Method: 使用软演员-评论家（SAC）算法训练DRL运动规划器生成优化轨迹，结合非线性PDE控制器计算扭矩并确保闭环稳定性。

Result: 仿真和实验验证了该方法在振动抑制和跟踪精度上优于传统方法。

Conclusion: 结合学习型运动规划和基于模型的控制可显著提升柔性机械臂的精度和稳定性。

Abstract: This article presents a motion planning and control framework for flexible
robotic manipulators, integrating deep reinforcement learning (DRL) with a
nonlinear partial differential equation (PDE) controller. Unlike conventional
approaches that focus solely on control, we demonstrate that the desired
trajectory significantly influences endpoint vibrations. To address this, a DRL
motion planner, trained using the soft actor-critic (SAC) algorithm, generates
optimized trajectories that inherently minimize vibrations. The PDE nonlinear
controller then computes the required torques to track the planned trajectory
while ensuring closed-loop stability using Lyapunov analysis. The proposed
methodology is validated through both simulations and real-world experiments,
demonstrating superior vibration suppression and tracking accuracy compared to
traditional methods. The results underscore the potential of combining
learning-based motion planning with model-based control for enhancing the
precision and stability of flexible robotic manipulators.

</details>


### [15] [ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel](https://arxiv.org/abs/2506.08706)
*Tomasz Winiarski,Jan Kaniuka,Daniel Giełdowski,Jakub Ostrysz,Krystian Radlak,Dmytro Kushnir*

Main category: cs.RO

TL;DR: 本文提出了一种将V模型开发范式与MeROS元模型SysML建模语言结合的方法，用于ROS系统的结构化开发，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统日益复杂和异构，现有工具如ROS和MBSE缺乏整合，需要一种结构化开发方法。

Method: 提出了一种基于MeROS的领域特定方法，结合V模型，支持ROS和ROS 2的开发，强调灵活性和可重用性。

Result: 通过HeROS多机器人平台的案例研究验证了方法的有效性，提升了系统一致性和可追溯性。

Conclusion: 该方法为ROS项目中的MBSE实践提供了结构化、工具无关的基础，具有扩展性和适应性。

Abstract: As robotic systems grow increasingly complex, heterogeneous, and
safety-critical, the need for structured development methodologies becomes
paramount. Although frameworks like the Robot Operating System (ROS) and
Model-Based Systems Engineering (MBSE) offer foundational tools, they often
lack integration when used together. This paper addresses that gap by aligning
the widely recognized V-model development paradigm with the MeROS metamodel
SysML-based modeling language tailored for ROS-based systems.
  We propose a domain-specific methodology that bridges ROS-centric modelling
with systems engineering practices. Our approach formalises the structure,
behaviour, and validation processes of robotic systems using MeROS, while
extending it with a generalized, adaptable V-model compatible with both ROS and
ROS 2. Rather than prescribing a fixed procedure, the approach supports
project-specific flexibility and reuse, offering guidance across all stages of
development.
  The approach is validated through a comprehensive case study on HeROS, a
heterogeneous multi-robot platform comprising manipulators, mobile units, and
dynamic test environments. This example illustrates how the MeROS-compatible
V-model enhances traceability and system consistency while remaining accessible
and extensible for future adaptation. The work contributes a structured,
tool-agnostic foundation for developers and researchers seeking to apply MBSE
practices in ROS-based projects.

</details>


### [16] [PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708)
*Liang Ma,Jiajun Wen,Min Lin,Rongtao Xu,Xiwen Liang,Bingqian Lin,Jun Ma,Yongxin Wang,Ziming Wei,Haokun Lin,Mingfei Han,Meng Cao,Bokui Chen,Ivan Laptev,Xiaodan Liang*

Main category: cs.RO

TL;DR: PhyBlock是一个用于评估视觉语言模型（VLMs）在物理理解和规划能力上的渐进式基准测试，通过3D积木组装任务和视觉问答任务，揭示了VLMs在高级规划和空间推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在理解物理现象和3D环境中的表现有限，需要一种系统化的评估方法来提升其在具身推理中的能力。

Method: PhyBench包含2600个任务（400个组装任务和2200个VQA任务），通过三个关键维度（部分完成、失败诊断和规划鲁棒性）评估21种先进VLMs。

Result: 实验表明，VLMs在高级规划和空间推理上表现不佳，任务复杂度增加时性能显著下降，且链式思维提示效果有限。

Conclusion: PhyBlock为提升VLMs在具身推理中的能力提供了统一测试平台，强调了其在现实物理问题解决中的潜力与挑战。

Abstract: While vision-language models (VLMs) have demonstrated promising capabilities
in reasoning and planning for embodied agents, their ability to comprehend
physical phenomena, particularly within structured 3D environments, remains
severely limited. To close this gap, we introduce PhyBlock, a progressive
benchmark designed to assess VLMs on physical understanding and planning
through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level
cognitive hierarchy assembly task alongside targeted Visual Question Answering
(VQA) samples, collectively aimed at evaluating progressive spatial reasoning
and fundamental physical comprehension, including object properties, spatial
relationships, and holistic scene understanding. PhyBlock includes 2600 block
tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three
key dimensions: partial completion, failure diagnosis, and planning robustness.
We benchmark 21 state-of-the-art VLMs, highlighting their strengths and
limitations in physically grounded, multi-step planning. Our empirical findings
indicate that the performance of VLMs exhibits pronounced limitations in
high-level planning and reasoning capabilities, leading to a notable decline in
performance for the growing complexity of the tasks. Error analysis reveals
persistent difficulties in spatial orientation and dependency reasoning.
Surprisingly, chain-of-thought prompting offers minimal improvements,
suggesting spatial tasks heavily rely on intuitive model comprehension. We
position PhyBlock as a unified testbed to advance embodied reasoning, bridging
vision-language understanding and real-world physical problem-solving.

</details>


### [17] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Main category: cs.RO

TL;DR: 论文提出了一种结合数据驱动学习与结构化推理的混合神经符号架构，以解决深度学习在动态环境中效率与可靠性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用需要适应性强、可解释且数据高效的学习方法，但现有深度学习在未知动态环境中表现不足。

Method: 结合可微分物理建模、贝叶斯推理和元学习，将物理符号推理嵌入神经网络模型。

Result: 提出了一种能超越训练数据泛化、适应新任务并持续扩展知识的机器人学习框架。

Conclusion: 混合神经符号架构是下一代自主系统的关键，并提供了研究路线图以加速其发展。

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [18] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.RO

TL;DR: 开发了一种基于摄像头和模仿学习的全自主假手控制系统，能够自动抓取和释放物体，减少用户的心理负担。


<details>
  <summary>Details</summary>
Motivation: 传统肌电信号控制方法对用户要求高，心理和生理负担大。本研究旨在提供一种更易用的假手控制接口。

Method: 通过摄像头采集数据，利用模仿学习训练模型，模仿人类动作实现自主控制。

Result: 模型仅需少量数据即可实现高成功率，并能泛化到不同用户和未见过的物体。

Conclusion: 该系统为肢体缺失者提供了一种简单、高效的假手控制方案，显著降低了使用难度。

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [19] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: FreqPolicy提出了一种基于频率一致性的流式视觉运动策略，通过约束频率域特征提升单步动作生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在机器人操作中因多步采样推理成本高而难以实时应用，且传统加速方法未考虑动作轨迹的时间依赖性。

Method: FreqPolicy通过频率一致性约束和自适应一致性损失，有效捕捉时间结构，实现高效单步动作生成。

Result: 在53个仿真任务和40个VLA模型任务中表现优异，推理频率达93.5Hz，且无性能损失。

Conclusion: FreqPolicy显著提升了生成模型在实时机器人操作中的适用性，同时保持高质量动作生成。

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


### [20] [MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains](https://arxiv.org/abs/2506.08840)
*Dewei Wang,Xinmiao Wang,Xinzhe Liu,Jiyuan Shi,Yingnan Zhao,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了一种新框架，通过混合潜在残差专家和多判别器训练RL策略，实现复杂地形下的可控仿人步态。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅适用于平坦地形且依赖本体感知，无法在复杂地形中实现仿人步态。

Method: 采用两阶段训练流程，先通过深度相机学习复杂地形行走，再实现步态切换，并设计步态奖励调整仿人行为。

Result: 仿真和实验表明，该框架在复杂地形中表现优异，并能无缝切换多种仿人步态。

Conclusion: 新框架显著提升了人形机器人在复杂地形中的仿人步态能力。

Abstract: Humanoid robots have demonstrated robust locomotion capabilities using
Reinforcement Learning (RL)-based approaches. Further, to obtain human-like
behaviors, existing methods integrate human motion-tracking or motion prior in
the RL framework. However, these methods are limited in flat terrains with
proprioception only, restricting their abilities to traverse challenging
terrains with human-like gaits. In this work, we propose a novel framework
using a mixture of latent residual experts with multi-discriminators to train
an RL policy, which is capable of traversing complex terrains in controllable
lifelike gaits with exteroception. Our two-stage training pipeline first
teaches the policy to traverse complex terrains using a depth camera, and then
enables gait-commanded switching between human-like gait patterns. We also
design gait rewards to adjust human-like behaviors like robot base height.
Simulation and real-world experiments demonstrate that our framework exhibits
exceptional performance in traversing complex terrains, and achieves seamless
transitions between multiple human-like gait patterns.

</details>


### [21] [Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization](https://arxiv.org/abs/2506.08851)
*Sepehr Samavi,Garvish Bhutani,Florian Shkurti,Angela P. Schoellig*

Main category: cs.RO

TL;DR: SICNav方法通过双层MPC框架将预测与规划结合，显式建模人机交互，解决了传统方法中忽视闭环交互的问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航方法将人类运动预测与机器人运动规划分离，忽略了人机闭环交互，导致机器人可能被困。

Method: 提出SICNav方法，采用双层MPC框架，将预测与规划结合为一个优化问题，显式建模多智能体交互。

Result: 在室内外环境中进行了近7公里的自主导航测试，系统运行初步分析显示效果良好。

Conclusion: SICNav方法通过显式建模交互，提升了机器人在拥挤环境中的导航安全性和效率。

Abstract: Safe and efficient navigation in crowded environments remains a critical
challenge for robots that provide a variety of service tasks such as food
delivery or autonomous wheelchair mobility. Classical robot crowd navigation
methods decouple human motion prediction from robot motion planning, which
neglects the closed-loop interactions between humans and robots. This lack of a
model for human reactions to the robot plan (e.g. moving out of the way) can
cause the robot to get stuck. Our proposed Safe and Interactive Crowd
Navigation (SICNav) method is a bilevel Model Predictive Control (MPC)
framework that combines prediction and planning into one optimization problem,
explicitly modeling interactions among agents. In this paper, we present a
systems overview of the crowd navigation platform we use to deploy SICNav in
previously unseen indoor and outdoor environments. We provide a preliminary
analysis of the system's operation over the course of nearly 7 km of autonomous
navigation over two hours in both indoor and outdoor environments.

</details>


### [22] [Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation](https://arxiv.org/abs/2506.08856)
*Jonathan P. King,Harnoor Ahluwalia,Michael Zhang,Nancy S. Pollard*

Main category: cs.RO

TL;DR: 本文提出了一种快速算法，用于计算全局最优的独立接触区域（ICRs），通过分治法实现实时规划。


<details>
  <summary>Details</summary>
Motivation: ICRs的位置为抓取和操作规划、学习及策略转移提供指导，但现有方法计算成本高，限制了应用。

Method: 基于增量n维Delaunay三角剖分的分治算法，支持平面接触的抓取。

Result: 实验显示，算法在速度和抓取质量上优于现有方法，速度提升100倍以上。

Conclusion: ICRs指导的策略具有鲁棒性，未来将扩展到3D实现，代码将开源以促进应用。

Abstract: This work presents a fast anytime algorithm for computing globally optimal
independent contact regions (ICRs). ICRs are regions such that one contact
within each region enables a valid grasp. Locations of ICRs can provide
guidance for grasp and manipulation planning, learning, and policy transfer.
However, ICRs for modern applications have been little explored, in part due to
the expense of computing them, as they have a search space exponential in the
number of contacts. We present a divide and conquer algorithm based on
incremental n-dimensional Delaunay triangulation that produces results with
bounded suboptimality in times sufficient for real-time planning. This paper
presents the base algorithm for grasps where contacts lie within a plane. Our
experiments show substantial benefits over competing grasp quality metrics and
speedups of 100X and more for competing approaches to computing ICRs. We
explore robustness of a policy guided by ICRs and outline a path to general 3D
implementation. Code will be released on publication to facilitate further
development and applications.

</details>


### [23] [MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation](https://arxiv.org/abs/2506.08868)
*Marco Ruggia*

Main category: cs.RO

TL;DR: MOMAV是一种全驱动、高度对称的多旋翼无人机，通过创新的设计和控制算法实现了高效飞行和精确控制。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够独立控制位置和方向的全驱动无人机，同时保持高飞行效率。

Method: 采用六旋臂八面体排列设计，每个旋臂可主动旋转，结合基于SQP的控制分配算法。

Result: 飞行测试显示，MOMAV在位置和方向控制上表现出色，误差分别为6.6mm/2.1°和11.8mm/3.3°。

Conclusion: MOMAV的设计和控制方法为全驱动无人机提供了高效且精确的解决方案。

Abstract: MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone
that is fully actuated, meaning it can control its orientation independently of
its position. MOMAV is also highly symmetrical, making its flight efficiency
largely unaffected by its current orientation. These characteristics are
achieved by a novel drone design where six rotor arms align with the vertices
of an octahedron, and where each arm can actively rotate along its long axis.
Various standout features of MOMAV are presented: The high flight efficiency
compared to arm configuration of other fully-actuated drones, the design of an
original rotating arm assembly featuring slip-rings used to enable continuous
arm rotation, and a novel control allocation algorithm based on sequential
quadratic programming (SQP) used to calculate throttle and arm-angle setpoints
in flight. Flight tests have shown that MOMAV is able to achieve remarkably low
mean position/orientation errors of 6.6mm, 2.1{\deg} ({\sigma}: 3.0mm,
1.0{\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\deg} ({\sigma}:
8.6mm, 2.0{\deg}) when sweeping orientation setpoints.

</details>


### [24] [CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks](https://arxiv.org/abs/2506.08931)
*Yixuan Li,Yutang Lin,Jieming Cui,Tengyu Liu,Wei Liang,Yixin Zhu,Siyuan Huang*

Main category: cs.RO

TL;DR: CLONE系统通过闭环误差校正实现高保真全身远程操作，解决了现有系统的协调性和漂移问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有远程操作系统在协调性和定位精度上的局限性，实现长时间精确的全身远程操作。

Method: 采用基于MoE的闭环误差校正系统，仅需MR头显的手部和头部追踪数据。

Result: CLONE系统实现了低漂移的长时间全身远程操作，支持复杂协调动作。

Conclusion: CLONE为长时间人机交互任务设定了新的里程碑。

Abstract: Humanoid teleoperation plays a vital role in demonstrating and collecting
data for complex humanoid-scene interactions. However, current teleoperation
systems face critical limitations: they decouple upper- and lower-body control
to maintain stability, restricting natural coordination, and operate open-loop
without real-time position feedback, leading to accumulated drift. The
fundamental challenge is achieving precise, coordinated whole-body
teleoperation over extended durations while maintaining accurate global
positioning. Here we show that an MoE-based teleoperation system, CLONE, with
closed-loop error correction enables unprecedented whole-body teleoperation
fidelity, maintaining minimal positional drift over long-range trajectories
using only head and hand tracking from an MR headset. Unlike previous methods
that either sacrifice coordination for stability or suffer from unbounded
drift, CLONE learns diverse motion skills while preventing tracking error
accumulation through real-time feedback, enabling complex coordinated movements
such as ``picking up objects from the ground.'' These results establish a new
milestone for whole-body humanoid teleoperation for long-horizon humanoid-scene
interaction tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: 提出了一种数据驱动的生物力学算法，结合人机交互机制，提升AR手术导航中变形建模的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统有限元方法在手术导航中计算成本高、无法处理大解剖变化的问题，同时提升AR叠加的可靠性。

Method: 采用数据驱动的生物力学算法，结合人机交互机制，允许外科医生动态修正解剖对齐。

Result: 算法平均目标配准误差为3.42 mm，结合交互机制后降至2.78 mm，优于现有方法。

Conclusion: 该框架实现了高效准确的变形建模，增强了外科医生与算法的协作，为更安全可靠的手术导航铺平了道路。

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [26] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.08052)
*Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: ReCogDrive通过结合视觉语言模型（VLM）和扩散规划器，解决端到端自动驾驶在长尾场景中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因领域差距、维度不匹配和模仿学习的局限性，难以处理罕见和长尾场景。

Method: 采用三阶段训练：1）用驾驶问答数据集训练VLM；2）扩散规划器进行模仿学习；3）强化学习微调。

Result: 在NAVSIM基准测试中，PDMS达到89.6，超越之前最佳方法5.6分。

Conclusion: ReCogDrive通过多阶段训练和强化学习，显著提升了自动驾驶的鲁棒性和安全性。

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [27] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe是一个用于评估文本到图像（T2I）系统文化代表性的新基准，通过分析系统对文本条件增加的响应来量化文化偏见。


<details>
  <summary>Details</summary>
Motivation: 现有T2I系统训练数据偏向欧美文化，忽视全球南方文化，需量化这种偏见。

Method: 利用Wikimedia知识图谱构建包含300个文化物品的层次化数据集，通过属性指定边际效用评估系统响应。

Result: CuRe评分与人类对感知相似性、图像-文本对齐和文化多样性的判断高度相关。

Conclusion: CuRe为T2I系统文化代表性提供了可扩展的评估工具，数据集和代码已开源。

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [28] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS是一个结合语义分割和图优化的框架，用于改进不完整的地面真实数据，显著提升运河和道路网络的映射准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的语义分割模型依赖高质量标注数据，但实际中地面真实数据常不完整或不准确，影响模型性能。利用基础设施网络的图属性（如可达性或连通性）可以改进这一问题。

Method: IGraSS结合了多模态（RGB、NDWI、DEM）的语义分割模块和图优化的地面真实数据细化模块，迭代优化结果。

Result: 实验显示，IGraSS将不可达运河段从18%降至3%，细化后的地面真实数据显著提升了运河识别性能。

Conclusion: IGraSS是一个通用框架，适用于改进噪声地面真实数据并高效映射基础设施网络，如运河和道路。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [29] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/abs/2506.08163)
*Harshvardhan Takawale,Nirupam Roy*

Main category: cs.CV

TL;DR: SpINRv2是一个基于神经网络的框架，用于使用FMCW雷达进行高保真体积重建，通过频域前向模型和隐式神经表示提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在高起始频率下相位混叠和子区间模糊问题，提升雷达体积重建的准确性。

Method: 引入完全可微分的频域前向模型，结合隐式神经表示，并加入稀疏性和平滑性正则化。

Result: SpINRv2显著优于传统和学习基线，特别是在高频条件下，为神经雷达3D成像设定了新标准。

Conclusion: SpINRv2通过频域建模和正则化技术，有效解决了高频率下的重建挑战，成为神经雷达3D成像的领先方法。

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [30] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 提出了一种基于离散扩散框架和视觉-语言-动作（VLA）流程的个性化外科医生指纹建模方法，通过多模态输入（如内窥镜视频、手术意图语言和隐私感知的身份嵌入）实现手势预测，并在JIGSAWS数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽视外科医生的个性化操作风格，而手术风格因训练、经验和行为差异而不同，因此需要一种能捕捉个性化信号的方法。

Method: 采用离散扩散框架与VLA流程结合，通过多模态输入（视频、语言、隐私嵌入）实现手势预测，并利用第三方语言模型编码个性化指纹。

Result: 方法能准确重建手势序列并学习到每位外科医生的独特运动指纹，但更个性化的嵌入会提高任务性能的同时增加身份泄露风险。

Conclusion: 个性化嵌入虽提升性能，但也增加隐私风险，需在手术建模中平衡个性化与隐私保护。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [31] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/abs/2506.08189)
*Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的开放世界场景图生成框架，利用预训练视觉语言模型实现零样本推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据集特定监督或微调，限制了在开放世界（新对象/关系）中的适用性。

Method: 结合多模态提示、嵌入对齐和轻量级配对优化策略，实现零样本结构化推理。

Result: 在Visual Genome等数据集上验证了预训练模型无需任务级训练即可实现关系理解。

Conclusion: 展示了预训练视觉语言模型在开放世界场景图生成中的潜力。

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [32] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/abs/2506.08191)
*Antoni Nowinowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: 本文扩展了DVP架构，使其能处理多物体场景，并通过改进训练方式和损失函数提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决DVP在处理多物体场景时的局限性，并改进训练效率。

Method: 扩展DVP架构，引入多物体处理能力，改进训练方式（如潜在空间损失函数），并设计新基准测试。

Result: 扩展后的DVP在重建质量和物体分解能力上优于基线模型（MONet和LIVE）。

Conclusion: 改进的DVP在多物体场景中表现优越，但可微分渲染在自编码器中仍有局限性。

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [33] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194)
*Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan*

Main category: cs.CV

TL;DR: GIQ是一个评估视觉和视觉语言基础模型几何推理能力的综合基准，揭示了当前模型在几何任务中的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标准基准上表现良好，但对几何属性的真实理解尚不明确，需要专门的评估工具。

Method: GIQ包含224种多面体的合成和真实图像，通过3D重建、对称检测、心理旋转和零样本分类任务系统评估模型。

Result: 当前模型在基础几何形状重建、详细几何区分和复杂多面体理解上表现不佳。

Conclusion: GIQ为几何智能的改进提供了结构化平台，推动未来几何感知表示学习的发展。

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [34] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究探讨了使用现代仅解码器LLM作为文本编码器在文本到图像扩散模型中的效果，发现多层归一化平均嵌入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型仍使用过时的T5和CLIP作为文本编码器，研究旨在评估现代LLM的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，分析不同LLM变体、嵌入提取方式及模型大小的影响。

Result: 多层归一化平均嵌入显著提升复杂提示的对齐效果，多数LLM表现优于T5基线。

Conclusion: 现代LLM作为文本编码器在文本到图像生成中表现更优，尤其在复杂视觉语言推理任务中。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [35] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: 提出了一种无需校准的跨受试者肌电图（EMG）模式识别方法，通过特征解耦实现跨受试者泛化。


<details>
  <summary>Details</summary>
Motivation: 跨受试者EMG模式识别因个体差异面临挑战，传统方法依赖特定校准数据，耗时且不实用。

Method: 采用端到端双分支对抗神经网络，将EMG特征解耦为模式特定和受试者特定成分。

Result: 实验表明，该方法在未见用户数据上表现优异，优于多种基线方法。

Conclusion: 为无需校准的跨受试者EMG模式识别提供了新视角，并展示了在任务无关生物识别系统中的潜力。

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [36] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/abs/2506.08214)
*Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar*

Main category: cs.CV

TL;DR: 论文提出了一种自监督训练方法，结合深度聚类和负采样，用于雷达卫星图像的水陆分割，无需人工标注，并通过集成模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要大量人工标注数据的高成本和低效率问题。

Method: 结合深度聚类和负采样进行自监督训练，并采用集成模型减少方差。

Result: 自监督集成模型在IOU指标上比全监督模型提升0.02。

Conclusion: 自监督方法在减少标注成本的同时，能够达到接近甚至优于全监督模型的性能。

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [37] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/abs/2506.08220)
*Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen*

Main category: cs.CV

TL;DR: 提出一种通过单目深度估计将2D关键点提升到3D空间的方法，学习稠密对应关系，无需显式3D监督或相机标注。


<details>
  <summary>Details</summary>
Motivation: 现有监督语义对应方法局限于稀疏标注的关键点，泛化能力不足。

Method: 利用单目深度估计构建连续3D规范流形，捕捉对象几何。

Result: 模型在未见关键点上显著优于监督基线，无监督基线在跨数据集泛化中表现更优。

Conclusion: 方法有效学习鲁棒对应关系，无监督方法在泛化中表现突出。

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [38] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/abs/2506.08227)
*Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge*

Main category: cs.CV

TL;DR: 论文分析了17个用于评估视觉语言模型组合理解能力的基准测试，揭示了其设计中的偏见，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估现有基准测试是否能有效衡量视觉语言模型的组合理解能力，并发现其潜在问题。

Method: 方法包括分析基准测试的设计选择（如数据来源和构建过程），并比较简单启发式方法与CLIP模型的性能。

Result: 结果显示基准测试存在分布不对称问题，导致其无法有效衡量组合理解能力。

Conclusion: 结论是提出改进基准测试设计的建议，以减少简单攻击的影响。

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [39] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D图像标记器通过高度压缩的一维序列表示图像，支持启发式标记操作实现图像编辑和生成，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 探索1D图像标记器在高度压缩下的表达能力，实现图像编辑和生成能力。

Method: 利用基于梯度的测试时优化和即插即用损失函数（如重建或CLIP相似性）构建图像生成流程。

Result: 在修复和文本引导图像编辑任务中生成多样且真实的样本。

Conclusion: 1D标记器的高压缩性和潜在空间表达能力为图像编辑和生成提供了新途径。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [40] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一个音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频，尤其擅长处理语音内容。


<details>
  <summary>Details</summary>
Motivation: 视频创作中音频与视觉的和谐整合至关重要，但现有方法要么忽略音频，要么局限于特定领域（如配音）。Mirage旨在填补这一空白，提供通用的音频到视频生成方案。

Method: Mirage采用基于自注意力的统一训练方法，支持从零开始训练或基于现有权重微调，无需依赖特定架构或损失函数。

Result: Mirage生成的视频在主观质量上优于其他方法，尤其在处理语音内容时表现突出。

Conclusion: Mirage为音频到视频生成提供了通用且高质量的解决方案，尤其在语音驱动视频生成方面具有显著优势。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [41] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEMA的新型注意力机制，解决了传统注意力在计算复杂度和聚焦能力上的问题，并在图像分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制（如全注意力和线性注意力）在计算复杂度和聚焦能力上存在不足，限制了其在计算机视觉任务中的应用。

Method: 提出SEMA，结合了Mamba形式的注意力机制，利用令牌定位避免分散，并通过算术平均捕捉全局注意力。

Result: 在Imagenet-1k上，SEMA在图像分类任务中表现优于线性注意力和近期视觉Mamba模型。

Conclusion: SEMA是一种可扩展且高效的注意力替代方案，适用于大规模图像任务。

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [42] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/abs/2506.08299)
*Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng*

Main category: cs.CV

TL;DR: 提出了一种新的反射数据集收集方法，构建了高质量、对齐且多样化的OpenRR-1k数据集，验证了其在真实场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有反射去除技术因缺乏高质量真实数据集而受限，需一种便捷、经济且可扩展的数据收集方法。

Method: 提出新范式收集反射数据集，确保数据高质量、对齐且多样化，构建OpenRR-1k数据集。

Result: 收集了1,000对高质量传输-反射图像对，验证了数据集在提升反射去除方法鲁棒性方面的有效性。

Conclusion: OpenRR-1k数据集为反射去除技术提供了高质量真实数据支持，推动了该领域的发展。

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [43] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/abs/2506.08324)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: STNet通过创新的空间-光谱Transformer模块，有效解决了高光谱图像分类中的高维数据和过拟合问题，提升了特征提取和融合能力。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、地物稀疏分布和光谱冗余等挑战，导致过拟合和泛化能力受限。

Method: 提出STNet网络架构，采用空间-光谱Transformer模块，通过解耦空间和光谱注意力及双门控机制，实现高效特征提取和融合。

Result: 在IN、UP和KSC数据集上表现优于主流方法，减少了小样本和高噪声场景的过拟合风险。

Conclusion: STNet在不增加网络深度或宽度的情况下提升了模型表示能力，为高光谱图像分类提供了新思路。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [44] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/abs/2506.08327)
*Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto*

Main category: cs.CV

TL;DR: 提出了一种使用事件相机实时定位网球拍击球点的方法，解决了高速相机内存消耗大和手动数字化耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 传统高速相机内存消耗大且手动处理耗时，限制了长时间场景捕捉和性能分析。

Method: 通过事件相机高效捕捉亮度变化，结合计算机视觉技术和原创事件处理（PATS）检测击球时间。

Result: 实验结果表明测量精度在允许范围内，计算时间满足实时应用需求。

Conclusion: 该方法为网球运动员性能分析提供了高效、实时的解决方案。

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [45] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出了一种名为Step AG的自适应引导策略，通过限制分类器自由引导在前几步去噪中，实现了20%到30%的速度提升，同时保持图像质量和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导方法在文本到视觉生成扩散模型中成本较高，且现有自适应引导方法缺乏分析和实证支持，无法通用。

Method: 提出Step AG策略，限制分类器自由引导在前几步去噪中，以减少计算成本。

Result: 实验表明，该方法在图像质量、文本对齐和速度上均有显著提升，适用于不同模型和设置。

Conclusion: Step AG是一种简单且通用的自适应引导策略，能有效提升效率而不牺牲生成质量。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [46] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/abs/2506.08356)
*Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE提出了一种动态适应医学影像模态的视觉-语言处理框架，通过多尺度特征提取和模态专家分支提升诊断上下文对齐。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言框架采用统一特征提取策略，忽视了不同模态的特定需求，导致诊断信息提取不足。

Method: MedMoE基于Swin Transformer提取多尺度特征，通过条件MoE模块动态路由到模态专家分支，实现空间自适应注意力。

Result: 实验表明，MedMoE在多种医学基准测试中提升了视觉-语言对齐和检索性能。

Conclusion: MedMoE证明了模态专用视觉表征在临床视觉-语言系统中的重要性。

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [47] [Image Demoiréing Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/abs/2506.08361)
*Yanting Mei,Zhilu Zhang,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 论文提出了一种利用双摄像头融合（DCID）去除图像摩尔纹的方法，通过超广角（UW）图像辅助广角（W）图像去摩尔纹，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代智能手机通常配备双摄像头，且超广角图像在广角图像存在摩尔纹时能提供正常颜色和纹理。

Method: 提出轻量级UW图像编码器，集成到现有去摩尔纹网络中，并采用快速两阶段图像对齐方式。

Result: 在包含约9,000样本的真实数据集上，方法表现优于现有技术。

Conclusion: DCID方法有效解决了大范围摩尔纹问题，代码和数据集已开源。

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [48] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391)
*Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: SECOND通过选择性对比解码减少视觉语言模型中的物体幻觉，利用多尺度视觉信息提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因物体幻觉问题导致性能受限，需更接近人类视觉感知的方法。

Method: 提出SECOND方法，逐步选择和整合多尺度视觉信息，并通过对比减少幻觉。

Result: SECOND显著减少幻觉，在多个基准测试中表现优异。

Conclusion: 多尺度视觉信息的优先对比应用在视觉语言模型中具有巨大潜力。

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [49] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/abs/2506.08418)
*Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为RadioDUN的方法，通过稀疏信号恢复和物理传播模型结合，解决了密集无线电地图构建的难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以结合无线电地图的物理特性，导致稀疏样本下构建密集无线电地图效果不佳。

Method: 将无线电地图估计建模为稀疏信号恢复问题，结合物理传播模型分解优化子问题，提出RadioDUN网络动态调整参数。

Result: 实验表明，RadioDUN优于现有方法。

Conclusion: RadioDUN通过结合物理特性和动态优化，显著提升了无线电地图估计性能。

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [50] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/abs/2506.08429)
*Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei*

Main category: cs.CV

TL;DR: SCALE提出了一种基于数据质量驱动的VLM指令调优数据集选择方法，通过跨模态评估框架解决图像与文本对齐噪声和文本模糊问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLM的性能依赖于大规模高质量数据集，但存在图像与文本对齐噪声和文本模糊的问题，影响模型效果。

Method: SCALE通过跨模态评估框架，为数据条目分配任务、生成多类型描述，并评估对齐、清晰度等指标。

Result: 研究发现单模态质量评估方法可能低估特定任务所需样本，而适当生成的图像描述可将多模态任务转化为统一文本模态。

Conclusion: SCALE为VLM指令调优数据集的选择提供了高效解决方案，提升了模型鲁棒性和任务适应性。

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [51] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 论文提出了一种自适应低通引导（ALG）方法，解决了图像到视频（I2V）生成中动态性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的I2V生成方法在微调预训练T2V模型时，常导致生成的视频动态性不足，作者发现这是由于高频细节过早暴露导致的采样偏差。

Method: 提出ALG方法，通过在去噪早期阶段自适应地调制输入图像的高频内容，以改善视频的动态性。

Result: 实验表明，ALG显著提升了生成视频的动态性（VBench-I2V测试中动态度平均提升36%），同时保持了图像质量和文本对齐。

Conclusion: ALG是一种简单有效的解决方案，能够在不牺牲图像质量的前提下提升I2V生成的动态性。

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [52] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/abs/2506.08470)
*Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MARMOT的掩码自编码器，用于建模瞬态成像，并通过自监督预训练在大量多样化的非视距（NLOS）瞬态数据集上学习特征，以支持NLOS应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在NLOS场景中主要优化体积密度或表面重建隐藏对象，而未利用从数据集中学习的先验知识。MARMOT旨在通过预训练模型填补这一空白。

Method: MARMOT采用基于Transformer的编码器-解码器结构，通过扫描模式掩码（SPM）从部分掩码的瞬态数据中学习特征，并预测完整测量结果。

Result: 在TransVerse数据集（包含50万3D模型的合成瞬态数据集）上预训练后，MARMOT通过直接特征转移或解码器微调适应下游任务，实验证明其优于现有方法。

Conclusion: MARMOT在NLOS瞬态成像任务中表现出高效性，为相关应用提供了新的解决方案。

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [53] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出了一种通用的上下文感知对比学习框架（UniCaCLF），用于解决多媒体取证领域中的时间伪造定位（TFL）问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将深度伪造检测视为分类任务，忽略了视频部分片段被篡改的情况，而时间伪造定位更符合实际应用需求。

Method: 采用监督对比学习，通过异常检测识别伪造片段，并引入上下文感知感知层和自适应上下文更新器，增强伪造与真实片段特征的区分度。

Result: 在五个公开数据集上的实验表明，UniCaCLF显著优于现有算法。

Conclusion: UniCaCLF为时间伪造定位提供了一种高效解决方案，具有实际应用潜力。

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [54] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: MLVTG框架通过MambaAligner和LLMRefiner模块，解决了视频时序定位中的冗余注意力和多模态对齐问题，实现了更精确的定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法存在冗余注意力和多模态对齐不足的问题，影响了视频时序定位的精度。

Method: MLVTG采用MambaAligner（基于Vision Mamba块）建模时序依赖，LLMRefiner利用预训练LLM的冻结层增强语义对齐。

Result: 在QVHighlights、Charades-STA和TVSum数据集上，MLVTG表现优于现有基线，达到SOTA性能。

Conclusion: MLVTG通过双对齐策略（时序建模和语义净化）显著提升了视频时序定位的精度。

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [55] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/abs/2506.08526)
*Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun*

Main category: cs.CV

TL;DR: 提出了一种结合多尺度特征学习和语义场景理解的框架，用于动态环境中的视觉定位，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态环境中光照、天气和移动物体等因素干扰视觉定位，现有绝对位姿回归方法难以保持一致性。

Method: 采用分层Transformer和跨尺度注意力融合几何细节与上下文信息，结合语义监督训练网络。

Result: 在TartanAir数据集上，该方法在动态物体、光照变化和遮挡等挑战性场景中表现优于现有位姿回归方法。

Conclusion: 多尺度处理与语义引导的结合为动态环境中的鲁棒视觉定位提供了有效策略。

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [56] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR是一种高效的视频超分辨率框架，通过结合动态时间注意力和注意力记忆缓存，显著提升了时间一致性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间一致性和计算成本方面存在不足，尤其是在长视频处理中。

Method: 提出混合时间建模机制，包括动态时间注意力（DTA）和注意力记忆缓存（AMC），并引入非对称采样策略。

Result: 在多个VSR基准测试中表现优异，计算成本显著降低。

Conclusion: LiftVSR在性能和效率上均达到先进水平，适用于长视频处理。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [57] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow是一种基于流匹配的运动预测框架，通过单次推理预测多模态轨迹，显著提升效率，同时通过排名损失和自条件训练技术优化性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中高效准确的运动预测对安全至关重要，现有生成方法在多模态预测时效率不足。

Method: 提出TrajFlow框架，结合流匹配和Plackett-Luce分布排名损失，并采用自条件训练技术。

Result: 在Waymo Open Motion Dataset上实现最优性能，显著降低计算开销。

Conclusion: TrajFlow为安全关键型自动驾驶应用提供了高效、准确的解决方案。

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [58] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/abs/2506.08543)
*Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li*

Main category: cs.CV

TL;DR: 论文提出输入空间线性假设（ISLH），并通过谱主路径（SPP）框架验证深度网络中线性表示的形成，增强AI的鲁棒性和透明度。


<details>
  <summary>Details</summary>
Motivation: 基于线性表示假设（LRH），探索概念对齐方向在输入空间的起源及其在深度网络中的选择性放大。

Method: 提出谱主路径（SPP）框架，分析深度网络中线性表示的逐步提炼过程，并在视觉语言模型（VLM）中验证其多模态鲁棒性。

Result: 验证了ISLH假设，展示了SPP框架在深度网络中线性表示形成的有效性及其多模态鲁棒性。

Conclusion: 该研究为深度网络中表示形成的结构化理论提供了支持，有助于提升AI的鲁棒性、公平性和透明度。

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [59] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/abs/2506.08553)
*Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneNet和KnowledgeNet是用于HD-EPIC VQA挑战赛的方法，分别利用场景图和外部常识知识提升视觉问答性能，组合后达到44.21%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂的第一人称视觉问答任务，通过捕捉细粒度对象交互和引入高层语义连接。

Method: SceneNet使用多模态大语言模型生成场景图；KnowledgeNet整合ConceptNet的常识知识。

Result: 在HD-EPIC基准测试中，组合方法达到44.21%的准确率。

Conclusion: 两种方法的结合有效提升了复杂视觉问答任务的性能。

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [60] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.08979)
*Longyu Yang,Ping Hu,Lu Zhang,Jun Liu,Yap-Peng Tan,Heng Tao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级模块化框架，提升LiDAR分割在恶劣天气下的泛化性能，通过几何异常抑制和反射率校准模块，显著改进现有模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于范围视图的LiDAR分割方法在恶劣天气下泛化性能不足，限制了其在实际环境中的可靠性。

Method: 提出双分支框架，分别处理几何属性和反射强度，引入GAS模块抑制天气引起的空间噪声，RDC模块校准反射失真。

Result: 在不同基准和基线模型上的实验表明，该方法显著提升了恶劣天气下的泛化性能，且推理开销极小。

Conclusion: 该框架为实际LiDAR分割提供了一种实用且高效的解决方案。

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [61] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/abs/2506.08562)
*Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为Hier-DETR的新框架，用于增量目标检测（IOD），通过利用神经崩溃和类标签的层次关系，兼顾效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中新物体不断出现，需要检测模型持续学习且避免灾难性遗忘，现有IOD模型性能有限且推理时间长。

Method: Hier-DETR框架结合神经崩溃处理数据不平衡和类标签的层次关系。

Result: 该框架在效率和性能上均表现出竞争力。

Conclusion: Hier-DETR为解决IOD问题提供了一种高效且性能优越的解决方案。

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [62] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/abs/2506.08997)
*Fabian Immel,Jan-Hendrik Pauls,Richard Fehler,Frank Bieder,Jonas Merkert,Christoph Stiller*

Main category: cs.CV

TL;DR: SDTagNet是一种在线高精地图构建方法，利用标准地图（如OpenStreetMap）提升远距离检测精度，结合语义信息和文本标注，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 高精地图维护成本高，而在线构建方法受限于传感器感知范围，标准地图（SD）作为先验信息可解决这一问题。

Method: SDTagNet引入语义信息和文本标注，结合点级SD地图编码器和正交元素标识符，统一整合各类地图元素。

Result: 在Argoverse 2和nuScenes数据集上，性能提升显著，最高达+5.9 mAP（45%）。

Conclusion: SDTagNet通过充分利用SD地图信息，显著提升了在线高精地图构建的远距离检测精度和性能。

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [63] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/abs/2506.08566)
*Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin*

Main category: cs.CV

TL;DR: 论文提出FCA-NIG框架，自动生成具有细粒度跨模态标注的导航指令，构建FCA-R2R数据集，显著提升VLN智能体的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏细粒度的跨模态对齐标注（如子指令级和实体级），限制了导航决策的准确性。

Method: FCA-NIG框架通过分割轨迹、检测地标、生成指令和选择实体，自动生成子指令-轨迹对和实体-地标标注，最终形成完整指令-轨迹对。

Result: 实验表明，FCA-R2R数据集显著提升了多个VLN智能体的性能，增强了状态感知和导航泛化能力。

Conclusion: FCA-NIG无需人工标注即可生成高质量训练数据，推动了复杂导航任务中的细粒度跨模态学习。

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [64] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 论文提出了一种多样性引导的MLP压缩方法（DGMR），显著减少大型视觉Transformer的参数，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型Transformer模型参数过多导致计算和内存成本高昂，研究发现MLP模块占用了大部分参数。

Method: 采用Gram-Schmidt权重剪枝策略消除MLP隐藏层的冗余神经元，同时保留权重多样性以在蒸馏过程中恢复性能。

Result: 实验表明，DGMR在多个大型视觉Transformer上实现了57%以上的参数和FLOPs减少，且性能几乎无损。

Conclusion: DGMR方法高效压缩模型，显著降低资源需求，适用于大规模视觉Transformer。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [65] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: 本文是第一篇专注于基于Transformer的高光谱图像分类的端到端综述，总结了300多篇论文，分析了其流程中的关键设计选择，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在长距离依赖学习中表现出色，但在高光谱成像（HSI）中的应用仍处于起步阶段，需要系统性的总结和指导。

Method: 研究通过分析300多篇论文，对HSI分类流程的每个阶段（如预处理、特征提取、注意力机制等）进行分类和对比，并评估其设计选择。

Result: 研究总结了HSI领域的进展与挑战，包括数据稀缺、计算开销大等问题，并提出了未来的研究重点。

Conclusion: 本文旨在为研究者提供指导，帮助选择或扩展适合下一代HSI应用的Transformer组件。

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [66] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/abs/2506.08611)
*Shiji Zhao,Chi Chen,Ranjie Duan,Xizhe Wang,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出了一种名为ABSLD的方法，通过调整教师模型软标签的平滑度，解决了对抗训练中的鲁棒公平性问题。


<details>
  <summary>Details</summary>
Motivation: 对抗训练（AT）和对抗鲁棒性蒸馏（ARD）存在鲁棒公平性问题，即模型对某些类别（易类别）表现强鲁棒性，而对其他类别（难类别）表现弱鲁棒性。本文旨在探索这一问题的原因并提出解决方案。

Method: 提出Anti-Bias Soft Label Distillation（ABSLD），通过为不同类别分配不同温度值来调整软标签的平滑度，从而减少学生模型在不同类别间的误差风险差距。

Result: 实验表明，ABSLD在鲁棒性和公平性的综合性能上优于现有方法。

Conclusion: ABSLD是一种高效且适应性强的解决方案，能够显著提升对抗鲁棒性的公平性。

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [67] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/abs/2506.08612)
*Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis Tömen,Hadi Jamali-Rad,Jan van Gemert*

Main category: cs.CV

TL;DR: 论文探讨了在数据不足情况下提升深度学习模型性能的方法，通过组织数据受限挑战赛，发现模型集成和数据增强是关键。


<details>
  <summary>Details</summary>
Motivation: 解决数据不足时深度学习模型性能下降的问题，探索如何通过先验知识提升数据效率。

Method: 组织数据受限挑战赛，限制参与者从头训练模型且禁止迁移学习，评估不同方法的效果。

Result: 成功案例中，模型集成（Transformer与CNN结合）和大量数据增强表现突出，部分方法利用先验知识取得成效。

Conclusion: 通过先验知识和创新方法，可以在数据不足时提升深度学习模型的性能。

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [68] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/abs/2506.08613)
*Joost van Dalen,Yuki M. Asano,Marc Russwurm*

Main category: cs.CV

TL;DR: SAMSelect是一种算法，用于从多光谱图像中选择最佳三通道可视化组合，以帮助海洋科学家更直观地识别海洋漂浮物。


<details>
  <summary>Details</summary>
Motivation: 海洋漂浮物在中分辨率图像中因成分异质性难以可视化，而专家通常依赖经验和启发式方法选择波段或光谱指数。SAMSelect旨在通过自动化选择提高可视化效果和分类准确性。

Method: SAMSelect利用Segment Anything Model在小标注数据集上选择分类准确性最高的波段或指数组合，假设最佳分割结果也能提供良好的视觉信息。

Result: 在加纳阿克拉和南非德班的Sentinel-2场景中测试，SAMSelect发现了新的未使用波段组合（如B8和B2的归一化差异指数），性能优于文献中的指数。

Conclusion: SAMSelect为海洋领域的视觉解译提供了有效工具，并开源了代码库，便于领域科学家使用。

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [69] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的NeRF方法，通过目标采样和新的表面重建损失，提升了3D重建和图像渲染的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因可扩展性问题无法训练所有可能的输入数据，导致渲染效果受限。

Method: 利用前景场景的隐式表面表示，在3D图像投影空间中建模概率密度函数，实现目标采样；提出新的表面重建损失。

Result: 结合新采样策略和损失函数，显著提升了3D重建和图像渲染的准确性，尤其是对感兴趣区域。

Conclusion: 该方法通过改进采样和损失函数，为NeRF提供了更高效的训练和更优的渲染效果。

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [70] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级高效CNN-Mamba网络（ECMNet），用于语义分割，结合CNN和Mamba的优势，通过增强的双注意力块和多尺度注意力单元提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在语义分割中全局上下文建模不足，Mamba在视觉任务中表现出长距离依赖建模的潜力。

Method: 设计了增强双注意力块（EDAB）和多尺度注意力单元（MSAU），并引入Mamba增强的特征融合模块（FFM）。

Result: 在Cityscapes和CamVid数据集上分别达到70.6%和73.6%的mIoU，参数和计算量较低。

Conclusion: ECMNet在准确性和效率上取得了平衡，适用于语义分割任务。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [71] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap提出了一种基于GAN和扩散模型的视频编辑框架，用于在未配对数据中替换机器人手臂，解决了跨平台机器人学习中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 解决视频条件下机器人学习中数据稀缺和跨平台泛化能力不足的问题。

Method: 结合GAN和扩散模型，先分割机器人手臂并训练GAN进行翻译，再用扩散模型增强视频的连贯性和运动真实性。

Result: 在三个基准测试中，RoboSwap在结构连贯性和运动一致性上优于现有方法。

Conclusion: RoboSwap为机器人学习提供了可靠的跨平台数据生成解决方案。

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [72] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/abs/2506.08635)
*Siddhant Ranade,Gonçalo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam*

Main category: cs.CV

TL;DR: 提出了一种基于隐式表示的快速、准确的无组织点云表面重建算法，通过懒查询、并行多尺度网格表示和跨尺度注意力机制，实现了速度与精度的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法要么需要针对单个对象训练的小模型（细节丰富但泛化性差），要么是泛化性强的大模型（细节不足且推理慢），因此需要一种兼顾速度与精度的新方法。

Method: 1. 懒查询：早期阶段无需使用查询点；2. 并行多尺度网格表示：适应不同噪声和分辨率；3. 跨尺度注意力机制：提升重建效果。

Result: 新方法在速度上优于所有基线，性能仅略低于最先进方法，实现了最佳的速度-精度平衡。

Conclusion: 通过三项关键贡献，新方法在隐式表示中实现了快速且高精度的表面重建。

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [73] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 论文提出了一种方向对齐的3D物体生成方法，通过构建数据集Objaverse-OA并微调现有模型，解决了现有3D生成模型因数据不一致导致的姿态对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型因训练数据不一致导致生成物体姿态不统一，限制了其在下游任务中的应用。

Method: 构建Objaverse-OA数据集（14,832个方向对齐的3D模型），并基于多视角扩散和3D变分自编码器框架微调模型。

Result: 实验表明，该方法优于后处理对齐方法，并能推广到未见过的物体类别。

Conclusion: 方向对齐的3D物体生成方法有效，支持零样本姿态估计和高效物体旋转操作等下游应用。

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [74] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/abs/2506.08649)
*Zhiyi Zhu,Xiaoyu Wu,Youwei Lu*

Main category: cs.CV

TL;DR: 论文提出了一种新的多模态视频记忆性预测模型TMCCL，通过文本-运动跨模态对比损失增强运动特征表示，并在两个数据集上取得最优性能。同时，提出了基于记忆性预测的视频摘要方法MWCVS，减少摘要标注的主观性。


<details>
  <summary>Details</summary>
Motivation: 现有视频记忆性预测模型未能充分利用运动线索，且运动特征表示在微调阶段因缺乏标注数据而受限。

Method: 引入TMCCL模型，利用文本描述相似性构建正负运动样本集，提升运动特征表示。提出MWCVS方法，将记忆性预测应用于视频摘要。

Result: TMCCL在两个视频记忆性预测数据集上达到最优性能；MWCVS在两个视频摘要数据集上验证了其有效性。

Conclusion: TMCCL和MWCVS展示了视频记忆性预测的潜力，为多模态学习和视频摘要提供了新思路。

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [75] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/abs/2506.08650)
*Peter Grönquist,Stepan Tulyakov,Dengxin Dai*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的神经物理模型（NPM），用于解决多相机间颜色一致性问题，适应性强且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 多相机系统中因传感器和光学差异导致的颜色不一致问题，限制了图像融合和ISP兼容性。现有方法在适应性、计算成本或实际需求上存在不足。

Method: 提出NPM模型，通过物理模拟估计设备间的转换关系，支持不同光照条件下的自适应，并可基于物理测量初始化或非配对数据训练。

Result: 在NUS和BeyondRGB等公开数据集上，NPM表现优于现有方法，实现了跨传感器和光学系统的稳健颜色一致性。

Conclusion: NPM为多相机系统的颜色一致性提供了一种高效、适应性强的解决方案。

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [76] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/abs/2506.08666)
*Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu*

Main category: cs.CV

TL;DR: LLaVA-c通过光谱感知巩固和无监督查询正则化改进LLaVA-1.5，解决多任务学习中的任务平衡和基础模型退化问题，持续学习表现优于多任务联合学习。


<details>
  <summary>Details</summary>
Motivation: 多任务学习存在任务平衡和扩展成本问题，持续学习虽能增量获取知识但忽视基础模型退化。

Method: 在LLaVA-1.5上引入光谱感知巩固和无监督查询正则化。

Result: LLaVA-c在持续预训练和微调中提升基准性能并保留通用能力。

Conclusion: 首次证明任务级持续学习可匹配或超越多任务联合学习。

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [77] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/abs/2506.08678)
*Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为ATAS的自蒸馏方法，旨在同时提升CLIP模型的语义一致性和细粒度对齐能力，无需额外模块或有监督微调。


<details>
  <summary>Details</summary>
Motivation: CLIP在细粒度、区域级理解上表现不足，限制了其在密集预测任务中的效果。

Method: 通过自蒸馏过程利用模型自身知识，在无标签图像上优化CLIP视觉编码器的表示。

Result: 在开放词汇对象检测和语义分割任务中，ATAS显著优于基线CLIP模型。

Conclusion: ATAS验证了同时保持语义一致性和细粒度对齐对开放词汇密集预测任务的重要性。

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [78] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/abs/2506.08690)
*Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia*

Main category: cs.CV

TL;DR: 加拿大2023年经历了严重的野火季节，研究提出高分辨率野火预测方法，利用多模态数据和深度学习模型，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致野火季节延长和加剧，需为北方社区提供更好的野火管理工具。

Method: 利用高分辨率卫星数据（Sentinel-2 L1C）、中分辨率卫星产品（MODIS）和环境数据（ERA5），结合两种深度学习架构，开发高分辨率野火预测模型。

Result: 多模态输入优于单模态输入，在2023年野火季节的F1分数达到60.3%。

Conclusion: 多模态深度学习模型在高分辨率和大陆尺度野火预测中具有潜力。

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [79] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/abs/2506.08691)
*Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu*

Main category: cs.CV

TL;DR: VReST是一种无需训练的增强视觉语言模型推理能力的方法，通过蒙特卡洛树搜索和自奖励机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现优异，但在复杂视觉推理中仍有局限，尤其是使用链式思维提示技术时。

Method: 提出VReST方法，利用蒙特卡洛树搜索构建推理路径，并通过自奖励机制评估推理步骤质量。

Result: 在三个多模态数学推理基准测试中表现最优，验证了测试时间扩展定律在多模态任务中的有效性。

Conclusion: VReST为未来研究提供了新方向，展示了无需训练即可提升推理能力的潜力。

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [80] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/abs/2506.08694)
*Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano*

Main category: cs.CV

TL;DR: 提出了一种基于运动引导的自监督学习框架，通过聚类密集点轨迹学习时空一致的表示，提升了动态场景和遮挡场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态增强，难以处理物体变形、遮挡和相机运动，导致特征学习不一致。

Method: 利用现成的点跟踪器提取长程运动轨迹，通过动量编码器的最优传输机制优化特征聚类，并沿跟踪点传播聚类分配以确保时间一致性。

Result: 在六个图像和视频数据集及四个评估基准上，性能提升了1%至6%。

Conclusion: 通过运动作为隐式监督信号，该方法学习到的表示具有跨帧泛化能力，并在动态场景中表现更优。

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [81] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/abs/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: 本文提出了一种针对无色点云的快速检测和5自由度姿态估计网络，通过神经网络预测物体的中心和顶部点来计算姿态。


<details>
  <summary>Details</summary>
Motivation: 无色点云的姿态估计在计算机视觉中具有挑战性，现有方法性能有限，因此需要一种快速且高效的方法。

Method: 使用合成数据训练神经网络，预测物体的中心和顶部点，从而实现5自由度姿态估计。

Result: 在基准数据集上表现优于所有无色方法，推理时间仅需250毫秒。

Conclusion: 该方法在无色点云姿态估计中达到了最先进水平，适用于多种实际场景。

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [82] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出TraGraph-GS方法，通过轨迹图实现大规模场景的高精度渲染，解决了现有方法在相机轨迹适应性和纹理细节失真上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大型场景的新视角合成中存在相机轨迹适应性差和纹理细节失真的问题。

Method: 提出基于图的场景分区方法，结合正则化约束和渐进渲染策略，减少高斯重叠导致的失真。

Result: 在四个航空和四个地面数据集上表现优异，PSNR平均提升1.86 dB（航空）和1.62 dB（地面）。

Conclusion: TraGraph-GS显著提升了大规模场景的渲染质量和效率。

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [83] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 论文提出了首个大规模基准测试，评估三种3D高斯溅射方法在3D空间中的表现，并引入GaussianWorld-49K数据集以支持泛化方法的研究。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯溅射方法主要在2D视图上评估，缺乏对整体3D理解的深入分析，因此需要系统化的3D基准测试。

Method: 提出大规模基准测试，评估三类方法（基于场景优化、无场景优化、泛化方法）在1060个场景中的表现，并引入49K场景的数据集。

Result: 泛化方法在放松场景限制、快速推理和分割性能上表现优越。

Conclusion: 泛化方法在3D场景理解中具有潜力，公开的代码和数据集将加速相关研究。

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [84] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 该论文提出了一种基于SE(3)-对称变换器模型的方法，用于预测腹主动脉瘤（AAA）的生长，通过保留血管表面的解剖结构和几何保真度，实现个性化监测策略。


<details>
  <summary>Details</summary>
Motivation: 当前AAA监测基于最大直径，忽略了3D形状与生长的复杂关系，可能导致标准化监测间隔不适用。个性化生长预测可优化监测策略。

Method: 使用SE(3)-对称变换器模型，直接在血管模型表面结合局部多物理特征预测AAA生长。训练数据为24名患者的113次CTA扫描。

Result: 模型预测AAA生长的中位直径误差为1.18毫米，并能以93%的准确率识别患者是否在两年内需手术修复。外部验证集结果也验证了模型的泛化能力。

Conclusion: 局部定向AAA生长预测可行，可为个性化监测策略提供支持。

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [85] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/abs/2506.08735)
*Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba提出了一种新的主干架构，通过正交带卷积和Mamba模块改进空间建模和全局上下文建模，在分类和下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决InceptionNeXt在空间依赖性和全局上下文建模上的局限性。

Method: 用正交带卷积替换传统一维条带卷积，并引入瓶颈Mamba模块增强全局建模。

Result: 在分类和下游任务中达到SOTA性能，参数和计算效率优越。

Conclusion: InceptionMamba通过改进的空间和全局建模方法，显著提升了性能。

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [86] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772)
*Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng*

Main category: cs.CV

TL;DR: 论文提出了一种名为RS-MTDF的半监督语义分割框架，利用预训练的视觉基础模型（VFMs）作为多教师，通过特征级蒸馏和知识融合提升遥感图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义分割依赖大量标注数据，但标注成本高昂。半监督方法虽能缓解数据依赖，但现有方法因标记与未标记数据分布不匹配导致泛化能力不足。

Method: 提出RS-MTDF框架，利用冻结的VFMs（如DINOv2和CLIP）作为教师模型，通过特征级蒸馏对齐学生模型特征，并将知识融合到解码器中。

Result: 在ISPRS Potsdam、LoveDA和DeepGlobe数据集上表现优异，尤其在LoveDA上超越现有方法，多数语义类别IoU最高。

Conclusion: 多教师VFM指导显著提升了遥感分割的泛化能力和语义理解，各模块贡献通过消融实验验证。

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [87] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: Gaussian2Scene是一种基于3D高斯泼溅的自监督学习框架，通过两阶段训练策略提升点云预训练的效果，解决了现有方法在计算和几何结构捕捉上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖隐式场景表示和高内存需求，且难以捕捉3D几何结构。

Method: 采用3D高斯泼溅（3DGS）进行预训练，分两阶段：双分支掩码自编码器学习2D和3D表示，随后通过高斯基元和RGB图像监督学习。

Result: 在多个3D目标检测任务中表现优于现有预训练方法。

Conclusion: Gaussian2Scene通过显式3D重建和高效计算，显著提升了自监督学习在点云预训练中的效果。

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [88] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: 本文介绍了Landsat-Bench，一套基于Landsat影像的基准测试，用于评估地理空间基础模型（GFM），并证明SSL4EO-L预训练的GFM优于ImageNet。


<details>
  <summary>Details</summary>
Motivation: Landsat数据缺乏基准测试，限制了基于Landsat的地理空间基础模型的发展。

Method: 提出Landsat-Bench，包含三个基准测试（EuroSAT-L、BigEarthNet-L、LC100-L），并采用标准化评估方法。

Result: SSL4EO-L预训练的GFM在下游任务中表现优于ImageNet，性能提升分别为+4% OA和+5.1% mAP。

Conclusion: Landsat-Bench为Landsat数据的模型评估提供了标准化工具，并验证了SSL4EO-L预训练的有效性。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [89] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/abs/2506.08784)
*Jongyub Seok,Chanjin Kang*

Main category: cs.CV

TL;DR: 提出了一种基于ImageNet预训练网络的新方法HomographyAD，用于解决现有异常检测方法在真实工业环境中性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法仅适用于完全对齐的数据集，而真实工业环境数据往往未对齐，限制了其应用。

Method: 通过深度单应性估计对齐输入前景，利用自单应性学习微调模型，提取正常样本特征并检测异常。

Result: 实验表明，该方法能显著提升现有异常检测方法的性能。

Conclusion: HomographyAD为工业环境中的异常检测提供了一种有效的解决方案。

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [90] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于偏微分方程（PDE）的单图像去雾方法，结合大气散射模型、非局部正则化和暗通道先验，改进了PDE框架。


<details>
  <summary>Details</summary>
Motivation: 解决单图像去雾问题，通过结合多种先验和模型，提升去雾效果。

Method: 提出改进的PDE框架，包含边缘保持扩散系数、高斯卷积算子和自适应正则化参数，并通过Lax-Milgram定理证明弱解的存在唯一性。

Result: 实验结果表明该方法是一种有效的去雾解决方案，并可推广到深度学习模型。

Conclusion: 该方法在单图像去雾任务中表现出色，具有通用性和高效性。

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [91] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/abs/2506.08796)
*Zhiyuan Ma,Ruixun Liu,Sixian Liu,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: Discretized-RF通过将直线路径离散化为可变速度子路径，引入噪声到速度而非数据，提升了多样性和多尺度噪声建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统RF模型中直线路径导致的多样性不足和多尺度噪声建模问题。

Method: 提出Discretized-RF，将直线路径离散化为动量场子路径，并在速度上引入噪声。

Result: 实验表明，该方法能生成多样且高效的轨迹，并持续产生高质量结果。

Conclusion: Discretized-RF通过改进路径设计和噪声建模，显著提升了RF模型的性能。

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [92] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，旨在解决人类-物体交互（HOI）视频生成中的关键限制，如对精选运动数据的依赖、对新物体/场景的泛化能力不足以及可访问性受限。


<details>
  <summary>Details</summary>
Motivation: 解决HOI视频生成中对精确输入的依赖、泛化能力不足和可访问性受限的问题。

Method: 通过稀疏解耦的运动引导，将外观和运动信号编码到多模态扩散变换器（MMDiT）的双输入空间，并在共享上下文空间中融合，生成时间一致且物理合理的交互。优化训练包括参数空间HOI适配器和面部交叉注意力适配器。

Result: 在弱监督下实现了交互自然性和泛化性的最先进性能，支持文本条件生成和交互式物体操作。

Conclusion: HunyuanVideo-HOMA在HOI视频生成中表现出色，具有高可控性和用户友好的界面。

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [93] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin是一种基于扩散模型的高效正弦图修复框架，通过分辨率引导的渐进推理减少内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 高分辨率正弦图修复对CT重建至关重要，但现有扩散模型因内存和计算需求过高而受限。

Method: HiSin采用分辨率引导的渐进推理，先在低分辨率提取全局结构，再在高分辨率处理小区域，并结合频率感知跳过和结构自适应步长分配。

Result: 实验显示，HiSin峰值内存降低31.25%，推理时间减少18.15%，且修复精度不受影响。

Conclusion: HiSin是一种高效且准确的高分辨率正弦图修复方法。

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [94] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817)
*Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT是一个创新的数据集和基准测试，旨在通过Chain-of-Thought方法提升视频内容理解的时空细节分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型在捕捉视频时空细节方面表现不足，需要更精细的数据集和方法来提升视频分析能力。

Method: 引入Video-CoT数据集，包含192,000个细粒度时空问答对和23,000个高质量CoT标注样本，并提供包含750张图像的基准测试和定制评估指标。

Result: 实验表明当前视觉语言模型在时空理解任务中表现不佳，突显了该任务的挑战性。

Conclusion: Video-CoT为多媒体理解研究开辟了新方向，支持未来智能系统中高级视频分析能力的创新。

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [95] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 该研究量化了文本到图像（T2I）模型在文化表现上的不足，发现其未能满足显性和隐性文化期望，并提出新基准CulturalFrames进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决T2I模型在多样化文化背景下的表现不足问题，尤其是对显性和隐性文化期望的忽视。

Method: 通过构建CulturalFrames基准，涵盖10个国家、5个社会文化领域，收集983个提示、3637张图像及10k+人工标注，系统评估T2I模型表现。

Result: 结果显示，T2I模型平均44%的情况下未能满足文化期望，显性期望失败率高达68%，隐性期望为49%。现有评估指标与人类判断相关性差。

Conclusion: 研究揭示了T2I模型在文化表现上的重大缺陷，为开发更具文化意识的模型和评估方法提供了方向。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [96] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/abs/2506.08849)
*Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Yingınst*

Main category: cs.CV

TL;DR: 该研究通过领域适应方法改进视觉语言基础模型在超声图像分析中的性能，利用大语言模型作为文本细化器，并在六个超声数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 超声图像分析中手动标注区域费时且易产生不一致，视觉语言基础模型在医学影像领域表现受限，需克服领域差异。

Method: 采用领域适应策略和任务驱动头，结合大语言模型作为文本细化器，对视觉语言基础模型进行微调。

Result: 在分割和分类任务中，该方法显著提升了性能，优于现有视觉语言和纯基础模型。

Conclusion: 研究成功开发了适应医学影像的视觉语言基础模型，为超声图像分析提供了高效解决方案。

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [97] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的深度学习方法，用于从全切片图像预测空间分辨基因表达，显著提升了基因表达预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于空间转录组数据获取成本高，大规模数据难以获得，因此需要一种高效的方法从现有数据中预测基因表达。

Method: 采用对比学习框架的深度学习方法，从全切片图像预测空间基因表达。

Result: 在六个疾病数据集上评估，预测高表达基因、高变基因和标记基因的Pearson相关系数分别提高了6.27%、6.11%和11.26%。

Conclusion: 该方法不仅保留了基因间相关性，还适用于小样本数据集，并在癌症组织定位中展现出潜力。

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [98] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一种实时处理未校准视频流并生成动态3D高斯泼溅表示的全前馈框架，解决了实时性、动态建模和长期稳定性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理未校准输入、动态场景建模和长期稳定性，StreamSplat旨在解决这些问题。

Method: 提出静态编码器中的概率采样机制和动态解码器中的双向变形场，实现高效动态建模。

Result: 在静态和动态基准测试中表现优于现有方法，支持任意长度视频流的在线重建。

Conclusion: StreamSplat在重建质量和动态场景建模上具有优势，适用于实时应用。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [99] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/abs/2506.08887)
*Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出DiscoVLA方法，通过同时解决视觉、语言和对齐三个差异，提升视频-文本检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型主要用于图像-文本匹配，但视频-文本检索需要视频级理解，现有方法仅关注视觉差异，忽略了语言和对齐差异。

Method: 提出Image-Video Features Fusion整合图像和视频特征，生成伪图像标题学习细粒度对齐，并通过Image-to-Video Alignment Distillation增强视频级对齐。

Result: 在MSRVTT数据集上，DiscoVLA以CLIP（ViT-B/16）为基础，R@1达到50.5%，优于之前方法1.5%。

Conclusion: DiscoVLA通过全面解决视觉、语言和对齐差异，显著提升了视频-文本检索性能。

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [100] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于Product of Experts（PoE）框架的训练无关方法，通过Annealed Importance Sampling（AIS）从异构模型中组合知识，提升了图像和视频合成的可控性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 整合来自视觉生成模型、视觉语言模型以及人工知识源（如图形引擎和物理模拟器）的多样化知识，以增强视觉生成任务的性能。

Method: 采用Product of Experts（PoE）框架，通过Annealed Importance Sampling（AIS）从异构模型的联合分布中进行采样。

Result: 在图像和视频合成任务中表现出更好的可控性，并提供了灵活的视觉生成目标指定接口。

Conclusion: 提出的PoE框架在无需训练的情况下，有效整合了异构模型的知识，提升了视觉生成任务的效果和用户体验。

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [101] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/abs/2506.08896)
*Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal*

Main category: cs.CV

TL;DR: WetCat是一个专为自动化技能评估设计的湿实验室白内障手术视频数据集，填补了现有资源在湿实验室环境中全面技能评估的空白。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验室训练依赖人工评估，效率低且主观性强，计算机视觉技术可提升评估效率和客观性。

Method: WetCat包含高分辨率视频，涵盖关键手术阶段的注释和语义分割，支持标准化技能评估。

Result: WetCat为开发可解释的AI评估工具奠定了基础，推动客观、可扩展的手术教育。

Conclusion: WetCat为眼科培训中的自动工作流分析和技能评估设定了新标准，数据集已公开。

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [102] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/abs/2506.08900)
*José Morano,Botond Fazekas,Emese Sükei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: MIRAGE是一种新型多模态基础模型，用于分析OCT和SLO图像，并在分类和分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有眼科AI模型依赖大量标注且泛化能力不足，基础模型（FMs）虽有潜力但缺乏验证。

Method: 提出MIRAGE多模态FM，并设计新的OCT/SLO分类和分割评估基准。

Result: MIRAGE在任务中优于通用和专业FM及分割方法。

Conclusion: MIRAGE适合作为稳健眼科AI系统的基础，模型和评估基准已开源。

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [103] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/abs/2506.08906)
*Peilin Yu,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Shuo Yang,Yunde Jia*

Main category: cs.CV

TL;DR: 提出了一种双曲双特征增强方法，用于开放环境中的特征生成，提升双曲学习算法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有双曲特征增强方法局限于封闭环境，无法处理开放环境中的未见类别。

Method: 结合神经ODE模块和元学习估计特征分布，引入正则化保持层次结构，推导损失上界支持无限增强。

Result: 在五个开放环境任务中显著提升双曲算法性能。

Conclusion: 方法有效增强开放环境中双曲算法的表现。

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [104] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文分析了VAR模型中的计算冗余问题，提出自动跳过步骤和替换无条件分支的方法，并引入SkipVAR框架动态选择加速策略，实验显示显著提升效率且保持模型质量。


<details>
  <summary>Details</summary>
Motivation: VAR模型的高频组件或后期步骤导致推理延迟高，但计算冗余问题尚未深入研究。

Method: 提出自动跳过步骤策略和替换无条件分支技术，并设计SkipVAR框架动态选择加速策略。

Result: SkipVAR在实验中实现平均SSIM 0.88以上，加速比达1.81倍，GenEval基准上速度提升2.62倍。

Conclusion: 频率感知、无需训练的适应性加速方法对可扩展自回归图像生成有效，代码已公开。

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [105] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: 提出了一种基于注意力的两阶段方法，通过二进制注意力掩码限制预测仅受关注区域影响，提高对虚假相关性和分布外背景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决上下文对物体感知的干扰问题，特别是在分布外背景下可能导致偏差表示的情况。

Method: 两阶段框架：第一阶段处理全图发现物体部分和任务相关区域，第二阶段通过注意力掩码限制接收域，过滤虚假信息。

Result: 在多个基准测试中显著提升对虚假相关性和分布外背景的鲁棒性。

Conclusion: 该方法通过注意力掩码和两阶段设计有效解决了上下文干扰问题，提升了模型性能。

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [106] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 论文探讨了如何通过蒙特卡洛树搜索（MCTS）算法在非推理型视觉语言模型（VLMs）中诱导长推理链，无需额外训练或监督。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索是否可以通过搜索机制激发非推理型模型的潜在知识，而无需放弃已部署的模型。

Method: 采用MCTS启发式算法，通过注入子问题-子答案对来引导模型生成长推理链。

Result: 在三个基准测试中表现一致提升，MMMU-PRO上总体提升2%，其中文科领域显著提升9%。

Conclusion: 通过将推理视为搜索过程，非推理型模型可以生成更长的推理链，证明了方法的有效性。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [107] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/abs/2506.08933)
*Wendong Bu,Yang Wu,Qifan Yu,Minghe Gao,Bingchen Miao,Zhenkui Zhang,Kaihang Pan,Yunfei Li,Mengze Li,Wei Ji,Juncheng Li,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: OmniBench是一个基于图的自生成、跨平台基准测试，通过子任务组合生成可控复杂度的任务。OmniEval是多维评估框架，评估虚拟代理的10种能力。数据集包含36k图结构任务，人工接受率91%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在任务复杂度不可控、人工标注场景有限和多维评估缺乏的问题。

Method: 提出OmniBench和OmniEval，前者通过子任务组合生成任务，后者提供多维评估框架。

Result: 数据集人工接受率91%，图结构数据比人工标注更高效。开源和闭源模型在多维评估中表现各异。

Conclusion: OmniBench和OmniEval为虚拟代理的未来发展提供了新方向。

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [108] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/abs/2506.08949)
*Hongjie Zhu,Xiwei Liu,Rundong Xue,Zeyu Zhang,Yong Xu,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.CV

TL;DR: 论文提出SSS方法，利用SAM-2的强特征提取能力增强半监督医学图像分割性能，通过DFE机制和多尺度增强优化特征，实验显示其在ACDC和BHSD数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在信息爆炸时代，减少对高质量标注数据的依赖并高效利用无标签数据是医学图像分析的关键挑战。

Method: 结合SAM-2的特征提取能力，提出DFE机制优化多视图特征差异，并开发PCSW提示生成器满足SAM-2的输入需求。

Result: 在BHSD数据集上平均Dice分数达53.15，超越之前最佳方法3.65分。

Conclusion: SSS方法显著提升了半监督医学图像分割性能，为未来研究提供了新方向。

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [109] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/abs/2506.08953)
*Anirudh Nanduri,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 本文研究了如何将预训练的Vision Transformer（ViT）模型应用于跨光谱人体识别任务，通过引入Side Information Embedding（SIE）提升性能，并探讨了遮挡问题在可见光-红外（VI）Re-ID中的影响。


<details>
  <summary>Details</summary>
Motivation: 跨光谱人体识别（如可见光和红外图像匹配）是一个具有挑战性的任务，现有方法在遮挡处理方面研究不足。本文旨在通过改进ViT模型和探索遮挡问题来填补这一空白。

Method: 作者在ViT模型中引入Side Information Embedding（SIE），编码相机信息以增强跨光谱匹配性能，并使用IJB-MDF数据集分析遮挡的影响。

Result: 实验表明，仅编码相机信息（而非域信息）即可在LLCM数据集上实现最佳性能。同时，IJB-MDF数据集的分析揭示了遮挡对VI-Re-ID任务的影响。

Conclusion: 本文展示了ViT在跨光谱人体识别中的潜力，并强调了遮挡问题的重要性，为未来研究提供了新的方向。

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [110] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEE的统一方法，用于解决不完全监督的隐蔽物体分割（ISCOS）问题。该方法利用SAM生成伪标签，并通过伪标签生成、存储和监督策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 隐蔽物体分割任务面临不完全标注数据和物体与背景相似性高的挑战，需要一种统一的方法来解决这些问题。

Method: 提出SEE框架，利用SAM生成伪标签，设计伪标签生成、存储和监督策略，并引入混合粒度特征分组模块。

Result: 实验表明，SEE在多个ISCOS任务中表现优异，达到了最先进的性能。

Conclusion: SEE是一种有效的解决方案，可作为即插即用的模块提升现有模型性能。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [111] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 提出了一种基于Fast AutoAugment的数据增强方法，显著提升了小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 小目标检测性能远低于大目标，是计算机视觉中的重要挑战。

Method: 使用Fast AutoAugment快速找到最优数据增强策略。

Result: 在DOTA数据集上实现了20%的性能提升。

Conclusion: 该方法有效解决了小目标检测性能退化问题。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [112] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/abs/2506.08964)
*Jinwoo Kim,Sangmin Han,Jinho Jeong,Jiwoo Choi,Dongyoung Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: ORIDa是一个大规模、真实捕获的数据集，用于对象合成任务，包含30,000多张图像和200个独特对象，提供多样化的场景和位置。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多样性和规模，无法全面探索真实场景中的对象合成问题。

Method: ORIDa包含两种数据类型：事实-反事实集（每组5张图像）和事实场景（单张图像）。

Result: ORIDa是首个公开的具有如此规模和复杂性的真实世界图像合成数据集。

Conclusion: ORIDa为对象合成研究提供了重要资源，推动了该领域的进一步发展。

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [113] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/abs/2506.08968)
*Amirreza Rouhi,Solmaz Arezoomandan,Knut Peterson,Joseph T. Woods,David K. Han*

Main category: cs.CV

TL;DR: ADAM是一种无需训练的开放世界对象标注框架，利用LLM和CLIP生成标签并通过自优化提升一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统目标检测模型依赖预定义类别、无法识别开放世界中新对象的问题。

Method: 结合LLM生成候选标签和CLIP视觉嵌入构建ELR，通过检索、投票和重排序为新对象分配标签，并引入自优化循环。

Result: 在COCO和PASCAL数据集上，ADAM成功标注新类别，无需微调或重新训练。

Conclusion: ADAM为开放世界对象标注提供了一种高效且无需训练的新方法。

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [114] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA方法通过适应预训练的视觉模型，高效实现医学视觉-语言对齐，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统跨模态对比学习方法视觉表示能力不足，而多模态掩码建模模型在视觉表示上表现优异但匹配能力有限。

Method: 提出ALTA方法，利用预训练的视觉模型进行适应，减少参数和计算消耗，并整合时间多视图输入增强一致性。

Result: ALTA在文本到图像和图像到文本检索任务中分别提升4%和6%的准确率。

Conclusion: ALTA高效且性能优越，同时促进了视觉和语言理解的提升。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [115] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: 论文指出当前的概念替换技术（CRTs）在图像到图像（I2I）模型中无法有效清除不可接受概念，并提出了一种兼顾效果和保真度的新方法AntiMirror。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散模型）在文本到图像（T2I）任务中表现优异，但如何避免生成不可接受内容（如侵权或冒犯性内容）仍具挑战性。现有CRTs在T2I中有效，但在新兴I2I场景中效果不佳，需解决这一差异。

Method: 首先通过I2I模型验证现有CRTs的不足，提出保真度（fidelity）的重要性，并设计了一种基于目标图像编辑的新技术AntiMirror。

Result: 实验表明AntiMirror在替换不可接受概念的同时能保持输入的其他概念，优于现有CRTs。

Conclusion: 论文揭示了CRTs在I2I场景中的局限性，提出了兼顾效果和保真度的解决方案AntiMirror，为未来研究提供了方向。

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [116] [Do MIL Models Transfer?](https://arxiv.org/abs/2506.09022)
*Daniel Shao,Richard J. Chen,Andrew H. Song,Joel Runevic,Ming Y. Lu,Tong Ding,Faisal Mahmood*

Main category: cs.CV

TL;DR: 该论文研究了多实例学习（MIL）模型在计算病理学中的迁移学习能力，发现预训练的MIL模型在不同器官和任务上表现优于从头训练的模型，且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: MIL在计算病理学中面临小规模、弱监督数据集的挑战，而迁移学习的潜力尚未充分探索。

Method: 系统评估了11个预训练MIL模型在21个任务上的表现，涵盖形态学和分子亚型预测。

Result: 预训练MIL模型在不同器官和目标任务上表现优异，泛化能力强，且数据需求低于其他方法。

Conclusion: MIL模型具有强大的迁移学习能力，为计算病理学提供了性能提升的有效途径，并开源了相关资源。

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [117] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 论文提出了一种名为DIsoN的去中心化OOD检测框架，能够在无法共享数据的情况下通过交换模型参数实现训练与测试数据的比较，并在医学影像数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如医学影像）部署ML模型时，需要检测训练数据分布之外的输入（OOD），但现有方法要么丢弃训练数据，要么假设数据集中存储，这在现实中难以实现。

Method: 提出了Isolation Network框架，通过解决二分类任务量化测试样本与训练数据的分离难度，并进一步扩展为DIsoN，支持在无法共享数据时通过交换模型参数进行比较。

Result: 在四个医学影像数据集上的12个OOD检测任务中，DIsoN表现优于现有方法，同时保护数据隐私。

Conclusion: DIsoN为ML开发者提供了一种新的服务模式，即远程、安全地利用训练数据进行OOD检测。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [118] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单插件式正则化方法，用于改进基于扩散的生成模型，无需预训练或额外参数。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习进展独立发展。本文旨在通过正则化改进模型。

Method: 提出Dispersive Loss，鼓励隐藏空间中的表示分散，类似于对比自监督学习，但无需正样本对。

Result: 在ImageNet数据集上评估，显示对多种模型均有改进。

Conclusion: Dispersive Loss有助于弥合生成建模与表示学习之间的差距。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [119] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/abs/2506.09035)
*Karhan Kayan,Stamatis Alexandropoulos,Rishabh Jain,Yiming Zuo,Erich Liang,Jia Deng*

Main category: cs.CV

TL;DR: Princeton365是一个包含365个视频的大规模多样化数据集，提供精确的相机位姿，填补了当前SLAM基准测试中精度与数据多样性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前SLAM基准测试在精度和数据多样性之间存在不足，Princeton365旨在通过新颖的地面真值采集框架解决这一问题。

Method: 数据集通过校准板和360度相机采集室内、室外和物体扫描视频，包含同步的单目、立体RGB视频和IMU数据，并提出基于光流的新评估指标。

Result: Princeton365提供了多样化的视频数据和新的评估指标，支持跨场景的SLAM方法比较，并提出了挑战性的新视角合成基准。

Conclusion: Princeton365为SLAM研究提供了更全面的数据集和评估工具，有助于分析方法的失败模式并推动领域发展。

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [120] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出ASVR方法，通过自回归框架联合学习视觉和文本模态，提升多模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）仅对文本序列进行自回归监督，未充分利用视觉模态，导致无法处理无标注图像、遗漏视觉细节及难以表达视觉中心内容等问题。

Method: 引入ASVR，在统一自回归框架中联合学习视觉和文本模态，通过自回归重建图像的语义表示而非原始外观。

Result: ASVR在多模态理解基准测试中表现稳定且一致，显著提升性能（如LLaVA-1.5平均得分提高5%）。

Conclusion: 自回归重建语义表示能有效增强多模态理解，优于原始视觉外观重建。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [121] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: Cosmos-Drive-Dreams是一个合成数据生成（SDG）管道，旨在生成具有挑战性的场景，以支持自动驾驶系统的感知和驾驶策略训练。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界数据收集和标注的高成本问题，尤其是难以捕捉的边缘案例。

Method: 利用NVIDIA Cosmos世界基础模型，开发了可控、高保真、多视角且时空一致的驾驶视频生成模型。

Result: 生成的数据增加了驾驶数据集的多样性和数量，缓解了长尾分布问题，并提升了3D车道检测、3D目标检测和驾驶策略学习等下游任务的泛化能力。

Conclusion: Cosmos-Drive-Dreams为自动驾驶系统提供了高效的数据生成工具，其开源工具包和数据集有望推动相关研究的发展。

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [122] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 论文提出了一种基于统一幅度规律的视频扩散模型加速方法MagCache，通过自适应跳过不重要时间步和缓存策略，显著提升速度并保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有加速方法依赖统一启发式或时间嵌入变体，需大量校准且易因提示过拟合导致输出不一致。

Method: 发现幅度比率规律，提出MagCache，通过误差建模和自适应缓存策略跳过不重要时间步。

Result: MagCache在Open-Sora和Wan 2.1上分别实现2.1倍和2.68倍加速，视觉质量优于现有方法。

Conclusion: MagCache仅需单样本校准，显著优于现有方法，适用于视频扩散模型加速。

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [123] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 本文提出了一种基于物理信息的神经模拟器，通过结合Kelvinlet先验和大规模FEM模拟，实现了高精度、低延迟的软组织变形实时模拟。


<details>
  <summary>Details</summary>
Motivation: 快速准确的软组织变形模拟对手术机器人和医学培训至关重要。

Method: 将Kelvinlet先验整合到神经模拟器中，结合线性和非线性软组织的FEM模拟，提升神经网络预测的准确性和物理一致性。

Result: 实验表明，该方法能高保真地模拟腹腔镜组织抓取工具的使用，验证了其有效性。

Conclusion: Kelvinlet增强学习是一种高效、物理感知的实时软组织模拟策略。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [124] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: 扩展版altiro3D库，支持从2D图像或视频流实时生成3D光场，结合AI技术提升性能，适用于多种输入源。


<details>
  <summary>Details</summary>
Motivation: 为玻璃全息显示提供更灵活的3D内容生成方案，支持实时处理和多样化输入。

Method: 使用MiDaS CNN从单张2D图像提取深度图，结合AI技术优化性能，实现多平台GUI支持。

Result: 成功扩展altiro3D库功能，支持实时3D渲染，兼容多种输入源（如桌面区域）。

Conclusion: 扩展版altiro3D库为3D显示设备提供了高效、灵活的内容生成工具。

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [125] [GATE: Geometry-Aware Trained Encoding](https://arxiv.org/abs/2506.08161)
*Jakub Bokšanský,Daniel Meister,Carsten Benthin*

Main category: cs.GR

TL;DR: 提出了一种基于三角网格的几何感知编码方法GATE，用于神经渲染相关算法，解决了哈希编码的局限性。


<details>
  <summary>Details</summary>
Motivation: 输入参数编码是神经网络算法的核心，但现有哈希编码存在哈希冲突、分辨率与场景大小选择等问题，需要更高效的编码方法。

Method: GATE将特征向量存储在三角网格表面，利用网格颜色解耦特征向量密度与几何密度，支持自适应细节控制。

Result: GATE避免了哈希冲突和内存访问不一致问题，提升了神经网络的训练效率和近似质量。

Conclusion: GATE为神经渲染相关算法提供了一种高效且灵活的编码方案，优于传统哈希编码。

Abstract: The encoding of input parameters is one of the fundamental building blocks of
neural network algorithms. Its goal is to map the input data to a
higher-dimensional space, typically supported by trained feature vectors. The
mapping is crucial for the efficiency and approximation quality of neural
networks. We propose a novel geometry-aware encoding called GATE that stores
feature vectors on the surface of triangular meshes. Our encoding is suitable
for neural rendering-related algorithms, for example, neural radiance caching.
It also avoids limitations of previous hash-based encoding schemes, such as
hash collisions, selection of resolution versus scene size, and divergent
memory access. Our approach decouples feature vector density from geometry
density using mesh colors, while allowing for finer control over neural network
training and adaptive level-of-detail.

</details>


### [126] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 论文提出了一种基于体积渲染的方法，通过将复杂微颗粒几何视为参与介质，利用统计特性（如粒子密度）进行建模，从而高效求解偏微分方程（PDE）。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以显式建模复杂微颗粒几何，因此需要一种无需离散化的高效模拟方法。

Method: 提出了两种新算法：体积球面行走（volumetric walk on spheres）和体积星形行走（volumetric walk on stars），用于在参与介质中模拟线性椭圆PDE。

Result: 实验表明，新算法在解决Laplace边界值问题时，比传统方法（如集合平均和均质化）更准确且高效。

Conclusion: 通过统计建模和蒙特卡洛算法，论文提供了一种高效且无需离散化的PDE求解方法，适用于复杂微颗粒几何问题。

Abstract: We consider the problem of solving partial differential equations (PDEs) in
domains with complex microparticle geometry that is impractical, or
intractable, to model explicitly. Drawing inspiration from volume rendering, we
propose tackling this problem by treating the domain as a participating medium
that models microparticle geometry stochastically, through aggregate
statistical properties (e.g., particle density). We first introduce the problem
setting of PDE simulation in participating media. We then specialize to
exponential media and describe the properties that make them an attractive
model of microparticle geometry for PDE simulation problems. We use these
properties to develop two new algorithms, volumetric walk on spheres and
volumetric walk on stars, that generalize previous Monte Carlo algorithms to
enable efficient and discretization-free simulation of linear elliptic PDEs
(e.g., Laplace) in participating media. We demonstrate experimentally that our
algorithms can solve Laplace boundary value problems with complex microparticle
geometry more accurately and more efficiently than previous approaches, such as
ensemble averaging and homogenization.

</details>


### [127] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: 论文提出了一种从手持相机拍摄的RGBD视频中重建关节物体的方法，解决了现有方法对数据采集要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 关节物体在日常生活中普遍存在，但现有方法需要精心采集的数据，限制了其实际应用。论文旨在通过普通智能手机拍摄的视频实现关节物体的重建。

Method: 采用从粗到细的框架，从动态RGBD视频中推断关节参数并分割可移动部分。

Result: 在合成和真实数据集上，该方法显著优于现有方法，能够跨类别重建关节物体。

Conclusion: 该方法为关节物体的实用、可扩展和通用化重建提供了新思路。

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [128] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出一种基于复值高斯基元的3D全息场景表示方法，显著提升渲染速度。


<details>
  <summary>Details</summary>
Motivation: 为了在3D表示中完整建模光的振幅和相位特性，以支持物理真实的渲染（尤其是全息显示）。

Method: 通过RGBD多视角图像直接优化复值高斯基元，避免依赖基于强度的中间表示。

Result: 相比现有方法，速度提升30倍至10,000倍，同时保持图像质量。

Conclusion: 该方法为几何对齐、物理真实的3D全息场景表示提供了初步解决方案。

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [129] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: 提出了一种基于ViT的鲁棒性材料选择方法，支持纹理和子纹理两级选择，并引入了新数据集DuMaS。


<details>
  <summary>Details</summary>
Motivation: 图像编辑中材料选择是关键步骤，但现有方法对光照和反射变化不鲁棒。

Method: 利用ViT模型和多分辨率处理策略，结合DuMaS数据集进行两级选择。

Result: 方法比现有方法更精细和稳定，适用于下游编辑任务。

Conclusion: 提出的方法在材料选择上表现优越，数据集支持更广泛的研究。

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [130] [AffectMachine-Pop: A controllable expert system for real-time pop music generation](https://arxiv.org/abs/2506.08200)
*Kat R. Agres,Adyasha Dash,Phoebe Chua,Stefan K. Ehrlich*

Main category: cs.HC

TL;DR: AffectMachine-Pop是一个专家系统，能够根据预先设定或用户实时情绪状态生成具有特定唤醒度和效价的复古流行音乐，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成系统多为黑箱，缺乏直接可控性，限制了灵活性和适应性。

Method: 开发AffectMachine-Pop系统，通过唤醒度和效价生成音乐，并进行听感研究验证。

Result: 系统能生成符合目标情绪的音乐，适用于交互式音乐生成或情绪自我调节。

Conclusion: AffectMachine-Pop为可控情感音乐生成提供了有效工具，适用于多种应用场景。

Abstract: Music is a powerful medium for influencing listeners' emotional states, and
this capacity has driven a surge of research interest in AI-based affective
music generation in recent years. Many existing systems, however, are a black
box which are not directly controllable, thus making these systems less
flexible and adaptive to users. We present \textit{AffectMachine-Pop}, an
expert system capable of generating retro-pop music according to arousal and
valence values, which can either be pre-determined or based on a listener's
real-time emotion states. To validate the efficacy of the system, we conducted
a listening study demonstrating that AffectMachine-Pop is capable of generating
affective music at target levels of arousal and valence. The system is tailored
for use either as a tool for generating interactive affective music based on
user input, or for incorporation into biofeedback or neurofeedback systems to
assist users with emotion self-regulation.

</details>


### [131] [Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling](https://arxiv.org/abs/2506.08294)
*Ruanqianqian Huang,Ayana Monroe,Peli de Halleux,Sorin Lerner,Nikolaj Bjørner*

Main category: cs.HC

TL;DR: 本文探讨了如何设计一个适合逻辑建模教学的教育环境，并提出了10条设计指南，最终实现了一个名为Z3Guide的开源工具。


<details>
  <summary>Details</summary>
Motivation: 逻辑建模在计算机科学中广泛应用，但教学资源稀缺且分散，亟需设计一个易用且满足师生需求的教育环境。

Method: 通过需求访谈研究和设计迭代，提炼出10条设计指南，并基于这些指南开发了Z3Guide工具。

Result: Z3Guide在100多名学生的逻辑建模学习工作坊中获得积极反馈，验证了其有效性。

Conclusion: Z3Guide为逻辑建模教学提供了实用工具，未来仍有改进空间。

Abstract: Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from
budgeting for grocery shopping to verifying software behavior. Logic modeling
helps solve CSPs programmatically using SMT solvers. Despite its importance in
many Computer Science disciplines, resources for teaching and learning logic
modeling are scarce and scattered, and challenges remain in designing
educational environments for logic modeling that are accessible and meet the
needs of teachers and students. This paper explores how to design such an
environment and probes the impact of the design on the learning experience.
From a need-finding interview study and a design iteration with teachers of
logic modeling, we curated 10 design guidelines spanning three main
requirements: providing easy access, supporting various educational modalities,
and allowing extensions for customized pedagogical needs. We implemented nine
guidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a
logic modeling learning workshop with more than 100 students, we gathered
positive feedback on its support for learning and identified opportunities for
future improvements.

</details>


### [132] [EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation](https://arxiv.org/abs/2506.08303)
*Thomas M. Kwok,Hilary HY Cheng,Wai Tuck Chow*

Main category: cs.HC

TL;DR: HJ-Pal是一种轻量级可穿戴触觉设备，通过EMG驱动的蜂窝状阻塞技术实现肌肉激活的动觉反馈，用于远程康复中的小肌肉评估。


<details>
  <summary>Details</summary>
Motivation: 解决远程康复中小肌肉评估的触觉反馈需求。

Method: 利用EMG信号驱动蜂窝状阻塞技术，将肌肉激活转化为动觉反馈。

Result: 实现了远程触诊功能，支持小肌肉评估。

Conclusion: HJ-Pal为远程康复提供了一种有效的触觉反馈解决方案。

Abstract: In this work, we introduce HJ-Pal, a lightweight wearable haptic device that
leverages EMG-driven honeycomb jamming to render muscle activation as
kinesthetic feedback, enabling remote palpation for small muscle assessment in
telerehabilitation.

</details>


### [133] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow是一个四阶段流程，结合扩散模型和语言模型，为新手提供实时反馈，支持非线性修改和学习。


<details>
  <summary>Details</summary>
Motivation: 当前AI绘图工具缺乏人类艺术家的分步过程，SakugaFlow旨在通过暴露中间输出和教学对话，将其转化为学习环境。

Method: 采用四阶段流程，结合扩散模型生成图像和语言模型提供反馈，支持非线性修改和分支版本。

Result: 将黑盒生成器转化为支持创意探索和技能学习的教学环境。

Conclusion: SakugaFlow通过分步反馈和教学对话，有效支持新手学习和创作。

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [134] [Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education](https://arxiv.org/abs/2506.08467)
*Prakash Shukla,Suchismita Naik,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parson*

Main category: cs.HC

TL;DR: 论文探讨了HCI教育中AI生成内容的引用问题，分析了学生在设计项目中的引用实践，提出了反思性引用策略。


<details>
  <summary>Details</summary>
Motivation: AI工具在设计项目中的广泛使用引发了如何引用AI生成内容的挑战，传统引用框架难以适应AI的动态特性。

Method: 通过定性分析35个团队项目和175名学生的反思，研究学生在设计过程中对AI的使用和引用方式。

Result: 发现学生的引用实践不一致，从正式引用到间接或无引用，揭示了现有框架的不足。

Conclusion: 建议将AI引用视为反思性教学实践，提出贡献声明和过程感知引用模型，以支持学生与AI的有意义合作。

Abstract: The growing integration of AI tools in student design projects presents an
unresolved challenge in HCI education: how should AI-generated content be cited
and documented? Traditional citation frameworks -- grounded in credibility,
retrievability, and authorship -- struggle to accommodate the dynamic and
ephemeral nature of AI outputs. In this paper, we examine how undergraduate
students in a UX design course approached AI usage and citation when given the
freedom to integrate generative tools into their design process. Through
qualitative analysis of 35 team projects and reflections from 175 students, we
identify varied citation practices ranging from formal attribution to indirect
or absent acknowledgment. These inconsistencies reveal gaps in existing
frameworks and raise questions about authorship, assessment, and pedagogical
transparency. We argue for rethinking AI citation as a reflective and
pedagogical practice; one that supports metacognitive engagement by prompting
students to critically evaluate how and why they used AI throughout the design
process. We propose alternative strategies -- such as AI contribution
statements and process-aware citation models that better align with the
iterative and reflective nature of design education. This work invites
educators to reconsider how citation practices can support meaningful
student--AI collaboration.

</details>


### [135] [Guidelines for Gaze-based Neural Preliminary Diagnosis](https://arxiv.org/abs/2506.08517)
*Mayar Elfares,Salma Younis,Pascal Reisert,Ralf Küsters,Tobias Renner,Andreas Bulling*

Main category: cs.HC

TL;DR: 论文探讨了眼动追踪技术作为神经疾病初步诊断工具的潜力，总结了现有研究中的主要发现，并提出了标准化指南。


<details>
  <summary>Details</summary>
Motivation: 传统神经诊断方法耗时且主观，眼动追踪技术提供了一种更客观的替代方案，但现有研究结果分散且矛盾，需要系统化。

Method: 论文通过总结现有眼动追踪研究的主要发现，系统化知识并提出标准化协议。

Result: 论文概述了已达成共识的眼动追踪发现，并提出了推进其作为神经初步诊断工具的关键指南。

Conclusion: 眼动追踪技术有望成为神经疾病初步诊断的标准化工具，但仍需进一步研究和协议统一。

Abstract: Neural disorders refer to any condition affecting the nervous system and that
influence how individuals perceive and interact with the world. Traditional
neural diagnoses rely on cumbersome, time-consuming, or subjective methods,
such as clinical interviews, behavioural observations, or medical imaging. Eye
tracking is an attractive alternative because analysing eye movements, such as
fixations and saccades, can provide more objective insights into brain function
and cognitive processing by capturing non-verbal and unconscious responses.
Despite its potential, existing gaze-based studies presented seemingly
contradictory findings. They are dispersed across diverse fields, requiring
further research to standardise protocols and expand their application,
particularly as a preliminary indicator of neural processes for differential
diagnosis. Therefore, this paper outlines the main agreed-upon findings and
provides a systematisation of knowledge and key guidelines towards advancing
gaze-based neural preliminary diagnosis.

</details>


### [136] [Exploring the Convergence of HCI and Evolving Technologies in Information Systems](https://arxiv.org/abs/2506.08549)
*Rajan Das Gupta,Ashikur Rahman,Md Imrul Hasan Showmick,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: cs.HC

TL;DR: 研究分析了50篇HCI界面设计论文，发现现有方法仍基于传统桌面模型，未能满足移动用户和位置服务的需求，建议结合敏捷方法和以人为中心的设计原则改进。


<details>
  <summary>Details</summary>
Motivation: 现代信息技术与日常生活深度融合，但HCI专业人员面临新挑战，需为多样化用户群体设计适应性强的界面。

Method: 综述50篇近期HCI界面设计论文，评估其是否满足现代技术需求。

Result: 多数HCI设计方法仍基于传统桌面模型，缺乏对移动用户和位置服务的支持，现有指南与新兴技术的灵活性不匹配。

Conclusion: 建议结合敏捷方法和以人为中心的设计原则改进界面设计，未来研究应纳入定性和定量方法，以弥合设计与技术发展间的差距。

Abstract: Modern technology driven information systems are part of our daily lives.
However, this deep integration poses new challenges to the human computer
interaction (HCI) professionals. With the rapid growth of mobile and cloud
computing and the Internet of Things (IoT), the demand for HCI specialists to
design user-friendly and adaptable interfaces has never been more pressing.
Especially for diverse user groups such as children, the elderly and people
with disabilities who need interfaces tailored to their needs regardless of
time and location. This study reviewed 50 recent papers on HCI interface design
for modern information systems. The goal is to see how well these methods
address the demands of current technology. The findings show that most HCI
design methods are still based on old desktop models and do not support mobile
users and location-based services well. Most existing interface design
guidelines do not align with the flexibility and dynamism of emerging
technologies. The goal of this study is to improve interface design by
combining agile methodologies with human-centered design principles. Future
studies should also incorporate both qualitative and quantitative approaches,
particularly in the context of cloud-based technologies and organizational
information systems. This approach aims to bridge the gap between current
interface design practices and the changing technological landscape.

</details>


### [137] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: MOSAIC-F是一个多模态反馈框架，结合了多模态学习分析、观察、传感器、AI和协作评估，为学生提供个性化反馈。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合人类评估和数据驱动的多模态分析，提供更准确、个性化和可操作的反馈，以改善学生的学习活动。

Method: 框架包括四个步骤：1) 使用标准化评分表进行同行和教授评估；2) 收集多模态数据（如视频、音频、生理信号）；3) 利用AI生成个性化反馈；4) 学生通过视频和自我评估进行反思。

Result: 在改善口头表达技能的测试中，框架表现良好。

Conclusion: MOSAIC-F通过结合人类评估和多模态数据分析，能够提供更有效的个性化反馈。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [138] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 论文探讨了t-SNE和UMAP在可视化分析中的误用问题，分析了原因并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 揭示t-SNE和UMAP在可视化分析中的常见误用现象，探讨其背后的原因及预防措施。

Method: 通过文献综述（114篇论文）和访谈研究，验证误用普遍性并分析原因。

Result: 发现误用主要源于对技术适用性的讨论不足，提出了改进建议。

Conclusion: 建议未来加强技术合理使用的讨论，并提供具体行动方向。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [139] [Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration](https://arxiv.org/abs/2506.08805)
*Stina Klein,Pooja Prajod,Katharina Weitz,Matteo Lavit Nicora,Dimitra Tsovaltzi,Elisabeth André*

Main category: cs.HC

TL;DR: 研究探讨了在工业环境中使用协作机器人（cobots）时，通过设计虚拟形象（avatars）提升工人互动体验的潜力。通过焦点小组研究，发现个性化沟通和任务辅助的重要性。


<details>
  <summary>Details</summary>
Motivation: 协作机器人在工业环境中的普及可能减少工人的社交互动，影响其福祉。虚拟形象被视为提升人机协作体验的潜在解决方案，但其实际效果尚未被充分研究。

Method: 研究采用焦点小组方法，参与者来自一家使用协作机器人的德国制造公司。在讨论前，参与者参与了实验室环境下的模拟工业协作场景。

Result: 研究发现虚拟形象在提升互动体验方面具有潜力，但需改进其行为设计。个性化沟通和任务辅助是关键因素。

Conclusion: 尽管研究结果受限于样本规模，但为未来开发适应性、情境感知的虚拟形象提供了初步方向。

Abstract: The integration of collaborative robots (cobots) in industrial settings
raises concerns about worker well-being, particularly due to reduced social
interactions. Avatars - designed to facilitate worker interactions and
engagement - are promising solutions to enhance the human-robot collaboration
(HRC) experience. However, real-world perspectives on avatar-supported HRC
remain unexplored. To address this gap, we conducted a focus group study with
employees from a German manufacturing company that uses cobots. Before the
discussion, participants engaged with a scripted, industry-like HRC demo in a
lab setting. This qualitative approach provided valuable insights into the
avatar's potential roles, improvements to its behavior, and practical
considerations for deploying them in industrial workcells. Our findings also
emphasize the importance of personalized communication and task assistance.
Although our study's limitations restrict its generalizability, it serves as an
initial step in recognizing the potential of adaptive, context-aware avatar
interactions in real-world industrial environments.

</details>


### [140] [From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags](https://arxiv.org/abs/2506.08881)
*Nicolas Grelier,Johannes Pfau,Nicolas Mathieu,Stéphane Kaufmann*

Main category: cs.HC

TL;DR: 本文通过数据驱动的方法分析Steam标签的演变，帮助理解视频游戏趋势，发现趋势可分为短时效、当代流行和稳定经典三类，平均持续约四年。


<details>
  <summary>Details</summary>
Motivation: 视频游戏市场竞争激烈且难以预测，行业需要理解趋势以应对变化。

Method: 基于Steam标签的数据分析、可视化和专家验证。

Result: 趋势可分为三类，平均持续四年，并通过专家验证。

Conclusion: 提供了一种开放的方法来解读视频游戏趋势的变化。

Abstract: The video game industry deals with a fast-paced, competitive and almost
unpredictable market. Trends of genres, settings and modalities change on a
perpetual basis, studios are often one big hit or miss away from surviving or
perishing, and hitting the pulse of the time has become one of the greatest
challenges for industrials, investors and other stakeholders. In this work, we
aim to support the understanding of video game trends over time based on
data-driven analysis, visualization and interpretation of Steam tag evolutions.
We confirm underlying groundwork that trends can be categorized in short-lived
fads, contemporary fashions, or stable classics, and derived that the surge of
a trend averages at about four years in the realm of video games. After using
industrial experts to validate our findings, we deliver visualizations,
insights and an open approach of deciphering shifts in video game trends.

</details>


### [141] [Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams](https://arxiv.org/abs/2506.08892)
*Tauhid Tanjim,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.HC

TL;DR: 研究探讨了多模态机器人通信在时间敏感环境中如何影响团队工作负荷和人类对机器人的感知，发现语音和视觉提示能有效降低负荷并提升易用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在时间敏感环境中如何通过多模态交互有效协助团队协作的研究空白。

Method: 通过实验研究，在医疗训练场景中使用机器人推车提供语音和非语言提示，分析其对团队协作的影响。

Result: 语音提示用于物品搜索任务和视觉提示用于任务提醒，能显著降低团队工作负荷并提升机器人的易用性和实用性感知。

Conclusion: 研究强调了多模态交互在HRI领域的重要性，并为时间敏感环境中机器人集成提供了实践指导。

Abstract: The human-robot interaction (HRI) field has recognized the importance of
enabling robots to interact with teams. Human teams rely on effective
communication for successful collaboration in time-sensitive environments.
Robots can play a role in enhancing team coordination through real-time
assistance. Despite significant progress in human-robot teaming research, there
remains an essential gap in how robots can effectively communicate with action
teams using multimodal interaction cues in time-sensitive environments. This
study addresses this knowledge gap in an experimental in-lab study to
investigate how multimodal robot communication in action teams affects workload
and human perception of robots. We explore team collaboration in a medical
training scenario where a robotic crash cart (RCC) provides verbal and
non-verbal cues to help users remember to perform iterative tasks and search
for supplies. Our findings show that verbal cues for object search tasks and
visual cues for task reminders reduce team workload and increase perceived ease
of use and perceived usefulness more effectively than a robot with no feedback.
Our work contributes to multimodal interaction research in the HRI field,
highlighting the need for more human-robot teaming research to understand best
practices for integrating collaborative robots in time-sensitive environments
such as in hospitals, search and rescue, and manufacturing applications.

</details>


### [142] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 论文提出了一种在NXP MCXN947微控制器上实现的关键词检测系统，利用集成NPU实现实时语音交互，结合MFCC特征提取和CNN分类器，通过量化感知训练优化模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决资源受限设备上实时语音交互的挑战，通过优化模型和硬件加速实现高效低功耗的语音接口。

Method: 采用MFCC特征提取和CNN分类器，结合量化感知训练优化模型，利用NPU加速推理。

Result: 实验显示，NPU加速比CPU快59倍，模型大小30.58 KB，准确率达97.06%。

Conclusion: 证明了在嵌入式平台上实现高效低功耗语音接口的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [144] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: KVmix是一种针对大型语言模型KV Cache的混合精度量化方法，通过梯度重要性分析和动态优化策略，显著降低内存占用并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: KV Cache的高内存需求限制了LLMs在资源受限平台上的部署，现有方法无法动态优化精度分配或处理长上下文任务。

Method: KVmix利用梯度重要性分析为不同层分配比特宽度，动态优化长上下文任务中的KV对精度，并提供高效的低比特量化和CUDA内核优化。

Result: 在Llama和Mistral等模型上，KVmix以极低量化配置（Key 2.19bit，Value 2.38bit）实现接近无损的推理性能，内存压缩4.9倍，推理速度提升5.3倍。

Conclusion: KVmix通过动态混合精度量化和长上下文优化，显著提升了LLMs在资源受限环境中的部署效率。

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [145] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 提出了一种半监督方法，将难民统计数据从行政边界细化到0.5度网格单元，覆盖25个撒哈拉以南非洲国家。


<details>
  <summary>Details</summary>
Motivation: 解决传统难民统计数据在区域和国家层面过于笼统的问题，揭示局部流离失所模式。

Method: 结合UNHCR的ProGres注册数据、Google Open Buildings的卫星建筑足迹和OpenStreetMap的地点坐标，使用标签传播算法生成高精度空间统计数据。

Result: 方法平均准确率达92.9%，成功将1000多万难民观测数据分配到网格单元，识别出局部流离失所模式。

Conclusion: 高分辨率数据集为深入研究流离失所的驱动因素提供了基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [146] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: 提出了一种Bi-level Unbalanced Optimal Transport (BUOT)模型，通过样本级和类级传输的合作机制，解决了部分域适应问题中的异常类识别和跨域对齐问题。


<details>
  <summary>Details</summary>
Motivation: 部分域适应问题需要同时对齐跨域样本并识别异常类，现有权重框架仅关注样本级关系，未能充分利用聚类结构且对预测误差敏感。

Method: 引入样本级和类级传输的合作机制，样本级传输提供结构信息，类级传输提供异常类识别信息，结合标签感知传输成本提升效率。

Result: 在基准数据集上的实验验证了BUOT的竞争力。

Conclusion: BUOT通过统一的双层传输框架有效解决了部分域适应问题，提升了异常类识别和跨域对齐的准确性。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [147] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: 论文提出了一种名为Agentic Neural Network（ANN）的框架，通过将多智能体协作建模为分层神经网络架构，动态优化任务分解与协作，显著提升了准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体协作方法依赖静态、手工配置，限制了灵活性和效率。ANN旨在通过神经网络的动态优化原则解决这一问题。

Method: ANN采用两阶段优化策略：前向阶段动态分解任务并构建协作团队，后向阶段通过反馈优化全局和局部协作。

Result: 在四个基准数据集上，ANN在相同配置下优于现有方法，表现出更高的准确性和适应性。

Conclusion: ANN为多智能体系统提供了一种可扩展、数据驱动的框架，结合了LLMs的协作能力和神经网络的高效灵活性。

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [148] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: 提出基于大语言模型（LLM）知识迁移的通用流场预测框架，解决传统CFD方法高计算成本和现有深度学习模型跨条件迁移能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法计算成本高，现有深度学习模型在跨条件迁移能力上有限，亟需一种高效且通用的解决方案。

Method: 结合POD降维与预训练LLM微调策略，设计流体动力学导向的文本模板，提升预测性能。

Result: 在少样本学习场景中优于传统Transformer模型，跨多种流入条件和翼型几何表现出色，预测时间从小时级降至秒级，保持90%以上准确率。

Conclusion: 该框架为快速流体动力学预测开辟新方向，可应用于气动优化、流动控制等工程领域。

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [149] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: MBPO是一种新的偏好学习框架，通过生成硬负样本和在线验证奖励，解决LMM中的模态不平衡问题，提升性能并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在推理中存在模态不平衡问题，语言先验偏见过强，导致下游任务泛化能力差和幻觉现象。现有偏好优化方法未能有效抑制LLM内部偏见，且依赖离线数据。

Method: MBPO通过对抗性扰动生成硬负样本构建离线偏好数据集，并利用闭端任务生成在线验证奖励数据，结合GRPO进行训练。

Result: 实验表明，MBPO能显著提升LMM在视觉语言任务中的性能，并有效减少幻觉。

Conclusion: MBPO通过平衡模态偏好和混合数据训练，为LMM的优化提供了有效解决方案。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [150] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: 论文探讨了在预训练中使用MXFP8格式的精度缩放技术，提出了一种改进的舍入模式以避免模型发散，并成功在15T tokens上训练了一个8B模型。


<details>
  <summary>Details</summary>
Motivation: 研究MX格式在预训练中的数值稳定性问题，解决因舍入模式导致的模型发散问题。

Method: 提出了一种改进的舍入模式（round-to-infinity），用于计算MXFP8格式的缩放因子。

Result: 成功在MXFP8格式下预训练了一个8B模型，使用了15T tokens的数据集。

Conclusion: 改进的舍入模式能够有效提升MX格式在预训练中的数值稳定性，实现高效且稳定的模型训练。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [151] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: 论文提出ST-GraphNet框架，通过时空图神经网络预测自动驾驶车辆（AV）碰撞严重性，使用细粒度和区域聚合的空间图，在测试中达到97.74%的准确率。


<details>
  <summary>Details</summary>
Motivation: 理解AV碰撞严重性的时空动态对提升城市交通安全和基础设施规划至关重要。

Method: 构建细粒度和粗粒度的空间图，结合多模态数据（如文本嵌入），评估多种GNN架构（如GCN、GAT、DSTGCN），最终提出基于DSTGCN的ST-GraphNet框架。

Result: ST-GraphNet在粗粒度H3图上达到97.74%的测试准确率，显著优于细粒度模型（64.7%）。

Conclusion: 空间聚合、动态消息传递和多模态特征整合能有效捕捉AV碰撞严重性的复杂时空模式。

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [152] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: STAMImputer是一种用于交通数据填补的时空注意力混合专家网络，通过动态图结构和混合专家框架解决块缺失和分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在块缺失数据场景中提取特征效果不佳，静态图结构限制了模型对非平稳交通数据的适应性。

Method: 提出混合专家框架捕获时空特征及其权重，设计低秩引导采样图注意力机制生成动态图。

Result: 在四个交通数据集上实验，STAMImputer性能显著优于现有方法。

Conclusion: STAMImputer有效解决了交通数据填补中的块缺失和分布偏移问题，性能优越。

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [153] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: 论文证明，通过推理时技术（如上下文学习）可以近似监督微调的能力，无需修改模型参数，并在理想和实际场景下提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）计算成本高，研究旨在探索是否可以通过推理时技术（如上下文学习）近似其能力，从而节省资源。

Method: 在理想假设（无限计算资源和完整数据集）下，证明基础Transformer模型可通过上下文学习近似SFT能力，并扩展到有限上下文和部分数据集的实际情况。

Result: 对于固定输出长度的文本生成任务和线性分类任务，给出了数据集大小的理论界限，证明了近似SFT行为的可行性。

Conclusion: 研究为资源高效部署大型语言模型提供了理论基础，并通过检索增强生成等技术将理论应用于实际。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [154] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: FairDICE是首个离线多目标强化学习框架，直接优化非线性福利目标，无需显式偏好权重。


<details>
  <summary>Details</summary>
Motivation: 现有线性标量化方法无法捕捉公平导向目标（如Nash社会福利或最大最小公平），且离线环境中非线性福利标准的统一优化方法尚未探索。

Method: FairDICE利用分布校正估计，联合考虑福利最大化和分布正则化，实现稳定且样本高效的学习。

Result: 在多个离线基准测试中，FairDICE表现出优于现有基线的公平感知性能。

Conclusion: FairDICE为离线多目标强化学习中的非线性福利优化提供了统一且高效的解决方案。

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [155] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: 提出了一种轻量级、高效的随机向量功能链接网络Lite-RVFL，用于在线学习中应对概念漂移，无需检测漂移或重新训练。


<details>
  <summary>Details</summary>
Motivation: 概念漂移对在线学习方法的可靠性构成挑战，现有方法计算成本高且不适合实时应用。

Method: Lite-RVFL通过指数加权新样本的损失函数，强调近期数据并实现快速适应。理论分析支持其可行性，并推导了增量更新规则。

Result: 实验验证了Lite-RVFL在适应漂移和捕捉时间模式上的高效性和有效性。

Conclusion: Lite-RVFL是一种适用于实时应用的高效概念漂移适应方法。

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [156] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: 论文提出Info-Coevolution框架，通过在线选择性标注减少数据冗余，提升数据集效率，降低标注和训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中数据增长带来的数据集构建和训练效率问题，避免传统方法的数据冗余和主动学习的复杂性及偏差。

Method: 提出Info-Coevolution框架，利用任务特定模型选择性标注和整合在线及网络数据，实现模型与数据的协同进化。

Result: 在ImageNet-1K等数据集上，标注和训练成本降低32%，且无需性能损失；结合半监督学习可进一步降低标注比例至50%。

Conclusion: Info-Coevolution是一种高效、无偏的数据选择方法，显著降低标注和训练成本，适用于实际应用。

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [157] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: 论文比较了多种时间序列基础模型（TSFMs）与传统方法在电力价格预测（EPF）中的表现，发现TSFMs表现与传统方法相当，但双季节性MSTL模型在一致性和性能上更优。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测对电力现货市场交易决策至关重要，但现有生成式AI和时间序列基础模型在EPF中的效果尚不明确。

Method: 使用德国、法国、荷兰、奥地利和比利时的2024年日前拍卖电力价格数据，对多种TSFMs和传统统计与机器学习方法进行基准测试。

Result: Chronos-Bolt和Time-MoE在TSFMs中表现最佳，与传统方法相当，但双季节性MSTL模型在一致性和性能上更优。

Conclusion: TSFMs在EPF中表现良好，但传统方法（如MSTL）仍具优势，尤其是在捕捉季节性和一致性方面。

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [158] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo是一个RL框架，通过改进长度奖励设计，提升语言模型推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升推理能力时多关注准确性，而忽略了效率，且直接长度奖励常导致准确性下降。

Method: Bingo引入显著性感知长度奖励和动态长度奖励，逐步减少无关标记并动态调整奖励。

Result: 实验表明Bingo在多个基准测试中优于其他方法，实现了效率与准确性的平衡。

Conclusion: Bingo展示了通过显式训练提升LLM推理效率的潜力。

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [159] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: 论文提出了一种名为NONA的可微分k-NN回归层，通过结合神经网络注意力和学习注意力掩码机制，提升了传统算法在监督微调中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统算法（如k-NN）在监督微调中表现优异，但由于其不可微分性和独特优化需求，难以直接整合到神经网络中。

Method: 提出NONA层，利用神经网络注意力和学习注意力掩码机制，模拟k-NN回归算法。

Result: 在多个非结构化数据集上，NONA在回归任务中优于密集层预测和k-NN。

Conclusion: NONA为传统算法在监督微调中的整合提供了可行方案，并展示了性能提升。

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [160] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT是一个自动收集高质量编码任务的管道，用于解决数据稀缺问题，构建了AutoSDT-5K数据集，并训练出性能显著提升的AutoSDT-Coder模型。


<details>
  <summary>Details</summary>
Motivation: 加速科学发现需要高质量数据，但现有数据稀缺且质量不足，因此提出AutoSDT来解决这一问题。

Method: 利用LLM的编码能力和参数知识，自动搜索、筛选和合成任务与代码，构建AutoSDT-5K数据集。

Result: AutoSDT-5K包含5,404个任务，93%有效，92.2%代码正确；AutoSDT-Coder模型在基准测试中表现显著提升。

Conclusion: AutoSDT有效解决了数据稀缺问题，并显著提升了AI在科学发现中的性能。

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [161] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出了一种高效的公平谱聚类方法（Fair SC），通过将问题转化为凸差函数（DC）框架，并采用新的变量增强策略和ADMM算法，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着决策算法公平性问题的日益重要，研究如何在谱聚类中实现群体公平性成为关键。

Method: 将公平谱聚类问题转化为DC框架，引入变量增强策略，并采用ADMM算法解决子问题。

Result: 实验表明，该方法在合成和真实数据集上均有效，计算速度显著优于现有方法。

Conclusion: 该研究为公平聚类在实际应用中的推广提供了重要进展。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [162] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: 提出了一种无需闭合形式本构方程即可识别异质材料力学性能的新框架，结合神经网络和物理驱动方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要闭合形式本构方程，限制了异质材料力学性能的识别。新框架旨在克服这一限制。

Method: 使用神经网络结合傅里叶特征捕捉应变场，基于NODE发现本构方程，并通过超网络处理异质性。

Result: 数值实验表明该方法在识别异质材料力学性能方面具有鲁棒性和通用性。

Conclusion: 该框架为异质材料力学性能识别提供了新的替代方案，优于传统逆向方法。

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [163] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 论文提出了一种分层结构的遗忘学习问题，并提出了一种新的双级优化算法BLUR，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型（LLM）在训练后能遗忘特定知识或能力，以符合数据法规和伦理要求。

Method: 采用双级优化框架，下层目标最小化遗忘损失，上层目标保持模型效用。

Result: BLUR算法在多种任务、模型和指标上均优于现有方法。

Conclusion: 分层结构和BLUR算法为遗忘学习问题提供了更优的解决方案。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [164] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL提出了一种新的联邦学习框架，通过两种正则化策略解决非IID数据导致的性能下降问题，显著提升了模型准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID数据下性能下降严重，传统方法成本高或适应性差，亟需一种高效解决方案。

Method: UniVarFL采用分类器方差正则化和超球面均匀正则化，模拟IID训练动态，减少局部分类器偏差。

Result: 在多个基准数据集上，UniVarFL在准确性上优于现有方法，展示了其高效性和可扩展性。

Conclusion: UniVarFL是一种高效且可扩展的联邦学习解决方案，尤其适用于资源受限的环境。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [165] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: 提出了一种基于随机神经网络的联邦学习方法（Federated stochastic neural networks），用于解决本地数据中的潜在噪声问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中本地数据可能因测量能力有限或人为错误引入噪声，影响模型性能。

Method: 在联邦学习框架中使用随机神经网络作为本地模型，以估计数据的真实状态并量化潜在噪声。

Result: 数值实验表明，该方法在处理非独立同分布数据时表现优异。

Conclusion: 随机神经网络的引入有效提升了联邦学习对噪声数据的鲁棒性。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [166] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文提出了一种基于遗传算法的个性化决策树构建方法，适用于联邦学习中的分类和回归任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习主要关注基于梯度的参数模型，而决策树等非参数模型研究较少，且现有方法受限于分类任务和分类数据。

Method: 利用遗传算法构建个性化决策树，支持分类和数值数据，适用于分类和回归任务。

Result: 实验表明，该方法优于仅基于本地数据训练的决策树和基准算法。

Conclusion: 遗传算法为联邦学习中的决策树构建提供了更灵活和高效的解决方案。

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [167] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 该论文探讨了相关噪声机制在差分隐私（DP）中的设计与分析，特别关注其在AI和机器学习模型私有训练中的应用，通过加权前缀和估计这一核心原语。


<details>
  <summary>Details</summary>
Motivation: 传统DP机制在随机梯度下降（SGD）学习算法的每一步注入独立噪声以保护训练数据的隐私，但研究表明引入（反）相关噪声可以通过抵消先前步骤的噪声显著改善隐私-效用权衡。

Method: 论文研究了相关噪声机制（如矩阵机制、因子分解机制和DP-FTRL）的设计与分析，并将其应用于学习算法中。

Result: 相关噪声机制在实践中具有重要影响，已在全球范围内实现工业级部署。

Conclusion: 相关噪声机制为差分隐私提供了更优的隐私-效用权衡，并在实际应用中展现出显著效果。

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [168] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的残余应力生成器（RSG），用于从有限测量中推断全场应力分布，以减少实验工作量。


<details>
  <summary>Details</summary>
Motivation: 残余应力会影响组件性能，但全场应力分布的实验表征工作量巨大，因此需要一种高效的方法。

Method: 通过大量工艺模拟构建数据集，使用U-Net架构的机器学习模型进行训练和超参数调优。

Result: 模型在模拟应力预测中表现出高准确性和泛化能力，实验数据验证了其有效性。

Conclusion: RSG方法能够从有限测量中全面理解残余应力分布，显著减少实验工作量。

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [169] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: 论文探讨了集成模型的解释性问题，通过计算复杂性理论分析了影响解释性的因素，发现基模型的数量、大小和类型对解释性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 集成模型（如提升树）通常被视为黑箱，缺乏对其解释性的严格数学理解。本文旨在填补这一空白。

Method: 应用计算复杂性理论，研究不同集成配置下生成解释的挑战。

Result: 发现基模型数量、大小和类型对解释性有复杂影响，例如小规模决策树集成可高效解释，而线性模型集成即使数量少也难解释。

Conclusion: 研究为理解集成模型的解释性提供了更坚实的基础，强调了计算复杂性视角的重要性。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [170] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Mondrian是一种基于Transformer的算子学习方法，通过将域分解为不重叠的子域并应用注意力机制，解决了高分辨率、多尺度PDE建模中的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在建模高分辨率、多尺度PDE时，由于注意力机制的二次计算成本和与离散化的耦合，难以扩展。

Method: Mondrian将域分解为子域，在每个子域内使用神经算子替代标准层，并通过基于softmax的函数内积计算子域间的注意力。

Result: Mondrian在Allen-Cahn和Navier-Stokes PDE上表现优异，支持无需重新训练的分辨率扩展。

Conclusion: Mondrian展示了域分解注意力在可扩展和通用神经算子中的潜力。

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [171] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: 论文研究了编码器-解码器自回归Transformer模型在自动驾驶领域中的联合运动预测和规划任务上的经验性扩展规律，发现模型性能随计算预算呈幂律增长，并探讨了训练和推理时的最优扩展策略。


<details>
  <summary>Details</summary>
Motivation: 研究自动驾驶领域中模型性能与计算预算的关系，优化训练和推理时的扩展策略，以提升模型在多样化驾驶场景中的表现。

Method: 使用50万小时驾驶数据集，分析模型性能与计算预算的关系，研究模型参数与数据量的最优扩展比例，并探讨推理时小模型的竞争力。

Result: 模型性能随计算预算呈幂律增长，训练时模型参数扩展速度应为数据量的1.5倍；推理时小模型通过采样和聚类可与大模型竞争，直至交叉点。

Conclusion: 优化训练和推理时的扩展策略是提升自动驾驶模型性能的关键，同时利用其他车辆的驾驶数据可缓解机器人数据稀缺问题。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [172] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 本文提出一个评估LLM提取临床数据质量的框架，结合性能基准、自动验证和复制分析，以提高数据可靠性和公平性。


<details>
  <summary>Details</summary>
Motivation: LLM在提取临床数据时虽高效，但存在可靠性、准确性和公平性问题，现有框架未能完全解决。

Method: 提出多维框架，包括性能基准、自动验证和复制分析，支持偏差评估。

Result: 框架能识别需改进变量、检测潜在错误，并确认数据适用性。

Conclusion: 该框架为LLM提取的RWD提供严格评估方法，推动行业标准并支持AI证据的可靠使用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [173] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: 本文探讨了随机数据增强的不足并提出改进方法，通过解决特征扭曲问题提升其泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随机数据增强成本低但效果有限，本文旨在解决其不足。

Method: 提出一种简单方法，通过解决特征扭曲问题改进随机增强的泛化效果。

Result: 在单源域泛化（sDG）基准测试中表现出强泛化性能。

Conclusion: 改进后的随机增强方法在泛化性能上表现优异。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [174] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY是一种基于模型的离线强化学习算法，通过生成目标域的新合成数据来解决源域和目标域动态不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因目标域数据有限，无法探索目标域，限制了策略学习的效果。

Method: MOBODY利用表示学习发现跨域共享的潜在状态和动态，并通过模型生成目标域数据增强策略学习。

Result: 在MuJoCo基准测试中，MOBODY显著优于现有方法，尤其在复杂场景中表现突出。

Conclusion: MOBODY通过动态学习和数据增强，有效解决了跨域动态不匹配问题，提升了离线强化学习的性能。

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [175] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: 提出了一种基于信号时序逻辑（STL）的框架，用于评估和改进大语言模型（LLMs）在数学推理任务中的逐步置信度，以减少错误输出。


<details>
  <summary>Details</summary>
Motivation: LLMs在数学推理任务中常产生高置信度但错误的输出，尤其在教育等领域可能带来风险。

Method: 使用STL建模逐步置信度为时序信号，定义约束条件并计算鲁棒性分数，同时引入不确定性重塑策略。

Result: 实验表明该方法显著改善了校准指标，提供了比传统方法更可靠的不确定性估计。

Conclusion: 提出的框架能有效提升LLMs在数学推理中的置信度可靠性和输出质量。

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [176] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: TAWM是一种基于时间感知的世界模型，通过动态调整时间步长来学习任务的高低频动态，提升性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统模型通常固定时间步长采样，无法适应不同频率的动态任务，因此需要一种能显式结合时间动态的模型。

Method: TAWM通过条件化时间步长Δ𝑡，并在多样化的Δ𝑡值上训练，学习任务的动态特性。

Result: 实验表明，TAWM在不同观测率下均优于传统模型，且训练样本和迭代次数相同。

Conclusion: TAWM通过时间感知设计，显著提升了模型在控制任务中的性能和数据效率。

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [177] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: Reinforced RCSL通过引入“分布内最优未来回报”概念，解决了RCSL缺乏拼接能力的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: RCSL因其简单稳定在离线强化学习中受到关注，但缺乏拼接能力限制了其性能。本文旨在解决这一问题。

Method: 提出Reinforced RCSL框架，引入“分布内最优未来回报”机制，避免复杂回报增强技术。

Result: 理论分析和实验结果表明，Reinforced RCSL在多个基准测试中显著优于标准RCSL。

Conclusion: Reinforced RCSL是一种简单有效的方法，能够提升RCSL的性能，适用于广泛的决策任务。

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [178] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: 提出一种零参数方法，通过在损失函数中添加近似等变性约束，简化了等变神经网络的实现，并在多个数据集上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有等变方法计算复杂、参数多且架构受限，需要更简单高效的解决方案。

Method: 在潜在表示中为有限群添加近似等变性约束，作为损失函数的附加项，并让网络学习潜在空间的群表示。

Result: 网络倾向于学习正则表示，实验表明该方法在多个数据集上性能接近或优于现有方法，且参数更少。

Conclusion: 提出的零参数方法能有效实现近似等变性，性能优越且计算高效。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [179] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: SHIELD结合超网络和区间算术，同时解决灾难性遗忘和对抗攻击问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络存在灾难性遗忘和对抗攻击脆弱性问题，目前没有模型能同时解决这两个问题。

Method: SHIELD通过超网络动态生成针对特定数据的子网络，并利用区间算术为输入数据提供严格的安全保证。

Result: SHIELD能在不牺牲网络适应性的情况下增强安全性。

Conclusion: SHIELD为持续学习中的安全性挑战提供了创新解决方案。

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [180] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: HC-RLHF是一种新方法，通过高置信度安全强化学习从人类反馈中确保语言模型的安全性和有用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常将安全性与有用性对立，导致敏感领域的不当回应，需一种可靠方法平衡二者。

Method: HC-RLHF将人类偏好分解为有用性和安全性，分别训练奖励模型和成本模型，并通过两步优化确保安全。

Result: 实验表明，HC-RLHF能高概率生成安全模型，并在安全性和有用性上优于现有方法。

Conclusion: HC-RLHF为语言模型对齐提供了一种可靠且高效的方法，尤其适用于敏感领域。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [181] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: LIES框架通过固定神经网络架构和优化策略，实现了高效且可解释的符号回归，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法在可扩展性和符号一致性上存在不足，LIES旨在解决这些问题。

Method: 采用LIES架构，结合过采样策略和定制损失函数，训练后通过剪枝简化表达式。

Result: LIES在基准测试中生成稀疏且准确的符号公式，优于所有基线方法。

Conclusion: LIES框架在符号回归中表现出色，其设计组件通过消融研究验证了重要性。

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [182] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 提出一种同时优化神经网络结构和权重的方法，通过连续潜在空间嵌入架构和参数信息，并通过梯度下降联合优化。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络设计依赖手动试错或离散化架构搜索，效率低且分离了架构和权重优化。

Method: 训练一个多尺度自编码器，将架构和参数嵌入连续潜在空间，通过梯度下降联合优化结构和权重，并加入稀疏性和紧凑性惩罚。

Result: 在合成回归任务中，方法能有效发现性能优异的稀疏紧凑神经网络。

Conclusion: 该方法提供了一种联合优化架构和权重的有效途径，优于传统离散化方法。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [183] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: UDEs结合神经网络与物理微分方程，用于智能电网中电池动态建模，解决传统方法难以泛化和捕捉未建模动态的问题。


<details>
  <summary>Details</summary>
Motivation: 智能电网中节点电池动态建模因太阳能输入随机性和家庭负载变化而复杂，传统方法泛化能力不足。

Method: 提出基于UDE的方法，将神经残差嵌入物理电池ODE中，学习节点特定电池动态。

Result: 实验显示UDE模型与真实电池轨迹高度吻合，收敛平滑，长期预测稳定。

Conclusion: UDE方法适用于分散式能源网络电池建模，对实时控制和优化可再生能源智能电网有广泛意义。

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [184] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: 该研究系统评估了12种特征缩放技术对14种机器学习算法和16个数据集的影响，发现集成方法对缩放不敏感，而其他模型（如逻辑回归、SVM等）性能受缩放影响显著。


<details>
  <summary>Details</summary>
Motivation: 解决特征缩放领域缺乏全面研究的问题，为实践者提供优化选择的指导。

Method: 使用12种缩放技术，评估14种算法在16个数据集上的预测性能和计算成本。

Result: 集成方法（如随机森林、XGBoost等）对缩放不敏感，而逻辑回归、SVM等模型性能受缩放影响显著。

Conclusion: 研究为实践者提供了模型特定的特征缩放选择建议，并公开了所有代码和结果以确保透明性和可重复性。

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [185] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: ECON通过将多LLM协调建模为不完全信息博弈，并寻求贝叶斯纳什均衡，显著提升了推理能力，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体框架中计算成本高和缺乏收敛保证的问题。

Method: 将多LLM协调建模为不完全信息博弈，引入ECON框架，结合分布式推理与集中输出。

Result: ECON在六个基准测试中平均表现优于现有方法11.2%，并具有更好的可扩展性。

Conclusion: ECON为更强大的多LLM集成提供了高效且可扩展的解决方案。

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [186] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: AR-Bench是一个新基准，用于评估大语言模型（LLMs）的主动推理能力，发现当前LLMs在主动推理方面表现较差，与被动推理能力存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估被动推理，而主动推理（需要与外部系统交互获取信息）缺乏系统性研究。

Method: 设计了AR-Bench，包含三类任务（侦探案例、情境谜题和猜数字），模拟真实场景并测试常识、逻辑和符号推理能力。

Result: 当代LLMs在主动推理中表现不佳，难以获取或利用必要信息，即使采用高级策略（如树搜索或后训练）改进有限。

Conclusion: 需开发新方法（如交互学习、实时反馈和环境感知目标）提升主动推理能力，AR-Bench已公开。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [187] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: 论文提出H$^2$GFM框架，旨在统一处理同质和异质文本属性图，通过上下文编码和自适应图变换器提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质文本属性图（HoTAGs），而忽略了异质文本属性图（HeTAGs），限制了图基础模型（GFM）的应用范围。

Method: 提出H$^2$GFM框架，利用统一文本空间投影多样元关系，采用上下文编码捕捉空间和高阶语义关系，并设计上下文自适应图变换器（CGT）生成鲁棒节点表示。

Result: 在多种HoTAGs和HeTAGs及学习场景下的实验验证了模型的有效性。

Conclusion: H$^2$GFM框架成功扩展了GFM的能力，适用于更广泛的图类型和任务。

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [188] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: 论文提出了一种可学习的时空位置编码方法L-STEP，解决了现有位置编码在适应性、动态性和计算效率上的不足，并在多个数据集和任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法在复杂属性图、动态拓扑和特征信息以及大规模数据上的计算效率方面存在局限性，需要一种更灵活、高效的方法。

Method: 提出了L-STEP模型，通过可学习的时空位置编码方案，结合MLP的表示能力，替代传统Transformer的注意力机制，同时验证了其理论复杂度和实际运行效率。

Result: L-STEP在13个经典数据集和10种算法中表现出色，并在最新的TGB基准测试中取得领先性能。

Conclusion: L-STEP是一种高效、灵活的位置编码方法，能够适应复杂动态图数据，并在性能和效率上优于现有方法。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [189] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [190] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文解释了离散扩散模型中掩码扩散（masking diffusion）表现优异的原因，并提出了一种新方法SCUD，通过利用离散马尔可夫过程的跳跃时间分布，改进了传统离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 研究离散扩散模型的性能差异，尤其是掩码扩散为何优于其他模型，并探索如何利用离散马尔可夫过程的特性改进模型。

Method: 提出SCUD方法，将跳跃时间分布嵌入到离散扩散模型中，结合图像的归纳偏置、文本和蛋白质数据，构建改进模型。

Result: SCUD方法在图像、文本和蛋白质数据上表现优于掩码扩散。

Conclusion: 通过利用离散马尔可夫过程的跳跃时间分布，SCUD方法能够超越传统离散扩散模型，包括掩码扩散。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [191] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: 本文系统综述了图提示（graph prompting）的最新进展，包括图预训练方法、主流图提示技术及其应用，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 图学习模型在大规模图数据中表现出色，但如何高效适应下游任务仍需探索。图提示作为一种新兴方法，通过可学习提示调整预训练模型，具有潜力。

Method: 1. 介绍代表性图预训练方法；2. 综述主流图提示技术，分析其可学习提示设计；3. 总结图提示在不同领域的应用。

Result: 图提示在多个领域展示了实际应用价值，但仍存在挑战。

Conclusion: 图提示是一个有前景的研究方向，未来需解决现有挑战以进一步推动其发展。

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [192] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: 本文提出了一种简化的理论框架，用于分析DDPM中VP-SDE的Euler--Maruyama离散化误差，并证明了离散噪声可以替代高斯噪声而不影响收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型离散化误差分析通常依赖复杂概率工具，本文旨在简化分析并提供更实用的噪声替代方案。

Method: 利用Grönwall不等式推导收敛速率，并验证离散噪声（如Rademacher或均匀噪声）的可行性。

Result: 实验验证了误差按预测比例缩放，离散噪声与高斯噪声样本质量相当，噪声缩放错误会降低性能。

Conclusion: 本文通过简化分析和离散噪声替代，将理论严谨性与扩散生成模型的实践效率相结合。

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [193] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: 论文提出了一种优化框架，通过将控制权转移给参数化策略，形成自治动力系统，从而无需直接使用动态规划或强化学习工具即可优化策略参数。


<details>
  <summary>Details</summary>
Motivation: 动机是简化策略优化过程，避免直接依赖复杂的动态规划和强化学习方法，同时统一处理策略参数与其他系统参数。

Method: 方法是在自治系统层面推导更简单的算法，证明其能计算与策略梯度、Hessian矩阵、自然梯度等相同的量，并扩展到近似策略迭代和离策略学习。

Result: 结果表明，该框架不仅适用于策略优化，还可用于行为克隆、机制设计、系统辨识、状态估计器学习等任务，甚至生成式AI模型的调优。

Conclusion: 结论是该框架提供了一种更通用的优化方法，概念上比强化学习更接近生成式AI模型的调优。

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [194] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: 提出了一种针对关系数据学习的差分隐私框架，解决了传统DP-SGD在关系学习中的挑战，包括实体参与多关系的高敏感性和多阶段耦合采样的隐私放大问题。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域中保护个体隐私至关重要，但传统DP-SGD难以直接应用于关系数据学习。

Method: 通过敏感性分析和自适应梯度裁剪方案，提出了一种适用于关系数据的DP-SGD变体，并扩展了隐私放大分析。

Result: 实验表明，该方法在文本属性网络关系数据上实现了良好的隐私-效用权衡。

Conclusion: 该框架为关系学习提供了严格的实体级差分隐私保证，具有实际应用潜力。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [195] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct是一种新颖的优化算法，通过根据激活方差调整学习率，提升神经元输出的稳定性，从而改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统激活正则化方法存在局限性，AdaAct旨在通过神经元级适应性提升训练稳定性与泛化能力。

Method: AdaAct通过动态调整学习率以适应激活方差，结合了Adam的快速收敛和SGD的强泛化能力。

Result: 在CIFAR和ImageNet等标准图像分类基准测试中，AdaAct表现优异，且执行时间具有竞争力。

Conclusion: AdaAct成功平衡了收敛速度与泛化能力，为优化算法提供了新的思路。

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [196] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: NysAct是一种可扩展的一阶梯度预处理方法，通过近似激活协方差矩阵作为预处理矩阵，平衡了一阶和二阶优化方法的优势，显著降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 自适应梯度方法计算高效但泛化能力差，二阶方法泛化能力强但计算和内存成本高，NysAct旨在平衡这两者的优势。

Method: 使用特征值偏移的Nystrom方法近似激活协方差矩阵作为预处理矩阵。

Result: 实验表明，NysAct在测试准确率上优于一阶和二阶方法，同时计算资源需求显著低于二阶方法。

Conclusion: NysAct是一种高效且泛化能力强的优化方法，适用于大规模任务。

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [197] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: AlphaFold数据库（AFDB）的结构预测精度高，但用于训练对原子几何敏感的深度学习模型时存在系统性偏差。研究提出DeSAE方法，通过去偏处理提升逆折叠等任务的性能。


<details>
  <summary>Details</summary>
Motivation: AFDB的结构预测虽然准确，但与实验数据（PDB）相比存在几何偏差，影响下游任务的泛化能力。

Method: 提出Debiasing Structure AutoEncoder（DeSAE），通过重构自然构象来学习更稳健的结构分布。

Result: DeSAE处理后的AFDB结构显著提升了逆折叠任务的性能。

Conclusion: 研究揭示了预测结构的系统性偏差问题，并提出去偏框架，为结构学习任务提供了改进方案。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [198] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: 论文提出了一种名为DPSDP的强化学习算法，通过动态规划和直接偏好学习训练LLM系统，以迭代优化答案，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在反馈空间和协调训练方面存在不足，导致性能不佳，因此需要一种更高效的多轮优化方法。

Method: 将多轮优化过程建模为马尔可夫决策过程，并引入DPSDP算法，通过直接偏好学习训练actor-critic LLM系统。

Result: 实验表明，DPSDP在分布内外基准测试中均表现优异，例如在MATH 500基准上准确率从58.2%提升至63.2%。

Conclusion: DPSDP通过多智能体协作和分布外泛化，显著提升了LLM的推理能力。

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [199] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的恶意流量检测方法，通过处理数据集不平衡问题并比较多种技术，发现结合重采样策略和集成方法（如gcForest）能显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）网络的快速扩展，实时检测恶意流量成为关键网络安全挑战。

Method: 使用IoT-23数据集，通过三种重采样策略处理类别不平衡问题，并比较多种机器学习技术。

Result: 结合不平衡处理技术和集成方法（如gcForest）的检测性能优于传统方法。

Conclusion: 该研究为物联网环境开发更智能、高效的自动化威胁检测系统提供了重要贡献，同时优化了计算资源使用。

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [200] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: 论文提出了一种新的框架Reinforcement-Learned Teachers (RLTs)，通过训练教师模型来优化下游蒸馏效果，避免了传统强化学习的探索挑战。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习训练推理语言模型依赖于初始探索能力，而RLTs专注于为下游学生模型提供有效的蒸馏内容。

Method: RLTs通过问题和解决方案生成详细解释，并使用密集奖励训练，奖励基于学生对解释的理解。

Result: 7B RLT在竞赛和研究生级任务中表现优于现有蒸馏方法，且适用于更大学生模型和零样本任务。

Conclusion: RLTs提高了RL推理框架的效率和可重用性。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [201] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: 论文探讨了利用深度学习和数据增强技术解决气旋快速增强检测中的类别不平衡问题，并验证了空间坐标作为输入特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 气旋快速增强是一种极端且罕见的事件，导致数据集类别不平衡，传统机器学习模型难以处理。

Method: 结合深度学习、集成学习和数据增强框架，生成模拟气旋的空间坐标和风强度数据，并使用深度学习模型进行分类。

Result: 数据增强显著提升了快速增强检测的准确性，空间坐标是关键输入特征。

Conclusion: 研究为时空数据中极端事件的合成数据生成提供了新思路。

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [202] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: 论文提出了一种基于模糊集的集合表示学习方法FUSE，用于分类扩展任务，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将集合建模为向量或几何对象，但这些方法不支持集合操作。模糊集能更好地建模语义概念的不确定性和信息量。

Method: 提出FUSE框架，通过模糊集的体积近似实现集合表示学习，支持所有集合操作且高效学习。

Result: 在分类扩展任务中，FUSE比现有基线提升高达23%。

Conclusion: FUSE是首个高效计算模糊集嵌入的方法，为集合表示学习提供了新思路。

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [203] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和无监督异常生成方法的新框架SGDA，用于三相电机的智能诊断，显著提升了诊断性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法主要依赖签名分析，虽为标准实践，但结合先进ML技术可进一步提升性能。

Method: 提出SGDA框架，结合ML算法和无监督异常生成方法，利用电机物理模型在健康电流信号的频域中合成物理合理的故障。

Result: 该方法在诊断准确性和可靠性上表现优异，具有广泛的工业应用潜力。

Conclusion: SGDA框架为电机诊断领域提供了高效、稳健的解决方案，具有重要的实际应用价值。

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [204] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 论文探讨了数据重用如何在线性回归中改进现有的缩放规律，通过多轮随机梯度下降（multi-pass SGD）实现更低的测试误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决大规模语言模型在数据不足时无法持续缩放的问题，探索数据重用的潜力。

Method: 方法是通过多轮SGD训练M维线性模型，假设数据协方差和真实参数具有特定的幂律谱特性。

Result: 结果显示多轮SGD的测试误差优于单轮SGD，验证了数据重用的有效性。

Conclusion: 结论是数据重用可以在数据受限的情况下改进缩放规律。

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [205] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新方法（SQOG），通过平滑贝尔曼算子（SBO）增强Q函数在分布外（OOD）区域的泛化能力，解决了离线强化学习中Q值高估问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，分布偏移导致Q值对OOD动作的高估，现有方法因过度约束限制了Q函数的泛化能力。

Method: 在凸包及其邻域（CHN）内，提出平滑贝尔曼算子（SBO），通过平滑OOD Q值与邻近样本Q值来更新Q值。

Result: SQOG算法在D4RL基准测试中表现优于现有方法，实现了接近准确的Q值估计。

Conclusion: SQOG通过SBO有效缓解了过度约束问题，提升了Q值估计的准确性和策略改进效果。

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [206] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: GALA框架动态调整学习率，通过梯度对齐和局部曲率估计，无需繁琐的超参数调优，提升优化器性能。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型中，学习率的精细调优对优化器性能至关重要，但传统方法需要大量网格搜索，效率低下。

Method: 提出GALA框架，动态调整学习率，基于连续梯度的对齐和局部曲率估计，并将其建模为一维在线学习问题。

Result: GALA在平滑非凸设置下为归一化SGD提供数据自适应收敛率，实验显示其与SGD和Adam结合时表现稳健且无需调优。

Conclusion: GALA为学习率调整提供了一种自适应且高效的方法，显著减少超参数调优的需求。

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [207] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: HASFL框架通过自适应控制批次大小和模型分割，解决了分裂联邦学习中的异构性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有分裂联邦学习方法因边缘设备能力异构性导致性能下降，需解决这一问题。

Method: 提出HASFL框架，通过收敛边界分析自适应控制批次大小和模型分割，平衡通信计算延迟与训练收敛。

Result: 实验验证HASFL在异构边缘网络中的有效性，优于现有基准方法。

Conclusion: HASFL通过自适应优化，显著提升了分裂联邦学习在异构环境中的性能。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [208] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）中的隐私风险，特别是梯度泄漏攻击（GLAs）在实际FL环境中的有效性。作者提出FedLeak，通过部分梯度匹配和梯度正则化技术，成功在现实条件下重建数据，揭示了FL系统的重大漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管FL旨在保护隐私，但梯度泄漏攻击（GLAs）的存在引发了争议。本文旨在证明GLAs在实际FL环境中仍能有效重建数据，填补研究空白。

Method: 作者提出FedLeak，结合部分梯度匹配和梯度正则化技术，优化梯度匹配问题，并通过实际评估协议验证其性能。

Result: FedLeak在现实FL环境中实现了高保真数据重建，表明FL系统存在严重漏洞。

Conclusion: FL系统存在显著隐私风险，亟需更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [209] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAC的高效二阶优化方法，通过近似Kronecker因子降低计算负担，适用于Transformer的注意力层，并在精度、训练时间和内存使用上优于KFAC等方法。


<details>
  <summary>Details</summary>
Motivation: 二阶优化方法（如KFAC）虽然收敛性好，但计算成本高。本文旨在通过分析Fisher信息矩阵的Kronecker因子，提出高效近似方法。

Method: 基于对激活和预激活梯度Kronecker因子特征谱的观察，提出高效近似方法MAC，并将其应用于Transformer的注意力层。

Result: MAC在多种网络架构和数据集上表现优于KFAC及其他先进方法，精度更高，训练时间和内存占用更少。

Conclusion: MAC是一种高效且通用的二阶优化方法，适用于复杂网络结构（如Transformer），并在理论和实验上均表现出色。

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [210] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种名为AsFT的安全微调方法，通过正则化项抑制有害方向的更新，显著减少有害行为并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在微调过程中容易受到安全风险的影响，少量恶意或无害数据可能破坏模型的安全性。

Method: 基于对齐方向的概念，提出AsFT方法，通过正则化项将微调约束在安全区域内。

Result: 实验表明，AsFT优于Safe LoRA，减少有害行为7.60%，提升模型性能3.44%。

Conclusion: AsFT在保持模型安全性的同时显著提升了性能，为LLM的安全微调提供了有效解决方案。

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [211] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种基于热力学的潜在空间动力学识别框架（tLaSDI），用于参数化非线性动力系统的降阶建模，结合自编码器和参数化GENERIC形式神经网络（pGFINNs），在保持热力学原理的同时高效学习参数化潜在动力学。通过物理信息主动学习策略提升性能，数值实验显示显著加速和成本降低。


<details>
  <summary>Details</summary>
Motivation: 解决参数化非线性动力系统的高效建模问题，同时确保热力学原理（如自由能守恒和熵生成）在参数空间中的保持。

Method: 结合自编码器降维和参数化GENERIC形式神经网络（pGFINNs），采用物理信息主动学习策略，通过残差误差指标自适应采样训练数据。

Result: 在Burgers方程和1D/1V Vlasov-Poisson方程上实现高达3,528倍加速，相对误差1-3%，训练和推理成本分别降低50-90%和57-61%。

Conclusion: tLaSDI框架高效且准确，揭示了系统的潜在热力学行为，为物理空间动力学提供了新见解。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [212] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 提出了一种基于抽象-精炼技术的高效方法，用于计算神经网络预测的可证明充分解释，解决了现有方法在可扩展性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释性方法缺乏形式化保证且计算效率低，需要一种既能提供可证明保证又能高效计算的方法。

Method: 通过构建大幅简化的神经网络抽象，逐步精炼网络规模，以高效验证解释的充分性。

Result: 实验表明，该方法显著提高了获取可证明充分解释的效率，并提供了对网络预测的细粒度解释。

Conclusion: 该方法在保证解释形式化可靠性的同时，显著提升了计算效率，为神经网络解释提供了新思路。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [213] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: 论文提出了SHAMs和DiffGradCAM，分别用于测试和提升CAM方法的鲁棒性，解决了传统CAM在对抗条件下的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 传统CAM方法仅关注单个logits，而softmax输出的概率估计依赖于logits之间的差异，这导致CAM容易受到对抗性攻击（如被动欺骗）。

Method: 提出SHAMs作为对抗性基准，并设计DiffGradCAM，一种轻量级且对比性的CAM方法，能够抵抗被动欺骗。

Result: 实验验证了SHAMs和DiffGradCAM在多类任务中的有效性，DiffGradCAM在非对抗情况下与传统CAM方法一致。

Conclusion: SHAMs和DiffGradCAM为基于显著性的解释方法提供了新的鲁棒性评估和改进框架。

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [214] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: ML4CFD竞赛展示了机器学习在流体动力学模拟中的潜力，部分模型性能超越传统求解器。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在科学计算中精度、泛化性和物理一致性不足的问题。

Method: 组织ML4CFD竞赛，提供OpenFOAM生成的数据集，多标准评估模型性能。

Result: 顶级模型在综合指标上优于OpenFOAM求解器。

Conclusion: 机器学习替代模型在特定条件下可超越传统方法，需优化评估框架。

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [215] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: 论文探讨了在大学习率下梯度下降（GD）优化神经网络时的动态行为，发现学习率在特定范围内时，GD会从纯利用型转变为探索-利用平衡型，且训练时间最短。


<details>
  <summary>Details</summary>
Motivation: 研究传统梯度下降算法在大学习率下的动态行为，探索如何通过调整学习率加速神经网络训练。

Method: 通过分析神经网络轨迹的最大Lyapunov指数，表征学习率对训练动态的影响，并在MNIST分类任务中验证。

Result: 特定学习率范围内，GD表现出探索-利用平衡，训练时间最短，且结果适用于多种任务和架构。

Conclusion: 研究表明，瞬态混沌动态在神经网络训练中具有建设性作用，可通过优化学习率加速训练。

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [216] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: EMNAS利用遗传算法优化自动驾驶强化学习的神经网络架构，通过并行化和师生方法提升效率，实验证明其性能优于人工设计模型。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中大规模强化学习的网络架构优化问题，提升奖励并减少模型规模。

Method: 采用遗传算法自动化网络设计，结合并行化和师生方法加速搜索并确保可扩展优化。

Result: 实验结果显示，EMNAS在奖励和参数效率上优于人工设计模型。

Conclusion: EMNAS为自动驾驶强化学习提供了高效网络架构优化方案，推动实际应用发展。

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [217] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm是首个专为通信系统制定设计的推理LLM，通过两阶段训练策略和开源数据集CSFRC，显著优于现有通用LLM。


<details>
  <summary>Details</summary>
Motivation: 现有通用LLM缺乏通信系统制定所需的领域知识和推理能力，亟需专用模型。

Method: 采用两阶段训练：SFT+CoT数据蒸馏领域知识，C-ReMax强化学习提升推理能力。

Result: 实验表明DeepForm性能领先，优于大型专有LLM。

Conclusion: DeepForm为通信系统制定提供了高效解决方案，相关资源将开源以推动研究。

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [218] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 研究发现，尽管LLMs的激活模式可用于区分答案正确性，但这种“真理几何”是任务依赖的，无法跨任务迁移。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs激活模式是否具有跨任务的通用性，以验证其可靠性。

Method: 通过线性分类器分析不同任务中LLMs的激活模式，并尝试更复杂的方法（如混合探针）。

Result: 线性分类器在不同任务间相似性低，稀疏正则化后支持集几乎不重叠，复杂方法也无法解决。

Conclusion: LLMs的“真理几何”是任务特定的，跨任务通用性有限。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [219] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: SLEEPYLAND是一个开源睡眠分期评估框架，旨在解决模型评估、泛化、偏见和标注差异问题，包含大量ID和OOD数据，并提出了SOMNUS集成模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在睡眠分期中的应用受限于模型评估、泛化能力、偏见和标注差异，需要标准化框架提升临床采纳率。

Method: 提出SLEEPYLAND框架，包含大量ID和OOD数据，并开发SOMNUS集成模型，通过软投票结合多种架构和通道配置。

Result: SOMNUS在24个数据集中表现稳健，优于个体模型和现有方法，甚至超越人类评分者，并量化了模型偏见。

Conclusion: SLEEPYLAND和SOMNUS为睡眠分期提供了标准化评估和高效模型，但需进一步解决偏见问题。

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [220] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: 提出一种基于生成式AI的深度学习模型，用于提升污水系统上下文预测的准确性，结合扩散模型和保形推理技术，确保预测的统计可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过捕捉复杂的环境信号相关性，提升极端天气下污水系统预测的准确性。

Method: 开发扩散模型处理多元时间序列数据，并采用保形推理技术校准预测区间。

Result: 实证测试显示模型在极端天气下仍能保持高准确性和可靠性。

Conclusion: 该模型为污水系统提供了可靠的上下文预测工具，适用于复杂环境条件。

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [221] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: Transformer模型通过深度学习学习符号计算，CALT库为非专家提供训练模型的工具。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在符号计算中的应用，为非专家提供便捷工具。

Method: 使用Transformer模型训练符号计算的序列到序列函数。

Result: 开发了CALT库，支持非专家训练符号计算模型。

Conclusion: CALT为符号计算社区提供了新的研究方向和实践工具。

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [222] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: PBFM是一种新的生成框架，通过显式嵌入物理约束（PDE残差和代数关系）到流匹配目标中，显著提高了物理残差的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法（如扩散模型和流匹配）通常隐式学习物理规律，缺乏显式物理约束，限制了准确性。

Method: 提出PBFM框架，结合流匹配损失和基于物理的残差损失，引入时间展开训练和随机采样策略。

Result: 在三个PDE问题上，PBFM比FM的物理残差准确性提高8倍，且在分布准确性上优于现有算法。

Conclusion: PBFM为物理和工程应用中的代理建模、不确定性量化和加速模拟提供了高效且原则性的框架。

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [223] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: 论文提出了一种名为CASE的高效样本选择方法，通过将示例选择问题建模为多臂老虎机问题，显著减少了LLM的调用次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习范式中，如何高效选择少量示例（few-shot）是关键，但现有方法在样本复杂度和计算成本上存在挑战。

Method: 将示例选择问题建模为多臂老虎机问题，提出CASE方法，通过维护“挑战者”臂列表，减少样本复杂度和LLM调用次数。

Result: CASE方法在运行时实现了7倍加速，LLM调用次数减少87%，且性能不下降。

Conclusion: CASE是一种高效且实用的示例选择方法，适用于资源受限的场景。

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [224] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: HSG-12M是首个大规模空间多重图数据集，保留了节点间多条几何路径，为几何感知图学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有图基准假设边是非空间且简单的，忽略了物理上不同的路径。HSG-12M填补了这一空白。

Method: 通过Poly2Graph管道，将一维晶体哈密顿量映射为谱图，生成静态和动态空间多重图。

Result: HSG-12M包含1160万静态和510万动态谱图，展示了多边几何学习的新挑战。

Conclusion: 谱图作为多项式、向量和矩阵的通用拓扑指纹，为数据驱动的科学发现提供了新机会。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [225] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: TiViT将时间序列转换为图像，利用预训练的视觉Transformer（ViT）提升时间序列分类性能，并在标准基准测试中达到最优表现。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在医疗和工业中很重要，但公开数据集稀缺限制了时间序列基础模型的发展。

Method: 提出TiViT框架，将时间序列转为图像，利用预训练的ViT模型提取特征。

Result: TiViT在标准时间序列分类任务中表现最优，且与时间序列基础模型的特征互补。

Conclusion: TiViT展示了视觉表征在非视觉领域的复用潜力。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [226] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: DICE框架用于解决策略诱导的稳态分布与目标分布不匹配问题，但近期改进方法削弱了其离线策略评估能力。本文提出新方法，通过半梯度优化实现准确成本估计和约束强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决DICE框架在离线约束强化学习中因半梯度优化导致的成本估计失败问题。

Method: 提出一种新方法，通过半梯度DICE实现离线策略评估和约束强化学习。

Result: 新方法在DSRL基准测试中实现了最先进的性能，并确保准确成本估计。

Conclusion: 通过半梯度DICE优化，成功解决了成本估计问题，提升了离线约束强化学习的性能。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [227] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: RP-KrossFuse提出了一种通过随机投影和Kronecker乘积融合跨模态和单模态嵌入的方法，旨在在保持跨模态对齐的同时提升单模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态嵌入（如CLIP）在跨模态对齐上表现良好，但在单模态任务上不如专用嵌入；而单模态嵌入缺乏跨模态能力。研究旨在统一两者优势。

Method: RP-KrossFuse利用随机投影和Kronecker乘积融合嵌入，通过核空间高效操作，支持可扩展实现（如随机傅里叶特征）。

Result: 实验表明，RP-KrossFuse在单模态任务上表现接近专用嵌入，同时保持跨模态对齐能力。

Conclusion: RP-KrossFuse成功弥合了跨模态与单模态嵌入的差距，兼具两者优势。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [228] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: JoFormer是一种基于非交换代数的Transformer架构，通过可学习的定向变换表示相对位置，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer中位置信息有效整合的挑战。

Method: 提出JoFormer，基于非交换代数设计，通过序列组合变换表示位置。

Result: 在Tiny Shakespeare任务中，JoFormer表现优于RoFormer，困惑度更低、收敛更快。

Conclusion: JoFormer为Transformer位置信息整合提供了更表达性强的方法，具有潜力。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [229] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: 研究发现，简单的k-NN基线方法在网络流量分类任务中表现优异，原因是数据集中存在大量冗余样本，导致复杂模型的性能被高估。


<details>
  <summary>Details</summary>
Motivation: 探讨为何简单的k-NN方法在网络流量分类中表现优于复杂神经网络，并揭示数据集冗余对模型评估的影响。

Method: 在12个数据集和15个分类任务中评估k-NN基线方法，分析数据集冗余及其对性能的影响。

Result: 多数数据集包含超过50%的冗余样本，导致模型性能被高估，且标准机器学习方法可能不适用于流量分类。

Conclusion: 提出新的任务定义和评估方向，以解决数据集冗余问题并优化流量分类研究。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [230] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: ChannelTokenFormer是一种基于Transformer的模型，用于处理多变量时间序列数据中的通道依赖、异步采样和缺失值问题，表现出优越的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列数据存在通道依赖、异步采样和缺失值等复杂问题，而现有模型通常基于过于简化的假设，无法应对这些挑战。

Method: 提出ChannelTokenFormer，一种灵活的Transformer架构，能够显式捕捉跨通道交互、适应通道异步采样并有效处理缺失值。

Result: 在三个基准数据集和一个真实工业数据集上的实验表明，ChannelTokenFormer在复杂现实条件下具有优越的鲁棒性和准确性。

Conclusion: ChannelTokenFormer为解决多变量时间序列数据中的实际问题提供了一种有效且可靠的解决方案。

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [231] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: 论文提出了一种针对变分自编码器图像压缩的额外微调训练步骤，通过在推理阶段对量化潜在表示进行再训练，提升了编码效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法在训练过程中对量化的近似处理不准确，导致网络性能次优，尤其是在熵约束量化器中。

Method: 在常规端到端训练后，对网络的部分进行基于量化潜在表示的再训练。

Result: 在Kodak和TecNick测试集上，平均比特率节省1%至2.2%。

Conclusion: 提出的微调方法显著提升了编码效率，且不增加推理复杂度。

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [232] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出了一种通过LLM生成的蓝图增强小型语言模型（SLM）推理能力的框架，并优化提示模板以减少SLM对提示变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: SLM因容量有限导致推理能力不足且对提示变化敏感，需一种轻量级解决方案。

Method: 利用LLM生成结构化推理蓝图，并集成提示模板搜索机制。

Result: 在数学、编程和逻辑推理任务中显著提升SLM性能，无需增加模型规模或额外训练。

Conclusion: 该框架为资源受限环境提供了高效、轻量的SLM增强方案。

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [233] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: 该论文研究了共识聚类问题，结合公平聚类的视角，提出了一种确保数据集中每个受保护组在聚类中比例表示的方法，并提供了常数因子近似算法。


<details>
  <summary>Details</summary>
Motivation: 共识聚类是机器学习和数据分析中的基础任务，但现有方法未考虑公平性。论文旨在解决这一问题，确保聚类结果不仅具有代表性，还能公平对待受保护属性。

Method: 论文提出了一种最小化修改现有聚类以强制公平性的算法，针对不同组比例的场景设计了近似算法，并证明了问题的NP难性。

Result: 论文提供了针对等比例组的最优算法，以及针对不等比例组的近线性时间常数因子近似算法，同时证明了问题的NP难性。

Conclusion: 该研究填补了公平共识聚类的空白，其方法可能对其他聚类问题的公平变体具有广泛影响。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [234] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: 本文提出了一种基于重要性采样的方法（IS-DAAs）来解决离线直接对齐算法（DAAs）中的过优化问题，通过引入重要性比率并限制其最大值，有效降低了过优化风险，并在实验中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 直接对齐算法（如DPO）在训练过程中容易出现过优化问题，导致模型性能下降，因此需要一种方法来缓解这一问题。

Method: 提出IS-DAAs方法，通过引入重要性比率并限制其最大值，减少过优化风险。

Result: 实验表明，IS-DAAs能有效缓解过优化问题，尤其在低正则化强度下表现更优。

Conclusion: IS-DAAs是一种有效的解决方案，能够提升直接对齐算法的性能，并公开了实现代码。

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [235] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分自编码器（VAE）的VAE-LF模型，用于高效表示和补充高维不完整（HDI）电力负荷监测（PLM）数据，提升电力负荷预测（PLF）性能。


<details>
  <summary>Details</summary>
Motivation: 智能电网中高维不完整的PLM数据对PLF模型性能提出了挑战，需要一种高效的数据表示和补充方法。

Method: VAE-LF通过编码器-解码器结构学习数据的低维潜在表示，将HDI PLM数据分块输入模型并生成补充数据。

Result: 在UK-DALE数据集上，VAE-LF在5%和10%稀疏度测试中表现优于其他基准模型，RMSE和MAE显著降低，尤其在低稀疏度数据上表现突出。

Conclusion: VAE-LF为智能电网中的电力负荷管理提供了一种高效的数据补充解决方案。

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [236] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: 论文提出利用LLM基准数据估算推理碳排放的框架R-ICE，解决现有工具的高数据需求、侵入性和误差问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速普及对能源和环境造成负担，现有碳排放估算工具存在不足，需要更实用的解决方案。

Method: 提出R-ICE框架，利用SOTA基准数据估算提示级推理碳排放，实现非侵入性估算。

Result: 验证结果表明基于基准的建模在碳排放估算中潜力巨大。

Conclusion: R-ICE为动态LLM路由和碳核算等新兴用例提供了实用方法，值得进一步研究。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [237] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: RRP是一种新的强化学习探索策略，通过向环境奖励添加零均值噪声增强策略多样性，提升探索效果。


<details>
  <summary>Details</summary>
Motivation: 现有探索策略（如动作扰动）可能不足，RRP通过奖励扰动补充探索效果。

Method: 在环境奖励中添加零均值噪声，与动作扰动策略兼容。

Result: 显著提升PPO和SAC的性能，提高样本效率并避免局部最优。

Conclusion: RRP是一种轻量级、通用的探索策略，可无缝集成到现有RL算法中。

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [238] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: 提出了一种多视图、多输出的GNN模型，结合政府检查数据和众包报告数据预测城市事件，并在纽约市案例中验证其优于单一数据源模型。


<details>
  <summary>Details</summary>
Motivation: 解决城市事件预测中政府检查数据稀疏和众包报告数据偏差的问题。

Method: 使用多视图、多输出的GNN模型，整合政府检查数据和众包报告数据。

Result: 模型在真实和半合成数据上表现优于单一数据源模型，并量化了众包报告的偏差。

Conclusion: 该方法为利用异构、稀疏和偏差数据进行潜在状态预测提供了广泛适用的解决方案。

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [239] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: 本文提出了一种适用于稀疏性和非独立同分布权重的深度神经网络稳定性定理，扩展了现有理论。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络中梯度爆炸或消失的问题，特别是针对稀疏性和非独立同分布权重的网络。

Method: 利用随机矩阵理论的最新进展，建立了一个广义的稳定性定理。

Result: 为具有结构化和依赖性随机性的现代神经网络提供了严格的光谱稳定性保证。

Conclusion: 扩展了初始化方案的理论基础，适用于更广泛的网络模型。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [240] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Main category: cs.LG

TL;DR: 提出了一套设计模式，用于构建具有可证明抵抗提示注入攻击能力的AI代理，并分析了其效用与安全性的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI代理能力增强，其安全性成为关键挑战，尤其是提示注入攻击的威胁。

Method: 提出了一套设计模式，系统分析其效用与安全性的权衡，并通过案例研究展示实际应用。

Result: 设计模式为AI代理提供了可证明的抵抗提示注入攻击的能力。

Conclusion: 该研究为构建安全的AI代理提供了实用的设计原则和方法。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [241] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 该研究利用世界银行的公开合成数据集IMAGIC-500，为社会经济数据中的缺失值填补提供了全面的基准测试，评估了多种填补方法的性能。


<details>
  <summary>Details</summary>
Motivation: 社会经济数据因隐私保护难以公开共享，导致缺失值填补研究的可重复性和基准测试受限。

Method: 使用IMAGIC-500数据集，在不同缺失机制（MCAR、MAR、MNAR）和缺失比例（10%-50%）下评估统计、传统机器学习和深度学习填补方法。

Result: 研究揭示了各种填补方法的优缺点，包括扩散模型等最新技术。

Conclusion: IMAGIC-500数据集和基准测试旨在促进稳健填补算法的开发和社会科学研究的可重复性。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [242] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: 论文提出了一种敏捷强化学习（aRL）方法，用于优化边缘计算中软实时应用的任务调度，通过智能探索和动作屏蔽提高效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算环境中任务调度复杂度高，传统启发式和元启发式算法难以适应动态环境，而强化学习算法学习时间长，探索效率低。

Method: 提出aRL方法，结合智能探索和动作屏蔽技术，减少无关动作的随机探索，提升RL-agent的预测性和适应性。

Result: 实验表明aRL在命中率和收敛速度上优于基线方法。

Conclusion: aRL是一种适用于边缘计算中软实时应用任务调度的有效方法。

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [243] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: SG-GNN通过构建具有更高标签同质性的新图结构，并结合多图视图，提升了GNN在异质性数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN假设节点间具有同质性，但在异质性数据（连接节点标签不同）上表现不佳，因此需要改进。

Method: 提出SG-GNN，通过链接具有相似结构属性的节点创建新图，并结合原始图和多图视图，自适应学习权重。

Result: 在异质性数据集上，SG-GNN达到或接近最优性能。

Conclusion: 利用结构信息指导GNN能有效提升其在异质性数据上的表现。

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [244] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: 论文探讨了数据插补技术在水分配网络监测中的应用，比较了多种方法的效果。


<details>
  <summary>Details</summary>
Motivation: 智能水表数据存在缺失，影响决策效率，需通过插补技术提升数据质量。

Method: 比较了k-Nearest Neighbors、MissForest、Transformers和Recurrent Neural Networks等插补方法。

Result: 有效的数据插补显著提升了水消耗数据的准确性和可靠性。

Conclusion: 数据插补技术可优化漏损检测和预测性维护等应用。

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [245] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: InfoDPCCA是一种动态概率CCA框架，用于提取两个相关序列的共享潜在表示，同时保留各自序列的特定信息。


<details>
  <summary>Details</summary>
Motivation: 解决高维序列数据中提取有意义潜在表示的挑战，应用于自然科学和工程领域。

Method: 采用信息论目标提取共享潜在表示，结合两步训练方案和残差连接机制。

Result: 在合成和医学fMRI数据实验中表现优异。

Conclusion: InfoDPCCA是一种有效的表示学习工具，提高了可解释性和鲁棒性。

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [246] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: SeerAttention-R是一个稀疏注意力框架，专为推理模型的长解码设计，通过自蒸馏门控机制学习注意力稀疏性，并优化解码性能。


<details>
  <summary>Details</summary>
Motivation: 解决长解码推理模型中注意力机制的计算效率问题，同时保持推理准确性。

Method: 扩展SeerAttention，移除查询池化以适应自回归解码，引入轻量级门控插件，无需修改预训练模型参数。

Result: 在AIME基准测试中，仅用0.4B token训练，4K token预算下保持接近无损的推理准确性；优化解码内核在H100 GPU上实现9倍加速。

Conclusion: SeerAttention-R高效灵活，适用于长解码任务，显著提升计算效率。

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [247] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 该论文提出了一种基于流匹配的意图条件流占用模型（InFOM），用于预测智能体在长期未来访问的状态，并通过潜在变量捕捉用户意图，显著提升了强化学习的预训练效果。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在机器学习中取得了成功，但在强化学习（RL）中仍面临长期依赖的挑战。本文旨在通过生成式AI的新工具，解决RL中的样本效率和鲁棒性问题。

Method: 提出了一种概率模型，使用流匹配预测智能体未来状态的占用度量，并引入潜在变量捕捉用户意图，以增强模型表达能力。

Result: 在36个基于状态和4个基于图像的基准任务中，InFOM实现了1.8倍的中位数回报提升和36%的成功率增加。

Conclusion: InFOM通过意图条件和流匹配技术，显著提升了强化学习预训练的效果，为RL的长期依赖问题提供了有效解决方案。

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [248] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: 论文提出两种多实验方程学习方法（ME-EQL），通过减少模拟需求提高从ABM数据中学习连续模型的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: ABM计算密集且难以解析，传统EQL方法需要大量模拟，泛化性不足。

Method: 提出OAT ME-EQL（逐个参数集学习并插值）和ES ME-EQL（统一模型库），应用于出生-死亡模型和空间结构ABM。

Result: 两种方法显著降低参数恢复误差，OAT ME-EQL在参数空间泛化性更优。

Conclusion: ME-EQL方法通过多实验学习提升复杂生物系统模型的泛化性和可解释性。

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [249] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: 论文提出了一种新的局部特征重要性方法LMDI+，用于解释树集成模型的预测，相比现有方法LIME和TreeSHAP，性能提升10%，且更稳定。


<details>
  <summary>Details</summary>
Motivation: 树集成模型在高风险领域广泛应用，但现有局部特征重要性方法（如LIME和TreeSHAP）依赖近似且不稳定，无法满足解释需求。

Method: 扩展全局MDI+方法到局部场景，提出LMDI+，利用决策树与线性模型在节点基础上的等价性。

Result: 在12个真实数据集上，LMDI+性能优于基线方法10%，且特征重要性排名更稳定。

Conclusion: LMDI+为局部可解释性提供了新工具，支持反事实识别和子群发现等应用。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [250] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: BioLangFusion通过整合预训练的DNA、mRNA和蛋白质语言模型，提出了一种简单的统一分子表示方法。


<details>
  <summary>Details</summary>
Motivation: 受分子生物学中心法则（基因到转录本到蛋白质的信息流）启发，旨在实现跨模态的直接对应。

Method: 采用三种标准融合技术：密码子级嵌入拼接、熵正则化注意力池化和跨模态多头注意力。

Result: 在五个分子属性预测任务中，BioLangFusion优于单模态基线。

Conclusion: 即使简单的预训练模型融合也能以最小开销捕获互补的多组学信息。

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [251] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: KARMA提出了一种自适应时间通道分解模块（ATCD）和混合频率-时间分解模块（HFTD），结合多尺度Mamba-based KarmaBlock，显著提升了多元长期时间序列预测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分解方法固定且单一，无法挖掘潜在信息；Transformer模型因计算复杂度高难以处理长序列和复杂动态关系。

Method: 引入ATCD动态提取趋势和季节性成分，HFTD进一步分解序列为频域和时域，结合多尺度KarmaBlock协调处理全局和局部信息。

Result: 在八个真实数据集上，KARMA在预测准确性和计算效率上显著优于主流基线方法。

Conclusion: KARMA通过动态分解和高效建模，解决了传统方法和Transformer模型的局限性，为多元长期时间序列预测提供了有效解决方案。

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [252] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习（DRL）中环境状态扰动的对抗攻击与防御，提出了一种名为BAT的防御框架，显著提升了代理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注环境状态扰动，而这是实际场景中的常见问题，因此需要提升DRL代理的鲁棒性。

Method: 提出了一种非目标攻击方法作为校准对手，并设计了BAT防御框架，结合监督学习和对抗训练。

Result: 实验表明主流代理在环境状态扰动下脆弱，BAT能显著提升鲁棒性。

Conclusion: BAT框架在多种情况下有效增强了代理对环境状态扰动的鲁棒性。

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [253] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: 提出了一种数据增强和扩展框架，通过偏好细化和多级直接偏好优化（M-DPO），使小数据集训练的生成奖励模型达到与大规模数据集相当的性能。


<details>
  <summary>Details</summary>
Motivation: 提高强化学习从人类反馈（RLHF）的效率和可扩展性，解决传统方法在样本配对和数据多样性上的不足。

Method: 结合偏好细化（使用Chain-of-Thought采样）、基于困惑度的评分机制和多级直接偏好优化（M-DPO）。

Result: 实验表明，该方法显著提升了数据效率和模型性能，小数据集训练的模型可媲美大规模数据集的结果。

Conclusion: 数据高效策略在奖励模型优化中具有潜力，为低资源RLHF应用提供了稳健解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [254] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: 论文提出了一种基于高斯过程生成的新数据集和TimeFlex模型，旨在揭示时间序列特性与模型性能的关联。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在时间序列预测中表现优异，但缺乏对数据特性与模型架构匹配性的系统研究。

Method: 使用高斯过程生成具有明确特性的数据集，并开发模块化架构的TimeFlex模型。

Result: TimeFlex模型在多样化时间序列条件下表现优于现有先进模型。

Conclusion: 研究为时间序列特性与模型选择提供了明确指导，TimeFlex模型展现了更强的适应性。

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [255] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: 研究了三种神经网络架构（Transformers、GCNs和LSTMs）在命题逻辑任务中的泛化能力，发现其对未见过模式（尤其是否定操作）的泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否能学习和表示符号规则，特别是在逻辑推理任务中的表现。

Method: 使用扩展的逻辑公式数据集，评估三种模型在生成满足分配任务中的泛化能力。

Result: 所有模型在训练分布内表现良好，但对未见过模式（特别是涉及否定的组合）泛化能力差。Transformers需引入结构偏置才能正确应用否定。

Conclusion: 标准架构在系统学习逻辑操作表示方面存在局限性，需要更强的归纳偏置以支持基于规则的推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [256] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: TabPFNv2是一种新兴的表格深度学习基础模型，本文研究了其微调策略及内部机制变化，发现全微调是最优方法，并揭示了其成功的原因。


<details>
  <summary>Details</summary>
Motivation: 探索TabPFNv2基础模型在表格数据上的最优微调方法及其内部机制变化，填补现有研究的空白。

Method: 系统评估多种微调策略，分析微调对TabPFNv2内部机制的影响，类比检索增强模型。

Result: 全微调在时间和效果上最优，微调后模型能更准确地加权上下文样本，提升性能。在50K规模数据集上表现良好，学术数据集上达到SOTA。

Conclusion: 全微调是TabPFNv2的最优策略，但在某些复杂数据集上稳定性不足，传统方法仍具优势。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [257] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 论文提出了一种自我感知弱点驱动的问题合成框架（SwS），通过识别模型在强化学习中的弱点并生成针对性问题，显著提升了模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖高质量问题集，但人工标注的数学问题稀缺且合成数据集效率低，限制了模型性能提升。

Method: SwS框架通过识别模型在训练中持续失败的问题，提取核心概念并合成新问题，针对性增强模型弱点。

Result: 在不依赖外部知识蒸馏的情况下，SwS框架在7B和32B模型上平均性能分别提升10.0%和7.7%。

Conclusion: SwS框架通过自我识别和解决弱点，显著提升了模型在复杂推理任务中的泛化能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [258] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 论文提出了一种新方法BranchSBM，用于解决现有生成模型在预测多路径轨迹时的局限性，能够捕捉从共同起源到多个不同结果的分支演化。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如流匹配和Schrödinger Bridge Matching）只能建模单一路径，无法处理分支或发散的多模态演化问题。

Method: 提出了Branched Schrödinger Bridge Matching (BranchSBM)，通过参数化多个时间依赖的速度场和增长过程，实现多终端分布的分支建模。

Result: BranchSBM在多路径表面导航、细胞命运分叉建模和细胞响应模拟等任务中表现出更强的表达能力。

Conclusion: BranchSBM为解决多路径生成问题提供了更灵活和有效的框架。

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [259] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种新的重要性分数外推框架，仅需在小数据子集上训练即可预测整个数据集的样本重要性，解决了现有数据剪枝技术需全量训练的局限性。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集训练机器学习模型计算成本高，现有数据剪枝技术需全量训练，效率低下。

Method: 引入重要性分数外推框架，基于小数据子集训练，采用k近邻和图神经网络预测全量数据重要性。

Result: 在2种剪枝方法、4个数据集和3种训练范式下验证了方法的有效性。

Conclusion: 分数外推是扩展昂贵分数计算任务（如剪枝、数据归因）的有前景方向。

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [260] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: SPEED是一种自适应在线RL课程，通过选择性选择中等难度的训练示例，显著提高训练效率，加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练中均匀采样提示效率低下，计算成本高，需要优化学习效率。

Method: 提出SPEED方法，选择性选择中等难度的提示，优化梯度估计器的信噪比。

Result: 实验显示，SPEED实现2x到6x的训练加速，且不降低准确性，无需手动调参。

Conclusion: SPEED高效、自适应，可无缝集成到标准RL算法中，显著提升训练效率。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [261] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Edit Flows是一种非自回归模型，通过编辑操作（插入、删除、替换）在序列空间上定义离散流，克服了传统非自回归模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决非自回归模型在生成变长序列时的局限性，提供更灵活的序列生成方式。

Method: 利用连续时间马尔可夫链在序列空间上建模编辑操作，并通过扩展状态空间和辅助变量实现高效训练。

Result: 在图像描述生成任务上优于自回归和掩码模型，在文本和代码生成任务上显著优于掩码构建方法。

Conclusion: Edit Flows提供了一种灵活且高效的序列生成方法，适用于多种任务。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [262] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种通过训练LLM进行上下文探索（in-context exploration）的方法，以提升推理能力并实现性能的外推（extrapolation）。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在外推性能上表现不佳，希望通过上下文探索来充分利用推理时的计算资源。

Method: 提出e3方法，包括：（1）结合LLM的非对称能力链式操作；（2）利用错误轨迹的负梯度增强探索；（3）通过课程设计将任务难度与训练预算耦合。

Result: e3-1.7B模型在AIME'25和HMMT'25上表现最佳，并能外推到2倍训练预算，显著提升pass@1和pass@k分数。

Conclusion: 上下文探索是提升LLM推理和外推性能的有效方法，e3方法为小模型实现高性能提供了可行路径。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [263] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: FZOO是一种快速零阶优化器，通过批量单边估计和自适应步长减少收敛所需的前向传递次数，显著提升速度和内存效率，性能优于MeZO。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调中GPU内存瓶颈问题，同时避免零阶优化器收敛速度慢的缺点。

Method: 使用批量单边估计和自适应步长，结合Rademacher随机向量扰动和CUDA并行处理。

Result: FZOO在11个任务中平均比MeZO准确率高3%，前向传递次数减少3倍；RoBERTa-large上准确率提升5.6%，前向传递减少18倍。

Conclusion: FZOO实现了单GPU高速全参数微调，为内存高效预训练提供了方向。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [264] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 论文提出了一种可视化方法来补充Performative Prediction的理论研究，并引入扩展的Performative Prediction设置。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注理论收敛性，缺乏对实际场景的可视化分析，希望通过可视化方法提供实用见解。

Method: 提出了一种解耦风险可视化方法，分析模型参数和数据参数的风险景观，并引入扩展的Performative Prediction设置。

Result: 可视化方法揭示了兴趣点的新特性，并验证了现有算法在更现实条件下的表现。

Conclusion: 可视化方法为Performative Prediction提供了实用补充，扩展设置更贴近现实场景。

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [265] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: 论文提出线性组合猜想，认为任务向量是原始演示的线性组合，并通过理论和实验支持这一观点。


<details>
  <summary>Details</summary>
Motivation: 探索任务向量在上下文学习中的工作原理，以加速推理并提升理解。

Method: 通过损失景观分析、理论预测和实验验证（如显着性分析和参数可视化）研究任务向量。

Result: 任务向量在低秩映射中有效，但在高秩映射中失败；多任务向量注入可增强性能。

Conclusion: 研究深化了对任务向量和transformer模型上下文学习机制的理解。

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [266] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: MasHost是一个基于强化学习的框架，用于自主和查询自适应的多智能体系统设计，通过图搜索和概率采样机制优化智能体角色和交互，引入组件合理性作为新设计原则，并通过HRPO策略实现多目标优化。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统构建方法依赖人工设计的交互机制或启发式规则，限制了自主能力并引入人为偏见，需要更自主的构建框架。

Method: 将多智能体系统构建建模为图搜索问题，通过概率采样机制联合采样智能体角色和交互，提出HRPO策略实现多目标优化。

Result: 在六个基准测试中，MasHost表现优于现有基线方法，验证了其有效性、效率和结构合理性。

Conclusion: MasHost是首个基于强化学习的自主多智能体系统图构建框架，为复杂任务提供了更高效的解决方案。

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [267] [A Practical Guide to Tuning Spiking Neuronal Dynamics](https://arxiv.org/abs/2506.08138)
*William Gebhardt,Alexander G. Ororbia,Nathan McDonald,Clare Thiem,Jack Lombardi*

Main category: cs.NE

TL;DR: 本文研究了脉冲神经网络（SNNs）的基本元素及其调优方法，重点关注LIF和RAF两种神经元单元，探讨了关键方程和超参数对行为的影响，并讨论了输入编码和兴奋-抑制群体设置等设计元素对LIF和RAF动态的影响。


<details>
  <summary>Details</summary>
Motivation: 探索SNNs中LIF和RAF神经元单元的基本特性及其调优方法，以优化网络性能。

Method: 分析LIF和RAF神经元的关键方程及超参数影响，并讨论输入编码和兴奋-抑制群体设置等设计元素。

Result: 揭示了超参数和设计元素对LIF和RAF神经元动态行为的具体影响。

Conclusion: 通过调优超参数和优化设计元素，可以有效提升SNNs中LIF和RAF神经元的性能。

Abstract: In this work, we examine fundamental elements of spiking neural networks
(SNNs) as well as how to tune them. Concretely, we focus on two different
foundational neuronal units utilized in SNNs -- the leaky integrate-and-fire
(LIF) and the resonate-and-fire (RAF) neuron. We explore key equations and how
hyperparameter values affect behavior. Beyond hyperparameters, we discuss other
important design elements of SNNs -- the choice of input encoding and the setup
for excitatory-inhibitory populations -- and how these impact LIF and RAF
dynamics.

</details>


### [268] [Efficient Fireworks Algorithm Equipped with an Explosion Mechanism based on Student's T-distribution](https://arxiv.org/abs/2506.08484)
*Cen Shipeng,Tan Ying*

Main category: cs.NE

TL;DR: 本文提出了一种基于学生t分布的烟花算法（TFWA），解决了传统烟花算法在凸问题和高维采样中的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统烟花算法在凸问题上表现不佳，且高维采样效率低，需要改进。

Method: 提出基于学生t分布的TFWA，利用其参数可调性优化开发能力。

Result: 在CEC2013和CEC2017基准测试中，TFWA成为最强烟花算法变体，性能媲美SOTA。

Conclusion: TFWA在极端点多场景中表现优异，显著优于现有算法。

Abstract: Many real-world problems can be transformed into optimization problems, which
can be classified into convex and non-convex. Although convex problems are
almost completely studied in theory, many related algorithms to many non-convex
problems do not work well and we need more optimization techniques. As a swarm
intelligence optimization algorithm, the Fireworks Algorithm(FWA) has been
widely studied and applied to many real-world scenarios, even including large
language model fine-tuning. But the current fireworks algorithm still has a
number of problems. Firstly, as a heuristic algorithm, its performance on
convex problems cannot match the SOTA results, and can even be said to be
unsatisfactory; secondly, the sampling methods (explosion) of most FWA variants
are still uniform sampling, which is actually inefficient in high dimensional
cases. This work of ours proposes a new student's t-distribution based
FWA(TFWA) with a solid theoretical foundation, which fully utilizes the
advantage that student's t-distribution can adjust the parameters (degrees of
freedom) and thus adjust the exploitation capability. We have fully
experimented on mainstream benchmarks CEC2013 and CEC2017, which proves that
TFWA not only becomes the strongest variant of the fireworks algorithm, but
also achieves results comparable to SOTA on the test set, and its performance
is far superior to that of the SOTA algorithm in some scenarios with a large
number of extreme points.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [269] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: 论文提出了一种基于语音大语言模型（SLLM）的语音后门攻击方法（SPBA），通过聚焦音色和情感等语音元素生成多样化触发器，并引入多梯度下降算法（MGDA）作为缓解策略。实验表明SPBA在攻击指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 语音分类任务（如关键词检测和说话人验证）的安全性易受后门攻击，现有方法因触发器功能限制只能生成有限数量的后门。

Method: 利用SLLM生成多样化触发器，并通过MGDA算法优化攻击策略。

Result: SPBA在两种语音分类任务中表现出显著的触发器有效性和攻击性能。

Conclusion: SPBA通过多样化触发器和MGDA策略，显著提升了后门攻击的效果和效率。

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [270] [Pureformer-VC: Non-parallel Voice Conversion with Pure Stylized Transformer Blocks and Triplet Discriminative Training](https://arxiv.org/abs/2506.08348)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: Pureformer-VC是一种基于Conformer和Zipformer的语音转换框架，通过变分解耦训练和注意力风格转移机制，显著提升了语音转换的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于GAN的语音转换方法在编码多样语音元素和合成自然语音方面存在挑战，需要更高效的解决方案。

Method: 采用Conformer块构建解耦编码器，Zipformer块构建风格转移解码器，结合变分自编码器和三重判别训练，并引入注意力风格转移机制。

Result: 在多个数据集上实验表明，Pureformer-VC在主观评分上与现有方法相当，但在客观指标上显著优于其他方法。

Conclusion: Pureformer-VC在语音转换任务中表现出色，尤其在多对多和多对一场景中具有优势。

Abstract: As a foundational technology for intelligent human-computer interaction,
voice conversion (VC) seeks to transform speech from any source timbre into any
target timbre. Traditional voice conversion methods based on Generative
Adversarial Networks (GANs) encounter significant challenges in precisely
encoding diverse speech elements and effectively synthesising these elements
into natural-sounding converted speech. To overcome these limitations, we
introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer
blocks to build a disentangled encoder and employs Zipformer blocks to create a
style transfer decoder. We adopt a variational decoupled training approach to
isolate speech components using a Variational Autoencoder (VAE), complemented
by triplet discriminative training to enhance the speaker's discriminative
capabilities. Furthermore, we incorporate the Attention Style Transfer
Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer
performance in the decoder. We conducted experiments on two multi-speaker
datasets. The experimental results demonstrate that the proposed model achieves
comparable subjective evaluation scores while significantly enhancing objective
metrics compared to existing approaches in many-to-many and many-to-one VC
scenarios.

</details>


### [271] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Main category: cs.SD

TL;DR: MD-ViSCo是一个统一的框架，能够通过单一模型从任意输入波形生成目标波形（如ECG、PPG或ABP），优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常针对特定源-目标波形对设计，导致多个模型并存，临床实用性受限。

Method: 采用浅层1D U-Net与Swin Transformer结合，利用AdaIN捕捉波形风格。

Result: 在两个公开数据集上，MD-ViSCo平均降低MAE 8.8%，提升PC 4.9%，生成的ABP波形满足AAMI和BHS标准。

Conclusion: MD-ViSCo为医疗监测提供了单一模型处理多种生命体征波形的统一框架。

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [272] [Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.08372)
*Rishabh Ranjan,Likhith Ayinala,Mayank Vatsa,Richa Singh*

Main category: cs.SD

TL;DR: 提出了一种多模态框架，用于检测深度伪造音频中的仇恨言论，在零样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低资源语言和跨模态任务中表现不佳，需要一种更鲁棒的方法。

Method: 采用对比学习联合对齐音频和文本表示，构建多语言基准数据集。

Result: 在两个多语言测试集上表现优于基线，准确率分别为0.819和0.701。

Conclusion: 多模态方法在合成媒体仇恨言论检测中具有优势，尤其在低资源语言中。

Abstract: This paper introduces a novel multimodal framework for hate speech detection
in deepfake audio, excelling even in zero-shot scenarios. Unlike previous
approaches, our method uses contrastive learning to jointly align audio and
text representations across languages. We present the first benchmark dataset
with 127,290 paired text and synthesized speech samples in six languages:
English and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil,
Telugu). Our model learns a shared semantic embedding space, enabling robust
cross-lingual and cross-modal classification. Experiments on two multilingual
test sets show our approach outperforms baselines, achieving accuracies of
0.819 and 0.701, and generalizes well to unseen languages. This demonstrates
the advantage of combining modalities for hate speech detection in synthetic
media, especially in low-resource settings where unimodal models falter. The
Dataset is available at https://www.iab-rubric.org/resources.

</details>


### [273] [A Review on Score-based Generative Models for Audio Applications](https://arxiv.org/abs/2506.08457)
*Ge Zhu,Yutong Wen,Zhiyao Duan*

Main category: cs.SD

TL;DR: 该论文综述了扩散模型在音频应用中的设计原则，提供了训练和采样的系统分析，并开源了一个代码库以支持可重复研究和快速原型设计。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对扩散模型设计选择的深入讨论和音频应用的指导，本文旨在填补这一空白。

Method: 采用分数建模视角作为统一框架，系统分析扩散模型的训练、采样及音频应用的调节机制。

Result: 开源代码库支持多种音频应用，并通过三个案例研究（音频生成、语音增强和文本到语音合成）展示了其能力。

Conclusion: 本文为音频扩散模型的设计提供了全面指导，并促进了可重复研究和快速原型设计。

Abstract: Diffusion models have emerged as powerful deep generative techniques,
producing high-quality and diverse samples in applications in various domains
including audio. These models have many different design choices suitable for
different applications, however, existing reviews lack in-depth discussions of
these design choices. The audio diffusion model literature also lacks
principled guidance for the implementation of these design choices and their
comparisons for different applications. This survey provides a comprehensive
review of diffusion model design with an emphasis on design principles for
quality improvement and conditioning for audio applications. We adopt the score
modeling perspective as a unifying framework that accommodates various
interpretations, including recent approaches like flow matching. We
systematically examine the training and sampling procedures of diffusion
models, and audio applications through different conditioning mechanisms. To
address the lack of audio diffusion model codebases and to promote reproducible
research and rapid prototyping, we introduce an open-source codebase at
https://github.com/gzhu06/AudioDiffuser that implements our reviewed framework
for various audio applications. We demonstrate its capabilities through three
case studies: audio generation, speech enhancement, and text-to-speech
synthesis, with benchmark evaluations on standard datasets.

</details>


### [274] [Passive acoustic non-line-of-sight localization without a relay surface](https://arxiv.org/abs/2506.08471)
*Tal I. Sommer,Ori Katz*

Main category: cs.SD

TL;DR: 利用障碍物边缘的衍射信号实现非视距（NLOS）声源的三维定位，针对门道和凸角两种场景提出定位方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖反射信号重建被遮挡场景，但在某些环境中可能受限，因此探索利用衍射信号扩展NLOS声学感知能力。

Method: 1. 门道场景：利用门的两个边缘作为虚拟探测器阵列；2. 凸角场景：基于刀锋衍射的频谱特征，灵感来自人类听觉的头部相关传递函数（HRTF）。

Result: 通过刀锋衍射实现了NLOS声源的三维定位，扩展了传统方法的适用范围。

Conclusion: 衍射信号为NLOS声源定位提供了新思路，尤其在传统方法受限的环境中表现出潜力。

Abstract: The detection and localization of a source hidden outside the Line-of-Sight
(LOS) traditionally rely on the acquisition of indirect signals, such as those
reflected from visible relay surfaces such as floors or walls. These reflected
signals are then utilized to reconstruct the obscured scene. In this study, we
present an approach that utilize signals diffracted from an edge of an obstacle
to achieve three-dimensional (3D) localization of an acoustic point source
situated outside the LOS. We address two scenarios - a doorway and a convex
corner - and propose a localization method for each of them. For the first
scenario, we utilize the two edges of the door as virtual detector arrays. For
the second scenario, we exploit the spectral signature of a knife-edge
diffraction, inspired by the human perception of sound location by the
head-related transfer function (HRTF). In both methods, knife-edge diffraction
is utilized to extend the capabilities of non-line-of-sight (NLOS) acoustic
sensing, enabling localization in environments where conventional relay-surface
based approaches may be limited.

</details>


### [275] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: ACORN框架通过声音教授LLMs物理感知，利用物理模拟器生成数据，构建AQA-PHY数据集，并连接音频编码器与LLMs，在模拟和现实任务中取得合理结果。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏对现实物理现象的理解，ACORN旨在通过声音填补这一空白。

Method: 结合物理模拟器生成数据，构建AQA-PHY数据集，提出音频编码器处理幅度和相位信息，并与LLMs连接。

Result: 在视线检测、多普勒效应估计和到达方向估计等任务中表现合理。

Conclusion: ACORN为LLMs理解物理世界提供了可行路径。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [276] [Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas and Partitas: Topological and Geometrical Explorations](https://arxiv.org/abs/2506.08540)
*Dima Mrad,Sara Najem*

Main category: cs.SD

TL;DR: 该论文提出了一种基于高阶网络的拓扑框架，用于分析巴赫的小提琴独奏奏鸣曲和组曲，捕捉音乐中的高阶交互和过渡。


<details>
  <summary>Details</summary>
Motivation: 传统图表示方法无法充分表达音乐的复杂性和深度，需要能捕捉高阶交互的模型。

Method: 使用高阶网络（单音为顶点，双音为边，三音为三角形等）建模音乐，并分析其几何和拓扑特性。

Result: 发现了不同乐章类型（如慢板、赋格、巴洛克舞曲）在欧拉特征、曲率和Gauss-Bonnet定理上的差异。

Conclusion: 该框架揭示了巴赫作品中的风格特异性模式，为音乐分析提供了新视角。

Abstract: Music is inherently complex, with structures and interactions that unfold
across multiple layers. Complex networks have emerged as powerful structures
for the quantitative analysis of Western classical music, revealing significant
features of its harmonic and structural organization. Although notable works
have used these approaches to study music, dyadic representations of
interactions fall short in conveying the underlying complexity and depth. In
recent years, the limitations of traditional graph representations have been
questioned and challenged in the context of interactions that could be
higher-dimensional. Effective musical analysis requires models that capture
higher-order interactions and a framework that simultaneously captures
transitions between them. Subsequently, in this paper, we present a topological
framework for analyzing J. S. Bach's Solo Violin Sonatas and Partitas that uses
higher-order networks where single notes are vertices, two-note chords are
edges, three-notes are triangles, etc. We subsequently account for the flow of
music, by modeling transitions between successive notes. We identify
genre-specific patterns in the works' geometric and topological properties. In
particular, we find signatures in the trends of the evolution of the Euler
characteristic and curvature, as well as examining adherence to the
Gauss-Bonnet theorem across different movement types. The distinctions are
revealed between slow movements, Fugues, and Baroque dance movements through
their simplicial complex representation.

</details>


### [277] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: 论文通过对比自回归解码和条件流匹配两种建模范式，系统分析了它们在文本到音乐生成中的表现，为未来模型设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音乐生成模型的多样性使得公平评估和确定关键设计选择变得复杂，研究专注于建模范式的影响。

Method: 使用相同数据集、训练配置和类似架构，对比自回归解码和条件流匹配两种范式，评估生成质量、鲁棒性等多方面性能。

Result: 揭示了两种范式各自的优势和局限性，为未来模型设计提供了实用见解。

Conclusion: 研究为文本到音乐生成系统的建模范式选择提供了明确指导，并展示了未来改进方向。

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


### [278] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: Step-Audio-AQAA是一种端到端的大型音频-语言模型，通过双码本音频标记器和1300亿参数LLM，结合神经声码器，显著提升了音频查询-音频回答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频-语言模型依赖文本输出，无法直接生成自然语音响应，限制了音频交互的流畅性。

Method: 模型采用双码本音频标记器提取特征，结合1300亿参数LLM和神经声码器，后训练中结合DPO和模型合并提升性能。

Result: 在StepEval-Audio-360基准测试中，Step-Audio-AQAA在语音控制等关键领域优于现有最佳模型。

Conclusion: 该研究为端到端音频-语言模型提供了有前景的解决方案，并突出了基于标记的声码器在提升性能中的关键作用。

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [279] [Sensor Fusion for Track Geometry Monitoring: Integrating On-Board Data and Degradation Models via Kalman Filtering](https://arxiv.org/abs/2506.08028)
*Huy Truong-Ba,Jacky Chin,Michael E. Cholette,Pietro Borghesani*

Main category: eess.SY

TL;DR: 该研究提出了一种通过卡尔曼滤波器框架整合低精度传感器信号与退化模型的方法，以提高轨道几何预测的可靠性。实验表明，频繁的传感器数据显著降低了预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 轨道几何监测对铁路安全至关重要，但传统轨道记录车（TRC）成本高且可用性有限，而车载传感器系统提供了低成本、高频的替代方案。

Method: 使用卡尔曼滤波器整合低精度传感器信号与退化模型，并通过安装在TRC上的低成本传感器系统进行实验验证。

Result: 实验结果表明，频繁的传感器数据显著降低了预测不确定性，即使数据噪声较大。

Conclusion: 研究为车载传感器的优化部署提供了指导，有助于提升轨道监测和维护计划的效率。

Abstract: Track geometry monitoring is essential for maintaining the safety and
efficiency of railway operations. While Track Recording Cars (TRCs) provide
accurate measurements of track geometry indicators, their limited availability
and high operational costs restrict frequent monitoring across large rail
networks. Recent advancements in on-board sensor systems installed on
in-service trains offer a cost-effective alternative by enabling
high-frequency, albeit less accurate, data collection. This study proposes a
method to enhance the reliability of track geometry predictions by integrating
low-accuracy sensor signals with degradation models through a Kalman filter
framework. An experimental campaign using a low-cost sensor system mounted on a
TRC evaluates the proposed approach. The results demonstrate that incorporating
frequent sensor data significantly reduces prediction uncertainty, even when
the data is noisy. The study also investigates how the frequency of data
recording influences the size of the credible prediction interval, providing
guidance on the optimal deployment of on-board sensors for effective track
monitoring and maintenance planning.

</details>


### [280] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: DCIDA是一种设计探索框架，通过联合训练的分布生成策略学习最优设计采样，显著降低设计误差。


<details>
  <summary>Details</summary>
Motivation: 现有设计方法在处理非可微评估、多变拓扑和连续布局时存在局限性，DCIDA旨在解决这些问题。

Method: DCIDA通过联合训练的分布生成策略，将设计决策转化为物理表示，利用Transformer网络优化设计。

Result: 实验表明，DCIDA在复杂传递函数情况下显著优于现有方法，设计误差更低。

Conclusion: DCIDA为分布式电路逆向设计提供了一种高效且适应性强的解决方案。

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [281] [Joint Routing and Control Optimization in VANET](https://arxiv.org/abs/2506.08038)
*Chen Huang,Dingxuan Wang,Ronghui Hou*

Main category: eess.SY

TL;DR: DynaRoute是一个动态车辆网络的联合优化框架，通过轨迹感知路由和安全约束的车辆协调，同时解决车队控制和数据传输问题。


<details>
  <summary>Details</summary>
Motivation: 动态车辆网络中需要同时优化车队控制和数据传输，以确保车辆连续移动和可靠通信。

Method: DynaRoute结合实时轨迹预测和安全约束的车辆协调，优化传输路径，并适应动态网络条件。

Result: 仿真结果显示，DynaRoute在复杂场景下保持控制和传输性能，显著提高了吞吐量和可靠性。

Conclusion: DynaRoute通过自适应优化，提升了智能交通系统的整体性能。

Abstract: In this paper, we introduce DynaRoute, an adaptive joint optimization
framework for dynamic vehicular networks that simultaneously addresses platoon
control and data transmission through trajectory-aware routing and
safety-constrained vehicle coordination. DynaRoute guarantees continuous
vehicle movement via platoon safety control with optimizing transmission paths
through real-time trajectory prediction and ensuring reliable data. Our
solution achieves three key objectives: (1) maintaining platoon stability
through accurate data transmission, (2) enabling adaptive routing based on
vehicle movement patterns, and (3) enhancing overall intelligent transportation
system performance. DynaRoute equires predefined traffic models and adapts to
dynamic network conditions using local vehicle state information. We present
comprehensive simulation results demonstrating that DynaRoute maintains control
and transmission performance in multiple complex scenarios while significantly
improving throughput and reliability compared to traditional approaches.

</details>


### [282] [Tram Positioning with Map-Enabled GNSS Data Reconciliation](https://arxiv.org/abs/2506.08032)
*Jakub Kašpar,Vít Fanta,Vladimír Havlena*

Main category: eess.SY

TL;DR: 提出了一种基于GNSS观测数据和轨道地图的有轨电车定位方法，解决了城市密集环境中GNSS信号多径传播的问题。


<details>
  <summary>Details</summary>
Motivation: 城市密集环境中GNSS信号因视线遮挡导致多径传播，定位性能不佳。

Method: 采用迭代扩展卡尔曼滤波（IEKF），复杂度线性，优于其他多径信号抑制技术。

Result: 模拟和真实数据实验均显示定位性能优于基线方法。

Conclusion: 该方法为概念验证，未来可通过模型库或异常值剔除进一步优化。

Abstract: This paper presents an approach to tackle the problem of tram localization
through utilizing a custom processing of Global Navigation Satellite System
(GNSS) observables and the track map. The method is motivated by suboptimal
performance in dense urban environments where the direct line of sight to GNSS
satellites is often obscured which leads to multipath propagation of GNSS
signals. The presented concept is based upon the iterated extended Kalman
filter (IEKF) and has linear complexity (with respect to the number of GNSS
measurements) as opposed to some other techniques mitigating the multipath
signal propagation. The technique is demonstrated both on a simulated example
and real data. The root-mean-squared errors from the simulated ground truth
positions show that the presented solution is able to improve performance
compared to a baseline localization approach. Similar result is achieved for
the experiment with real data, while treating orthogonal projections onto the
tram track as the true position, which is unavailable in the realistic
scenario. This proof-of-concept shows results which may be further improved
with implementation of a bank-of-models method or $\chi^2$-based rejection of
outlying GNSS pseudorange measurements.

</details>


### [283] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: 论文提出了一种基于CNN和MLP的替代模型，用于近似二维壁域中参与性气体的辐射热传递解，以减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 减少数值模拟的计算成本，同时保持工业可接受的精度。

Method: 通过调整输入（气体和壁属性）以适应CNN架构，并使用ICARUS2D生成数据集，比较CNN和MLP的性能。

Result: CNN在精度和鲁棒性上优于MLP，且两种架构均显著加速计算。

Conclusion: CNN是更优的替代模型选择，适用于工业应用。

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


### [284] [Exploring Noncommutative Polynomial Equation Methods for Discrete-Time Quaternionic Control](https://arxiv.org/abs/2506.08034)
*Michael Sebek*

Main category: eess.SY

TL;DR: 论文提出了基于多项式的方法处理离散时间四元数系统，探讨了非交换乘法对经典控制方法的影响，并提出了四元数反馈设计方法。


<details>
  <summary>Details</summary>
Motivation: 研究四元数系统的控制问题，填补经典控制方法在非交换乘法下的空白。

Method: 通过后移算子定义四元数多项式，分析传递函数的左右分数表示，并设计基于四元数多项式方程的反馈控制方法。

Result: 证明了右零点与四元数矩阵右特征值的相似类对应，并首次实现了基于四元数多项式方程的极点配置。

Conclusion: 该方法为四元数系统控制提供了新的理论工具，扩展了经典控制理论的应用范围。

Abstract: We present new polynomial-based methods for discrete-time quaternionic
systems, highlighting how noncommutative multiplication modifies classical
control approaches. Defining quaternionic polynomials via a backward-shift
operator, we examine left and right fraction representations of transfer
functions, showing that right zeros correspond to similarity classes of
quaternionic matrix right eigenvalues. We then propose a feedback design
procedure that generalizes pole placement to quaternions - a first approach
using a genuine quaternionic polynomial equation.

</details>


### [285] [Followstopper Revisited: Phase-space Lagrangian Controller for Traffic Decongestion](https://arxiv.org/abs/2506.08036)
*Rahul Bhadani*

Main category: eess.SY

TL;DR: 本文重新审视了Followerstopper，一种基于相空间的控制系统，用于缓解混合自动驾驶环境中的交通拥堵。


<details>
  <summary>Details</summary>
Motivation: 研究Followerstopper在混合自动驾驶场景中缓解交通拥堵的能力，并优化其控制策略。

Method: 基于可配置的二次曲线设计非线性控制律，调节速度以避免碰撞和不安全的加速度。

Result: 提供了详细的非线性控制律描述，并讨论了避免不现实加速度的标称控制律。

Conclusion: Followerstopper通过相空间控制有效缓解交通拥堵，同时确保安全性和实用性。

Abstract: This paper revisits Followerstopper, a phase-space-based control system that
had demonstrated its ability to mitigate emergent traffic jams due to
stop-and-go traffic during rush hour in the mixed-autonomy setting.
Followerstopper was deployed on an autonomous vehicle. The controller
attenuates the emanant traffic waves by regulating its velocity according to
the relative distance and velocity of the leader car. While regulating the
velocity, the controller also prevents the collision of the ego vehicle with
the lead vehicle within the range specified by the controller's design
parameter. The controller design is based on a configurable quadratic curve on
relative distance-relative velocity phase-space that allows the transition of
the regulated velocity from (i) no modification of input, (ii) decelerating to
match the leader's velocity (iii) braking to avoid any imminent collision. In
this paper, we explore the phase-space properties of Followerstopper and
provide a detailed description of a nonlinear control law that regulates the
reference input to Followerstopper within the physics-informed boundaries. We
also provide a new discussion on the nominal control law that regulates the
reference speed to Followerstopper to avoid unrealistic and unsafe
acceleration.

</details>


### [286] [Continuous-Time Output Feedback Adaptive Control for Stabilization and Tracking with Experimental Results](https://arxiv.org/abs/2506.08042)
*Mohammad Mirtaba,Ankit Goel*

Main category: eess.SY

TL;DR: 提出了一种连续时间输出反馈自适应控制技术，用于稳定和跟踪控制问题，结合粒子群优化自动调参，并通过数值和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 受经典离散时间回顾成本自适应控制算法启发，解决模型无关的输出反馈控制问题。

Method: 基于粒子群优化框架自动调整自适应算法的超参数，设计连续时间输出反馈自适应控制器。

Result: 在双积分器和双旋翼系统的跟踪问题中数值验证，并在姿态稳定问题中实验验证，证明其有效性。

Conclusion: 该控制器是一种有效的模型无关输出反馈控制技术。

Abstract: This paper presents a continuous-time output feedback adaptive control
technique for stabilization and tracking control problems. The adaptive
controller is motivated by the classical discrete-time retrospective cost
adaptive control algorithm. The particle swarm optimization framework automates
the adaptive algorithm's hyper-parameter tuning. The proposed controller is
numerically validated in the tracking problems of a double integrator and a
bicopter system and is experimentally validated in an attitude stabilization
problem. Numerical and experimental results show that the proposed controller
is an effective technique for model-free output feedback control.

</details>


### [287] [Standard LSParameter Estimators Ensure Finite Convergence Time for Linear Regression Equations Under an Interval Excitation Assumption](https://arxiv.org/abs/2506.08211)
*Romeo Ortega,Jose Guadalupe Romero,Stanislav Aranovskiy,Gang Tao*

Main category: eess.SY

TL;DR: 线性回归方程（LRE）在区间激励（IE）回归量下，标准最小二乘（LS）参数估计器能确保参数估计的有限收敛时间（FCT）。


<details>
  <summary>Details</summary>
Motivation: 探讨在区间激励条件下，最小二乘估计器如何实现参数估计的有限时间收敛。

Method: 通过理论分析，证明在IE条件下，LS估计器能够实现有限时间收敛。

Result: 收敛时间等于满足IE假设所需的时间长度，IE是LRE可辨识性的充要条件。

Conclusion: IE是参数估计问题在线或离线解的最弱假设。

Abstract: In this brief note we recall the little-known fact that, for linear
regression equations (LRE) with intervally excited (IE) regressors, standard
Least Square (LS) parameter estimators ensure finite convergence time (FCT) of
the estimated parameters. The convergence time being equal to the time length
needed to comply with the IE assumption. As is well-known, IE is necessary and
sufficient for the identifiability of the LRE-hence, it is the weakest
assumption for the on-or off-line solution of the parameter estimation problem.

</details>


### [288] [DEKC: Data-Enable Control for Tethered Space Robot Deployment in the Presence of Uncertainty via Koopman Operator Theory](https://arxiv.org/abs/2506.08319)
*Ao Jin,Qinyi Wang,Sijie Wen,Ya Liu,Ganghui Shen,Panfeng Huang,Fan Zhang*

Main category: eess.SY

TL;DR: 提出了一种名为DEKC的数据驱动框架，用于在未知不确定性下部署系留空间机器人，通过离线训练和在线执行实现高精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决系留空间机器人在未知不确定性环境中的部署问题，提高其准确性和鲁棒性。

Method: 将未知不确定性建模为动态系统，利用数据驱动的Koopman理论构建代理模型，并通过深度神经网络参数化提升函数。离线阶段学习提升函数，在线阶段通过代理模型补偿基线控制器。

Result: 数值模拟验证了DEKC框架的有效性，能够显著减弱或消除不确定性的影响。

Conclusion: DEKC框架为系留空间机器人在不确定性环境中的部署提供了一种高效解决方案，并公开了实现代码。

Abstract: This work focuses the deployment of tethered space robot in the presence of
unknown uncertainty. A data-enable framework called DEKC which contains offline
training part and online execution part is proposed to deploy tethered space
robot in the presence of uncertainty. The main idea of this work is modeling
the unknown uncertainty as a dynamical system, which enables high accuracy and
convergence of capturing uncertainty. The core part of proposed framework is a
proxy model of uncertainty, which is derived from data-driven Koopman theory
and is separated with controller design. In the offline stage, the lifting
functions associated with Koopman operator are parameterized with deep neural
networks. Then by solving an optimization problem, the lifting functions are
learned from sampling data. In the online execution stage, the proxy model
cooperates the learned lifting functions obtained in the offline phase to
capture the unknown uncertainty. Then the output of proxy model is compensated
to the baseline controller such that the effect of uncertainty can be
attenuated or even eliminated. Furthermore, considering some scenarios in which
the performance of proxy model may weaken, a receding-horizon scheme is
proposed to update the proxy model online. Finally, the extensive numerical
simulations demonstrate the effectiveness of our proposed framework. The
implementation of proposed DEKC framework is publicly available at
https://github.com/NPU-RCIR/DEKC.git.

</details>


### [289] [Learning event-triggered controllers for linear parameter-varying systems from data](https://arxiv.org/abs/2506.08366)
*Renjie Ma,Su Zhang,Wenjie Liu,Zhijian Hu,Peng Shi*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的控制策略，用于事件触发的线性参数变化（LPV）系统，并通过稳定性验证。理论分析了LPV系统的θ-激励持续性，推导了基于数据的表示，并利用Petersen引理证明了稳定性。方法适用于参考轨迹跟踪，并通过数值模拟验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 工程应用中的非线性动态行为可通过LPV表示近似，但精确建模困难。本文旨在开发数据驱动的控制策略，解决这一问题。

Method: 提出理论分析θ-激励持续性，推导数据驱动表示；利用Petersen引理证明稳定性，生成可计算的半定规划问题；扩展方法至参考轨迹跟踪场景。

Result: 通过数值模拟验证了理论推导的有效性，证明了方法的可行性。

Conclusion: 本文提出的数据驱动控制策略为事件触发的LPV系统提供了稳定性保证，适用于实际工程应用。

Abstract: Nonlinear dynamical behaviours in engineering applications can be
approximated by linear-parameter varying (LPV) representations, but obtaining
precise model knowledge to develop a control algorithm is difficult in
practice. In this paper, we develop the data-driven control strategies for
event-triggered LPV systems with stability verifications. First, we provide the
theoretical analysis of ${\theta}$-persistence of excitation for LPV systems,
which leads to the feasible data-based representations. Then, in terms of the
available perturbed data, we derive the stability certificates for
event-triggered LPV systems with the aid of Petersen's lemma in the sense of
robust control, resulting in the computationally tractable semidefinite
programmings, the feasible solutions of which yields the optimal gain
schedulings. Besides, we generalize the data-driven eventtriggered LPV control
methods to the scenario of reference trajectory tracking, and discuss the
robust tracking stability accordingly. Finally, we verify the effectiveness of
our theoretical derivations by numerical simulations.

</details>


### [290] [Compact Amplified Laser Power Stabilization Using Robust Active Disturbance Rejection Control with Sensor Noise Decoupling](https://arxiv.org/abs/2506.08404)
*Yanpei Shi,Jingxuan Zhang,Zhuo Shi,Chenyao Zhang,Yuze Guo,Rui Feng*

Main category: eess.SY

TL;DR: 提出了一种基于双环主动抗扰控制（DLADRC）的紧凑型放大激光器功率稳定方法，显著降低了功率不稳定性，提高了光学泵磁强计（OPM）的性能。


<details>
  <summary>Details</summary>
Motivation: 激光功率不稳定性（包括随机抖动和慢漂移）限制了光学泵磁强计（OPMs）在检测超弱磁场时的性能，尤其是在大规模OPM阵列中。

Method: 采用双环主动抗扰控制（DLADRC）策略，结合扩展状态观测器（ESOs）和控制误差动态的指数衰减估计，实现功率稳定。

Result: 实验验证表明，该方法显著提高了长期稳定性，1小时功率不稳定性降低了85.7%，Allan方差在10^2 s--10^3 s范围内降低了十倍。

Conclusion: 该方法在多种操作场景下均表现出鲁棒性，为高灵敏度生物磁场检测提供了可靠支持。

Abstract: Laser power instability, encompassing random jitter and slow drift, severely
limits the performance of optically pumped magnetometers (OPMs) in detecting
ultra-weak magnetic fields, especially in large-scale OPM arrays for
magnetoencephalography. Although a unified amplified laser (AL) architecture
improves integration, fluctuations in the pump beam progressively degrade
performance across all channels, exacerbated by environmental disturbances and
system uncertainties. To address this challenge, this paper presents a compact
AL power stabilization approach based on an innovative dual-loop active
disturbance rejection control (DLADRC) strategy, while integrating a
comprehensive quantitative stability analysis through novel exponential decay
estimates for extended state observers (ESOs) and control error dynamics. As
validated through physical experimental results, the proposed method
significantly improves AL's long-term stability with sensor noise decoupling,
achieving an over 85.7% reduction in 1-hour power instability and a tenfold
decrease in Allan variance for correlation times 10^2 s--10^3 s, compared to
standard ADRC. Crucially, the strategy demonstrates robust effectiveness across
diverse operating scenarios, enabling AL-based OPM systems to achieve their
full potential in high-sensitivity biomagnetic field detection.

</details>


### [291] [Theoretical Foundations of Waste Factor and Waste Figure with Applications to Fixed Wireless Access and Relay Systems](https://arxiv.org/abs/2506.08414)
*Nurullah Sevim,Mostafa Ibrahim,Sabit Ekin,Theodore S. Rappaport*

Main category: eess.SY

TL;DR: 论文提出了一种基于Waste Factor（W）的框架，用于评估和优化无线通信系统的能效，并将其与Shannon理论极限联系起来。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统能耗的急剧增加，特别是在下一代无线系统中，需要一种统一的能效评估和优化框架。

Method: 通过扩展Waste Factor的概念，将其与Consumption Factor（CF）结合，推导出能量每比特的闭式表达式，并开发了一种决策规则来选择更节能的通信路径。

Result: 提出了针对直接和继电器辅助链路的能量每比特表达式，并将框架扩展到固定无线接入（FWA）场景。

Conclusion: Waste Factor框架具有广泛的适用性，包括新兴的6G用例，如反射智能表面（RIS）。

Abstract: The exponential rise in energy consumption across wireless communication
systems, particularly in anticipation of next-generation wireless systems,
necessitates rigorous frameworks for evaluating and optimizing energy
efficiency. This paper revisits and expands the concept of the Waste Factor
(W), or Waste Figure (WF) in decibel scale, as a unifying metric that captures
both utilized and wasted power in cascaded communication systems. Building upon
its foundation in system-level power modeling, we integrate the Waste Factor
into a refined formulation of the Consumption Factor (CF), the ratio of data
rate to total consumed power, linking it directly to Shannon's theoretical
limit on energy per bit. This analysis introduces additive energy waste into
the classical energy-per-bit derivation through the Waste Factor term.
  We derive closed-form expressions for energy-per-bit expenditure in both
direct and relay-assisted links and develop a decision rule to determine which
communication path is more energy efficient under given conditions. While not
modeled explicitly, Reflective Intelligent Surfaces (RIS) can be interpreted as
a special case of relay-based architectures within this unified formulation,
suggesting broader applicability of the Waste Factor framework to emerging 6G
use cases. The framework is then extended to a Fixed Wireless Access (FWA)
scenario, where uplink and downlink asymmetries, traffic directionality, and
component inefficiencies are jointly considered to analyze energy-optimal
deployment strategies.

</details>


### [292] [Predictive reinforcement learning based adaptive PID controller](https://arxiv.org/abs/2506.08509)
*Chaoqun Ma,Zhiyong Zhang*

Main category: eess.SY

TL;DR: 提出了一种基于预测强化学习的自适应PID控制器（PRL-PID），结合数据驱动和模型驱动方法的优势，解决了不稳定和非线性系统的控制问题。


<details>
  <summary>Details</summary>
Motivation: 针对不稳定和非线性系统控制的挑战，结合数据驱动和模型驱动方法的优势，提出更高效的解决方案。

Method: 引入预测强化学习框架，采用动作平滑策略抑制超调和振荡，并使用分层奖励函数支持训练。

Result: 实验表明，PRL-PID在非线性、不稳定和强耦合系统中表现出优异的稳定性和跟踪精度，优于现有RL调谐PID方法，且具有鲁棒性和适应性。

Conclusion: PRL-PID通过预测学习将系统模型先验融入数据驱动控制，提升了训练效率和控制器稳定性，实现了模型与数据驱动方法的平衡。

Abstract: Purpose: This study aims to address the challenges of controlling unstable
and nonlinear systems by proposing an adaptive PID controller based on
predictive reinforcement learning (PRL-PID), where the PRL-PID combines the
advantages of both data-driven and model-driven approaches.
Design/methodology/approach: A predictive reinforcement learning framework is
introduced, incorporating action smooth strategy to suppress overshoot and
oscillations, and a hierarchical reward function to support training. Findings:
Experimental results show that the PRL-PID controller achieves superior
stability and tracking accuracy in nonlinear, unstable, and strongly coupled
systems, consistently outperforming existing RL-tuned PID methods while
maintaining excellent robustness and adaptability across diverse operating
conditions. Originality/Value: By adopting predictive learning, the proposed
PRL-PID integrates system model priors into data-driven control, enhancing both
the control framework's training efficiency and the controller's stability. As
a result, PRL-PID provides a balanced blend of model-based and data-driven
approaches, delivering robust, high-performance control.

</details>


### [293] [The Invariant Zonotopic Set-Membership Filter for State Estimation on Groups](https://arxiv.org/abs/2506.08530)
*Tao Li,Yi Li,Lulin Zhang,Jiuxiang Dong*

Main category: eess.SY

TL;DR: 论文提出了一种基于群不变理论的Invariant Zonotopic Set-Membership Filter (InZSMF)方法，用于解决噪声统计特性未知但有界的状态估计问题，并通过实验验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于统计的滤波方法在处理噪声统计特性未知的问题时存在局限性，因此需要探索不变理论在非统计滤波领域的应用。

Method: 将状态空间从欧几里得向量空间转换到李群空间，构建群仿射离散系统，提出两种观测器增益调谐算法（极点配置法和F半径优化法）。

Result: 仿真实验表明，InZSMF方法在收敛速度、估计精度和区间面积等方面均优于传统ZSMF方法。

Conclusion: InZSMF方法成功将不变滤波理论扩展到非统计滤波领域，为噪声特性未知的状态估计问题提供了有效解决方案。

Abstract: The invariant filtering theory based on the group theory has been successful
in statistical filtering methods. However, there exists a class of state
estimation problems with unknown statistical properties of noise disturbances,
and it is worth discussing whether the invariant observer still has performance
advantages. In this paper, considering the problem of state estimation with
unknown but bounded noise disturbances, an Invariant Zonotopic Set-Membership
Filter (InZSMF) method on groups is innovatively proposed, which extends the
invariant filtering theory to the field of non-statistical filtering
represented by set-membership filtering. Firstly, the InZSMF method transforms
the state space from the traditional Euclidean vector space to the Lie group
space to construct group affine discrete systems with unknown but bounded noise
uncertainty defined by the zonotope on groups. Secondly, the nonlinear observer
on the group is defined and the corresponding linearized estimation error is
derived. Then, two observer gain tuning algorithms under the InZSMF method are
proposed, respectively, the pole configuration method and the F-radius
optimization method. Finally, through simulation experiments, it is shown that
the InZSMF state estimation method is generally superior to the traditional
Zonotopic Set-Membership Filter (ZSMF) state estimation method. Especially,
when the initial estimations are imprecise, the convergence speed of state
estimation, the accuracy of set-membership center estimation, and the average
interval area of zonotopic estimation of the InZSMF method are significantly
better than those of the ZSMF method.

</details>


### [294] [Toward Low-Altitude Airspace Management and UAV Operations: Requirements, Architecture and Enabling Technologies](https://arxiv.org/abs/2506.08579)
*Guiyang Luo,Jinglin Li,Qixun Zhang,Zhiyong Feng,Quan Yuan,Yijing Lin,Hui Zhang,Nan Cheng,Ping Zhang*

Main category: eess.SY

TL;DR: 论文提出UTICN架构，解决低空经济（LAE）中的动态空域管理和无人机操作问题，通过多领域技术集成实现智能化与协调性。


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展，现有系统分散且缺乏协调，亟需统一架构解决动态空域管理和无人机操作等挑战。

Method: 提出UTICN架构，整合多领域感知、高精度定位、智能通信等技术，并引入ISAC、群协调等关键技术。

Result: 通过城市级LAE管理平台和多频协作ISAC系统验证了UTICN的可行性。

Conclusion: UTICN为低空经济提供了统一的操作基础和空域管理架构参考。

Abstract: The low-altitude economy (LAE) is rapidly advancing toward intelligence,
connectivity, and coordination, bringing new challenges in dynamic airspace
management, unmanned aerial vehicle (UAV) operation, and security management.
Existing systems remain fragmented and lack effective coordination. To bridge
these gaps, we propose UTICN (Ubiquitous and Trusted Intelligent
Cellular-native Network) for LAE, a unified cellular-native architecture that
integrates multi-domain sensing, high-precision positioning, intelligent
aircraft-to-everything communication, dynamic airspace management, and UAV
operational services. UTICN introduces key technologies such as integrated
sensing and communication (ISAC), passive and active positioning, intelligent
machine communication, swarm coordination, and control-data decoupled
management frameworks. We demonstrate UTICN's feasibility through two use
cases, i.e., a city-level LAE management platform and a multi-frequency
collaborative ISAC system. This work provides a fundamental reference for
building a unified operational foundation and airspace management architecture
for the LAE.

</details>


### [295] [Q-learning-based Hierarchical Cooperative Local Search for Steelmaking-continuous Casting Scheduling Problem](https://arxiv.org/abs/2506.08608)
*Yang Lv,Rong Hu,Bin Qian,Jian-Bo Yang*

Main category: eess.SY

TL;DR: 本文提出了一种基于Q学习的层次化合作局部搜索框架（HierC_Q），用于解决钢铁连铸调度问题（SCCSP），旨在最小化最大完成时间和平均等待时间的加权和。


<details>
  <summary>Details</summary>
Motivation: 钢铁连铸调度问题复杂且动态性强，传统启发式和元启发式方法难以适应实际需求，因此需要一种更高效的调度方法。

Method: HierC_Q采用层次化架构，包括学习改进层（L2I）和扰动重建层（D2R）。L2I层利用Q学习局部搜索框架（QLSF）进行深度开发，D2R层通过扰动和构建策略避免早熟收敛。

Result: 通过与11种局部搜索框架和9种先进算法的对比，验证了HierC_Q的优越性和有效性。

Conclusion: HierC_Q通过耦合度量奖励函数和层次化架构，显著提升了钢铁连铸调度问题的求解效率。

Abstract: The steelmaking continuous casting scheduling problem (SCCSP) is a critical
and complex challenge in modern steel production, requiring the coordinated
assignment and sequencing of steel charges across multiple production stages.
Efficient scheduling not only enhances productivity but also significantly
reduces energy consumption. However, both traditional heuristics (e.g.,
two-stage local search) and recent metaheuristics often struggle to adapt to
the dynamic characteristics of practical SCCSP instances. To address these
limitations, this paper introduces a novel Q learning based hierarchical
cooperative local search framework, termed HierC_Q, aimed at minimizing the
weighted sum of the maximum completion time and the average waiting time in
SCCSP. The core contributions of HierC_Q are twofold. First, considering the
intrinsic coupling properties of the SCCSP, a dedicated reward function is
proposed based on a novel coupling measure (CM), guiding the exploration
process towards promising regions of the solution space. Second, a hierarchical
architecture is devised, comprising two distinct tiers: the learn to improve
(L2I) tier and the "disturb to renovate" (D2R) tier. The L2I tier performs deep
exploitation within promising regions using two independent Q-learning-based
local search frameworks (QLSFs) tailored for subproblems, along with a synergy
QLSF designed for the main problem. To enhance the effectiveness of local
search, a validity evaluation approach and a speed-up evaluation method are
also intro-duced, grounded in a detailed study of the problem's structure.
Meanwhile, the D2R tier incorporates a perturbation and construction based
solution renewal strategy to mitigate the risk of premature convergence. The
superiority and effectiveness of HierC_Q are demonstrated through extensive
comparisons with eleven local search frameworks and nine state-of-the-art
algorithms.

</details>


### [296] [Linguistic Ordered Weighted Averaging based deep learning pooling for fault diagnosis in a wastewater treatment plant](https://arxiv.org/abs/2506.08676)
*Alicia Beneyto-Rodriguez,Gregorio I. Sainz-Palmero,Marta Galende-Hernández,María J. Fuente*

Main category: eess.SY

TL;DR: 提出了一种基于语言有序加权平均（OWA）池化的深度卷积神经网络（DCNN）方法，用于污水处理厂的故障诊断，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 污水处理厂（WWTP）的故障诊断对水资源再利用至关重要，但传统方法难以应对其复杂性和大规模数据。

Method: 采用滑动时间窗口和语言OWA池化的DCNN，利用少量监测样本和迭代实现早期故障诊断。

Result: 在5种故障类型和140个变量的测试中，准确率、召回率和F1分数均超过91%，优于传统池化方法。

Conclusion: 语言OWA池化的DCNN在污水处理厂故障诊断中表现优异，提供了更直观和高效的解决方案。

Abstract: Nowadays, water reuse is a serious challenge to help address water shortages.
Here, the wastewater treatment plants (WWTP) play a key role, and its proper
operation is mandatory. So, fault diagnosis is a key activity for these plants.
Their high complexity and large-scale require of smart methodologies for that
fault diagnosis and safety operation. All these large-scale and complex
industrial processes are monitored, allowing the data collection about the
plant operation, so data driven approaches for fault diagnosis can be applied.
A popular approach to fault diagnosis is deep learning-based methodologies.
Here, a fault diagnosis methodology is proposed for a WWTP using a new
linguistic Ordered Weighted Averaging (OWA) pooling based Deep Convolutional
Neural Network (DCNN) and a sliding and overlapping time window. This window
slides over input data based on the monitoring sampling time, then the
diagnosis is carried out by the linguistic OWA pooling based DCNN. This
alternative linguistic pooling uses well-known linguistic OWA quantifiers,
which permit terms such as \textsl{Most, AtLeast, etc.}, supplying new
intuitive options for the pooling tasks. This sliding time window and the OWA
pooling based network permit a better and earlier fault diagnosis, at each
sampling time, using a few monitoring samples and a fewer learning iterations
than DCNN standard pooling. Several linguistic OWA operators have been checked
with a benchmark for WWTPs. A set of 5 fault types has been used, taking into
account 140 variables sampled at 15 minutes time intervals. The performance has
been over $91\%$ for $Accuracy$, $Recall$ or $F1-Score$, and better than other
competitive methodologies. Moreover, these linguistic OWA operators for DCNN
pooling have shown a better performance than the standard \textsl{Max} and
\textsl{Average} options.

</details>


### [297] [Efficient Uncertainty Propagation with Guarantees in Wasserstein Distance](https://arxiv.org/abs/2506.08689)
*Eduardo Figueiredo,Steven Adams,Peyman Mohajerin Esfahani,Luca Laurenti*

Main category: eess.SY

TL;DR: 本文提出了一种基于Wasserstein距离的方法，用于量化非线性函数传播不确定分布时的结果不确定性，并通过离散化支持分布保证计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效量化非线性函数传播不确定分布时的结果不确定性，特别是在动态系统中。

Method: 通过近似输入集中的名义分布为离散支持分布，利用半离散最优传输和分布鲁棒优化技术，确保计算效率并控制误差。

Result: 理论证明和实验验证表明，该方法能有效量化线性和非线性随机系统中的不确定性，误差可控制在任意小范围内。

Conclusion: 该方法在动态系统中具有高效性和准确性，适用于高维非线性系统的不确定性量化。

Abstract: In this paper, we consider the problem of propagating an uncertain
distribution by a possibly non-linear function and quantifying the resulting
uncertainty. We measure the uncertainty using the Wasserstein distance, and for
a given input set of distributions close in the Wasserstein distance, we
compute a set of distributions centered at a discrete distribution that is
guaranteed to contain the pushforward of any distribution in the input set. Our
approach is based on approximating a nominal distribution from the input set to
a discrete support distribution for which the exact computation of the
pushforward distribution is tractable, thus guaranteeing computational
efficiency to our approach. Then, we rely on results from semi-discrete optimal
transport and distributional robust optimization to show that for any $\epsilon
> 0$ the error introduced by our approach can be made smaller than $\epsilon$.
Critically, in the context of dynamical systems, we show how our results allow
one to efficiently approximate the distribution of a stochastic dynamical
system with a discrete support distribution for a possibly infinite horizon
while bounding the resulting approximation error. We empirically investigate
the effectiveness of our framework on various benchmarks, including a 10-D
non-linear system, showing the effectiveness of our approach in quantifying
uncertainty in linear and non-linear stochastic systems.

</details>


### [298] [Efficient Learning of Vehicle Controller Parameters via Multi-Fidelity Bayesian Optimization: From Simulation to Experiment](https://arxiv.org/abs/2506.08719)
*Yongpeng Zhao,Maik Pfefferkorn,Maximilian Templer,Rolf Findeisen*

Main category: eess.SY

TL;DR: 提出了一种多保真度贝叶斯优化方法，通过结合低保真度仿真数据和少量真实实验，高效学习最优控制器参数，减少人工调参和昂贵实地测试的需求。


<details>
  <summary>Details</summary>
Motivation: 车辆控制器参数调优在汽车开发中成本高且耗时，传统方法依赖大量实地测试，效率低下。

Method: 采用多保真度贝叶斯优化方法，结合自回归多保真度高斯过程模型，实现不同保真度级别间的知识迁移。

Result: 仿真和真实实验验证表明，该方法仅需极少真实实验即可实现高质量控制器性能。

Conclusion: 该方法为工业应用中的智能车辆控制调优提供了实用且可扩展的解决方案。

Abstract: Parameter tuning for vehicle controllers remains a costly and time-intensive
challenge in automotive development. Traditional approaches rely on extensive
real-world testing, making the process inefficient. We propose a multi-fidelity
Bayesian optimization approach that efficiently learns optimal controller
parameters by leveraging both low-fidelity simulation data and a very limited
number of real-world experiments. Our approach significantly reduces the need
for manual tuning and expensive field testing while maintaining the standard
two-stage development workflow used in industry. The core contribution is the
integration of an auto-regressive multi-fidelity Gaussian process model into
Bayesian optimization, enabling knowledge transfer between different fidelity
levels without requiring additional low-fidelity evaluations during real-world
testing. We validate our approach through both simulation studies and realworld
experiments. The results demonstrate that our method achieves high-quality
controller performance with only very few real-world experiments, highlighting
its potential as a practical and scalable solution for intelligent vehicle
control tuning in industrial applications.

</details>


### [299] [Minimal Order Recovery through Rank-adaptive Identification](https://arxiv.org/abs/2506.08720)
*Frédéric Zheng,Yassir Jedra,Alexandre Proutière*

Main category: eess.SY

TL;DR: 论文提出了一种名为Thresholded Ho-Kalman的算法，用于从噪声输入输出轨迹中识别线性系统，通过秩自适应方法估计Hankel矩阵，并提供了误差界限和样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 解决从噪声输入输出轨迹中识别线性系统的问题，特别是在系统阶数未知的情况下。

Method: 引入Thresholded Ho-Kalman算法，利用秩自适应方法估计Hankel矩阵，平衡关键奇异值的准确推断与其余部分的近似误差。

Result: 建立了Hankel矩阵的有限样本Frobenius范数误差界限，并恢复了系统阶数和Markov参数，提供了样本复杂度和有限时间误差界限。

Conclusion: 该算法在无需已知系统阶数的情况下，达到了与现有最优算法相同的性能界限。

Abstract: This paper addresses the problem of identifying linear systems from noisy
input-output trajectories. We introduce Thresholded Ho-Kalman, an algorithm
that leverages a rank-adaptive procedure to estimate a Hankel-like matrix
associated with the system. This approach optimally balances the trade-off
between accurately inferring key singular values and minimizing approximation
errors for the rest. We establish finite-sample Frobenius norm error bounds for
the estimated Hankel matrix. Our algorithm further recovers both the system
order and its Markov parameters, and we provide upper bounds for the sample
complexity required to identify the system order and finite-time error bounds
for estimating the Markov parameters. Interestingly, these bounds match those
achieved by state-of-the-art algorithms that assume prior knowledge of the
system order.

</details>


### [300] [Future Deployment and Flexibility of Distributed Energy Resources in the Distribution Grids of Switzerland](https://arxiv.org/abs/2506.08724)
*Lorenzo Zapparoli,Alfredo Oneto,María Parajeles Herrera,Blazhe Gjorgiev,Gabriela Hug,Giovanni Sansavini*

Main category: eess.SY

TL;DR: 论文介绍了一个瑞士中低压配电网中分布式能源资源的综合数据库，支持灵活性和电网韧性研究。


<details>
  <summary>Details</summary>
Motivation: 全球脱碳目标推动配电网在波动条件下运行，但分布式能源资源数据分散且不完整。

Method: 构建了一个覆盖200多万连接点的数据库，包含可控设备的灵活性能力，并与国家预测对齐。

Result: 数据库支持分布式能源资源的灵活性、电网韧性和能源政策研究，具有模块化结构。

Conclusion: 该数据库为配电网规划和政策制定提供了重要工具，具有广泛适用性。

Abstract: The decarbonization goals worldwide drive the energy transition of power
distribution grids, which operate under increasingly volatile conditions and
closer to their technical limits. In this context, localized operational data
with high temporal and spatial resolution is essential for their effective
planning and regulation. Nevertheless, information on grid-connected
distributed energy resources, such as electric vehicles, photovoltaic systems,
and heat pumps, is often fragmented, inconsistent, and unavailable. This work
introduces a comprehensive database of distributed energy resources and
non-controllable loads allocated in Switzerland's medium- and low-voltage
distribution grid models, covering over 2 million points of connection.
Remarkably, this data specifies the flexibility capabilities of the
controllable devices, with a set of projections aligned with national forecasts
for 2030, 2040, and 2050. The database supports studies on flexibility
provision of distributed energy resources, distribution grid resilience, and
national energy policy, among other topics. Importantly, its modular structure
allows users to extract national- and local-scale information across medium-
and low-voltage systems, enabling broad applicability across locations.

</details>


### [301] [Distributed component-level modeling and control of energy dynamics in electric power systems](https://arxiv.org/abs/2506.08861)
*Hiya Gada,Rupamathi Jaddivada,Marija Ilic*

Main category: eess.SY

TL;DR: 本文提出了一种基于能量空间建模框架的多层分布式控制架构，用于解决现代电力系统中快速、非线性和异构系统的建模与控制问题。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因电力电子技术的广泛应用而变得快速、非线性和异构，传统建模与控制方法已无法满足需求。

Method: 采用并扩展了基于能量守恒原理的能量空间建模框架，提出了多层分布式控制架构，设计了两种能量空间控制器（FBLC和SMC）。

Result: 在逆变器控制的RLC电路和同步发电机连接负载的系统中，能量控制改善了瞬态响应并减少了控制开销。

Conclusion: 能量空间建模和分布式控制架构为现代电力系统提供了有效的解决方案。

Abstract: The widespread deployment of power electronic-based technologies is
transforming modern power systems into fast, nonlinear, and heterogeneous
systems. Conventional modeling and control approaches, rooted in quasi-static
analysis and centralized control, are inadequate for these converter-dominated
systems, which operate on fast timescales and involve proprietary models of
diverse components. This paper adopts and extends a previously introduced
energy space modeling framework grounded in energy conservation principles to
address these challenges. We generalize the notion of a port interaction
variable, which encodes energy exchange between interconnected, heterogeneous
components in a unified and physically intuitive manner. A multilayered
distributed control architecture is proposed, wherein the nonlinear physical
dynamics of each component are lifted to a higher-level linear energy space
through well-defined mappings. Distributed controllers are designed in this
energy space using only local states and minimal neighbor information via port
interaction variables. Two control designs, energy-based feedback linearizing
control (FBLC) and sliding mode control (SMC), are proven to achieve asymptotic
convergence to reference outputs. The approach is validated on two systems: an
inverter-controlled RLC circuit and a synchronous generator connected to a
load. In both cases, energy-based control improves transient response and
reduces control effort.

</details>


### [302] [HabSim: Architecture for modelling disruptions, propagation, detection and repair in deep space habitats](https://arxiv.org/abs/2506.08903)
*Luca Vaccino,Alana K. Lund,Shirley J. Dyke,Mohsen Azimi,Ethan Vallerga*

Main category: eess.SY

TL;DR: 论文提出了一种用于模拟深空栖息地系统中破坏传播、检测和修复的仿真架构，结合物理和现象学模型，实现高效且逼真的实时模拟。


<details>
  <summary>Details</summary>
Motivation: 深空长期人类定居面临极端环境和资源限制等挑战，需要自主和弹性的栖息地系统设计，现有仿真工具缺乏对破坏传播和修复的综合建模。

Method: 结合物理和现象学模型，协调不同时间尺度的子系统，开发高效且可扩展的仿真架构，并在HabSim模型中应用。

Result: 通过月球栖息地火灾传播的模拟，验证了架构对破坏传播、检测和修复的支持，展示了HabSim模型的实用性。

Conclusion: 该架构高效且可扩展，为研究栖息地系统的弹性和自主决策提供了有力工具。

Abstract: Establishing long-term human settlements in deep space presents significant
challenges. Harsh environmental conditions, such as extreme temperature
fluctuations, micrometeorite impacts, seismic activity, and exposure to solar
and cosmic radiation pose obstacles to the design and operation of habitat
systems. Prolonged mission duration and the vast distances from Earth introduce
further complications in the form of delayed communication and limited
resources, making autonomy especially desirable.
  Enabling simulation of the consequences of disruptions and their propagation
through the various habitat subsystems is important for the development of
autonomous and resilient space habitats. While existing simulation tools can
assist in modeling some of these aspects, the integration of damage
propagation, detection and repair in a computational model is rarely
considered. This paper introduces and demonstrates a simulation architecture
designed to model these aspects efficiently. By combining physics-based and
phenomenological models, our approach balances computational efficiency with
model fidelity. Furthermore, by coordinating subsystems operating at different
time scales, we achieve real-time simulation capabilities.
  After describing the architecture, we demonstrate its application within
HabSim, a space habitat system model developed by the NASA-funded Resilient
Extraterrestrial Habitat Institute (RETHi). In these scenarios we consider fire
hazard propagation within a lunar habitat to illustrate both how our
architecture supports the modeling of disruption propagation, detection, and
repair in a simulation environment and how the HabSim model can be leveraged
for through stochastic simulations to support resilience assessment. The
architecture developed herein is efficient and scalable, enabling researchers
to gain insight into resilience, autonomy and decision-making.

</details>


### [303] [Quantitative Indices for Improving Metro Load Curve, Using Distributed Generation](https://arxiv.org/abs/2506.08975)
*Masoud Behbahani,Alireza Fereidunian*

Main category: eess.SY

TL;DR: 论文提出利用分布式发电（DG）改善地铁负荷曲线的方法，通过分析地铁主要负荷的规模和消费模式，提取典型负荷曲线，并展示DG对负荷曲线的显著改善效果。


<details>
  <summary>Details</summary>
Motivation: 地铁负荷曲线需求高、负荷因子低，且峰值与国家电网重合，导致供电问题。传统方法如需求侧管理（DSM）和分布式存储（DS）效果有限，因此探索DG的潜力。

Method: 分析地铁主要负荷的规模和消费模式，提取典型负荷曲线，并评估DG对负荷曲线的改善效果，计算经济指标如初始成本和投资回报率（ROI）。

Result: DG显著改善了地铁负荷曲线，并通过定量参数展示了其效果，这些参数可用于经济性评估。

Conclusion: DG是改善地铁负荷曲线的有效方法，具有实际应用和经济价值。

Abstract: This paper promises the idea of using DG (Distributed Generation) to improve
the Metro load curve. Public transportation systems are often based on gasoline
and diesel. However, with the gradual development in usage of the Metro and
monorail, a new load with heavy demand, inappropriate load curve and middle LF
(Load factor) is added to the electricity grid. In addition to supply problem
of this massive consumer, the Metro load curve is another problem, which has a
relatively low LF. Furthermore, Metro load peak hours coincide with the peaks
of national grid. Improvement of the load curve is well-known in electrical
engineering literature, which depending on the type of load curve, offers
general recommendations in three approaches; DSM (Demand Side Management), DS
(Distributed Storage) and DG. In this paper, to achieve quantitative indices of
improvement for Metro load curve using DG, firstly based on the analysis of
volume and consumption pattern of the main loads in Metro, the typical load
curve has been extracted. Using this curve, the result of using DG is shown by
quantitative parameters which represent the significant improvement in load
curve. These parameters can be used to calculate economic indicators such as
initial cost and ROI (Return of Investment).

</details>


### [304] [Online Learning Control Strategies for Industrial Processes with Application for Loosening and Conditioning](https://arxiv.org/abs/2506.08983)
*Yue Wu,Jianfu Cao,Ye Cao*

Main category: eess.SY

TL;DR: 提出了一种自适应Koopman模型预测控制框架HPC-AK-MPC，结合在线学习和历史安全约束，解决工业过程中的时变动态和安全操作问题。


<details>
  <summary>Details</summary>
Motivation: 解决复杂工业过程中时变动态和模型不确定性带来的安全操作挑战。

Method: 采用rEDMDc技术构建自适应Koopman模型，并结合历史过程约束（HPC）机制生成动态安全走廊。

Result: 在烟草松散和调节过程中应用，显著提高了关键质量变量的过程能力指数（Cpk）。

Conclusion: HPC-AK-MPC框架在提升控制性能的同时确保了操作安全，具有工业应用潜力。

Abstract: This paper proposes a novel adaptive Koopman Model Predictive Control (MPC)
framework, termed HPC-AK-MPC, designed to address the dual challenges of
time-varying dynamics and safe operation in complex industrial processes. The
framework integrates two core strategies: online learning and
historically-informed safety constraints. To contend with process
time-variance, a Recursive Extended Dynamic Mode Decomposition (rEDMDc)
technique is employed to construct an adaptive Koopman model capable of
updating its parameters from real-time data, endowing the controller with the
ability to continuously learn and track dynamic changes. To tackle the critical
issue of safe operation under model uncertainty, we introduce a novel
Historical Process Constraint (HPC) mechanism. This mechanism mines successful
operational experiences from a historical database and, by coupling them with
the confidence level of the online model, generates a dynamic "safety corridor"
for the MPC optimization problem. This approach transforms implicit expert
knowledge into explicit, adaptive constraints, establishing a dynamic balance
between pursuing optimal performance and ensuring robust safety. The proposed
HPC-AK-MPC method is applied to a real-world tobacco loosening and conditioning
process and systematically validated using an "advisor mode" simulation
framework with industrial data. Experimental results demonstrate that, compared
to historical operations, the proposed method significantly improves the
Process Capability Index (Cpk) for key quality variables across all tested
batches, proving its substantial potential in enhancing control performance
while guaranteeing operational safety.

</details>
