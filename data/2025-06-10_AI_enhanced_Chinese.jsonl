{"id": "2506.06837", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06837", "abs": "https://arxiv.org/abs/2506.06837", "authors": ["Eyal Briman", "Ehud Shapiro", "Nimrod Talmon"], "title": "AI-Generated Compromises for Coalition Formation", "comment": null, "summary": "The challenge of finding compromises between agent proposals is fundamental\nto AI subfields such as argumentation, mediation, and negotiation. Building on\nthis tradition, Elkind et al. (2021) introduced a process for coalition\nformation that seeks majority-supported proposals preferable to the status quo,\nusing a metric space where each agent has an ideal point. A crucial step in\nthis process involves identifying compromise proposals around which agent\ncoalitions can unite. How to effectively find such compromise proposals remains\nan open question. We address this gap by formalizing a model that incorporates\nagent bounded rationality and uncertainty, and by developing AI methods to\ngenerate compromise proposals. We focus on the domain of collaborative document\nwriting, such as the democratic drafting of a community constitution. Our\napproach uses natural language processing techniques and large language models\nto induce a semantic metric space over text. Based on this space, we design\nalgorithms to suggest compromise points likely to receive broad support. To\nevaluate our methods, we simulate coalition formation processes and show that\nAI can facilitate large-scale democratic text editing, a domain where\ntraditional tools are limited.", "AI": {"tldr": "论文提出了一种结合AI技术的方法，用于在协作文档写作中寻找妥协提案，以解决传统工具的局限性。", "motivation": "研究动机是解决如何在多代理系统中有效找到妥协提案的问题，尤其是在民主化文本编辑等场景中。", "method": "方法包括形式化一个结合代理有限理性和不确定性的模型，并利用自然语言处理技术和大型语言模型生成妥协提案。", "result": "通过模拟联盟形成过程，验证了AI方法在大规模民主文本编辑中的有效性。", "conclusion": "结论表明AI可以促进大规模民主文本编辑，填补传统工具的不足。"}}
{"id": "2506.07232", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07232", "abs": "https://arxiv.org/abs/2506.07232", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "comment": null, "summary": "Large language models (LLMs) possess extensive knowledge bases and strong\nreasoning capabilities, making them promising tools for complex, multi-agent\nplanning in embodied environments. However, despite LLMs' advanced abilities\nand the sophisticated modular design of agentic methods, existing LLM-based\nplanning algorithms remain limited by weak adaptation capabilities to\nmulti-agent embodied scenarios. We address this limitation by introducing a\nframework that enables LLM agents to learn and evolve both before and during\ntest time, equipping them with environment-relevant knowledge for better\nplanning and enhanced communication for improved cooperation. Inspired by\ncentralized training with decentralized execution in multi-agent reinforcement\nlearning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)}\nparadigm for multi-agent LLMs adaptation. At the individual level, LLM agents\nlearn a local utility function from exploratory datasets to better comprehend\nthe embodied environment, which is then queried during test time to support\ninformed decision-making. At the team level, LLM agents collaboratively and\niteratively maintain and update a shared cooperation knowledge list based on\nnew experiences, using it to guide more effective communication. By combining\nindividual learning with team evolution, LIET enables comprehensive and\nflexible adaptation for LLM agents. Our experiments on Communicative\nWatch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate\nthat LIET, instantiated with both LLaMA and GPT-4o, outperforms existing\nbaselines and exhibits strong cooperative planning abilities.", "AI": {"tldr": "论文提出了一种名为LIET的框架，通过个体学习和团队进化提升LLM在多智能体环境中的适应性和协作能力。", "motivation": "尽管LLM在多智能体规划中具有潜力，但其适应性不足限制了实际应用。LIET旨在解决这一问题。", "method": "采用个体学习局部效用函数和团队共享协作知识的LIET范式，结合集中训练与分散执行。", "result": "在Communicative Watch-And-Help和ThreeD-World基准测试中，LIET表现优于现有基线。", "conclusion": "LIET通过个体与团队的协同进化，显著提升了LLM在多智能体环境中的规划与协作能力。"}}
{"id": "2506.07332", "categories": ["cs.MA", "93A16"], "pdf": "https://arxiv.org/pdf/2506.07332", "abs": "https://arxiv.org/abs/2506.07332", "authors": ["Bo Fu", "Mingjie Bi", "Shota Umeda", "Takahiro Nakano", "Youichi Nonaka", "Quan Zhou", "Takaharu Matsui", "Dawn M. Tilbury", "Kira Barton"], "title": "Digital Twin-based Smart Manufacturing: Dynamic Line Reconfiguration for Disturbance Handling", "comment": "IEEE Transactions on Automation Science and Engineering (T-ASE) and\n  CASE 2025", "summary": "The increasing complexity of modern manufacturing, coupled with demand\nfluctuation, supply chain uncertainties, and product customization, underscores\nthe need for manufacturing systems that can flexibly update their\nconfigurations and swiftly adapt to disturbances. However, current research\nfalls short in providing a holistic reconfigurable manufacturing framework that\nseamlessly monitors system disturbances, optimizes alternative line\nconfigurations based on machine capabilities, and automates simulation\nevaluation for swift adaptations. This paper presents a dynamic manufacturing\nline reconfiguration framework to handle disturbances that result in operation\ntime changes. The framework incorporates a system process digital twin for\nmonitoring disturbances and triggering reconfigurations, a capability-based\nontology model capturing available agent and resource options, a configuration\noptimizer generating optimal line configurations, and a simulation generation\nprogram initializing simulation setups and evaluating line configurations at\napproximately 400x real-time speed. A case study of a battery production line\nhas been conducted to evaluate the proposed framework. In two implemented\ndisturbance scenarios, the framework successfully recovers system throughput\nwith limited resources, preventing the 26% and 63% throughput drops that would\nhave occurred without a reconfiguration plan. The reconfiguration optimizer\nefficiently finds optimal solutions, taking an average of 0.03 seconds to find\na reconfiguration plan for a manufacturing line with 51 operations and 40\navailable agents across 8 agent types.", "AI": {"tldr": "提出动态制造线重构框架，应对操作时间变化引起的干扰，通过数字孪生监控、能力本体模型、配置优化器和快速仿真，显著提升系统吞吐量。", "motivation": "现代制造复杂性增加，需求波动和供应链不确定性凸显了灵活适应干扰的制造系统的需求，现有研究缺乏全面解决方案。", "method": "结合数字孪生监控、能力本体模型、配置优化器和快速仿真生成程序，动态优化生产线配置。", "result": "在电池生产线案例中，成功防止26%和63%的吞吐量下降，优化器平均0.03秒找到最优配置。", "conclusion": "该框架有效应对干扰，显著提升制造系统适应性和吞吐量。"}}
{"id": "2506.06320", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.06320", "abs": "https://arxiv.org/abs/2506.06320", "authors": ["Beatrice F. R. Citterio", "Andrea Tangherloni"], "title": "EvoGrad: Metaheuristics in a Differentiable Wonderland", "comment": null, "summary": "Differentiable programming has revolutionised optimisation by enabling\nefficient gradient-based training of complex models, such as Deep Neural\nNetworks (NNs) with billions and trillions of parameters. However, traditional\nEvolutionary Computation (EC) and Swarm Intelligence (SI) algorithms, widely\nsuccessful in discrete or complex search spaces, typically do not leverage\nlocal gradient information, limiting their optimisation efficiency. In this\npaper, we introduce EvoGrad, a unified differentiable framework that integrates\nEC and SI with gradient-based optimisation through backpropagation. EvoGrad\nconverts conventional evolutionary and swarm operators (e.g., selection,\nmutation, crossover, and particle updates) into differentiable operators,\nfacilitating end-to-end gradient optimisation. Extensive experiments on\nbenchmark optimisation functions and training of small NN regressors reveal\nthat our differentiable versions of EC and SI metaheuristics consistently\noutperform traditional, gradient-agnostic algorithms in most scenarios. Our\nresults show the substantial benefits of fully differentiable evolutionary and\nswarm optimisation, setting a new standard for hybrid optimisation frameworks.", "AI": {"tldr": "EvoGrad是一个将进化计算（EC）和群体智能（SI）与梯度优化结合的统一框架，通过可微分算子提升优化效率。", "motivation": "传统EC和SI算法在离散或复杂搜索空间中成功，但未利用局部梯度信息，限制了优化效率。", "method": "EvoGrad将传统进化与群体算子（如选择、变异、交叉和粒子更新）转化为可微分算子，支持端到端梯度优化。", "result": "在基准优化函数和小型神经网络回归器训练中，EvoGrad的表现优于传统梯度无关算法。", "conclusion": "可微分进化与群体优化具有显著优势，为混合优化框架设定了新标准。"}}
{"id": "2506.07388", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07388", "abs": "https://arxiv.org/abs/2506.07388", "authors": ["Yun Hua", "Haosheng Chen", "Shiqin Wang", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) show strong collaborative performance in\nmulti-agent systems with predefined roles and workflows. However, in open-ended\nenvironments lacking coordination rules, agents tend to act in self-interested\nways. The central challenge in achieving coordination lies in credit assignment\n-- fairly evaluating each agent's contribution and designing pricing mechanisms\nthat align their heterogeneous goals. This problem is critical as LLMs\nincreasingly participate in complex human-AI collaborations, where fair\ncompensation and accountability rely on effective pricing mechanisms. Inspired\nby how human societies address similar coordination challenges (e.g., through\ntemporary collaborations such as employment or subcontracting), we propose a\ncooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley\nChain-of-Thought -- leveraging marginal contributions as a principled basis for\npricing -- with structured negotiation protocols for effective price matching,\nenabling LLM agents to coordinate through rational task-time pricing and\npost-task reward redistribution. This approach aligns agent incentives, fosters\ncooperation, and maintains autonomy. We evaluate Shapley-Coop across two\nmulti-agent games and a software engineering simulation, demonstrating that it\nconsistently enhances LLM agent collaboration and facilitates equitable credit\nassignment. These results highlight the effectiveness of Shapley-Coop's pricing\nmechanisms in accurately reflecting individual contributions during task\nexecution.", "AI": {"tldr": "论文提出了Shapley-Coop方法，通过Shapley Chain-of-Thought和结构化谈判协议解决开放环境中LLM代理的协作问题，实现公平信用分配和激励对齐。", "motivation": "在开放环境中，缺乏协调规则的LLM代理倾向于自利行为，导致协作困难。公平信用分配和定价机制设计是实现协调的关键。", "method": "提出Shapley-Coop方法，结合Shapley Chain-of-Thought（基于边际贡献的定价）和结构化谈判协议，实现任务时间定价和奖励再分配。", "result": "在多个多代理游戏和软件工程模拟中，Shapley-Coop显著提升了LLM代理的协作效果，并实现了公平的信用分配。", "conclusion": "Shapley-Coop通过有效的定价机制和激励对齐，成功解决了开放环境中LLM代理的协作问题。"}}
{"id": "2506.06322", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06322", "abs": "https://arxiv.org/abs/2506.06322", "authors": ["Polad Geidarov"], "title": "Neural networks with image recognition by pairs", "comment": null, "summary": "Neural networks based on metric recognition methods have a strictly\ndetermined architecture. Number of neurons, connections, as well as weights and\nthresholds values are calculated analytically, based on the initial conditions\nof tasks: number of recognizable classes, number of samples, metric expressions\nused. This paper discusses the possibility of transforming these networks in\norder to apply classical learning algorithms to them without using analytical\nexpressions that calculate weight values. In the received network, training is\ncarried out by recognizing images in pairs. This approach simplifies the\nlearning process and easily allows to expand the neural network by adding new\nimages to the recognition task. The advantages of these networks, including\nsuch as: 1) network architecture simplicity and transparency; 2) training\nsimplicity and reliability; 3) the possibility of using a large number of\nimages in the recognition problem using a neural network; 4) a consistent\nincrease in the number of recognizable classes without changing the previous\nvalues of weights and thresholds.", "AI": {"tldr": "论文探讨了将基于度量识别方法的神经网络转化为可应用经典学习算法的形式，避免使用解析表达式计算权重值，简化学习过程并支持网络扩展。", "motivation": "传统基于度量识别的神经网络架构固定，权重和阈值由解析表达式计算，限制了灵活性和扩展性。研究旨在通过改造网络，使其能应用经典学习算法，提升灵活性和可扩展性。", "method": "通过配对图像识别进行训练，避免解析表达式计算权重，简化学习过程并支持网络扩展。", "result": "改造后的网络具有架构简单透明、训练简便可靠、支持大量图像识别、可动态增加识别类别而不改变现有权重和阈值等优势。", "conclusion": "提出的方法成功将基于度量的神经网络转化为更灵活的形式，简化了学习过程并支持动态扩展，为图像识别任务提供了高效解决方案。"}}
{"id": "2506.07162", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2506.07162", "abs": "https://arxiv.org/abs/2506.07162", "authors": ["Mohammad T. Hajiaghayi", "Piotr Krysta", "Mohammad Mahdavi", "Suho Shin"], "title": "Delegation with Costly Inspection", "comment": "To appear at ACM EC 2025", "summary": "We study the problem of delegated choice with inspection cost (DCIC), which\nis a variant of the delegated choice problem by Kleinberg and Kleinberg (EC'18)\nas well as an extension of the Pandora's box problem with nonobligatory\ninspection (PNOI) by Doval (JET'18). In our model, an agent may strategically\nmisreport the proposed element's utility, unlike the standard delegated choice\nproblem which assumes that the agent truthfully reports the utility for the\nproposed alternative. Thus, the principal needs to inspect the proposed element\npossibly along with other alternatives to maximize its own utility, given an\nexogenous cost of inspecting each element. Further, the delegation itself\nincurs a fixed cost, thus the principal can decide whether to delegate or not\nand inspect by herself.\n  We show that DCIC indeed is a generalization of PNOI where the side\ninformation from a strategic agent is available at certain cost, implying its\nNP-hardness by Fu, Li, and Liu (STOC'23). We first consider a costless\ndelegation setting in which the cost of delegation is free. We prove that the\nmaximal mechanism over the pure delegation with a single inspection and an PNOI\npolicy without delegation achieves a $3$-approximation for DCIC with costless\ndelegation, which is further proven to be tight. These results hold even when\nthe cost comes from an arbitrary monotone set function, and can be improved to\na $2$-approximation if the cost of inspection is the same for every element. We\nextend these techniques by presenting a constant factor approximate mechanism\nfor the general setting for rich class of instances.", "AI": {"tldr": "本文研究了带有检查成本的委托选择问题（DCIC），扩展了Pandora盒子问题（PNOI），并探讨了代理可能策略性误报效用的情况。通过分析，证明了在无成本委托下，单一检查的纯委托机制与PNOI策略的3-近似解是紧的，并在特定条件下改进为2-近似。", "motivation": "研究DCIC问题的动机在于解决代理可能策略性误报效用的现实场景，同时扩展PNOI问题的应用范围。", "method": "通过理论分析，比较纯委托机制与PNOI策略，并设计近似机制以优化委托与检查成本。", "result": "在无成本委托下，证明了3-近似解是紧的，并在相同检查成本下改进为2-近似。", "conclusion": "DCIC问题在理论和实践中具有重要意义，提出的近似机制为复杂场景提供了有效解决方案。"}}
{"id": "2506.06290", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06290", "abs": "https://arxiv.org/abs/2506.06290", "authors": ["Mingyu Lu", "Ethan Weinberger", "Chanwoo Kim", "Su-In Lee"], "title": "CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning", "comment": null, "summary": "High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.", "AI": {"tldr": "CellCLIP是一种用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新型通道编码方案，显著提升了跨模态检索和下游任务的性能。", "motivation": "高内涵筛选数据（如Cell Painting）的语义与自然图像差异大，且不同扰动类型（如小分子与CRISPR基因敲除）难以在同一潜在空间表示，因此需要新的方法。", "method": "CellCLIP结合预训练图像编码器、新型通道编码方案和自然语言编码器，学习统一的潜在空间以对齐扰动与其形态效应。", "result": "CellCLIP在跨模态检索和下游任务中表现最佳，同时显著减少计算时间。", "conclusion": "CellCLIP为高内涵筛选数据提供了一种高效的跨模态对比学习解决方案。"}}
{"id": "2506.06361", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06361", "abs": "https://arxiv.org/abs/2506.06361", "authors": ["Tim Schneider", "Guillaume Duret", "Cristiana de Farias", "Roberto Calandra", "Liming Chen", "Jan Peters"], "title": "Tactile MNIST: Benchmarking Active Tactile Perception", "comment": null, "summary": "Tactile perception has the potential to significantly enhance dexterous\nrobotic manipulation by providing rich local information that can complement or\nsubstitute for other sensory modalities such as vision. However, because\ntactile sensing is inherently local, it is not well-suited for tasks that\nrequire broad spatial awareness or global scene understanding on its own. A\nhuman-inspired strategy to address this issue is to consider active perception\ntechniques instead. That is, to actively guide sensors toward regions with more\ninformative or significant features and integrate such information over time in\norder to understand a scene or complete a task. Both active perception and\ndifferent methods for tactile sensing have received significant attention\nrecently. Yet, despite advancements, both fields lack standardized benchmarks.\nTo bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an\nopen-source, Gymnasium-compatible benchmark specifically designed for active\ntactile perception tasks, including localization, classification, and volume\nestimation. Our benchmark suite offers diverse simulation scenarios, from\nsimple toy environments all the way to complex tactile perception tasks using\nvision-based tactile sensors. Furthermore, we also offer a comprehensive\ndataset comprising 13,500 synthetic 3D MNIST digit models and 153,600\nreal-world tactile samples collected from 600 3D printed digits. Using this\ndataset, we train a CycleGAN for realistic tactile simulation rendering. By\nproviding standardized protocols and reproducible evaluation frameworks, our\nbenchmark suite facilitates systematic progress in the fields of tactile\nsensing and active perception.", "AI": {"tldr": "论文提出了Tactile MNIST Benchmark Suite，一个用于主动触觉感知任务的开源基准测试套件，填补了触觉感知和主动感知领域缺乏标准化基准的空白。", "motivation": "触觉感知能增强机器人操作的灵活性，但其局部性限制了全局场景理解。主动感知技术可以弥补这一缺陷，但相关领域缺乏标准化基准。", "method": "引入Tactile MNIST Benchmark Suite，提供多样化的模拟场景和数据集，包括合成3D MNIST数字模型和真实触觉样本，并训练CycleGAN进行逼真渲染。", "result": "基准套件支持定位、分类和体积估计等任务，提供了标准化的协议和可复现的评估框架。", "conclusion": "该基准套件促进了触觉感知和主动感知领域的系统性进展。"}}
{"id": "2506.06451", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06451", "abs": "https://arxiv.org/abs/2506.06451", "authors": ["Joachim Deutscher", "Julian Zimmer"], "title": "A Koopman-backstepping approach to data-driven robust output regulation for linear parabolic systems", "comment": "11 pages, 3 figures", "summary": "In this paper a solution of the data-driven robust output regulation problem\nfor linear parabolic systems is presented. Both the system as well as the ODE,\ni.e., the disturbance model, describing the disturbances are unknown, but\nfinite-time sequential data obtained from measurements of the output to be\ncontrolled and additional boundary outputs are available. The data-driven\ncontroller is designed in the Koopman operator framework for PDEs, where the\nKoopman modes and eigenvalues are obtained from data using Hankel-DMD. It is\nshown that all system parameters and the eigenvalues of the disturbance model\ncan be recovered from the available measurements by solving an inverse\nSturm-Liouville problem. This allows to directly apply backstepping methods for\nthe robust regulator design. For this, closed-loop stability in the presence of\nsmall errors in the Hankel-DMD is verified in the nominal case. Robust output\nregulation is shown for non-destabilizing model uncertainties. A numerical\nexample demonstrates the results of the paper.", "AI": {"tldr": "本文提出了一种数据驱动的线性抛物系统鲁棒输出调节问题解决方案，利用Koopman算子框架设计控制器，并通过Hankel-DMD从数据中提取参数。", "motivation": "解决未知系统和扰动模型下的鲁棒输出调节问题，仅依赖有限时间序列数据。", "method": "基于Koopman算子框架设计数据驱动控制器，利用Hankel-DMD提取Koopman模式和特征值，并通过逆Sturm-Liouville问题恢复系统参数。", "result": "验证了闭环稳定性，并在非破坏性模型不确定性下实现了鲁棒输出调节。", "conclusion": "数值示例验证了方法的有效性，为数据驱动的鲁棒控制提供了新思路。"}}
{"id": "2506.06689", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.06689", "abs": "https://arxiv.org/abs/2506.06689", "authors": ["Wendi Sang", "Kai Li", "Runxuan Yang", "Jianqiang Huang", "Xiaolin Hu"], "title": "A Fast and Lightweight Model for Causal Audio-Visual Speech Separation", "comment": "8 pages, 5 figures", "summary": "Audio-visual speech separation (AVSS) aims to extract a target speech signal\nfrom a mixed signal by leveraging both auditory and visual (lip movement) cues.\nHowever, most existing AVSS methods exhibit complex architectures and rely on\nfuture context, operating offline, which renders them unsuitable for real-time\napplications. Inspired by the pipeline of RTFSNet, we propose a novel streaming\nAVSS model, named Swift-Net, which enhances the causal processing capabilities\nrequired for real-time applications. Swift-Net adopts a lightweight visual\nfeature extraction module and an efficient fusion module for audio-visual\nintegration. Additionally, Swift-Net employs Grouped SRUs to integrate\nhistorical information across different feature spaces, thereby improving the\nutilization efficiency of historical information. We further propose a causal\ntransformation template to facilitate the conversion of non-causal AVSS models\ninto causal counterparts. Experiments on three standard benchmark datasets\n(LRS2, LRS3, and VoxCeleb2) demonstrated that under causal conditions, our\nproposed Swift-Net exhibited outstanding performance, highlighting the\npotential of this method for processing speech in complex environments.", "AI": {"tldr": "Swift-Net是一种新型的流式音频-视觉语音分离模型，专注于实时应用，通过轻量级视觉特征提取和高效融合模块提升性能。", "motivation": "现有音频-视觉语音分离方法通常架构复杂且依赖未来上下文，无法满足实时需求。", "method": "采用轻量级视觉特征提取模块、高效融合模块和分组SRUs整合历史信息，并提出因果转换模板。", "result": "在LRS2、LRS3和VoxCeleb2数据集上表现出色，适用于复杂环境。", "conclusion": "Swift-Net展示了在实时音频-视觉语音分离中的潜力，尤其适合复杂环境。"}}
{"id": "2506.06440", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06440", "abs": "https://arxiv.org/abs/2506.06440", "authors": ["Chuhao Chen", "Zhiyang Dou", "Chen Wang", "Yiming Huang", "Anjun Chen", "Qiao Feng", "Jiatao Gu", "Lingjie Liu"], "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation", "comment": "Accepted by CVPR 2025", "summary": "Faithfully reconstructing textured shapes and physical properties from videos\npresents an intriguing yet challenging problem. Significant efforts have been\ndedicated to advancing such a system identification problem in this area.\nPrevious methods often rely on heavy optimization pipelines with a\ndifferentiable simulator and renderer to estimate physical parameters. However,\nthese approaches frequently necessitate extensive hyperparameter tuning for\neach scene and involve a costly optimization process, which limits both their\npracticality and generalizability. In this work, we propose a novel framework,\nVid2Sim, a generalizable video-based approach for recovering geometry and\nphysical properties through a mesh-free reduced simulation based on Linear\nBlend Skinning (LBS), offering high computational efficiency and versatile\nrepresentation capability. Specifically, Vid2Sim first reconstructs the\nobserved configuration of the physical system from video using a feed-forward\nneural network trained to capture physical world knowledge. A lightweight\noptimization pipeline then refines the estimated appearance, geometry, and\nphysical properties to closely align with video observations within just a few\nminutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,\nmesh-free simulation with high efficiency. Extensive experiments demonstrate\nthat our method achieves superior accuracy and efficiency in reconstructing\ngeometry and physical properties from video data.", "AI": {"tldr": "Vid2Sim是一种基于视频的通用框架，通过无网格简化模拟（基于LBS）高效恢复几何和物理属性，避免了传统方法的高成本优化问题。", "motivation": "从视频中忠实重建纹理形状和物理属性是一个具有挑战性的问题，传统方法依赖高成本优化和超参数调整，限制了实用性和泛化性。", "method": "Vid2Sim首先通过前馈神经网络从视频中重建物理系统的观察配置，随后通过轻量级优化管道细化估计的外观、几何和物理属性。", "result": "实验表明，Vid2Sim在从视频数据重建几何和物理属性方面具有更高的准确性和效率。", "conclusion": "Vid2Sim提供了一种高效、通用的解决方案，显著提升了从视频中恢复几何和物理属性的能力。"}}
{"id": "2506.06447", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.06447", "abs": "https://arxiv.org/abs/2506.06447", "authors": ["Jacob Erickson"], "title": "Fake Friends and Sponsored Ads: The Risks of Advertising in Conversational Search", "comment": "Accepted for publication at ACM CUI 2025", "summary": "Digital commerce thrives on advertising, with many of the largest technology\ncompanies relying on it as a significant source of revenue. However, in the\ncontext of information-seeking behavior, such as search, advertising may\ndegrade the user experience by lowering search quality, misusing user data for\ninappropriate personalization, potentially misleading individuals, or even\nleading them toward harm. These challenges remain significant as conversational\nsearch technologies, such as ChatGPT, become widespread. This paper critically\nexamines the future of advertising in conversational search, utilizing several\nspeculative examples to illustrate the potential risks posed to users who seek\nguidance on sensitive topics. Additionally, it provides an overview of the\nforms that advertising might take in this space and introduces the \"fake friend\ndilemma,\" the idea that a conversational agent may exploit unaligned user trust\nto achieve other objectives. This study presents a provocative discussion on\nthe future of online advertising in the space of conversational search and ends\nwith a call to action.", "AI": {"tldr": "论文探讨了对话搜索中广告的未来，分析了其对用户体验的潜在负面影响，并提出了“假朋友困境”的概念。", "motivation": "研究对话搜索技术（如ChatGPT）普及后，广告如何可能降低搜索质量、滥用用户数据或误导用户，尤其是敏感话题。", "method": "通过推测性案例和概念分析，探讨广告在对话搜索中的形式及其风险。", "result": "揭示了广告可能利用用户信任实现其他目标的风险，提出了“假朋友困境”的概念。", "conclusion": "呼吁关注对话搜索中广告的潜在危害，并提出行动建议。"}}
{"id": "2506.06283", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06283", "abs": "https://arxiv.org/abs/2506.06283", "authors": ["Juexiao Zhou", "Zhongyi Han", "Mankun Xin", "Xingwei He", "Guotao Wang", "Jiaoyan Song", "Gongning Luo", "Wenjia He", "Xintong Li", "Yuetan Chu", "Juanwen Chen", "Bo Wang", "Xia Wu", "Wenwen Duan", "Zhixia Guo", "Liyan Bai", "Yilin Pan", "Xuefei Bi", "Lu Liu", "Long Feng", "Xiaonan He", "Xin Gao"], "title": "Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow", "comment": null, "summary": "Global population aging presents increasing challenges to healthcare systems,\nwith coronary artery disease (CAD) responsible for approximately 17.8 million\ndeaths annually, making it a leading cause of global mortality. As CAD is\nlargely preventable, early detection and proactive management are essential. In\nthis work, we introduce DigitalShadow, an advanced early warning system for\nCAD, powered by a fine-tuned facial foundation model. The system is pre-trained\non 21 million facial images and subsequently fine-tuned into LiveCAD, a\nspecialized CAD risk assessment model trained on 7,004 facial images from 1,751\nsubjects across four hospitals in China. DigitalShadow functions passively and\ncontactlessly, extracting facial features from live video streams without\nrequiring active user engagement. Integrated with a personalized database, it\ngenerates natural language risk reports and individualized health\nrecommendations. With privacy as a core design principle, DigitalShadow\nsupports local deployment to ensure secure handling of user data.", "AI": {"tldr": "DigitalShadow是一种基于面部基础模型的CAD早期预警系统，通过无接触方式从视频流中提取面部特征，生成个性化风险报告和健康建议。", "motivation": "全球老龄化加剧了医疗系统压力，CAD是主要死因之一，早期检测和管理至关重要。", "method": "系统预训练2100万张面部图像，微调为LiveCAD模型，使用7004张来自1751名受试者的面部图像进行CAD风险评估。", "result": "DigitalShadow能被动、无接触地工作，生成自然语言风险报告和个性化建议，支持本地部署以保护隐私。", "conclusion": "DigitalShadow为CAD早期检测提供了高效、隐私安全的解决方案。"}}
{"id": "2506.07398", "categories": ["cs.MA", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07398", "abs": "https://arxiv.org/abs/2506.07398", "authors": ["Guibin Zhang", "Muxin Fu", "Guancheng Wan", "Miao Yu", "Kun Wang", "Shuicheng Yan"], "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.", "AI": {"tldr": "G-Memory是一种分层、代理化的记忆系统，用于提升多代理系统的自我进化能力，通过三层图层次结构管理交互，显著提高了任务成功率。", "motivation": "现有多代理系统（MAS）的记忆机制过于简单，忽视了代理间协作的复杂性，且缺乏跨试验和代理定制化，限制了系统的自我进化能力。", "method": "提出G-Memory，一种基于组织记忆理论的分层记忆系统，通过三层图（洞察图、查询图和交互图）管理代理间交互，并支持双向记忆检索。", "result": "在五个基准测试、三种LLM骨干和三种MAS框架中，G-Memory将任务成功率提高了20.89%，知识问答准确率提高了10.12%。", "conclusion": "G-Memory通过改进记忆架构，显著提升了多代理系统的协作能力和自我进化潜力，且无需修改现有框架。"}}
{"id": "2506.06325", "categories": ["cs.NE", "cs.AI", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.06325", "abs": "https://arxiv.org/abs/2506.06325", "authors": ["Viorica Rozina Chifu", "Tudor Cioara", "Cristina Bianca Pop", "Ionut Anghel"], "title": "Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies", "comment": null, "summary": "This paper proposes a decentralized model of energy cooperation between\nmicrogrids, in which decisions are made locally, at the level of the microgrid\ncommunity. Each microgrid is modeled as an autonomous agent that adopts a Hawk\nor Dove strategy, depending on the level of energy stored in the battery and\nits role in the energy trading process. The interactions between selling and\nbuying microgrids are modeled through an evolutionary algorithm. An individual\nin the algorithm population is represented as an energy trading matrix that\nencodes the amounts of energy traded between the selling and buying microgrids.\nThe population evolution is achieved by recombination and mutation operators.\nRecombination uses a specialized operator for matrix structures, and mutation\nis applied to the matrix elements according to a Gaussian distribution. The\nevaluation of an individual is made with a multi-criteria fitness function that\nconsiders the seller profit, the degree of energy stability at the community\nlevel, penalties for energy imbalance at the community level and for the\ndegradation of microgrids batteries. The method was tested on a simulated\nscenario with 100 microgrids, each with its own selling and buying thresholds,\nto reflect a realistic environment with variable storage characteristics of\nmicrogrids batteries. By applying the algorithm on this scenario, 95 out of the\n100 microgrids reached a stable energy state. This result confirms the\neffectiveness of the proposed model in achieving energy balance both at the\nindividual level, for each microgrid, and at the level of the entire community.", "AI": {"tldr": "论文提出了一种微电网间去中心化的能源合作模型，通过本地决策和进化算法实现能源平衡。", "motivation": "解决微电网间能源交易中的局部决策和全局能源平衡问题。", "method": "采用Hawk或Dove策略的自治代理模型，结合进化算法（重组和变异）优化能源交易矩阵。", "result": "在100个微电网的模拟场景中，95个达到稳定能源状态。", "conclusion": "模型在个体和社区层面均能有效实现能源平衡。"}}
{"id": "2506.07186", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2506.07186", "abs": "https://arxiv.org/abs/2506.07186", "authors": ["Jiarui Gan", "Rupak Majumdar"], "title": "Value-Set Iteration: Computing Optimal Correlated Equilibria in Infinite-Horizon Multi-Player Stochastic Games", "comment": null, "summary": "We study the problem of computing optimal correlated equilibria (CEs) in\ninfinite-horizon multi-player stochastic games, where correlation signals are\nprovided over time. In this setting, optimal CEs require history-dependent\npolicies; this poses new representational and algorithmic challenges as the\nnumber of possible histories grows exponentially with the number of time steps.\nWe focus on computing $(\\epsilon, \\delta)$-optimal CEs -- solutions that\nachieve a value within $\\epsilon$ of an optimal CE, while allowing the agents'\nincentive constraints to be violated by at most $\\delta$. Our main result is an\nalgorithm that computes an $(\\epsilon,\\delta)$-optimal CE in time polynomial in\n$1/(\\epsilon\\delta(1 - \\gamma))^{n+1}$, where $\\gamma$ is the discount factor,\nand $n$ is the number of agents. For (a slightly more general variant of)\nturn-based games, we further reduce the complexity to a polynomial in $n$. We\nalso establish that the bi-criterion approximation is necessary by proving\nmatching inapproximability bounds.\n  Our technical core is a novel approach based on inducible value sets, which\nleverages a compact representation of history-dependent CEs through the values\nthey induce to overcome the representational challenge. We develop the\nvalue-set iteration algorithm -- which operates by iteratively updating\nestimates of inducible value sets -- and characterize CEs as the greatest fixed\npoint of the update map. Our algorithm provides a groundwork for computing\noptimal CEs in general multi-player stochastic settings.", "AI": {"tldr": "该论文研究了在无限时间多玩家随机游戏中计算最优相关均衡（CEs）的问题，提出了一种基于可诱导值集的算法，能够在多项式时间内计算(ε, δ)-最优CE。", "motivation": "由于历史依赖策略在无限时间随机游戏中难以表示和计算，论文旨在解决这一挑战，并提出高效的近似算法。", "method": "提出了一种基于可诱导值集的算法，通过迭代更新值集估计来克服历史依赖策略的表示问题。", "result": "算法在多项式时间内计算(ε, δ)-最优CE，并在回合制游戏中进一步降低复杂度。论文还证明了双准则近似的必要性。", "conclusion": "该算法为多玩家随机游戏中的最优CE计算提供了基础，并通过可诱导值集的方法解决了历史依赖策略的表示难题。"}}
{"id": "2506.06291", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.06291", "abs": "https://arxiv.org/abs/2506.06291", "authors": ["Xiaoke Wang", "Batuhan Altundas", "Zhaoxin Li", "Aaron Zhao", "Matthew Gombolay"], "title": "Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks", "comment": "4 pages, 4 figures", "summary": "Mixed Integer Linear Programs (MILPs) are essential tools for solving\nplanning and scheduling problems across critical industries such as\nconstruction, manufacturing, and logistics. However, their widespread adoption\nis limited by long computational times, especially in large-scale, real-time\nscenarios. To address this, we present a learning-based framework that\nleverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph\nNeural Networks (GNNs), producing high-quality initial solutions for\nwarm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling\nProblems. Experimental results demonstrate that our method reduces optimization\ntime and variance compared to traditional techniques while maintaining solution\nquality and feasibility.", "AI": {"tldr": "提出了一种基于学习的方法，结合行为克隆和强化学习训练图神经网络，为混合整数线性规划（MILP）求解器提供高质量初始解，显著减少计算时间和方差。", "motivation": "混合整数线性规划在关键行业（如建筑、制造和物流）中广泛应用，但大规模实时场景下计算时间过长限制了其普及。", "method": "采用行为克隆（BC）和强化学习（RL）训练图神经网络（GNN），为多智能体任务分配和调度问题生成初始解，用于预热MILP求解器。", "result": "实验表明，该方法在保持解质量和可行性的同时，显著减少了优化时间和方差。", "conclusion": "该框架为MILP求解提供了高效且可靠的初始解生成方法，有望推动其在大规模实时场景中的应用。"}}
{"id": "2506.06381", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.HC", "cs.MA", "C.3; C.4; D.2.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.06381", "abs": "https://arxiv.org/abs/2506.06381", "authors": ["Trisanth Srinivasan", "Santosh Patapati", "Himani Musku", "Idhant Gode", "Aditya Arora", "Samvit Bhattacharya", "Abubakr Nazriev", "Sanika Hirave", "Zaryab Kanjiani", "Srinjoy Ghose", "Srinidhi Shetty"], "title": "CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems", "comment": null, "summary": "Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to\noperate in critical applications. However, traditional verification and\nvalidation methods often struggle to handle the unpredictable and dynamic\nnature of AI components. In this paper, we introduce CPS-Guard, a novel\nframework that employs multi-role orchestration to automate the iterative\nassurance process for AI-powered CPS. By assigning specialized roles (e.g.,\nsafety monitoring, security assessment, fault injection, and recovery planning)\nto dedicated agents within a simulated environment, CPS-Guard continuously\nevaluates and refines AI behavior against a range of dependability\nrequirements. We demonstrate the framework through a case study involving an\nautonomous vehicle navigating an intersection with an AI-based planner. Our\nresults show that CPS-Guard effectively detects vulnerabilities, manages\nperformance impacts, and supports adaptive recovery strategies, thereby\noffering a structured and extensible solution for rigorous V&V in safety- and\nsecurity-critical systems.", "AI": {"tldr": "CPS-Guard是一个通过多角色编排自动化AI驱动的CPS保证过程的框架，有效检测漏洞并支持自适应恢复策略。", "motivation": "传统验证方法难以应对AI组件的动态性和不可预测性，需要一种更有效的解决方案。", "method": "采用多角色编排，在模拟环境中分配专用代理（如安全监控、故障注入等）持续评估和优化AI行为。", "result": "在自动驾驶车辆的案例中，CPS-Guard成功检测漏洞、管理性能影响并支持自适应恢复。", "conclusion": "CPS-Guard为安全和安全关键系统提供了结构化且可扩展的V&V解决方案。"}}
{"id": "2506.06484", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06484", "abs": "https://arxiv.org/abs/2506.06484", "authors": ["Manuel Sage", "Khalil Al Handawi", "Yaoyao Fiona Zhao"], "title": "The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage", "comment": "Accepted for publication at the 19th ASME International Conference on\n  Energy Sustainability", "summary": "Power-to-Gas (P2G) technologies gain recognition for enabling the integration\nof intermittent renewables, such as wind and solar, into electricity grids.\nHowever, determining the most cost-effective operation of these systems is\ncomplex due to the volatile nature of renewable energy, electricity prices, and\nloads. Additionally, P2G systems are less efficient in converting and storing\nenergy compared to battery energy storage systems (BESs), and the benefits of\nconverting electricity into gas are not immediately apparent. Deep\nReinforcement Learning (DRL) has shown promise in managing the operation of\nenergy systems amidst these uncertainties. Yet, DRL techniques face\ndifficulties with the delayed reward characteristic of P2G system operation.\nPrevious research has mostly focused on short-term studies that look at the\nenergy conversion process, neglecting the long-term storage capabilities of\nP2G.\n  This study presents a new method by thoroughly examining how DRL can be\napplied to the economic operation of P2G systems, in combination with BESs and\ngas turbines, over extended periods. Through three progressively more complex\ncase studies, we assess the performance of DRL algorithms, specifically Deep\nQ-Networks and Proximal Policy Optimization, and introduce modifications to\nenhance their effectiveness. These modifications include integrating forecasts,\nimplementing penalties on the reward function, and applying strategic cost\ncalculations, all aimed at addressing the issue of delayed rewards. Our\nfindings indicate that while DRL initially struggles with the complex\ndecision-making required for P2G system operation, the adjustments we propose\nsignificantly improve its capability to devise cost-effective operation\nstrategies, thereby unlocking the potential for long-term energy storage in P2G\ntechnologies.", "AI": {"tldr": "本文提出了一种结合深度强化学习（DRL）的方法，用于优化Power-to-Gas（P2G）系统的长期经济运营，解决了延迟奖励问题，并通过改进算法提升了性能。", "motivation": "P2G技术在整合可再生能源方面具有潜力，但其运营成本高且效率低，且现有研究多关注短期能源转换，忽略了长期存储能力。", "method": "通过三个逐步复杂的案例研究，评估了Deep Q-Networks和Proximal Policy Optimization算法，并引入预测整合、奖励惩罚和战略成本计算等改进。", "result": "改进后的DRL显著提升了P2G系统的成本效益运营能力，解决了延迟奖励问题，释放了其长期储能潜力。", "conclusion": "DRL结合改进方法为P2G系统的长期经济运营提供了有效解决方案，推动了其在可再生能源整合中的应用。"}}
{"id": "2506.06756", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.06756", "abs": "https://arxiv.org/abs/2506.06756", "authors": ["Bikash Dutta", "Rishabh Ranjan", "Shyam Sathvik", "Mayank Vatsa", "Richa Singh"], "title": "Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?", "comment": "Accepted in Interspeech 2025", "summary": "Quantization is essential for deploying large audio language models (LALMs)\nefficiently in resource-constrained environments. However, its impact on\ncomplex tasks, such as zero-shot audio spoofing detection, remains\nunderexplored. This study evaluates the zero-shot capabilities of five LALMs,\nGAMA, LTU-AS, MERaLiON, Qwen-Audio, and SALMONN, across three distinct\ndatasets: ASVspoof2019, In-the-Wild, and WaveFake, and investigates their\nrobustness to quantization (FP32, FP16, INT8). Despite high initial spoof\ndetection accuracy, our analysis demonstrates severe predictive biases toward\nspoof classification across all models, rendering their practical performance\nequivalent to random classification. Interestingly, quantization to FP16\nprecision resulted in negligible performance degradation compared to FP32,\neffectively halving memory and computational requirements without materially\nimpacting accuracy. However, INT8 quantization intensified model biases,\nsignificantly degrading balanced accuracy. These findings highlight critical\narchitectural limitations and emphasize FP16 quantization as an optimal\ntrade-off, providing guidelines for practical deployment and future model\nrefinement.", "AI": {"tldr": "研究评估了五种大型音频语言模型在零样本音频欺骗检测任务中的表现，并探讨了量化（FP32、FP16、INT8）对其性能的影响。结果显示，尽管初始检测准确率高，但模型普遍存在对欺骗分类的预测偏差。FP16量化对性能影响小，而INT8量化显著降低平衡准确率。", "motivation": "量化是资源受限环境中部署大型音频语言模型的关键技术，但其对复杂任务（如零样本音频欺骗检测）的影响尚未充分研究。", "method": "研究评估了五种模型（GAMA、LTU-AS、MERaLiON、Qwen-Audio、SALMONN）在三个数据集（ASVspoof2019、In-the-Wild、WaveFake）上的零样本能力，并测试了不同量化精度（FP32、FP16、INT8）的鲁棒性。", "result": "模型普遍存在对欺骗分类的预测偏差，FP16量化对性能影响小，INT8量化显著降低平衡准确率。", "conclusion": "FP16量化是实用部署和未来模型优化的最佳折衷方案。"}}
{"id": "2506.06462", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06462", "abs": "https://arxiv.org/abs/2506.06462", "authors": ["Nicolás Violante", "Andreas Meuleman", "Alban Gauthier", "Frédo Durand", "Thibault Groueix", "George Drettakis"], "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements", "comment": "SIGGRAPH Conference Papers 2025. Project site:\n  https://repo-sam.inria.fr/nerphys/splat-and-replace/", "summary": "We leverage repetitive elements in 3D scenes to improve novel view synthesis.\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly\nimproved novel view synthesis but renderings of unseen and occluded parts\nremain low-quality if the training views are not exhaustive enough. Our key\nobservation is that our environment is often full of repetitive elements. We\npropose to leverage those repetitions to improve the reconstruction of\nlow-quality parts of the scene due to poor coverage and occlusions. We propose\na method that segments each repeated instance in a 3DGS reconstruction,\nregisters them together, and allows information to be shared among instances.\nOur method improves the geometry while also accounting for appearance\nvariations across instances. We demonstrate our method on a variety of\nsynthetic and real scenes with typical repetitive elements, leading to a\nsubstantial improvement in the quality of novel view synthesis.", "AI": {"tldr": "利用3D场景中的重复元素提升新视角合成质量。", "motivation": "现有方法（如NeRF和3DGS）在训练视角不足时，对未见过或被遮挡部分的渲染质量较低。环境中的重复元素为解决这一问题提供了机会。", "method": "提出一种方法，通过分割3DGS重建中的重复实例、对齐它们并共享信息，同时考虑实例间的外观变化。", "result": "在合成和真实场景中验证，新视角合成质量显著提升。", "conclusion": "利用重复元素能有效改善场景重建和新视角合成的质量。"}}
{"id": "2506.06473", "categories": ["cs.HC", "J.0"], "pdf": "https://arxiv.org/pdf/2506.06473", "abs": "https://arxiv.org/abs/2506.06473", "authors": ["Imran Fahad", "Danny Scott", "Azizul Zahid", "Matthew Bringle", "Srinayana Patil", "Ella Bevins", "Carmen Palileo", "Sai Swaminathan"], "title": "RadioGami: Batteryless, Long-Range Wireless Paper Sensors Using Tunnel Diodes", "comment": "The paper is published in the Proceedings of the ACM on Interactive,\n  Mobile, Wearable and Ubiquitous Technologies (IMWUT) and will be presented at\n  UbiComp 2025", "summary": "Paper-based interactive RF devices have opened new possibilities for wireless\nsensing, yet they are typically constrained by short operational ranges. This\npaper introduces RadioGami, a method for creating long-range, batteryless RF\nsensing surfaces on paper using low-cost, DIY materials like copper tape,\npaper, and off-the-shelf electronics paired with an affordable radio receiver\n(approx. $20). We explore the design space enabled by RadioGami, including\nsensing paper deformations like bending, tearing, and origami patterns (Miura,\nKresling) at ranges up to 45.73 meters. RadioGami employs a novel ultra-low\npower (35uW) switching circuit with a tunnel diode for wireless functionality.\nThese surfaces can sustainably operate by harvesting energy using tiny\nphotodiodes. We demonstrate applications that monitor object status, track user\ninteractions (rotation, sliding), and detect environmental changes. We\ncharacterize performance, sensitivity, range, and power consumption with\ndeployment studies. RadioGami advances sustainable, tangible, and batteryless\ninterfaces for embodied interaction.", "AI": {"tldr": "RadioGami是一种低成本、无电池的射频传感方法，利用铜带、纸张和现成电子设备实现长距离（45.73米）无线传感，适用于纸张变形和环境监测。", "motivation": "解决传统纸基射频设备操作距离短的问题，推动可持续、无电池的交互界面发展。", "method": "采用超低功耗（35uW）开关电路和隧道二极管，结合光二极管能量收集技术，实现长距离无线传感。", "result": "成功监测纸张变形（弯曲、撕裂、折纸）、物体状态和用户交互，最大范围达45.73米。", "conclusion": "RadioGami为可持续、无电池的交互界面提供了创新解决方案，扩展了纸基射频设备的应用潜力。"}}
{"id": "2506.06389", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06389", "abs": "https://arxiv.org/abs/2506.06389", "authors": ["Rifat Sadik", "Tanvir Rahman", "Arpan Bhattacharjee", "Bikash Chandra Halder", "Ismail Hossain"], "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images", "comment": null, "summary": "Deep learning models have shown remarkable success in dermatological image\nanalysis, offering potential for automated skin disease diagnosis. Previously,\nconvolutional neural network(CNN) based architectures have achieved immense\npopularity and success in computer vision (CV) based task like skin image\nrecognition, generation and video analysis. But with the emergence of\ntransformer based models, CV tasks are now are nowadays carrying out using\nthese models. Vision Transformers (ViTs) is such a transformer-based models\nthat have shown success in computer vision. It uses self-attention mechanisms\nto achieve state-of-the-art performance across various tasks. However, their\nreliance on global attention mechanisms makes them susceptible to adversarial\nperturbations. This paper aims to investigate the susceptibility of ViTs for\nmedical images to adversarial watermarking-a method that adds so-called\nimperceptible perturbations in order to fool models. By generating adversarial\nwatermarks through Projected Gradient Descent (PGD), we examine the\ntransferability of such attacks to CNNs and analyze the performance defense\nmechanism -- adversarial training. Results indicate that while performance is\nnot compromised for clean images, ViTs certainly become much more vulnerable to\nadversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,\nadversarial training raises it up to 90.0%.", "AI": {"tldr": "论文研究了Vision Transformers（ViTs）在医学图像中对对抗性水印攻击的脆弱性，并通过对抗训练提升了防御性能。", "motivation": "随着ViTs在计算机视觉任务中的成功应用，研究其在医学图像中的对抗性攻击脆弱性具有重要意义。", "method": "使用Projected Gradient Descent（PGD）生成对抗性水印，测试ViTs和CNNs的脆弱性，并分析对抗训练的效果。", "result": "ViTs对对抗性攻击非常脆弱（准确率降至27.6%），但对抗训练能显著提升防御性能（准确率恢复至90.0%）。", "conclusion": "ViTs在医学图像中易受对抗性攻击，但对抗训练是一种有效的防御手段。"}}
{"id": "2506.07400", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07400", "abs": "https://arxiv.org/abs/2506.07400", "authors": ["Philip Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "comment": "7 pages, 6 figures. Accepted to the 2025 IEEE 8th International\n  Conference on Multimedia Information Processing and Retrieval (MIPR). Code\n  and platform available at https://github.com/Purdue-M2/MedChat", "summary": "The integration of deep learning-based glaucoma detection with large language\nmodels (LLMs) presents an automated strategy to mitigate ophthalmologist\nshortages and improve clinical reporting efficiency. However, applying general\nLLMs to medical imaging remains challenging due to hallucinations, limited\ninterpretability, and insufficient domain-specific medical knowledge, which can\npotentially reduce clinical accuracy. Although recent approaches combining\nimaging models with LLM reasoning have improved reporting, they typically rely\non a single generalist agent, restricting their capacity to emulate the diverse\nand complex reasoning found in multidisciplinary medical teams. To address\nthese limitations, we propose MedChat, a multi-agent diagnostic framework and\nplatform that combines specialized vision models with multiple role-specific\nLLM agents, all coordinated by a director agent. This design enhances\nreliability, reduces hallucination risk, and enables interactive diagnostic\nreporting through an interface tailored for clinical review and educational\nuse. Code available at https://github.com/Purdue-M2/MedChat.", "AI": {"tldr": "MedChat是一个多代理诊断框架，结合专用视觉模型和角色特定的LLM代理，通过协调提升青光眼检测的可靠性和交互性。", "motivation": "解决通用LLM在医学影像中存在的幻觉、解释性不足和领域知识缺乏问题，同时模拟多学科医疗团队的复杂推理。", "method": "提出MedChat框架，结合专用视觉模型和多个角色特定的LLM代理，由导演代理协调，优化诊断报告。", "result": "提升了可靠性，减少了幻觉风险，并支持交互式诊断报告，适用于临床和教育用途。", "conclusion": "MedChat通过多代理设计有效解决了通用LLM在医学应用中的局限性，为自动化青光眼检测提供了可行方案。"}}
{"id": "2506.06332", "categories": ["cs.NE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06332", "abs": "https://arxiv.org/abs/2506.06332", "authors": ["Mikko Stenlund"], "title": "Introduction to Predictive Coding Networks for Machine Learning", "comment": "22 pages", "summary": "Predictive coding networks (PCNs) constitute a biologically inspired\nframework for understanding hierarchical computation in the brain, and offer an\nalternative to traditional feedforward neural networks in ML. This note serves\nas a quick, onboarding introduction to PCNs for machine learning practitioners.\nWe cover the foundational network architecture, inference and learning update\nrules, and algorithmic implementation. A concrete image-classification task\n(CIFAR-10) is provided as a benchmark-smashing application, together with an\naccompanying Python notebook containing the PyTorch implementation.", "AI": {"tldr": "本文简要介绍了预测编码网络（PCNs），为机器学习从业者提供快速入门，包括网络架构、推理和学习规则，以及具体实现。", "motivation": "PCNs是一种生物启发的框架，用于理解大脑中的分层计算，并为机器学习提供替代传统前馈神经网络的方法。", "method": "介绍了PCNs的基础架构、推理和学习更新规则，并通过PyTorch实现了一个具体的图像分类任务（CIFAR-10）。", "result": "提供了CIFAR-10任务的基准测试结果，并附有Python笔记本实现。", "conclusion": "PCNs为机器学习提供了一种新的生物启发式方法，并通过具体任务展示了其潜力。"}}
{"id": "2506.07316", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2506.07316", "abs": "https://arxiv.org/abs/2506.07316", "authors": ["Azhar Iqbal", "Ishan Honhaga", "Eyoel Teffera", "Anthony Perry", "Robin Baker", "Glenn Pearce", "Claudia Szabo"], "title": "Vulnerability and Defence: A Case for Stackelberg Game Dynamics", "comment": "20 pages, 5 figures", "summary": "This paper examines the tactical interaction between drones and tanks in\nmodern warfare through game theory, particularly focusing on Stackelberg\nequilibrium and backward induction. It describes a high-stakes conflict between\ntwo teams: one using advanced drones for attack, and the other defending using\ntanks. The paper conceptualizes this as a sequential game, illustrating the\ncomplex strategic dynamics similar to Stackelberg competition, where moves and\ncountermoves are carefully analyzed and predicted.", "AI": {"tldr": "论文通过博弈论分析无人机与坦克在现代战争中的战术互动，聚焦于斯塔克尔伯格均衡和逆向归纳法。", "motivation": "研究现代战争中无人机与坦克的高风险对抗，揭示其战略动态。", "method": "采用斯塔克尔伯格竞争模型，将冲突建模为序贯博弈，分析双方的策略互动。", "result": "展示了无人机攻击与坦克防御的复杂策略动态，验证了斯塔克尔伯格均衡的适用性。", "conclusion": "论文为现代战争中的战术决策提供了理论支持，强调了博弈论在军事战略中的重要性。"}}
{"id": "2506.06292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06292", "abs": "https://arxiv.org/abs/2506.06292", "authors": ["Tianyuan Shi", "Canbin Huang", "Fanqi Wan", "Longguang Zhong", "Ziyi Yang", "Weizhou Shen", "Xiaojun Quan", "Ming Yan"], "title": "Mutual-Taught for Co-adapting Policy and Reward Models", "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "During the preference optimization of large language models (LLMs),\ndistribution shifts may arise between newly generated model samples and the\ndata used to train the reward model (RM). This shift reduces the efficacy of\nthe RM, which in turn negatively impacts the performance of the policy model\n(PM). To address this challenge, we propose Mutual-Taught, a self-training\nmethod that iteratively improves both the PM and RM without requiring\nadditional human annotation. Our approach mirrors the expectation-maximization\n(EM) algorithm. In the E-step, the PM is updated using feedback from the\ncurrent RM, guiding the PM toward a better approximation of the latent optimal\npreference distribution. In the M-step, we update the RM by constructing\ntraining data from the outputs of the PM before and after the E-step update.\nThis process ensures that the RM adapts to the evolving policy distribution.\nExperimental results demonstrate that this iterative approach leads to\nconsistent improvements in both models. Specifically, our 8B policy model,\nLLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on\nAlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par\nwith GPT-4o-2024-08-06 on RewardBench.", "AI": {"tldr": "论文提出Mutual-Taught方法，通过自训练迭代优化策略模型和奖励模型，解决分布偏移问题，无需额外人工标注。", "motivation": "在大型语言模型偏好优化中，分布偏移会降低奖励模型的效果，进而影响策略模型性能。", "method": "采用类似EM算法的自训练方法，交替更新策略模型和奖励模型。", "result": "实验显示迭代方法显著提升模型性能，8B策略模型在AlpacaEval-2上胜率为54.1%，8B奖励模型性能媲美GPT-4o。", "conclusion": "Mutual-Taught方法有效解决了分布偏移问题，提升了模型性能。"}}
{"id": "2506.06394", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06394", "abs": "https://arxiv.org/abs/2506.06394", "authors": ["Yash Turkar", "Youngjin Kim", "Karthik Dantu"], "title": "Active Illumination Control in Low-Light Environments using NightHawk", "comment": null, "summary": "Subterranean environments such as culverts present significant challenges to\nrobot vision due to dim lighting and lack of distinctive features. Although\nonboard illumination can help, it introduces issues such as specular\nreflections, overexposure, and increased power consumption. We propose\nNightHawk, a framework that combines active illumination with exposure control\nto optimize image quality in these settings. NightHawk formulates an online\nBayesian optimization problem to determine the best light intensity and\nexposure-time for a given scene. We propose a novel feature detector-based\nmetric to quantify image utility and use it as the cost function for the\noptimizer. We built NightHawk as an event-triggered recursive optimization\npipeline and deployed it on a legged robot navigating a culvert beneath the\nErie Canal. Results from field experiments demonstrate improvements in feature\ndetection and matching by 47-197% enabling more reliable visual estimation in\nchallenging lighting conditions.", "AI": {"tldr": "NightHawk框架通过结合主动照明和曝光控制，优化地下环境中的图像质量，提升特征检测和匹配性能。", "motivation": "地下环境（如涵洞）光线昏暗且缺乏特征，机器人视觉面临挑战。现有照明方法存在反射、过曝和功耗问题。", "method": "提出NightHawk框架，利用在线贝叶斯优化动态调整光照强度和曝光时间，并基于特征检测器设计新指标作为优化目标。", "result": "实验表明，特征检测和匹配性能提升47-197%，视觉估计更可靠。", "conclusion": "NightHawk有效解决了地下环境中的视觉挑战，提升了机器人导航能力。"}}
{"id": "2506.06519", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06519", "abs": "https://arxiv.org/abs/2506.06519", "authors": ["Yuyan Lin", "Hao Zhou", "Chengming Hu", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Hierarchical Debate-Based Large Language Model (LLM) for Complex Task Planning of 6G Network Management", "comment": null, "summary": "6G networks have become increasingly complicated due to novel network\narchitecture and newly emerging signal processing and transmission techniques,\nleading to significant burdens to 6G network management. Large language models\n(LLMs) have recently been considered a promising technique to equip 6G networks\nwith AI-native intelligence. Different from most existing studies that only\nconsider a single LLM, this work involves a multi-LLM debate-based scheme for\n6G network management, where multiple LLMs can collaboratively improve the\ninitial solution sequentially. Considering the complex nature of 6G domain, we\npropose a novel hierarchical debate scheme: LLMs will first debate the sub-task\ndecomposition, and then debate each subtask step-by-step. Such a hierarchical\napproach can significantly reduce the overall debate difficulty by sub-task\ndecomposition, aligning well with the complex nature of 6G networks and\nensuring the final solution qualities. In addition, to better evaluate the\nproposed technique, we have defined a novel dataset named 6GPlan, including 110\ncomplex 6G network management tasks and 5000 keyword solutions. Finally, the\nexperiments show that the proposed hierarchical debate can significantly\nimprove performance compared to baseline techniques, e.g. more than 30%\ncoverage rate and global recall rate improvement.", "AI": {"tldr": "论文提出了一种基于多LLM分层辩论的方案，用于6G网络管理，通过子任务分解和逐步辩论提升解决方案质量，实验表明性能显著优于基线方法。", "motivation": "6G网络架构复杂，传统方法难以有效管理，多LLM协作能提供更智能的解决方案。", "method": "采用分层辩论方案，先分解子任务，再逐步辩论每个子任务，结合6GPlan数据集进行验证。", "result": "实验显示，分层辩论方案在覆盖率和全局召回率上比基线方法提升超过30%。", "conclusion": "多LLM分层辩论方案能有效应对6G网络管理的复杂性，显著提升性能。"}}
{"id": "2506.06772", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.06772", "abs": "https://arxiv.org/abs/2506.06772", "authors": ["Rishabh Ranjan", "Kishan Pipariya", "Mayank Vatsa", "Richa Singh"], "title": "SynHate: Detecting Hate Speech in Synthetic Deepfake Audio", "comment": "Accepted in Interspeech 2025", "summary": "The rise of deepfake audio and hate speech, powered by advanced\ntext-to-speech, threatens online safety. We present SynHate, the first\nmultilingual dataset for detecting hate speech in synthetic audio, spanning 37\nlanguages. SynHate uses a novel four-class scheme: Real-normal, Real-hate,\nFake-normal, and Fake-hate. Built from MuTox and ADIMA datasets, it captures\ndiverse hate speech patterns globally and in India. We evaluate five leading\nself-supervised models (Whisper-small/medium, XLS-R, AST, mHuBERT), finding\nnotable performance differences by language, with Whisper-small performing best\noverall. Cross-dataset generalization remains a challenge. By releasing SynHate\nand baseline code, we aim to advance robust, culturally sensitive, and\nmultilingual solutions against synthetic hate speech. The dataset is available\nat https://www.iab-rubric.org/resources.", "AI": {"tldr": "SynHate是首个多语言数据集，用于检测合成音频中的仇恨言论，覆盖37种语言，采用四类分类方案。评估了五种自监督模型，发现Whisper-small表现最佳。跨数据集泛化仍是挑战。", "motivation": "深度伪造音频和仇恨言论的兴起威胁在线安全，需要多语言和跨文化的解决方案。", "method": "基于MuTox和ADIMA数据集构建SynHate，采用四类分类方案（Real-normal, Real-hate, Fake-normal, Fake-hate），并评估五种自监督模型。", "result": "Whisper-small在整体性能上表现最佳，但跨数据集泛化仍具挑战性。", "conclusion": "SynHate的发布旨在推动针对合成仇恨言论的鲁棒、文化敏感和多语言解决方案。"}}
{"id": "2506.06483", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.06483", "abs": "https://arxiv.org/abs/2506.06483", "authors": ["Yao Ni", "Song Wen", "Piotr Koniusz", "Anoop Cherian"], "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis", "comment": null, "summary": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by\nadapting the model to generate images containing specific subjects. However,\nexisting fine-tuning methods suffer from two key issues: underfitting, where\nthe model fails to reliably capture subject identity, and overfitting, where it\nmemorizes the subject image and reduces background diversity. To address these\nchallenges, we propose two auxiliary consistency losses for diffusion\nfine-tuning. First, a prior consistency regularization loss ensures that the\npredicted diffusion noise for prior (non-subject) images remains consistent\nwith that of the pretrained model, improving fidelity. Second, a subject\nconsistency regularization loss enhances the fine-tuned model's robustness to\nmultiplicative noise modulated latent code, helping to preserve subject\nidentity while improving diversity. Our experimental results demonstrate that\nincorporating these losses into fine-tuning not only preserves subject identity\nbut also enhances image diversity, outperforming DreamBooth in terms of CLIP\nscores, background variation, and overall visual quality.", "AI": {"tldr": "论文提出两种一致性损失函数，解决Stable Diffusion微调中的欠拟合和过拟合问题，提升生成图像的多样性和主体保真度。", "motivation": "现有微调方法在主体驱动图像合成中存在欠拟合（无法可靠捕捉主体身份）和过拟合（记忆主体图像并减少背景多样性）问题。", "method": "提出两种辅助一致性损失：1）先验一致性正则化损失，确保非主体图像的扩散噪声与预训练模型一致；2）主体一致性正则化损失，增强模型对噪声调制潜码的鲁棒性。", "result": "实验表明，加入这些损失后，模型在CLIP分数、背景多样性和视觉质量上优于DreamBooth。", "conclusion": "通过一致性损失，模型在保持主体身份的同时提升了图像多样性，解决了微调中的关键问题。"}}
{"id": "2506.06774", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.06774", "abs": "https://arxiv.org/abs/2506.06774", "authors": ["Luca-Maxim Meinhardt", "Simon Demharter", "Michael Rietzler", "Mark Colley", "Thomas Eßmeyer", "Enrico Rukzio"], "title": "Mind Games! Exploring the Impact of Dark Patterns in Mixed Reality Scenarios", "comment": null, "summary": "Mixed Reality (MR) integrates virtual objects with the real world, offering\npotential but raising concerns about misuse through dark patterns. This study\nexplored the effects of four dark patterns, adapted from prior research, and\napplied to MR across three targets: places, products, and people. In a\ntwo-factorial within-subject study with 74 participants, we analyzed 13 videos\nsimulating MR experiences during a city walk. Results show that all dark\npatterns significantly reduced user comfort, increased reactance, and decreased\nthe intention to use MR glasses, with the most disruptive effects linked to\npersonal or monetary manipulation. Additionally, the dark patterns of Emotional\nand Sensory Manipulation and Hiding Information produced similar impacts on the\nuser in MR, suggesting a re-evaluation of current classifications to go beyond\ndeceptive design techniques. Our findings highlight the importance of\ndeveloping ethical design guidelines and tools to detect and prevent dark\npatterns as immersive technologies continue to evolve.", "AI": {"tldr": "研究探讨了混合现实（MR）中四种黑暗模式对用户的影响，发现它们显著降低舒适度、增加抗拒感并减少使用意愿，尤其是涉及个人或金钱操纵的模式。", "motivation": "随着混合现实技术的发展，其潜在滥用问题（如黑暗模式）引发了伦理担忧，需研究其影响并制定对策。", "method": "采用双因素被试内设计，74名参与者观看13个模拟MR城市漫步的视频，分析四种黑暗模式对地点、产品和目标的影响。", "result": "所有黑暗模式均显著降低用户舒适度、增加抗拒感并减少使用意愿，个人或金钱操纵效果最显著。情感与感官操纵及信息隐藏效果相似。", "conclusion": "需重新评估黑暗模式分类，并制定伦理设计指南及工具，以预防沉浸技术中的滥用。"}}
{"id": "2506.06480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06480", "abs": "https://arxiv.org/abs/2506.06480", "authors": ["A. Postlmayr", "P. Cosman", "S. Dey"], "title": "(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training", "comment": null, "summary": "We introduce a fitness tracking system that enables remote monitoring for\nexercises using only a RGB smartphone camera, making fitness tracking more\nprivate, scalable, and cost effective. Although prior work explored automated\nexercise supervision, existing models are either too limited in exercise\nvariety or too complex for real-world deployment. Prior approaches typically\nfocus on a small set of exercises and fail to generalize across diverse\nmovements. In contrast, we develop a robust, multitask motion analysis model\ncapable of performing exercise detection and repetition counting across\nhundreds of exercises, a scale far beyond previous methods. We overcome\nprevious data limitations by assembling a large-scale fitness dataset, Olympia\ncovering more than 1,900 exercises. To our knowledge, our vision-language model\nis the first that can perform multiple tasks on skeletal fitness data. On\nOlympia, our model can detect exercises with 76.5% accuracy and count\nrepetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting\na single vision-language transformer model for both exercise identification and\nrep counting, we take a significant step toward democratizing AI-powered\nfitness tracking.", "AI": {"tldr": "提出了一种基于RGB智能手机摄像头的远程健身追踪系统，具有隐私性、可扩展性和成本效益。", "motivation": "现有健身追踪模型要么运动种类有限，要么过于复杂难以部署，无法泛化到多样化动作。", "method": "开发了一个多任务运动分析模型，结合大规模健身数据集Olympia（覆盖1900多种运动），利用视觉-语言模型进行运动检测和重复计数。", "result": "模型在Olympia数据集上运动检测准确率为76.5%，重复计数准确率为85.3%。", "conclusion": "通过单一视觉-语言模型实现运动识别和计数，推动了AI健身追踪的普及。"}}
{"id": "2506.07935", "categories": ["cs.MA", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2506.07935", "abs": "https://arxiv.org/abs/2506.07935", "authors": ["Pavel Naumov", "Jia Tao"], "title": "Diffusion of Responsibility in Collective Decision Making", "comment": null, "summary": "The term \"diffusion of responsibility'' refers to situations in which\nmultiple agents share responsibility for an outcome, obscuring individual\naccountability. This paper examines this frequently undesirable phenomenon in\nthe context of collective decision-making mechanisms.\n  The work shows that if a decision is made by two agents, then the only way to\navoid diffusion of responsibility is for one agent to act as a \"dictator'',\nmaking the decision unilaterally. In scenarios with more than two agents, any\ndiffusion-free mechanism is an \"elected dictatorship'' where the agents elect a\nsingle agent to make a unilateral decision.\n  The technical results are obtained by defining a bisimulation of\ndecision-making mechanisms, proving that bisimulation preserves\nresponsibility-related properties, and establishing the results for a smallest\nbisimular mechanism.", "AI": {"tldr": "论文研究了集体决策机制中的责任扩散现象，发现避免责任扩散的唯一方式是让一个代理人独裁决策。在多代理人场景中，无扩散机制需选举一个独裁者。", "motivation": "探讨集体决策中责任扩散的普遍问题，明确个体责任模糊的原因。", "method": "通过定义决策机制的互模拟，证明互模拟保留责任相关属性，并在最小互模拟机制中验证结果。", "result": "在双代理人场景中，避免责任扩散需独裁决策；多代理人场景中需选举独裁者。", "conclusion": "避免责任扩散的唯一方式是引入独裁决策机制，无论代理人数量多少。"}}
{"id": "2506.06362", "categories": ["cs.NE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06362", "abs": "https://arxiv.org/abs/2506.06362", "authors": ["Dejun Xu", "Jijia Chen", "Gary G. Yen", "Min Jiang"], "title": "CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms", "comment": null, "summary": "Bilevel optimization poses a significant computational challenge due to its\nnested structure, where each upper-level candidate solution requires solving a\ncorresponding lower-level problem. While evolutionary algorithms (EAs) are\neffective at navigating such complex landscapes, their high resource demands\nremain a key bottleneck -- particularly the redundant evaluation of numerous\nunpromising lower-level tasks. Despite recent advances in multitasking and\ntransfer learning, resource waste persists. To address this issue, we propose a\nnovel resource allocation framework for bilevel EAs that selectively identifies\nand focuses on promising lower-level tasks. Central to our approach is a\ncontrastive ranking network that learns relational patterns between paired\nupper- and lower-level solutions online. This knowledge guides a\nreference-based ranking strategy that prioritizes tasks for optimization and\nadaptively controls resampling based on estimated population quality.\nComprehensive experiments across five state-of-the-art bilevel algorithms show\nthat our framework significantly reduces computational cost while preserving --\nor even enhancing -- solution accuracy. This work offers a generalizable\nstrategy to improve the efficiency of bilevel EAs, paving the way for more\nscalable bilevel optimization.", "AI": {"tldr": "提出了一种新的资源分配框架，通过对比排序网络选择性优化有潜力的下层任务，显著降低计算成本并保持或提升解的质量。", "motivation": "双层优化的嵌套结构导致计算成本高，现有方法存在资源浪费问题，需改进。", "method": "使用对比排序网络在线学习上下层解的关系模式，指导基于参考的排序策略，优先优化有潜力的任务。", "result": "在五种先进双层算法上验证，显著减少计算成本，同时保持或提升解精度。", "conclusion": "该框架为双层EA提供了通用高效策略，推动更可扩展的双层优化。"}}
{"id": "2506.06293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06293", "abs": "https://arxiv.org/abs/2506.06293", "authors": ["Junyi Liu", "Stanley Kok"], "title": "Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks", "comment": "WITS 2024 (Workshop on Information Technologies and Systems 2024)", "summary": "Agencies such as Standard & Poor's and Moody's provide bank credit ratings\nthat influence economic stability and decision-making by stakeholders. Accurate\nand timely predictions support informed decision-making, regulatory actions,\nand investor protection. However, a complete interbank connection graph is\noften unavailable due to privacy concerns, complicating the direct application\nof Graph Neural Networks (GNNs) for rating prediction. our research utilizes\npersistent homology to construct a network that captures relationships among\nbanks and combines this with a traditional lending network to create a\nheterogeneous network that integrates information from both sources, leading to\nimproved predictions. Experiments on a global, real-world dataset validate the\neffectiveness of HTGNN. This research has implications for investors and\nregulatory bodies in enhancing proactive risk mitigation and the implementation\nof effective market interventions.The code can be find at\nhttps://github.com/Liu-Jun-Yi/HTGNN.", "AI": {"tldr": "论文提出了一种结合持久同源性和传统借贷网络的异构图神经网络（HTGNN），用于银行信用评级预测，解决了隐私问题导致的完整银行间连接图缺失问题。", "motivation": "银行信用评级对经济稳定和利益相关者决策至关重要，但隐私问题限制了完整银行间连接图的获取，影响了图神经网络的直接应用。", "method": "利用持久同源性构建银行关系网络，与传统借贷网络结合形成异构图，通过HTGNN整合信息进行评级预测。", "result": "在真实全球数据集上的实验验证了HTGNN的有效性，提升了预测准确性。", "conclusion": "HTGNN为投资者和监管机构提供了增强风险缓解和市场干预的工具，代码已开源。"}}
{"id": "2506.06474", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "cs.NI", "I.4.8; I.2.10; I.2.11; I.2.9; C.2.4"], "pdf": "https://arxiv.org/pdf/2506.06474", "abs": "https://arxiv.org/abs/2506.06474", "authors": ["Everett Richards", "Bipul Thapa", "Lena Mashayekhy"], "title": "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception", "comment": "This paper has been accepted to IEEE EDGE 2025. The final version\n  will be published in IEEE Xplore later this year", "summary": "Accurate and reliable object detection is critical for ensuring the safety\nand efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board\nperception systems have limited accuracy due to occlusions and blind spots,\nwhile cloud-based solutions introduce significant latency, making them\nunsuitable for real-time processing demands required for autonomous driving in\ndynamic environments. To address these challenges, we introduce an innovative\nframework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that\nleverages edge computing and multi-CAV collaboration for real-time,\nmulti-perspective object detection. Our ECOD framework integrates two key\nalgorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and\nVariable Object Tally and Evaluation (VOTE). PACE aggregates detection data\nfrom multiple CAVs on an edge server to enhance perception in scenarios where\nindividual CAVs have limited visibility. VOTE utilizes a consensus-based voting\nmechanism to improve the accuracy of object classification by integrating data\nfrom multiple CAVs. Both algorithms are designed at the edge to operate in\nreal-time, ensuring low-latency and reliable decision-making for CAVs. We\ndevelop a hardware-based controlled testbed consisting of camera-equipped\nrobotic CAVs and an edge server to evaluate the efficacy of our framework. Our\nexperimental results demonstrate the significant benefits of ECOD in terms of\nimproved object classification accuracy, outperforming traditional\nsingle-perspective onboard approaches by up to 75%, while ensuring low-latency,\nedge-driven real-time processing. This research highlights the potential of\nedge computing to enhance collaborative perception for latency-sensitive\nautonomous systems.", "AI": {"tldr": "论文提出了一种基于边缘计算和多车协作的实时物体检测框架ECOD，通过PACE和VOTE算法提升CAV的感知能力，实验显示其分类准确率比传统方法高75%。", "motivation": "传统车载感知系统因遮挡和盲区精度有限，云端解决方案延迟高，无法满足实时需求，因此需要一种低延迟、高精度的协作感知框架。", "method": "ECOD框架结合PACE（感知聚合与协作估计）和VOTE（基于共识的投票机制）算法，利用边缘计算和多车协作实现实时物体检测。", "result": "实验表明，ECOD在物体分类准确率上比传统单视角方法提升75%，同时保持低延迟。", "conclusion": "边缘计算能显著提升协作感知能力，适用于延迟敏感的自动驾驶系统。"}}
{"id": "2506.06564", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06564", "abs": "https://arxiv.org/abs/2506.06564", "authors": ["Han Wang", "Keyan Miao", "Diego Madeira", "Antonis Papachristodoulou"], "title": "Learning Neural Controllers with Optimality and Stability Guarantees Using Input-Output Dissipativity", "comment": "submitted to Automatica", "summary": "Deep learning methods have demonstrated significant potential for addressing\ncomplex nonlinear control problems. For real-world safety-critical tasks,\nhowever, it is crucial to provide formal stability guarantees for the designed\ncontrollers. In this paper, we propose a new framework for designing neural\ncontrollers that achieve both stability and optimality with respect to certain\nfunctions. Our key idea is to exploit the concept of input-output dissipativity\nof nonlinear systems by learning neural storage functions and supply rate\nfunctions. As a generalization of Lyapunov theory, dissipativity theory\nprovides a natural connection to optimal control theory, offering both\nstability guarantees and meaningful optimality certificates. The neural\ncontrollers can be directly derived from the learned supply rate functions and\nguarantee closed-loop stability while inheriting optimality properties that can\nbe shaped towards user-defined control objectives. Extensive numerical\nexperiments demonstrate the effectiveness of our approach.", "AI": {"tldr": "提出了一种基于输入-输出耗散性理论的神经控制器设计框架，兼顾稳定性和最优性。", "motivation": "为安全关键任务提供具有形式化稳定性保证的神经控制器。", "method": "利用非线性系统的输入-输出耗散性理论，学习神经存储函数和供应率函数。", "result": "神经控制器在闭环稳定性下继承了最优性，并通过数值实验验证了有效性。", "conclusion": "该框架为神经控制器设计提供了稳定性和最优性的理论支持。"}}
{"id": "2506.07036", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07036", "abs": "https://arxiv.org/abs/2506.07036", "authors": ["Jiawei Jin", "Zhuhan Yang", "Yixuan Zhou", "Zhiyong Wu"], "title": "\"In This Environment, As That Speaker\": A Text-Driven Framework for Multi-Attribute Speech Conversion", "comment": "Accepted by Interspeech2025", "summary": "We propose TES-VC (Text-driven Environment and Speaker controllable Voice\nConversion), a text-driven voice conversion framework with independent control\nof speaker timbre and environmental acoustics. TES-VC processes simultaneous\ntext inputs for target voice and environment, accurately generating speech\nmatching described timbre/environment while preserving source content. Trained\non synthetic data with decoupled vocal/environment features via latent\ndiffusion modeling, our method eliminates interference between attributes. The\nRetrieval-Based Timbre Control (RBTC) module enables precise manipulation using\nabstract descriptions without paired data. Experiments confirm TES-VC\neffectively generates contextually appropriate speech in both timbre and\nenvironment with high content retention and superior controllability which\ndemonstrates its potential for widespread applications.", "AI": {"tldr": "TES-VC是一种文本驱动的语音转换框架，可独立控制说话者音色和环境声学，通过潜在扩散建模和检索式音色控制模块实现高精度生成。", "motivation": "解决语音转换中音色和环境声学独立控制的难题，同时保留源内容。", "method": "使用潜在扩散建模分离音色和环境特征，结合检索式音色控制模块（RBTC）实现精确控制。", "result": "实验表明TES-VC能生成上下文匹配的语音，保留内容且控制性强。", "conclusion": "TES-VC在广泛应用中具有潜力，展示了高可控性和内容保留能力。"}}
{"id": "2506.06494", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.06494", "abs": "https://arxiv.org/abs/2506.06494", "authors": ["Lei Lan", "Zixuan Lu", "Chun Yuan", "Weiwei Xu", "Hao Su", "Huamin Wang", "Chenfanfu Jiang", "Yin Yang"], "title": "JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics", "comment": null, "summary": "In parallel simulation, convergence and parallelism are often seen as\ninherently conflicting objectives. Improved parallelism typically entails\nlighter local computation and weaker coupling, which unavoidably slow the\nglobal convergence. This paper presents a novel GPU algorithm that achieves\nconvergence rates comparable to fullspace Newton's method while maintaining\ngood parallelizability just like the Jacobi method. Our approach is built on a\nkey insight into the phenomenon of overshoot. Overshoot occurs when a local\nsolver aggressively minimizes its local energy without accounting for the\nglobal context, resulting in a local update that undermines global convergence.\nTo address this, we derive a theoretically second-order optimal solution to\nmitigate overshoot. Furthermore, we adapt this solution into a pre-computable\nform. Leveraging Cubature sampling, our runtime cost is only marginally higher\nthan the Jacobi method, yet our algorithm converges nearly quadratically as\nNewton's method. We also introduce a novel full-coordinate formulation for more\nefficient pre-computation. Our method integrates seamlessly with the\nincremental potential contact method and achieves second-order convergence for\nboth stiff and soft materials. Experimental results demonstrate that our\napproach delivers high-quality simulations and outperforms state-of-the-art GPU\nmethods with 50 to 100 times better convergence.", "AI": {"tldr": "提出一种新颖的GPU算法，在保持良好并行性的同时，实现接近牛顿法的收敛速度。通过解决局部求解器的超调问题，并结合预计算和全坐标公式，显著提升了收敛效率。", "motivation": "在并行模拟中，收敛性和并行性通常被视为冲突目标。本文旨在解决这一矛盾，提出一种既能高效并行又能快速收敛的方法。", "method": "基于对超调现象的理论分析，提出二阶最优解并转化为预计算形式。结合Cubature采样和全坐标公式，实现高效并行计算。", "result": "实验表明，该方法收敛速度接近牛顿法，性能优于现有GPU方法50至100倍。", "conclusion": "该方法成功平衡了收敛性和并行性，适用于刚性和软材料的高质量模拟。"}}
{"id": "2506.06825", "categories": ["cs.HC", "cs.CR", "68T10, 68T45, 68M25", "I.4.9; I.5.4; K.4.1; K.6.5"], "pdf": "https://arxiv.org/pdf/2506.06825", "abs": "https://arxiv.org/abs/2506.06825", "authors": ["Shijing He", "Yaxiong Lei", "Zihan Zhang", "Yuzhou Sun", "Shujun Li", "Chi Zhang", "Juan Ye"], "title": "Identity Deepfake Threats to Biometric Authentication Systems: Public and Expert Perspectives", "comment": null, "summary": "Generative AI (Gen-AI) deepfakes pose a rapidly evolving threat to biometric\nauthentication, yet a significant gap exists between expert understanding of\nthese risks and public perception. This disconnection creates critical\nvulnerabilities in systems trusted by millions. To bridge this gap, we\nconducted a comprehensive mixed-method study, surveying 408 professionals\nacross key sectors and conducting in-depth interviews with 37 participants (25\nexperts, 12 general public [non-experts]). Our findings reveal a paradox: while\nthe public increasingly relies on biometrics for convenience, experts express\ngrave concerns about the spoofing of static modalities like face and voice\nrecognition. We found significant demographic and sector-specific divides in\nawareness and trust, with finance professionals, for example, showing\nheightened skepticism. To systematically analyze these threats, we introduce a\nnovel Deepfake Kill Chain model, adapted from Hutchins et al.'s cybersecurity\nframeworks to map the specific attack vectors used by malicious actors against\nbiometric systems. Based on this model and our empirical findings, we propose a\ntri-layer mitigation framework that prioritizes dynamic biometric signals\n(e.g., eye movements), robust privacy-preserving data governance, and targeted\neducational initiatives. This work provides the first empirically grounded\nroadmap for defending against AI-generated identity threats by aligning\ntechnical safeguards with human-centered insights.", "AI": {"tldr": "论文探讨了生成式AI（Gen-AI）深度伪造对生物识别认证的威胁，揭示了专家与公众认知的差异，并提出了一种新的Deepfake Kill Chain模型和三层次缓解框架。", "motivation": "生成式AI深度伪造技术对生物识别认证构成快速演变的威胁，但公众对其风险的认知与专家存在显著差距，导致系统脆弱性增加。", "method": "通过混合方法研究，调查了408名专业人士，并深入访谈了37名参与者（25名专家，12名非专家），提出了Deepfake Kill Chain模型。", "result": "研究发现公众依赖生物识别的便利性，而专家则担忧静态模态（如面部和语音识别）的欺骗风险，并揭示了不同行业和人群的认知差异。", "conclusion": "论文提出了一个三层次缓解框架，结合动态生物信号、数据治理和教育倡议，为防御AI生成的身份威胁提供了实证支持的路线图。"}}
{"id": "2506.06517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06517", "abs": "https://arxiv.org/abs/2506.06517", "authors": ["Mingqi Jiang", "Chanho Kim", "Chen Ziwen", "Li Fuxin"], "title": "GS4: Generalizable Sparse Splatting Semantic SLAM", "comment": "13 pages, 6 figures", "summary": "Traditional SLAM algorithms are excellent at camera tracking but might\ngenerate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting\n(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map\nbuilding. However, existing GS-based SLAM methods rely on per-scene\noptimization which is time-consuming and does not generalize to diverse scenes\nwell. In this work, we introduce the first generalizable GS-based semantic SLAM\nalgorithm that incrementally builds and updates a 3D scene representation from\nan RGB-D video stream using a learned generalizable network. Our approach\nstarts from an RGB-D image recognition backbone to predict the Gaussian\nparameters from every downsampled and backprojected image location.\nAdditionally, we seamlessly integrate 3D semantic segmentation into our GS\nframework, bridging 3D mapping and recognition through a shared backbone. To\ncorrect localization drifting and floaters, we propose to optimize the GS for\nonly 1 iteration following global localization. We demonstrate state-of-the-art\nsemantic SLAM performance on the real-world benchmark ScanNet with an order of\nmagnitude fewer Gaussians compared to other recent GS-based methods, and\nshowcase our model's generalization capability through zero-shot transfer to\nthe NYUv2 and TUM RGB-D datasets.", "AI": {"tldr": "提出了一种基于高斯泼溅（GS）的可泛化语义SLAM算法，通过学习网络从RGB-D视频流增量构建3D场景表示，解决了现有GS-SLAM方法依赖场景优化且泛化性差的问题。", "motivation": "传统SLAM算法在相机跟踪上表现优秀，但生成的3D地图分辨率低且不完整。现有GS-SLAM方法依赖场景优化，耗时长且泛化性不足。", "method": "使用可泛化网络从RGB-D图像预测高斯参数，集成3D语义分割，并通过全局定位后仅优化1次高斯泼溅来修正定位漂移和浮点问题。", "result": "在ScanNet基准测试中表现最优，高斯数量比其他GS方法少一个数量级，并在NYUv2和TUM RGB-D数据集上展示了零样本泛化能力。", "conclusion": "该方法在语义SLAM中实现了高效且泛化的3D地图构建，为未来研究提供了新方向。"}}
{"id": "2506.06374", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.06374", "abs": "https://arxiv.org/abs/2506.06374", "authors": ["Maxime Fabre", "Lyubov Dudchenko", "Emre Neftci"], "title": "Structured State Space Model Dynamics and Parametrization for Spiking Neural Networks", "comment": null, "summary": "Multi-state spiking neurons such as the adaptive leaky integrate-and-fire\n(AdLIF) neuron offer compelling alternatives to conventional deep learning\nmodels thanks to their sparse binary activations, second-order nonlinear\nrecurrent dynamics, and efficient hardware realizations. However, such internal\ndynamics can cause instabilities during inference and training, often limiting\nperformance and scalability. Meanwhile, state space models (SSMs) excel in long\nsequence processing using linear state-intrinsic recurrence resembling spiking\nneurons' subthreshold regime. Here, we establish a mathematical bridge between\nSSMs and second-order spiking neuron models. Based on structure and\nparametrization strategies of diagonal SSMs, we propose two novel spiking\nneuron models. The first extends the AdLIF neuron through timestep training and\nlogarithmic reparametrization to facilitate training and improve final\nperformance. The second additionally brings initialization and structure from\ncomplex-state SSMs, broadening the dynamical regime to oscillatory dynamics.\nTogether, our two models achieve beyond or near state-of-the-art (SOTA)\nperformances for reset-based spiking neuron models across both event-based and\nraw audio speech recognition datasets. We achieve this with a favorable number\nof parameters and required dynamic memory while maintaining high activity\nsparsity. Our models demonstrate enhanced scalability in network size and\nstrike a favorable balance between performance and efficiency with respect to\nSSM models.", "AI": {"tldr": "论文提出两种新型脉冲神经元模型，通过结合状态空间模型（SSMs）和二阶脉冲神经元，提升了训练稳定性和性能，在语音识别任务中达到或接近SOTA水平。", "motivation": "传统脉冲神经元模型在推理和训练中存在不稳定性，限制了性能和扩展性；而SSMs在长序列处理中表现优异。论文旨在结合两者的优势。", "method": "基于对角SSMs的结构和参数化策略，提出两种模型：一种改进AdLIF神经元，另一种引入复数状态SSMs的初始化和结构。", "result": "两种模型在语音识别任务中表现优异，参数和动态内存需求较低，同时保持高活动稀疏性。", "conclusion": "模型在性能和效率之间取得了良好平衡，展示了在更大网络规模中的扩展潜力。"}}
{"id": "2506.06294", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2506.06294", "abs": "https://arxiv.org/abs/2506.06294", "authors": ["Yunqing Liu", "Wenqi Fan", "Xiaoyong Wei", "Qing Li"], "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning", "comment": null, "summary": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on.", "AI": {"tldr": "GLProtein是一个结合全局结构相似性和局部氨基酸细节的蛋白质预训练框架，显著提升了预测准确性。", "motivation": "尽管蛋白质序列分析已取得进展，但整合蛋白质结构信息仍有潜力，尤其是从局部氨基酸到全局结构相似性的多层次信息。", "method": "GLProtein创新性地结合了蛋白质掩码建模、三重结构相似性评分、3D距离编码和基于子结构的氨基酸分子编码。", "result": "实验表明，GLProtein在蛋白质-蛋白质相互作用预测、接触预测等任务中优于现有方法。", "conclusion": "GLProtein为蛋白质功能预测提供了更全面的结构信息整合方法，具有广泛的应用潜力。"}}
{"id": "2506.06476", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06476", "abs": "https://arxiv.org/abs/2506.06476", "authors": ["Pushyami Kaveti", "Ambjorn Grimsrud Waldum", "Hanumant Singh", "Martin Ludvigsen"], "title": "Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception", "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs)\ndemand robust spatial perception capabilities, including Simultaneous\nLocalization and Mapping (SLAM), to support both remote and autonomous tasks.\nVision-based systems have been integral to these advancements, capturing rich\ncolor and texture at low cost while enabling semantic scene understanding.\nHowever, underwater conditions -- such as light attenuation, backscatter, and\nlow contrast -- often degrade image quality to the point where traditional\nvision-based SLAM pipelines fail. Moreover, these pipelines typically rely on\nmonocular or stereo inputs, limiting their scalability to the multi-camera\nconfigurations common on many vehicles. To address these issues, we propose to\nleverage multi-modal sensing that fuses data from multiple sensors-including\ncameras, inertial measurement units (IMUs), and acoustic devices-to enhance\nsituational awareness and enable robust, real-time SLAM. We explore both\ngeometric and learning-based techniques along with semantic analysis, and\nconduct experiments on the data collected from a work-class ROV during several\nfield deployments in the Trondheim Fjord. Through our experimental results, we\ndemonstrate the feasibility of real-time reliable state estimation and\nhigh-quality 3D reconstructions in visually challenging underwater conditions.\nWe also discuss system constraints and identify open research questions, such\nas sensor calibration, limitations with learning-based methods, that merit\nfurther exploration to advance large-scale underwater operations.", "AI": {"tldr": "论文提出了一种多模态感知方法，融合多种传感器数据以提升水下SLAM的鲁棒性和实时性。", "motivation": "水下环境中的光线衰减、散射和低对比度等问题导致传统视觉SLAM失效，且现有方法多依赖单目或双目输入，难以适应多摄像头配置。", "method": "结合几何与学习技术，融合摄像头、IMU和声学设备数据，进行语义分析，并在真实水下环境中测试。", "result": "实验证明在视觉挑战性水下条件下，能实现实时可靠的状态估计和高质量3D重建。", "conclusion": "方法有效但存在传感器校准和学习方法局限性等问题，需进一步研究以推动大规模水下作业。"}}
{"id": "2506.06575", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06575", "abs": "https://arxiv.org/abs/2506.06575", "authors": ["Ryan Piansky", "Daniel K. Molzahn", "Nicole D. Jackson", "J. Kyle Skolfield"], "title": "Evaluating Undergrounding Decisions for Wildfire Ignition Risk Mitigation across Multiple Hazards", "comment": null, "summary": "With electric power infrastructure increasingly susceptible to impacts from\nclimate-driven natural disasters, there is an increasing need for optimization\nalgorithms that determine where to harden the power grid. Prior work has\nprimarily developed optimal hardening approaches for specific acute disaster\nscenarios. Given the extensive costs of hardening the grid, it is important to\nunderstand how a particular set of resilience investments will perform under\nmultiple types of natural hazards. Using a large-scale test case representing\nthe Texas power system, this paper aims to understand how line undergrounding\ninvestment decisions made for wildfire ignition risk mitigation perform during\na range of wildfire, hurricane, and wind events. Given the varying geographical\nspread and damage profile of these events, we show that investment decisions\nmade to address one type of natural disaster do not necessarily improve broader\nresilience outcomes, supporting the need for co-optimization across a range of\nhazards.", "AI": {"tldr": "本文研究了电力基础设施在多种自然灾害下的优化加固策略，指出单一灾害的加固投资未必能提升整体韧性。", "motivation": "电力基础设施易受气候灾害影响，需优化加固策略，但现有研究多针对单一灾害，缺乏多灾害协同优化的分析。", "method": "以德克萨斯电力系统为例，分析针对野火的线路地下化投资在野火、飓风和风灾中的表现。", "result": "针对单一灾害的投资未必能提升整体韧性，需多灾害协同优化。", "conclusion": "电力系统加固需考虑多灾害协同优化，单一灾害策略效果有限。"}}
{"id": "2506.07073", "categories": ["cs.SD", "cs.HC", "eess.AS", "68T01", "J.5"], "pdf": "https://arxiv.org/pdf/2506.07073", "abs": "https://arxiv.org/abs/2506.07073", "authors": ["Emmanuel Deruty", "Maarten Grachten"], "title": "Insights on Harmonic Tones from a Generative Music Experiment", "comment": "15th International Workshop on Machine Learning and Music, September\n  9, 2024, Vilnius, Lithuania", "summary": "The ultimate purpose of generative music AI is music production. The\nstudio-lab, a social form within the art-science branch of\ncross-disciplinarity, is a way to advance music production with AI music\nmodels. During a studio-lab experiment involving researchers, music producers,\nand an AI model for music generating bass-like audio, it was observed that the\nproducers used the model's output to convey two or more pitches with a single\nharmonic complex tone, which in turn revealed that the model had learned to\ngenerate structured and coherent simultaneous melodic lines using monophonic\nsequences of harmonic complex tones. These findings prompt a reconsideration of\nthe long-standing debate on whether humans can perceive harmonics as distinct\npitches and highlight how generative AI can not only enhance musical creativity\nbut also contribute to a deeper understanding of music.", "AI": {"tldr": "生成音乐AI的终极目标是音乐制作，通过跨学科的studio-lab实验，发现AI模型能生成结构化的多旋律线，挑战了人类对谐波音高的感知传统观点。", "motivation": "探索生成音乐AI在音乐制作中的应用，以及其对音乐创作和理解的潜在贡献。", "method": "通过studio-lab实验，结合研究人员、音乐制作人和AI模型，观察模型生成的多音高谐波复合音的应用。", "result": "AI模型能够生成结构化和连贯的多旋律线，揭示了谐波音高感知的新视角。", "conclusion": "生成音乐AI不仅提升音乐创造力，还深化了对音乐的理解，挑战了传统音高感知理论。"}}
{"id": "2506.07020", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.07020", "abs": "https://arxiv.org/abs/2506.07020", "authors": ["Qiujie Dong", "Jiepeng Wang", "Rui Xu", "Cheng Lin", "Yuan Liu", "Shiqing Xin", "Zichun Zhong", "Xin Li", "Changhe Tu", "Taku Komura", "Leif Kobbelt", "Scott Schaefer", "Wenping Wang"], "title": "CrossGen: Learning and Generating Cross Fields for Quad Meshing", "comment": "Project page: https://anonymousproject-homepage.github.io/", "summary": "Cross fields play a critical role in various geometry processing tasks,\nespecially for quad mesh generation. Existing methods for cross field\ngeneration often struggle to balance computational efficiency with generation\nquality, using slow per-shape optimization. We introduce CrossGen, a novel\nframework that supports both feed-forward prediction and latent generative\nmodeling of cross fields for quad meshing by unifying geometry and cross field\nrepresentations within a joint latent space. Our method enables extremely fast\ncomputation of high-quality cross fields of general input shapes, typically\nwithin one second without per-shape optimization. Our method assumes a\npoint-sampled surface, or called a point-cloud surface, as input, so we can\naccommodate various different surface representations by a straightforward\npoint sampling process. Using an auto-encoder network architecture, we encode\ninput point-cloud surfaces into a sparse voxel grid with fine-grained latent\nspaces, which are decoded into both SDF-based surface geometry and cross\nfields. We also contribute a dataset of models with both high-quality signed\ndistance fields (SDFs) representations and their corresponding cross fields,\nand use it to train our network. Once trained, the network is capable of\ncomputing a cross field of an input surface in a feed-forward manner, ensuring\nhigh geometric fidelity, noise resilience, and rapid inference. Furthermore,\nleveraging the same unified latent representation, we incorporate a diffusion\nmodel for computing cross fields of new shapes generated from partial input,\nsuch as sketches. To demonstrate its practical applications, we validate\nCrossGen on the quad mesh generation task for a large variety of surface\nshapes. Experimental results...", "AI": {"tldr": "CrossGen是一个新颖的框架，通过联合几何和交叉场表示在潜在空间中，实现了交叉场的高效生成和质量平衡，适用于四边形网格生成。", "motivation": "现有交叉场生成方法在计算效率和质量之间难以平衡，通常需要缓慢的逐形状优化。", "method": "使用自编码器网络架构，将输入点云表面编码为稀疏体素网格，解码为基于SDF的几何和交叉场。结合扩散模型支持部分输入生成。", "result": "方法能在秒级内生成高质量交叉场，具有高几何保真度、噪声鲁棒性和快速推理能力。", "conclusion": "CrossGen在四边形网格生成任务中表现出色，适用于多种表面形状。"}}
{"id": "2506.06829", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.06829", "abs": "https://arxiv.org/abs/2506.06829", "authors": ["Hritom Das", "Imran Fahad", "SNB Tushar", "Sk Hasibul Alam", "Graham Buchanan", "Danny Scott", "Garrett S. Rose", "Sai Swaminathan"], "title": "In-Sensor Motion Recognition with Memristive System and Light Sensing Surfaces", "comment": "The paper was published in the 2024 IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI)", "summary": "In this paper, we introduce a novel device architecture that merges\nmemristive devices with light-sensing surfaces, for energy-efficient motion\nrecognition at the edge. Our light-sensing surface captures motion data through\nin-sensor computation. This data is then processed using a memristive system\nequipped with a HfO2-based synaptic device, coupled with a winner-take-all\n(WTA) circuit, tailored for low-power motion classification tasks. We validate\nour end-to-end system using four distinct human hand gestures - left-to-right,\nright-to-left, bottom-to-top, and top-to-bottom movements - to assess energy\nefficiency and classification robustness. Our experiments show that the system\nrequires an average of only 4.17 nJ for taking our processed analog signal and\nmapping weights onto our memristive system and 0.952 nJ for testing per\nmovement class, achieving 97.22% accuracy even under 5% noise interference. A\nkey advantage of our proposed architecture is its low energy requirement,\nenabling the integration of energy-harvesting solutions such as solar power for\nsustainable autonomous operation. Additionally, our approach enhances data\nprivacy by processing data locally, reducing the need for external data\ntransmission and storage.", "AI": {"tldr": "提出了一种结合忆阻器和光感表面的新型设备架构，用于边缘计算中的高效运动识别。", "motivation": "解决边缘计算中运动识别的高能耗问题，同时提升数据隐私。", "method": "利用光感表面进行传感器内计算，结合基于HfO2的忆阻系统和WTA电路进行低功耗分类。", "result": "系统平均能耗为4.17 nJ（训练）和0.952 nJ（测试），分类准确率达97.22%（5%噪声下）。", "conclusion": "该架构低能耗且支持能量收集，适用于可持续自主运行，同时增强数据隐私。"}}
{"id": "2506.06537", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.06537", "abs": "https://arxiv.org/abs/2506.06537", "authors": ["Seung-jae Lee", "Paul Hongsuck Seo"], "title": "Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models", "comment": "Accepted on INTERSPEECH2025", "summary": "Audiovisual segmentation (AVS) aims to identify visual regions corresponding\nto sound sources, playing a vital role in video understanding, surveillance,\nand human-computer interaction. Traditional AVS methods depend on large-scale\npixel-level annotations, which are costly and time-consuming to obtain. To\naddress this, we propose a novel zero-shot AVS framework that eliminates\ntask-specific training by leveraging multiple pretrained models. Our approach\nintegrates audio, vision, and text representations to bridge modality gaps,\nenabling precise sound source segmentation without AVS-specific annotations. We\nsystematically explore different strategies for connecting pretrained models\nand evaluate their efficacy across multiple datasets. Experimental results\ndemonstrate that our framework achieves state-of-the-art zero-shot AVS\nperformance, highlighting the effectiveness of multimodal model integration for\nfinegrained audiovisual segmentation.", "AI": {"tldr": "提出了一种零样本视听分割框架，利用预训练模型，无需任务特定训练即可实现声音源分割。", "motivation": "传统方法依赖大规模像素级标注，成本高且耗时。", "method": "整合音频、视觉和文本表示，探索预训练模型连接策略。", "result": "在多个数据集上实现最先进的零样本性能。", "conclusion": "多模态模型整合对精细视听分割有效。"}}
{"id": "2506.06765", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.06765", "abs": "https://arxiv.org/abs/2506.06765", "authors": ["Raoof HojatJalali", "Edmondo Trentin"], "title": "Employing Discrete Fourier Transform in Representational Learning", "comment": "Preprint", "summary": "Image Representation learning via input reconstruction is a common technique\nin machine learning for generating representations that can be effectively\nutilized by arbitrary downstream tasks. A well-established approach is using\nautoencoders to extract latent representations at the network's compression\npoint. These representations are valuable because they retain essential\ninformation necessary for reconstructing the original input from the compressed\nlatent space. In this paper, we propose an alternative learning objective.\nInstead of using the raw input as the reconstruction target, we employ the\nDiscrete Fourier Transform (DFT) of the input. The DFT provides meaningful\nglobal information at each frequency level, making individual frequency\ncomponents useful as separate learning targets. When dealing with\nmultidimensional input data, the DFT offers remarkable flexibility by enabling\nselective transformation across specific dimensions while preserving others in\nthe computation. Moreover, certain types of input exhibit distinct patterns in\ntheir frequency distributions, where specific frequency components consistently\ncontain most of the magnitude, allowing us to focus on a subset of frequencies\nrather than the entire spectrum. These characteristics position the DFT as a\nviable learning objective for representation learning and we validate our\napproach by achieving 52.8% top-1 accuracy on CIFAR-10 with ResNet-50 and\noutperforming the traditional autoencoder by 12.8 points under identical\narchitectural configurations. Additionally, we demonstrate that training on\nonly the lower-frequency components - those with the highest magnitudes yields\nresults comparable to using the full frequency spectrum, with only minimal\nreductions in accuracy.", "AI": {"tldr": "论文提出了一种基于离散傅里叶变换（DFT）的图像表示学习方法，替代传统的输入重建目标，通过聚焦频率分量提升表示学习效果。", "motivation": "传统自编码器以原始输入为重建目标，而DFT能提供全局频率信息，更灵活且高效。", "method": "使用DFT作为重建目标，选择特定频率分量进行训练，验证其在多维数据中的灵活性。", "result": "在CIFAR-10上达到52.8%的top-1准确率，比传统自编码器高12.8个百分点。", "conclusion": "DFT是一种有效的表示学习目标，且仅使用低频分量也能达到接近全频谱的效果。"}}
{"id": "2506.06295", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.06295", "abs": "https://arxiv.org/abs/2506.06295", "authors": ["Zhiyuan Liu", "Yicun Yang", "Yaojie Zhang", "Junjie Chen", "Chang Zou", "Qingyuan Wei", "Shaobo Wang", "Linfeng Zhang"], "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching", "comment": null, "summary": "Autoregressive Models (ARMs) have long dominated the landscape of Large\nLanguage Models. Recently, a new paradigm has emerged in the form of\ndiffusion-based Large Language Models (dLLMs), which generate text by\niteratively denoising masked segments. This approach has shown significant\nadvantages and potential. However, dLLMs suffer from high inference latency.\nTraditional ARM acceleration techniques, such as Key-Value caching, are\nincompatible with dLLMs due to their bidirectional attention mechanism. To\naddress this specific challenge, our work begins with a key observation that\ndLLM inference involves a static prompt and a partially dynamic response, where\nmost tokens remain stable across adjacent denoising steps. Based on this, we\npropose dLLM-Cache, a training-free adaptive caching framework that combines\nlong-interval prompt caching with partial response updates guided by feature\nsimilarity. This design enables efficient reuse of intermediate computations\nwithout compromising model performance. Extensive experiments on representative\ndLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1\nx speedup over standard inference without compromising output quality. Notably,\nour method brings dLLM inference latency close to that of ARMs under many\nsettings. Codes are provided in the supplementary material and will be released\npublicly on GitHub.", "AI": {"tldr": "dLLM-Cache是一种无需训练的缓存框架，通过结合长间隔提示缓存和部分响应更新，显著降低了扩散式大语言模型（dLLMs）的推理延迟，实现了高达9.1倍的加速。", "motivation": "扩散式大语言模型（dLLMs）在生成文本时具有优势，但存在高推理延迟的问题，且传统ARM加速技术不适用。", "method": "基于dLLM推理中静态提示和部分动态响应的特点，提出dLLM-Cache框架，通过长间隔提示缓存和特征相似性指导的部分响应更新，实现高效计算重用。", "result": "在LLaDA 8B和Dream 7B等模型上，dLLM-Cache实现了高达9.1倍的加速，且不损失输出质量。", "conclusion": "dLLM-Cache显著降低了dLLMs的推理延迟，使其接近ARM的延迟水平，为dLLMs的实际应用提供了可行性。"}}
{"id": "2506.06487", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06487", "abs": "https://arxiv.org/abs/2506.06487", "authors": ["Zibo Zhou", "Yue Hu", "Lingkai Zhang", "Zonglin Li", "Siheng Chen"], "title": "BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation", "comment": null, "summary": "Zero-shot object navigation (ZSON) allows robots to find target objects in\nunfamiliar environments using natural language instructions, without relying on\npre-built maps or task-specific training. Recent general-purpose models, such\nas large language models (LLMs) and vision-language models (VLMs), equip agents\nwith semantic reasoning abilities to estimate target object locations in a\nzero-shot manner. However, these models often greedily select the next goal\nwithout maintaining a global understanding of the environment and are\nfundamentally limited in the spatial reasoning necessary for effective\nnavigation. To overcome these limitations, we propose a novel 3D voxel-based\nbelief map that estimates the target's prior presence distribution within a\nvoxelized 3D space. This approach enables agents to integrate semantic priors\nfrom LLMs and visual embeddings with hierarchical spatial structure, alongside\nreal-time observations, to build a comprehensive 3D global posterior belief of\nthe target's location. Building on this 3D voxel map, we introduce\nBeliefMapNav, an efficient navigation system with two key advantages: i)\ngrounding LLM semantic reasoning within the 3D hierarchical semantics voxel\nspace for precise target position estimation, and ii) integrating sequential\npath planning to enable efficient global navigation decisions. Experiments on\nHM3D, MP3D, and HSSD benchmarks show that BeliefMapNav achieves\nstate-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length\n(SPL), with a notable 46.4% SPL improvement over the previous best SR method,\nvalidating its effectiveness and efficiency.", "AI": {"tldr": "论文提出了一种基于3D体素信念地图的零样本物体导航方法BeliefMapNav，结合大语言模型和视觉语言模型的语义推理能力，显著提升了导航效率和成功率。", "motivation": "解决现有零样本物体导航方法在全局环境理解和空间推理上的局限性。", "method": "提出3D体素信念地图，整合语义先验和实时观测，结合层次化空间结构和路径规划。", "result": "在HM3D、MP3D和HSSD基准测试中取得SOTA性能，SPL提升46.4%。", "conclusion": "BeliefMapNav通过全局信念地图和高效路径规划，显著提升了零样本物体导航的性能。"}}
{"id": "2506.06594", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06594", "abs": "https://arxiv.org/abs/2506.06594", "authors": ["Daniel Leite", "Igor Škrjanc", "Fernando Gomide"], "title": "From Model-Based and Adaptive Control to Evolving Fuzzy Control", "comment": "4 pages, 2 figures. Fuzz-IEEE 2025 Booklet: 60 Years of Fuzzy Set\n  Theory", "summary": "Evolving fuzzy systems build and adapt fuzzy models - such as predictors and\ncontrollers - by incrementally updating their rule-base structure from data\nstreams. On the occasion of the 60-year anniversary of fuzzy set theory,\ncommemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the\nhistorical development and core contributions of classical fuzzy and adaptive\nmodeling and control frameworks. It then highlights the emergence and\nsignificance of evolving intelligent systems in fuzzy modeling and control,\nemphasizing their advantages in handling nonstationary environments. Key\nchallenges and future directions are discussed, including safety,\ninterpretability, and principled structural evolution.", "AI": {"tldr": "本文回顾了模糊集理论60年来的发展，重点介绍了经典模糊和自适应建模与控制框架的核心贡献，并探讨了演化智能系统在模糊建模与控制中的优势与未来挑战。", "motivation": "纪念模糊集理论60周年，总结其发展历程及演化智能系统在非平稳环境中的重要性。", "method": "通过回顾历史文献和框架，分析演化模糊系统的核心贡献和优势。", "result": "演化模糊系统在非平稳环境中表现出色，但仍面临安全性、可解释性和结构演化等挑战。", "conclusion": "演化模糊系统是模糊建模与控制的重要发展方向，未来需解决安全性、可解释性和结构演化等问题。"}}
{"id": "2506.07081", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07081", "abs": "https://arxiv.org/abs/2506.07081", "authors": ["Sathvik Udupa", "Shinji Watanabe", "Petr Schwarz", "Jan Cernocky"], "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training", "comment": null, "summary": "Accurate, low-latency endpointing is crucial for effective spoken dialogue\nsystems. While traditional endpointers often rely on spectrum-based audio\nfeatures, this work proposes real-time speech endpointing for multi-turn\ndialogues using streaming, low-bitrate Neural Audio Codec (NAC) features,\nbuilding upon recent advancements in neural audio codecs. To further reduce\ncutoff errors, we introduce a novel label delay training scheme. At a fixed\nmedian latency of 160 ms, our combined NAC and label delay approach achieves\nsignificant relative cutoff error reductions: 42.7% for a single-stream\nendpointer and 37.5% for a two-stream configuration, compared to baseline\nmethods. Finally, we demonstrate efficient integration with a codec-based\npretrained speech large language model, improving its median response time by\n1200 ms and reducing its cutoff error by 35%.", "AI": {"tldr": "该论文提出了一种基于神经音频编解码器（NAC）特征的实时语音端点检测方法，结合标签延迟训练方案，显著减少了多轮对话中的端点错误。", "motivation": "传统端点检测方法依赖频谱特征，难以满足实时多轮对话的低延迟需求。", "method": "使用流式低比特率NAC特征，并引入标签延迟训练方案。", "result": "在160毫秒的中位延迟下，端点错误相对减少42.7%（单流）和37.5%（双流），与预训练语音大模型结合后响应时间减少1200毫秒，错误减少35%。", "conclusion": "NAC特征与标签延迟训练结合，显著提升了端点检测的准确性和实时性。"}}
{"id": "2506.07069", "categories": ["cs.GR", "cs.AR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07069", "abs": "https://arxiv.org/abs/2506.07069", "authors": ["Zhican Wang", "Guanghui He", "Dantong Liu", "Lingjun Gao", "Shell Xu Hu", "Chen Zhang", "Zhuoran Song", "Nicholas Lane", "Wayne Luk", "Hongxiang Fan"], "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization", "comment": "Preprint. Under review", "summary": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field.", "AI": {"tldr": "本文提出了一种架构-算法协同设计方法，通过轴定向光栅化、神经排序和可重构处理阵列，显著提升了3D高斯溅射（3DGS）在资源受限设备上的实时渲染性能。", "motivation": "尽管3DGS在高质量和高效视图合成方面表现出色，但在资源受限设备上的实时渲染仍面临挑战，主要受限于功耗和面积预算。", "method": "1. 轴定向光栅化以减少冗余计算；2. 神经排序替代硬件排序器；3. 可重构处理阵列支持光栅化和神经网络推理；4. π轨迹瓦片调度优化内存访问。", "result": "实验表明，设计在保持渲染质量的同时，速度提升23.4~27.8倍，能耗降低28.8~51.4倍。", "conclusion": "该设计为资源受限设备上的3DGS实时渲染提供了高效解决方案，并计划开源以推动领域发展。"}}
{"id": "2506.06874", "categories": ["cs.HC", "cs.AI", "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods"], "pdf": "https://arxiv.org/pdf/2506.06874", "abs": "https://arxiv.org/abs/2506.06874", "authors": ["Ala Yankouskaya", "Areej B. Babiker", "Syeda W. F. Rizvi", "Sameha Alshakhsi", "Magnus Liebherr", "Raian Ali"], "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models", "comment": null, "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.", "AI": {"tldr": "研究者开发了一个名为LLM-D12的12项问卷，用于测量人们对大型语言模型（LLM）的依赖程度，分为工具性依赖和关系性依赖两个维度。", "motivation": "现有评估LLM依赖的工具较少且基于传统行为成瘾症状，而LLM与人类的关系更为复杂，需要新的视角。", "method": "基于作者的理论工作开发问卷，收集526名英国参与者的数据，通过探索性和验证性因子分析验证量表结构。", "result": "量表支持两因素结构（工具性依赖和关系性依赖），具有良好内部一致性和区分效度。", "conclusion": "LLM依赖不一定代表功能障碍，但在某些情境下可能成为问题，LLM-D12为研究提供了新工具。"}}
{"id": "2506.06563", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06563", "abs": "https://arxiv.org/abs/2506.06563", "authors": ["Thushari Hapuarachchi", "Long Dang", "Kaiqi Xiong"], "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles", "comment": null, "summary": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks.", "AI": {"tldr": "本文研究了针对交通标志识别的深度神经网络（DNNs）的鲁棒性，提出了一种基于数据增强的训练方法以抵御误差最小化攻击，并开发了一个检测模型识别中毒数据。", "motivation": "由于DNNs在交通标志识别中的广泛应用，且训练数据来源未知，确保模型在训练过程中不被攻击或中毒至关重要。", "method": "首先对DNNs进行误差最小化攻击，然后提出一种基于非线性变换的数据增强训练方法以提高模型鲁棒性，并开发了一个检测模型识别中毒数据。", "result": "攻击使DNNs的预测准确率从99.90%降至10.6%，但提出的方法成功恢复至96.05%，检测模型识别攻击的成功率超过99%。", "conclusion": "研究表明，在交通标志识别系统中采用先进的训练方法对抵御数据中毒攻击至关重要。"}}
{"id": "2506.06904", "categories": ["cs.NE", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.06904", "abs": "https://arxiv.org/abs/2506.06904", "authors": ["Yuhan Helena Liu", "Guangyu Robert Yang", "Christopher J. Cueva"], "title": "Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example", "comment": null, "summary": "Understanding how the brain learns may be informed by studying biologically\nplausible learning rules. These rules, often approximating gradient descent\nlearning to respect biological constraints such as locality, must meet two\ncritical criteria to be considered an appropriate brain model: (1) good\nneuroscience task performance and (2) alignment with neural recordings. While\nextensive research has assessed the first criterion, the second remains\nunderexamined. Employing methods such as Procrustes analysis on well-known\nneuroscience datasets, this study demonstrates the existence of a biologically\nplausible learning rule -- namely e-prop, which is based on gradient truncation\nand has demonstrated versatility across a wide range of tasks -- that can\nachieve neural data similarity comparable to Backpropagation Through Time\n(BPTT) when matched for task accuracy. Our findings also reveal that model\narchitecture and initial conditions can play a more significant role in\ndetermining neural similarity than the specific learning rule. Furthermore, we\nobserve that BPTT-trained models and their biologically plausible counterparts\nexhibit similar dynamical properties at comparable accuracies. These results\nunderscore the substantial progress made in developing biologically plausible\nlearning rules, highlighting their potential to achieve both competitive task\nperformance and neural data similarity.", "AI": {"tldr": "研究探讨了生物合理学习规则（如e-prop）在神经任务表现和神经记录对齐方面的潜力，发现其与BPTT在神经相似性上表现相当，且模型架构和初始条件对相似性影响更大。", "motivation": "研究生物合理学习规则是否既能满足神经任务表现，又能与神经记录对齐，填补了后者研究的空白。", "method": "采用Procrustes分析等方法，比较e-prop和BPTT在神经相似性上的表现，并探讨模型架构和初始条件的影响。", "result": "e-prop在神经相似性上与BPTT相当，且模型架构和初始条件对相似性影响更大；两者在动态特性上相似。", "conclusion": "生物合理学习规则（如e-prop）在任务表现和神经相似性上均具潜力，为未来研究提供了方向。"}}
{"id": "2506.06296", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06296", "abs": "https://arxiv.org/abs/2506.06296", "authors": ["Hanaa El Afia", "Said Ohamouddou", "Raddouane Chiheb", "Abdellatif El Afia"], "title": "Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets", "comment": null, "summary": "We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph\nConvolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks\n(KAN) for the classification of three-dimensional point clouds. This method\nreplaces Multi-Layer Perceptron (MLP) layers with adaptable univariate\npolynomial expansions within a streamlined DGCNN architecture, circumventing\ndeep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In\ncomparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi\npolynomials outperform the traditional linear layer-based DGCNN baseline in\nterms of accuracy and convergence speed, while maintaining parameter\nefficiency. Our results demonstrate that higher polynomial degrees do not\nautomatically improve performance, highlighting the need for further\ntheoretical and empirical investigation to fully understand the interactions\nbetween polynomial bases, degrees, and the mechanisms of graph-based learning.", "AI": {"tldr": "Jacobi-KAN-DGCNN结合动态图卷积神经网络与Jacobi多项式KAN，用于三维点云分类，性能优于传统DGCNN。", "motivation": "探索多项式扩展在点云分类中的潜力，替代传统MLP层，提升准确性和收敛速度。", "method": "在DGCNN架构中用Jacobi多项式KAN替换MLP层，避免深层结构，实现逐层比较。", "result": "在ModelNet40数据集上，Jacobi-KAN-DGCNN在准确性和收敛速度上优于传统DGCNN，且参数高效。", "conclusion": "多项式阶数并非越高越好，需进一步研究多项式基、阶数与图学习机制的交互。"}}
{"id": "2506.06535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06535", "abs": "https://arxiv.org/abs/2506.06535", "authors": ["Vineet Bhat", "Naman Patel", "Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping", "comment": null, "summary": "Robotic manipulation of unseen objects via natural language commands remains\nchallenging. Language driven robotic grasping (LDRG) predicts stable grasp\nposes from natural language queries and RGB-D images. Here we introduce\nMask-guided feature pooling, a lightweight enhancement to existing LDRG\nmethods. Our approach employs a two-stage training strategy: first, a\nvision-language model generates feature maps from CLIP-fused embeddings, which\nare upsampled and weighted by text embeddings to produce segmentation masks.\nNext, the decoder generates separate feature maps for grasp prediction, pooling\nonly token features within these masked regions to efficiently predict grasp\nposes. This targeted pooling approach reduces computational complexity,\naccelerating both training and inference. Incorporating mask pooling results in\na 12% improvement over prior approaches on the OCID-VLG benchmark. Furthermore,\nwe introduce RefGraspNet, an open-source dataset eight times larger than\nexisting alternatives, significantly enhancing model generalization for\nopen-vocabulary grasping. By extending 2D grasp predictions to 3D via depth\nmapping and inverse kinematics, our modular method achieves performance\ncomparable to recent Vision-Language-Action (VLA) models on the LIBERO\nsimulation benchmark, with improved generalization across different task\nsuites. Real-world experiments on a 7 DoF Franka robotic arm demonstrate a 57%\nsuccess rate with unseen objects, surpassing competitive baselines by 7%. Code\nwill be released post publication.", "AI": {"tldr": "提出了一种基于掩码引导特征池化的轻量级增强方法，用于语言驱动的机器人抓取任务，显著提升了性能并降低了计算复杂度。", "motivation": "解决自然语言驱动的机器人抓取任务中未见物体的挑战性问题，提升抓取精度和效率。", "method": "采用两阶段训练策略：首先生成分割掩码，随后在掩码区域内进行特征池化以预测抓取姿态。", "result": "在OCID-VLG基准上性能提升12%，并在真实机器人实验中达到57%的成功率。", "conclusion": "该方法在性能和泛化能力上优于现有方法，同时开源了一个大规模数据集RefGraspNet。"}}
{"id": "2506.06620", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06620", "abs": "https://arxiv.org/abs/2506.06620", "authors": ["Marena Trujillo", "Amir Sajadi", "Jonathan Shaw", "Bri-Mathias Hodge"], "title": "Computationally Efficient Analytical Models of Frequency and Voltage in Low-Inertia Systems", "comment": null, "summary": "In this paper, low-order models of the frequency and voltage response of\nmixed-generation, low-inertia systems are presented. These models are unique in\ntheir ability to efficiently and accurately model frequency and voltage\ndynamics without increasing the computational burden as the share of inverters\nis increased in a system. The models are validated against industry-grade\nelectromagnetic transient simulation, compared to which the proposed models are\nseveral orders of magnitude faster. The accuracy and efficiency of the\nlow-inertia frequency and voltage models makes them well suited for a variety\nof planning and operational studies, especially for multi-scenario and\nprobabilistic studies, as well as for screening studies to establish impact\nzones based on the dynamic interactions between inverters and synchronous\ngenerators.", "AI": {"tldr": "本文提出了低阶模型，用于高效准确地模拟低惯量混合发电系统的频率和电压响应，计算速度快且适用于多场景研究。", "motivation": "研究低惯量系统中频率和电压动态的准确建模问题，解决传统方法计算负担随逆变器比例增加而增大的问题。", "method": "开发低阶模型，通过验证与电磁暂态仿真对比，证明其高效性和准确性。", "result": "模型计算速度快几个数量级，适用于规划和运行研究，尤其是多场景和概率研究。", "conclusion": "低阶模型在低惯量系统中具有高效性和准确性，适合动态交互分析和多场景应用。"}}
{"id": "2506.07118", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07118", "abs": "https://arxiv.org/abs/2506.07118", "authors": ["Yu-Xuan Wu", "Ziyan Huang", "Bin Hu", "Zhi-Hong Guan"], "title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis", "comment": "14 pages", "summary": "This article proposes a robust brain-inspired audio feature extractor\n(RBA-FE) model for depression diagnosis, using an improved hierarchical network\narchitecture. Most deep learning models achieve state-of-the-art performance\nfor image-based diagnostic tasks, ignoring the counterpart audio features. In\norder to tailor the noise challenge, RBA-FE leverages six acoustic features\nextracted from the raw audio, capturing both spatial characteristics and\ntemporal dependencies. This hybrid attribute helps alleviate the precision\nlimitation in audio feature extraction within other learning models like deep\nresidual shrinkage networks. To deal with the noise issues, our model\nincorporates an improved spiking neuron model, called adaptive rate smooth\nleaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of\n``retuning of cellular signal selectivity\" in the brain attention systems,\nwhich enhances the model robustness against environmental noises in audio data.\nExperimental results demonstrate that RBA-FE achieves state-of-the-art accuracy\non the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in\nprecision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014\nand DAIC-WOZ datasets both show enhancements in noise robustness. It is further\nindicated by comparison that the ARSLIF neuron model suggest the abnormal\nfiring pattern within the feature extraction on depressive audio data, offering\nbrain-inspired interpretability.", "AI": {"tldr": "本文提出了一种基于脑启发的鲁棒音频特征提取器（RBA-FE）模型，用于抑郁症诊断，通过改进的分层网络架构解决噪声问题。", "motivation": "现有深度学习模型在图像诊断任务中表现优异，但忽略了音频特征，且音频数据易受噪声干扰。", "method": "RBA-FE提取六种声学特征，结合改进的脉冲神经元模型（ARSLIF），模拟大脑注意力系统的信号选择性重调机制。", "result": "在MODMA数据集上，RBA-FE在精确度、准确率、召回率和F1分数上均达到0.87以上，并在噪声鲁棒性上表现优异。", "conclusion": "RBA-FE不仅提升了抑郁症诊断的准确性，还通过ARSLIF模型提供了脑启发的可解释性。"}}
{"id": "2506.07209", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07209", "abs": "https://arxiv.org/abs/2506.07209", "authors": ["Lei Li", "Angela Dai"], "title": "HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance", "comment": "Project page: https://hoipage.github.io/ Video:\n  https://youtu.be/b1pJU9lKQTE", "summary": "We present HOI-PAGE, a new approach to synthesizing 4D human-object\ninteractions (HOIs) from text prompts in a zero-shot fashion, driven by\npart-level affordance reasoning. In contrast to prior works that focus on\nglobal, whole body-object motion for 4D HOI synthesis, we observe that\ngenerating realistic and diverse HOIs requires a finer-grained understanding --\nat the level of how human body parts engage with object parts. We thus\nintroduce Part Affordance Graphs (PAGs), a structured HOI representation\ndistilled from large language models (LLMs) that encodes fine-grained part\ninformation along with contact relations. We then use these PAGs to guide a\nthree-stage synthesis: first, decomposing input 3D objects into geometric\nparts; then, generating reference HOI videos from text prompts, from which we\nextract part-based motion constraints; finally, optimizing for 4D HOI motion\nsequences that not only mimic the reference dynamics but also satisfy\npart-level contact constraints. Extensive experiments show that our approach is\nflexible and capable of generating complex multi-object or multi-person\ninteraction sequences, with significantly improved realism and text alignment\nfor zero-shot 4D HOI generation.", "AI": {"tldr": "HOI-PAGE是一种从文本提示中零样本合成4D人-物交互（HOI）的新方法，通过部分级功能推理实现。", "motivation": "现有方法主要关注全局的全身-物体运动，而生成真实多样的HOI需要更细粒度的理解，即人体部分如何与物体部分交互。", "method": "引入部分功能图（PAGs），从大语言模型中提取结构化HOI表示，指导三阶段合成：分解3D物体、生成参考视频并提取运动约束、优化4D HOI序列。", "result": "实验表明，该方法灵活，能生成复杂多对象或多人的交互序列，显著提升零样本4D HOI生成的逼真度和文本对齐性。", "conclusion": "HOI-PAGE通过细粒度部分推理，实现了更真实和多样化的4D HOI合成。"}}
{"id": "2506.07041", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07041", "abs": "https://arxiv.org/abs/2506.07041", "authors": ["Leijie Wang", "Weizi Wu", "Lirong Que", "Nirvan Tyagi", "Amy X. Zhang"], "title": "From Inquisitorial to Adversarial: Using Legal Theory to Redesign Online Reporting Systems", "comment": "Under review", "summary": "User reporting systems are central to addressing interpersonal conflicts and\nprotecting users from harm in online spaces, particularly those with heightened\nprivacy expectations. However, users often express frustration at their lack of\ninsight and input into the reporting process. Drawing on offline legal\nliterature, we trace these frustrations to the inquisitorial nature of today's\nonline reporting systems, where moderators lead evidence gathering and case\ndevelopment. In contrast, adversarial models can grant users greater control\nand thus are better for procedural justice and privacy protection, despite\ntheir increased risks of system abuse. This motivates us to explore the\npotential of incorporating adversarial practices into online reporting systems.\nThrough literature review, formative interviews, and threat modeling, we find a\nrich design space for empowering users to collect and present their evidence\nwhile mitigating potential abuse in the reporting process. In particular, we\npropose designs that minimize the amount of information shared for reporting\npurposes, as well as supporting evidence authentication. Finally, we discuss\nhow our findings can inform new cryptographic tools and new efforts to apply\ncomparative legal frameworks to online moderation.", "AI": {"tldr": "论文探讨了在线举报系统的改进，提出采用对抗性模型以增强用户控制和隐私保护，同时减少系统滥用的风险。", "motivation": "用户对当前在线举报系统缺乏透明度和参与感感到不满，这源于系统的审问式特性。对抗性模型能更好地实现程序正义和隐私保护。", "method": "通过文献综述、形成性访谈和威胁建模，探索了对抗性实践在在线举报系统中的潜力。", "result": "提出了支持用户收集和呈现证据的设计，同时减少信息共享和验证证据真实性。", "conclusion": "研究发现可为新型加密工具和在线内容审核的法律框架提供参考。"}}
{"id": "2506.06569", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06569", "abs": "https://arxiv.org/abs/2506.06569", "authors": ["Yannis Spyridis", "Vasileios Argyriou"], "title": "Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models", "comment": null, "summary": "Automated sorting is crucial for improving the efficiency and scalability of\ntextile recycling, but accurately identifying material composition and\ndetecting contaminants from sensor data remains challenging. This paper\ninvestigates the use of standard RGB imagery, a cost-effective sensing\nmodality, for key pre-processing tasks in an automated system. We present\ncomputer vision components designed for a conveyor belt setup to perform (a)\nclassification of four common textile types and (b) segmentation of non-textile\nfeatures such as buttons and zippers. For classification, several pre-trained\narchitectures were evaluated using transfer learning and cross-validation, with\nEfficientNetB0 achieving the best performance on a held-out test set with\n81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the\nGrounding DINO open-vocabulary detector with the Segment Anything Model (SAM)\nwas employed, demonstrating excellent performance with a mIoU of 0.90 for the\ngenerated masks against ground truth. This study demonstrates the feasibility\nof using RGB images coupled with modern deep learning techniques, including\ntransfer learning for classification and foundation models for zero-shot\nsegmentation, to enable essential analysis steps for automated textile\nrecycling pipelines.", "AI": {"tldr": "论文探讨了利用RGB图像和深度学习技术（如迁移学习和基础模型）在自动化纺织品回收中实现分类和分割任务的可行性。", "motivation": "提高纺织品回收的效率和可扩展性，解决从传感器数据中准确识别材料成分和检测污染物的挑战。", "method": "使用预训练架构（如EfficientNetB0）进行纺织品分类，结合Grounding DINO和Segment Anything Model（SAM）进行零样本特征分割。", "result": "分类任务达到81.25%准确率，分割任务mIoU为0.90。", "conclusion": "RGB图像结合现代深度学习技术可用于自动化纺织品回收的关键分析步骤。"}}
{"id": "2506.06979", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.06979", "abs": "https://arxiv.org/abs/2506.06979", "authors": ["MaolinYang", "Yaohui Wang", "Pingyu Jiang"], "title": "Research on Aerodynamic Performance Prediction of Airfoils Based on a Fusion Algorithm of Transformer and GAN", "comment": "33 pages,10 figures", "summary": "Predicting of airfoil aerodynamic performance is a key part of aircraft\ndesign optimization, but the traditional methods (such as wind tunnel test and\nCFD simulation) have the problems of high cost and low efficiency, and the\nexisting data-driven models face the challenges of insufficient accuracy and\nstrong data dependence in multi-objective prediction. Therefore, this study\nproposes a deep learning model, Deeptrans, based on the fusion of improved\nTransformer and generative Adversarial network (GAN), which aims to predict the\nmulti-parameter aerodynamic performance of airfoil efficiently. By constructing\na large-scale data set and designing a model structure that integrates a\nTransformer coding-decoding framework and confrontation training, synchronous\nand high-precision prediction of aerodynamic parameters is realized.\nExperiments show that the MSE loss of Deeptrans on the verification set is\nreduced to 5.6*10-6, and the single-sample prediction time is only 0.0056\nseconds, which is nearly 700 times more efficient than the traditional CFD\nmethod. Horizontal comparison shows that the prediction accuracy is\nsignificantly better than the original Transformer, GAN, and VAE models. This\nstudy provides an efficient data-driven solution for airfoil aerodynamic\nperformance prediction and a new idea for deep learning modeling complex flow\nproblems.", "AI": {"tldr": "提出了一种基于改进Transformer和GAN融合的深度学习模型Deeptrans，用于高效预测翼型多参数气动性能，解决了传统方法成本高、效率低以及数据驱动模型精度不足的问题。", "motivation": "传统翼型气动性能预测方法（如风洞试验和CFD模拟）成本高、效率低，而现有数据驱动模型在多目标预测中面临精度不足和数据依赖性强的问题。", "method": "通过构建大规模数据集，结合Transformer编码-解码框架和对抗训练，设计了Deeptrans模型，实现了多参数气动性能的高精度同步预测。", "result": "实验表明，Deeptrans在验证集上的MSE损失降至5.6*10-6，单样本预测时间仅0.0056秒，效率比传统CFD方法提高近700倍，且预测精度显著优于原始Transformer、GAN和VAE模型。", "conclusion": "该研究为翼型气动性能预测提供了高效的数据驱动解决方案，并为深度学习建模复杂流动问题提供了新思路。"}}
{"id": "2506.06297", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06297", "abs": "https://arxiv.org/abs/2506.06297", "authors": ["Bozhi Sun", "Seda Tierney", "Jeffrey A. Feinstein", "Frederick Damen", "Alison L. Marsden", "Daniele E. Schiavazzi"], "title": "Optimal patient allocation for echocardiographic assessments", "comment": null, "summary": "Scheduling echocardiographic exams in a hospital presents significant\nchallenges due to non-deterministic factors (e.g., patient no-shows, patient\narrival times, diverse exam durations, etc.) and asymmetric resource\nconstraints between fetal and non-fetal patient streams. To address these\nchallenges, we first conducted extensive pre-processing on one week of\noperational data from the Echo Laboratory at Stanford University's Lucile\nPackard Children's Hospital, to estimate patient no-show probabilities and\nderive empirical distributions of arrival times and exam durations. Based on\nthese inputs, we developed a discrete-event stochastic simulation model using\nSimPy, and integrate it with the open source Gymnasium Python library. As a\nbaseline for policy optimization, we developed a comparative framework to\nevaluate on-the-fly versus reservation-based allocation strategies, in which\ndifferent proportions of resources are reserved in advance. Considering a\nhospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2\nratio of fetal to non-fetal sonographers, we show that on-the-fly allocation\ngenerally yields better performance, more effectively adapting to patient\nvariability and resource constraints. Building on this foundation, we apply\nreinforcement learning (RL) to derive an approximated optimal dynamic\nallocation policy. This RL-based policy is benchmarked against the\nbest-performing rule-based strategies, allowing us to quantify their\ndifferences and provide actionable insights for improving echo lab efficiency\nthrough intelligent, data-driven resource management.", "AI": {"tldr": "论文提出了一种基于离散事件随机模拟和强化学习的动态资源分配策略，用于优化医院超声心动图检查的调度问题。", "motivation": "医院超声心动图检查调度面临非确定性因素（如患者缺席、到达时间、检查时长等）和资源不对称约束的挑战。", "method": "通过预处理数据估计患者缺席概率和检查时长分布，建立SimPy离散事件模拟模型，结合Gymnasium库，比较动态分配与预留策略，并应用强化学习优化策略。", "result": "动态分配策略优于预留策略，能更好适应患者变异性；强化学习策略进一步提升了资源管理效率。", "conclusion": "数据驱动的动态资源分配策略可显著提升超声心动图检查的调度效率。"}}
{"id": "2506.06560", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06560", "abs": "https://arxiv.org/abs/2506.06560", "authors": ["Mihir Dharmadhikari", "Kostas Alexis"], "title": "Semantics-aware Predictive Inspection Path Planning", "comment": "Accepted at IEEE Transactions on Field Robotics", "summary": "This paper presents a novel semantics-aware inspection path planning paradigm\ncalled \"Semantics-aware Predictive Planning\" (SPP). Industrial environments\nthat require the inspection of specific objects or structures (called\n\"semantics\"), such as ballast water tanks inside ships, often present\nstructured and repetitive spatial arrangements of the semantics of interest.\nMotivated by this, we first contribute an algorithm that identifies spatially\nrepeating patterns of semantics - exact or inexact - in a semantic scene graph\nrepresentation and makes predictions about the evolution of the graph in the\nunseen parts of the environment using these patterns. Furthermore, two\ninspection path planning strategies, tailored to ballast water tank inspection,\nthat exploit these predictions are proposed. To assess the performance of the\nnovel predictive planning paradigm, both simulation and experimental\nevaluations are performed. First, we conduct a simulation study comparing the\nmethod against relevant state-of-the-art techniques and further present tests\nshowing its ability to handle imperfect patterns. Second, we deploy our method\nonboard a collision-tolerant aerial robot operating inside the ballast tanks of\ntwo real ships. The results, both in simulation and field experiments,\ndemonstrate significant improvement over the state-of-the-art in terms of\ninspection time while maintaining equal or better semantic surface coverage. A\nset of videos describing the different parts of the method and the field\ndeployments is available at https://tinyurl.com/spp-videos. The code for this\nwork is made available at https://github.com/ntnu-arl/predictive_planning_ros.", "AI": {"tldr": "提出了一种名为“语义感知预测规划”（SPP）的新方法，用于工业环境中特定对象的检查路径规划，通过识别语义场景图中的重复模式并预测未探索区域，显著提升了检查效率。", "motivation": "工业环境（如船舶压载水舱）中需要检查特定对象或结构（语义），这些环境通常具有重复的空间布局，因此需要一种能够利用这些重复模式的方法来优化检查路径规划。", "method": "提出了一种算法，用于识别语义场景图中的重复模式（精确或不精确），并基于这些模式预测未探索区域的语义布局。随后设计了两种针对压载水舱检查的路径规划策略。", "result": "通过仿真和实验验证，SPP方法在检查时间和语义表面覆盖率上均显著优于现有技术，且能够处理不完美的重复模式。", "conclusion": "SPP方法通过利用语义重复模式，显著提升了工业环境中的检查效率，具有实际应用价值。"}}
{"id": "2506.06679", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06679", "abs": "https://arxiv.org/abs/2506.06679", "authors": ["Taoran Wu", "Yiling Xue", "Dejin Ren", "Arvind Easwaran", "Martin Fränzle", "Bai Xue"], "title": "Controlled Reach-avoid Set Computation for Discrete-time Polynomial Systems via Convex Optimization", "comment": null, "summary": "This paper addresses the computation of controlled reach-avoid sets (CRASs)\nfor discrete-time polynomial systems subject to control inputs. A CRAS is a set\nencompassing initial states from which there exist control inputs driving the\nsystem into a target set while avoiding unsafe sets. However, efficiently\ncomputing CRASs remains an open problem, especially for discrete-time systems.\nIn this paper, we propose a novel framework for computing CRASs which takes\nadvantage of a probabilistic perspective. This framework transforms the\nfundamentally nonlinear problem of computing CRASs into a computationally\ntractable convex optimization problem. By regarding control inputs as\ndisturbances obeying certain probability distributions, a CRAS can be\nequivalently treated as a 0-reach-avoid set in the probabilistic sense, which\nconsists of initial states from which the probability of eventually entering\nthe target set while remaining within the safe set is greater than zero. Thus,\nwe can employ the convex optimization method of computing 0-reach-avoid sets to\nestimate CRASs. Furthermore, inspired by the $\\epsilon$-greedy strategy widely\nused in reinforcement learning, we propose an approach that iteratively updates\nthe aforementioned probability distributions imposed on control inputs to\ncompute larger CRASs. We demonstrate the effectiveness of the proposed method\non extensive examples.", "AI": {"tldr": "提出了一种计算离散时间多项式系统控制可达避免集（CRAS）的新框架，通过概率视角将非线性问题转化为凸优化问题，并结合强化学习中的ε-greedy策略迭代更新控制输入的概率分布。", "motivation": "高效计算离散时间系统的CRAS仍是一个未解决的问题，现有方法难以处理非线性特性。", "method": "将CRAS转化为概率意义上的0-可达避免集，利用凸优化方法计算，并通过ε-greedy策略迭代优化控制输入的概率分布。", "result": "在多个示例中验证了该方法的有效性，能够高效计算更大的CRAS。", "conclusion": "提出的概率框架和迭代优化方法为计算CRAS提供了一种高效且可扩展的解决方案。"}}
{"id": "2506.07149", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07149", "abs": "https://arxiv.org/abs/2506.07149", "authors": ["Mengze Hong", "Di Jiang"], "title": "Technical Report: A Practical Guide to Kaldi ASR Optimization", "comment": null, "summary": "This technical report introduces innovative optimizations for Kaldi-based\nAutomatic Speech Recognition (ASR) systems, focusing on acoustic model\nenhancement, hyperparameter tuning, and language model efficiency. We developed\na custom Conformer block integrated with a multistream TDNN-F structure,\nenabling superior feature extraction and temporal modeling. Our approach\nincludes advanced data augmentation techniques and dynamic hyperparameter\noptimization to boost performance and reduce overfitting. Additionally, we\npropose robust strategies for language model management, employing Bayesian\noptimization and $n$-gram pruning to ensure relevance and computational\nefficiency. These systematic improvements significantly elevate ASR accuracy\nand robustness, outperforming existing methods and offering a scalable solution\nfor diverse speech recognition scenarios. This report underscores the\nimportance of strategic optimizations in maintaining Kaldi's adaptability and\ncompetitiveness in rapidly evolving technological landscapes.", "AI": {"tldr": "本文介绍了针对Kaldi语音识别系统的创新优化方法，包括声学模型增强、超参数调优和语言模型效率提升，显著提高了识别准确性和鲁棒性。", "motivation": "提升Kaldi语音识别系统的性能，适应快速发展的技术需求，保持其竞争力。", "method": "结合Conformer模块与多流TDNN-F结构，采用数据增强和动态超参数优化，以及贝叶斯优化和n-gram剪枝的语言模型管理策略。", "result": "系统优化显著提升了ASR的准确性和鲁棒性，优于现有方法。", "conclusion": "战略优化对维持Kaldi的适应性和竞争力至关重要。"}}
{"id": "2506.07558", "categories": ["cs.GR", "math.DG", "math.DS", "math.GT", "68U05, 51M20", "I.3.7; I.3.5"], "pdf": "https://arxiv.org/pdf/2506.07558", "abs": "https://arxiv.org/abs/2506.07558", "authors": ["Fabian Lander", "Diaaeldin Taha"], "title": "Immersive Visualization of Flat Surfaces Using Ray Marching", "comment": "Presented at Bridges Math and Art Conference, Eindhoven 2025. Online\n  demo and code available at\n  https://fabianlander.github.io/apps/raymarchingflatsurfacesapp/ and\n  https://github.com/FabianLander/RayMarchingFlatSurfaces", "summary": "We present an effective method for visualizing flat surfaces using ray\nmarching. Our approach provides an intuitive way to explore translation\nsurfaces, mirror rooms, unfolded polyhedra, and translation prisms while\nmaintaining computational efficiency. We demonstrate the utility of the method\nthrough various examples and provide implementation insights for programmers.\nFinally, we discuss the use of our visualizations in outreach. We make our\nsimulations and code available online.", "AI": {"tldr": "提出了一种基于光线步进的平面可视化方法，适用于多种几何结构，并保持计算效率。", "motivation": "探索平移曲面、镜面房间、展开多面体和平移棱柱等几何结构的直观可视化方法。", "method": "使用光线步进技术实现高效可视化，并提供编程实现细节。", "result": "通过多个示例验证了方法的实用性，并公开了模拟和代码。", "conclusion": "该方法不仅适用于研究，还可用于科普推广。"}}
{"id": "2506.07193", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07193", "abs": "https://arxiv.org/abs/2506.07193", "authors": ["Tobias King", "Michael Knierim", "Philipp Lepold", "Christopher Clarke", "Hans Gellersen", "Michael Beigl", "Tobias Röddiger"], "title": "earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor", "comment": "12 pages", "summary": "Eye tracking technology is frequently utilized to diagnose eye and\nneurological disorders, assess sleep and fatigue, study human visual\nperception, and enable novel gaze-based interaction methods. However,\ntraditional eye tracking methodologies are constrained by bespoke hardware that\nis often cumbersome to wear, complex to apply, and demands substantial\ncomputational resources. To overcome these limitations, we investigated\nElectrooculography (EOG) eye tracking using 14 electrodes positioned around the\nears, integrated into a custom-built headphone form factor device. In a\ncontrolled experiment, 16 participants tracked stimuli designed to induce\nsmooth pursuits and saccades. Data analysis identified optimal electrode pairs\nfor vertical and horizontal eye movement tracking, benchmarked against\ngold-standard EOG and camera-based methods. The electrode montage nearest the\neyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG\nshowed high correlation with gold-standard measures ($r_{\\mathrm{EOG}} = 0.81,\np = 0.01$; $r_{\\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were\nweakly correlated ($r_{\\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\\mathrm{CAM}} =\n0.35, p = 0.05$). Voltage deflections when performing saccades showed strong\ncorrelation in the horizontal direction ($r_{\\mathrm{left}} = 0.99, p = 0.0$;\n$r_{\\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical\ndirection ($r_{\\mathrm{up}} = 0.6, p = 0.23$; $r_{\\mathrm{down}} = 0.19, p =\n0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating\nits potential effectiveness, while vertical earEOG results were poor,\nsuggesting limited feasibility in our current setup.", "AI": {"tldr": "研究利用耳机形式的EOG眼动追踪技术，解决了传统方法的硬件限制，水平追踪效果显著，但垂直追踪效果较差。", "motivation": "传统眼动追踪技术硬件笨重且复杂，研究旨在开发更轻便、高效的替代方案。", "method": "使用14个电极集成到耳机设备中，通过实验测试水平和垂直眼动追踪效果。", "result": "水平追踪与金标准高度相关（r=0.81），垂直追踪相关性较弱（r=0.28）。", "conclusion": "耳机EOG在水平追踪上表现良好，垂直追踪需进一步改进。"}}
{"id": "2506.06578", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06578", "abs": "https://arxiv.org/abs/2506.06578", "authors": ["Anees Nashath Shaik", "Barbara Villarini", "Vasileios Argyriou"], "title": "A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance", "comment": null, "summary": "Surveillance systems play a critical role in security and reconnaissance, but\ntheir performance is often compromised by low-quality images and videos,\nleading to reduced accuracy in face recognition. Additionally, existing\nAI-based facial analysis models suffer from biases related to skin tone\nvariations and partially occluded faces, further limiting their effectiveness\nin diverse real-world scenarios. These challenges are the results of data\nlimitations and imbalances, where available training datasets lack sufficient\ndiversity, resulting in unfair and unreliable facial recognition performance.\nTo address these issues, we propose a data-driven platform that enhances\nsurveillance capabilities by generating synthetic training data tailored to\ncompensate for dataset biases. Our approach leverages deep learning-based\nfacial attribute manipulation and reconstruction using autoencoders and\nGenerative Adversarial Networks (GANs) to create diverse and high-quality\nfacial datasets. Additionally, our system integrates an image enhancement\nmodule, improving the clarity of low-resolution or occluded faces in\nsurveillance footage. We evaluate our approach using the CelebA dataset,\ndemonstrating that the proposed platform enhances both training data diversity\nand model fairness. This work contributes to reducing bias in AI-based facial\nanalysis and improving surveillance accuracy in challenging environments,\nleading to fairer and more reliable security applications.", "AI": {"tldr": "论文提出了一种数据驱动平台，通过生成合成训练数据来解决人脸识别中的偏见问题，提升监控系统的准确性和公平性。", "motivation": "现有监控系统因图像质量低和人脸识别模型存在肤色和遮挡偏见，导致识别效果不佳，主要原因是训练数据缺乏多样性。", "method": "利用深度学习的自动编码器和生成对抗网络（GANs）生成多样化高质量合成数据，并集成图像增强模块提升低分辨率或遮挡人脸的清晰度。", "result": "在CelebA数据集上的实验表明，该平台显著提升了训练数据的多样性和模型公平性。", "conclusion": "该方法减少了AI人脸分析的偏见，提高了监控系统在复杂环境中的准确性和可靠性。"}}
{"id": "2506.07293", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.07293", "abs": "https://arxiv.org/abs/2506.07293", "authors": ["Seabin Lee", "Joonyeol Sim", "Changjoo Nam"], "title": "Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution", "comment": "15 pages", "summary": "We consider the Multi-Robot Task Allocation (MRTA) problem that aims to\noptimize an assignment of multiple robots to multiple tasks in challenging\nenvironments which are with densely populated obstacles and narrow passages. In\nsuch environments, conventional methods optimizing the sum-of-cost are often\nineffective because the conflicts between robots incur additional costs (e.g.,\ncollision avoidance, waiting). Also, an allocation that does not incorporate\nthe actual robot paths could cause deadlocks, which significantly degrade the\ncollective performance of the robots.\n  We propose a scalable MRTA method that considers the paths of the robots to\navoid collisions and deadlocks which result in a fast completion of all tasks\n(i.e., minimizing the \\textit{makespan}). To incorporate robot paths into task\nallocation, the proposed method constructs a roadmap using a Generalized\nVoronoi Diagram. The method partitions the roadmap into several components to\nknow how to redistribute robots to achieve all tasks with less conflicts\nbetween the robots. In the redistribution process, robots are transferred to\ntheir final destinations according to a push-pop mechanism with the first-in\nfirst-out principle. From the extensive experiments, we show that our method\ncan handle instances with hundreds of robots in dense clutter while competitors\nare unable to compute a solution within a time limit.", "AI": {"tldr": "提出了一种考虑机器人路径的多机器人任务分配方法，通过构建基于广义Voronoi图的路线图，避免冲突和死锁，显著提升任务完成效率。", "motivation": "在密集障碍和狭窄通道的环境中，传统方法因机器人冲突和路径未优化导致效率低下，甚至死锁。", "method": "使用广义Voronoi图构建路线图，分区并采用先进先出的推送机制重新分配机器人路径。", "result": "实验表明，该方法能高效处理数百机器人的密集环境，优于现有方法。", "conclusion": "该方法通过路径优化和冲突避免，显著提升了多机器人任务分配的效率和可扩展性。"}}
{"id": "2506.07253", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.07253", "abs": "https://arxiv.org/abs/2506.07253", "authors": ["Peter DelMastro", "Arjun Karuvally", "Hananel Hazan", "Hava Siegelmann", "Edward Rietman"], "title": "Transient Dynamics in Lattices of Differentiating Ring Oscillators", "comment": "15 pages, 10 figures", "summary": "Recurrent neural networks (RNNs) are machine learning models widely used for\nlearning temporal relationships. Current state-of-the-art RNNs use integrating\nor spiking neurons -- two classes of computing units whose outputs depend\ndirectly on their internal states -- and accordingly there is a wealth of\nliterature characterizing the behavior of large networks built from these\nneurons. On the other hand, past research on differentiating neurons, whose\noutputs are computed from the derivatives of their internal states, remains\nlimited to small hand-designed networks with fewer than one-hundred neurons.\nHere we show via numerical simulation that large lattices of differentiating\nneuron rings exhibit local neural synchronization behavior found in the\nKuramoto model of interacting oscillators. We begin by characterizing the\nperiodic orbits of uncoupled rings, herein called ring oscillators. We then\nshow the emergence of local correlations between oscillators that grow over\ntime when these rings are coupled together into lattices. As the correlation\nlength grows, transient dynamics arise in which large regions of the lattice\nsettle to the same periodic orbit, and thin domain boundaries separate\nadjacent, out-of-phase regions. The steady-state scale of these correlated\nregions depends on how the neurons are shared between adjacent rings, which\nsuggests that lattices of differentiating ring oscillator might be tuned to be\nused as reservoir computers. Coupled with their simple circuit design and\npotential for low-power consumption, differentiating neural nets therefore\nrepresent a promising substrate for neuromorphic computing that will enable\nlow-power AI applications.", "AI": {"tldr": "论文探讨了基于微分神经元的RNN在大规模网络中表现出的局部神经同步行为，类似于Kuramoto模型中的振荡器相互作用，并展示了其在低功耗AI应用中的潜力。", "motivation": "当前RNN研究主要集中于积分或脉冲神经元，而微分神经元的研究局限于小型网络。本文旨在探索大规模微分神经元网络的行为及其在神经形态计算中的应用。", "method": "通过数值模拟，研究未耦合环振荡器的周期性轨道，以及耦合后环振荡器之间的局部相关性及其动态行为。", "result": "耦合环振荡器表现出局部同步行为，形成大区域同步和薄域边界，相关区域的稳态尺度取决于神经元共享方式。", "conclusion": "微分神经网络因其简单电路设计和低功耗潜力，有望成为神经形态计算的理想平台，支持低功耗AI应用。"}}
{"id": "2506.06298", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06298", "abs": "https://arxiv.org/abs/2506.06298", "authors": ["Daniel Halpern", "Evi Micha", "Ariel D. Procaccia", "Itai Shapira"], "title": "Pairwise Calibrated Rewards for Pluralistic Alignment", "comment": null, "summary": "Current alignment pipelines presume a single, universal notion of desirable\nbehavior. However, human preferences often diverge across users, contexts, and\ncultures. As a result, disagreement collapses into the majority signal and\nminority perspectives are discounted. To address this, we propose reflecting\ndiverse human preferences through a distribution over multiple reward\nfunctions, each inducing a distinct aligned policy. The distribution is learned\ndirectly from pairwise preference without annotator identifiers or predefined\ngroups. Instead, annotator disagreements are treated as informative soft\nlabels. Our central criterion is pairwise calibration: for every pair of\ncandidate responses, the proportion of reward functions preferring one response\nmatches the fraction of annotators with that preference. We prove that even a\nsmall outlier-free ensemble can accurately represent diverse preference\ndistributions. Empirically, we introduce and validate a practical training\nheuristic to learn such ensembles, and demonstrate its effectiveness through\nimproved calibration, implying a more faithful representation of pluralistic\nvalues.", "AI": {"tldr": "论文提出了一种通过多奖励函数分布反映多样化人类偏好的方法，解决了当前对齐流程中忽视少数观点的问题。", "motivation": "人类偏好因用户、背景和文化而异，但现有对齐方法假设单一理想行为，导致少数观点被忽视。", "method": "提出学习多奖励函数分布，直接从成对偏好中学习，无需标注者标识或预定义组。", "result": "证明小规模无异常值集合能准确表示多样化偏好分布，实验验证了方法的有效性。", "conclusion": "该方法能更忠实反映多元化价值观，通过改进校准实现更公平的对齐。"}}
{"id": "2506.06562", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06562", "abs": "https://arxiv.org/abs/2506.06562", "authors": ["Chad R Samuelson", "Timothy W McLain", "Joshua G Mangelson"], "title": "Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "High-level autonomous operations depend on a robot's ability to construct a\nsufficiently expressive model of its environment. Traditional three-dimensional\n(3D) scene representations, such as point clouds and occupancy grids, provide\ndetailed geometric information but lack the structured, semantic organization\nneeded for high-level reasoning. 3D scene graphs (3DSGs) address this\nlimitation by integrating geometric, topological, and semantic relationships\ninto a multi-level graph-based representation. By capturing hierarchical\nabstractions of objects and spatial layouts, 3DSGs enable robots to reason\nabout environments in a structured manner, improving context-aware\ndecision-making and adaptive planning. Although most recent work has focused on\nindoor 3DSGs, this paper investigates their construction and utility in outdoor\nenvironments. We present a method for generating a task-agnostic\nmetric-semantic point cloud for large outdoor settings and propose\nmodifications to existing indoor 3DSG generation techniques for outdoor\napplicability. Our preliminary qualitative results demonstrate the feasibility\nof outdoor 3DSGs and highlight their potential for future deployment in\nreal-world field robotic applications.", "AI": {"tldr": "论文探讨了3D场景图（3DSGs）在户外环境中的应用，提出了一种生成任务无关的度量语义点云的方法，并改进了现有室内3DSG生成技术以适应户外场景。", "motivation": "传统3D场景表示（如点云和占用网格）缺乏语义组织，限制了机器人高级推理能力。3DSGs通过整合几何、拓扑和语义关系，提供了结构化表示，但现有研究主要集中于室内环境。", "method": "提出了一种生成户外任务无关度量语义点云的方法，并改进了现有室内3DSG生成技术以适应户外场景。", "result": "初步定性结果表明户外3DSGs的可行性，并展示了其在真实世界机器人应用中的潜力。", "conclusion": "户外3DSGs有望提升机器人在复杂环境中的上下文感知决策和自适应规划能力。"}}
{"id": "2506.06687", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06687", "abs": "https://arxiv.org/abs/2506.06687", "authors": ["Ryan Piansky", "Rahul K. Gupta", "Daniel K. Molzahn"], "title": "Optimizing Battery and Line Undergrounding Investments for Transmission Systems under Wildfire Risk Scenarios: A Benders Decomposition Approach", "comment": null, "summary": "With electric power infrastructure posing an increasing risk of igniting\nwildfires under continuing climate change, utilities are frequently\nde-energizing power lines to mitigate wildfire ignition risk, which can cause\nload shedding. Recent research advocates for installing battery energy storage\nsystems as well as undergrounding risky overhead lines to reduce the load\nshedding during such de-energizations. Since wildfire ignition risk can exhibit\nsubstantial geographic and temporal variations, it is important to plan battery\ninstallation and line undergrounding investments while considering multiple\npossible scenarios. This paper presents a scenario-based framework for\noptimizing battery installation and line undergrounding investments while\nconsidering many scenarios, each consisting of a day-long time series of\nuncertain parameters for the load demand, renewable generation, and wildfire\nignition risks. This problem is difficult to solve due to a large number of\nscenarios and binary variables associated with the battery placements as well\nas the lines to be undergrounded. To address the computational challenges, we\ndecompose the problem in a two-stage scheme via a Benders decomposition\napproach. The first stage is a master problem formulated as a mixed integer\nlinear programming (MILP) model that makes decisions on the locations and sizes\nof batteries as well as the lines to be undergrounded. The second stage\nconsists of a linear programming model that assesses these battery and line\nundergrounding decisions as modeled by a DC OPF formulation. We demonstrate the\neffectiveness of the proposed scheme on a large-scale transmission network with\nreal world data on wildfire ignition risks, load, and renewable generation.", "AI": {"tldr": "本文提出了一种基于场景的优化框架，用于规划电池安装和线路地下化投资，以减少因电力线路断电导致的负荷削减。通过两阶段分解方法（Benders分解）解决计算难题。", "motivation": "气候变化下电力基础设施引发野火的风险增加，断电措施导致负荷削减，需优化电池和线路地下化投资以减少影响。", "method": "采用两阶段优化：主问题为混合整数线性规划（MILP）决定电池和线路地下化；子问题为线性规划（DC OPF）评估决策。", "result": "在大规模输电网络上验证了方法的有效性，考虑了野火风险、负荷和可再生能源的时空变化。", "conclusion": "提出的框架能有效优化投资决策，减少负荷削减，适用于多场景不确定性下的规划。"}}
{"id": "2506.07199", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.07199", "abs": "https://arxiv.org/abs/2506.07199", "authors": ["Ben Hayes", "Charalampos Saitis", "György Fazekas"], "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching", "comment": "Accepted at ISMIR 2025", "summary": "Many audio synthesizers can produce the same signal given different parameter\nconfigurations, meaning the inversion from sound to parameters is an inherently\nill-posed problem. We show that this is largely due to intrinsic symmetries of\nthe synthesizer, and focus in particular on permutation invariance. First, we\ndemonstrate on a synthetic task that regressing point estimates under\npermutation symmetry degrades performance, even when using a\npermutation-invariant loss function or symmetry-breaking heuristics. Then,\nviewing equivalent solutions as modes of a probability distribution, we show\nthat a conditional generative model substantially improves performance.\nFurther, acknowledging the invariance of the implicit parameter distribution,\nwe find that performance is further improved by using a permutation equivariant\ncontinuous normalizing flow. To accommodate intricate symmetries in real\nsynthesizers, we also propose a relaxed equivariance strategy that adaptively\ndiscovers relevant symmetries from data. Applying our method to Surge XT, a\nfull-featured open source synthesizer used in real world audio production, we\nfind our method outperforms regression and generative baselines across audio\nreconstruction metrics.", "AI": {"tldr": "论文研究了音频合成器参数反演问题，提出了一种基于条件生成模型和置换等变连续归一化流的方法，显著提升了性能。", "motivation": "音频合成器参数反演是一个病态问题，主要由于合成器的内在对称性（如置换不变性）。", "method": "使用条件生成模型和置换等变连续归一化流，并提出了自适应发现对称性的策略。", "result": "在Surge XT合成器上，该方法在音频重建指标上优于回归和生成基线。", "conclusion": "通过建模参数分布和对称性，显著改善了音频合成器参数反演的性能。"}}
{"id": "2506.07657", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07657", "abs": "https://arxiv.org/abs/2506.07657", "authors": ["Zeyu Xiao", "Zhenyi Wu", "Mingyang Sun", "Qipeng Yan", "Yufan Guo", "Zhuoer Liang", "Lihua Zhang"], "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians", "comment": null, "summary": "3D Gaussian Splatting has achieved remarkable success in reconstructing both\nstatic and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\nprimitives, interactions between objects suffer from inaccurate 3D\nsegmentation, imprecise deformation among different materials, and severe\nrendering artifacts. To address these challenges, we introduce PIG:\nPhysically-Based Multi-Material Interaction with 3D Gaussians, a novel approach\nthat combines 3D object segmentation with the simulation of interacting objects\nin high precision. Firstly, our method facilitates fast and accurate mapping\nfrom 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.\nSecondly, we assign unique physical properties to correspondingly segmented\nobjects within the scene for multi-material coupled interactions. Finally, we\nhave successfully embedded constraint scales into deformation gradients,\nspecifically clamping the scaling and rotation properties of the Gaussian\nprimitives to eliminate artifacts and achieve geometric fidelity and visual\nconsistency. Experimental results demonstrate that our method not only\noutperforms the state-of-the-art (SOTA) in terms of visual quality, but also\nopens up new directions and pipelines for the field of physically realistic\nscene generation.", "AI": {"tldr": "PIG是一种基于物理的多材料交互3D高斯方法，解决了3D高斯场景中对象交互的精度问题，通过2D到3D的快速映射、多材料交互和约束变形梯度，提升了视觉质量和几何保真度。", "motivation": "3D高斯场景中对象交互存在分割不准确、变形不精确和渲染伪影问题，需要一种新方法来提升精度和视觉一致性。", "method": "结合3D对象分割与高精度交互模拟，实现2D到3D的快速映射，为对象分配物理属性，并通过约束变形梯度消除伪影。", "result": "实验表明，PIG在视觉质量上优于现有方法，并为物理真实场景生成开辟了新方向。", "conclusion": "PIG通过多材料交互和约束变形梯度，显著提升了3D高斯场景的交互精度和视觉质量。"}}
{"id": "2506.07211", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07211", "abs": "https://arxiv.org/abs/2506.07211", "authors": ["Gionnieve Lim", "Bryan Chen Zhengyu Tan", "Kellie Yu Hui Sim", "Weiyan Shi", "Ming Hui Chew", "Ming Shan Hee", "Roy Ka-Wei Lee", "Simon T. Perrault", "Kenny Tsu Wei Choo"], "title": "Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation", "comment": null, "summary": "The emergence of Large Language Models (LLMs) presents a dual challenge in\nthe fight against disinformation. These powerful tools, capable of generating\nhuman-like text at scale, can be weaponised to produce sophisticated and\npersuasive disinformation, yet they also hold promise for enhancing detection\nand mitigation strategies. This paper investigates the complex dynamics between\nLLMs and disinformation through a communication game that simulates online\nforums, inspired by the game Werewolf, with 25 participants. We analyse how\nDisinformers, Moderators, and Users leverage LLMs to advance their goals,\nrevealing both the potential for misuse and combating disinformation. Our\nfindings highlight the varying uses of LLMs depending on the participants'\nroles and strategies, underscoring the importance of understanding their\neffectiveness in this context. We conclude by discussing implications for\nfuture LLM development and online platform design, advocating for a balanced\napproach that empowers users and fosters trust while mitigating the risks of\nLLM-assisted disinformation.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在虚假信息中的双重作用，通过模拟在线论坛的游戏实验，分析了不同角色如何利用LLMs，并提出了平衡发展的建议。", "motivation": "研究LLMs在虚假信息传播中的潜在威胁与应对能力，揭示其在不同角色中的动态应用。", "method": "采用类似狼人杀的游戏模拟在线论坛，25名参与者分别扮演散布者、管理者和用户，分析LLMs的使用策略。", "result": "发现LLMs的使用因角色和策略而异，既有滥用风险，也有助于对抗虚假信息。", "conclusion": "建议未来LLM开发和平台设计需平衡用户赋能与风险控制，以增强信任并减少虚假信息风险。"}}
{"id": "2506.06596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06596", "abs": "https://arxiv.org/abs/2506.06596", "authors": ["Youssef Farah", "Federico Paredes-Vallés", "Guido De Croon", "Muhammad Ahmed Humais", "Hussain Sajwani", "Yahya Zweiri"], "title": "EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras", "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Nashville, 2025", "summary": "Event cameras are novel bio-inspired sensors that capture motion dynamics\nwith much higher temporal resolution than traditional cameras, since pixels\nreact asynchronously to brightness changes. They are therefore better suited\nfor tasks involving motion such as motion segmentation. However, training\nevent-based networks still represents a difficult challenge, as obtaining\nground truth is very expensive, error-prone and limited in frequency. In this\narticle, we introduce EV-LayerSegNet, a self-supervised CNN for event-based\nmotion segmentation. Inspired by a layered representation of the scene\ndynamics, we show that it is possible to learn affine optical flow and\nsegmentation masks separately, and use them to deblur the input events. The\ndeblurring quality is then measured and used as self-supervised learning loss.\nWe train and test the network on a simulated dataset with only affine motion,\nachieving IoU and detection rate up to 71% and 87% respectively.", "AI": {"tldr": "EV-LayerSegNet是一种自监督CNN，用于事件相机的运动分割，通过分层场景动态表示学习光流和分割掩码，并利用去模糊质量作为自监督损失。", "motivation": "事件相机在运动任务中表现优异，但获取地面真实数据昂贵且困难，因此需要自监督方法。", "method": "提出EV-LayerSegNet，通过分层动态表示分别学习仿射光流和分割掩码，并利用去模糊质量作为自监督损失。", "result": "在仅含仿射运动的模拟数据集上，IoU和检测率分别达到71%和87%。", "conclusion": "EV-LayerSegNet展示了自监督方法在事件相机运动分割中的潜力。"}}
{"id": "2506.07468", "categories": ["cs.LG", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.07468", "abs": "https://arxiv.org/abs/2506.07468", "authors": ["Mickel Liu", "Liwei Jiang", "Yancheng Liang", "Simon Shaolei Du", "Yejin Choi", "Tim Althoff", "Natasha Jaques"], "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "comment": null, "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).", "AI": {"tldr": "Self-RedTeam是一种在线自博弈强化学习算法，通过攻击者和防御者角色的动态交互提升语言模型的安全性。", "motivation": "传统语言模型安全对齐是反应式的，攻击者和防御者分离，导致防御滞后于威胁。Self-RedTeam旨在通过动态协同进化解决这一问题。", "method": "将安全对齐建模为零和博弈，模型交替扮演攻击者和防御者角色，通过奖励模型裁决结果，实现动态协同适应。", "result": "Self-RedTeam发现更多样化的攻击（+21.8% SBERT），并在安全基准测试中表现更鲁棒（如WildJailBreak上+65.5%）。", "conclusion": "研究提倡从被动修补转向主动协同进化，通过多智能体强化学习实现语言模型的自主、鲁棒自我提升。"}}
{"id": "2506.06558", "categories": ["cs.LG", "cs.NE", "68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.06558", "abs": "https://arxiv.org/abs/2506.06558", "authors": ["Atamert Rahma", "Chinmay Datar", "Ana Cukarska", "Felix Dietrich"], "title": "Rapid training of Hamiltonian graph networks without gradient descent", "comment": "10 pages, 7 figures, 2 tables, and an appendix", "summary": "Learning dynamical systems that respect physical symmetries and constraints\nremains a fundamental challenge in data-driven modeling. Integrating physical\nlaws with graph neural networks facilitates principled modeling of complex\nN-body dynamics and yields accurate and permutation-invariant models. However,\ntraining graph neural networks with iterative, gradient-based optimization\nalgorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,\nespecially for large, complex systems. In comparison to 15 different\noptimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained\nup to 600x faster--but with comparable accuracy--by replacing iterative\noptimization with random feature-based parameter construction. We show robust\nperformance in diverse simulations, including N-body mass-spring systems in up\nto 3 dimensions with different geometries, while retaining essential physical\ninvariances with respect to permutation, rotation, and translation. We reveal\nthat even when trained on minimal 8-node systems, the model can generalize in a\nzero-shot manner to systems as large as 4096 nodes without retraining. Our work\nchallenges the dominance of iterative gradient-descent-based optimization\nalgorithms for training neural network models for physical systems.", "AI": {"tldr": "论文提出了一种基于哈密顿图网络（HGN）的方法，通过随机特征参数构造替代传统迭代优化，显著提升了训练速度（高达600倍），同时保持了物理对称性和约束。", "motivation": "解决数据驱动建模中物理对称性和约束的挑战，并优化传统图神经网络训练速度慢的问题。", "method": "使用哈密顿图网络（HGN）结合随机特征参数构造，替代传统的梯度下降优化算法（如Adam、RMSProp）。", "result": "在多种模拟中表现稳健，包括3维N体弹簧系统，并能零样本泛化到4096节点系统。", "conclusion": "挑战了传统梯度下降优化算法在物理系统神经网络训练中的主导地位。"}}
{"id": "2506.06300", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.06300", "abs": "https://arxiv.org/abs/2506.06300", "authors": ["Yuanye Zhou", "Zhaokun Wang", "Kai Zhou", "Hui Tang", "Xiaofan Li"], "title": "LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization", "comment": null, "summary": "Physics-informed neural networks (PINNs) have emerged as a powerful meshless\ntool for topology optimization, capable of simultaneously determining optimal\ntopologies and physical solutions. However, conventional PINNs rely on\ndensity-based topology descriptions, which necessitate manual interpolation and\nlimit their applicability to complex geometries. To address this, we propose\nLagrangian topology-conscious PINNs (LT-PINNs), a novel framework for\nboundary-focused engineering optimization. By parameterizing the control\nvariables of topology boundary curves as learnable parameters, LT-PINNs\neliminate the need for manual interpolation and enable precise boundary\ndetermination. We further introduce specialized boundary condition loss\nfunction and topology loss function to ensure sharp and accurate boundary\nrepresentations, even for intricate topologies. The accuracy and robustness of\nLT-PINNs are validated via two types of partial differential equations (PDEs),\nincluding elastic equation with Dirichlet boundary conditions and Laplace's\nequation with Neumann boundary conditions. Furthermore, we demonstrate\neffectiveness of LT-PINNs on more complex time-dependent and time-independent\nflow problems without relying on measurement data, and showcase their\nengineering application potential in flow velocity rearrangement, transforming\na uniform upstream velocity into a sine-shaped downstream profile. The results\ndemonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors\ncompared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)\nLT-PINNs can handle arbitrary boundary conditions, making them suitable for a\nwide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries\nwithout manual interpolation, especially for complex topologies.", "AI": {"tldr": "LT-PINNs是一种新型的物理信息神经网络框架，通过参数化拓扑边界曲线，解决了传统PINNs在复杂几何中的局限性，并显著降低了误差。", "motivation": "传统PINNs依赖基于密度的拓扑描述，需要手动插值且难以处理复杂几何。LT-PINNs旨在消除这些限制，实现精确的边界优化。", "method": "LT-PINNs将拓扑边界曲线的控制变量参数化为可学习参数，并引入边界条件和拓扑损失函数，确保边界清晰准确。", "result": "实验验证了LT-PINNs在多种PDE问题中的准确性和鲁棒性，显著降低了相对L2误差，并能处理任意边界条件。", "conclusion": "LT-PINNs在工程优化中具有广泛应用潜力，尤其适用于复杂拓扑和边界问题。"}}
{"id": "2506.06567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06567", "abs": "https://arxiv.org/abs/2506.06567", "authors": ["Bowei Li", "Peiqi Yu", "Zhenran Tang", "Han Zhou", "Yifan Sun", "Ruixuan Liu", "Changliu Liu"], "title": "NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing", "comment": "10 pages, 5 figures. Accepted to the RSS 2025 Workshop on\n  Benchmarking Robot Manipulation: Improving Interoperability and Modularity.\n  First Prize in the WBCD competition at ICRA 2025. Equal contribution by Bowei\n  Li and Peiqi Yu", "summary": "This paper presents NeSyPack, a neuro-symbolic framework for bimanual\nlogistics packing. NeSyPack combines data-driven models and symbolic reasoning\nto build an explainable hierarchical system that is generalizable,\ndata-efficient, and reliable. It decomposes a task into subtasks via\nhierarchical reasoning, and further into atomic skills managed by a symbolic\nskill graph. The graph selects skill parameters, robot configurations, and\ntask-specific control strategies for execution. This modular design enables\nrobustness, adaptability, and efficient reuse - outperforming end-to-end models\nthat require large-scale retraining. Using NeSyPack, our team won the First\nPrize in the What Bimanuals Can Do (WBCD) competition at the 2025 IEEE\nInternational Conference on Robotics and Automation.", "AI": {"tldr": "NeSyPack是一个结合数据驱动模型与符号推理的神经符号框架，用于双手物流包装任务，具有可解释性、通用性和高效性。", "motivation": "解决双手物流包装任务中端到端模型需要大规模重新训练的问题，提供更高效、可靠且可解释的解决方案。", "method": "通过分层推理将任务分解为子任务，并使用符号技能图管理原子技能，选择参数、配置和控制策略。", "result": "NeSyPack在2025年IEEE国际机器人与自动化会议的WBCD竞赛中获得一等奖。", "conclusion": "NeSyPack的模块化设计在鲁棒性、适应性和高效复用方面优于端到端模型。"}}
{"id": "2506.06746", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06746", "abs": "https://arxiv.org/abs/2506.06746", "authors": ["Ziming Wang", "Yihuai Zhang", "Chenguang Zhao", "Huan Yu"], "title": "Adaptive Event-triggered Formation Control of Autonomous Vehicles", "comment": null, "summary": "This paper presents adaptive event-triggered formation control strategies for\nautonomous vehicles (AVs) subject to longitudinal and lateral motion\nuncertainties. The proposed framework explores various vehicular formations to\nenable safe and efficient navigation in complex traffic scenarios, such as\nnarrow passages, collaborative obstacle avoidance, and adaptation to cut-in\nmaneuvers. In contrast to conventional platoon control strategies that rely on\npredefined communication topologies and continuous state transmission, our\napproach employs a sampling-based observer to reconstruct vehicle dynamics.\nBuilding upon an adaptive backstepping continuous-time controller, we design\nthree distinct event-triggered mechanisms, each offering a different trade-off\nbetween formation tracking performance and control efficiency by reducing the\nfrequency of control signal updates. A Lyapunov-based stability analysis is\nconducted to guarantee bounded tracking errors and to avoid Zeno behavior.\nFinally, the proposed event-triggered controllers are validated through\nsimulations of vehicular formation in three scenarios, highlighting their\nimpact on traffic safety and mobility.", "AI": {"tldr": "本文提出了一种自适应事件触发的编队控制策略，用于解决自主车辆在纵向和横向运动不确定性下的编队问题，通过减少控制信号更新频率来平衡跟踪性能和控制效率。", "motivation": "传统编队控制策略依赖预定义的通信拓扑和连续状态传输，难以适应复杂交通场景（如狭窄通道、协作避障和切入机动）。本文旨在设计更灵活高效的控制方法。", "method": "采用基于采样的观测器重构车辆动力学，结合自适应反步连续时间控制器，设计了三种不同的事件触发机制。通过李雅普诺夫稳定性分析确保跟踪误差有界并避免Zeno行为。", "result": "仿真验证了所提控制器在三种交通场景下的有效性，提升了交通安全性和机动性。", "conclusion": "自适应事件触发编队控制策略在复杂交通场景中表现出色，平衡了性能与效率，具有实际应用潜力。"}}
{"id": "2506.07207", "categories": ["cs.SD", "eess.AS", "00A65", "J.5"], "pdf": "https://arxiv.org/pdf/2506.07207", "abs": "https://arxiv.org/abs/2506.07207", "authors": ["Emmanuel Deruty", "Pascal Arbez-Nicolas", "David Meredith"], "title": "Methods for pitch analysis in contemporary popular music: Vitalic's use of tones that do not operate on the principle of acoustic resonance", "comment": null, "summary": "Vitalic is an electronic music producer who has been active since 2001.\nVitalic's 2005 track \"No Fun\" features a main synthesiser part built from a\nsequence of single inharmonic tones that evoke two simultaneous melodies. This\npart serves as a starting point for examining Vitalic's use of tones that do\nnot operate on the principle of acoustic resonance. The study considers tones\nthat evoke two or more simultaneous pitches and examines various inharmonic\npartial layouts. Examples outside Vitalic's music are also provided to suggest\nthat similar tone properties can be found elsewhere in contemporary popular\nmusic.", "AI": {"tldr": "论文分析了Vitalic在电子音乐中使用非谐波音调的技术，探讨了其如何通过单音序列创造双重旋律效果，并指出类似技术在当代流行音乐中的普遍性。", "motivation": "研究Vitalic如何利用非谐波音调创造独特的音乐效果，并探讨这种技术在更广泛音乐领域的应用。", "method": "通过分析Vitalic的曲目（如2005年的“No Fun”），研究其音调结构和非谐波部分布局，同时提供其他流行音乐中的类似例子。", "result": "发现Vitalic通过非谐波音调成功创造出双重旋律效果，且这种技术在其他当代流行音乐中也有体现。", "conclusion": "非谐波音调技术为音乐创作提供了新的可能性，且在当代音乐中有广泛应用潜力。"}}
{"id": "2506.07897", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07897", "abs": "https://arxiv.org/abs/2506.07897", "authors": ["Shuja Khalid", "Mohamed Ibrahim", "Yang Liu"], "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution", "comment": null, "summary": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.", "AI": {"tldr": "提出了一种轻量级生成模型，通过Hessian辅助采样策略提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率限制。", "motivation": "当前3DGS方法受限于输入分辨率，无法从训练视图中推断更精细的细节。", "method": "采用轻量级生成模型预测并细化额外的3D高斯分布，结合Hessian辅助采样策略智能识别需要密集化的区域。", "result": "在单消费级GPU上实现实时推理（0.015s），几何精度和渲染质量显著优于现有方法。", "conclusion": "为分辨率无关的3D场景增强建立了新范式。"}}
{"id": "2506.07278", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07278", "abs": "https://arxiv.org/abs/2506.07278", "authors": ["Victor B. Santos", "Cauã O. Jordão", "Leonardo J. O. Ibiapina", "Gabriel M. Silva", "Mirella E. B. Santana", "Matheus A. Garrido", "Lucas R. C. Farias"], "title": "IDEIA: A Generative AI-Based System for Real-Time Editorial Ideation in Digital Journalism", "comment": "9 pages, 5 figures", "summary": "This paper presents IDEIA (Intelligent Engine for Editorial Ideation and\nAssistance), a generative AI-powered system designed to optimize the\njournalistic ideation process by combining real-time trend analysis with\nautomated content suggestion. Developed in collaboration with the Sistema\nJornal do Commercio de Comunica\\c{c}\\~ao (SJCC), the largest media conglomerate\nin Brazil's North and Northeast regions, IDEIA integrates the Google Trends API\nfor data-driven topic monitoring and the Google Gemini API for the generation\nof context-aware headlines and summaries. The system adopts a modular\narchitecture based on Node.js, React, and PostgreSQL, supported by Docker\ncontainerization and a CI/CD pipeline using GitHub Actions and Vercel.\nEmpirical results demonstrate a significant reduction in the time and cognitive\neffort required for editorial planning, with reported gains of up to 70\\% in\nthe content ideation stage. This work contributes to the field of computational\njournalism by showcasing how intelligent automation can enhance productivity\nwhile maintaining editorial quality. It also discusses the technical and\nethical implications of incorporating generative models into newsroom\nworkflows, highlighting scalability and future applicability across sectors\nbeyond journalism.", "AI": {"tldr": "IDEIA是一个基于生成式AI的系统，通过结合实时趋势分析和自动化内容建议优化新闻创意过程，显著提升效率。", "motivation": "旨在通过智能自动化减少新闻编辑过程中的时间和认知负担，同时保持编辑质量。", "method": "集成Google Trends API进行主题监控，使用Google Gemini API生成上下文相关的标题和摘要，采用Node.js、React、PostgreSQL等技术栈，并支持Docker和CI/CD流程。", "result": "实证结果显示，编辑规划阶段的时间和认知努力显著减少，内容创意效率提升高达70%。", "conclusion": "IDEIA展示了智能自动化在新闻领域的潜力，同时探讨了生成模型在新闻工作流程中的技术和伦理问题。"}}
{"id": "2506.06600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06600", "abs": "https://arxiv.org/abs/2506.06600", "authors": ["Tan-Hanh Pham", "Chris Ngo"], "title": "RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints", "comment": "Under review", "summary": "The growing integration of vision-language models (VLMs) in medical\napplications offers promising support for diagnostic reasoning. However,\ncurrent medical VLMs often face limitations in generalization, transparency,\nand computational efficiency-barriers that hinder deployment in real-world,\nresource-constrained settings. To address these challenges, we propose a\nReasoning-Aware Reinforcement Learning framework, \\textbf{RARL}, that enhances\nthe reasoning capabilities of medical VLMs while remaining efficient and\nadaptable to low-resource environments. Our approach fine-tunes a lightweight\nbase model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward\nfunctions that jointly consider diagnostic accuracy and reasoning quality.\nTraining is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the\nfeasibility of deploying such models in constrained environments. We evaluate\nthe model using an LLM-as-judge framework that scores both correctness and\nexplanation quality. Experimental results show that RARL significantly improves\nVLM performance in medical image analysis and clinical reasoning, outperforming\nsupervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while\nrequiring fewer computational resources. Additionally, we demonstrate the\ngeneralization capabilities of our approach on unseen datasets, achieving\naround 27% improved performance compared to supervised fine-tuning and about 4%\nover traditional RL fine-tuning. Our experiments also illustrate that diversity\nprompting during training and reasoning prompting during inference are crucial\nfor enhancing VLM performance. Our findings highlight the potential of\nreasoning-guided learning and reasoning prompting to steer medical VLMs toward\nmore transparent, accurate, and resource-efficient clinical decision-making.\nCode and data are publicly available.", "AI": {"tldr": "论文提出了一种名为RARL的推理感知强化学习框架，旨在提升医疗视觉语言模型（VLM）的推理能力，同时保持高效性和低资源适应性。实验表明，RARL在医疗图像分析和临床推理任务中表现优异，且计算资源需求较低。", "motivation": "当前医疗VLM在泛化性、透明性和计算效率方面存在局限，阻碍了其在资源受限环境中的实际应用。", "method": "采用低秩适应（Low-Rank Adaptation）和自定义奖励函数对轻量级基础模型Qwen2-VL-2B-Instruct进行微调，结合诊断准确性和推理质量。训练在单块NVIDIA A100-PCIE-40GB GPU上完成。", "result": "RARL在推理任务中比监督微调性能提升约7.78%，在未见数据集上泛化能力提升27%，且计算资源需求更低。", "conclusion": "推理引导学习和推理提示技术可显著提升医疗VLM的透明度、准确性和资源效率，具有实际应用潜力。"}}
{"id": "2506.07755", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07755", "abs": "https://arxiv.org/abs/2506.07755", "authors": ["Nikolaos Bousias", "Lars Lindemann", "George Pappas"], "title": "Deep Equivariant Multi-Agent Control Barrier Functions", "comment": null, "summary": "With multi-agent systems increasingly deployed autonomously at scale in\ncomplex environments, ensuring safety of the data-driven policies is critical.\nControl Barrier Functions have emerged as an effective tool for enforcing\nsafety constraints, yet existing learning-based methods often lack in\nscalability, generalization and sampling efficiency as they overlook inherent\ngeometric structures of the system. To address this gap, we introduce\nsymmetries-infused distributed Control Barrier Functions, enforcing the\nsatisfaction of intrinsic symmetries on learnable graph-based safety\ncertificates. We theoretically motivate the need for equivariant\nparametrization of CBFs and policies, and propose a simple, yet efficient and\nadaptable methodology for constructing such equivariant group-modular networks\nvia the compatible group actions. This approach encodes safety constraints in a\ndistributed data-efficient manner, enabling zero-shot generalization to larger\nand denser swarms. Through extensive simulations on multi-robot navigation\ntasks, we demonstrate that our method outperforms state-of-the-art baselines in\nterms of safety, scalability, and task success rates, highlighting the\nimportance of embedding symmetries in safe distributed neural policies.", "AI": {"tldr": "论文提出了一种基于对称性的分布式控制屏障函数方法，用于提升多智能体系统的安全性和可扩展性。", "motivation": "随着多智能体系统在复杂环境中的自主部署，确保数据驱动策略的安全性至关重要。现有方法在可扩展性、泛化性和采样效率方面存在不足。", "method": "引入对称性注入的分布式控制屏障函数，通过可学习的基于图的安全证书满足内在对称性。提出了一种构建等变群模块网络的简单高效方法。", "result": "在仿真实验中，该方法在安全性、可扩展性和任务成功率方面优于现有基准。", "conclusion": "嵌入对称性对分布式神经策略的安全性至关重要，该方法具有零样本泛化能力。"}}
{"id": "2506.07109", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.07109", "abs": "https://arxiv.org/abs/2506.07109", "authors": ["Rong-Xi Tan", "Ming Chen", "Ke Xue", "Yao Wang", "Yaoyuan Wang", "Sheng Fu", "Chao Qian"], "title": "Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings", "comment": "ICML 2025", "summary": "The pursuit of universal black-box optimization (BBO) algorithms is a\nlongstanding goal. However, unlike domains such as language or vision, where\nscaling structured data has driven generalization, progress in offline BBO\nremains hindered by the lack of unified representations for heterogeneous\nnumerical spaces. Thus, existing offline BBO approaches are constrained to\nsingle-task and fixed-dimensional settings, failing to achieve cross-domain\nuniversal optimization. Recent advances in language models (LMs) offer a\npromising path forward: their embeddings capture latent relationships in a\nunifying way, enabling universal optimization across different data types\npossible. In this paper, we discuss multiple potential approaches, including an\nend-to-end learning framework in the form of next-token prediction, as well as\nprioritizing the learning of latent spaces with strong representational\ncapabilities. To validate the effectiveness of these methods, we collect\noffline BBO tasks and data from open-source academic works for training.\nExperiments demonstrate the universality and effectiveness of our proposed\nmethods. Our findings suggest that unifying language model priors and learning\nstring embedding space can overcome traditional barriers in universal BBO,\npaving the way for general-purpose BBO algorithms. The code is provided at\nhttps://github.com/lamda-bbo/universal-offline-bbo.", "AI": {"tldr": "论文探讨了如何利用语言模型的嵌入能力实现跨领域通用黑盒优化（BBO），提出多种方法并验证其有效性。", "motivation": "传统离线BBO方法受限于异构数值空间的统一表示问题，无法实现跨领域通用优化，而语言模型的嵌入能力为解决这一问题提供了新思路。", "method": "提出多种方法，包括基于下一个令牌预测的端到端学习框架，以及学习具有强表示能力的潜在空间。", "result": "实验证明所提方法在通用性和有效性上表现优异。", "conclusion": "结合语言模型先验和学习字符串嵌入空间可以突破传统BBO的限制，为通用BBO算法开辟新路径。"}}
{"id": "2506.06303", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.06303", "abs": "https://arxiv.org/abs/2506.06303", "authors": ["Kefan Song", "Amir Moeini", "Peng Wang", "Lei Gong", "Rohan Chandra", "Yanjun Qi", "Shangtong Zhang"], "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "comment": null, "summary": "Reinforcement learning (RL) is a human-designed framework for solving\nsequential decision making problems. In this work, we demonstrate that,\nsurprisingly, RL emerges in LLM's (Large Language Model) inference time -- a\nphenomenon known as in-context RL (ICRL). Specifically, we propose a novel\nmulti-round prompting framework called ICRL prompting. The goal is to prompt\nthe LLM to complete a task. After the LLM generates a response at the current\nround, we give numerical scalar feedbacks for the response, called the rewards.\nAt the next round, we prompt the LLM again with the same task and a context\nconsisting of all previous responses and rewards. We observe that the quality\nof the LLM's response increases as the context grows. In other words, the LLM\nis able to maximize the scalar reward signal in the inference time, just like\nan RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,\ncreative writing, and ScienceWorld) and demonstrate significant performance\nimprovements over baseline methods such as Self-Refine and Reflexion.\nSurprisingly, in some experiments the reward signals are generated by the LLM\nitself, yet performance improvements are still observed from ICRL prompting,\noffering a promising paradigm for scaling test-time compute.", "AI": {"tldr": "论文提出了一种名为ICRL prompting的多轮提示框架，展示了大型语言模型（LLM）在推理过程中表现出类似强化学习（RL）的行为，即通过上下文中的反馈优化响应质量。", "motivation": "探索LLM在推理时是否能够通过多轮反馈机制（类似RL）优化任务表现。", "method": "提出ICRL prompting框架，通过多轮提示和数值反馈（奖励）逐步优化LLM的响应。", "result": "在Game of 24、创意写作和ScienceWorld三个基准测试中，ICRL prompting显著优于基线方法，且即使奖励信号由LLM自身生成，性能仍能提升。", "conclusion": "ICRL prompting为LLM在推理时优化任务表现提供了新范式，展示了类似RL的行为，具有扩展测试时计算的潜力。"}}
{"id": "2506.06570", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06570", "abs": "https://arxiv.org/abs/2506.06570", "authors": ["Aryaman Gupta", "Yusuf Umut Ciftci", "Somil Bansal"], "title": "Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data", "comment": null, "summary": "As robotic systems become increasingly integrated into real-world\nenvironments, ranging from autonomous vehicles to household assistants, they\ninevitably encounter diverse and unstructured scenarios that lead to failures.\nWhile such failures pose safety and reliability challenges, they also provide\nrich perceptual data for improving future performance. However, manually\nanalyzing large-scale failure datasets is impractical. In this work, we present\na method for automatically organizing large-scale robotic failure data into\nsemantically meaningful clusters, enabling scalable learning from failure\nwithout human supervision. Our approach leverages the reasoning capabilities of\nMultimodal Large Language Models (MLLMs), trained on internet-scale data, to\ninfer high-level failure causes from raw perceptual trajectories and discover\ninterpretable structure within uncurated failure logs. These semantic clusters\nreveal latent patterns and hypothesized causes of failure, enabling scalable\nlearning from experience. We demonstrate that the discovered failure modes can\nguide targeted data collection for policy refinement, accelerating iterative\nimprovement in agent policies and overall safety. Additionally, we show that\nthese semantic clusters can be employed for online failure detection, offering\na lightweight yet powerful safeguard for real-time adaptation. We demonstrate\nthat this framework enhances robot learning and robustness by transforming\nreal-world failures into actionable and interpretable signals for adaptation.", "AI": {"tldr": "提出一种利用多模态大语言模型（MLLMs）自动聚类大规模机器人失败数据的方法，以无监督方式从失败中学习。", "motivation": "机器人系统在现实环境中常因多样化和非结构化场景而失败，手动分析大规模失败数据不切实际，需自动化方法。", "method": "利用MLLMs从原始感知轨迹推断高级失败原因，发现未标注失败日志中的可解释结构，形成语义聚类。", "result": "语义聚类揭示了潜在失败模式和原因，可用于指导策略优化和在线失败检测，提升机器人学习和鲁棒性。", "conclusion": "该框架将现实失败转化为可操作的信号，加速机器人策略迭代改进和实时适应。"}}
{"id": "2506.06824", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06824", "abs": "https://arxiv.org/abs/2506.06824", "authors": ["Chi Liu", "Zhezhuang Xu", "Jiawei Zhou", "Yazhou Yuan", "Kai Ma", "Meng Yuan"], "title": "Deep reinforcement learning-based joint real-time energy scheduling for green buildings with heterogeneous battery energy storage devices", "comment": null, "summary": "Green buildings (GBs) with renewable energy and building energy management\nsystems (BEMS) enable efficient energy use and support sustainable development.\nElectric vehicles (EVs), as flexible storage resources, enhance system\nflexibility when integrated with stationary energy storage systems (ESS) for\nreal-time scheduling. However, differing degradation and operational\ncharacteristics of ESS and EVs complicate scheduling strategies. This paper\nproposes a model-free deep reinforcement learning (DRL) method for joint\nreal-time scheduling based on a combined battery system (CBS) integrating ESS\nand EVs. We develop accurate degradation models and cost estimates, prioritize\nEV travel demands, and enable collaborative ESS-EV operation under varying\nconditions. A prediction model optimizes energy interaction between CBS and\nBEMS. To address heterogeneous states, action coupling, and learning\nefficiency, the DRL algorithm incorporates double networks, a dueling\nmechanism, and prioritized experience replay. Experiments show a 37.94 percent\nto 40.01 percent reduction in operating costs compared to a mixed-integer\nlinear programming (MILP) approach.", "AI": {"tldr": "本文提出了一种基于深度强化学习（DRL）的无模型方法，用于联合实时调度结合储能系统（ESS）和电动汽车（EV）的复合电池系统（CBS），显著降低了运营成本。", "motivation": "绿色建筑（GBs）和电动汽车（EVs）的能源管理面临储能系统和电动汽车的不同退化特性和操作复杂性，需要高效的调度策略。", "method": "开发了精确的退化模型和成本估算，优先满足EV出行需求，并在不同条件下实现ESS-EV协作。采用双网络、决斗机制和优先经验回放的DRL算法优化调度。", "result": "实验显示，与混合整数线性规划（MILP）方法相比，运营成本降低了37.94%至40.01%。", "conclusion": "提出的DRL方法在联合调度ESS和EV方面表现出色，显著提升了能源管理效率和经济效益。"}}
{"id": "2506.07294", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07294", "abs": "https://arxiv.org/abs/2506.07294", "authors": ["Xuanjun Chen", "I-Ming Lin", "Lin Zhang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "Towards Generalized Source Tracing for Codec-Based Deepfake Speech", "comment": "Submitted to IEEE ASRU 2025", "summary": "Recent attempts at source tracing for codec-based deepfake speech\n(CodecFake), generated by neural audio codec-based speech generation (CoSG)\nmodels, have exhibited suboptimal performance. However, how to train source\ntracing models using simulated CoSG data while maintaining strong performance\non real CoSG-generated audio remains an open challenge. In this paper, we show\nthat models trained solely on codec-resynthesized data tend to overfit to\nnon-speech regions and struggle to generalize to unseen content. To mitigate\nthese challenges, we introduce the Semantic-Acoustic Source Tracing Network\n(SASTNet), which jointly leverages Whisper for semantic feature encoding and\nWav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet\nachieves state-of-the-art performance on the CoSG test set of the CodecFake+\ndataset, demonstrating its effectiveness for reliable source tracing.", "AI": {"tldr": "论文提出SASTNet模型，结合语义和声学特征编码，显著提升CodecFake语音的溯源性能。", "motivation": "现有CodecFake语音溯源方法性能不足，且模拟数据训练的模型难以泛化到真实数据。", "method": "提出SASTNet，联合使用Whisper（语义特征）和Wav2vec2与AudioMAE（声学特征）进行编码。", "result": "SASTNet在CodecFake+数据集的CoSG测试集上达到最优性能。", "conclusion": "SASTNet有效解决了CodecFake语音溯源的泛化问题，性能领先。"}}
{"id": "2506.07917", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07917", "abs": "https://arxiv.org/abs/2506.07917", "authors": ["Allen Tu", "Haiyang Ying", "Alex Hanson", "Yonghan Lee", "Tom Goldstein", "Matthias Zwicker"], "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes", "comment": "Project Page: https://speede3dgs.github.io/", "summary": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve\nhigh-quality novel view synthesis by using neural networks to predict the\ntime-varying deformation of each Gaussian. However, performing per-Gaussian\nneural inference at every frame poses a significant bottleneck, limiting\nrendering speed and increasing memory and compute requirements. In this paper,\nwe present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general\npipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS\nrepresentations by reducing neural inference through two complementary\ntechniques. First, we propose a temporal sensitivity pruning score that\nidentifies and removes Gaussians with low contribution to the dynamic scene\nreconstruction. We also introduce an annealing smooth pruning mechanism that\nimproves pruning robustness in real-world scenes with imprecise camera poses.\nSecond, we propose GroupFlow, a motion analysis technique that clusters\nGaussians by trajectory similarity and predicts a single rigid transformation\nper group instead of separate deformations for each Gaussian. Together, our\ntechniques accelerate rendering by $10.37\\times$, reduce model size by\n$7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset.\nSpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on\nthe D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be\nintegrated into any deformable 3DGS or 4DGS framework.", "AI": {"tldr": "SpeeDe3DGS通过时间敏感剪枝和GroupFlow技术，显著提升了动态3D高斯泼溅的渲染速度，减少了模型大小和训练时间。", "motivation": "动态3D高斯泼溅（3DGS）中，每帧对每个高斯进行神经推断导致渲染速度慢、内存和计算需求高，亟需优化。", "method": "提出时间敏感剪枝分数和退火平滑剪枝机制，去除低贡献高斯；引入GroupFlow技术，通过轨迹相似性聚类高斯并预测每组刚性变换。", "result": "在NeRF-DS数据集上，渲染速度提升10.37倍，模型大小减少7.71倍，训练时间缩短2.71倍；在D-NeRF和HyperNeRF vrig数据集上也有显著提升。", "conclusion": "SpeeDe3DGS是一种通用且模块化的方法，可显著提升动态3DGS和4DGS的渲染效率。"}}
{"id": "2506.07281", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07281", "abs": "https://arxiv.org/abs/2506.07281", "authors": ["Leah Hope Ajmani", "Nuredin Ali Abdelkadir", "Stevie Chancellor"], "title": "Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency", "comment": null, "summary": "As AI technologies become more human-facing, there have been numerous calls\nto adapt participatory approaches to AI development -- spurring the idea of\nparticipatory AI. However, these calls often focus only on primary\nstakeholders, such as end-users, and not secondary stakeholders. This paper\nseeks to translate the ideals of participatory AI to a broader population of\nsecondary AI stakeholders through semi-structured interviews. We theorize that\nmeaningful participation involves three participatory ideals: (1) informedness,\n(2) consent, and (3) agency. We also explore how secondary stakeholders realize\nthese ideals by traversing a complicated problem space. Like walking up the\nrungs of a ladder, these ideals build on one another. We introduce three\nstakeholder archetypes: the reluctant data contributor, the unsupported\nactivist, and the well-intentioned practitioner, who must navigate systemic\nbarriers to achieving agentic AI relationships. We envision an AI future where\nsecondary stakeholders are able to meaningfully participate with the AI systems\nthey influence and are influenced by.", "AI": {"tldr": "论文探讨了如何将参与式AI理念扩展到次要利益相关者，提出三个参与理想（知情、同意、代理），并通过半结构化访谈研究其实现方式。", "motivation": "随着AI技术更面向人类，现有参与式AI方法主要关注主要利益相关者（如终端用户），而忽略了次要利益相关者。本文旨在填补这一空白。", "method": "通过半结构化访谈，研究次要利益相关者如何实现三个参与理想（知情、同意、代理），并分析其面临的系统性障碍。", "result": "提出了三种次要利益相关者原型（不情愿的数据贡献者、无支持的活动家、善意的实践者），揭示了他们在实现代理关系时的挑战。", "conclusion": "呼吁未来AI发展中，次要利益相关者能够更有效地参与并影响AI系统。"}}
{"id": "2506.06602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06602", "abs": "https://arxiv.org/abs/2506.06602", "authors": ["Santhosh Kakarla", "Gautama Shastry Bulusu Venkata"], "title": "Zero Shot Composed Image Retrieval", "comment": "8 pages, 3 figures", "summary": "Composed image retrieval (CIR) allows a user to locate a target image by\napplying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove\nstripes'') to a reference image. Zero-shot CIR, which embeds the image and the\ntext with separate pretrained vision-language encoders, reaches only 20-25\\%\nRecall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2\nwith a lightweight Q-Former that fuses visual and textual features into a\nsingle embedding, raising Recall@10 to 45.6\\% (shirt), 40.1\\% (dress), and\n50.4\\% (top-tee) and increasing the average Recall@50 to 67.6\\%. We also\nexamine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct\nPreference Optimization loss applied to FAISS-mined hard negatives. Despite\nextensive tuning of the scaling factor, index, and sampling strategy,\nRetrieval-DPO attains only 0.02\\% Recall@10 -- far below zero-shot and\nprompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)\nuses a margin objective misaligned with top-$K$ metrics, (iii) relies on\nlow-quality negatives, and (iv) keeps the vision and Transformer layers frozen.\nOur results show that effective preference-based CIR requires genuine\nmultimodal fusion, ranking-aware objectives, and carefully curated negatives.", "AI": {"tldr": "通过微调BLIP-2和轻量级Q-Former，将FashionIQ基准上的Recall@10提升至45.6%（衬衫）、40.1%（连衣裙）和50.4%（T恤），平均Recall@50达67.6%。Retrieval-DPO方法因缺乏多模态融合和高质量负样本，效果较差。", "motivation": "提升零样本组合图像检索（CIR）的性能，解决现有方法在FashionIQ基准上Recall@10仅为20-25%的问题。", "method": "1. 微调BLIP-2，使用轻量级Q-Former融合视觉和文本特征；2. 尝试Retrieval-DPO方法，优化CLIP文本编码器。", "result": "BLIP-2方法显著提升Recall@10和Recall@50；Retrieval-DPO因多模态融合不足和负样本质量低，效果不佳。", "conclusion": "有效的基于偏好的CIR需要多模态融合、排名感知目标和高质量负样本。"}}
{"id": "2506.07121", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.07121", "abs": "https://arxiv.org/abs/2506.07121", "authors": ["Ren-Jian Wang", "Ke Xue", "Zeyu Qin", "Ziniu Li", "Sheng Tang", "Hao-Tian Li", "Shengcai Liu", "Chao Qian"], "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models", "comment": null, "summary": "Ensuring safety of large language models (LLMs) is important. Red teaming--a\nsystematic approach to identifying adversarial prompts that elicit harmful\nresponses from target LLMs--has emerged as a crucial safety evaluation method.\nWithin this framework, the diversity of adversarial prompts is essential for\ncomprehensive safety assessments. We find that previous approaches to\nred-teaming may suffer from two key limitations. First, they often pursue\ndiversity through simplistic metrics like word frequency or sentence embedding\nsimilarity, which may not capture meaningful variation in attack strategies.\nSecond, the common practice of training a single attacker model restricts\ncoverage across potential attack styles and risk categories. This paper\nintroduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to\naddress these limitations. QDRT achieves goal-driven diversity through\nbehavior-conditioned training and implements a behavioral replay buffer in an\nopen-ended manner. Additionally, it trains multiple specialized attackers\ncapable of generating high-quality attacks across diverse styles and risk\ncategories. Our empirical evaluation demonstrates that QDRT generates attacks\nthat are both more diverse and more effective against a wide range of target\nLLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the\nfield of LLM safety by providing a systematic and effective approach to\nautomated red-teaming, ultimately supporting the responsible deployment of\nLLMs.", "AI": {"tldr": "QDRT框架通过行为条件训练和多样化攻击者模型，解决了传统红队测试中攻击多样性和覆盖范围的不足，提升了大型语言模型的安全性评估。", "motivation": "传统红队测试方法在攻击多样性和覆盖范围上存在局限，无法全面评估大型语言模型的安全性。", "method": "提出QDRT框架，通过行为条件训练和开放式的行为重放缓冲区，训练多个专业化攻击者模型。", "result": "QDRT生成的攻击更有效且多样化，适用于多种目标模型（如GPT-2、Llama-3等）。", "conclusion": "QDRT为自动化红队测试提供了系统化方法，支持大型语言模型的安全部署。"}}
{"id": "2506.06327", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06327", "abs": "https://arxiv.org/abs/2506.06327", "authors": ["Zilang Chen"], "title": "Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study", "comment": "14pages, 7figures,2tables", "summary": "Accurate and reproducible wine-quality assessment is critical for production\ncontrol yet remains dominated by subjective, labour-intensive tasting panels.\nWe present the first unified benchmark of five ensemble learners (Random\nForest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho\nVerde red- and white-wine datasets (1,599 and 4,898 instances, 11\nphysicochemical attributes). Our leakage-free workflow employs an 80:20\nstratified train-test split, five-fold StratifiedGroupKFold within the training\nset, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost\nweighting, Optuna hyper-parameter search (120-200 trials per model) and a\ntwo-stage feature-selection refit. Final scores on untouched test sets are\nreported with weighted F1 as the headline metric. Gradient Boosting achieves\nthe highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016\nfor white), followed within three percentage points by Random Forest and\nXGBoost. Limiting each model to its five top-ranked variables lowers\ndimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage\npoints for red and 3.0 percentage points for white, indicating that alcohol,\nvolatile acidity, sulphates, free SO2 and chlorides capture most predictive\nsignal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency\ngradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and\nLightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We\ntherefore recommend Random Forest as the most cost-effective production model,\nXGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as\nthe accuracy ceiling for offline benchmarking. The fully documented pipeline\nand metric set provide a reproducible baseline for future work on imbalanced\nmulti-class wine-quality prediction.", "AI": {"tldr": "论文比较了五种集成学习模型在葡萄酒质量评估中的表现，推荐了不同场景下的最优模型。", "motivation": "传统的葡萄酒质量评估依赖主观的人工品鉴，效率低且不可靠，需要更准确、可重复的自动化方法。", "method": "采用泄漏控制的工作流程，包括数据分割、标准化、重采样、超参数搜索和特征选择，评估五种集成学习模型。", "result": "Gradient Boosting准确率最高，但Random Forest性价比最佳；特征选择可显著降低维度且性能损失小。", "conclusion": "推荐Random Forest为生产模型，Gradient Boosting为离线基准，提供了可复现的流程和指标。"}}
{"id": "2506.06612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06612", "abs": "https://arxiv.org/abs/2506.06612", "authors": ["Akshaya Agrawal", "Evan Palmer", "Zachary Kingston", "Geoffrey A. Hollinger"], "title": "Underwater Multi-Robot Simulation and Motion Planning in Angler", "comment": "Accepted for OCEANS 2025 Brest", "summary": "Deploying multi-robot systems in underwater environments is expensive and\nlengthy; testing algorithms and software in simulation improves development by\ndecoupling software and hardware. However, this requires a simulation framework\nthat closely resembles the real-world. Angler is an open-source framework that\nsimulates low-level communication protocols for an onboard autopilot, such as\nArduSub, providing a framework that is close to reality, but unfortunately\nlacking support for simulating multiple robots. We present an extension to\nAngler that supports multi-robot simulation and motion planning. Our extension\nhas a modular architecture that creates non-conflicting communication channels\nbetween Gazebo, ArduSub Software-in-the-Loop (SITL), and MAVROS to operate\nmultiple robots simultaneously in the same environment. Our multi-robot motion\nplanning module interfaces with cascaded controllers via a JointTrajectory\ncontroller in ROS~2. We also provide an integration with the Open Motion\nPlanning Library (OMPL), a collision avoidance module, and tools for procedural\nenvironment generation. Our work enables the development and benchmarking of\nunderwater multi-robot motion planning in dynamic environments.", "AI": {"tldr": "扩展了Angler框架以支持多机器人水下仿真与运动规划，提供模块化架构和工具。", "motivation": "水下多机器人系统部署成本高且耗时，仿真框架需贴近现实以支持算法开发。", "method": "扩展Angler框架，创建非冲突通信通道，集成Gazebo、ArduSub SITL和MAVROS，支持多机器人同时仿真。", "result": "实现了多机器人运动规划模块，集成OMPL、碰撞避免模块及环境生成工具。", "conclusion": "该工作为动态环境下的水下多机器人运动规划开发与测试提供了支持。"}}
{"id": "2506.06931", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.06931", "abs": "https://arxiv.org/abs/2506.06931", "authors": ["Zhe Shen", "Yitaek Kim", "Christoffer Sloth"], "title": "Towards Data-Driven Model-Free Safety-Critical Control", "comment": "submitted to IROS 2025", "summary": "This paper presents a framework for enabling safe velocity control of general\nrobotic systems using data-driven model-free Control Barrier Functions (CBFs).\nModel-free CBFs rely on an exponentially stable velocity controller and a\ndesign parameter (e.g. alpha in CBFs); this design parameter depends on the\nexponential decay rate of the controller. However, in practice, the decay rate\nis often unavailable, making it non-trivial to use model-free CBFs, as it\nrequires manual tuning for alpha. To address this, a Neural Network is used to\nlearn the Lyapunov function from data, and the maximum decay rate of the\nsystems built-in velocity controller is subsequently estimated. Furthermore, to\nintegrate the estimated decay rate with model-free CBFs, we derive a\nprobabilistic safety condition that incorporates a confidence bound on the\nviolation rate of the exponential stability condition, using Chernoff bound.\nThis enhances robustness against uncertainties in stability violations. The\nproposed framework has been tested on a UR5e robot in multiple experimental\nsettings, and its effectiveness in ensuring safe velocity control with\nmodel-free CBFs has been demonstrated.", "AI": {"tldr": "提出了一种基于数据驱动的无模型控制屏障函数（CBFs）框架，用于实现机器人系统的安全速度控制，通过神经网络学习Lyapunov函数并估计速度控制器的最大衰减率，结合概率安全条件增强鲁棒性。", "motivation": "实际应用中，无模型CBFs的设计参数（如alpha）依赖于速度控制器的指数衰减率，但衰减率通常未知，需手动调整，限制了其应用。", "method": "使用神经网络从数据中学习Lyapunov函数，估计速度控制器的最大衰减率，并基于Chernoff边界推导概率安全条件，整合到无模型CBFs中。", "result": "在UR5e机器人上的实验验证了该框架能有效确保无模型CBFs的安全速度控制。", "conclusion": "提出的框架解决了无模型CBFs中衰减率未知的问题，通过数据驱动方法增强了安全性和鲁棒性。"}}
{"id": "2506.07323", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07323", "abs": "https://arxiv.org/abs/2506.07323", "authors": ["Haoyuan Yang", "Yue Zhang", "Liqiang Jing"], "title": "Speech Recognition on TV Series with Video-guided Post-Correction", "comment": null, "summary": "Automatic Speech Recognition (ASR) has achieved remarkable success with deep\nlearning, driving advancements in conversational artificial intelligence, media\ntranscription, and assistive technologies. However, ASR systems still struggle\nin complex environments such as TV series, where overlapping speech,\ndomain-specific terminology, and long-range contextual dependencies pose\nsignificant challenges to transcription accuracy. Existing multimodal\napproaches fail to correct ASR outputs with the rich temporal and contextual\ninformation available in video. To address this limitation, we propose a novel\nmultimodal post-correction framework that refines ASR transcriptions by\nleveraging contextual cues extracted from video. Our framework consists of two\nstages: ASR Generation and Video-based Post-Correction, where the first stage\nproduces the initial transcript and the second stage corrects errors using\nVideo-based Contextual Information Extraction and Context-aware ASR Correction.\nWe employ the Video-Large Multimodal Model (VLMM) to extract key contextual\ninformation using tailored prompts, which is then integrated with a Large\nLanguage Model (LLM) to refine the ASR output. We evaluate our method on a\nmultimodal benchmark for TV series ASR and demonstrate its effectiveness in\nimproving ASR performance by leveraging video-based context to enhance\ntranscription accuracy in complex multimedia environments.", "AI": {"tldr": "提出了一种新颖的多模态后校正框架，通过利用视频中的上下文信息来改进ASR转录准确性。", "motivation": "ASR系统在复杂环境（如电视剧）中表现不佳，现有方法未能充分利用视频中的丰富信息。", "method": "采用两阶段框架：ASR生成和基于视频的后校正，结合VLMM和LLM提取和利用上下文信息。", "result": "在电视剧ASR多模态基准测试中，该方法显著提升了转录准确性。", "conclusion": "视频上下文信息能有效提升ASR在复杂多媒体环境中的性能。"}}
{"id": "2506.07932", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07932", "abs": "https://arxiv.org/abs/2506.07932", "authors": ["Rishit Dagli", "Yushi Guan", "Sankeerth Durvasula", "Mohammadreza Mofayezi", "Nandita Vijaykumar"], "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "comment": null, "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.", "AI": {"tldr": "Squeeze3D是一种利用预训练3D生成模型隐式先验知识实现高压缩比3D数据压缩的新框架。", "motivation": "现有3D数据压缩方法效率不足，Squeeze3D旨在通过预训练模型实现高效压缩。", "method": "通过映射网络连接预训练编码器和生成模型的潜在空间，将3D数据压缩为紧凑潜在代码，再通过生成模型解压缩。", "result": "实验显示Squeeze3D在纹理网格、点云和辐射场上的压缩比分别高达2187x、55x和619x，且视觉质量接近现有方法。", "conclusion": "Squeeze3D无需特定对象训练即可实现高效压缩和解压缩，支持多种3D数据格式。"}}
{"id": "2506.07389", "categories": ["cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.07389", "abs": "https://arxiv.org/abs/2506.07389", "authors": ["Guanming Qiao", "Partha Protim Paul"], "title": "Human Side of Smart Contract Fuzzing: An Empirical Study", "comment": null, "summary": "Smart contract (SC) fuzzing is a critical technique for detecting\nvulnerabilities in blockchain applications. However, its adoption remains\nchallenging for practitioners due to fundamental differences between SCs and\ntraditional software systems. In this study, we investigate the challenges\npractitioners face when adopting SC fuzzing tools by conducting an inductive\ncontent analysis of 381 GitHub issues from two widely used SC fuzzers: Echidna\nand Foundry. Furthermore, we conducted a user study to examine how these\nchallenges affect different practitioner groups, SC developers, and traditional\nsoftware security professionals, and identify strategies practitioners use to\novercome them. We systematically categorize these challenges into a taxonomy\nbased on their nature and occurrence within the SC fuzzing workflow. Our\nfindings reveal domain-specific ease-of-use and usefulness challenges,\nincluding technical issues with blockchain emulation, and human issues with a\nlack of accessible documentation and process automation. Our results provide\nactionable insights for tool developers and researchers, guiding future\nimprovements in SC fuzzer tool design.", "AI": {"tldr": "研究分析了智能合约（SC）模糊测试在实践中的挑战，通过分析GitHub问题和用户研究，揭示了易用性和实用性方面的具体问题，并提出了改进建议。", "motivation": "智能合约模糊测试对区块链应用安全至关重要，但其采用因与传统软件的差异而面临挑战。", "method": "通过分析381个GitHub问题（来自Echidna和Foundry）和用户研究，归纳挑战并分类。", "result": "发现区块链模拟技术问题和文档不足等挑战，影响不同实践者群体。", "conclusion": "研究结果为工具开发者和研究者提供了改进智能合约模糊测试工具的实用指导。"}}
{"id": "2506.06631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06631", "abs": "https://arxiv.org/abs/2506.06631", "authors": ["Minghao Zou", "Qingtian Zeng", "Yongping Miao", "Shangkun Liu", "Zilong Wang", "Hantao Liu", "Wei Zhou"], "title": "PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments", "comment": null, "summary": "Visual parsing of images and videos is critical for a wide range of\nreal-world applications. However, progress in this field is constrained by\nlimitations of existing datasets: (1) insufficient annotation granularity,\nwhich impedes fine-grained scene understanding and high-level reasoning; (2)\nlimited coverage of domains, particularly a lack of datasets tailored for\neducational scenarios; and (3) lack of explicit procedural guidance, with\nminimal logical rules and insufficient representation of structured task\nprocess. To address these gaps, we introduce PhysLab, the first video dataset\nthat captures students conducting complex physics experiments. The dataset\nincludes four representative experiments that feature diverse scientific\ninstruments and rich human-object interaction (HOI) patterns. PhysLab comprises\n620 long-form videos and provides multilevel annotations that support a variety\nof vision tasks, including action recognition, object detection, HOI analysis,\netc. We establish strong baselines and perform extensive evaluations to\nhighlight key challenges in the parsing of procedural educational videos. We\nexpect PhysLab to serve as a valuable resource for advancing fine-grained\nvisual parsing, facilitating intelligent classroom systems, and fostering\ncloser integration between computer vision and educational technologies. The\ndataset and the evaluation toolkit are publicly available at\nhttps://github.com/ZMH-SDUST/PhysLab.", "AI": {"tldr": "PhysLab是一个针对教育场景的视频数据集，专注于复杂物理实验，提供多级注释以支持多种视觉任务。", "motivation": "现有数据集在注释粒度、领域覆盖和程序指导方面存在不足，限制了视觉解析的进展。", "method": "引入PhysLab数据集，包含620个长视频和四类代表性实验，提供多级注释。", "result": "建立了基线并进行了广泛评估，突出了教育视频解析中的关键挑战。", "conclusion": "PhysLab有望推动细粒度视觉解析和智能课堂系统的发展。"}}
{"id": "2506.07526", "categories": ["cs.SD", "cs.NE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07526", "abs": "https://arxiv.org/abs/2506.07526", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy"], "title": "Generative Voice Bursts during Phone Call", "comment": "12 pages, 2 figures", "summary": "In critical situations, conventional mobile telephony fails to convey\nemergency voice messages to a callee already engaged in another call. The\nstandard call waiting alert does not provide the urgency or content of the\nwaiting call. This paper proposes a novel method for transmitting Generative\nVoice Bursts short, context aware audio messages during ongoing calls, from\neither preauthorized or dynamically prioritized callers. By leveraging\ngenerative AI techniques, the system automatically generates spoken messages\nfrom contextual inputs example like location, health data, images, background\nnoise when the caller is unable to speak due to incapacitation or environmental\nconstraints. The solution incorporates voice, text, and priority inference\nmechanisms, allowing high priority emergency messages to bypass conventional\ncall waiting barriers. The approach employs models such as GPT Neo for\ngenerative text, which is synthesized into audio and delivered in configurable\nintervals G seconds and counts N times, ensuring minimal disruption while\npreserving urgency. This method holds potential for significant impact across\ntelecom, mobile device manufacturing, and emergency communication platforms.", "AI": {"tldr": "提出了一种在通话中传输紧急语音消息的新方法，利用生成式AI技术自动生成上下文感知的音频消息。", "motivation": "解决传统移动电话在紧急情况下无法传递紧急语音消息的问题，尤其是当被叫方正在通话时。", "method": "结合生成式AI（如GPT Neo）生成文本并合成音频，通过优先级推断机制绕过传统呼叫等待限制。", "result": "系统能够生成并传递紧急语音消息，最小化通话干扰的同时保持紧急性。", "conclusion": "该方法在电信、移动设备和紧急通信平台中有广泛应用潜力。"}}
{"id": "2506.06330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06330", "abs": "https://arxiv.org/abs/2506.06330", "authors": ["James Afful"], "title": "ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications", "comment": null, "summary": "As machine learning systems are increasingly deployed in high-stakes domains\nsuch as criminal justice, finance, and healthcare, the demand for interpretable\nand trustworthy models has intensified. Despite the proliferation of local\nexplanation techniques, including SHAP, LIME, and counterfactual methods, there\nexists no standardized, reproducible framework for their comparative\nevaluation, particularly in fairness-sensitive settings.\n  We introduce ExplainBench, an open-source benchmarking suite for systematic\nevaluation of local model explanations across ethically consequential datasets.\nExplainBench provides unified wrappers for popular explanation algorithms,\nintegrates end-to-end pipelines for model training and explanation generation,\nand supports evaluation via fidelity, sparsity, and robustness metrics. The\nframework includes a Streamlit-based graphical interface for interactive\nexploration and is packaged as a Python module for seamless integration into\nresearch workflows.\n  We demonstrate ExplainBench on datasets commonly used in fairness research,\nsuch as COMPAS, UCI Adult Income, and LendingClub, and showcase how different\nexplanation methods behave under a shared experimental protocol. By enabling\nreproducible, comparative analysis of local explanations, ExplainBench advances\nthe methodological foundations of interpretable machine learning and\nfacilitates accountability in real-world AI systems.", "AI": {"tldr": "ExplainBench是一个开源基准测试套件，用于系统评估局部模型解释方法，特别是在公平敏感场景中。", "motivation": "随着机器学习系统在高风险领域的广泛应用，对可解释和可信赖模型的需求增加，但缺乏标准化、可复现的评估框架。", "method": "ExplainBench提供统一的解释算法封装、端到端模型训练和解释生成流程，支持通过保真度、稀疏性和鲁棒性指标进行评估。", "result": "在COMPAS、UCI Adult Income和LendingClub等数据集上展示了不同解释方法的行为。", "conclusion": "ExplainBench通过可复现的比较分析，推动了可解释机器学习的方法学基础，并提升了实际AI系统的问责性。"}}
{"id": "2506.06624", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06624", "abs": "https://arxiv.org/abs/2506.06624", "authors": ["Mojtaba Mollahossein", "Farshad Haghgoo Daryakenari", "Mohammad Hossein Rohban", "Gholamreza Vossoughi"], "title": "Attention-Based Convolutional Neural Network Model for Human Lower Limb Activity Recognition using sEMG", "comment": "6 pages, 3 figures", "summary": "Accurate classification of lower limb movements using surface\nelectromyography (sEMG) signals plays a crucial role in assistive robotics and\nrehabilitation systems. In this study, we present a lightweight attention-based\ndeep neural network (DNN) for real-time movement classification using\nmulti-channel sEMG data from the publicly available BASAN dataset. The proposed\nmodel consists of only 62,876 parameters and is designed without the need for\ncomputationally expensive preprocessing, making it suitable for real-time\ndeployment. We employed a leave-oneout validation strategy to ensure\ngeneralizability across subjects, and evaluated the model on three movement\nclasses: walking, standing with knee flexion, and sitting with knee extension.\nThe network achieved 86.74% accuracy on the validation set and 85.38% on the\ntest set, demonstrating strong classification performance under realistic\nconditions. Comparative analysis with existing models in the literature\nhighlights the efficiency and effectiveness of our approach, especially in\nscenarios where computational cost and real-time response are critical. The\nresults indicate that the proposed model is a promising candidate for\nintegration into upper-level controllers in human-robot interaction systems.", "AI": {"tldr": "提出了一种轻量级注意力深度神经网络（DNN），用于实时分类下肢运动，基于多通道sEMG数据，模型参数少且无需复杂预处理，验证性能优异。", "motivation": "下肢运动分类在辅助机器人和康复系统中至关重要，现有方法计算成本高，需轻量级实时解决方案。", "method": "使用轻量级注意力DNN处理多通道sEMG数据，无需复杂预处理，采用留一法验证。", "result": "模型在三类运动中验证集准确率86.74%，测试集85.38%，计算效率优于现有方法。", "conclusion": "该模型适合集成到人机交互系统中，尤其在实时性和计算成本敏感场景中表现突出。"}}
{"id": "2506.07068", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07068", "abs": "https://arxiv.org/abs/2506.07068", "authors": ["Hassan Alnahhal", "Sifeddine Benahmed", "Soulaimane Berkane", "Tarel Hamel"], "title": "Attitude Estimation Using Scalar Measurements", "comment": "6 pages", "summary": "This paper revisits the problem of orientation estimation for rigid bodies\nthrough a novel framework based on scalar measurements. Unlike traditional\nvector-based methods, the proposed approach enables selective utilization of\nonly the reliable axes of vector measurements while seamlessly incorporating\nalternative scalar modalities such as Pitot tubes, barometers with range\nsensors, and landmark-based constraints. The estimation problem is reformulated\nwithin a linear time-varying (LTV) framework, allowing the application of a\ndeterministic linear Kalman filter. This design guarantees Global Uniform\nExponential Stability (GES) under the Uniform Observability (UO) condition.\nSimulation results demonstrate the effectiveness of the proposed approach in\nachieving robust and accurate attitude estimation, even with partial vector\nmeasurements that simulate sensor axis failure.", "AI": {"tldr": "本文提出了一种基于标量测量的新框架，用于刚体姿态估计，通过选择性利用可靠轴和整合其他标量模态，实现了鲁棒且准确的估计。", "motivation": "传统基于矢量的方法在传感器轴失效时表现不佳，因此需要一种能够灵活利用标量测量并保证稳定性的新方法。", "method": "将估计问题重新表述为线性时变（LTV）框架，并应用确定性线性卡尔曼滤波器，确保在均匀可观测性（UO）条件下的全局一致指数稳定性（GES）。", "result": "仿真结果表明，即使在模拟传感器轴失效的部分矢量测量情况下，该方法仍能实现鲁棒且准确的姿态估计。", "conclusion": "该框架为姿态估计提供了一种灵活且稳定的解决方案，尤其适用于传感器部分失效的场景。"}}
{"id": "2506.07358", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07358", "abs": "https://arxiv.org/abs/2506.07358", "authors": ["Kuiyuan Zhang", "Wenjie Pei", "Rushi Lan", "Yifang Guo", "Zhongyun Hua"], "title": "Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework", "comment": null, "summary": "Deepfakes are AI-synthesized multimedia data that may be abused for spreading\nmisinformation. Deepfake generation involves both visual and audio\nmanipulation. To detect audio-visual deepfakes, previous studies commonly\nemploy two relatively independent sub-models to learn audio and visual\nfeatures, respectively, and fuse them subsequently for deepfake detection.\nHowever, this may underutilize the inherent correlations between audio and\nvisual features. Moreover, utilizing two isolated feature learning sub-models\ncan result in redundant neural layers, making the overall model inefficient and\nimpractical for resource-constrained environments.\n  In this work, we design a lightweight network for audio-visual deepfake\ndetection via a single-stream multi-modal learning framework. Specifically, we\nintroduce a collaborative audio-visual learning block to efficiently integrate\nmulti-modal information while learning the visual and audio features. By\niteratively employing this block, our single-stream network achieves a\ncontinuous fusion of multi-modal features across its layers. Thus, our network\nefficiently captures visual and audio features without the need for excessive\nblock stacking, resulting in a lightweight network design. Furthermore, we\npropose a multi-modal classification module that can boost the dependence of\nthe visual and audio classifiers on modality content. It also enhances the\nwhole resistance of the video classifier against the mismatches between audio\nand visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and\nDFDC benchmark datasets. Compared to state-of-the-art audio-visual joint\ndetection methods, our method is significantly lightweight with only 0.48M\nparameters, yet it achieves superiority in both uni-modal and multi-modal\ndeepfakes, as well as in unseen types of deepfakes.", "AI": {"tldr": "提出了一种轻量级单流多模态学习框架，用于音频-视觉深度伪造检测，通过协作学习块和多模态分类模块提升性能。", "motivation": "现有方法通常独立学习音频和视觉特征，未能充分利用其相关性，且模型冗余，效率低下。", "method": "设计协作音频-视觉学习块实现多模态特征融合，提出多模态分类模块增强模态依赖性。", "result": "在多个基准数据集上表现优异，模型仅0.48M参数，优于现有方法。", "conclusion": "该方法轻量高效，能有效检测多模态及未知类型深度伪造。"}}
{"id": "2506.07781", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.07781", "abs": "https://arxiv.org/abs/2506.07781", "authors": ["Mart Kartašev", "David Dörner", "Özer Özkahraman", "Petter Ögren", "Ivan Stenius", "John Folkesson"], "title": "SMaRCSim: Maritime Robotics Simulation Modules", "comment": null, "summary": "Developing new functionality for underwater robots and testing them in the\nreal world is time-consuming and resource-intensive. Simulation environments\nallow for rapid testing before field deployment. However, existing tools lack\ncertain functionality for use cases in our project: i) developing\nlearning-based methods for underwater vehicles; ii) creating teams of\nautonomous underwater, surface, and aerial vehicles; iii) integrating the\nsimulation with mission planning for field experiments. A holistic solution to\nthese problems presents great potential for bringing novel functionality into\nthe underwater domain. In this paper we present SMaRCSim, a set of simulation\npackages that we have developed to help us address these issues.", "AI": {"tldr": "SMaRCSim是一个模拟工具包，旨在解决水下机器人开发中的快速测试和团队协作问题。", "motivation": "现有工具无法满足学习型方法开发、多类型自主载具团队协作及与任务规划集成的需求。", "method": "开发了SMaRCSim模拟工具包，支持水下、水面和空中载具的团队协作及任务规划集成。", "result": "SMaRCSim为水下领域提供了快速测试和功能开发的新工具。", "conclusion": "SMaRCSim填补了现有工具的空白，为水下机器人开发提供了高效解决方案。"}}
{"id": "2506.07393", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07393", "abs": "https://arxiv.org/abs/2506.07393", "authors": ["Anna Yokokubo", "Takeo Hamada", "Tatsuya Ishizuka", "Hiroaki Mori", "Noboru Koshizuka"], "title": "Happiness Finder: Exploring the Role of AI in Enhancing Well-Being During Four-Leaf Clover Searches", "comment": null, "summary": "A four-leaf clover (FLC) symbolizes luck and happiness worldwide, but it is\nhard to distinguish it from the common three-leaf clover. While AI technology\ncan assist in searching for FLC, it may not replicate the traditional search's\nsense of achievement. This study explores searcher feelings when AI aids the\nFLC search. In this study, we developed a system called ``Happiness Finder''\nthat uses object detection algorithms on smartphones or tablets to support the\nsearch. We exhibited HappinessFinder at an international workshop, allowing\nparticipants to experience four-leaf clover searching using potted artificial\nclovers and the HappinessFinder app. This paper reports the findings from this\ndemonstration.", "AI": {"tldr": "研究探讨了AI辅助寻找四叶草时用户的感受，开发了名为“Happiness Finder”的系统，并展示了其效果。", "motivation": "四叶草象征幸运，但传统搜索与AI辅助搜索的成就感不同，研究旨在探索用户感受。", "method": "开发基于智能手机的“Happiness Finder”系统，使用对象检测算法，并在国际工作坊中展示。", "result": "通过人工盆栽和应用程序演示，收集了用户使用AI辅助搜索四叶草的反馈。", "conclusion": "研究揭示了AI辅助搜索与传统搜索的情感差异，为未来技术设计提供参考。"}}
{"id": "2506.06643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06643", "abs": "https://arxiv.org/abs/2506.06643", "authors": ["Moushumi Medhi", "Rajiv Ranjan Sahay"], "title": "Dark Channel-Assisted Depth-from-Defocus from a Single Image", "comment": null, "summary": "In this paper, we utilize the dark channel as a complementary cue to estimate\nthe depth of a scene from a single space-variant defocus blurred image due to\nits effectiveness in implicitly capturing the local statistics of blurred\nimages and the scene structure. Existing depth-from-defocus (DFD) techniques\ntypically rely on multiple images with varying apertures or focus settings to\nrecover depth information. Very few attempts have focused on DFD from a single\ndefocused image due to the underconstrained nature of the problem. Our method\ncapitalizes on the relationship between local defocus blur and contrast\nvariations as key depth cues to enhance the overall performance in estimating\nthe scene's structure. The entire pipeline is trained adversarially in a fully\nend-to-end fashion. Experiments conducted on real data with realistic\ndepth-induced defocus blur demonstrate that incorporating dark channel prior\ninto single image DFD yields meaningful depth estimation results, validating\nthe effectiveness of our approach.", "AI": {"tldr": "利用暗通道作为补充线索，从单张空间变异散焦模糊图像中估计场景深度，通过对抗性训练实现端到端深度估计。", "motivation": "现有深度估计方法通常依赖多张图像，而单张散焦图像的深度估计问题尚未充分解决。本文旨在通过暗通道先验和局部模糊与对比度变化的关系，提升单张图像的深度估计性能。", "method": "利用暗通道先验捕捉模糊图像的局部统计信息，结合局部散焦模糊与对比度变化的关系，通过对抗性训练实现端到端深度估计。", "result": "在真实数据上的实验表明，暗通道先验的引入显著提升了单张散焦图像的深度估计效果。", "conclusion": "暗通道先验为单张散焦图像的深度估计提供了有效补充，验证了该方法的实用性。"}}
{"id": "2506.06333", "categories": ["cs.LG", "cs.FL"], "pdf": "https://arxiv.org/pdf/2506.06333", "abs": "https://arxiv.org/abs/2506.06333", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "AI": {"tldr": "AALpy是一个Python开源自动机学习库，专注于IO行为系统的主动学习，新增了被动学习中的状态合并方法实现。", "motivation": "提供通用且高度可配置的状态合并框架实现，简化算法开发。", "method": "使用通用内部表示实现红蓝框架的状态合并算法，兼容性标准和评分由用户定义。", "result": "显著减少现有和新型状态合并算法的实现代码量。", "conclusion": "AALpy为自动机学习提供了高效且灵活的工具，尤其适合状态合并算法的快速实现。"}}
{"id": "2506.06630", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06630", "abs": "https://arxiv.org/abs/2506.06630", "authors": ["Heeju Ko", "Sungjune Kim", "Gyeongrok Oh", "Jeongyoon Yoon", "Honglak Lee", "Sujin Jang", "Seungryong Kim", "Sangpil Kim"], "title": "Active Test-time Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation (VLN) policies trained on offline datasets often\nexhibit degraded task performance when deployed in unfamiliar navigation\nenvironments at test time, where agents are typically evaluated without access\nto external interaction or feedback. Entropy minimization has emerged as a\npractical solution for reducing prediction uncertainty at test time; however,\nit can suffer from accumulated errors, as agents may become overconfident in\nincorrect actions without sufficient contextual grounding. To tackle these\nchallenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time\nactive learning framework that enables a practical human-robot interaction via\nepisodic feedback on uncertain navigation outcomes. In particular, ATENA learns\nto increase certainty in successful episodes and decrease it in failed ones,\nimproving uncertainty calibration. Here, we propose mixture entropy\noptimization, where entropy is obtained from a combination of the action and\npseudo-expert distributions-a hypothetical action distribution assuming the\nagent's selected action to be optimal-controlling both prediction confidence\nand action preference. In addition, we propose a self-active learning strategy\nthat enables an agent to evaluate its navigation outcomes based on confident\npredictions. As a result, the agent stays actively engaged throughout all\niterations, leading to well-grounded and adaptive decision-making. Extensive\nevaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate\nthat ATENA successfully overcomes distributional shifts at test time,\noutperforming the compared baseline methods across various settings.", "AI": {"tldr": "ATENA是一种测试时主动学习框架，通过人类-机器人交互优化导航策略，减少预测不确定性并提升任务性能。", "motivation": "离线训练的视觉语言导航策略在陌生环境中性能下降，传统熵最小化方法可能因错误累积而失效。", "method": "提出混合熵优化和自我主动学习策略，结合动作和伪专家分布优化不确定性校准。", "result": "在多个VLN基准测试中表现优异，成功克服测试时的分布偏移。", "conclusion": "ATENA通过主动学习和不确定性校准，显著提升了导航策略的适应性和性能。"}}
{"id": "2506.07079", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07079", "abs": "https://arxiv.org/abs/2506.07079", "authors": ["Mostafa Eslami", "Maryam Babazadeh"], "title": "On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)", "comment": "This paper presents an early investigation of Data-Assisted Control\n  (DAC) with reinforcement learning, showcasing its potential through a simple\n  example. Theoretical analysis is ongoing to establish formal support and\n  guarantees for the proposed approach", "summary": "This paper introduces a hypothetical hybrid control framework for\nport-Hamiltonian (p$\\mathcal{H}$) systems, employing a dynamic decomposition\nbased on Data-Assisted Control (DAC). The system's evolution is split into two\nparts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow\nhandling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a\ndissipative/input flow addressing both structural and parametric uncertainties.\nA virtual port variable $\\Pi$ serves as the interface between these two\ncomponents. A nonlinear controller manages the intrinsic Hamiltonian flow,\ndetermining a desired port control value $\\Pi_c$. Concurrently, Reinforcement\nLearning (RL) is applied to the dissipative/input flow to learn an agent for\nproviding optimal policy in mapping $\\Pi_c$ to the actual system input. This\nhybrid approach effectively manages RHS uncertainties while preserving the\nsystem's inherent structure. Key advantages include adjustable performance via\nLHS controller parameters, enhanced AI explainability and interpretability\nthrough the port variable $\\Pi$, the ability to guarantee safety and state\nattainability with hard/soft constraints, reduced complexity in learning\nhypothesis classes compared to end-to-end solutions, and improved\nstate/parameter estimation using LHS prior knowledge and system Hamiltonian to\naddress partial observability. The paper details the p$\\mathcal{H}$\nformulation, derives the decomposition, and presents the modular controller\narchitecture. Beyond design, crucial aspects of stability and robustness\nanalysis and synthesis are investigated, paving the way for deeper theoretical\ninvestigations. An application example, a pendulum with nonlinear dynamics, is\nsimulated to demonstrate the approach's empirical and phenomenological benefits\nfor future research.", "AI": {"tldr": "本文提出了一种基于数据辅助控制（DAC）的混合控制框架，用于端口哈密顿（pℋ）系统，通过动态分解实现鲁棒性和灵活性。", "motivation": "解决端口哈密顿系统中参数和结构不确定性的管理问题，同时保持系统的固有结构。", "method": "将系统动态分解为右半部分（RHS，处理参数不确定性）和左半部分（LHS，处理结构和参数不确定性），并通过虚拟端口变量Π连接。RHS由非线性控制器管理，LHS通过强化学习（RL）优化策略。", "result": "该框架有效管理不确定性，保持系统结构，并提供可调性能、增强AI可解释性、安全性保证和简化学习复杂度等优势。", "conclusion": "通过仿真验证了方法的有效性，为未来理论研究和实际应用奠定了基础。"}}
{"id": "2506.07473", "categories": ["cs.SD", "eess.AS", "00A65", "J.5"], "pdf": "https://arxiv.org/pdf/2506.07473", "abs": "https://arxiv.org/abs/2506.07473", "authors": ["Emmanuel Deruty"], "title": "An introduction to pitch strength in contemporary popular music analysis and production", "comment": "In Music 2024, Innovation in Music Conference, 14-16 June, 2024,\n  Kristiania University College, Oslo, Norway", "summary": "Music information retrieval distinguishes between low- and high-level\ndescriptions of music. Current generative AI models rely on text descriptions\nthat are higher level than the controls familiar to studio musicians. Pitch\nstrength, a low-level perceptual parameter of contemporary popular music, may\nbe one feature that could make such AI models more suited to music production.\nSignal and perceptual analyses suggest that pitch strength (1) varies\nsignificantly across and inside songs; (2) contributes to both small- and\nlarge-scale structure; (3) contributes to the handling of polyphonic\ndissonance; and (4) may be a feature of upper harmonics made audible in a\nperspective of perceptual richness.", "AI": {"tldr": "论文探讨了音乐信息检索中高低层次描述的差异，提出音高强度作为低层次特征可能提升生成式AI模型在音乐制作中的适用性。", "motivation": "当前生成式AI模型依赖高层次文本描述，而音乐制作需要更接近实际音乐控制的低层次特征，音高强度可能填补这一需求。", "method": "通过信号和感知分析，研究了音高强度在歌曲内外的变化及其对音乐结构的影响。", "result": "音高强度在歌曲内外差异显著，影响音乐大小结构，处理复调不和谐音，并可能与听觉感知丰富性相关。", "conclusion": "音高强度作为低层次特征，有望提升生成式AI模型在音乐制作中的实用性。"}}
{"id": "2506.07707", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07707", "abs": "https://arxiv.org/abs/2506.07707", "authors": ["Maryam Teimouri", "Filip Ginter", "Tomi \"bgt\" Suovuo"], "title": "Interaction Analysis by Humans and AI: A Comparative Perspective", "comment": null, "summary": "This paper explores how Mixed Reality (MR) and 2D video conferencing\ninfluence children's communication during a gesture-based guessing game.\nFinnish-speaking participants engaged in a short collaborative task using two\ndifferent setups: Microsoft HoloLens MR and Zoom. Audio-video recordings were\ntranscribed and analyzed using Large Language Models (LLMs), enabling iterative\ncorrection, translation, and annotation. Despite limitations in annotations'\naccuracy and agreement, automated approaches significantly reduced processing\ntime and allowed non-Finnish-speaking researchers to participate in data\nanalysis. Evaluations highlight both the efficiency and constraints of\nLLM-based analyses for capturing children's interactions across these\nplatforms. Initial findings indicate that MR fosters richer interaction,\nevidenced by higher emotional expression during annotation, and heightened\nengagement, while Zoom offers simplicity and accessibility. This study\nunderscores the potential of MR to enhance collaborative learning experiences\nfor children in distributed settings.", "AI": {"tldr": "研究比较了混合现实（MR）和2D视频会议（Zoom）对儿童在手势猜谜游戏中沟通的影响，发现MR能促进更丰富的情感表达和参与度，而Zoom更简单易用。", "motivation": "探讨MR和2D视频会议在儿童协作任务中的沟通效果差异，为分布式环境中的协作学习提供优化方向。", "method": "使用Microsoft HoloLens MR和Zoom进行实验，通过LLM分析音频视频数据，实现快速处理和跨语言研究参与。", "result": "MR在情感表达和参与度上表现更优，Zoom则更简单易用；LLM分析显著提升效率，但存在标注准确性限制。", "conclusion": "MR有潜力提升分布式环境中儿童的协作学习体验，LLM分析为跨语言研究提供了高效工具。"}}
{"id": "2506.06645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06645", "abs": "https://arxiv.org/abs/2506.06645", "authors": ["Cheng Peng", "Jingxiang Sun", "Yushuo Chen", "Zhaoqi Su", "Zhuo Su", "Yebin Liu"], "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling", "comment": "Project Page: https://pengc02.github.io/pghm/", "summary": "Photorealistic and animatable human avatars are a key enabler for\nvirtual/augmented reality, telepresence, and digital entertainment. While\nrecent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering\nquality and efficiency, existing methods still face fundamental challenges,\nincluding time-consuming per-subject optimization and poor generalization under\nsparse monocular inputs. In this work, we present the Parametric Gaussian Human\nModel (PGHM), a generalizable and efficient framework that integrates human\npriors into 3DGS for fast and high-fidelity avatar reconstruction from\nmonocular videos. PGHM introduces two core components: (1) a UV-aligned latent\nidentity map that compactly encodes subject-specific geometry and appearance\ninto a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that\npredicts Gaussian attributes by decomposing static, pose-dependent, and\nview-dependent components via conditioned decoders. This design enables robust\nrendering quality under challenging poses and viewpoints, while allowing\nefficient subject adaptation without requiring multi-view capture or long\noptimization time. Experiments show that PGHM is significantly more efficient\nthan optimization-from-scratch methods, requiring only approximately 20 minutes\nper subject to produce avatars with comparable visual quality, thereby\ndemonstrating its practical applicability for real-world monocular avatar\ncreation.", "AI": {"tldr": "PGHM是一种基于3D高斯泼溅的通用框架，通过引入UV对齐的潜在身份图和分离的多头U-Net，实现了从单目视频中快速重建高保真人类化身。", "motivation": "解决现有3D高斯泼溅方法在稀疏单目输入下泛化能力差和优化时间长的问题。", "method": "PGHM包含UV对齐的潜在身份图编码几何和外观，以及分离的多头U-Net预测高斯属性。", "result": "PGHM仅需约20分钟即可生成视觉质量相当的化身，效率显著优于从头优化方法。", "conclusion": "PGHM为实际单目化身创建提供了高效且高质量的解决方案。"}}
{"id": "2506.06337", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06337", "abs": "https://arxiv.org/abs/2506.06337", "authors": ["Ali Murad", "Bo Hui", "Wei-Shinn Ku"], "title": "Optimized Local Updates in Federated Learning via Reinforcement Learning", "comment": "This paper is accepted at IEEE IJCNN 2025", "summary": "Federated Learning (FL) is a distributed framework for collaborative model\ntraining over large-scale distributed data, enabling higher performance while\nmaintaining client data privacy. However, the nature of model aggregation at\nthe centralized server can result in a performance drop in the presence of\nnon-IID data across different clients. We remark that training a client locally\non more data than necessary does not benefit the overall performance of all\nclients. In this paper, we devise a novel framework that leverages a Deep\nReinforcement Learning (DRL) agent to select an optimized amount of data\nnecessary to train a client model without oversharing information with the\nserver. Starting without awareness of the client's performance, the DRL agent\nutilizes the change in training loss as a reward signal and learns to optimize\nthe amount of training data necessary for improving the client's performance.\nSpecifically, after each aggregation round, the DRL algorithm considers the\nlocal performance as the current state and outputs the optimized weights for\neach class, in the training data, to be used during the next round of local\ntraining. In doing so, the agent learns a policy that creates an optimized\npartition of the local training dataset during the FL rounds. After FL, the\nclient utilizes the entire local training dataset to further enhance its\nperformance on its own data distribution, mitigating the non-IID effects of\naggregation. Through extensive experiments, we demonstrate that training FL\nclients through our algorithm results in superior performance on multiple\nbenchmark datasets and FL frameworks. Our code is available at\nhttps://github.com/amuraddd/optimized_client_training.git.", "AI": {"tldr": "提出了一种基于深度强化学习（DRL）的联邦学习框架，优化客户端训练数据量，提升性能并减少非独立同分布（non-IID）数据的影响。", "motivation": "解决联邦学习中因非独立同分布数据导致的性能下降问题，避免客户端过度共享数据。", "method": "利用DRL代理动态选择客户端训练数据量，以训练损失变化为奖励信号，优化每轮训练的类别权重。", "result": "在多个基准数据集和联邦学习框架上表现出优越性能。", "conclusion": "该框架有效提升了联邦学习在非独立同分布数据下的性能，同时保护了数据隐私。"}}
{"id": "2506.06658", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06658", "abs": "https://arxiv.org/abs/2506.06658", "authors": ["Calvin Luo", "Zilai Zeng", "Mingxi Jia", "Yilun Du", "Chen Sun"], "title": "Self-Adapting Improvement Loops for Robotic Learning", "comment": null, "summary": "Video generative models trained on expert demonstrations have been utilized\nas performant text-conditioned visual planners for solving robotic tasks.\nHowever, generalization to unseen tasks remains a challenge. Whereas improved\ngeneralization may be facilitated by leveraging learned prior knowledge from\nadditional pre-collected offline data sources, such as web-scale video\ndatasets, in the era of experience we aim to design agents that can\ncontinuously improve in an online manner from self-collected behaviors. In this\nwork we thus propose the Self-Adapting Improvement Loop (SAIL), where an\nin-domain video model iteratively updates itself on self-produced trajectories,\ncollected through adaptation with an internet-scale pretrained video model, and\nsteadily improves its performance for a specified task of interest. We apply\nSAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks\non a real robot arm, and find that performance improvements continuously emerge\nover multiple iterations for novel tasks initially unseen during original\nin-domain video model training. Furthermore, we discover that SAIL is\nsurprisingly robust regarding if and how the self-collected experience is\nfiltered, and the quality of the initial in-domain demonstrations. Through\nadaptation with summarized internet-scale data, and learning through online\nexperience, we thus demonstrate a way to iteratively bootstrap a\nhigh-performance video model for solving novel robotic tasks through\nself-improvement.", "AI": {"tldr": "论文提出了一种自我适应改进循环（SAIL），通过结合互联网规模预训练视频模型和自我收集的行为数据，持续提升视频生成模型在新任务中的性能。", "motivation": "解决视频生成模型在未见任务上泛化能力不足的问题，并探索如何通过在线自我改进提升性能。", "method": "提出SAIL框架，利用互联网规模预训练视频模型和自收集轨迹数据迭代更新模型。", "result": "在MetaWorld任务和真实机器人任务中，SAIL持续提升性能，且对数据过滤和初始演示质量具有鲁棒性。", "conclusion": "SAIL通过在线自我改进和互联网规模数据适应，为机器人任务提供了一种高效视频模型提升方法。"}}
{"id": "2506.07102", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07102", "abs": "https://arxiv.org/abs/2506.07102", "authors": ["Wei Huo", "Changxin Liu", "Kemi Ding", "Karl Henrik Johansson", "Ling Shi"], "title": "Decentralized Optimization with Amplified Privacy via Efficient Communication", "comment": null, "summary": "Decentralized optimization is crucial for multi-agent systems, with\nsignificant concerns about communication efficiency and privacy. This paper\nexplores the role of efficient communication in decentralized stochastic\ngradient descent algorithms for enhancing privacy preservation. We develop a\nnovel algorithm that incorporates two key features: random agent activation and\nsparsified communication. Utilizing differential privacy, we demonstrate that\nthese features reduce noise without sacrificing privacy, thereby amplifying the\nprivacy guarantee and improving accuracy. Additionally, we analyze the\nconvergence and the privacy-accuracy-communication trade-off of the proposed\nalgorithm. Finally, we present experimental results to illustrate the\neffectiveness of our algorithm.", "AI": {"tldr": "本文提出了一种结合随机代理激活和稀疏通信的去中心化随机梯度下降算法，通过差分隐私增强隐私保护，同时提高准确性。", "motivation": "多智能体系统中的去中心化优化对通信效率和隐私保护有重要需求。", "method": "开发了一种新算法，结合随机代理激活和稀疏通信，利用差分隐私减少噪声。", "result": "算法在保证隐私的同时提高了准确性，并分析了收敛性和隐私-准确性-通信的权衡。", "conclusion": "实验证明了算法的有效性，展示了其在隐私保护和通信效率上的优势。"}}
{"id": "2506.07494", "categories": ["cs.SD", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07494", "abs": "https://arxiv.org/abs/2506.07494", "authors": ["Peng Huang", "Imdad Ullah", "Xiaotong Wei", "Tariq Ahamed Ahanger", "Najm Hassan", "Zawar Hussain Shah"], "title": "Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A Proposal for Offline Speech Recognition and IoT Integration", "comment": null, "summary": "The smart home systems, based on AI speech recognition and IoT technology,\nenable people to control devices through verbal commands and make people's\nlives more efficient. However, existing AI speech recognition services are\nprimarily deployed on cloud platforms on the Internet. When users issue a\ncommand, speech recognition devices like ``Amazon Echo'' will post a recording\nthrough numerous network nodes, reach multiple servers, and then receive\nresponses through the Internet. This mechanism presents several issues,\nincluding unnecessary energy consumption, communication latency, and the risk\nof a single-point failure. In this position paper, we propose a smart home\nconcept based on offline speech recognition and IoT technology: 1) integrating\noffline keyword spotting (KWS) technologies into household appliances with\nlimited resource hardware to enable them to understand user voice commands; 2)\ndesigning a local IoT network with decentralized architecture to manage and\nconnect various devices, enhancing the robustness and scalability of the\nsystem. This proposal of a smart home based on offline speech recognition and\nIoT technology will allow users to use low-latency voice control anywhere in\nthe home without depending on the Internet and provide better scalability and\nenergy sustainability.", "AI": {"tldr": "论文提出了一种基于离线语音识别和物联网技术的智能家居系统，解决了现有云平台部署的语音识别服务在能耗、延迟和单点故障方面的问题。", "motivation": "现有智能家居系统依赖云平台的语音识别服务，存在能耗高、通信延迟和单点故障风险。", "method": "1) 在资源有限的硬件设备中集成离线关键词检测技术；2) 设计去中心化的本地物联网网络。", "result": "系统实现了低延迟的离线语音控制，提升了鲁棒性和可扩展性。", "conclusion": "离线语音识别与物联网技术结合的智能家居方案更具可持续性和实用性。"}}
{"id": "2506.07777", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07777", "abs": "https://arxiv.org/abs/2506.07777", "authors": ["Brandon Lyman", "Yichi Zhang", "Celia Pearce", "Miso Kim", "Casper Harteveld", "Leanne Chukoskie", "Bob De Schutter"], "title": "Supporting Aging Well through Accessible Digital Games: The Supplemental Role of AI in Game Design for Older Adults", "comment": "21 pages, 1 figure", "summary": "As the population continues to age, and gaming continues to grow as a hobby\nfor older people, heterogeneity among older adult gamers is increasing. We\nargue that traditional game-based accessibility features, such as simplified\ninput schemes, redundant information channels, and increased legibility of\ndigital user interfaces, are increasingly limited in the face of this\nheterogeneity. This is because such features affect all older adult players\nsimultaneously and therefore are designed generically. We introduce artificial\nintelligence, although it has its own limitations and ethical concerns, as a\nmethod of creating player-based accessibility features, given the adaptive\nnature of the emerging technology. These accessibility features may help to\naddress unique assemblage of accessibility needs an individual may accumulate\nthrough age. We adopt insights from gerontology, HCI, and disability studies\ninto the digital game design discourse for older adults, and we contribute\ninsight that can guide the integration of player-based accessibility features\nto supplement game-based counterparts. The accessibility of digital games for\nheterogenous older adult audience is paramount, as the medium offers short-term\nsocial, emotional, psychological, cognitive, and physical that support the\nlong-term goal of aging well.", "AI": {"tldr": "论文探讨了老龄化人口中游戏玩家的多样性，指出传统游戏无障碍功能的局限性，并提出利用人工智能设计个性化无障碍功能以应对这一挑战。", "motivation": "随着老龄化加剧和老年人游戏爱好者的增加，传统无障碍功能无法满足多样化的需求，亟需新的解决方案。", "method": "引入人工智能技术，结合老年学、人机交互和残疾研究的理论，设计个性化的游戏无障碍功能。", "result": "提出了一种基于人工智能的个性化无障碍功能框架，以补充传统功能，提升老年玩家的游戏体验。", "conclusion": "个性化无障碍功能对满足老年玩家的多样化需求至关重要，有助于实现健康老龄化的长期目标。"}}
{"id": "2506.06667", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.06667", "abs": "https://arxiv.org/abs/2506.06667", "authors": ["Yu-Hsuan Ho", "Ali Mostafavi"], "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery", "comment": null, "summary": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.", "AI": {"tldr": "Flood-DamageSense是一种专为洪水灾害建筑损坏评估设计的深度学习框架，融合多源数据，显著提升分类性能。", "motivation": "现有模型在洪水灾害后建筑损坏分类中表现不佳，因破坏特征不明显。", "method": "结合SAR/InSAR、高分辨率光学底图和洪水风险层，采用多模态Mamba架构预测损坏状态、洪水范围和建筑轮廓。", "result": "在Hurricane Harvey数据上，F1分数比现有方法提升19个百分点，尤其在轻微和中度损坏分类中表现突出。", "conclusion": "Flood-DamageSense通过风险感知建模和SAR全天候能力，提供更快、更精细的洪水损坏情报。"}}
{"id": "2506.06359", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06359", "abs": "https://arxiv.org/abs/2506.06359", "authors": ["Gabriel Antonesi", "Tudor Cioara", "Ionut Anghel", "Vasilis Michalakopoulos", "Elissaios Sarmas", "Liana Toderean"], "title": "From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins", "comment": null, "summary": "Artificial intelligence (AI) has long promised to improve energy management\nin smart grids by enhancing situational awareness and supporting more effective\ndecision-making. While traditional machine learning has demonstrated notable\nresults in forecasting and optimization, it often struggles with\ngeneralization, situational awareness, and heterogeneous data integration.\nRecent advances in foundation models such as Transformer architecture and Large\nLanguage Models (LLMs) have demonstrated improved capabilities in modelling\ncomplex temporal and contextual relationships, as well as in multi-modal data\nfusion which is essential for most AI applications in the energy sector. In\nthis review we synthesize the rapid expanding field of AI applications in the\nenergy domain focusing on Transformers and LLMs. We examine the architectural\nfoundations, domain-specific adaptations and practical implementations of\ntransformer models across various forecasting and grid management tasks. We\nthen explore the emerging role of LLMs in the field: adaptation and fine tuning\nfor the energy sector, the type of tasks they are suited for, and the new\nchallenges they introduce. Along the way, we highlight practical\nimplementations, innovations, and areas where the research frontier is rapidly\nexpanding. These recent developments reviewed underscore a broader trend:\nGenerative AI (GenAI) is beginning to augment decision-making not only in\nhigh-level planning but also in day-to-day operations, from forecasting and\ngrid balancing to workforce training and asset onboarding. Building on these\ndevelopments, we introduce the concept of the Agentic Digital Twin, a\nnext-generation model that integrates LLMs to bring autonomy, proactivity, and\nsocial interaction into digital twin-based energy management systems.", "AI": {"tldr": "该综述探讨了AI在能源领域的应用，重点分析了Transformer和LLMs在智能电网中的潜力与挑战，并提出了Agentic Digital Twin的概念。", "motivation": "传统机器学习在能源管理中面临泛化能力和异构数据整合的局限性，而Transformer和LLMs展示了更强的复杂关系建模能力，推动了AI在能源领域的进一步应用。", "method": "综述了Transformer和LLMs的架构基础、领域适应性及实际应用，并探讨了LLMs在能源任务中的适应性和新挑战。", "result": "生成式AI（GenAI）已开始在高层次规划和日常运营中增强决策能力，并提出了Agentic Digital Twin这一下一代模型概念。", "conclusion": "Transformer和LLMs为能源管理带来了新的可能性，Agentic Digital Twin的提出标志着AI在能源领域的进一步创新。"}}
{"id": "2506.06659", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06659", "abs": "https://arxiv.org/abs/2506.06659", "authors": ["Wenhao Yao", "Zhenxin Li", "Shiyi Lan", "Zi Wang", "Xinglong Sun", "Jose M. Alvarez", "Zuxuan Wu"], "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning", "comment": "15 pages, 6 figures", "summary": "In complex driving environments, autonomous vehicles must navigate safely.\nRelying on a single predicted path, as in regression-based approaches, usually\ndoes not explicitly assess the safety of the predicted trajectory.\nSelection-based methods address this by generating and scoring multiple\ntrajectory candidates and predicting the safety score for each, but face\noptimization challenges in precisely selecting the best option from thousands\nof possibilities and distinguishing subtle but safety-critical differences,\nespecially in rare or underrepresented scenarios. We propose DriveSuprim to\novercome these challenges and advance the selection-based paradigm through a\ncoarse-to-fine paradigm for progressive candidate filtering, a rotation-based\naugmentation method to improve robustness in out-of-distribution scenarios, and\na self-distillation framework to stabilize training. DriveSuprim achieves\nstate-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS\nin NAVSIM v2 without extra data, demonstrating superior safetycritical\ncapabilities, including collision avoidance and compliance with rules, while\nmaintaining high trajectory quality in various driving scenarios.", "AI": {"tldr": "DriveSuprim通过渐进候选过滤、旋转增强和自蒸馏框架提升自动驾驶轨迹选择的安全性和性能。", "motivation": "解决基于选择的方法在优化和区分安全关键差异上的挑战，提升自动驾驶的安全性和轨迹质量。", "method": "采用粗到细的渐进候选过滤、旋转增强方法和自蒸馏框架。", "result": "在NAVSIM v1和v2中分别达到93.5% PDMS和87.1% EPDMS，表现出卓越的安全性和轨迹质量。", "conclusion": "DriveSuprim在安全性和性能上优于现有方法，适用于复杂驾驶场景。"}}
{"id": "2506.07139", "categories": ["eess.SY", "cond-mat.mtrl-sci", "cs.AR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07139", "abs": "https://arxiv.org/abs/2506.07139", "authors": ["Arev Hambardzumyan", "Rafayel Ghasabyan", "Vahagn Tamazyan"], "title": "FPGA-Based Material Testing Machine Controller", "comment": null, "summary": "In the realm of contemporary materials testing, the demand for scalability,\nadaptability, parallelism, and speed has surged due to the proliferation of\ndiverse materials and testing standards. Traditional controller-based systems\noften fall short in meeting these requirements, resulting in adaptability and\nprocessing speed limitations. Conversely, FPGA-based controllers present a\nmultifaceted, high-performance solution. Key advantages of FPGA-based\ncontrollers in materials testing encompass reconfiguration capabilities for\ncost-effective adaptation to evolving materials and standards. FPGAs also\nenable the integration of parallel control and data acquisition circuits, vital\nfor multichannel test equipment demanding simultaneous, independent operation\nof multiple control channels.", "AI": {"tldr": "FPGA-based controllers offer scalable, adaptable, and high-performance solutions for modern materials testing, overcoming limitations of traditional systems.", "motivation": "The increasing demand for scalability, adaptability, parallelism, and speed in materials testing due to diverse materials and standards.", "method": "Utilization of FPGA-based controllers for their reconfiguration capabilities and parallel control/data acquisition circuits.", "result": "FPGA-based controllers provide cost-effective adaptation and enable simultaneous, independent operation of multiple control channels.", "conclusion": "FPGA-based controllers are a superior alternative to traditional systems for meeting modern materials testing requirements."}}
{"id": "2506.07520", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07520", "abs": "https://arxiv.org/abs/2506.07520", "authors": ["Shun Lei", "Yaoxun Xu", "Zhiwei Lin", "Huaicheng Zhang", "Wei Tan", "Hangting Chen", "Jianwei Yu", "Yixuan Zhang", "Chenyu Yang", "Haina Zhu", "Shuai Wang", "Zhiyong Wu", "Dong Yu"], "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment", "comment": null, "summary": "Recent advances in large language models (LLMs) and audio language models\nhave significantly improved music generation, particularly in lyrics-to-song\ngeneration. However, existing approaches still struggle with the complex\ncomposition of songs and the scarcity of high-quality data, leading to\nlimitations in sound quality, musicality, instruction following, and\nvocal-instrument harmony. To address these challenges, we introduce LeVo, an\nLM-based framework consisting of LeLM and a music codec. LeLM is capable of\nparallelly modeling two types of tokens: mixed tokens, which represent the\ncombined audio of vocals and accompaniment to achieve vocal-instrument harmony,\nand dual-track tokens, which separately encode vocals and accompaniment for\nhigh-quality song generation. It employs two decoder-only transformers and a\nmodular extension training strategy to prevent interference between different\ntoken types. To further enhance musicality and instruction following, we\nintroduce a multi-preference alignment method based on Direct Preference\nOptimization (DPO). This method handles diverse human preferences through a\nsemi-automatic data construction process and DPO post-training. Experimental\nresults demonstrate that LeVo consistently outperforms existing methods on both\nobjective and subjective metrics. Ablation studies further justify the\neffectiveness of our designs. Audio examples are available at\nhttps://levo-demo.github.io/.", "AI": {"tldr": "LeVo是一个基于语言模型的框架，通过LeLM和音乐编解码器解决歌词到歌曲生成中的复杂性和数据稀缺问题，显著提升了音质、音乐性和人声-乐器和谐度。", "motivation": "现有方法在歌曲生成的复杂性、音质、音乐性和人声-乐器和谐度方面存在不足，亟需改进。", "method": "LeVo采用LeLM并行建模混合令牌和双轨令牌，结合模块化扩展训练策略和多偏好对齐方法（基于DPO）。", "result": "实验表明，LeVo在客观和主观指标上均优于现有方法，消融研究验证了设计的有效性。", "conclusion": "LeVo通过创新的框架和训练策略，显著提升了歌词到歌曲生成的质量和多样性。"}}
{"id": "2506.07830", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07830", "abs": "https://arxiv.org/abs/2506.07830", "authors": ["Yichi Zhang", "Brandon Lyman", "Celia Pearce", "Miso Kim", "Casper Harteveld", "Leanne Chukoskie", "Bob De Schutter"], "title": "Integrating Artificial Intelligence as Assistive Technology for Older Adult Gamers: A Pilot Study", "comment": "9 pages, 1 figure", "summary": "With respect to digital games, older adults are a demographic that is often\nunderserved due to an industry-wide focus on younger audiences' preferences and\nskill sets. Meanwhile, as artificial intelligence (AI) continues to expand into\neveryday technologies, its assistive capabilities have been recognized,\nsuggesting its potential in improving the gaming experience for older gamers.\nTo study this potential, we iteratively developed a pilot survey aimed at\nunderstanding older adult gamers' current gameplay preference, challenges they\nare facing, and their perspectives of AI usage in gaming. This article\ncontributes an overview of our iterative survey-design workflow, and pilot\nresults from 39 participants. During each iteration, we analyzed the survey's\nefficacy and adjusted the content, language, and format to better capture\nmeaningful data, and was able to create a refined survey for a larger, more\nrepresentative future parent study. At the same time, preliminary findings\nsuggest that for older adult gamers, usability issues in gaming remain key\nobstacles, while this demographic's perceptions of AI are shaped by both its\npractical benefits and concerns about autonomy and complexity. These findings\nalso offer early insights for the design of age-inclusive, AI-supported gaming\nexperiences.", "AI": {"tldr": "论文探讨了AI如何改善老年玩家的游戏体验，通过迭代设计调查了解其偏好、挑战及对AI的看法，初步发现可用性问题和AI的双面性是关键。", "motivation": "老年玩家在数字游戏中常被忽视，而AI的辅助能力可能改善其体验。", "method": "通过迭代设计调查，分析39名参与者的反馈，优化调查内容。", "result": "发现游戏可用性问题和AI的实用性与复杂性是老年玩家的主要关注点。", "conclusion": "研究为设计包容老年玩家的AI支持游戏提供了早期见解。"}}
{"id": "2506.06680", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06680", "abs": "https://arxiv.org/abs/2506.06680", "authors": ["Radha Kodali", "Venkata Rao Dhulipalla", "Venkata Siva Kishor Tatavarty", "Madhavi Nadakuditi", "Bharadwaj Thiruveedhula", "Suryanarayana Gunnam", "Durga Prasad Bavirisetti"], "title": "Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment", "comment": null, "summary": "Infertility has a considerable impact on individuals' quality of life,\naffecting them socially and psychologically, with projections indicating a rise\nin the upcoming years. In vitro fertilization (IVF) emerges as one of the\nprimary techniques within economically developed nations, employed to address\nthe rising problem of low fertility. Expert embryologists conventionally grade\nembryos by reviewing blastocyst images to select the most optimal for transfer,\nyet this process is time-consuming and lacks efficiency. Blastocyst images\nprovide a valuable resource for assessing embryo viability. In this study, we\nintroduce an explainable artificial intelligence (XAI) framework for\nclassifying embryos, employing a fusion of convolutional neural network (CNN)\nand long short-term memory (LSTM) architecture, referred to as CNN-LSTM.\nUtilizing deep learning, our model achieves high accuracy in embryo\nclassification while maintaining interpretability through XAI.", "AI": {"tldr": "本研究提出了一种结合CNN和LSTM的XAI框架，用于高效分类胚胎图像，提高体外受精中胚胎选择的准确性和可解释性。", "motivation": "不孕症对生活质量有显著影响，传统胚胎分级方法效率低下，需要更高效且可解释的技术。", "method": "采用CNN-LSTM架构，结合深度学习与XAI技术，对胚胎图像进行分类。", "result": "模型在胚胎分类中实现了高准确性，同时保持了可解释性。", "conclusion": "XAI框架为胚胎选择提供了高效且透明的解决方案，有望提升体外受精成功率。"}}
{"id": "2506.06380", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2506.06380", "abs": "https://arxiv.org/abs/2506.06380", "authors": ["Jingyi Gu", "Xuan Zhang", "Guiling Wang"], "title": "Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events", "comment": null, "summary": "Extreme events, such as market crashes, natural disasters, and pandemics, are\nrare but catastrophic, often triggering cascading failures across\ninterconnected systems. Accurate prediction and early warning can help minimize\nlosses and improve preparedness. While data-driven methods offer powerful\ncapabilities for extreme event modeling, they require abundant training data,\nyet extreme event data is inherently scarce, creating a fundamental challenge.\nSynthetic data generation has emerged as a powerful solution. However, existing\nsurveys focus on general data with privacy preservation emphasis, rather than\nextreme events' unique performance requirements. This survey provides the first\noverview of synthetic data generation for extreme events. We systematically\nreview generative modeling techniques and large language models, particularly\nthose enhanced by statistical theory as well as specialized training and\nsampling mechanisms to capture heavy-tailed distributions. We summarize\nbenchmark datasets and introduce a tailored evaluation framework covering\nstatistical, dependence, visual, and task-oriented metrics. A central\ncontribution is our in-depth analysis of each metric's applicability in\nextremeness and domain-specific adaptations, providing actionable guidance for\nmodel evaluation in extreme settings. We categorize key application domains and\nidentify underexplored areas like behavioral finance, wildfires, earthquakes,\nwindstorms, and infectious outbreaks. Finally, we outline open challenges,\nproviding a structured foundation for advancing synthetic rare-event research.", "AI": {"tldr": "该论文首次综述了极端事件合成数据生成方法，系统回顾了生成模型和大语言模型，并提出了针对极端事件的评估框架和应用领域。", "motivation": "极端事件（如市场崩溃、自然灾害和疫情）罕见但破坏性强，准确预测和早期预警可减少损失。然而，极端事件数据稀缺，传统数据驱动方法面临挑战。", "method": "论文系统回顾了生成建模技术和大语言模型，特别关注统计理论增强的方法，以及捕捉重尾分布的专业训练和采样机制。", "result": "提出了一个针对极端事件的评估框架，包括统计、依赖、视觉和任务导向的指标，并分析了各指标在极端环境中的适用性。", "conclusion": "论文总结了关键应用领域和未充分探索的方向，并提出了未来研究的开放挑战，为极端事件合成数据研究提供了结构化基础。"}}
{"id": "2506.06664", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06664", "abs": "https://arxiv.org/abs/2506.06664", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Joshua Chen", "Nadine Chang", "Maying Shen", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "comment": "The 1st place solution of the End-to-end Driving Track at the CVPR\n  2025 Autonomous Grand Challenge", "summary": "End-to-end multi-modal planning is a promising paradigm in autonomous\ndriving, enabling decision-making with diverse trajectory candidates. A key\ncomponent is a robust trajectory scorer capable of selecting the optimal\ntrajectory from these candidates. While recent trajectory scorers focus on\nscoring either large sets of static trajectories or small sets of dynamically\ngenerated ones, both approaches face significant limitations in generalization.\nStatic vocabularies provide effective coarse discretization but struggle to\nmake fine-grained adaptation, while dynamic proposals offer detailed precision\nbut fail to capture broader trajectory distributions. To overcome these\nchallenges, we propose GTRS (Generalized Trajectory Scoring), a unified\nframework for end-to-end multi-modal planning that combines coarse and\nfine-grained trajectory evaluation. GTRS consists of three complementary\ninnovations: (1) a diffusion-based trajectory generator that produces diverse\nfine-grained proposals; (2) a vocabulary generalization technique that trains a\nscorer on super-dense trajectory sets with dropout regularization, enabling its\nrobust inference on smaller subsets; and (3) a sensor augmentation strategy\nthat enhances out-of-domain generalization while incorporating refinement\ntraining for critical trajectory discrimination. As the winning solution of the\nNavsim v2 Challenge, GTRS demonstrates superior performance even with\nsub-optimal sensor inputs, approaching privileged methods that rely on\nground-truth perception. Code will be available at\nhttps://github.com/NVlabs/GTRS.", "AI": {"tldr": "GTRS提出了一种统一的多模态规划框架，结合粗粒度和细粒度轨迹评估，解决了静态和动态轨迹评分方法的局限性。", "motivation": "现有轨迹评分方法在泛化能力上存在显著不足，静态方法缺乏细粒度适应性，动态方法难以捕捉广泛轨迹分布。", "method": "GTRS包含三个创新：扩散式轨迹生成器、词汇泛化技术和传感器增强策略。", "result": "GTRS在Navsim v2挑战赛中表现优异，接近依赖真实感知的基准方法。", "conclusion": "GTRS通过结合粗粒度和细粒度评估，显著提升了轨迹评分的泛化能力。"}}
{"id": "2506.07347", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07347", "abs": "https://arxiv.org/abs/2506.07347", "authors": ["Armin Lederer", "Erfaun Noorani", "Andreas Krause"], "title": "Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems", "comment": null, "summary": "Ensuring safety in multi-agent systems is a significant challenge,\nparticularly in settings where centralized coordination is impractical. In this\nwork, we propose a novel risk-sensitive safety filter for discrete-time\nmulti-agent systems with uncertain dynamics that leverages control barrier\nfunctions (CBFs) defined through value functions. Our approach relies on\ncentralized risk-sensitive safety conditions based on exponential risk\noperators to ensure robustness against model uncertainties. We introduce a\ndistributed formulation of the safety filter by deriving two alternative\nstrategies: one based on worst-case anticipation and another on proximity to a\nknown safe policy. By allowing agents to switch between strategies, feasibility\ncan be ensured. Through detailed numerical evaluations, we demonstrate the\nefficacy of our approach in maintaining safety without being overly\nconservative.", "AI": {"tldr": "提出了一种基于控制屏障函数（CBFs）的风险敏感安全过滤器，用于离散时间多智能体系统，确保在模型不确定性下的鲁棒性。", "motivation": "解决多智能体系统中因集中协调不切实际而带来的安全性挑战。", "method": "利用基于指数风险算子的集中化风险敏感安全条件，并引入两种分布式策略：最坏情况预测和接近已知安全策略。", "result": "通过数值评估验证了该方法在保持安全性的同时避免过度保守。", "conclusion": "该方法有效解决了多智能体系统中的安全性问题，具有实用性和灵活性。"}}
{"id": "2506.07930", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07930", "abs": "https://arxiv.org/abs/2506.07930", "authors": ["Kieran J. Smith", "Tristan C. Endsley", "Torin K. Clark"], "title": "Predicting Situation Awareness from Physiological Signals", "comment": "15 pages, 6 figures, submitted to IEEE Transactions on Human-Machine\n  Systems", "summary": "Situation awareness (SA)--comprising the ability to 1) perceive critical\nelements in the environment, 2) comprehend their meanings, and 3) project their\nfuture states--is critical for human operator performance. Due to the\ndisruptive nature of gold-standard SA measures, researchers have sought\nphysiological indicators to provide real-time information about SA. We extend\nprior work by using a multimodal suite of neurophysiological,\npsychophysiological, and behavioral signals, predicting all three levels of SA\nalong a continuum, and predicting a comprehensive measure of SA in a complex\nmulti-tasking simulation. We present a lab study in which 31 participants\ncontrolled an aircraft simulator task battery while wearing physiological\nsensors and responding to SA 'freeze-probe' assessments. We demonstrate the\nvalidity of task and assessment for measuring SA. Multimodal physiological\nmodels predict SA with greater predictive performance ($Q^2$ for levels 1-3 and\ntotal, respectively: 0.14, 0.00, 0.26, and 0.36) than models built with\nshuffled labels, demonstrating that multimodal physiological signals provide\nuseful information in predicting all SA levels. Level 3 SA (projection) was\nbest predicted, and level 2 SA comprehension) was the most challenging to\npredict. Ablation analysis and single sensor models found EEG and eye-tracking\nsignals to be particularly useful to predictions of level 3 and total SA. A\nreduced sensor fusion model showed that predictive performance can be\nmaintained with a subset of sensors. This first rigorous cross-validation\nassessment of predictive performance demonstrates the utility of multimodal\nphysiological signals for inferring complex, holistic, objective measures of SA\nat all levels, non-disruptively, and along a continuum.", "AI": {"tldr": "该研究通过多模态生理信号预测情境意识（SA）的三个层次，展示了其在复杂任务中的有效性，尤其是EEG和眼动信号对预测SA的贡献。", "motivation": "情境意识（SA）对人类操作员表现至关重要，但传统测量方法具有干扰性，因此研究寻求生理指标以实时评估SA。", "method": "研究使用多模态神经生理、心理生理和行为信号，结合飞机模拟器任务和SA冻结探针评估，预测SA的三个层次。", "result": "多模态生理模型在预测SA时表现优于随机标签模型，其中EEG和眼动信号对SA的第三层次（预测）和总体SA贡献显著。", "conclusion": "多模态生理信号能非干扰性地预测SA的所有层次，为复杂任务中的SA评估提供了新方法。"}}
{"id": "2506.06710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.06710", "abs": "https://arxiv.org/abs/2506.06710", "authors": ["Qianqian Zhao", "Chunle Guo", "Tianyi Zhang", "Junpei Zhang", "Peiyang Jia", "Tan Su", "Wenjie Jiang", "Chongyi Li"], "title": "A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution", "comment": null, "summary": "Omnidirectional image and video super-resolution is a crucial research topic\nin low-level vision, playing an essential role in virtual reality and augmented\nreality applications. Its goal is to reconstruct high-resolution images or\nvideo frames from low-resolution inputs, thereby enhancing detail preservation\nand enabling more accurate scene analysis and interpretation. In recent years,\nnumerous innovative and effective approaches have been proposed, predominantly\nbased on deep learning techniques, involving diverse network architectures,\nloss functions, projection strategies, and training datasets. This paper\npresents a systematic review of recent progress in omnidirectional image and\nvideo super-resolution, focusing on deep learning-based methods. Given that\nexisting datasets predominantly rely on synthetic degradation and fall short in\ncapturing real-world distortions, we introduce a new dataset, 360Insta, that\ncomprises authentically degraded omnidirectional images and videos collected\nunder diverse conditions, including varying lighting, motion, and exposure\nsettings. This dataset addresses a critical gap in current omnidirectional\nbenchmarks and enables more robust evaluation of the generalization\ncapabilities of omnidirectional super-resolution methods. We conduct\ncomprehensive qualitative and quantitative evaluations of existing methods on\nboth public datasets and our proposed dataset. Furthermore, we provide a\nsystematic overview of the current status of research and discuss promising\ndirections for future exploration. All datasets, methods, and evaluation\nmetrics introduced in this work are publicly available and will be regularly\nupdated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.", "AI": {"tldr": "本文综述了基于深度学习的全向图像和视频超分辨率方法，并提出了一个真实退化数据集360Insta，以弥补现有合成数据集的不足。", "motivation": "全向图像和视频超分辨率在虚拟现实和增强现实中至关重要，但现有数据集多为合成退化，无法反映真实场景的复杂性。", "method": "系统回顾了深度学习方法，并引入真实退化数据集360Insta，进行定性和定量评估。", "result": "360Insta填补了现有全向基准的空白，提升了方法泛化能力的评估。", "conclusion": "本文总结了研究现状，并展望了未来方向，所有资源公开且定期更新。"}}
{"id": "2506.06398", "categories": ["cs.LG", "cs.AI", "68T07, 68Q32", "I.2.6; I.2.7; F.1.1"], "pdf": "https://arxiv.org/pdf/2506.06398", "abs": "https://arxiv.org/abs/2506.06398", "authors": ["Yin Li"], "title": "Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization", "comment": null, "summary": "Positional encodings are a core part of transformer-based models, enabling\nprocessing of sequential data without recurrence. This paper presents a\ntheoretical framework to analyze how various positional encoding methods,\nincluding sinusoidal, learned, relative, and bias-based methods like Attention\nwith Linear Biases (ALiBi), impact a transformer's expressiveness,\ngeneralization ability, and extrapolation to longer sequences. Expressiveness\nis defined via function approximation, generalization bounds are established\nusing Rademacher complexity, and new encoding methods based on orthogonal\nfunctions, such as wavelets and Legendre polynomials, are proposed. The\nextrapolation capacity of existing and proposed encodings is analyzed,\nextending ALiBi's biasing approach to a unified theoretical context.\nExperimental evaluation on synthetic sequence-to-sequence tasks shows that\northogonal transform-based encodings outperform traditional sinusoidal\nencodings in generalization and extrapolation. This work addresses a critical\ngap in transformer theory, providing insights for design choices in natural\nlanguage processing, computer vision, and other transformer applications.", "AI": {"tldr": "该论文提出了一种理论框架，分析不同位置编码方法对Transformer模型表达能力、泛化能力和长序列外推能力的影响，并提出基于正交函数的新编码方法。", "motivation": "研究位置编码方法如何影响Transformer模型的性能，填补Transformer理论中的关键空白。", "method": "通过函数逼近定义表达能力，使用Rademacher复杂度建立泛化界限，并提出基于正交函数（如小波和Legendre多项式）的新编码方法。", "result": "实验表明，基于正交变换的编码方法在泛化和外推方面优于传统的正弦编码方法。", "conclusion": "该研究为自然语言处理、计算机视觉等领域中Transformer的设计选择提供了理论依据。"}}
{"id": "2506.06677", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06677", "abs": "https://arxiv.org/abs/2506.06677", "authors": ["Songhao Han", "Boxiang Qiu", "Yue Liao", "Siyuan Huang", "Chen Gao", "Shuicheng Yan", "Si Liu"], "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "comment": "23 pages, 18 figures", "summary": "Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.", "AI": {"tldr": "RoboCerebra是一个新的基准测试，用于评估机器人操作中的高级推理能力，弥补了现有基准测试在长期规划和语义推理方面的不足。", "motivation": "现有研究多集中于反应性策略（System 1），而忽视了视觉语言模型（VLMs）在语义推理和长期规划（System 2）方面的潜力。", "method": "提出了RoboCerebra基准测试，包括大规模模拟数据集、分层框架（VLM规划器和VLA控制器）和结构化评估协议。", "result": "RoboCerebra在任务序列长度和注释密度上优于现有基准，并测试了VLMs作为System 2模块的性能。", "conclusion": "RoboCerebra推动了更强大和通用的机器人规划器的发展。"}}
{"id": "2506.07374", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07374", "abs": "https://arxiv.org/abs/2506.07374", "authors": ["Mengze Yu", "Wei Wang", "Jiaqi Yan"], "title": "Extended Version of \"Distributed Adaptive Resilient Consensus Control for Uncertain Nonlinear Multiagent Systems Against Deception Attacks\"", "comment": "7 pages, 6 figures. submitted to IEEE Control Systems Letters", "summary": "This paper studies distributed resilient consensus problem for a class of\nuncertain nonlinear multiagent systems susceptible to deception attacks. The\nattacks invade both sensor and actuator channels of each agent. A specific\nclass of Nussbaum functions is adopted to manage the attack-incurred multiple\nunknown control directions. Additionally, a general form of these Nussbaum\nfunctions is provided, which helps to ease the degeneration of output\nperformance caused by Nussbaum gains. Then, by introducing finite-time\ndistributed reference systems and local-error-based dynamic gains, we propose a\nnovel distributed adaptive backstepping-based resilient consensus control\nstrategy. We prove that all the closed-loop signals are uniformly bounded under\nattacks, and output consensus errors converge in finite time to a\nclearly-defined residual set whose size can be reduced by tuning control\nparameters, which is superior to existing results. Simulation results display\nthe effectiveness of the proposed controllers.", "AI": {"tldr": "本文研究了易受欺骗攻击的非线性多智能体系统的分布式弹性共识问题，提出了一种基于Nussbaum函数和自适应反步控制的解决方案。", "motivation": "多智能体系统在传感器和执行器通道受到攻击时，共识问题变得复杂且具有挑战性，需要一种弹性的控制策略。", "method": "采用Nussbaum函数处理未知控制方向，引入有限时间分布式参考系统和动态增益，提出自适应反步控制策略。", "result": "闭环信号在攻击下有界，输出共识误差在有限时间内收敛到可调残差集。", "conclusion": "所提控制器优于现有方法，仿真验证了其有效性。"}}
{"id": "2506.07722", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07722", "abs": "https://arxiv.org/abs/2506.07722", "authors": ["Yassine El Kheir", "Omnia Ibrahim", "Amit Meghanani", "Nada Almarwani", "Hawau Olamide Toyin", "Sadeen Alharbi", "Modar Alfadly", "Lamya Alkanhal", "Ibrahim Selim", "Shehab Elbatal", "Salima Mdhaffar", "Thomas Hain", "Yasser Hifny", "Mostafa Shahin", "Ahmed Ali"], "title": "Towards a Unified Benchmark for Arabic Pronunciation Assessment: Quranic Recitation as Case Study", "comment": "Accepted Interspeech 2025 and ArabicNLP Shared Task 2025", "summary": "We present a unified benchmark for mispronunciation detection in Modern\nStandard Arabic (MSA) using Qur'anic recitation as a case study. Our approach\nlays the groundwork for advancing Arabic pronunciation assessment by providing\na comprehensive pipeline that spans data processing, the development of a\nspecialized phoneme set tailored to the nuances of MSA pronunciation, and the\ncreation of the first publicly available test set for this task, which we term\nas the Qur'anic Mispronunciation Benchmark (QuranMB.v1). Furthermore, we\nevaluate several baseline models to provide initial performance insights,\nthereby highlighting both the promise and the challenges inherent in assessing\nMSA pronunciation. By establishing this standardized framework, we aim to\nfoster further research and development in pronunciation assessment in Arabic\nlanguage technology and related applications.", "AI": {"tldr": "提出了一个统一的现代标准阿拉伯语（MSA）发音错误检测基准，以古兰经诵读为例，涵盖数据处理、专用音素集开发和首个公开测试集（QuranMB.v1），并评估基线模型。", "motivation": "为阿拉伯语发音评估提供标准化框架，推动相关技术研究与应用。", "method": "开发数据处理流程、专用音素集和公开测试集（QuranMB.v1），并评估基线模型。", "result": "展示了MSA发音评估的潜力与挑战。", "conclusion": "该框架为阿拉伯语发音评估及相关应用的研究与发展奠定了基础。"}}
{"id": "2506.07955", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07955", "abs": "https://arxiv.org/abs/2506.07955", "authors": ["Zewei", "Tian", "Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zachary Zhang", "Kevin He", "Min Sun"], "title": "Implementation Considerations for Automated AI Grading of Student Work", "comment": null, "summary": "This study explores the classroom implementation of an AI-powered grading\nplatform in K-12 settings through a co-design pilot with 19 teachers. We\ncombine platform usage logs, surveys, and qualitative interviews to examine how\nteachers use AI-generated rubrics and grading feedback. Findings reveal that\nwhile teachers valued the AI's rapid narrative feedback for formative purposes,\nthey distrusted automated scoring and emphasized the need for human oversight.\nStudents welcomed fast, revision-oriented feedback but remained skeptical of\nAI-only grading. We discuss implications for the design of trustworthy,\nteacher-centered AI assessment tools that enhance feedback while preserving\npedagogical agency.", "AI": {"tldr": "研究探讨了AI评分平台在K-12课堂中的应用，发现教师信任AI的快速反馈但质疑其自动评分，学生则偏好快速反馈但对纯AI评分持怀疑态度。", "motivation": "探索AI评分平台在K-12教育中的实际应用效果，以了解教师和学生对AI工具的接受度与需求。", "method": "通过19名教师的合作设计试点，结合平台使用日志、调查和定性访谈，分析教师对AI生成评分标准和反馈的使用情况。", "result": "教师重视AI的快速反馈但对其自动评分不信任，学生喜欢快速反馈但对纯AI评分持怀疑态度。", "conclusion": "设计可信赖、以教师为中心的AI评估工具需平衡AI反馈与人工监督，以增强反馈效果并保留教学自主权。"}}
{"id": "2506.06712", "categories": ["cs.CV", "math.AP"], "pdf": "https://arxiv.org/pdf/2506.06712", "abs": "https://arxiv.org/abs/2506.06712", "authors": ["Saiyu Hu", "Chunlei He", "Jianfeng Zhang", "Dexing Kong", "Shoujun Huang"], "title": "Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation", "comment": null, "summary": "Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are\nwidely used in image segmentation, which however depend heavily on the\nselection of initial curve configurations. In this paper, we firstly propose\nseveral hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce\ntunable initial velocity fields, enabling adaptive optimization for diverse\nsegmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows\nand establish the numerical equivalence between dissipative HMCF formulations\nand certain wave equations using the level set method with signed distance\nfunction. Building on this framework, we furthermore develop hyperbolic\ndual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth\nHeaviside functions for edge-aware force modulation to suppress over-diffusion\nnear weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta\nalgorithm with nine-point stencil spatial discretization when solving the\nabove-mentioned wave equations. Experiments show that both HMCF-ACMs and\nHDRF-ACMs could achieve more precise segmentations with superior noise\nresistance and numerical stability due to task-adaptive configurations of\ninitial velocities and initial contours.", "AI": {"tldr": "本文提出了基于双曲平均曲率流的主动轮廓模型（HMCF-ACMs）和双模式正则化流驱动的主动轮廓模型（HDRF-ACMs），通过可调初始速度场和边缘感知力调制，提升了图像分割的精度和抗噪能力。", "motivation": "传统的抛物线平均曲率流驱动的主动轮廓模型（PMCF-ACMs）对初始曲线配置依赖性强，限制了其适应性和性能。本文旨在通过引入双曲平均曲率流和正则化技术，解决这一问题。", "method": "1. 提出HMCF-ACMs，引入可调初始速度场；2. 证明HMCF-ACMs是法向流，并通过水平集方法建立数值等价性；3. 开发HDRF-ACMs，利用平滑Heaviside函数抑制弱边界处的过度扩散；4. 优化加权四阶Runge-Kutta算法求解波方程。", "result": "实验表明，HMCF-ACMs和HDRF-ACMs在噪声抑制和数值稳定性方面表现优异，能够实现更精确的分割。", "conclusion": "HMCF-ACMs和HDRF-ACMs通过任务自适应的初始配置，显著提升了图像分割的性能，适用于多样化场景。"}}
{"id": "2506.06411", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06411", "abs": "https://arxiv.org/abs/2506.06411", "authors": ["Paul Fogel", "Christophe Geissler", "George Luta"], "title": "CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis", "comment": "7 pages, 3 figures, Conference on Lifetime Data Science 2025,\n  Brooklyn, New York, USA", "summary": "The interpretation of the results of survival analysis often benefits from\nlatent factor representations of baseline covariates. However, existing\nmethods, such as Nonnegative Matrix Factorization (NMF), do not incorporate\nsurvival information, limiting their predictive power. We present CoxNTF, a\nnovel approach that uses non-negative tensor factorization (NTF) to derive\nmeaningful latent representations that are closely associated with survival\noutcomes. CoxNTF constructs a weighted covariate tensor in which survival\nprobabilities derived from the Coxnet model are used to guide the tensorization\nprocess. Our results show that CoxNTF achieves survival prediction performance\ncomparable to using Coxnet with the original covariates, while providing a\nstructured and interpretable clustering framework. In addition, the new\napproach effectively handles feature redundancy, making it a powerful tool for\njoint clustering and prediction in survival analysis.", "AI": {"tldr": "论文提出CoxNTF方法，结合非负张量分解和生存分析，提升预测性能和可解释性。", "motivation": "现有方法（如NMF）未结合生存信息，限制了预测能力。", "method": "使用非负张量分解（NTF）构建加权协变量张量，结合Coxnet模型的生存概率指导分解。", "result": "CoxNTF预测性能与原始协变量的Coxnet相当，同时提供结构化聚类框架，有效处理特征冗余。", "conclusion": "CoxNTF是生存分析中联合聚类和预测的有力工具。"}}
{"id": "2506.06683", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06683", "abs": "https://arxiv.org/abs/2506.06683", "authors": ["Shiying Duan", "Pei Ren", "Nanxiang Jiang", "Zhengping Che", "Jian Tang", "Yifan Sun", "Zhaoxin Fan", "Wenjun Wu"], "title": "RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks", "comment": null, "summary": "Dual-arm robots play a crucial role in improving efficiency and flexibility\nin complex multitasking scenarios. While existing methods have achieved\npromising results in task planning, they often fail to fully optimize task\nparallelism, limiting the potential of dual-arm collaboration. To address this\nissue, we propose RoboPARA, a novel large language model (LLM)-driven framework\nfor dual-arm task parallelism planning. RoboPARA employs a two-stage process:\n(1) Dependency Graph-based Planning Candidates Generation, which constructs\ndirected acyclic graphs (DAGs) to model task dependencies and eliminate\nredundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which\noptimizes DAG traversal to maximize parallelism while maintaining task\ncoherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task\ndataset (X-DAPT dataset), the first dataset specifically designed to evaluate\ndual-arm task parallelism across diverse scenarios and difficulty levels.\nExtensive experiments on the X-DAPT dataset demonstrate that RoboPARA\nsignificantly outperforms existing methods, achieving higher efficiency and\nreliability, particularly in complex task combinations. The code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "RoboPARA是一个基于大语言模型的双臂机器人任务并行规划框架，通过两阶段优化任务并行性，显著提升效率和可靠性。", "motivation": "现有方法在双臂机器人任务规划中未能充分优化任务并行性，限制了协作潜力。", "method": "采用两阶段方法：依赖图规划候选生成和图重遍历双臂并行规划，优化任务依赖和并行性。", "result": "在X-DAPT数据集上表现优异，尤其在复杂任务组合中效率和可靠性显著提升。", "conclusion": "RoboPARA为双臂机器人任务并行规划提供了高效解决方案，并发布了首个相关数据集。"}}
{"id": "2506.07519", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07519", "abs": "https://arxiv.org/abs/2506.07519", "authors": ["Jussi Sihvo", "Noël Hallemans", "Ai Hui Tan", "David A. Howey", "Stephen. R. Duncan", "Tomi Roinila"], "title": "Pseudo-random sequences for low-cost operando impedance measurements of Li-ion batteries", "comment": "10 pages, 7 figures", "summary": "Operando impedance measurements are promising for monitoring batteries in the\nfield. In this work, we present pseudo-random sequences for low-cost operando\nbattery impedance measurements. The quadratic-residue ternary sequence and\ndirect-synthesis ternary sequence exhibit specific properties related to\neigenvectors of the discrete Fourier transform matrix that allow\ncomputationally efficient compensation for drifts and transients in operando\nimpedance measurements. We describe the application of pseudo-random sequences\nand provide the data processing required to suppress drift and transients,\nvalidated on simulations. Finally, we perform experimental operando impedance\nmeasurements on a Li-ion battery cell during fast-charging, demonstrating the\napplicability of the proposed method. It's low-cost hardware requirements, fast\nmeasurements, and simple data-processing make the method practical for\nembedding in battery management systems.", "AI": {"tldr": "提出了一种低成本、高效的伪随机序列方法，用于实时电池阻抗测量，适用于电池管理系统。", "motivation": "实时阻抗测量对电池监测至关重要，但现有方法成本高且复杂。", "method": "使用二次剩余三元序列和直接合成三元序列，利用离散傅里叶变换矩阵特性补偿漂移和瞬态。", "result": "通过仿真和实验验证了方法的有效性，适用于快速充电场景。", "conclusion": "该方法硬件成本低、测量快速、数据处理简单，适合嵌入电池管理系统。"}}
{"id": "2506.07997", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.07997", "abs": "https://arxiv.org/abs/2506.07997", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "title": "Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System", "comment": null, "summary": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers.", "AI": {"tldr": "论文提出了一种基于多代理对话系统的AI解决方案，旨在改善建筑工人的心理健康和工作支持，并通过实验验证了其优于单代理系统的效果。", "motivation": "建筑行业存在高心理和生理风险，但心理健康支持有限。AI（尤其是大语言模型）的潜力尚未充分挖掘。", "method": "开发了一个结合领域知识的对话多代理系统，满足工人的心理需求并提供问题解决支持。", "result": "用户研究表明，系统在可用性、自主性、社交存在感和信任度上分别提升了18%、40%、60%和60%。", "conclusion": "LLM驱动的AI系统在建筑行业特定支持中展现出巨大潜力。"}}
{"id": "2506.06719", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06719", "abs": "https://arxiv.org/abs/2506.06719", "authors": ["Mufhumudzi Muthivhi", "Jiahao Huo", "Fredrik Gustafsson", "Terence L. van Zyl"], "title": "Improving Wildlife Out-of-Distribution Detection: Africas Big Five", "comment": null, "summary": "Mitigating human-wildlife conflict seeks to resolve unwanted encounters\nbetween these parties. Computer Vision provides a solution to identifying\nindividuals that might escalate into conflict, such as members of the Big Five\nAfrican animals. However, environments often contain several varied species.\nThe current state-of-the-art animal classification models are trained under a\nclosed-world assumption. They almost always remain overconfident in their\npredictions even when presented with unknown classes. This study investigates\nout-of-distribution (OOD) detection of wildlife, specifically the Big Five. To\nthis end, we select a parametric Nearest Class Mean (NCM) and a non-parametric\ncontrastive learning approach as baselines to take advantage of pretrained and\nprojected features from popular classification encoders. Moreover, we compare\nour baselines to various common OOD methods in the literature. The results show\nfeature-based methods reflect stronger generalisation capability across varying\nclassification thresholds. Specifically, NCM with ImageNet pre-trained features\nachieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the\nbest OOD methods, respectively. The code can be found here\nhttps://github.com/pxpana/BIG5OOD", "AI": {"tldr": "论文研究了野生动物（特别是非洲五大动物）的分布外检测问题，比较了参数化和非参数化方法，并发现基于特征的方法在泛化能力上表现更优。", "motivation": "解决人类与野生动物冲突需要准确识别潜在威胁物种，但现有分类模型在未知类别上表现不佳，因此研究分布外检测方法。", "method": "选择了参数化的最近类均值（NCM）和非参数化的对比学习方法作为基线，并与其他常见分布外检测方法进行比较。", "result": "基于特征的方法（如NCM）在多个指标上优于其他方法，特别是在AUPR-IN、AUPR-OUT和AUTC上分别提升了2%、4%和22%。", "conclusion": "特征方法在野生动物分布外检测中表现更优，为实际应用提供了更可靠的解决方案。"}}
{"id": "2506.06412", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06412", "abs": "https://arxiv.org/abs/2506.06412", "authors": ["Junming Wang", "Yi Shi"], "title": "NeurNCD: Novel Class Discovery via Implicit Neural Representation", "comment": "Accepted by ICMR 2024", "summary": "Discovering novel classes in open-world settings is crucial for real-world\napplications. Traditional explicit representations, such as object descriptors\nor 3D segmentation maps, are constrained by their discrete, hole-prone, and\nnoisy nature, which hinders accurate novel class discovery. To address these\nchallenges, we introduce NeurNCD, the first versatile and data-efficient\nframework for novel class discovery that employs the meticulously designed\nEmbedding-NeRF model combined with KL divergence as a substitute for\ntraditional explicit 3D segmentation maps to aggregate semantic embedding and\nentropy in visual embedding space. NeurNCD also integrates several key\ncomponents, including feature query, feature modulation and clustering,\nfacilitating efficient feature augmentation and information exchange between\nthe pre-trained semantic segmentation network and implicit neural\nrepresentations. As a result, our framework achieves superior segmentation\nperformance in both open and closed-world settings without relying on densely\nlabelled datasets for supervised training or human interaction to generate\nsparse label supervision. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches on the NYUv2 and Replica\ndatasets.", "AI": {"tldr": "NeurNCD是一种新颖的开放世界类别发现框架，通过结合Embedding-NeRF模型和KL散度，替代传统显式3D分割图，提升了分割性能。", "motivation": "传统显式表示（如对象描述符或3D分割图）存在离散、易产生空洞和噪声的问题，限制了新类别的发现。", "method": "NeurNCD采用Embedding-NeRF模型和KL散度，结合特征查询、调制和聚类，实现高效特征增强和信息交换。", "result": "在NYUv2和Replica数据集上显著优于现有方法，无需密集标注或人工干预。", "conclusion": "NeurNCD为开放世界中的新类别发现提供了高效、数据友好的解决方案。"}}
{"id": "2506.06690", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06690", "abs": "https://arxiv.org/abs/2506.06690", "authors": ["Hao Wang", "Chengkai Hou", "Xianglong Li", "Yankai Fu", "Chenxuan Li", "Ning Chen", "Gaole Dai", "Jiaming Liu", "Tiejun Huang", "Shanghang Zhang"], "title": "SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game", "comment": null, "summary": "Learning to control high-speed objects in the real world remains a\nchallenging frontier in robotics. Table tennis serves as an ideal testbed for\nthis problem, demanding both rapid interception of fast-moving balls and\nprecise adjustment of their trajectories. This task presents two fundamental\nchallenges: it requires a high-precision vision system capable of accurately\npredicting ball trajectories, and it necessitates intelligent strategic\nplanning to ensure precise ball placement to target regions. The dynamic nature\nof table tennis, coupled with its real-time response requirements, makes it\nparticularly well-suited for advancing robotic control capabilities in\nfast-paced, precision-critical domains. In this paper, we present\nSpikePingpong, a novel system that integrates spike-based vision with imitation\nlearning for high-precision robotic table tennis. Our approach introduces two\nkey attempts that directly address the aforementioned challenges: SONIC, a\nspike camera-based module that achieves millimeter-level precision in\nball-racket contact prediction by compensating for real-world uncertainties\nsuch as air resistance and friction; and IMPACT, a strategic planning module\nthat enables accurate ball placement to targeted table regions. The system\nharnesses a 20 kHz spike camera for high-temporal resolution ball tracking,\ncombined with efficient neural network models for real-time trajectory\ncorrection and stroke planning. Experimental results demonstrate that\nSpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target\narea and 71% in the more challenging 20 cm accuracy task, surpassing previous\nstate-of-the-art approaches by 38% and 37% respectively. These significant\nperformance improvements enable the robust implementation of sophisticated\ntactical gameplay strategies, providing a new research perspective for robotic\ncontrol in high-speed dynamic tasks.", "AI": {"tldr": "SpikePingpong系统结合尖峰视觉与模仿学习，通过SONIC和IMPACT模块解决高速乒乓球机器人控制的两大挑战，实现了高精度击球和战略规划。", "motivation": "高速物体控制在机器人领域具有挑战性，乒乓球作为测试平台，需要快速拦截和精确轨迹调整。", "method": "系统采用20kHz尖峰相机进行高分辨率球跟踪，结合SONIC模块补偿现实不确定性，IMPACT模块实现精确落点规划。", "result": "实验显示，SpikePingpong在30cm和20cm精度任务中分别达到91%和71%的成功率，超越现有方法38%和37%。", "conclusion": "该系统为高速动态任务中的机器人控制提供了新视角，实现了战术性游戏策略的稳健执行。"}}
{"id": "2506.07710", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07710", "abs": "https://arxiv.org/abs/2506.07710", "authors": ["Ronald Wijermars", "Yi-Han Ou-Yang", "Sijun Du", "Dante Gabriel Muratore"], "title": "A 40.68-MHz, 200-ns-Settling Active Rectifier and TX-Side Load Monitoring for Minimizing Radiated Power in Biomedical Implants", "comment": null, "summary": "This letter describes a 40.68 MHz wireless power transfer receiver for\nimplantable applications focused on minimizing tissue heating. The system\nfeatures a novel power radiated efficiency optimization strategy and a\nfast-settling active rectifier that maintains high efficiency during load and\nlink variations required for downlink communication. The power radiated\nefficiency optimization explicitly reduces tissue heating while enabling\ntransmitter-side load monitoring for closed-loop control. The active rectifier\nwas fabricated in 40nm CMOS and achieves a voltage conversion ratio of 93.9%\nand a simulated power conversion efficiency of 90.1% in a 0.19 $mm^2$ area,\nresulting in a 118 mW/$mm^2$ power density while integrating the resonance and\nfilter capacitors. The worst-case settling of the on- and off-delay\ncompensation in the active rectifier is 200 ns, which is the fastest reported\nto date.", "AI": {"tldr": "本文介绍了一种40.68 MHz无线能量传输接收器，专注于减少组织加热，采用新颖的功率辐射效率优化策略和快速稳定的有源整流器。", "motivation": "为植入式应用设计高效且安全的无线能量传输系统，减少组织加热并支持下行通信。", "method": "采用功率辐射效率优化策略和有源整流器，优化能量传输效率并减少组织加热。", "result": "有源整流器在40nm CMOS工艺下实现93.9%的电压转换比和90.1%的功率转换效率，功率密度达118 mW/mm²，且具有200 ns的最快稳定时间。", "conclusion": "该系统在植入式应用中实现了高效、安全的无线能量传输，并显著减少了组织加热。"}}
{"id": "2506.06603", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.06603", "abs": "https://arxiv.org/abs/2506.06603", "authors": ["Joseph T Colonel", "Carolyn Hagler", "Guiselle Wismer", "Laura Curtis", "Jacqueline Becker", "Juan Wisnivesky", "Alex Federman", "Gaurav Pandey"], "title": "CAtCh: Cognitive Assessment through Cookie Thief", "comment": null, "summary": "Several machine learning algorithms have been developed for the prediction of\nAlzheimer's disease and related dementia (ADRD) from spontaneous speech.\nHowever, none of these algorithms have been translated for the prediction of\nbroader cognitive impairment (CI), which in some cases is a precursor and risk\nfactor of ADRD. In this paper, we evaluated several speech-based open-source\nmethods originally proposed for the prediction of ADRD, as well as methods from\nmultimodal sentiment analysis for the task of predicting CI from patient audio\nrecordings. Results demonstrated that multimodal methods outperformed unimodal\nones for CI prediction, and that acoustics-based approaches performed better\nthan linguistics-based ones. Specifically, interpretable acoustic features\nrelating to affect and prosody were found to significantly outperform\nBERT-based linguistic features and interpretable linguistic features,\nrespectively. All the code developed for this study is available at\nhttps://github.com/JTColonel/catch.", "AI": {"tldr": "本文评估了多种基于语音的开源方法，用于从患者音频记录中预测认知障碍（CI），发现多模态方法优于单模态方法，声学特征优于语言学特征。", "motivation": "现有机器学习算法主要用于预测阿尔茨海默病及相关痴呆（ADRD），但未广泛用于预测更广泛的认知障碍（CI），而CI可能是ADRD的前兆和风险因素。", "method": "评估了基于语音的开源方法（原用于ADRD预测）和多模态情感分析方法，用于从音频记录中预测CI。", "result": "多模态方法优于单模态方法；声学特征（尤其是与情感和韵律相关的可解释特征）显著优于基于BERT的语言学特征和可解释语言学特征。", "conclusion": "声学特征在预测CI方面表现更优，多模态方法具有潜力，相关代码已开源。"}}
{"id": "2506.06729", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.06729", "abs": "https://arxiv.org/abs/2506.06729", "authors": ["Zixian Gao", "Chao Yang", "Zhanhui Zhou", "Xing Xu", "Chaochao Lu"], "title": "Mitigating Object Hallucination via Robust Local Perception Search", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings.", "AI": {"tldr": "论文提出了一种名为LPS的解码方法，用于减少多模态大语言模型中的幻觉现象，无需训练且兼容多种模型。", "motivation": "尽管多模态大语言模型在视觉和语言结合方面取得了显著成功，但仍存在输出与图像内容不符的幻觉问题，需要一种简单有效的方法来缓解。", "method": "引入Local Perception Search (LPS)，一种在推理过程中利用局部视觉先验信息作为价值函数来修正解码过程的方法。", "result": "在广泛使用的幻觉基准测试和噪声数据上，LPS显著减少了幻觉现象，尤其在噪声环境下表现优异。", "conclusion": "LPS是一种即插即用的方法，能有效抑制多模态大语言模型的幻觉现象，尤其在噪声场景下效果显著。"}}
{"id": "2506.06443", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2506.06443", "abs": "https://arxiv.org/abs/2506.06443", "authors": ["Luis Pinto"], "title": "Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers", "comment": null, "summary": "Pretrained molecular encoders have become indispensable in computational\nchemistry for tasks such as property prediction and molecular generation.\nHowever, the standard practice of relying solely on final-layer embeddings for\ndownstream tasks may discard valuable information. In this work, we challenge\nthis convention by conducting a comprehensive layer-wise analysis of five\ndiverse molecular encoders across 22 ADMET property prediction tasks. Our\nresults demonstrate that embeddings from intermediate layers consistently\noutperform final-layer representations. Specifically, using fixed embeddings\nfrom the optimal intermediate layers improved downstream performance by an\naverage of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to\nthese intermediate layers yielded even greater average improvements of 8.5%,\nwith performance increases as high as 40.8%, achieving new state-of-the-art\nresults on several benchmarks. Additionally, a strong positive correlation\nbetween fixed embedding performance and finetuning outcomes supports an\nefficient evaluate-then-finetune approach, enabling identification of optimal\nlayers with reduced computational cost. These findings highlight the importance\nof exploring the full representational depth of molecular encoders to achieve\nsubstantial performance improvements and computational efficiency. The code is\nmade publicly available at\nhttps://github.com/luispintoc/Unlocking-Chemical-Insights.", "AI": {"tldr": "研究发现，分子编码器的中间层嵌入比最终层嵌入在ADMET属性预测任务中表现更好，平均提升5.4%，最高达28.6%。微调中间层可进一步提升性能，平均提高8.5%，最高达40.8%。", "motivation": "挑战仅依赖最终层嵌入的常规做法，探索分子编码器各层的潜在价值。", "method": "对五种分子编码器进行分层分析，比较中间层与最终层嵌入在22个ADMET任务中的表现。", "result": "中间层嵌入表现更优，固定嵌入平均提升5.4%，微调后平均提升8.5%，部分任务达到新SOTA。", "conclusion": "充分利用分子编码器的各层表示可显著提升性能与计算效率，建议采用评估后微调的策略。"}}
{"id": "2506.06798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06798", "abs": "https://arxiv.org/abs/2506.06798", "authors": ["Arif Ahmed", "Ritvik Agarwal", "Gaurav Srikar", "Nathaniel Rose", "Parikshit Maini"], "title": "SARAL-Bot: Autonomous Robot for Strawberry Plant Care", "comment": "Awarded Best Written Report @ Robotics Design Challenge (Advanced),\n  ASABE 2024", "summary": "Strawberry farming demands intensive labor for monitoring and maintaining\nplant health. To address this, Team SARAL develops an autonomous robot for the\n2024 ASABE Student Robotics Challenge, capable of navigation, unhealthy leaf\ndetection, and removal. The system addresses labor shortages, reduces costs,\nand supports sustainable farming through vision-based plant assessment. This\nwork demonstrates the potential of robotics to modernize strawberry cultivation\nand enable scalable, intelligent agricultural solutions.", "AI": {"tldr": "Team SARAL开发了一款自主机器人，用于草莓种植中的导航、病叶检测和移除，旨在解决劳动力短缺问题并支持可持续农业。", "motivation": "草莓种植需要大量劳动力监测和维护植物健康，劳动力短缺和成本问题促使开发自动化解决方案。", "method": "机器人采用视觉技术进行植物健康评估，具备导航、病叶检测和移除功能。", "result": "该系统展示了机器人技术在草莓种植中实现智能农业的潜力。", "conclusion": "该研究为草莓种植的现代化和规模化提供了可行的机器人解决方案。"}}
{"id": "2506.06862", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.06862", "abs": "https://arxiv.org/abs/2506.06862", "authors": ["Chenguang Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "comment": "accepted to International Journal of Robotics Research (IJRR). 24\n  pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and\n  AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/", "summary": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "AI": {"tldr": "论文提出了一种多模态空间语言地图（VLMaps和AVLMaps），通过融合预训练多模态特征与3D环境重建，实现了自然语言命令到空间目标的翻译，并支持多模态目标导航。", "motivation": "现有方法在环境映射中缺乏空间精度或多模态信息整合，无法满足复杂导航需求。", "method": "构建视觉-语言地图（VLMaps）和音频-视觉-语言地图（AVLMaps），结合预训练多模态特征与3D重建，并通过大型语言模型（LLMs）实现目标定位。", "result": "实验证明，该方法在模拟和真实环境中实现了零样本多模态目标导航，并在模糊场景中召回率提升了50%。", "conclusion": "多模态空间语言地图为机器人导航和交互提供了更灵活、精确的解决方案，支持视觉、音频和空间线索的整合。"}}
{"id": "2506.06733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06733", "abs": "https://arxiv.org/abs/2506.06733", "authors": ["Ruoxuan Zhang", "Jidong Gao", "Bin Wen", "Hongxia Xie", "Chenming Zhang", "Honghan-shuai", "Wen-Huang Cheng"], "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation", "comment": "This is an extended version of arXiv:2503.05228", "summary": "Creating recipe images is a key challenge in food computing, with\napplications in culinary education and multimodal recipe assistants. However,\nexisting datasets lack fine-grained alignment between recipe goals, step-wise\ninstructions, and visual content. We present RecipeGen, the first large-scale,\nreal-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video\n(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,\n196,724 images, and 4,491 videos, covering diverse ingredients, cooking\nprocedures, styles, and dish types. We further propose domain-specific\nevaluation metrics to assess ingredient fidelity and interaction modeling,\nbenchmark representative T2I, I2V, and T2V models, and provide insights for\nfuture recipe generation models. Project page is available now.", "AI": {"tldr": "RecipeGen是一个大规模、真实世界的基准数据集，用于基于食谱的文本到图像（T2I）、图像到视频（I2V）和文本到视频（T2V）生成，填补了现有数据集中细粒度对齐的不足。", "motivation": "解决现有数据集中食谱目标、步骤指令和视觉内容之间缺乏细粒度对齐的问题，推动食品计算在烹饪教育和多模态食谱助手中的应用。", "method": "提出RecipeGen数据集，包含26,453个食谱、196,724张图像和4,491个视频，覆盖多样化的食材、烹饪过程、风格和菜品类型，并设计领域特定的评估指标。", "result": "通过评估代表性模型，提供了未来食谱生成模型的改进方向。", "conclusion": "RecipeGen为食谱生成任务提供了重要基准，并推动了相关领域的研究进展。"}}
{"id": "2506.06444", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.06444", "abs": "https://arxiv.org/abs/2506.06444", "authors": ["Ruizhong Qiu", "Gaotang Li", "Tianxin Wei", "Jingrui He", "Hanghang Tong"], "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance", "comment": "19 pages", "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .", "AI": {"tldr": "论文提出SAFFRON方法，通过推理扩展增强LLM安全性，解决传统方法在安全场景下的低效问题。", "motivation": "现有安全保证方法易受攻击，推理扩展虽提升LLM推理能力，但在安全领域未充分探索。", "method": "提出SAFFRON框架，引入多叉奖励模型（MRM）减少评估次数，结合部分监督训练、保守探索约束和Trie缓存策略。", "result": "实验验证SAFFRON有效性，并公开模型Saffron-1和数据集Safety4M。", "conclusion": "SAFFRON为LLM安全提供高效解决方案，推动未来研究。"}}
{"id": "2506.06804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06804", "abs": "https://arxiv.org/abs/2506.06804", "authors": ["Hongming Chen", "Yiyang Lin", "Ziliang Li", "Biyu Ye", "Yuying Zhang", "Ximin Lyu"], "title": "IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion", "comment": null, "summary": "Indoor scene understanding remains a fundamental challenge in robotics, with\ndirect implications for downstream tasks such as navigation and manipulation.\nTraditional approaches often rely on closed-set recognition or loop closure,\nlimiting their adaptability in open-world environments. With the advent of\nvisual foundation models (VFMs), open-vocabulary recognition and natural\nlanguage querying have become feasible, unlocking new possibilities for 3D\nscene graph construction.\n  In this paper, we propose a robust and efficient framework for instance-level\n3D scene graph construction via LiDAR-camera fusion. Leveraging LiDAR's wide\nfield of view (FOV) and long-range sensing capabilities, we rapidly acquire\nroom-level geometric priors. Multi-level VFMs are employed to improve the\naccuracy and consistency of semantic extraction. During instance fusion,\nroom-based segmentation enables parallel processing, while the integration of\ngeometric and semantic cues significantly enhances fusion accuracy and\nrobustness. Compared to state-of-the-art methods, our approach achieves up to\nan order-of-magnitude improvement in construction speed while maintaining high\nsemantic precision.\n  Extensive experiments in both simulated and real-world environments validate\nthe effectiveness of our approach. We further demonstrate its practical value\nthrough a language-guided semantic navigation task, highlighting its potential\nfor real-world robotic applications.", "AI": {"tldr": "提出了一种基于LiDAR-相机融合的3D场景图构建框架，结合几何与语义信息，显著提升了构建速度和语义精度。", "motivation": "室内场景理解对机器人任务至关重要，但传统方法在开放环境中的适应性有限。视觉基础模型的出现为开放词汇识别和自然语言查询提供了可能。", "method": "利用LiDAR的广视角和远距离感知能力获取几何先验，结合多级视觉基础模型提升语义提取精度，通过并行处理和几何语义融合增强鲁棒性。", "result": "实验表明，该方法在构建速度上比现有方法提升了一个数量级，同时保持高语义精度，并在语言引导的语义导航任务中验证了实用性。", "conclusion": "该框架为机器人应用中的3D场景理解提供了高效且鲁棒的解决方案，具有实际应用潜力。"}}
{"id": "2506.07877", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.07877", "abs": "https://arxiv.org/abs/2506.07877", "authors": ["Andrea Tiranti", "Francesco Wanderlingh", "Enrico Simetti", "Marco Baglietto", "Giovanni Indiveri", "Antonio Pascoal"], "title": "A distributed motion planning approach to cooperative underwater acoustic source tracking and pursuit", "comment": null, "summary": "This paper addresses the problem of underwater acoustic source tracking and\npursuit with a team of autonomous underwater vehicles. Producing distributed\ncontrol strategies in an underwater sensor network is not trivial since\ncommunication is primarily acoustic, which makes it intermittent and often\nplagued with major difficulties. For this reason, we propose an optimization\nscheme based on a Partially Observable Markov Decision Process for improving\nthe performance of underwater mobile sensor networks, in which autonomous\nunderwater vehicles (agents) play the role of moving nodes of a network. The\nkey idea is to adjust the agents' guidance strategies to achieve coordinated\nmotion planning, enabling optimal geometric configurations between the agents\nand the target to enhance tracking performance. Such a problem is cast as a\nmulti-objective optimization problem that is solved through a receding horizon\nlookahead optimization scheme since we are interested in long-term tracking\naccuracy. The planning strategy is distributed using the sequential multi-agent\ndecision-making paradigm to make the solving tractable since the optimization\ndepends on the joint action domain. A distributed control framework has been\nimplemented in a simulation environment to validate the proposed approach,\nwhich explicitly accounts for the major limitations imposed by acoustic\ncommunications.", "AI": {"tldr": "本文提出了一种基于部分可观测马尔可夫决策过程的优化方案，用于提升水下移动传感器网络的性能，通过调整自主水下车辆的导航策略实现协调运动规划。", "motivation": "水下声源追踪与追踪团队的控制策略设计面临通信困难，主要由于声学通信的间歇性和不可靠性。", "method": "采用部分可观测马尔可夫决策过程和多目标优化问题，通过分布式控制框架和滚动时域优化方案实现长期追踪精度。", "result": "仿真环境中验证了所提出的分布式控制框架，能够有效应对声学通信的限制。", "conclusion": "该方法通过协调运动规划和优化几何配置，显著提升了水下声源追踪的性能。"}}
{"id": "2506.07078", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.07078", "abs": "https://arxiv.org/abs/2506.07078", "authors": ["Jiaheng Dong", "Hong Jia", "Soumyajit Chatterjee", "Abhirup Ghosh", "James Bailey", "Ting Dang"], "title": "E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models", "comment": "Under Review", "summary": "Speech Foundation Models encounter significant performance degradation when\ndeployed in real-world scenarios involving acoustic domain shifts, such as\nbackground noise and speaker accents. Test-time adaptation (TTA) has recently\nemerged as a viable strategy to address such domain shifts at inference time\nwithout requiring access to source data or labels. However, existing TTA\napproaches, particularly those relying on backpropagation, are\nmemory-intensive, limiting their applicability in speech tasks and\nresource-constrained settings. Although backpropagation-free methods offer\nimproved efficiency, existing ones exhibit poor accuracy. This is because they\nare predominantly developed for vision tasks, which fundamentally differ from\nspeech task formulations, noise characteristics, and model architecture, posing\nunique transferability challenges. In this paper, we introduce E-BATS, the\nfirst Efficient BAckpropagation-free TTA framework designed explicitly for\nspeech foundation models. E-BATS achieves a balance between adaptation\neffectiveness and memory efficiency through three key components: (i)\nlightweight prompt adaptation for a forward-pass-based feature alignment, (ii)\na multi-scale loss to capture both global (utterance-level) and local\ndistribution shifts (token-level) and (iii) a test-time exponential moving\naverage mechanism for stable adaptation across utterances. Experiments\nconducted on four noisy speech datasets spanning sixteen acoustic conditions\ndemonstrate consistent improvements, with 4.1%-13.5% accuracy gains over\nbackpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to\nbackpropagation-based methods. By enabling scalable and robust adaptation under\nacoustic variability, this work paves the way for developing more efficient\nadaptation approaches for practical speech processing systems in real-world\nenvironments.", "AI": {"tldr": "E-BATS是一种高效的无反向传播测试时间适应框架，专为语音基础模型设计，通过轻量级提示适应、多尺度损失和测试时间指数移动平均机制，平衡了适应效果和内存效率。", "motivation": "语音基础模型在现实场景中因声学域偏移（如背景噪声和口音）性能下降，现有测试时间适应方法要么内存消耗大，要么准确性低。", "method": "E-BATS包含三个关键组件：轻量级提示适应、多尺度损失和测试时间指数移动平均机制。", "result": "在四个噪声语音数据集上的实验表明，E-BATS比无反向传播基线准确率提高4.1%-13.5%，内存节省2.0-6.4倍。", "conclusion": "E-BATS为实际语音处理系统提供了高效且鲁棒的适应方法，适用于现实环境中的声学变化。"}}
{"id": "2506.07275", "categories": ["cs.LG", "cs.HC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.07275", "abs": "https://arxiv.org/abs/2506.07275", "authors": ["Haochen Song", "Dominik Hofer", "Rania Islambouli", "Laura Hawkins", "Ananya Bhattacharjee", "Meredith Franklin", "Joseph Jay Williams"], "title": "Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models", "comment": null, "summary": "Machine learning approaches, such as contextual multi-armed bandit (cMAB)\nalgorithms, offer a promising strategy to reduce sedentary behavior by\ndelivering personalized interventions to encourage physical activity. However,\ncMAB algorithms typically require large participant samples to learn\neffectively and may overlook key psychological factors that are not explicitly\nencoded in the model. In this study, we propose a hybrid approach that combines\ncMAB for selecting intervention types with large language models (LLMs) to\npersonalize message content. We evaluate four intervention types: behavioral\nself-monitoring, gain-framed, loss-framed, and social comparison, each\ndelivered as a motivational message aimed at increasing motivation for physical\nactivity and daily step count. Message content is further personalized using\ndynamic contextual factors including daily fluctuations in self-efficacy,\nsocial influence, and regulatory focus. Over a seven-day trial, participants\nreceive daily messages assigned by one of four models: cMAB alone, LLM alone,\ncombined cMAB with LLM personalization (cMABxLLM), or equal randomization\n(RCT). Outcomes include daily step count and message acceptance, assessed via\necological momentary assessments (EMAs). We apply a causal inference framework\nto evaluate the effects of each model. Our findings offer new insights into the\ncomplementary roles of LLM-based personalization and cMAB adaptation in\npromoting physical activity through personalized behavioral messaging.", "AI": {"tldr": "论文提出了一种结合cMAB和LLM的混合方法，用于个性化干预以增加体育活动，并通过实验评估其效果。", "motivation": "传统cMAB算法需要大样本且可能忽略心理因素，因此结合LLM以提升个性化干预效果。", "method": "采用cMAB选择干预类型，LLM个性化消息内容，评估四种干预类型和四种模型的效果。", "result": "通过因果推断框架评估，发现cMAB和LLM结合在促进体育活动方面具有互补作用。", "conclusion": "cMAB与LLM结合的方法为个性化行为干预提供了新思路。"}}
{"id": "2506.06748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06748", "abs": "https://arxiv.org/abs/2506.06748", "authors": ["Mingqi Gao", "Haoran Duan", "Tianlu Zhang", "Jungong Han"], "title": "THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation", "comment": null, "summary": "In this report, we describe our approach to egocentric video object\nsegmentation. Our method combines large-scale visual pretraining from SAM2 with\ndepth-based geometric cues to handle complex scenes and long-term tracking. By\nintegrating these signals in a unified framework, we achieve strong\nsegmentation performance. On the VISOR test set, our method reaches a J&F score\nof 90.1%.", "AI": {"tldr": "本文提出了一种结合视觉预训练和深度几何线索的自中心视频对象分割方法，性能优异。", "motivation": "解决复杂场景和长期跟踪中的视频对象分割问题。", "method": "结合SAM2的大规模视觉预训练和深度几何线索，构建统一框架。", "result": "在VISOR测试集上J&F得分达到90.1%。", "conclusion": "该方法通过多信号融合显著提升了分割性能。"}}
{"id": "2506.06454", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06454", "abs": "https://arxiv.org/abs/2506.06454", "authors": ["Abrar Majeedi", "Viswanatha Reddy Gajjala", "Satya Sai Srinath Namburi GNVV", "Nada Magdi Elkordi", "Yin Li"], "title": "LETS Forecast: Learning Embedology for Time Series Forecasting", "comment": "Accepted at International Conference on Machine Learning (ICML) 2025", "summary": "Real-world time series are often governed by complex nonlinear dynamics.\nUnderstanding these underlying dynamics is crucial for precise future\nprediction. While deep learning has achieved major success in time series\nforecasting, many existing approaches do not explicitly model the dynamics. To\nbridge this gap, we introduce DeepEDM, a framework that integrates nonlinear\ndynamical systems modeling with deep neural networks. Inspired by empirical\ndynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel\ndeep model that learns a latent space from time-delayed embeddings, and employs\nkernel regression to approximate the underlying dynamics, while leveraging\nefficient implementation of softmax attention and allowing for accurate\nprediction of future time steps. To evaluate our method, we conduct\ncomprehensive experiments on synthetic data of nonlinear dynamical systems as\nwell as real-world time series across domains. Our results show that DeepEDM is\nrobust to input noise, and outperforms state-of-the-art methods in forecasting\naccuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.", "AI": {"tldr": "DeepEDM是一个结合非线性动力学建模与深度学习的框架，通过时间延迟嵌入学习潜在空间，利用核回归近似动力学，并在预测准确性上优于现有方法。", "motivation": "现实世界的时间序列通常具有复杂的非线性动力学特性，现有深度学习方法未能明确建模这些动力学，因此需要一种新框架来填补这一空白。", "method": "DeepEDM结合经验动态建模（EDM）和Takens定理，通过时间延迟嵌入学习潜在空间，使用核回归近似动力学，并采用高效的softmax注意力机制。", "result": "实验表明，DeepEDM对输入噪声具有鲁棒性，并在合成数据和真实世界时间序列的预测准确性上优于现有方法。", "conclusion": "DeepEDM成功地将非线性动力学建模与深度学习结合，显著提升了时间序列预测的准确性。"}}
{"id": "2506.06811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06811", "abs": "https://arxiv.org/abs/2506.06811", "authors": ["Shahid Mohammad Mulla", "Aryan Kanakapudi", "Lakshmi Narasimhan", "Anuj Tiwari"], "title": "RF-Source Seeking with Obstacle Avoidance using Real-time Modified Artificial Potential Fields in Unknown Environments", "comment": "14 pages, 16 figures, 1 table, shorter version under review for IEEE\n  ICCAS 2025 conference", "summary": "Navigation of UAVs in unknown environments with obstacles is essential for\napplications in disaster response and infrastructure monitoring. However,\nexisting obstacle avoidance algorithms, such as Artificial Potential Field\n(APF) are unable to generalize across environments with different obstacle\nconfigurations. Furthermore, the precise location of the final target may not\nbe available in applications such as search and rescue, in which case\napproaches such as RF source seeking can be used to align towards the target\nlocation. This paper proposes a real-time trajectory planning method, which\ninvolves real-time adaptation of APF through a sampling-based approach. The\nproposed approach utilizes only the bearing angle of the target without its\nprecise location, and adjusts the potential field parameters according to the\nenvironment with new obstacle configurations in real time. The main\ncontributions of the article are i) an RF source seeking algorithm to provide a\nbearing angle estimate using RF signal calculations based on antenna placement,\nand ii) a modified APF for adaptable collision avoidance in changing\nenvironments, which are evaluated separately in the simulation software Gazebo,\nusing ROS2 for communication. Simulation results show that the RF\nsource-seeking algorithm achieves high accuracy, with an average angular error\nof just 1.48 degrees, and with this estimate, the proposed navigation algorithm\nimproves the success rate of reaching the target by 46% and reduces the\ntrajectory length by 1.2% compared to standard potential fields.", "AI": {"tldr": "论文提出了一种实时轨迹规划方法，结合RF源追踪和自适应人工势场（APF），用于无人机在未知环境中的导航。", "motivation": "现有障碍物避障算法（如APF）无法适应不同障碍物配置的环境，且目标位置可能不精确，如搜救任务中。", "method": "通过采样方法实时调整APF参数，仅利用目标的方位角（通过RF信号计算），无需精确位置。", "result": "仿真结果显示，RF源追踪算法平均角度误差仅1.48度，导航算法成功率达到目标提升46%，轨迹长度减少1.2%。", "conclusion": "该方法在未知环境中显著提升了无人机导航的适应性和效率。"}}
{"id": "2506.06459", "categories": ["cs.LG", "cs.ET", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.06459", "abs": "https://arxiv.org/abs/2506.06459", "authors": ["Ruitao Chen", "Mozhang Guo", "Jinge Li"], "title": "Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control", "comment": null, "summary": "Automated driving (AD) has substantially improved vehicle safety and driving\ncomfort, but their impact on passenger well-being, particularly infant sleep,\nis not sufficiently studied. Sudden acceleration, abrupt braking, and sharp\nmaneuvers can disrupt infant sleep, compromising both passenger comfort and\nparental convenience. To solve this problem, this paper explores the\nintegration of reinforcement learning (RL) within AD to personalize driving\nbehavior and optimally balance occupant comfort and travel efficiency. In\nparticular, we propose an intelligent cruise control framework that adapts to\nvarying driving conditions to enhance infant sleep quality by effectively\nsynergizing wearable sensing and vehicle data. Long short-term memory (LSTM)\nand transformer-based neural networks are integrated with RL to model the\nrelationship between driving behavior and infant sleep quality under diverse\ntraffic and road conditions. Based on the sleep quality indicators from the\nwearable sensors, driving action data from vehicle controllers, and map data\nfrom map applications, the model dynamically computes the optimal driving\naggressiveness level, which is subsequently translated into specific AD control\nstrategies, e.g., the magnitude and frequency of acceleration, lane change, and\novertaking. Simulation results demonstrate that the proposed solution\nsignificantly improves infant sleep quality compared to baseline methods, while\npreserving desirable travel efficiency.", "AI": {"tldr": "论文提出了一种结合强化学习和智能巡航控制的框架，通过个性化驾驶行为优化婴儿睡眠质量，同时保持出行效率。", "motivation": "研究自动驾驶对婴儿睡眠的影响，解决因突然加速或急刹车等行为导致的睡眠干扰问题。", "method": "整合LSTM和Transformer神经网络与强化学习，结合穿戴设备数据和车辆数据，动态计算最优驾驶策略。", "result": "仿真结果表明，该方法显著提高了婴儿睡眠质量，同时保持了出行效率。", "conclusion": "提出的框架有效平衡了驾驶舒适性和效率，为自动驾驶中的乘客福祉提供了新思路。"}}
{"id": "2506.06757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06757", "abs": "https://arxiv.org/abs/2506.06757", "authors": ["Ziyu Yue", "Ruixi You", "Feng Xu"], "title": "SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image", "comment": "13 pages, 12 figures", "summary": "To translate synthetic aperture radar (SAR) image into interpretable forms\nfor human understanding is the ultimate goal of SAR advanced information\nretrieval. Existing methods mainly focus on 3D surface reconstruction or local\ngeometric feature extraction of targets, neglecting the role of structural\nmodeling in capturing semantic information. This paper proposes a novel task:\nSAR target structure recovery, which aims to infer the components of a target\nand the structural relationships between its components, specifically symmetry\nand adjacency, from a single-view SAR image. Through learning the structural\nconsistency and geometric diversity across the same type of targets as observed\nin different SAR images, it aims to derive the semantic representation of\ntarget directly from its 2D SAR image. To solve this challenging task, a\ntwo-step algorithmic framework based on structural descriptors is developed.\nSpecifically, in the training phase, it first detects 2D keypoints from real\nSAR images, and then learns the mapping from these keypoints to 3D hierarchical\nstructures using simulated data. During the testing phase, these two steps are\nintegrated to infer the 3D structure from real SAR images. Experimental results\nvalidated the effectiveness of each step and demonstrated, for the first time,\nthat 3D semantic structural representation of aircraft targets can be directly\nderived from a single-view SAR image.", "AI": {"tldr": "本文提出了一种新任务：SAR目标结构恢复，旨在从单视角SAR图像推断目标的组成部分及其结构关系（对称性和相邻性）。通过两步算法框架，结合真实和模拟数据，首次实现了从SAR图像直接获取3D语义结构表示。", "motivation": "现有方法多关注3D表面重建或局部几何特征提取，忽视了结构建模在语义信息捕捉中的作用。本文旨在填补这一空白。", "method": "采用两步算法框架：训练阶段从真实SAR图像检测2D关键点，并通过模拟数据学习这些关键点到3D层次结构的映射；测试阶段整合这两步，从真实SAR图像推断3D结构。", "result": "实验验证了每一步的有效性，首次证明可以从单视角SAR图像直接获取飞机目标的3D语义结构表示。", "conclusion": "该方法为SAR图像的高级信息检索提供了新思路，展示了结构建模在语义信息提取中的潜力。"}}
{"id": "2506.06455", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06455", "abs": "https://arxiv.org/abs/2506.06455", "authors": ["Antonio Jesús Banegas-Luna", "Horacio Pérez-Sánchez", "Carlos Martínez-Cortés"], "title": "WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets", "comment": "27 pages, 11 figures, 2 tables, 13 equations", "summary": "While predictive accuracy is often prioritized in machine learning (ML)\nmodels, interpretability remains essential in scientific and high-stakes\ndomains. However, diverse interpretability algorithms frequently yield\nconflicting explanations, highlighting the need for consensus to harmonize\nresults. In this study, six ML models were trained on six synthetic datasets\nwith known ground truths, utilizing various model-agnostic interpretability\ntechniques. Consensus explanations were generated using established methods and\na novel approach: WISCA (Weighted Scaled Consensus Attributions), which\nintegrates class probability and normalized attributions. WISCA consistently\naligned with the most reliable individual method, underscoring the value of\nrobust consensus strategies in improving explanation reliability.", "AI": {"tldr": "论文提出了一种新的共识方法WISCA，用于整合机器学习模型的可解释性结果，以提高解释的可靠性。", "motivation": "在科学和高风险领域，模型的可解释性至关重要，但现有方法常产生冲突解释，需要共识策略。", "method": "训练六个ML模型于合成数据集，使用多种模型无关解释技术，并提出WISCA方法整合概率和归一化归因。", "result": "WISCA与最可靠的个体方法一致，验证了共识策略的价值。", "conclusion": "WISCA是一种有效的共识方法，可提升解释的可靠性。"}}
{"id": "2506.06532", "categories": ["cs.LG", "cs.AI", "cs.NI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.06532", "abs": "https://arxiv.org/abs/2506.06532", "authors": ["Zijiang Yan", "Hao Zhou", "Jianhua Pei", "Hina Tabassum"], "title": "Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks", "comment": "Accepted in ICML 2025 Workshop on Machine Learning for Wireless\n  Communication and Networks (ML4Wireless)", "summary": "Unmanned aerial vehicles (UAVs) have been widely adopted in various\nreal-world applications. However, the control and optimization of multi-UAV\nsystems remain a significant challenge, particularly in dynamic and constrained\nenvironments. This work explores the joint motion and communication control of\nmultiple UAVs operating within integrated terrestrial and non-terrestrial\nnetworks that include high-altitude platform stations (HAPS). Specifically, we\nconsider an aerial highway scenario in which UAVs must accelerate, decelerate,\nand change lanes to avoid collisions and maintain overall traffic flow.\nDifferent from existing studies, we propose a novel hierarchical and\ncollaborative method based on large language models (LLMs). In our approach, an\nLLM deployed on the HAPS performs UAV access control, while another LLM onboard\neach UAV handles motion planning and control. This LLM-based framework\nleverages the rich knowledge embedded in pre-trained models to enable both\nhigh-level strategic planning and low-level tactical decisions. This\nknowledge-driven paradigm holds great potential for the development of\nnext-generation 3D aerial highway systems. Experimental results demonstrate\nthat our proposed collaborative LLM-based method achieves higher system\nrewards, lower operational costs, and significantly reduced UAV collision rates\ncompared to baseline approaches.", "AI": {"tldr": "本文提出了一种基于大语言模型（LLM）的分层协作方法，用于多无人机（UAV）在动态约束环境中的联合运动与通信控制，显著提升了系统性能。", "motivation": "多无人机系统在动态和受限环境中的控制与优化是一个重要挑战，尤其是在集成地面和非地面网络（如高空平台站HAPS）中。", "method": "采用分层协作的LLM框架，HAPS上的LLM负责无人机接入控制，而每架无人机上的LLM处理运动规划与控制。", "result": "实验表明，该方法在系统奖励、运营成本和无人机碰撞率方面优于基线方法。", "conclusion": "基于LLM的知识驱动范式为下一代3D空中高速公路系统的发展提供了巨大潜力。"}}
{"id": "2506.06759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06759", "abs": "https://arxiv.org/abs/2506.06759", "authors": ["Nidheesh Gorthi", "Kartik Thakral", "Rishabh Ranjan", "Richa Singh", "Mayank Vatsa"], "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security", "comment": "Accepted in Interspeech 2025", "summary": "Biometric authentication systems are increasingly being deployed in critical\napplications, but they remain susceptible to spoofing. Since most of the\nresearch efforts focus on modality-specific anti-spoofing techniques, building\na unified, resource-efficient solution across multiple biometric modalities\nremains a challenge. To address this, we propose LitMAS, a\n$\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal\n$\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing\nattacks in speech, face, iris, and fingerprint-based biometric systems. At the\ncore of LitMAS is a Modality-Aligned Concentration Loss, which enhances\ninter-class separability while preserving cross-modal consistency and enabling\nrobust spoof detection across diverse biometric traits. With just 6M\nparameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average\nEER across seven datasets, demonstrating high efficiency, strong\ngeneralizability, and suitability for edge deployment. Code and trained models\nare available at https://github.com/IAB-IITJ/LitMAS.", "AI": {"tldr": "LitMAS是一个轻量级、通用的多模态反欺骗框架，用于检测语音、人脸、虹膜和指纹生物识别系统中的欺骗攻击，性能优于现有方法。", "motivation": "生物识别认证系统易受欺骗攻击，且现有研究多为模态特定的反欺骗技术，缺乏统一的跨模态解决方案。", "method": "提出LitMAS框架，采用模态对齐集中损失（Modality-Aligned Concentration Loss），增强类间分离性并保持跨模态一致性。", "result": "LitMAS仅需6M参数，在七个数据集上的平均EER比现有方法高1.36%，表现出高效性和强泛化能力。", "conclusion": "LitMAS是一种高效、通用的多模态反欺骗解决方案，适合边缘部署。"}}
{"id": "2506.07004", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07004", "abs": "https://arxiv.org/abs/2506.07004", "authors": ["Zhe Huang", "Ye-Ji Mun", "Fatemeh Cheraghi Pouria", "Katherine Driggs-Campbell"], "title": "Hierarchical Intention Tracking with Switching Trees for Real-Time Adaptation to Dynamic Human Intentions during Collaboration", "comment": "15 pages, 10 figures", "summary": "During collaborative tasks, human behavior is guided by multiple levels of\nintentions that evolve over time, such as task sequence preferences and\ninteraction strategies. To adapt to these changing preferences and promptly\ncorrect any inaccurate estimations, collaborative robots must accurately track\nthese dynamic human intentions in real time. We propose a Hierarchical\nIntention Tracking (HIT) algorithm for collaborative robots to track dynamic\nand hierarchical human intentions effectively in real time. HIT represents\nhuman intentions as intention trees with arbitrary depth, and probabilistically\ntracks human intentions by Bayesian filtering, upward measurement propagation,\nand downward posterior propagation across all levels. We develop a HIT-based\nrobotic system that dynamically switches between Interaction-Task and\nVerification-Task trees for a collaborative assembly task, allowing the robot\nto effectively coordinate human intentions at three levels: task-level (subtask\ngoal locations), interaction-level (mode of engagement with the robot), and\nverification-level (confirming or correcting intention recognition). Our user\nstudy shows that our HIT-based collaborative robot system surpasses existing\ncollaborative robot solutions by achieving a balance between efficiency,\nphysical workload, and user comfort while ensuring safety and task completion.\nPost-experiment surveys further reveal that the HIT-based system enhances the\nuser trust and minimizes interruptions to user's task flow through its\neffective understanding of human intentions across multiple levels.", "AI": {"tldr": "提出了一种分层意图跟踪（HIT）算法，用于协作机器人实时跟踪动态和分层的人类意图，显著提升了效率和用户体验。", "motivation": "人类在协作任务中的意图是多层次且动态变化的，机器人需要准确跟踪这些意图以适配变化并纠正错误估计。", "method": "HIT算法通过贝叶斯滤波、向上测量传播和向下后验传播，以任意深度的意图树表示人类意图。", "result": "HIT系统在协作装配任务中优于现有方案，平衡了效率、工作量和舒适度，同时增强了用户信任。", "conclusion": "HIT算法有效提升了协作机器人对多层次人类意图的理解能力，优化了用户体验和任务完成效果。"}}
{"id": "2506.07348", "categories": ["cs.RO", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.07348", "abs": "https://arxiv.org/abs/2506.07348", "authors": ["Pablo Moraes", "Mónica Rodríguez", "Sebastian Barcelona", "Angel Da Silva", "Santiago Fernandez", "Hiago Sodre", "Igor Nunes", "Bruna Guterres", "Ricardo Grando"], "title": "UruBots Autonomous Cars Challenge Pro Team Description Paper for FIRA 2025", "comment": null, "summary": "This paper describes the development of an autonomous car by the UruBots team\nfor the 2025 FIRA Autonomous Cars Challenge (Pro). The project involves\nconstructing a compact electric vehicle, approximately the size of an RC car,\ncapable of autonomous navigation through different tracks. The design\nincorporates mechanical and electronic components and machine learning\nalgorithms that enable the vehicle to make real-time navigation decisions based\non visual input from a camera. We use deep learning models to process camera\nimages and control vehicle movements. Using a dataset of over ten thousand\nimages, we trained a Convolutional Neural Network (CNN) to drive the vehicle\neffectively, through two outputs, steering and throttle. The car completed the\ntrack in under 30 seconds, achieving a pace of approximately 0.4 meters per\nsecond while avoiding obstacles.", "AI": {"tldr": "UruBots团队开发了一款用于2025 FIRA自动驾驶汽车挑战赛的自主小车，采用深度学习模型实现实时导航。", "motivation": "参加2025 FIRA自动驾驶汽车挑战赛，设计一款能自主导航的小型电动车。", "method": "结合机械电子组件和机器学习算法，使用CNN处理摄像头图像，控制车辆的转向和油门。", "result": "车辆在30秒内完成赛道，速度约0.4米/秒，并能避开障碍物。", "conclusion": "项目成功展示了深度学习在小型自动驾驶车辆中的有效应用。"}}
{"id": "2506.06771", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06771", "abs": "https://arxiv.org/abs/2506.06771", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Dorian Cojocaru", "Sorin Grigorescu"], "title": "LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping", "comment": null, "summary": "In this study, we introduce LoopDB, which is a challenging loop closure\ndataset comprising over 1000 images captured across diverse environments,\nincluding parks, indoor scenes, parking spaces, as well as centered around\nindividual objects. Each scene is represented by a sequence of five consecutive\nimages. The dataset was collected using a high resolution camera, providing\nsuitable imagery for benchmarking the accuracy of loop closure algorithms,\ntypically used in simultaneous localization and mapping. As ground truth\ninformation, we provide computed rotations and translations between each\nconsecutive images. Additional to its benchmarking goal, the dataset can be\nused to train and fine-tune loop closure methods based on deep neural networks.\nLoopDB is publicly available at https://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopDB是一个包含1000多张图像的闭环数据集，涵盖多种环境，用于评估和训练闭环算法。", "motivation": "为闭环算法（常用于SLAM）提供高质量的基准数据集，支持深度学习方法的训练和微调。", "method": "使用高分辨率相机采集图像，每场景由五张连续图像组成，并提供旋转和平移的地面真值。", "result": "数据集公开可用，适用于算法评估和深度学习训练。", "conclusion": "LoopDB是一个多功能数据集，支持闭环算法的研究和开发。"}}
{"id": "2506.06482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06482", "abs": "https://arxiv.org/abs/2506.06482", "authors": ["Zhiyuan Zhao", "Juntong Ni", "Shangqing Xu", "Haoxin Liu", "Wei Jin", "B. Aditya Prakash"], "title": "TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness", "comment": "46 pages, 1 figure, 28 tables", "summary": "Time-series forecasting is an essential task with wide real-world\napplications across domains. While recent advances in deep learning have\nenabled time-series forecasting models with accurate predictions, there remains\nconsiderable debate over which architectures and design components, such as\nseries decomposition or normalization, are most effective under varying\nconditions. Existing benchmarks primarily evaluate models at a high level,\noffering limited insight into why certain designs work better. To mitigate this\ngap, we propose TimeRecipe, a unified benchmarking framework that\nsystematically evaluates time-series forecasting methods at the module level.\nTimeRecipe conducts over 10,000 experiments to assess the effectiveness of\nindividual components across a diverse range of datasets, forecasting horizons,\nand task settings. Our results reveal that exhaustive exploration of the design\nspace can yield models that outperform existing state-of-the-art methods and\nuncover meaningful intuitions linking specific design choices to forecasting\nscenarios. Furthermore, we release a practical toolkit within TimeRecipe that\nrecommends suitable model architectures based on these empirical insights. The\nbenchmark is available at: https://github.com/AdityaLab/TimeRecipe.", "AI": {"tldr": "TimeRecipe是一个统一的时间序列预测基准框架，通过模块级评估揭示设计选择的有效性，并推荐最佳模型架构。", "motivation": "解决现有基准在高层次评估模型时无法深入理解设计组件有效性的问题。", "method": "通过10,000多次实验，系统评估不同设计组件在多样化数据集、预测范围和任务设置中的表现。", "result": "发现设计空间的详尽探索能超越现有最优方法，并揭示设计选择与预测场景的关联。", "conclusion": "TimeRecipe提供了实用工具包，基于实证推荐模型架构，填补了现有基准的不足。"}}
{"id": "2506.07006", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07006", "abs": "https://arxiv.org/abs/2506.07006", "authors": ["Zechen Hu", "Tong Xu", "Xuesu Xiao", "Xuan Wang"], "title": "CARoL: Context-aware Adaptation for Robot Learning", "comment": null, "summary": "Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is\noften inefficient. Leveraging prior knowledge has the potential to\nsignificantly enhance learning efficiency, which, however, raises two critical\nchallenges: how to determine the relevancy of existing knowledge and how to\nadaptively integrate them into learning a new task. In this paper, we propose\nContext-aware Adaptation for Robot Learning (CARoL), a novel framework to\nefficiently learn a similar but distinct new task from prior knowledge. CARoL\nincorporates context awareness by analyzing state transitions in system\ndynamics to identify similarities between the new task and prior knowledge. It\nthen utilizes these identified similarities to prioritize and adapt specific\nknowledge pieces for the new task. Additionally, CARoL has a broad\napplicability spanning policy-based, value-based, and actor-critic RL\nalgorithms. We validate the efficiency and generalizability of CARoL on both\nsimulated robotic platforms and physical ground vehicles. The simulations\ninclude CarRacing and LunarLander environments, where CARoL demonstrates faster\nconvergence and higher rewards when learning policies for new tasks. In\nreal-world experiments, we show that CARoL enables a ground vehicle to quickly\nand efficiently adapt policies learned in simulation to smoothly traverse\nreal-world off-road terrain.", "AI": {"tldr": "CARoL框架通过上下文感知和知识适配，提升机器人学习新任务的效率。", "motivation": "利用先验知识提升机器人学习效率，但需解决知识相关性和适应性整合的挑战。", "method": "提出CARoL框架，通过分析状态转移识别任务相似性，并适配先验知识。", "result": "在模拟和真实实验中，CARoL表现出更快收敛和更高奖励。", "conclusion": "CARoL能高效学习新任务，并具有广泛适用性。"}}
{"id": "2506.07540", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.07540", "abs": "https://arxiv.org/abs/2506.07540", "authors": ["Sreeja Roy-Singh", "Sarvesh Kolekar", "Daniel P. Bonny", "Kyle Foss"], "title": "Fractional Collisions: A Framework for Risk Estimation of Counterfactual Conflicts using Autonomous Driving Behavior Simulations", "comment": null, "summary": "We present a methodology for estimating collision risk from counterfactual\nsimulated scenarios built on sensor data from automated driving systems (ADS)\nor naturalistic driving databases. Two-agent conflicts are assessed by\ndetecting and classifying conflict type, identifying the agents' roles\n(initiator or responder), identifying the point of reaction of the responder,\nand modeling their human behavioral expectations as probabilistic\ncounterfactual trajectories. The states are used to compute velocity\ndifferentials at collision, which when combined with crash models, estimates\nseverity of loss in terms of probabilistic injury or property damage,\nhenceforth called fractional collisions. The probabilistic models may also be\nextended to include other uncertainties associated with the simulation,\nfeatures, and agents. We verify the effectiveness of the methodology in a\nsynthetic simulation environment using reconstructed trajectories from 300+\ncollision and near-collision scenes sourced from VTTI's SHRP2 database and\nNexar dashboard camera data. Our methodology predicted fractional collisions\nwithin 1% of ground truth collisions. We then evaluate agent-initiated\ncollision risk of an arbitrary ADS software release by replacing the\nnaturalistic responder in these synthetic reconstructions with an ADS simulator\nand comparing the outcome to human-response outcomes. Our ADS reduced\nnaturalistic collisions by 4x and fractional collision risk by ~62%. The\nframework's utility is also demonstrated on 250k miles of proprietary,\nopen-loop sensor data collected on ADS test vehicles, re-simulated with an\narbitrary ADS software release. The ADS initiated conflicts that caused 0.4\ninjury-causing and 1.7 property-damaging fractional collisions, and the ADS\nimproved collision risk in 96% of the agent-initiated conflicts.", "AI": {"tldr": "提出了一种基于传感器数据估算碰撞风险的方法，通过模拟场景评估冲突类型、角色分配和行为预期，预测碰撞严重性，并在合成环境中验证其准确性。", "motivation": "开发一种能够准确估算自动驾驶系统（ADS）或自然驾驶数据中碰撞风险的方法，以评估和改进ADS的安全性。", "method": "通过检测和分类冲突类型、识别角色（发起者或响应者）、建模行为预期为概率性反事实轨迹，结合速度差和碰撞模型估算碰撞严重性。", "result": "方法在合成环境中预测的碰撞与实际碰撞误差在1%以内；ADS软件版本将自然碰撞风险降低了4倍，分数碰撞风险降低了约62%。", "conclusion": "该框架能有效评估ADS的碰撞风险，并在实际测试中显著降低碰撞风险，证明了其实用性和准确性。"}}
{"id": "2506.06780", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06780", "abs": "https://arxiv.org/abs/2506.06780", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations", "comment": "Extended abstract, presented at the CVPR Workshop on 4D Vision", "summary": "Tracking and forecasting the rotation of objects is fundamental in computer\nvision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor\nobservations can be noisy and sparse, (2) motion patterns can be governed by\ncomplex dynamics, and (3) application settings can demand long-term\nforecasting. This work proposes modeling continuous-time rotational object\ndynamics on $SO(3)$ using Neural Controlled Differential Equations guided by\nSavitzky-Golay paths. Unlike existing methods that rely on simplified motion\nassumptions, our method learns a general latent dynamical system of the\nunderlying object trajectory while respecting the geometric structure of\nrotations. Experimental results on real-world data demonstrate compelling\nforecasting capabilities compared to existing approaches.", "AI": {"tldr": "该论文提出了一种基于神经控制微分方程和Savitzky-Golay路径的方法，用于建模SO(3)上的连续时间旋转物体动力学，解决了噪声、稀疏观测和复杂动态的挑战。", "motivation": "SO(3)外推在计算机视觉和机器人学中具有基础性，但面临噪声、稀疏观测、复杂动态和长期预测需求的挑战。", "method": "使用神经控制微分方程和Savitzky-Golay路径建模SO(3)上的连续时间旋转物体动力学，避免简化运动假设。", "result": "实验结果表明，该方法在真实数据上表现出优于现有方法的预测能力。", "conclusion": "该方法能够学习潜在动态系统并尊重旋转的几何结构，为SO(3)外推提供了有效解决方案。"}}
{"id": "2506.06486", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06486", "abs": "https://arxiv.org/abs/2506.06486", "authors": ["Umit Yigit Basaran", "Sk Miraj Ahmed", "Amit Roy-Chowdhury", "Basak Guler"], "title": "A Certified Unlearning Approach without Access to Source Data", "comment": "Accepted by ICML 2025", "summary": "With the growing adoption of data privacy regulations, the ability to erase\nprivate or copyrighted information from trained models has become a crucial\nrequirement. Traditional unlearning methods often assume access to the complete\ntraining dataset, which is unrealistic in scenarios where the source data is no\nlonger available. To address this challenge, we propose a certified unlearning\nframework that enables effective data removal \\final{without access to the\noriginal training data samples}. Our approach utilizes a surrogate dataset that\napproximates the statistical properties of the source data, allowing for\ncontrolled noise scaling based on the statistical distance between the two.\n\\updated{While our theoretical guarantees assume knowledge of the exact\nstatistical distance, practical implementations typically approximate this\ndistance, resulting in potentially weaker but still meaningful privacy\nguarantees.} This ensures strong guarantees on the model's behavior\npost-unlearning while maintaining its overall utility. We establish theoretical\nbounds, introduce practical noise calibration techniques, and validate our\nmethod through extensive experiments on both synthetic and real-world datasets.\nThe results demonstrate the effectiveness and reliability of our approach in\nprivacy-sensitive settings.", "AI": {"tldr": "提出了一种无需原始训练数据的认证遗忘框架，通过替代数据集近似源数据统计特性，实现有效数据删除。", "motivation": "数据隐私法规日益严格，传统遗忘方法需完整训练数据，但实际中源数据常不可用。", "method": "利用替代数据集近似源数据统计特性，基于统计距离控制噪声缩放，提供理论保证和实用噪声校准技术。", "result": "实验验证了方法在隐私敏感场景下的有效性和可靠性。", "conclusion": "该方法在无需原始数据的情况下，实现了强隐私保证和模型实用性。"}}
{"id": "2506.07062", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07062", "abs": "https://arxiv.org/abs/2506.07062", "authors": ["Dongryung Lee", "Sejune Joo", "Kimin Lee", "Beomjoon Kim"], "title": "Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search", "comment": "The International Journal of Robotics Research (IJRR)", "summary": "The problem of relocating a set of objects to designated areas amidst movable\nobstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)\nproblem, a subclass of task and motion planning (TAMP). Traditional approaches\nto G-TAMP have relied either on domain-independent heuristics or on learning\nfrom planning experience to guide the search, both of which typically demand\nsignificant computational resources or data. In contrast, humans often use\ncommon sense to intuitively decide which objects to manipulate in G-TAMP\nproblems. Inspired by this, we propose leveraging Large Language Models (LLMs),\nwhich have common sense knowledge acquired from internet-scale data, to guide\ntask planning in G-TAMP problems. To enable LLMs to perform geometric\nreasoning, we design a predicate-based prompt that encodes geometric\ninformation derived from a motion planning algorithm. We then query the LLM to\ngenerate a task plan, which is then used to search for a feasible set of\ncontinuous parameters. Since LLMs are prone to mistakes, instead of committing\nto LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action\nspace and use the LLM to guide the search. Unlike the previous approach that\ncalls an LLM at every node and incurs high computational costs, we use it to\nwarm-start the MCTS with the nodes explored in completing the LLM's task plan.\nOn six different G-TAMP problems, we show our method outperforms previous LLM\nplanners and pure search algorithms. Code can be found at:\nhttps://github.com/iMSquared/prime-the-search", "AI": {"tldr": "论文提出了一种利用大型语言模型（LLM）指导几何任务与运动规划（G-TAMP）问题的方法，通过结合LLM的常识推理和蒙特卡洛树搜索（MCTS）提高规划效率。", "motivation": "传统G-TAMP方法依赖启发式或学习经验，计算成本高；而人类常利用常识直觉解决问题，因此尝试用LLM的常识知识辅助规划。", "method": "设计基于谓词的提示，将几何信息编码后输入LLM生成任务计划，再用MCTS搜索可行参数，避免直接依赖LLM输出。", "result": "在六个G-TAMP问题上，该方法优于传统LLM规划器和纯搜索算法。", "conclusion": "结合LLM和MCTS的方法能有效提升G-TAMP问题的解决效率，减少计算资源需求。"}}
{"id": "2506.07876", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.07876", "abs": "https://arxiv.org/abs/2506.07876", "authors": ["Xinghao Zhu", "Yuxin Chen", "Lingfeng Sun", "Farzad Niroui", "Simon Le CleacH", "Jiuguang Wang", "Kuan Fang"], "title": "Versatile Loco-Manipulation through Flexible Interlimb Coordination", "comment": null, "summary": "The ability to flexibly leverage limbs for loco-manipulation is essential for\nenabling autonomous robots to operate in unstructured environments. Yet, prior\nwork on loco-manipulation is often constrained to specific tasks or\npredetermined limb configurations. In this work, we present Reinforcement\nLearning for Interlimb Coordination (ReLIC), an approach that enables versatile\nloco-manipulation through flexible interlimb coordination. The key to our\napproach is an adaptive controller that seamlessly bridges the execution of\nmanipulation motions and the generation of stable gaits based on task demands.\nThrough the interplay between two controller modules, ReLIC dynamically assigns\neach limb for manipulation or locomotion and robustly coordinates them to\nachieve the task success. Using efficient reinforcement learning in simulation,\nReLIC learns to perform stable gaits in accordance with the manipulation goals\nin the real world. To solve diverse and complex tasks, we further propose to\ninterface the learned controller with different types of task specifications,\nincluding target trajectories, contact points, and natural language\ninstructions. Evaluated on 12 real-world tasks that require diverse and complex\ncoordination patterns, ReLIC demonstrates its versatility and robustness by\nachieving a success rate of 78.9% on average. Videos and code can be found at\nhttps://relic-locoman.github.io/.", "AI": {"tldr": "ReLIC通过强化学习实现灵活的肢体协调，支持多任务操作与稳定步态生成，平均任务成功率78.9%。", "motivation": "解决现有肢体协调方法局限于特定任务或固定配置的问题，提升机器人在非结构化环境中的操作能力。", "method": "采用自适应控制器，动态分配肢体功能（操作或移动），并通过强化学习在仿真中训练，最终应用于现实任务。", "result": "在12项复杂任务中平均成功率达78.9%，支持多种任务规范（如轨迹、接触点、自然语言）。", "conclusion": "ReLIC展示了在多样化任务中灵活协调肢体的能力，为机器人自主操作提供了新思路。"}}
{"id": "2506.06802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06802", "abs": "https://arxiv.org/abs/2506.06802", "authors": ["Mohammad Ali Rezaei", "Helia Hajikazem", "Saeed Khanehgir", "Mahdi Javanmardi"], "title": "Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models", "comment": null, "summary": "While diffusion models have demonstrated remarkable generative capabilities,\nexisting style transfer techniques often struggle to maintain identity while\nachieving high-quality stylization. This limitation is particularly acute for\nimages where faces are small or exhibit significant camera-to-face distances,\nfrequently leading to inadequate identity preservation. To address this, we\nintroduce a novel, training-free framework for identity-preserved stylized\nimage synthesis using diffusion models. Key contributions include: (1) the\n\"Mosaic Restored Content Image\" technique, significantly enhancing identity\nretention, especially in complex scenes; and (2) a training-free content\nconsistency loss that enhances the preservation of fine-grained content details\nby directing more attention to the original image during stylization. Our\nexperiments reveal that the proposed approach substantially surpasses the\nbaseline model in concurrently maintaining high stylistic fidelity and robust\nidentity integrity, particularly under conditions of small facial regions or\nsignificant camera-to-face distances, all without necessitating model\nretraining or fine-tuning.", "AI": {"tldr": "提出了一种无需训练的扩散模型框架，用于保持身份的高质量风格化图像合成。", "motivation": "现有风格迁移技术在保持身份和高质量风格化之间存在矛盾，尤其是在小面部或远距离拍摄的图像中。", "method": "采用“马赛克恢复内容图像”技术和无训练内容一致性损失，增强身份保留和内容细节。", "result": "实验表明，该方法在保持高风格保真度和身份完整性方面显著优于基线模型。", "conclusion": "该方法无需重新训练或微调，适用于复杂场景和小面部图像。"}}
{"id": "2506.06488", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06488", "abs": "https://arxiv.org/abs/2506.06488", "authors": ["Pratiksha Thaker", "Neil Kale", "Zhiwei Steven Wu", "Virginia Smith"], "title": "Membership Inference Attacks for Unseen Classes", "comment": "Preprint", "summary": "Shadow model attacks are the state-of-the-art approach for membership\ninference attacks on machine learning models. However, these attacks typically\nassume an adversary has access to a background (nonmember) data distribution\nthat matches the distribution the target model was trained on. We initiate a\nstudy of membership inference attacks where the adversary or auditor cannot\naccess an entire subclass from the distribution -- a more extreme but realistic\nversion of distribution shift than has been studied previously. In this\nsetting, we first show that the performance of shadow model attacks degrades\ncatastrophically, and then demonstrate the promise of another approach,\nquantile regression, that does not have the same limitations. We show that\nquantile regression attacks consistently outperform shadow model attacks in the\nclass dropout setting -- for example, quantile regression attacks achieve up to\n11$\\times$ the TPR of shadow models on the unseen class on CIFAR-100, and\nachieve nontrivial TPR on ImageNet even with 90% of training classes removed.\nWe also provide a theoretical model that illustrates the potential and\nlimitations of this approach.", "AI": {"tldr": "论文研究了在无法访问完整子类数据分布的情况下，成员推理攻击的性能下降问题，并提出了一种基于分位数回归的新方法，其性能优于传统影子模型攻击。", "motivation": "研究在极端但现实的分布偏移情况下，成员推理攻击的有效性，特别是当攻击者无法访问完整子类数据时。", "method": "比较了影子模型攻击和分位数回归攻击在类丢失设置下的性能，并提供了理论模型分析。", "result": "分位数回归攻击在类丢失设置下表现优于影子模型攻击，例如在CIFAR-100上TPR提高了11倍，在ImageNet上即使移除90%的训练类仍能取得显著效果。", "conclusion": "分位数回归攻击在极端分布偏移情况下更具优势，为成员推理攻击提供了新的研究方向。"}}
{"id": "2506.07127", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07127", "abs": "https://arxiv.org/abs/2506.07127", "authors": ["Wenke xia", "Yichu Yang", "Hongtao Wu", "Xiao Ma", "Tao Kong", "Di Hu"], "title": "Robotic Policy Learning via Human-assisted Action Preference Optimization", "comment": null, "summary": "Establishing a reliable and iteratively refined robotic system is essential\nfor deploying real-world applications. While Vision-Language-Action (VLA)\nmodels are widely recognized as the foundation model for such robotic\ndeployment, their dependence on expert demonstrations hinders the crucial\ncapabilities of correction and learning from failures. To mitigate this\nlimitation, we introduce a Human-assisted Action Preference Optimization method\nnamed HAPO, designed to correct deployment failures and foster effective\nadaptation through preference alignment for VLA models. This method begins with\na human-robot collaboration framework for reliable failure correction and\ninteraction trajectory collection through human intervention. These\nhuman-intervention trajectories are further employed within the action\npreference optimization process, facilitating VLA models to mitigate failure\naction occurrences while enhancing corrective action adaptation. Specifically,\nwe propose an adaptive reweighting algorithm to address the issues of\nirreversible interactions and token probability mismatch when introducing\npreference optimization into VLA models, facilitating model learning from\nbinary desirability signals derived from interactions. Through combining these\nmodules, our human-assisted action preference optimization method ensures\nreliable deployment and effective learning from failure for VLA models. The\nexperiments conducted in simulation and real-world scenarios prove superior\ngeneralization and robustness of our framework across a variety of manipulation\ntasks.", "AI": {"tldr": "论文提出了一种名为HAPO的人辅助动作偏好优化方法，旨在解决VLA模型依赖专家演示的问题，通过人类干预纠正失败并优化动作偏好。", "motivation": "VLA模型在机器人部署中依赖专家演示，限制了其从失败中学习和纠正的能力。", "method": "HAPO方法结合人机协作框架收集干预轨迹，并通过自适应重加权算法优化动作偏好，解决不可逆交互和概率不匹配问题。", "result": "实验证明HAPO在模拟和真实场景中具有优异的泛化性和鲁棒性。", "conclusion": "HAPO方法有效提升了VLA模型的可靠部署和从失败中学习的能力。"}}
{"id": "2506.07929", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.07929", "abs": "https://arxiv.org/abs/2506.07929", "authors": ["Amirreza Yasami", "Mohammadali Tofigh", "Mahdi Shahbakhti", "Charles Robert Koch"], "title": "A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle", "comment": null, "summary": "Accurate driving cycle construction is crucial for vehicle design, fuel\neconomy analysis, and environmental impact assessments. A generative\nPhysics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs\nrepresentative driving cycles by capturing transient dynamics, acceleration,\ndeceleration, idling, and road grade transitions while ensuring model fidelity\nis introduced. Leveraging a physics-informed reinforcement learning framework\nwith Monte Carlo sampling, PIESMC delivers efficient cycle construction with\nreduced computational cost. Experimental evaluations on two real-world datasets\ndemonstrate that PIESMC replicates key kinematic and energy metrics, achieving\nup to a 57.3% reduction in cumulative kinematic fragment errors compared to the\nMicro-trip-based (MTB) method and a 10.5% reduction relative to the\nMarkov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude\nfaster than conventional techniques. Analyses of vehicle-specific power\ndistributions and wavelet-transformed frequency content further confirm its\nability to reproduce experimental central tendencies and variability.", "AI": {"tldr": "提出了一种基于物理信息的强化学习方法（PIESMC），用于高效构建代表性驾驶循环，显著降低计算成本并提高准确性。", "motivation": "精确构建驾驶循环对车辆设计、燃油经济性分析和环境影响评估至关重要。", "method": "采用物理信息强化学习框架结合蒙特卡洛采样（PIESMC），捕捉瞬态动态、加减速、怠速和道路坡度变化。", "result": "实验表明，PIESMC在关键运动学和能量指标上表现优异，比MTB方法减少57.3%的误差，比MCB方法减少10.5%，且速度快一个数量级。", "conclusion": "PIESMC能高效准确地构建驾驶循环，适用于实际应用。"}}
{"id": "2506.06818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06818", "abs": "https://arxiv.org/abs/2506.06818", "authors": ["Chao Yin", "Hao Li", "Kequan Yang", "Jide Li", "Pinpin Zhu", "Xiaoqiang Li"], "title": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation", "comment": "under review", "summary": "While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}", "AI": {"tldr": "RDVP-MSD是一种无需训练的自适应框架，通过结合区域约束的双流视觉提示和多模态逐步分解链式思维，解决了伪装目标分割中的语义模糊和空间分离问题，实现了最先进的性能。", "motivation": "现有任务通用提示分割方法在伪装目标分割中存在语义模糊和空间分离问题，导致分割不准确。", "method": "提出RDVP-MSD框架，结合区域约束的双流视觉提示（RDVP）和多模态逐步分解链式思维（MSD-CoT），逐步消除语义模糊并注入空间约束。", "result": "在多个伪装目标分割基准上达到最先进性能，且推理速度更快。", "conclusion": "RDVP-MSD无需训练即可显著提升分割准确性和效率，具有广泛应用潜力。"}}
{"id": "2506.06489", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06489", "abs": "https://arxiv.org/abs/2506.06489", "authors": ["Daniel Kunin", "Giovanni Luca Marchetti", "Feng Chen", "Dhruva Karkada", "James B. Simon", "Michael R. DeWeese", "Surya Ganguli", "Nina Miolane"], "title": "Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks", "comment": "35 pages, 7 figures", "summary": "What features neural networks learn, and how, remains an open question. In\nthis paper, we introduce Alternating Gradient Flows (AGF), an algorithmic\nframework that describes the dynamics of feature learning in two-layer networks\ntrained from small initialization. Prior works have shown that gradient flow in\nthis regime exhibits a staircase-like loss curve, alternating between plateaus\nwhere neurons slowly align to useful directions and sharp drops where neurons\nrapidly grow in norm. AGF approximates this behavior as an alternating two-step\nprocess: maximizing a utility function over dormant neurons and minimizing a\ncost function over active ones. AGF begins with all neurons dormant. At each\nround, a dormant neuron activates, triggering the acquisition of a feature and\na drop in the loss. AGF quantifies the order, timing, and magnitude of these\ndrops, matching experiments across architectures. We show that AGF unifies and\nextends existing saddle-to-saddle analyses in fully connected linear networks\nand attention-only linear transformers, where the learned features are singular\nmodes and principal components, respectively. In diagonal linear networks, we\nprove AGF converges to gradient flow in the limit of vanishing initialization.\nApplying AGF to quadratic networks trained to perform modular addition, we give\nthe first complete characterization of the training dynamics, revealing that\nnetworks learn Fourier features in decreasing order of coefficient magnitude.\nAltogether, AGF offers a promising step towards understanding feature learning\nin neural networks.", "AI": {"tldr": "论文提出交替梯度流（AGF）框架，用于描述小初始化下两层神经网络的特征学习动态，统一并扩展了现有分析。", "motivation": "研究神经网络学习特征的动态过程，填补小初始化下特征学习理解的空白。", "method": "AGF将梯度流近似为交替两步过程：休眠神经元最大化效用函数，活跃神经元最小化成本函数。", "result": "AGF量化了特征获取的顺序、时间和幅度，实验验证了其准确性，并在多种架构中统一了现有分析。", "conclusion": "AGF为理解神经网络特征学习提供了新视角，是迈向全面理解的重要一步。"}}
{"id": "2506.07150", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07150", "abs": "https://arxiv.org/abs/2506.07150", "authors": ["Xintao Yan", "Erdao Liang", "Jiawei Wang", "Haojie Zhu", "Henry X. Liu"], "title": "Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset", "comment": null, "summary": "Datasets pertaining to autonomous vehicles (AVs) hold significant promise for\na range of research fields, including artificial intelligence (AI), autonomous\ndriving, and transportation engineering. Nonetheless, these datasets often\nencounter challenges related to the states of traffic signals, such as missing\nor inaccurate data. Such issues can compromise the reliability of the datasets\nand adversely affect the performance of models developed using them. This\nresearch introduces a fully automated approach designed to tackle these issues\nby utilizing available vehicle trajectory data alongside knowledge from the\ntransportation domain to effectively impute and rectify traffic signal\ninformation within the Waymo Open Motion Dataset (WOMD). The proposed method is\nrobust and flexible, capable of handling diverse intersection geometries and\ntraffic signal configurations in real-world scenarios. Comprehensive\nvalidations have been conducted on the entire WOMD, focusing on over 360,000\nrelevant scenarios involving traffic signals, out of a total of 530,000\nreal-world driving scenarios. In the original dataset, 71.7% of traffic signal\nstates are either missing or unknown, all of which were successfully imputed by\nour proposed method. Furthermore, in the absence of ground-truth signal states,\nthe accuracy of our approach is evaluated based on the rate of red-light\nviolations among vehicle trajectories. Results show that our method reduces the\nestimated red-light running rate from 15.7% in the original data to 2.9%,\nthereby demonstrating its efficacy in rectifying data inaccuracies. This paper\nsignificantly enhances the quality of AV datasets, contributing to the wider AI\nand AV research communities and benefiting various downstream applications. The\ncode and improved traffic signal data are open-sourced at\nhttps://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement", "AI": {"tldr": "本文提出了一种自动化方法，利用车辆轨迹数据和交通领域知识，修复Waymo Open Motion Dataset中缺失或不准确的交通信号状态，显著提升了数据集质量。", "motivation": "自动驾驶车辆数据集在AI和交通工程研究中具有重要价值，但交通信号状态的缺失或不准确会影响数据可靠性。本文旨在解决这一问题。", "method": "结合车辆轨迹数据和交通领域知识，提出一种自动化方法，用于修复和补全交通信号状态。", "result": "在360,000多个交通信号相关场景中，成功补全了71.7%的缺失信号状态，并将红灯违规率从15.7%降至2.9%。", "conclusion": "该方法显著提升了自动驾驶数据集的质量，为AI和自动驾驶研究提供了更可靠的数据支持。"}}
{"id": "2506.06822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06822", "abs": "https://arxiv.org/abs/2506.06822", "authors": ["Chenlu Zhan", "Yufei Zhang", "Gaoang Wang", "Hongwei Wang"], "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting", "comment": null, "summary": "Modeling 3D language fields with Gaussian Splatting for open-ended language\nqueries has recently garnered increasing attention. However, recent 3DGS-based\nmodels leverage view-dependent 2D foundation models to refine 3D semantics but\nlack a unified 3D representation, leading to view inconsistencies.\nAdditionally, inherent open-vocabulary challenges cause inconsistencies in\nobject and relational descriptions, impeding hierarchical semantic\nunderstanding. In this paper, we propose Hi-LSplat, a view-consistent\nHierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.\nTo achieve view-consistent 3D hierarchical semantics, we first lift 2D features\nto 3D features by constructing a 3D hierarchical semantic tree with layered\ninstance clustering, which addresses the view inconsistency issue caused by 2D\nsemantic features. Besides, we introduce instance-wise and part-wise\ncontrastive losses to capture all-sided hierarchical semantic representations.\nNotably, we construct two hierarchical semantic datasets to better assess the\nmodel's ability to distinguish different semantic levels. Extensive experiments\nhighlight our method's superiority in 3D open-vocabulary segmentation and\nlocalization. Its strong performance on hierarchical semantic datasets\nunderscores its ability to capture complex hierarchical semantics within 3D\nscenes.", "AI": {"tldr": "Hi-LSplat提出了一种基于3D高斯溅射的层次化语言模型，解决了现有方法在视图一致性和开放词汇查询中的问题。", "motivation": "现有3DGS模型依赖2D基础模型，导致视图不一致和层次语义理解不足。", "method": "通过构建3D层次语义树和引入实例与部件对比损失，实现视图一致的3D语义表示。", "result": "实验表明Hi-LSplat在开放词汇分割和定位中表现优异，尤其在层次语义数据集上。", "conclusion": "Hi-LSplat能够有效捕捉3D场景中的复杂层次语义，解决了视图一致性和语义一致性问题。"}}
{"id": "2506.06499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06499", "abs": "https://arxiv.org/abs/2506.06499", "authors": ["Alex Havrilla", "Edward Hughes", "Mikayel Samvelyan", "Jacob Abernethy"], "title": "Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms", "comment": null, "summary": "Large language model (LLM) driven synthetic data generation has emerged as a\npowerful method for improving model reasoning capabilities. However, most\nmethods either distill large state-of-the-art models into small students or use\nnatural ground-truth problem statements to guarantee problem statement quality.\nThis limits the scalability of these approaches to more complex and diverse\nproblem domains. To address this, we present SPARQ: Synthetic Problem\nGeneration for Reasoning via Quality-Diversity Algorithms, a novel approach for\ngenerating high-quality and diverse synthetic math problem and solution pairs\nusing only a single model by measuring a problem's solve-rate: a proxy for\nproblem difficulty. Starting from a seed dataset of 7.5K samples, we generate\nover 20 million new problem-solution pairs. We show that filtering the\ngenerated data by difficulty and then fine-tuning the same model on the\nresulting data improves relative model performance by up to 24\\%. Additionally,\nwe conduct ablations studying the impact of synthetic data quantity, quality\nand diversity on model generalization. We find that higher quality, as measured\nby problem difficulty, facilitates better in-distribution performance. Further,\nwhile generating diverse synthetic data does not as strongly benefit\nin-distribution performance, filtering for more diverse data facilitates more\nrobust OOD generalization. We also confirm the existence of model and data\nscaling laws for synthetically generated problems, which positively benefit\ndownstream model generalization.", "AI": {"tldr": "SPARQ利用质量-多样性算法生成高质量、多样化的数学问题-解决方案对，通过单一模型测量问题解决率（难度代理），显著提升模型性能。", "motivation": "解决现有方法在复杂多样化问题领域扩展性不足的问题。", "method": "使用质量-多样性算法生成问题-解决方案对，并通过难度和多样性筛选数据。", "result": "生成2000万对数据，模型性能提升24%；发现高质量数据提升分布内性能，多样性数据增强分布外泛化。", "conclusion": "SPARQ方法有效，且模型和数据规模定律对下游泛化有积极影响。"}}
{"id": "2506.07204", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07204", "abs": "https://arxiv.org/abs/2506.07204", "authors": ["Harsh Modi", "Hao Su", "Xiao Liang", "Minghui Zheng"], "title": "MorphoCopter: Design, Modeling, and Control of a New Transformable Quad-Bi Copter", "comment": null, "summary": "This paper presents a novel morphing quadrotor, named MorphoCopter, covering\nits design, modeling, control, and experimental tests. It features a unique\nsingle rotary joint that enables rapid transformation into an ultra-narrow\nprofile. Although quadrotors have seen widespread adoption in applications such\nas cinematography, agriculture, and disaster management with increasingly\nsophisticated control systems, their hardware configurations have remained\nlargely unchanged, limiting their capabilities in certain environments. Our\ndesign addresses this by enabling the hardware configuration to change on the\nfly when required. In standard flight mode, the MorphoCopter adopts an X\nconfiguration, functioning as a traditional quadcopter, but can quickly fold\ninto a stacked bicopters arrangement or any configuration in between. Existing\nmorphing designs often sacrifice controllability in compact configurations or\nrely on complex multi-joint systems. Moreover, our design achieves a greater\nwidth reduction than any existing solution. We develop a new inertia and\ncontrol-action aware adaptive control system that maintains robust performance\nacross all rotary-joint configurations. The prototype can reduce its width from\n447 mm to 138 mm (nearly 70\\% reduction) in just a few seconds. We validated\nthe MorphoCopter through rigorous simulations and a comprehensive series of\nflight experiments, including robustness tests, trajectory tracking, and\nnarrow-gap passing tests.", "AI": {"tldr": "MorphoCopter是一种新型可变形态四旋翼无人机，通过单旋转关节实现快速变形为超窄形态，解决了传统四旋翼在特定环境中的局限性。", "motivation": "传统四旋翼硬件配置固定，限制了其在某些环境中的应用。MorphoCopter通过可变形态设计解决了这一问题。", "method": "采用单旋转关节设计，支持从X形态快速折叠为堆叠双旋翼形态，开发了自适应控制系统以保持稳定性。", "result": "原型机宽度可从447毫米降至138毫米（减少70%），并通过仿真和飞行实验验证了性能。", "conclusion": "MorphoCopter通过创新的可变形态设计和自适应控制，显著提升了四旋翼在复杂环境中的适应性。"}}
{"id": "2506.06823", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06823", "abs": "https://arxiv.org/abs/2506.06823", "authors": ["Qi Li", "Liangzhi Li", "Zhouqiang Jiang", "Bowen Wang", "Keke Tang"], "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.10992", "summary": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy.", "AI": {"tldr": "本文探讨了视觉提示（VP）在鲁棒源模型下的表现，提出了Prompt Boundary Loosening（PBL）策略以缓解鲁棒性与泛化能力的权衡问题。", "motivation": "研究VP在鲁棒源模型场景下的表现，探索其是否能继承鲁棒性及是否存在鲁棒性与泛化能力的权衡。", "method": "提出PBL策略，作为一种轻量级、即插即用的方法，与VP兼容。", "result": "实验表明PBL能有效继承鲁棒性并提升泛化能力。", "conclusion": "PBL策略在多种数据集上表现优异，验证了其普适性和有效性。"}}
{"id": "2506.06501", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06501", "abs": "https://arxiv.org/abs/2506.06501", "authors": ["Ran Levinstein", "Amit Attia", "Matan Schliserman", "Uri Sherman", "Tomer Koren", "Daniel Soudry", "Itay Evron"], "title": "Optimal Rates in Continual Linear Regression via Increasing Regularization", "comment": null, "summary": "We study realizable continual linear regression under random task orderings,\na common setting for developing continual learning theory. In this setup, the\nworst-case expected loss after $k$ learning iterations admits a lower bound of\n$\\Omega(1/k)$. However, prior work using an unregularized scheme has only\nestablished an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our\npaper proves that this gap can be narrowed, or even closed, using two\nfrequently used regularization schemes: (1) explicit isotropic $\\ell_2$\nregularization, and (2) implicit regularization via finite step budgets. We\nshow that these approaches, which are used in practice to mitigate forgetting,\nreduce to stochastic gradient descent (SGD) on carefully defined surrogate\nlosses. Through this lens, we identify a fixed regularization strength that\nyields a near-optimal rate of $O(\\log k / k)$. Moreover, formalizing and\nanalyzing a generalized variant of SGD for time-varying functions, we derive an\nincreasing regularization strength schedule that provably achieves an optimal\nrate of $O(1/k)$. This suggests that schedules that increase the regularization\ncoefficient or decrease the number of steps per task are beneficial, at least\nin the worst case.", "AI": {"tldr": "论文研究了随机任务顺序下的可实现持续线性回归，通过两种正则化方案缩小了理论上的性能差距，并提出了最优收敛速率的调度策略。", "motivation": "持续学习理论中，随机任务顺序下的最坏情况损失下界为Ω(1/k)，但现有方法的理论上界仅为O(1/k^{1/4})，存在显著差距。论文旨在通过正则化方案缩小或消除这一差距。", "method": "使用两种正则化方案：(1)显式各向同性ℓ2正则化，(2)通过有限步预算的隐式正则化，将其转化为对代理损失的随机梯度下降(SGD)。进一步分析时变函数的广义SGD变体，提出最优调度策略。", "result": "固定正则化强度达到接近最优的收敛速率O(log k / k)，而递增的正则化强度调度则实现了最优速率O(1/k)。", "conclusion": "研究表明，增加正则化系数或减少每任务步数的调度策略在理论上是有益的，尤其是在最坏情况下。"}}
{"id": "2506.07271", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07271", "abs": "https://arxiv.org/abs/2506.07271", "authors": ["Hikaru Sawafuji", "Ryota Ozaki", "Takuto Motomura", "Toyohisa Matsuda", "Masanori Tojima", "Kento Uchida", "Shinichi Shirakawa"], "title": "Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers", "comment": null, "summary": "Self-localization is an important technology for automating bulldozers.\nConventional bulldozer self-localization systems rely on RTK-GNSS (Real Time\nKinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are\nsometimes lost in certain mining conditions. Therefore, self-localization\nmethods that do not depend on RTK-GNSS are required. In this paper, we propose\na machine learning-based self-localization method for bulldozers. The proposed\nmethod consists of two steps: estimating local velocities using a machine\nlearning model from internal sensors, and incorporating these estimates into an\nExtended Kalman Filter (EKF) for global localization. We also created a novel\ndataset for bulldozer odometry and conducted experiments across various driving\nscenarios, including slalom, excavation, and driving on slopes. The result\ndemonstrated that the proposed self-localization method suppressed the\naccumulation of position errors compared to kinematics-based methods,\nespecially when slip occurred. Furthermore, this study showed that\nbulldozer-specific sensors, such as blade position sensors and hydraulic\npressure sensors, contributed to improving self-localization accuracy.", "AI": {"tldr": "提出一种基于机器学习的推土机自定位方法，通过内部传感器估计局部速度并结合扩展卡尔曼滤波（EKF）实现全局定位，实验证明其优于传统运动学方法。", "motivation": "RTK-GNSS信号在采矿环境中可能丢失，需要不依赖RTK-GNSS的自定位方法。", "method": "分为两步：1）使用机器学习模型从内部传感器估计局部速度；2）将估计值输入EKF进行全局定位。", "result": "实验表明，该方法能有效抑制位置误差累积，尤其在打滑情况下，且推土机专用传感器（如刀片位置和液压压力传感器）提升了定位精度。", "conclusion": "基于机器学习的自定位方法在推土机应用中具有潜力，尤其在GNSS信号不可靠的环境中。"}}
{"id": "2506.06826", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06826", "abs": "https://arxiv.org/abs/2506.06826", "authors": ["Chenfei Yuan", "Nanshan Jia", "Hangqi Li", "Peter W. Glynn", "Zeyu Zheng"], "title": "Controllable Coupled Image Generation via Diffusion Models", "comment": null, "summary": "We provide an attention-level control method for the task of coupled image\ngeneration, where \"coupled\" means that multiple simultaneously generated images\nare expected to have the same or very similar backgrounds. While backgrounds\ncoupled, the centered objects in the generated images are still expected to\nenjoy the flexibility raised from different text prompts. The proposed method\ndisentangles the background and entity components in the model's\ncross-attention modules, attached with a sequence of time-varying weight\ncontrol parameters depending on the time step of sampling. We optimize this\nsequence of weight control parameters with a combined objective that assesses\nhow coupled the backgrounds are as well as text-to-image alignment and overall\nvisual quality. Empirical results demonstrate that our method outperforms\nexisting approaches across these criteria.", "AI": {"tldr": "提出了一种注意力级别控制方法，用于耦合图像生成任务，确保背景相似的同时保持中心对象的灵活性。", "motivation": "解决多图像生成中背景耦合与对象灵活性的平衡问题。", "method": "通过解耦背景和实体组件，并引入时间变化的权重控制参数进行优化。", "result": "在背景耦合、文本对齐和视觉质量上优于现有方法。", "conclusion": "该方法有效实现了背景耦合与对象灵活性的统一。"}}
{"id": "2506.06505", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.06505", "abs": "https://arxiv.org/abs/2506.06505", "authors": ["Keisuke Sugiura", "Hiroki Matsutani"], "title": "InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models", "comment": null, "summary": "Training deep neural networks (DNNs) requires significantly more computation\nand memory than inference, making runtime adaptation of DNNs challenging on\nresource-limited IoT platforms. We propose InstantFT, an FPGA-based method for\nultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and\nbackward computations in parameter-efficient fine-tuning (PEFT). Experiments on\ndatasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained\nCNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,\nwhile achieving comparable accuracy. Our FPGA-based InstantFT reduces the\nfine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,\nenabling on-the-fly adaptation of CNNs to non-stationary data distributions.", "AI": {"tldr": "InstantFT是一种基于FPGA的超快速CNN微调方法，显著提升IoT设备上的计算效率和能源效率。", "motivation": "解决在资源有限的IoT平台上，深度神经网络（DNNs）训练计算和内存需求高的问题。", "method": "通过优化参数高效微调（PEFT）中的前向和反向计算，实现超快速CNN微调。", "result": "实验显示，InstantFT比现有LoRA方法快17.4倍，微调时间仅0.36秒，能源效率提升16.3倍。", "conclusion": "InstantFT能够实时适应非稳态数据分布，适用于IoT设备。"}}
{"id": "2506.07283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07283", "abs": "https://arxiv.org/abs/2506.07283", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Jie Zhao"], "title": "Model Analysis And Design Of Ellipse Based Segmented Varying Curved Foot For Biped Robot Walking", "comment": null, "summary": "This paper presents the modeling, design, and experimental validation of an\nEllipse-based Segmented Varying Curvature (ESVC) foot for bipedal robots.\nInspired by the segmented curvature rollover shape of human feet, the ESVC foot\naims to enhance gait energy efficiency while maintaining analytical\ntractability for foot location based controller. First, we derive a complete\nanalytical contact model for the ESVC foot by formulating spatial\ntransformations of elliptical segments only using elementary functions. Then a\nnonlinear programming approach is engaged to determine optimal elliptical\nparameters of hind foot and fore foot based on a known mid-foot. An error\ncompensation method is introduced to address approximation inaccuracies in\nrollover length calculation. The proposed ESVC foot is then integrated with a\nHybrid Linear Inverted Pendulum model-based walking controller and validated\nthrough both simulation and physical experiments on the TT II biped robot.\nExperimental results across marking time, sagittal, and lateral walking tasks\nshow that the ESVC foot consistently reduces energy consumption compared to\nline, and flat feet, with up to 18.52\\% improvement in lateral walking. These\nfindings demonstrate that the ESVC foot provides a practical and\nenergy-efficient alternative for real-world bipedal locomotion. The proposed\ndesign methodology also lays a foundation for data-driven foot shape\noptimization in future research.", "AI": {"tldr": "本文提出了一种基于椭圆的分段变曲率（ESVC）足部设计，用于双足机器人，旨在提高步态能效并保持控制器的解析可操作性。通过实验验证，ESVC足部在多种行走任务中显著降低了能耗。", "motivation": "受人类足部分段曲率滚动形状的启发，设计ESVC足部以提升双足机器人的能量效率，同时保持足部位置控制的解析可操作性。", "method": "1. 推导ESVC足部的完整解析接触模型；2. 使用非线性规划优化椭圆参数；3. 引入误差补偿方法；4. 结合混合线性倒立摆模型控制器进行仿真和物理实验验证。", "result": "实验表明，ESVC足部在标记时间、矢状面和横向行走任务中能耗显著降低，横向行走能效提升高达18.52%。", "conclusion": "ESVC足部为双足机器人提供了一种实用且节能的解决方案，并为未来数据驱动的足部形状优化奠定了基础。"}}
{"id": "2506.06830", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06830", "abs": "https://arxiv.org/abs/2506.06830", "authors": ["Guankun Wang", "Rui Tang", "Mengya Xu", "Long Bai", "Huxin Gao", "Hongliang Ren"], "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery", "comment": "Accepted by Advanced Intelligent Systems", "summary": "Endoscopic surgery is the gold standard for robotic-assisted minimally\ninvasive surgery, offering significant advantages in early disease detection\nand precise interventions. However, the complexity of surgical scenes,\ncharacterized by high variability in different surgical activity scenarios and\nconfused image features between targets and the background, presents challenges\nfor surgical environment understanding. Traditional deep learning models often\nstruggle with cross-activity interference, leading to suboptimal performance in\neach downstream task. To address this limitation, we explore multi-task\nlearning, which utilizes the interrelated features between tasks to enhance\noverall task performance. In this paper, we propose EndoARSS, a novel\nmulti-task learning framework specifically designed for endoscopy surgery\nactivity recognition and semantic segmentation. Built upon the DINOv2\nfoundation model, our approach integrates Low-Rank Adaptation to facilitate\nefficient fine-tuning while incorporating Task Efficient Shared Low-Rank\nAdapters to mitigate gradient conflicts across diverse tasks. Additionally, we\nintroduce the Spatially-Aware Multi-Scale Attention that enhances feature\nrepresentation discrimination by enabling cross-spatial learning of global\ninformation. In order to evaluate the effectiveness of our framework, we\npresent three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored\nfor endoscopic surgery scenarios with detailed annotations for both activity\nrecognition and semantic segmentation tasks. Extensive experiments demonstrate\nthat EndoARSS achieves remarkable performance across multiple benchmarks,\nsignificantly improving both accuracy and robustness in comparison to existing\nmodels. These results underscore the potential of EndoARSS to advance AI-driven\nendoscopic surgical systems, offering valuable insights for enhancing surgical\nsafety and efficiency.", "AI": {"tldr": "EndoARSS是一个基于DINOv2的多任务学习框架，用于内窥镜手术活动识别和语义分割，通过低秩适应和空间感知多尺度注意力提升性能。", "motivation": "内窥镜手术场景复杂，传统深度学习模型在跨活动干扰下表现不佳，需多任务学习提升性能。", "method": "提出EndoARSS框架，结合低秩适应和任务共享适配器，引入空间感知多尺度注意力增强特征表示。", "result": "在三个新数据集上表现优异，显著提升准确性和鲁棒性。", "conclusion": "EndoARSS有望推动AI驱动的内窥镜手术系统发展，提升手术安全与效率。"}}
{"id": "2506.06521", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06521", "abs": "https://arxiv.org/abs/2506.06521", "authors": ["Shulun Chen", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "title": "Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs", "comment": "30 pages", "summary": "We consider the gap-dependent regret bounds for episodic MDPs. We show that\nthe Monotonic Value Propagation (MVP) algorithm achieves a variance-aware\ngap-dependent regret bound of $$\\tilde{O}\\left(\\left(\\sum_{\\Delta_h(s,a)>0}\n\\frac{H^2 \\log K \\land \\mathtt{Var}_{\\max}^{\\text{c}}}{\\Delta_h(s,a)}\n+\\sum_{\\Delta_h(s,a)=0}\\frac{ H^2 \\land\n\\mathtt{Var}_{\\max}^{\\text{c}}}{\\Delta_{\\mathrm{min}}} + SAH^4 (S \\lor H)\n\\right) \\log K\\right),$$ where $H$ is the planning horizon, $S$ is the number\nof states, $A$ is the number of actions, and $K$ is the number of episodes.\nHere, $\\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality\ngap and $\\Delta_{\\mathrm{min}} := \\min_{\\Delta_h (s,a) > 0} \\Delta_h(s,a)$. The\nterm $\\mathtt{Var}_{\\max}^{\\text{c}}$ denotes the maximum conditional total\nvariance, calculated as the maximum over all $(\\pi, h, s)$ tuples of the\nexpected total variance under policy $\\pi$ conditioned on trajectories visiting\nstate $s$ at step $h$. $\\mathtt{Var}_{\\max}^{\\text{c}}$ characterizes the\nmaximum randomness encountered when learning any $(h, s)$ pair. Our result\nstems from a novel analysis of the weighted sum of the suboptimality gap and\ncan be potentially adapted for other algorithms. To complement the study, we\nestablish a lower bound of $$\\Omega \\left( \\sum_{\\Delta_h(s,a)>0} \\frac{H^2\n\\land \\mathtt{Var}_{\\max}^{\\text{c}}}{\\Delta_h(s,a)}\\cdot \\log K\\right),$$\ndemonstrating the necessity of dependence on $\\mathtt{Var}_{\\max}^{\\text{c}}$\neven when the maximum unconditional total variance (without conditioning on\n$(h, s)$) approaches zero.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.06836", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06836", "abs": "https://arxiv.org/abs/2506.06836", "authors": ["Zelin He", "Sarah Alnegheimish", "Matthew Reimherr"], "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection", "comment": null, "summary": "Time-series anomaly detection (TSAD) has played a vital role in a variety of\nfields, including healthcare, finance, and industrial monitoring. Prior\nmethods, which mainly focus on training domain-specific models on numerical\ndata, lack the visual-temporal reasoning capacity that human experts have to\nidentify contextual anomalies. To fill this gap, we explore a solution based on\nvision language models (VLMs). Recent studies have shown the ability of VLMs\nfor visual reasoning tasks, yet their direct application to time series has\nfallen short on both accuracy and efficiency. To harness the power of VLMs for\nTSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening\nstage built on a relatively lightweight pretrained vision encoder, which\nleverages 2-D time-series representations to accurately localize candidate\nanomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal\ncontext and VLM reasoning capacity to refine the detection upon the candidates\nprovided by ViT4TS. We show that without any time-series training, VLM4TS\noutperforms time-series pretrained and from-scratch baselines in most cases,\nyielding a 24.6 percent improvement in F1-max score over the best baseline.\nMoreover, VLM4TS also consistently outperforms existing language-model-based\nTSAD methods and is on average 36 times more efficient in token usage.", "AI": {"tldr": "论文提出了一种基于视觉语言模型（VLM）的两阶段时间序列异常检测方法，结合视觉筛选和VLM推理，显著提升了检测性能。", "motivation": "现有方法缺乏视觉-时间推理能力，无法像人类专家那样识别上下文异常。", "method": "提出两阶段方法：ViT4TS（视觉筛选）和VLM4TS（VLM推理），结合2D时间序列表示和全局时间上下文。", "result": "VLM4TS在未进行时间序列训练的情况下，F1-max分数比最佳基线提升24.6%，且效率更高。", "conclusion": "VLM方法在时间序列异常检测中表现出色，兼具高效性和准确性。"}}
{"id": "2506.07325", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.07325", "abs": "https://arxiv.org/abs/2506.07325", "authors": ["Hardik Parwana", "Taekyung Kim", "Kehan Long", "Bardh Hoxha", "Hideki Okamoto", "Georgios Fainekos", "Dimitra Panagou"], "title": "BR-MPPI: Barrier Rate guided MPPI for Enforcing Multiple Inequality Constraints with Learned Signed Distance Field", "comment": null, "summary": "Model Predictive Path Integral (MPPI) controller is used to solve\nunconstrained optimal control problems and Control Barrier Function (CBF) is a\ntool to impose strict inequality constraints, a.k.a, barrier constraints. In\nthis work, we propose an integration of these two methods that employ CBF-like\nconditions to guide the control sampling procedure of MPPI. CBFs provide an\ninequality constraint restricting the rate of change of barrier functions by a\nclassK function of the barrier itself. We instead impose the CBF condition as\nan equality constraint by choosing a parametric linear classK function and\ntreating this parameter as a state in an augmented system. The time derivative\nof this parameter acts as an additional control input that is designed by MPPI.\nA cost function is further designed to reignite Nagumo's theorem at the\nboundary of the safe set by promoting specific values of classK parameter to\nenforce safety. Our problem formulation results in an MPPI subject to multiple\nstate and control-dependent equality constraints which are non-trivial to\nsatisfy with randomly sampled control inputs. We therefore also introduce state\ntransformations and control projection operations, inspired by the literature\non path planning for manifolds, to resolve the aforementioned issue. We show\nempirically through simulations and experiments on quadrotor that our proposed\nalgorithm exhibits better sampled efficiency and enhanced capability to operate\ncloser to the safe set boundary over vanilla MPPI.", "AI": {"tldr": "将MPPI控制器与CBF结合，通过CBF条件指导MPPI的控制采样，提升安全性和采样效率。", "motivation": "解决MPPI在无约束最优控制问题中缺乏严格不等式约束的问题，同时提升其在安全集边界附近的性能。", "method": "将CBF条件转化为等式约束，通过参数化线性类K函数处理，并设计成本函数增强安全性。引入状态变换和控制投影操作以满足约束。", "result": "仿真和四旋翼实验表明，该方法比传统MPPI具有更高的采样效率和更接近安全集边界的能力。", "conclusion": "提出的方法有效整合了MPPI和CBF，显著提升了控制性能和安全操作能力。"}}
{"id": "2506.06846", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06846", "abs": "https://arxiv.org/abs/2506.06846", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui jia"], "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles", "comment": "AAAI 2025", "summary": "In recent years, there has been a growing demand to stylize a given 3D scene\nto align with the artistic style of reference images for creative purposes.\nWhile 3D Gaussian Splatting(GS) has emerged as a promising and efficient method\nfor realistic 3D scene modeling, there remains a challenge in adapting it to\nstylize 3D GS to match with multiple styles through automatic local style\ntransfer or manual designation, while maintaining memory efficiency for\nstylization training. In this paper, we introduce a novel 3D GS stylization\nsolution termed Multi-StyleGS to tackle these challenges. In particular, we\nemploy a bipartite matching mechanism to au tomatically identify\ncorrespondences between the style images and the local regions of the rendered\nimages. To facilitate local style transfer, we introduce a novel semantic style\nloss function that employs a segmentation network to apply distinct styles to\nvarious objects of the scene and propose a local-global feature matching to\nenhance the multi-view consistency. Furthermore, this technique can achieve\nmemory efficient training, more texture details and better color match. To\nbetter assign a robust semantic label to each Gaussian, we propose several\ntechniques to regularize the segmentation network. As demonstrated by our\ncomprehensive experiments, our approach outperforms existing ones in producing\nplausible stylization results and offering flexible editing.", "AI": {"tldr": "论文提出了一种名为Multi-StyleGS的新方法，用于解决3D高斯泼溅（GS）在多风格适配中的挑战，通过自动局部风格转移和语义风格损失函数实现高效训练和灵活编辑。", "motivation": "近年来，对3D场景进行艺术风格化以匹配参考图像的需求增长，但现有方法在适配多风格和保持内存效率方面存在挑战。", "method": "采用二分匹配机制自动识别风格图像与渲染图像局部区域的对应关系，提出语义风格损失函数和局部-全局特征匹配以增强多视角一致性。", "result": "实验表明，该方法在生成合理的风格化结果和提供灵活编辑方面优于现有方法，同时实现了内存高效训练和更好的纹理细节。", "conclusion": "Multi-StyleGS通过创新的语义风格损失和局部-全局匹配机制，有效解决了3D GS风格化的挑战，为未来研究提供了新思路。"}}
{"id": "2506.06549", "categories": ["cs.LG", "cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.06549", "abs": "https://arxiv.org/abs/2506.06549", "authors": ["Atefeh Gilani", "Naima Tasnim", "Lalitha Sankar", "Oliver Kosut"], "title": "GeoClip: Geometry-Aware Clipping for Differentially Private SGD", "comment": null, "summary": "Differentially private stochastic gradient descent (DP-SGD) is the most\nwidely used method for training machine learning models with provable privacy\nguarantees. A key challenge in DP-SGD is setting the per-sample gradient\nclipping threshold, which significantly affects the trade-off between privacy\nand utility. While recent adaptive methods improve performance by adjusting\nthis threshold during training, they operate in the standard coordinate system\nand fail to account for correlations across the coordinates of the gradient. We\npropose GeoClip, a geometry-aware framework that clips and perturbs gradients\nin a transformed basis aligned with the geometry of the gradient distribution.\nGeoClip adaptively estimates this transformation using only previously released\nnoisy gradients, incurring no additional privacy cost. We provide convergence\nguarantees for GeoClip and derive a closed-form solution for the optimal\ntransformation that minimizes the amount of noise added while keeping the\nprobability of gradient clipping under control. Experiments on both tabular and\nimage datasets demonstrate that GeoClip consistently outperforms existing\nadaptive clipping methods under the same privacy budget.", "AI": {"tldr": "GeoClip是一种几何感知框架，通过变换梯度分布的基础来优化DP-SGD中的梯度裁剪和扰动，显著提升隐私与效用的权衡。", "motivation": "解决DP-SGD中梯度裁剪阈值设置的挑战，现有方法未能考虑梯度坐标间的相关性。", "method": "提出GeoClip框架，在变换后的基础上裁剪和扰动梯度，自适应估计变换且不增加隐私成本。", "result": "实验表明，GeoClip在相同隐私预算下优于现有自适应裁剪方法。", "conclusion": "GeoClip通过几何感知的梯度处理，显著提升了DP-SGD的性能。"}}
{"id": "2506.07339", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07339", "abs": "https://arxiv.org/abs/2506.07339", "authors": ["Kevin Black", "Manuel Y. Galliker", "Sergey Levine"], "title": "Real-Time Execution of Action Chunking Flow Policies", "comment": null, "summary": "Modern AI systems, especially those interacting with the physical world,\nincreasingly require real-time performance. However, the high latency of\nstate-of-the-art generalist models, including recent vision-language action\nmodels (VLAs), poses a significant challenge. While action chunking has enabled\ntemporal consistency in high-frequency control tasks, it does not fully address\nthe latency problem, leading to pauses or out-of-distribution jerky movements\nat chunk boundaries. This paper presents a novel inference-time algorithm that\nenables smooth asynchronous execution of action chunking policies. Our method,\nreal-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out\nof the box with no re-training. It generates the next action chunk while\nexecuting the current one, \"freezing\" actions guaranteed to execute and\n\"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly\ndynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging\nreal-world bimanual manipulation tasks. Results demonstrate that RTC is fast,\nperformant, and uniquely robust to inference delay, significantly improving\ntask throughput and enabling high success rates in precise tasks\n$\\unicode{x2013}$ such as lighting a match $\\unicode{x2013}$ even in the\npresence of significant latency. See\nhttps://pi.website/research/real_time_chunking for videos.", "AI": {"tldr": "本文提出了一种实时分块（RTC）算法，用于解决现代AI系统在高频控制任务中的延迟问题，通过异步执行动作分块策略实现平滑操作。", "motivation": "现代AI系统在物理世界交互中需要实时性能，但现有通用模型的高延迟问题导致动作分块边界出现停顿或不连贯动作。", "method": "RTC算法无需重新训练，适用于任何基于扩散或流的视觉语言动作模型，通过在执行当前动作分块时生成下一个分块，实现平滑异步执行。", "result": "在12个高动态任务和6个真实世界双手机器人任务中，RTC显著提高了任务吞吐量和成功率，尤其在存在延迟的情况下表现优异。", "conclusion": "RTC是一种高效、性能优越且对延迟鲁棒的方法，适用于需要高精度和实时性的任务。"}}
{"id": "2506.06850", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.06850", "abs": "https://arxiv.org/abs/2506.06850", "authors": ["Sara M. Cerqueira", "Manuel Palermo", "Cristina P. Santos"], "title": "Deep Inertial Pose: A deep learning approach for human pose estimation", "comment": null, "summary": "Inertial-based Motion capture system has been attracting growing attention\ndue to its wearability and unsconstrained use. However, accurate human joint\nestimation demands several complex and expertise demanding steps, which leads\nto expensive software such as the state-of-the-art MVN Awinda from Xsens\nTechnologies. This work aims to study the use of Neural Networks to abstract\nthe complex biomechanical models and analytical mathematics required for pose\nestimation. Thus, it presents a comparison of different Neural Network\narchitectures and methodologies to understand how accurately these methods can\nestimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)\nMagnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method\nwas the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle\ndistance error of 7.96, using Mtw Awinda data. Also, an ablation study was\nconducted to study the impact of data augmentation, output representation,\nwindow size, loss function and magnetometer data on the pose estimation error.\nThis work indicates that Neural Networks can be trained to estimate human pose,\nwith results comparable to the state-of-the-art fusion filters.", "AI": {"tldr": "论文研究了利用神经网络简化惯性运动捕捉系统中复杂的人体姿态估计方法，比较了不同架构，发现Hybrid LSTM-Madgwick方法效果最佳。", "motivation": "传统惯性运动捕捉系统依赖复杂模型和昂贵软件，研究旨在探索神经网络是否能简化这一过程。", "method": "比较多种神经网络架构和方法，使用低成本和高端的MARG传感器进行实验。", "result": "Hybrid LSTM-Madgwick方法误差最低（7.96），接近现有最优融合滤波器。", "conclusion": "神经网络可用于高效姿态估计，性能接近传统复杂方法。"}}
{"id": "2506.06556", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.06556", "abs": "https://arxiv.org/abs/2506.06556", "authors": ["Long Dang", "Thushari Hapuarachchi", "Kaiqi Xiong", "Yi Li"], "title": "SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks", "comment": "The 34th International Conference on Computer Communications and\n  Networks (ICCCN 2025)", "summary": "As the development of autonomous and connected vehicles advances, the\ncomplexity of modern vehicles increases, with numerous Electronic Control Units\n(ECUs) integrated into the system. In an in-vehicle network, these ECUs\ncommunicate with one another using an standard protocol called Controller Area\nNetwork (CAN). Securing communication among ECUs plays a vital role in\nmaintaining the safety and security of the vehicle. This paper proposes a\nrobust SDN-based False Data Detection and Mitigation System (FDDMS) for\nin-vehicle networks. Leveraging the unique capabilities of Software-Defined\nNetworking (SDN), FDDMS is designed to monitor and detect false data injection\nattacks in real-time. Specifically, we focus on brake-related ECUs within an\nSDN-enabled in-vehicle network. First, we decode raw CAN data to create an\nattack model that illustrates how false data can be injected into the system.\nThen, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection\nmodel, is used to identify false data injection attacks. We further propose an\neffective variant of DeepFool attack to evaluate the model's robustness. To\ncountermeasure the impacts of four adversarial attacks including Fast gradient\ndescent method, Basic iterative method, DeepFool, and the DeepFool variant, we\nfurther enhance a re-training technique method with a threshold based selection\nstrategy. Finally, a mitigation scheme is implemented to redirect attack\ntraffic by dynamically updating flow rules through SDN. Our experimental\nresults show that the proposed FDDMS is robust against adversarial attacks and\neffectively detects and mitigates false data injection attacks in real-time.", "AI": {"tldr": "本文提出了一种基于SDN的虚假数据检测与缓解系统（FDDMS），用于实时监测和防御车载网络中的虚假数据注入攻击。", "motivation": "随着自动驾驶和互联车辆的发展，车载网络中电子控制单元（ECU）的通信安全至关重要，需防止虚假数据注入攻击。", "method": "利用SDN技术，结合LSTM检测模型，设计FDDMS系统，并通过改进的DeepFool攻击评估其鲁棒性。", "result": "实验结果表明，FDDMS能有效实时检测和缓解虚假数据注入攻击。", "conclusion": "FDDMS系统在对抗攻击中表现出鲁棒性，为车载网络安全提供了有效解决方案。"}}
{"id": "2506.07345", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07345", "abs": "https://arxiv.org/abs/2506.07345", "authors": ["Xinling Li", "Meshal Alharbi", "Daniele Gammelli", "James Harrison", "Filipe Rodrigues", "Maximilian Schiffer", "Marco Pavone", "Emilio Frazzoli", "Jinhua Zhao", "Gioele Zardini"], "title": "Reproducibility in the Control of Autonomous Mobility-on-Demand Systems", "comment": null, "summary": "Autonomous Mobility-on-Demand (AMoD) systems, powered by advances in\nrobotics, control, and Machine Learning (ML), offer a promising paradigm for\nfuture urban transportation. AMoD offers fast and personalized travel services\nby leveraging centralized control of autonomous vehicle fleets to optimize\noperations and enhance service performance. However, the rapid growth of this\nfield has outpaced the development of standardized practices for evaluating and\nreporting results, leading to significant challenges in reproducibility. As\nAMoD control algorithms become increasingly complex and data-driven, a lack of\ntransparency in modeling assumptions, experimental setups, and algorithmic\nimplementation hinders scientific progress and undermines confidence in the\nresults. This paper presents a systematic study of reproducibility in AMoD\nresearch. We identify key components across the research pipeline, spanning\nsystem modeling, control problems, simulation design, algorithm specification,\nand evaluation, and analyze common sources of irreproducibility. We survey\nprevalent practices in the literature, highlight gaps, and propose a structured\nframework to assess and improve reproducibility. Specifically, concrete\nguidelines are offered, along with a \"reproducibility checklist\", to support\nfuture work in achieving replicable, comparable, and extensible results. While\nfocused on AMoD, the principles and practices we advocate generalize to a\nbroader class of cyber-physical systems that rely on networked autonomy and\ndata-driven control. This work aims to lay the foundation for a more\ntransparent and reproducible research culture in the design and deployment of\nintelligent mobility systems.", "AI": {"tldr": "本文研究了自动驾驶按需出行（AMoD）研究中的可重复性问题，提出了评估和改进可重复性的结构化框架和具体指南。", "motivation": "AMoD领域的快速发展导致标准化评估和报告实践的缺失，影响了研究的可重复性和科学进展。", "method": "通过系统分析AMoD研究流程中的关键组件（如系统建模、控制问题、仿真设计等），识别不可重复性的常见来源，并提出结构化框架和检查表。", "result": "提出了具体的指南和“可重复性检查表”，以支持未来研究实现可复制、可比较和可扩展的结果。", "conclusion": "本文为智能出行系统的设计和部署奠定了更透明和可重复的研究文化基础，其原则可推广至其他依赖网络化自主性和数据驱动控制的网络物理系统。"}}
{"id": "2506.06852", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06852", "abs": "https://arxiv.org/abs/2506.06852", "authors": ["John Waithaka", "Moise Busogi"], "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation", "comment": null, "summary": "Semantic segmentation of satellite imagery is crucial for Earth observation\napplications, but remains constrained by limited labelled training data. While\nself-supervised pretraining methods like Masked Autoencoders (MAE) have shown\npromise, they focus on reconstruction rather than localisation-a fundamental\naspect of segmentation tasks. We propose adapting LOCA (Location-aware), a\nposition prediction self-supervised learning method, for multimodal satellite\nimagery semantic segmentation. Our approach addresses the unique challenges of\nsatellite data by extending SatMAE's channel grouping from multispectral to\nmultimodal data, enabling effective handling of multiple modalities, and\nintroducing same-group attention masking to encourage cross-modal interaction\nduring pretraining. The method uses relative patch position prediction,\nencouraging spatial reasoning for localisation rather than reconstruction. We\nevaluate our approach on the Sen1Floods11 flood mapping dataset, where it\nsignificantly outperforms existing reconstruction-based self-supervised\nlearning methods for satellite imagery. Our results demonstrate that position\nprediction tasks, when properly adapted for multimodal satellite imagery, learn\nrepresentations more effective for satellite image semantic segmentation than\nreconstruction-based approaches.", "AI": {"tldr": "论文提出了一种基于位置预测的自监督学习方法（LOCA），用于多模态卫星图像语义分割，显著优于现有的基于重建的自监督学习方法。", "motivation": "卫星图像的语义分割对地球观测应用至关重要，但受限于标记训练数据的不足。现有方法（如MAE）侧重于重建而非定位，而定位是分割任务的基础。", "method": "扩展SatMAE的通道分组至多模态数据，引入同组注意力掩码以促进跨模态交互，采用相对补丁位置预测任务以增强空间推理能力。", "result": "在Sen1Floods11洪水测绘数据集上，该方法显著优于基于重建的自监督学习方法。", "conclusion": "针对多模态卫星图像的位置预测任务能学习到更有效的表示，优于基于重建的方法。"}}
{"id": "2506.06854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06854", "abs": "https://arxiv.org/abs/2506.06854", "authors": ["Markus Knoche", "Daan de Geus", "Bastian Leibe"], "title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "comment": null, "summary": "Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Different from existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, enhancing the\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the network the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future, and further improves the\nperformance. With experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.", "AI": {"tldr": "DONUT是一种仅解码器网络，用于预测轨迹，通过自回归模型迭代预测未来轨迹，性能优于编码器-解码器基线。", "motivation": "自动驾驶需要预测其他代理的运动，现有编码器-解码器模型存在信息不一致问题。", "method": "使用仅解码器自回归模型预测轨迹，并引入‘超预测’策略，预测更长时间范围的轨迹。", "result": "在Argoverse 2单代理运动预测基准上达到新SOTA。", "conclusion": "仅解码器模型在轨迹预测中表现更优，未来可进一步探索多代理场景。"}}
{"id": "2506.06571", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06571", "abs": "https://arxiv.org/abs/2506.06571", "authors": ["Mattie Ji", "Amauri H. Souza", "Vikas Garg"], "title": "Graph Persistence goes Spectral", "comment": "24 pages, 4 figures, 6 tables", "summary": "Including intricate topological information (e.g., cycles) provably enhances\nthe expressivity of message-passing graph neural networks (GNNs) beyond the\nWeisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods\nare increasingly employed for graph representation learning. In this context,\nrecent works have proposed decorating classical PH diagrams with vertex and\nedge features for improved expressivity. However, due to their dependence on\nfeatures, these methods still fail to capture basic graph structural\ninformation. In this paper, we propose SpectRe -- a new topological descriptor\nfor graphs that integrates spectral information into PH diagrams. Notably,\nSpectRe is strictly more expressive than existing descriptors on graphs. We\nalso introduce notions of global and local stability to analyze existing\ndescriptors and establish that SpectRe is locally stable. Finally, experiments\non synthetic and real-world datasets demonstrate the effectiveness of SpectRe\nand its potential to enhance the capabilities of graph models in relevant\nlearning tasks.", "AI": {"tldr": "SpectRe是一种新的图拓扑描述符，通过将谱信息融入持久同调图，显著提升了表达能力，并证明了其局部稳定性。", "motivation": "现有方法依赖顶点和边特征，无法捕捉基本图结构信息，需要更强大的描述符。", "method": "提出SpectRe，将谱信息融入持久同调图，并分析其全局和局部稳定性。", "result": "SpectRe在合成和真实数据集上表现优异，提升了图模型的学习能力。", "conclusion": "SpectRe是一种更强大的拓扑描述符，为图表示学习提供了新方向。"}}
{"id": "2506.07350", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07350", "abs": "https://arxiv.org/abs/2506.07350", "authors": ["Yijie Deng", "Shuaihang Yuan", "Congcong Wen", "Hao Huang", "Anthony Tzes", "Geeta Chandra Raju Bethala", "Yi Fang"], "title": "MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation", "comment": null, "summary": "Spatial awareness is a critical capability for embodied agents, as it enables\nthem to anticipate and reason about unobserved regions. The primary challenge\narises from learning the distribution of indoor semantics, complicated by\nsparse, imbalanced object categories and diverse spatial scales. Existing\nmethods struggle to robustly generate unobserved areas in real time and do not\ngeneralize well to new environments. To this end, we propose \\textbf{MapBERT},\na novel framework designed to effectively model the distribution of unseen\nspaces. Motivated by the observation that the one-hot encoding of semantic maps\naligns naturally with the binary structure of bit encoding, we, for the first\ntime, leverage a lookup-free BitVAE to encode semantic maps into compact\nbitwise tokens. Building on this, a masked transformer is employed to infer\nmissing regions and generate complete semantic maps from limited observations.\nTo enhance object-centric reasoning, we propose an object-aware masking\nstrategy that masks entire object categories concurrently and pairs them with\nlearnable embeddings, capturing implicit relationships between object\nembeddings and spatial tokens. By learning these relationships, the model more\neffectively captures indoor semantic distributions crucial for practical\nrobotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves\nstate-of-the-art semantic map generation, balancing computational efficiency\nwith accurate reconstruction of unobserved regions.", "AI": {"tldr": "MapBERT是一个新框架，通过BitVAE和掩码变换器建模未观测区域的语义分布，提升空间感知能力。", "motivation": "现有方法难以实时生成未观测区域且泛化能力差，需解决稀疏、不平衡的语义分布问题。", "method": "使用BitVAE编码语义图为比特令牌，结合掩码变换器推断缺失区域，并引入对象感知掩码策略。", "result": "在Gibson基准测试中，MapBERT实现了最先进的语义图生成，平衡了计算效率和准确性。", "conclusion": "MapBERT通过比特编码和对象感知策略，有效建模室内语义分布，适用于实际机器人任务。"}}
{"id": "2506.06856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06856", "abs": "https://arxiv.org/abs/2506.06856", "authors": ["Chaoyang Wang", "Zeyu Zhang", "Haiyun Jiang"], "title": "Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning", "comment": null, "summary": "Visual reasoning is crucial for understanding complex multimodal data and\nadvancing Artificial General Intelligence. Existing methods enhance the\nreasoning capability of Multimodal Large Language Models (MLLMs) through\nReinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL\napproaches sample action groups solely from the policy model itself, which\nlimits the upper boundary of the model's reasoning capability and leads to\ninefficient training. To address these limitations, this paper proposes a novel\nRL framework called \\textbf{Vision-EKIPL}. The core of this framework lies in\nintroducing high-quality actions generated by external auxiliary models during\nthe RL training process to guide the optimization of the policy model. The\npolicy learning with knowledge infusion from external models significantly\nexpands the model's exploration space, effectively improves the reasoning\nboundary, and substantially accelerates training convergence speed and\nefficiency. Experimental results demonstrate that our proposed Vision-EKIPL\nachieved up to a 5\\% performance improvement on the Reason-RFT-CoT Benchmark\ncompared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can\novercome the limitations of traditional RL methods, significantly enhance the\nvisual reasoning performance of MLLMs, and provide a new effective paradigm for\nresearch in this field.", "AI": {"tldr": "本文提出了一种名为Vision-EKIPL的新型强化学习框架，通过引入外部辅助模型生成的高质量动作来优化策略模型，显著提升了多模态大语言模型的视觉推理能力。", "motivation": "现有强化学习方法仅从策略模型本身采样动作组，限制了模型的推理能力上限并导致训练效率低下。", "method": "提出Vision-EKIPL框架，在强化学习训练过程中引入外部辅助模型生成的高质量动作，指导策略模型优化。", "result": "在Reason-RFT-CoT Benchmark上实现了5%的性能提升，优于现有最佳方法。", "conclusion": "Vision-EKIPL克服了传统强化学习方法的局限性，显著提升了视觉推理性能，为该领域研究提供了新范式。"}}
{"id": "2506.06579", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.06579", "abs": "https://arxiv.org/abs/2506.06579", "authors": ["Adarsh Prasad Behera", "Jaya Prakash Champati", "Roberto Morabito", "Sasu Tarkoma", "James Gross"], "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques", "comment": null, "summary": "Recent progress in Language Models (LMs) has dramatically advanced the field\nof natural language processing (NLP), excelling at tasks like text generation,\nsummarization, and question answering. However, their inference remains\ncomputationally expensive and energy intensive, especially in settings with\nlimited hardware, power, or bandwidth. This makes it difficult to deploy LMs in\nmobile, edge, or cost sensitive environments. To address these challenges,\nrecent approaches have introduced multi LLM intelligent model selection\nstrategies that dynamically allocate computational resources based on query\ncomplexity -- using lightweight models for simpler queries and escalating to\nlarger models only when necessary. This survey explores two complementary\nstrategies for efficient LLM inference: (i) routing, which selects the most\nsuitable model based on the query, and (ii) cascading or hierarchical inference\n(HI), which escalates queries through a sequence of models until a confident\nresponse is found. Both approaches aim to reduce computation by using\nlightweight models for simpler tasks while offloading only when needed. We\nprovide a comparative analysis of these techniques across key performance\nmetrics, discuss benchmarking efforts, and outline open challenges. Finally, we\noutline future research directions to enable faster response times, adaptive\nmodel selection based on task complexity, and scalable deployment across\nheterogeneous environments, making LLM based systems more efficient and\naccessible for real world applications.", "AI": {"tldr": "该论文探讨了语言模型（LMs）在计算资源受限环境下的高效推理策略，重点介绍了路由和级联推理两种方法，以减少计算成本并提升效率。", "motivation": "尽管语言模型在自然语言处理任务中表现出色，但其推理过程计算成本高且耗能大，限制了在资源受限环境中的部署。", "method": "论文提出了两种策略：(i) 路由：根据查询选择最合适的模型；(ii) 级联推理：通过一系列模型逐步处理查询，直至获得可靠响应。", "result": "这些方法能够显著减少计算资源的使用，同时保持任务性能。", "conclusion": "未来研究方向包括更快响应时间、基于任务复杂度的自适应模型选择，以及跨异构环境的可扩展部署。"}}
{"id": "2506.07454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07454", "abs": "https://arxiv.org/abs/2506.07454", "authors": ["Jared Strader", "Aaron Ray", "Jacob Arkin", "Mason B. Peterson", "Yun Chang", "Nathan Hughes", "Christopher Bradley", "Yi Xuan Jia", "Carlos Nieto-Granda", "Rajat Talak", "Chuchu Fan", "Luca Carlone", "Jonathan P. How", "Nicholas Roy"], "title": "Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs", "comment": "12 pages, 4 figures", "summary": "In this paper, we introduce a multi-robot system that integrates mapping,\nlocalization, and task and motion planning (TAMP) enabled by 3D scene graphs to\nexecute complex instructions expressed in natural language. Our system builds a\nshared 3D scene graph incorporating an open-set object-based map, which is\nleveraged for multi-robot 3D scene graph fusion. This representation supports\nreal-time, view-invariant relocalization (via the object-based map) and\nplanning (via the 3D scene graph), allowing a team of robots to reason about\ntheir surroundings and execute complex tasks. Additionally, we introduce a\nplanning approach that translates operator intent into Planning Domain\nDefinition Language (PDDL) goals using a Large Language Model (LLM) by\nleveraging context from the shared 3D scene graph and robot capabilities. We\nprovide an experimental assessment of the performance of our system on\nreal-world tasks in large-scale, outdoor environments.", "AI": {"tldr": "提出了一种基于3D场景图的多机器人系统，整合了建图、定位和任务与运动规划，支持自然语言指令的复杂任务执行。", "motivation": "解决多机器人在复杂环境中执行自然语言指令的任务，通过共享3D场景图实现实时定位与规划。", "method": "构建共享3D场景图，结合开放集对象地图，利用大型语言模型（LLM）将操作意图转化为PDDL目标。", "result": "系统在大型户外环境中成功执行了真实任务，验证了其有效性。", "conclusion": "该系统为多机器人协作提供了高效、灵活的解决方案，支持复杂任务的实时执行。"}}
{"id": "2506.06864", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06864", "abs": "https://arxiv.org/abs/2506.06864", "authors": ["Junyu Liu", "Jianfeng Ren", "Sunhong Liang", "Xudong Jiang"], "title": "Face recognition on point cloud with cgan-top for denoising", "comment": "Published in ICASSP 2023", "summary": "Face recognition using 3D point clouds is gaining growing interest, while raw\npoint clouds often contain a significant amount of noise due to imperfect\nsensors. In this paper, an end-to-end 3D face recognition on a noisy point\ncloud is proposed, which synergistically integrates the denoising and\nrecognition modules. Specifically, a Conditional Generative Adversarial Network\non Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the\nnoise in the point cloud, and recover the underlying features for subsequent\nrecognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is\nthen adapted to recognize faces from the processed point cloud, which\nhierarchically links both the local point features and neighboring features of\nmultiple scales. The proposed method is validated on the Bosphorus dataset. It\nsignificantly improves the recognition accuracy under all noise settings, with\na maximum gain of 14.81%.", "AI": {"tldr": "提出了一种端到端的3D人脸识别方法，结合去噪与识别模块，显著提高了噪声点云下的识别精度。", "motivation": "原始点云常因传感器不完美而包含大量噪声，影响3D人脸识别的准确性。", "method": "设计了基于条件生成对抗网络（cGAN-TOP）的去噪模块和动态图卷积神经网络（LDGCNN）的识别模块。", "result": "在Bosphorus数据集上验证，所有噪声设置下识别精度显著提升，最高增益达14.81%。", "conclusion": "该方法有效解决了噪声点云下的3D人脸识别问题，具有实际应用潜力。"}}
{"id": "2506.06582", "categories": ["cs.LG", "stat.ML", "I.5.1"], "pdf": "https://arxiv.org/pdf/2506.06582", "abs": "https://arxiv.org/abs/2506.06582", "authors": ["Diaaeldin Taha", "James Chapman", "Marzieh Eidi", "Karel Devriendt", "Guido Montúfar"], "title": "Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing", "comment": "50 pages, 12 figures, published at ICLR 2025. The Thirteenth\n  International Conference on Learning Representations. 2025", "summary": "Topological deep learning (TDL) has emerged as a powerful tool for modeling\nhigher-order interactions in relational data. However, phenomena such as\noversquashing in topological message-passing remain understudied and lack\ntheoretical analysis. We propose a unifying axiomatic framework that bridges\ngraph and topological message-passing by viewing simplicial and cellular\ncomplexes and their message-passing schemes through the lens of relational\nstructures. This approach extends graph-theoretic results and algorithms to\nhigher-order structures, facilitating the analysis and mitigation of\noversquashing in topological message-passing networks. Through theoretical\nanalysis and empirical studies on simplicial networks, we demonstrate the\npotential of this framework to advance TDL.", "AI": {"tldr": "论文提出了一种统一的公理化框架，将图和拓扑消息传递联系起来，以分析并缓解拓扑消息传递中的过度压缩问题。", "motivation": "拓扑深度学习（TDL）在建模关系数据中的高阶交互方面表现出强大能力，但拓扑消息传递中的过度压缩现象缺乏理论分析。", "method": "通过将单纯和胞腔复形及其消息传递方案视为关系结构，扩展图论结果和算法到高阶结构。", "result": "通过理论分析和单纯网络的实证研究，验证了该框架在推动TDL发展中的潜力。", "conclusion": "该框架为拓扑消息传递中的问题提供了理论支持，并展示了其在高阶结构中的应用前景。"}}
{"id": "2506.07490", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07490", "abs": "https://arxiv.org/abs/2506.07490", "authors": ["Zhaoliang Wan", "Zetong Bi", "Zida Zhou", "Hao Ren", "Yiming Zeng", "Yihan Li", "Lu Qi", "Xu Yang", "Ming-Hsuan Yang", "Hui Cheng"], "title": "RAPID Hand: A Robust, Affordable, Perception-Integrated, Dexterous Manipulation Platform for Generalist Robot Autonomy", "comment": null, "summary": "This paper addresses the scarcity of low-cost but high-dexterity platforms\nfor collecting real-world multi-fingered robot manipulation data towards\ngeneralist robot autonomy. To achieve it, we propose the RAPID Hand, a\nco-optimized hardware and software platform where the compact 20-DoF hand,\nrobust whole-hand perception, and high-DoF teleoperation interface are jointly\ndesigned. Specifically, RAPID Hand adopts a compact and practical hand ontology\nand a hardware-level perception framework that stably integrates wrist-mounted\nvision, fingertip tactile sensing, and proprioception with sub-7 ms latency and\nspatial alignment. Collecting high-quality demonstrations on high-DoF hands is\nchallenging, as existing teleoperation methods struggle with precision and\nstability on complex multi-fingered systems. We address this by co-optimizing\nhand design, perception integration, and teleoperation interface through a\nuniversal actuation scheme, custom perception electronics, and two retargeting\nconstraints. We evaluate the platform's hardware, perception, and teleoperation\ninterface. Training a diffusion policy on collected data shows superior\nperformance over prior works, validating the system's capability for reliable,\nhigh-quality data collection. The platform is constructed from low-cost and\noff-the-shelf components and will be made public to ensure reproducibility and\nease of adoption.", "AI": {"tldr": "本文提出RAPID Hand，一个低成本、高灵活性的多指机器人平台，用于收集高质量操作数据，支持通用机器人自主性。", "motivation": "解决低成本高灵活性多指机器人平台稀缺的问题，以支持通用机器人自主性的数据收集。", "method": "通过硬件和软件协同设计，包括紧凑的20自由度手、全手感知系统和高效远程操作接口，并采用通用驱动方案和定制感知电子设备。", "result": "实验表明，该平台在硬件、感知和远程操作方面表现优异，收集的数据训练扩散策略优于现有工作。", "conclusion": "RAPID Hand是一个低成本、高性能的平台，适合高质量数据收集，并将公开以促进可重复性和广泛应用。"}}
{"id": "2506.06886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06886", "abs": "https://arxiv.org/abs/2506.06886", "authors": ["Wafaa Kasri", "Yassine Himeur", "Abigail Copiaco", "Wathiq Mansoor", "Ammar Albanna", "Valsamma Eapen"], "title": "Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis", "comment": "7 pages, 4 figures and 2 tables", "summary": "Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early\nintervention. This study presents a hybrid deep learning framework combining\nVision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking\ndata. The model uses attention-based fusion to integrate visual, speech, and\nfacial cues, capturing both spatial and temporal dynamics. Unlike traditional\nhandcrafted methods, it applies state-of-the-art deep learning and explainable\nAI techniques to enhance diagnostic accuracy and transparency. Tested on the\nSaliency4ASD dataset, the proposed ViT-Mamba model outperformed existing\nmethods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94\nspecificity. These findings show the model's promise for scalable,\ninterpretable ASD screening, especially in resource-constrained or remote\nclinical settings where access to expert diagnosis is limited.", "AI": {"tldr": "提出了一种结合Vision Transformers和Vision Mamba的混合深度学习框架，用于通过眼动数据检测自闭症谱系障碍（ASD），显著提升了诊断准确性和可解释性。", "motivation": "早期诊断ASD对干预至关重要，但传统手工方法效果有限，需要更准确且可解释的自动化诊断工具。", "method": "采用Vision Transformers和Vision Mamba的混合框架，通过注意力机制融合视觉、语音和面部线索，捕捉时空动态。", "result": "在Saliency4ASD数据集上，模型表现优于现有方法，准确率达0.96，F1分数0.95，灵敏度0.97，特异性0.94。", "conclusion": "该模型在资源有限或远程临床环境中具有潜力，为ASD筛查提供了可扩展且可解释的解决方案。"}}
{"id": "2506.06584", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06584", "abs": "https://arxiv.org/abs/2506.06584", "authors": ["Mo Zhou", "Weihang Xu", "Maryam Fazel", "Simon S. Du"], "title": "Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures", "comment": "77 pages", "summary": "Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine\nlearning, with the Expectation-Maximization (EM) algorithm and its popular\nvariant gradient EM being arguably the most widely used algorithms in practice.\nIn the exact-parameterized setting, where both the ground truth GMM and the\nlearning model have the same number of components $m$, a vast line of work has\naimed to establish rigorous recovery guarantees for EM. However, global\nconvergence has only been proven for the case of $m=2$, and EM is known to fail\nto recover the ground truth when $m\\geq 3$.\n  In this paper, we consider the $\\textit{over-parameterized}$ setting, where\nthe learning model uses $n>m$ components to fit an $m$-component ground truth\nGMM. In contrast to the exact-parameterized case, we provide a rigorous global\nconvergence guarantee for gradient EM. Specifically, for any well separated\nGMMs in general position, we prove that with only mild over-parameterization $n\n= \\Omega(m\\log m)$, randomly initialized gradient EM converges globally to the\nground truth at a polynomial rate with polynomial samples. Our analysis\nproceeds in two stages and introduces a suite of novel tools for Gaussian\nMixture analysis. We use Hermite polynomials to study the dynamics of gradient\nEM and employ tensor decomposition to characterize the geometric landscape of\nthe likelihood loss. This is the first global convergence and recovery result\nfor EM or Gradient EM beyond the special case of $m=2$.", "AI": {"tldr": "该论文研究了高斯混合模型（GMMs）在过参数化设置下的全局收敛性，证明了梯度EM算法在随机初始化下可以全局收敛到真实参数。", "motivation": "尽管EM算法及其变种梯度EM在实践中广泛应用，但在精确参数化设置下（即学习模型与真实模型组件数相同），仅对m=2的情况有全局收敛性证明，而对m≥3时EM算法会失败。因此，论文探索了过参数化设置（学习模型组件数多于真实模型）下的收敛性。", "method": "论文采用梯度EM算法，利用Hermite多项式分析其动态特性，并通过张量分解刻画似然损失的几何景观。", "result": "对于任何良好分离且处于一般位置的高斯混合模型，仅需轻度过参数化（n=Ω(mlogm)），随机初始化的梯度EM算法能以多项式速率全局收敛到真实参数。", "conclusion": "这是首次在m=2之外的情况下，为EM或梯度EM算法提供全局收敛性和恢复性结果，为高斯混合模型分析提供了新的工具和理论支持。"}}
{"id": "2506.07509", "categories": ["cs.RO", "I.2.7; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.07509", "abs": "https://arxiv.org/abs/2506.07509", "authors": ["Shoon Kit Lim", "Melissa Jia Ying Chong", "Jing Huey Khor", "Ting Yang Ling"], "title": "Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent", "comment": "Source code available at:\n  https://github.com/limshoonkit/ros2-agent-ws", "summary": "Recent advances in agentic and physical artificial intelligence (AI) have\nlargely focused on ground-based platforms such as humanoid and wheeled robots,\nleaving aerial robots relatively underexplored. Meanwhile, state-of-the-art\nunmanned aerial vehicle (UAV) multimodal vision-language systems typically rely\non closed-source models accessible only to well-resourced organizations. To\ndemocratize natural language control of autonomous drones, we present an\nopen-source agentic framework that integrates PX4-based flight control, Robot\nOperating System 2 (ROS 2) middleware, and locally hosted models using Ollama.\nWe evaluate performance both in simulation and on a custom quadcopter platform,\nbenchmarking four large language model (LLM) families for command generation\nand three vision-language model (VLM) families for scene understanding.", "AI": {"tldr": "本文提出了一种开源框架，用于实现无人机的自然语言控制，整合了PX4飞行控制、ROS 2中间件和本地托管模型。", "motivation": "当前空中机器人的研究相对不足，且现有的无人机多模态视觉语言系统通常依赖闭源模型，限制了资源的普及。", "method": "通过整合PX4飞行控制、ROS 2中间件和本地托管模型（如Ollama），开发了一个开源框架，并在仿真和自定义四旋翼平台上评估了性能。", "result": "评估了四种大型语言模型（LLM）用于命令生成和三种视觉语言模型（VLM）用于场景理解。", "conclusion": "该框架为无人机自然语言控制的普及提供了开源解决方案。"}}
{"id": "2506.06898", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.06898", "abs": "https://arxiv.org/abs/2506.06898", "authors": ["Reese Kneeland", "Paul S. Scotti", "Ghislain St-Yves", "Jesse Breedlove", "Kendrick Kay", "Thomas Naselaris"], "title": "NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery", "comment": "Published at CVPR 2025", "summary": "We release NSD-Imagery, a benchmark dataset of human fMRI activity paired\nwith mental images, to complement the existing Natural Scenes Dataset (NSD), a\nlarge-scale dataset of fMRI activity paired with seen images that enabled\nunprecedented improvements in fMRI-to-image reconstruction efforts. Recent\nmodels trained on NSD have been evaluated only on seen image reconstruction.\nUsing NSD-Imagery, it is possible to assess how well these models perform on\nmental image reconstruction. This is a challenging generalization requirement\nbecause mental images are encoded in human brain activity with relatively lower\nsignal-to-noise and spatial resolution; however, generalization from seen to\nmental imagery is critical for real-world applications in medical domains and\nbrain-computer interfaces, where the desired information is always internally\ngenerated. We provide benchmarks for a suite of recent NSD-trained open-source\nvisual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et\nal.) on NSD-Imagery, and show that the performance of decoding methods on\nmental images is largely decoupled from performance on vision reconstruction.\nWe further demonstrate that architectural choices significantly impact\ncross-decoding performance: models employing simple linear decoding\narchitectures and multimodal feature decoding generalize better to mental\nimagery, while complex architectures tend to overfit visual training data. Our\nfindings indicate that mental imagery datasets are critical for the development\nof practical applications, and establish NSD-Imagery as a useful resource for\nbetter aligning visual decoding methods with this goal.", "AI": {"tldr": "NSD-Imagery是一个新发布的基准数据集，用于评估fMRI到图像重建模型在心理图像上的表现，填补了现有NSD数据集的空白。", "motivation": "现有模型仅在视觉图像重建上表现良好，但心理图像重建在医学和脑机接口应用中更为关键。", "method": "通过NSD-Imagery评估多个NSD训练的视觉解码模型（如MindEye1、Brain Diffuser等）在心理图像上的表现。", "result": "心理图像解码性能与视觉重建性能脱钩，简单线性架构和多模态特征解码表现更好。", "conclusion": "心理图像数据集对实际应用至关重要，NSD-Imagery为视觉解码方法的优化提供了重要资源。"}}
{"id": "2506.06599", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06599", "abs": "https://arxiv.org/abs/2506.06599", "authors": ["Yuanjie Shi", "Hooman Shahrokhi", "Xuesong Jia", "Xiongzhi Chen", "Janardhan Rao Doppa", "Yan Yan"], "title": "Direct Prediction Set Minimization via Bilevel Conformal Classifier Training", "comment": "Accepted for Publication at International Conference on Machine\n  Learning (ICML), 2025", "summary": "Conformal prediction (CP) is a promising uncertainty quantification framework\nwhich works as a wrapper around a black-box classifier to construct prediction\nsets (i.e., subset of candidate classes) with provable guarantees. However,\nstandard calibration methods for CP tend to produce large prediction sets which\nmakes them less useful in practice. This paper considers the problem of\nintegrating conformal principles into the training process of deep classifiers\nto directly minimize the size of prediction sets. We formulate conformal\ntraining as a bilevel optimization problem and propose the {\\em Direct\nPrediction Set Minimization (DPSM)} algorithm to solve it. The key insight\nbehind DPSM is to minimize a measure of the prediction set size (upper level)\nthat is conditioned on the learned quantile of conformity scores (lower level).\nWe analyze that DPSM has a learning bound of $O(1/\\sqrt{n})$ (with $n$ training\nsamples), while prior conformal training methods based on stochastic\napproximation for the quantile has a bound of $\\Omega(1/s)$ (with batch size\n$s$ and typically $s \\ll \\sqrt{n}$). Experiments on various benchmark datasets\nand deep models show that DPSM significantly outperforms the best prior\nconformal training baseline with $20.46\\%\\downarrow$ in the prediction set size\nand validates our theory.", "AI": {"tldr": "论文提出了一种名为DPSM的算法，通过将共形预测原则融入深度分类器的训练过程，直接最小化预测集的大小，显著优于现有方法。", "motivation": "标准共形预测的校准方法通常生成较大的预测集，限制了其实际应用价值。", "method": "将共形训练建模为一个双层优化问题，提出DPSM算法，通过最小化预测集大小的度量（上层）和基于学习的一致性分数分位数（下层）来实现。", "result": "DPSM在多个基准数据集和深度模型上表现优异，预测集大小减少了20.46%，验证了其理论优势。", "conclusion": "DPSM通过直接优化预测集大小，显著提升了共形预测的实用性，为不确定性量化提供了更高效的解决方案。"}}
{"id": "2506.07530", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07530", "abs": "https://arxiv.org/abs/2506.07530", "authors": ["Hongyu Wang", "Chuyan Xiong", "Ruiping Wang", "Xilin Chen"], "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation", "comment": "Work in progress", "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.", "AI": {"tldr": "BitVLA是首个1位三元参数（-1, 0, 1）的视觉-语言-动作（VLA）模型，通过蒸馏感知训练策略进一步压缩视觉编码器至1.58位，内存占用仅为29.8%，性能接近4位量化模型。", "motivation": "解决VLA模型在资源受限机器人系统上部署的挑战，探索1位预训练在VLA模型中的应用。", "method": "提出BitVLA模型，采用三元参数和蒸馏感知训练策略压缩视觉编码器。", "result": "在LIBERO基准测试中性能接近4位量化模型OpenVLA-OFT，内存占用显著降低。", "conclusion": "BitVLA展示了在内存受限边缘设备上部署的潜力，代码和模型已开源。"}}
{"id": "2506.06906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06906", "abs": "https://arxiv.org/abs/2506.06906", "authors": ["Nima Jamali", "Matina Mahdizadeh Sani", "Hanieh Naderi", "Shohreh Kasaei"], "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search", "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable performance in\nanalyzing 3D point cloud data. However, their vulnerability to adversarial\nattacks-such as point dropping, shifting, and adding-poses a critical challenge\nto the reliability of 3D vision systems. These attacks can compromise the\nsemantic and structural integrity of point clouds, rendering many existing\ndefense mechanisms ineffective. To address this issue, a defense strategy named\nKNN-Defense is proposed, grounded in the manifold assumption and\nnearest-neighbor search in feature space. Instead of reconstructing surface\ngeometry or enforcing uniform point distributions, the method restores\nperturbed inputs by leveraging the semantic similarity of neighboring samples\nfrom the training set. KNN-Defense is lightweight and computationally\nefficient, enabling fast inference and making it suitable for real-time and\npractical applications. Empirical results on the ModelNet40 dataset\ndemonstrated that KNN-Defense significantly improves robustness across various\nattack types. In particular, under point-dropping attacks-where many existing\nmethods underperform due to the targeted removal of critical points-the\nproposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on\nPointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that\nKNN-Defense offers a scalable and effective solution for enhancing the\nadversarial resilience of 3D point cloud classifiers. (An open-source\nimplementation of the method, including code and data, is available at\nhttps://github.com/nimajam41/3d-knn-defense).", "AI": {"tldr": "KNN-Defense是一种轻量级防御策略，通过利用训练集中邻近样本的语义相似性恢复受扰动的3D点云数据，显著提高了对抗攻击下的鲁棒性。", "motivation": "深度神经网络在3D点云数据分析中表现出色，但对对抗攻击（如点丢弃、移动和添加）的脆弱性威胁了3D视觉系统的可靠性。", "method": "基于流形假设和特征空间中的最近邻搜索，KNN-Defense通过语义相似性恢复受扰动的输入，而非重建表面几何或强制均匀点分布。", "result": "在ModelNet40数据集上，KNN-Defense显著提升了对抗攻击下的鲁棒性，特别是在点丢弃攻击下，对PointNet等模型的准确率提升显著。", "conclusion": "KNN-Defense为增强3D点云分类器的对抗鲁棒性提供了一种可扩展且高效的解决方案。"}}
{"id": "2506.06909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06909", "abs": "https://arxiv.org/abs/2506.06909", "authors": ["Vladimir Yugay", "Thies Kersten", "Luca Carlone", "Theo Gevers", "Martin R. Oswald", "Lukas Schmid"], "title": "Gaussian Mapping for Evolving Scenes", "comment": null, "summary": "Mapping systems with novel view synthesis (NVS) capabilities are widely used\nin computer vision, with augmented reality, robotics, and autonomous driving\napplications. Most notably, 3D Gaussian Splatting-based systems show high NVS\nperformance; however, many current approaches are limited to static scenes.\nWhile recent works have started addressing short-term dynamics (motion within\nthe view of the camera), long-term dynamics (the scene evolving through changes\nout of view) remain less explored. To overcome this limitation, we introduce a\ndynamic scene adaptation mechanism that continuously updates the 3D\nrepresentation to reflect the latest changes. In addition, since maintaining\ngeometric and semantic consistency remains challenging due to stale\nobservations disrupting the reconstruction process, we propose a novel keyframe\nmanagement mechanism that discards outdated observations while preserving as\nmuch information as possible. We evaluate Gaussian Mapping for Evolving Scenes\n(GaME) on both synthetic and real-world datasets and find it to be more\naccurate than the state of the art.", "AI": {"tldr": "论文提出了一种动态场景适应机制和关键帧管理机制，用于解决3D高斯泼溅技术在长期动态场景中的局限性。", "motivation": "当前3D高斯泼溅技术在静态场景中表现优异，但在长期动态场景（场景在视野外变化）中表现不足，亟需改进。", "method": "引入动态场景适应机制持续更新3D表示，并提出关键帧管理机制以保持几何和语义一致性。", "result": "在合成和真实数据集上评估，GaME方法比现有技术更准确。", "conclusion": "GaME方法有效解决了长期动态场景的挑战，提升了3D高斯泼溅技术的实用性。"}}
{"id": "2506.06606", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06606", "abs": "https://arxiv.org/abs/2506.06606", "authors": ["Xinyu Luo", "Cedar Site Bai", "Bolian Li", "Petros Drineas", "Ruqi Zhang", "Brian Bullins"], "title": "Stacey: Promoting Stochastic Steepest Descent via Accelerated $\\ell_p$-Smooth Nonconvex Optimization", "comment": null, "summary": "While popular optimization methods such as SGD, AdamW, and Lion depend on\nsteepest descent updates in either $\\ell_2$ or $\\ell_\\infty$ norms, there\nremains a critical gap in handling the non-Euclidean structure observed in\nmodern deep networks training. In this work, we address this need by\nintroducing a new accelerated $\\ell_p$ steepest descent algorithm, called\nStacey, which uses interpolated primal-dual iterate sequences to effectively\nnavigate non-Euclidean smooth optimization tasks. In addition to providing\nnovel theoretical guarantees for the foundations of our algorithm, we\nempirically compare our approach against these popular methods on tasks\nincluding image classification and language model (LLM) pretraining,\ndemonstrating both faster convergence and higher final accuracy. We further\nevaluate different values of $p$ across various models and datasets,\nunderscoring the importance and efficiency of non-Euclidean approaches over\nstandard Euclidean methods. Code can be found at\nhttps://github.com/xinyuluo8561/Stacey .", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07633", "abs": "https://arxiv.org/abs/2506.07633", "authors": ["Ana Tanevska", "Ananthapathmanabhan Ratheesh Kumar", "Arabinda Ghosh", "Ernesto Casablanca", "Ginevra Castellano", "Sadegh Soudjani"], "title": "Blending Participatory Design and Artificial Awareness for Trustworthy Autonomous Vehicles", "comment": "Submitted to IEEE RO-MAN 2025", "summary": "Current robotic agents, such as autonomous vehicles (AVs) and drones, need to\ndeal with uncertain real-world environments with appropriate situational\nawareness (SA), risk awareness, coordination, and decision-making. The SymAware\nproject strives to address this issue by designing an architecture for\nartificial awareness in multi-agent systems, enabling safe collaboration of\nautonomous vehicles and drones. However, these agents will also need to\ninteract with human users (drivers, pedestrians, drone operators), which in\nturn requires an understanding of how to model the human in the interaction\nscenario, and how to foster trust and transparency between the agent and the\nhuman.\n  In this work, we aim to create a data-driven model of a human driver to be\nintegrated into our SA architecture, grounding our research in the principles\nof trustworthy human-agent interaction. To collect the data necessary for\ncreating the model, we conducted a large-scale user-centered study on human-AV\ninteraction, in which we investigate the interaction between the AV's\ntransparency and the users' behavior.\n  The contributions of this paper are twofold: First, we illustrate in detail\nour human-AV study and its findings, and second we present the resulting Markov\nchain models of the human driver computed from the study's data. Our results\nshow that depending on the AV's transparency, the scenario's environment, and\nthe users' demographics, we can obtain significant differences in the model's\ntransitions.", "AI": {"tldr": "论文提出了一种数据驱动的人类驾驶员模型，用于增强多智能体系统中的情境感知和信任交互。通过大规模用户研究，分析了自动驾驶车辆透明度对用户行为的影响，并建立了马尔可夫链模型。", "motivation": "解决自动驾驶车辆和无人机在多智能体系统中与人类用户的安全协作问题，特别是如何建模人类行为并提升信任与透明度。", "method": "通过大规模用户中心研究收集数据，研究自动驾驶车辆透明度与用户行为的交互关系，并基于数据建立马尔可夫链模型。", "result": "研究发现，自动驾驶车辆的透明度、场景环境和用户人口统计特征显著影响模型的状态转移。", "conclusion": "研究为多智能体系统中的人类行为建模提供了数据支持，并强调了透明度和用户特征在信任交互中的重要性。"}}
{"id": "2506.06912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06912", "abs": "https://arxiv.org/abs/2506.06912", "authors": ["Olivier Papillon", "Rafik Goubran", "James Green", "Julien Larivière-Chartier", "Caitlin Higginson", "Frank Knoefel", "Rébecca Robillard"], "title": "Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM", "comment": "Submitted to IEEE MeMeA 2025", "summary": "Accurate sleep stage classification is essential for diagnosing sleep\ndisorders, particularly in aging populations. While traditional polysomnography\n(PSG) relies on electroencephalography (EEG) as the gold standard, its\ncomplexity and need for specialized equipment make home-based sleep monitoring\nchallenging. To address this limitation, we investigate the use of\nelectrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive\nalternatives for five-stage sleep-wake classification. This study introduces a\nnovel approach that leverages ImageBind, a multimodal embedding deep learning\nmodel, to integrate PSM data with dual-channel EOG signals for sleep stage\nclassification. Our method is the first reported approach that fuses PSM and\nEOG data for sleep stage classification with ImageBind. Our results demonstrate\nthat fine-tuning ImageBind significantly improves classification accuracy,\noutperforming existing models based on single-channel EOG (DeepSleepNet),\nexclusively PSM data (ViViT), and other multimodal deep learning approaches\n(MBT). Notably, the model also achieved strong performance without fine-tuning,\nhighlighting its adaptability to specific tasks with limited labeled data,\nmaking it particularly advantageous for medical applications. We evaluated our\nmethod using 85 nights of patient recordings from a sleep clinic. Our findings\nsuggest that pre-trained multimodal embedding models, even those originally\ndeveloped for non-medical domains, can be effectively adapted for sleep\nstaging, with accuracies approaching systems that require complex EEG data.", "AI": {"tldr": "研究提出了一种基于ImageBind的多模态嵌入深度学习模型，结合PSM和EOG数据用于睡眠阶段分类，显著提高了准确性。", "motivation": "传统PSG依赖EEG，复杂且不便，研究探索了PSM和EOG作为替代方案。", "method": "使用ImageBind模型整合PSM和双通道EOG信号，首次融合这两种数据。", "result": "模型在未微调时表现良好，微调后显著优于现有单模态或多模态方法。", "conclusion": "预训练多模态模型可有效用于睡眠分类，接近依赖EEG的系统性能。"}}
{"id": "2506.06632", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.06632", "abs": "https://arxiv.org/abs/2506.06632", "authors": ["Shubham Parashar", "Shurui Gui", "Xiner Li", "Hongyi Ling", "Sushil Vemuri", "Blake Olson", "Eric Li", "Yu Zhang", "James Caverlee", "Dileep Kalathil", "Shuiwang Ji"], "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "comment": null, "summary": "We aim to improve the reasoning capabilities of language models via\nreinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1\nhave demonstrated reasoning abilities on mathematical and coding tasks.\nHowever, prior studies suggest that using RL alone to improve reasoning on\ninherently difficult tasks is less effective. Here, we draw inspiration from\ncurriculum learning and propose to schedule tasks from easy to hard (E2H),\nallowing LLMs to build reasoning skills gradually. Our method is termed E2H\nReasoner. Empirically, we observe that, although easy tasks are important\ninitially, fading them out through appropriate scheduling is essential in\npreventing overfitting. Theoretically, we establish convergence guarantees for\nE2H Reasoner within an approximate policy iteration framework. We derive\nfinite-sample complexity bounds and show that when tasks are appropriately\ndecomposed and conditioned, learning through curriculum stages requires fewer\ntotal samples than direct learning. Experiments across multiple domains show\nthat E2H Reasoner significantly improves the reasoning ability of small LLMs\n(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,\nhighlighting the effectiveness of our method.", "AI": {"tldr": "论文提出了一种名为E2H Reasoner的方法，通过从易到难的课程学习（E2H）提升语言模型的推理能力，避免直接使用强化学习（RL）的局限性。", "motivation": "现有研究表明，单独使用RL提升语言模型在复杂任务上的推理能力效果有限，因此需要一种更有效的方法。", "method": "采用从易到难的任务调度（E2H），逐步构建模型的推理能力，并通过理论分析确保收敛性。", "result": "实验表明，E2H Reasoner显著提升了小型语言模型（1.5B至3B）的推理能力，且样本效率更高。", "conclusion": "E2H Reasoner是一种有效的提升语言模型推理能力的方法，尤其适用于小型模型。"}}
{"id": "2506.07639", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07639", "abs": "https://arxiv.org/abs/2506.07639", "authors": ["Zhekai Duan", "Yuan Zhang", "Shikai Geng", "Gaowen Liu", "Joschka Boedecker", "Chris Xiaoxuan Lu"], "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse", "comment": null, "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.", "AI": {"tldr": "Fast ECoT通过缓存和并行化推理步骤，显著降低了ECoT的推理延迟，提升了实时性。", "motivation": "ECoT推理的序列化生成导致高延迟，限制了实时应用。", "method": "提出Fast ECoT，利用缓存和并行化推理步骤，并引入异步调度器。", "result": "实验显示延迟降低7.5%，任务成功率和推理准确性保持或提升。", "conclusion": "Fast ECoT无需模型修改或额外训练，适用于现有VLA系统，推动ECoT实时部署。"}}
{"id": "2506.06918", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06918", "abs": "https://arxiv.org/abs/2506.06918", "authors": ["Carl Brander", "Giovanni Cioffi", "Nico Messikommer", "Davide Scaramuzza"], "title": "Reading in the Dark with Foveated Event Vision", "comment": "CVPR 2025 Workshop on Event-based Vision", "summary": "Current smart glasses equipped with RGB cameras struggle to perceive the\nenvironment in low-light and high-speed motion scenarios due to motion blur and\nthe limited dynamic range of frame cameras. Additionally, capturing dense\nimages with a frame camera requires large bandwidth and power consumption,\nconsequently draining the battery faster. These challenges are especially\nrelevant for developing algorithms that can read text from images. In this\nwork, we propose a novel event-based Optical Character Recognition (OCR)\napproach for smart glasses. By using the eye gaze of the user, we foveate the\nevent stream to significantly reduce bandwidth by around 98% while exploiting\nthe benefits of event cameras in high-dynamic and fast scenes. Our proposed\nmethod performs deep binary reconstruction trained on synthetic data and\nleverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our\nresults demonstrate the ability to read text in low light environments where\nRGB cameras struggle while using up to 2400 times less bandwidth than a\nwearable RGB camera.", "AI": {"tldr": "提出了一种基于事件相机和用户眼动的智能眼镜OCR方法，显著降低带宽需求，并在低光和高动态场景中优于传统OCR。", "motivation": "解决智能眼镜在低光和高动态场景下因运动模糊和帧相机动态范围限制导致的文本识别困难，同时减少带宽和功耗。", "method": "利用用户眼动聚焦事件流，结合深度二进制重建和合成数据训练，并采用多模态LLM进行OCR。", "result": "在低光环境下成功读取文本，带宽需求比传统RGB相机降低2400倍。", "conclusion": "该方法在低光和高动态场景中表现优异，显著降低了带宽和功耗需求。"}}
{"id": "2506.06633", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06633", "abs": "https://arxiv.org/abs/2506.06633", "authors": ["Chi-Sheng Chen"], "title": "Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification", "comment": null, "summary": "Recent advancements in quantum machine learning have shown promise in\nenhancing classical neural network architectures, particularly in domains\ninvolving complex, high-dimensional data. Building upon prior work in temporal\nsequence modeling, this paper introduces Vision-QRWKV, a hybrid\nquantum-classical extension of the Receptance Weighted Key Value (RWKV)\narchitecture, applied for the first time to image classification tasks. By\nintegrating a variational quantum circuit (VQC) into the channel mixing\ncomponent of RWKV, our model aims to improve nonlinear feature transformation\nand enhance the expressive capacity of visual representations.\n  We evaluate both classical and quantum RWKV models on a diverse collection of\n14 medical and standard image classification benchmarks, including MedMNIST\ndatasets, MNIST, and FashionMNIST. Our results demonstrate that the\nquantum-enhanced model outperforms its classical counterpart on a majority of\ndatasets, particularly those with subtle or noisy class distinctions (e.g.,\nChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first\nsystematic application of quantum-enhanced RWKV in the visual domain, offering\ninsights into the architectural trade-offs and future potential of quantum\nmodels for lightweight and efficient vision tasks.", "AI": {"tldr": "论文提出了一种混合量子经典架构Vision-QRWKV，用于图像分类任务，通过引入变分量子电路提升特征表达能力，并在多个数据集上验证了其优于经典模型的表现。", "motivation": "量子机器学习在复杂高维数据领域展现出潜力，本文旨在探索量子增强的RWKV架构在视觉任务中的应用，提升非线性特征转换能力。", "method": "在RWKV的通道混合组件中集成变分量子电路（VQC），构建Vision-QRWKV模型，并在14个医学和标准图像分类数据集上进行评估。", "result": "量子增强模型在多数数据集上优于经典模型，尤其是在类别区分细微或噪声较多的数据集（如ChestMNIST、RetinaMNIST）上表现突出。", "conclusion": "研究首次系统地将量子增强RWKV应用于视觉领域，为轻量高效量子视觉模型的未来发展提供了参考。"}}
{"id": "2506.07696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07696", "abs": "https://arxiv.org/abs/2506.07696", "authors": ["Yongqi Zhao", "Xinrui Zhang", "Tomislav Mihalj", "Martin Schabauer", "Luis Putzer", "Erik Reichmann-Blaga", "Ádám Boronyák", "András Rövid", "Gábor Soós", "Peizhi Zhang", "Lu Xiong", "Jia Hu", "Arno Eichberger"], "title": "A Communication-Latency-Aware Co-Simulation Platform for Safety and Comfort Evaluation of Cloud-Controlled ICVs", "comment": "11 pages, 8 figures", "summary": "Testing cloud-controlled intelligent connected vehicles (ICVs) requires\nsimulation environments that faithfully emulate both vehicle behavior and\nrealistic communication latencies. This paper proposes a latency-aware\nco-simulation platform integrating CarMaker and Vissim to evaluate safety and\ncomfort under real-world vehicle-to-cloud (V2C) latency conditions. Two\ncommunication latency models, derived from empirical 5G measurements in China\nand Hungary, are incorporated and statistically modeled using Gamma\ndistributions. A proactive conflict module (PCM) is proposed to dynamically\ncontrol background vehicles and generate safety-critical scenarios. The\nplatform is validated through experiments involving an exemplary system under\ntest (SUT) across six testing conditions combining two PCM modes\n(enabled/disabled) and three latency conditions (none, China, Hungary). Safety\nand comfort are assessed using metrics including collision rate, distance\nheadway, post-encroachment time, and the spectral characteristics of\nlongitudinal acceleration. Results show that the PCM effectively increases\ndriving environment criticality, while V2C latency primarily affects ride\ncomfort. These findings confirm the platform's effectiveness in systematically\nevaluating cloud-controlled ICVs under diverse testing conditions.", "AI": {"tldr": "论文提出了一种延迟感知的协同仿真平台，用于评估云控智能网联车辆的安全性和舒适性，结合了两种通信延迟模型和主动冲突模块。", "motivation": "测试云控智能网联车辆需要模拟真实车辆行为和通信延迟，现有仿真平台缺乏对延迟的准确建模。", "method": "平台整合了CarMaker和Vissim，采用基于5G实测数据的Gamma分布延迟模型，并引入主动冲突模块（PCM）生成安全关键场景。", "result": "实验表明，PCM能有效提升驾驶环境的关键性，而V2C延迟主要影响舒适性。", "conclusion": "该平台能系统评估云控智能网联车辆在不同测试条件下的表现，验证了其有效性。"}}
{"id": "2506.06928", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06928", "abs": "https://arxiv.org/abs/2506.06928", "authors": ["George Lydakis", "Alexander Hermans", "Ali Athar", "Daan de Geus", "Bastian Leibe"], "title": "How Important are Videos for Training Video LLMs?", "comment": "Project page on\n  https://visualcomputinginstitute.github.io/videollm-pseudovideo-training/", "summary": "Research into Video Large Language Models (LLMs) has progressed rapidly, with\nnumerous models and benchmarks emerging in just a few years. Typically, these\nmodels are initialized with a pretrained text-only LLM and finetuned on both\nimage- and video-caption datasets. In this paper, we present findings\nindicating that Video LLMs are more capable of temporal reasoning after\nimage-only training than one would assume, and that improvements from\nvideo-specific training are surprisingly small. Specifically, we show that\nimage-trained versions of two LLMs trained with the recent LongVU algorithm\nperform significantly above chance level on TVBench, a temporal reasoning\nbenchmark. Additionally, we introduce a simple finetuning scheme involving\nsequences of annotated images and questions targeting temporal capabilities.\nThis baseline results in temporal reasoning performance close to, and\noccasionally higher than, what is achieved by video-trained LLMs. This suggests\nsuboptimal utilization of rich temporal features found in real video by current\nmodels. Our analysis motivates further research into the mechanisms that allow\nimage-trained LLMs to perform temporal reasoning, as well as into the\nbottlenecks that render current video training schemes inefficient.", "AI": {"tldr": "研究发现，仅通过图像训练的Video LLMs在时间推理能力上表现优于预期，而视频训练的改进效果有限。", "motivation": "探索Video LLMs在时间推理上的能力，尤其是图像训练与视频训练的效果差异。", "method": "使用LongVU算法训练的两个LLMs，并在TVBench上进行测试；引入基于图像序列和问题的微调方案。", "result": "图像训练的LLMs在时间推理任务上表现显著高于随机水平，且接近或超过视频训练的LLMs。", "conclusion": "当前视频训练方案未充分利用时间特征，需进一步研究图像训练LLMs的时间推理机制及视频训练的瓶颈。"}}
{"id": "2506.06637", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.06637", "abs": "https://arxiv.org/abs/2506.06637", "authors": ["Olimjon Toirov", "Wei Yu"], "title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning", "comment": "10 pages, 3 figures, 2025 2nd International Conference on Digital\n  Society and Artificial Intelligence (DSAI 2025), Conference dates: May 23-25,\n  2025", "summary": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and\nenergy consumption of each electrical device in the circuit by analyzing the\nelectrical signals at the bus, which is of great significance for smart power\nmanagement. However, the complex and changeable load combinations and\napplication environments lead to the challenges of poor feature robustness and\ninsufficient model generalization of traditional NILM methods. To this end,\nthis paper proposes a new non-intrusive load monitoring method that integrates\n\"image load signature\" and continual learning. This method converts\nmulti-dimensional power signals such as current, voltage, and power factor into\nvisual image load feature signatures, and combines deep convolutional neural\nnetworks to realize the identification and classification of multiple devices;\nat the same time, self-supervised pre-training is introduced to improve feature\ngeneralization, and continual online learning strategies are used to overcome\nmodel forgetting to adapt to the emergence of new loads. This paper conducts a\nlarge number of experiments on high-sampling rate load datasets, and compares a\nvariety of existing methods and model variants. The results show that the\nproposed method has achieved significant improvements in recognition accuracy.", "AI": {"tldr": "本文提出了一种结合图像负载特征和持续学习的非侵入式负载监测方法，通过将多维电力信号转换为图像特征，并利用深度卷积神经网络实现设备识别，同时引入自监督预训练和持续在线学习策略，显著提高了识别精度。", "motivation": "传统非侵入式负载监测方法在复杂多变的负载组合和应用环境中存在特征鲁棒性差和模型泛化能力不足的问题。", "method": "将电流、电压、功率因数等多维电力信号转换为视觉图像负载特征，结合深度卷积神经网络进行设备识别；引入自监督预训练和持续在线学习策略。", "result": "在高采样率负载数据集上的实验表明，该方法在识别精度上有显著提升。", "conclusion": "该方法通过图像负载特征和持续学习策略，有效解决了传统方法的局限性，提升了负载监测的准确性和适应性。"}}
{"id": "2506.06944", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06944", "abs": "https://arxiv.org/abs/2506.06944", "authors": ["Mellon M. Zhang", "Glen Chou", "Saibal Mukhopadhyay"], "title": "Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences", "comment": null, "summary": "Accurate and efficient object detection is essential for autonomous vehicles,\nwhere real-time perception requires low latency and high throughput. LiDAR\nsensors provide robust depth information, but conventional methods process full\n360{\\deg} scans in a single pass, introducing significant delay. Streaming\napproaches address this by sequentially processing partial scans in the native\npolar coordinate system, yet they rely on translation-invariant convolutions\nthat are misaligned with polar geometry -- resulting in degraded performance or\nrequiring complex distortion mitigation. Recent Mamba-based state space models\n(SSMs) have shown promise for LiDAR perception, but only in the full-scan\nsetting, relying on geometric serialization and positional embeddings that are\nmemory-intensive and ill-suited to streaming. We propose Polar Hierarchical\nMamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming\nLiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial\nencoding and a global forward Mamba for inter-sector temporal modeling,\nreplacing convolutions and positional encodings with distortion-aware,\ndimensionally-decomposed operations. PHiM sets a new state-of-the-art among\nstreaming detectors on the Waymo Open Dataset, outperforming the previous best\nby 10\\% and matching full-scan baselines at twice the throughput. Code will be\navailable at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .", "AI": {"tldr": "PHiM是一种新型状态空间模型，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba块实现高效检测，性能优于现有方法。", "motivation": "自动驾驶需要低延迟、高吞吐的实时感知，传统方法处理全扫描LiDAR数据延迟高，流式方法因极坐标几何与卷积不匹配导致性能下降。", "method": "PHiM采用局部双向Mamba块进行空间编码和全局前向Mamba块进行时间建模，替代卷积和位置编码，实现失真感知操作。", "result": "在Waymo Open Dataset上，PHiM性能优于之前最佳流式检测器10%，吞吐量翻倍。", "conclusion": "PHiM为极坐标流式LiDAR提供了一种高效解决方案，性能与全扫描基线相当，代码将开源。"}}
{"id": "2506.06644", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06644", "abs": "https://arxiv.org/abs/2506.06644", "authors": ["Chong You", "Kan Wu", "Zhipeng Jia", "Lin Chen", "Srinadh Bhojanapalli", "Jiaxian Guo", "Utku Evci", "Jan Wassenberg", "Praneeth Netrapalli", "Jeremiah J. Willcock", "Suvinay Subramanian", "Felix Chern", "Alek Andreev", "Shreya Pathak", "Felix Yu", "Prateek Jain", "David E. Culler", "Henry M. Levy", "Sanjiv Kumar"], "title": "Spark Transformer: Reactivating Sparsity in FFN and Attention", "comment": null, "summary": "The discovery of the lazy neuron phenomenon in trained Transformers, where\nthe vast majority of neurons in their feed-forward networks (FFN) are inactive\nfor each token, has spurred tremendous interests in activation sparsity for\nenhancing large model efficiency. While notable progress has been made in\ntranslating such sparsity to wall-time benefits, modern Transformers have moved\naway from the ReLU activation function crucial to this phenomenon. Existing\nefforts on re-introducing activation sparsity often degrade model quality,\nincrease parameter count, complicate or slow down training. Sparse attention,\nthe application of sparse activation to the attention mechanism, often faces\nsimilar challenges.\n  This paper introduces the Spark Transformer, a novel architecture that\nachieves a high level of activation sparsity in both FFN and the attention\nmechanism while maintaining model quality, parameter count, and standard\ntraining procedures. Our method realizes sparsity via top-k masking for\nexplicit control over sparsity level. Crucially, we introduce statistical\ntop-k, a hardware-accelerator-friendly, linear-time approximate algorithm that\navoids costly sorting and mitigates significant training slowdown from standard\ntop-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN\nparameters and attention key embeddings to form a low-cost predictor for\nidentifying activated entries. This design not only mitigates quality loss from\nenforced sparsity, but also enhances wall-time benefit. Pretrained with the\nGemma-2 recipe, Spark Transformer demonstrates competitive performance on\nstandard benchmarks while exhibiting significant sparsity: only 8% of FFN\nneurons are activated, and each token attends to a maximum of 256 tokens. This\nsparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time\nspeedups of up to 1.79x on CPU and 1.40x on GPU.", "AI": {"tldr": "Spark Transformer通过top-k掩码和统计top-k算法实现高激活稀疏性，保持模型质量的同时显著减少计算量。", "motivation": "探索如何在现代Transformer中重新引入激活稀疏性，同时避免模型质量下降、参数增加或训练复杂化。", "method": "采用top-k掩码和统计top-k算法控制稀疏性，并重新分配参数以低成本预测激活条目。", "result": "Spark Transformer在标准基准测试中表现优异，FFN神经元激活率仅8%，计算量减少2.5倍，解码速度提升1.79x（CPU）和1.40x（GPU）。", "conclusion": "Spark Transformer成功实现了高激活稀疏性，为大型模型效率提升提供了可行方案。"}}
{"id": "2506.07823", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07823", "abs": "https://arxiv.org/abs/2506.07823", "authors": ["Lorenzo Amatucci", "João Sousa-Pinto", "Giulio Turrisi", "Dominique Orban", "Victor Barasuol", "Claudio Semini"], "title": "Primal-Dual iLQR for GPU-Accelerated Learning and Control in Legged Robots", "comment": null, "summary": "This paper introduces a novel Model Predictive Control (MPC) implementation\nfor legged robot locomotion that leverages GPU parallelization. Our approach\nenables both temporal and state-space parallelization by incorporating a\nparallel associative scan to solve the primal-dual Karush-Kuhn-Tucker (KKT)\nsystem. In this way, the optimal control problem is solved in\n$\\mathcal{O}(n\\log{N} + m)$ complexity, instead of $\\mathcal{O}(N(n + m)^3)$,\nwhere $n$, $m$, and $N$ are the dimension of the system state, control vector,\nand the length of the prediction horizon. We demonstrate the advantages of this\nimplementation over two state-of-the-art solvers (acados and crocoddyl),\nachieving up to a 60\\% improvement in runtime for Whole Body Dynamics (WB)-MPC\nand a 700\\% improvement for Single Rigid Body Dynamics (SRBD)-MPC when varying\nthe prediction horizon length. The presented formulation scales efficiently\nwith the problem state dimensions as well, enabling the definition of a\ncentralized controller for up to 16 legged robots that can be computed in less\nthan 25 ms. Furthermore, thanks to the JAX implementation, the solver supports\nlarge-scale parallelization across multiple environments, allowing the\npossibility of performing learning with the MPC in the loop directly in GPU.", "AI": {"tldr": "本文提出了一种基于GPU并行化的新型模型预测控制（MPC）方法，用于腿式机器人运动控制，显著提高了计算效率。", "motivation": "传统MPC方法在解决腿式机器人运动控制问题时计算复杂度高，难以满足实时性需求。本文旨在通过GPU并行化降低计算复杂度。", "method": "采用并行关联扫描技术求解原始-对偶KKT系统，实现时间和状态空间的并行化，将计算复杂度从O(N(n+m)^3)降至O(nlogN + m)。", "result": "相比现有求解器（acados和crocoddyl），WB-MPC运行时间提升60%，SRBD-MPC提升700%，并能支持16个机器人的集中控制（计算时间<25ms）。", "conclusion": "该方法通过GPU并行化显著提升了MPC的计算效率，支持大规模并行化，为机器人运动控制提供了新的可能性。"}}
{"id": "2506.06952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06952", "abs": "https://arxiv.org/abs/2506.06952", "authors": ["Ying Shen", "Zhiyang Xu", "Jiuhai Chen", "Shizhe Diao", "Jiaxin Zhang", "Yuguang Yao", "Joy Rimchala", "Ismini Lourentzou", "Lifu Huang"], "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer", "comment": "Unified multimodal model, Flow-matching", "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.", "AI": {"tldr": "LaTtE-Flow是一种新型高效架构，统一了图像理解和生成，通过分层时间步专家流设计显著提升效率，实验显示其在多模态任务中表现优异且生成速度快。", "motivation": "现有统一模型需要大量预训练且性能不如专用模型，生成速度慢，限制了实际应用。", "method": "基于预训练视觉语言模型，引入分层时间步专家流架构和残差注意力机制，提高生成效率。", "result": "在多模态理解任务中表现优异，图像生成质量竞争性且推理速度快6倍。", "conclusion": "LaTtE-Flow为多模态任务提供了一种高效统一的解决方案。"}}
{"id": "2506.06649", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06649", "abs": "https://arxiv.org/abs/2506.06649", "authors": ["Yishan Shen", "Yuyang Ye", "Hui Xiong", "Yong Chen"], "title": "SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes", "comment": "Accepted by ICML 2025", "summary": "Dynamic treatment regimes (DTRs) are critical to precision medicine,\noptimizing long-term outcomes through personalized, real-time decision-making\nin evolving clinical contexts, but require careful supervision for unsafe\ntreatment risks. Existing efforts rely primarily on clinician-prescribed gold\nstandards despite the absence of a known optimal strategy, and predominantly\nusing structured EHR data without extracting valuable insights from clinical\nnotes, limiting their reliability for treatment recommendations. In this work,\nwe introduce SAFER, a calibrated risk-aware tabular-language recommendation\nframework for DTR that integrates both structured EHR and clinical notes,\nenabling them to learn from each other, and addresses inherent label\nuncertainty by assuming ambiguous optimal treatment solution for deceased\npatients. Moreover, SAFER employs conformal prediction to provide statistical\nguarantees, ensuring safe treatment recommendations while filtering out\nuncertain predictions. Experiments on two publicly available sepsis datasets\ndemonstrate that SAFER outperforms state-of-the-art baselines across multiple\nrecommendation metrics and counterfactual mortality rate, while offering robust\nformal assurances. These findings underscore SAFER potential as a trustworthy\nand theoretically grounded solution for high-stakes DTR applications.", "AI": {"tldr": "SAFER是一个结合结构化电子健康记录和临床笔记的风险感知推荐框架，用于动态治疗策略（DTR），通过统计保证提供安全治疗建议。", "motivation": "现有DTR方法依赖临床医生规定的金标准，且未充分利用临床笔记，限制了治疗建议的可靠性。", "method": "SAFER整合结构化数据和临床笔记，利用共形预测提供统计保证，并处理标签不确定性。", "result": "在公开的脓毒症数据集上，SAFER在推荐指标和反事实死亡率上优于现有方法。", "conclusion": "SAFER是一个可信赖且理论扎实的高风险DTR解决方案。"}}
{"id": "2506.06953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06953", "abs": "https://arxiv.org/abs/2506.06953", "authors": ["Maciej Zyrek", "Tomasz Tarasiewicz", "Jakub Sadel", "Aleksandra Krzywon", "Michal Kawulok"], "title": "Task-driven real-world super-resolution of document scans", "comment": null, "summary": "Single-image super-resolution refers to the reconstruction of a\nhigh-resolution image from a single low-resolution observation. Although recent\ndeep learning-based methods have demonstrated notable success on simulated\ndatasets -- with low-resolution images obtained by degrading and downsampling\nhigh-resolution ones -- they frequently fail to generalize to real-world\nsettings, such as document scans, which are affected by complex degradations\nand semantic variability. In this study, we introduce a task-driven, multi-task\nlearning framework for training a super-resolution network specifically\noptimized for optical character recognition tasks. We propose to incorporate\nauxiliary loss functions derived from high-level vision tasks, including text\ndetection using the connectionist text proposal network, text recognition via a\nconvolutional recurrent neural network, keypoints localization using Key.Net,\nand hue consistency. To balance these diverse objectives, we employ dynamic\nweight averaging mechanism, which adaptively adjusts the relative importance of\neach loss term based on its convergence behavior. We validate our approach upon\nthe SRResNet architecture, which is a well-established technique for\nsingle-image super-resolution. Experimental evaluations on both simulated and\nreal-world scanned document datasets demonstrate that the proposed approach\nimproves text detection, measured with intersection over union, while\npreserving overall image fidelity. These findings underscore the value of\nmulti-objective optimization in super-resolution models for bridging the gap\nbetween simulated training regimes and practical deployment in real-world\nscenarios.", "AI": {"tldr": "论文提出了一种针对光学字符识别任务优化的多任务学习框架，通过结合高级视觉任务的辅助损失函数，动态调整权重，提升了真实场景下的超分辨率性能。", "motivation": "现有深度学习方法在模拟数据集上表现良好，但在真实场景（如文档扫描）中泛化能力不足，需解决复杂退化和语义变化问题。", "method": "采用多任务学习框架，结合文本检测、识别、关键点定位和色调一致性等辅助损失函数，并动态调整权重。基于SRResNet架构实现。", "result": "实验表明，该方法在模拟和真实文档数据集上提升了文本检测性能（IoU指标），同时保持了图像保真度。", "conclusion": "多目标优化有助于缩小模拟训练与真实部署之间的差距，提升超分辨率模型在实际场景中的应用价值。"}}
{"id": "2506.06656", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06656", "abs": "https://arxiv.org/abs/2506.06656", "authors": ["Ittai Rubinstein", "Samuel B. Hopkins"], "title": "Rescaled Influence Functions: Accurate Data Attribution in High Dimension", "comment": null, "summary": "How does the training data affect a model's behavior? This is the question we\nseek to answer with data attribution. The leading practical approaches to data\nattribution are based on influence functions (IF). IFs utilize a first-order\nTaylor approximation to efficiently predict the effect of removing a set of\nsamples from the training set without retraining the model, and are used in a\nwide variety of machine learning applications. However, especially in the\nhigh-dimensional regime (# params $\\geq \\Omega($# samples$)$), they are often\nimprecise and tend to underestimate the effect of sample removals, even for\nsimple models such as logistic regression. We present rescaled influence\nfunctions (RIF), a new tool for data attribution which can be used as a drop-in\nreplacement for influence functions, with little computational overhead but\nsignificant improvement in accuracy. We compare IF and RIF on a range of\nreal-world datasets, showing that RIFs offer significantly better predictions\nin practice, and present a theoretical analysis explaining this improvement.\nFinally, we present a simple class of data poisoning attacks that would fool\nIF-based detections but would be detected by RIF.", "AI": {"tldr": "论文提出了RIF方法，作为IF的改进，显著提高了数据归因的准确性。", "motivation": "研究训练数据如何影响模型行为，特别是IF在高维情况下不精确的问题。", "method": "提出RIF方法，作为IF的替代方案，计算开销小但精度更高。", "result": "实验表明RIF在真实数据集上预测效果显著优于IF。", "conclusion": "RIF能更准确地预测数据影响，并能检测IF无法识别的数据投毒攻击。"}}
{"id": "2506.07924", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07924", "abs": "https://arxiv.org/abs/2506.07924", "authors": ["Zhizun Xu", "Baozhu Jia", "Weichao Shi"], "title": "Design and Implementation of a Peer-to-Peer Communication, Modular and Decentral YellowCube UUV", "comment": null, "summary": "The underwater Unmanned Vehicles(UUVs) are pivot tools for offshore\nengineering and oceanographic research. Most existing UUVs do not facilitate\neasy integration of new or upgraded sensors. A solution to this problem is to\nhave a modular UUV system with changeable payload sections capable of carrying\ndifferent sensor to suite different missions. The design and implementation of\na modular and decentral UUV named YellowCube is presented in the paper. Instead\na centralised software architecture which is adopted by the other modular\nunderwater vehicles designs, a Peer-To-Peer(P2P) communication mechanism is\nimplemented among the UUV's modules. The experiments in the laboratory and sea\ntrials have been executed to verify the performances of the UUV.", "AI": {"tldr": "论文提出了一种模块化和去中心化的水下无人航行器（UUV）YellowCube，采用P2P通信机制，解决了现有UUV难以集成新传感器的问题。", "motivation": "现有UUV难以集成或升级传感器，限制了其多功能性和适应性。", "method": "设计并实现了一种模块化和去中心化的UUV YellowCube，采用P2P通信机制替代集中式架构。", "result": "实验室和海上试验验证了YellowCube的性能。", "conclusion": "YellowCube的模块化和P2P通信机制为UUV的传感器集成提供了灵活解决方案。"}}
{"id": "2506.06962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06962", "abs": "https://arxiv.org/abs/2506.06962", "authors": ["Jingyuan Qi", "Zhiyang Xu", "Qifan Wang", "Lifu Huang"], "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation", "comment": "Image Generation, Retrieval Augmented Generation", "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.", "AI": {"tldr": "AR-RAG是一种通过自回归方式在图像生成过程中动态检索并融合相关视觉参考的新方法，解决了现有方法的局限性。", "motivation": "现有方法在生成前进行静态检索，可能导致过度复制或风格偏差，AR-RAG旨在通过动态检索提升生成质量。", "method": "提出两种框架：DAiD（直接合并预测与检索分布）和FAiD（通过多尺度卷积平滑检索特征）。", "result": "在多个基准测试中显著优于现有图像生成模型。", "conclusion": "AR-RAG通过动态检索和融合机制有效提升了图像生成的灵活性和质量。"}}
{"id": "2506.06665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06665", "abs": "https://arxiv.org/abs/2506.06665", "authors": ["Hong-Ming Chiu", "Hao Chen", "Huan Zhang", "Richard Y. Zhang"], "title": "SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming", "comment": "ICML 2025", "summary": "Neural network verifiers based on linear bound propagation scale impressively\nto massive models but can be surprisingly loose when neuron coupling is\ncrucial. Conversely, semidefinite programming (SDP) verifiers capture\ninter-neuron coupling naturally, but their cubic complexity restricts them to\nonly small models. In this paper, we propose SDP-CROWN, a novel hybrid\nverification framework that combines the tightness of SDP relaxations with the\nscalability of bound-propagation verifiers. At the core of SDP-CROWN is a new\nlinear bound, derived via SDP principles, that explicitly captures\n$\\ell_{2}$-norm-based inter-neuron coupling while adding only one extra\nparameter per layer. This bound can be integrated seamlessly into any linear\nbound-propagation pipeline, preserving the inherent scalability of such methods\nyet significantly improving tightness. In theory, we prove that our\ninter-neuron bound can be up to a factor of $\\sqrt{n}$ tighter than traditional\nper-neuron bounds. In practice, when incorporated into the state-of-the-art\n$\\alpha$-CROWN verifier, we observe markedly improved verification performance\non large models with up to 65 thousand neurons and 2.47 million parameters,\nachieving tightness that approaches that of costly SDP-based methods.", "AI": {"tldr": "SDP-CROWN结合了SDP的紧致性和线性边界传播的可扩展性，显著提升了大规模神经网络的验证性能。", "motivation": "现有线性边界传播验证器在神经元耦合时表现松散，而SDP验证器虽紧致但计算复杂度高，无法应用于大规模模型。", "method": "提出SDP-CROWN框架，通过SDP原理推导新的线性边界，显式捕获神经元间的耦合，同时保持可扩展性。", "result": "理论证明新边界比传统方法紧致√n倍，实践验证在大型模型上性能显著提升，接近SDP方法的紧致性。", "conclusion": "SDP-CROWN成功结合了SDP的紧致性和线性边界传播的可扩展性，为大规模神经网络验证提供了高效解决方案。"}}
{"id": "2506.07961", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07961", "abs": "https://arxiv.org/abs/2506.07961", "authors": ["Peiyan Li", "Yixiang Chen", "Hongtao Wu", "Xiao Ma", "Xiangnan Wu", "Yan Huang", "Liang Wang", "Tao Kong", "Tieniu Tan"], "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models", "comment": "In Submission", "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/", "AI": {"tldr": "BridgeVLA是一种新型3D视觉-语言-动作模型，通过将3D输入投影到2D图像并利用2D热图进行动作预测，显著提高了样本效率和性能。", "motivation": "现有方法在将3D信号融入视觉-语言模型（VLMs）时未能充分利用3D数据的空间结构，导致样本效率低下。", "method": "BridgeVLA将3D输入投影为多个2D图像，并使用2D热图进行动作预测，同时提出了一种可扩展的预训练方法。", "result": "在多个仿真基准测试和真实机器人实验中，BridgeVLA表现优于现有方法，平均成功率显著提升，并展示了出色的样本效率。", "conclusion": "BridgeVLA通过统一输入和输出空间，有效提升了3D操作学习的效率和性能，具有广泛的应用潜力。"}}
{"id": "2506.06966", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06966", "abs": "https://arxiv.org/abs/2506.06966", "authors": ["Siyuan Jing", "Guangxue Wang", "Haoyang Zhai", "Qin Tao", "Jun Yang", "Bing Wang", "Peng Jin"], "title": "Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition", "comment": "18 pages, 3 figures", "summary": "Due to the emergence of many sign language datasets, isolated sign language\nrecognition (ISLR) has made significant progress in recent years. In addition,\nthe development of various advanced deep neural networks is another reason for\nthis breakthrough. However, challenges remain in applying the technique in the\nreal world. First, existing sign language datasets do not cover the whole sign\nvocabulary. Second, most of the sign language datasets provide only single view\nRGB videos, which makes it difficult to handle hand occlusions when performing\nISLR. To fill this gap, this paper presents a dual-view sign language dataset\nfor ISLR named NationalCSL-DP, which fully covers the Chinese national sign\nlanguage vocabulary. The dataset consists of 134140 sign videos recorded by ten\nsigners with respect to two vertical views, namely, the front side and the left\nside. Furthermore, a CNN transformer network is also proposed as a strong\nbaseline and an extremely simple but effective fusion strategy for prediction.\nExtensive experiments were conducted to prove the effectiveness of the datasets\nas well as the baseline. The results show that the proposed fusion strategy can\nsignificantly increase the performance of the ISLR, but it is not easy for the\nsequence-to-sequence model, regardless of whether the early-fusion or\nlate-fusion strategy is applied, to learn the complementary features from the\nsign videos of two vertical views.", "AI": {"tldr": "本文提出了一个双视角中国手语数据集NationalCSL-DP，并设计了CNN-Transformer网络作为基线模型，通过实验验证了数据集和融合策略的有效性。", "motivation": "现有手语数据集覆盖不全且多为单视角，难以应对手部遮挡问题，因此需要构建更全面的双视角数据集。", "method": "提出双视角手语数据集NationalCSL-DP，并设计CNN-Transformer网络及简单有效的融合策略。", "result": "实验表明融合策略显著提升识别性能，但序列到序列模型难以从双视角视频中学习互补特征。", "conclusion": "双视角数据集和融合策略有效，但需进一步优化模型以更好地利用双视角信息。"}}
{"id": "2506.06666", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06666", "abs": "https://arxiv.org/abs/2506.06666", "authors": ["Oktay Karakuş", "Hasan Arkadaş"], "title": "Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering", "comment": "12 pages and 5 figures", "summary": "Line-breaking passes (LBPs) are crucial tactical actions in football,\nallowing teams to penetrate defensive lines and access high-value spaces. In\nthis study, we present an unsupervised, clustering-based framework for\ndetecting and analysing LBPs using synchronised event and tracking data from\nelite matches. Our approach models opponent team shape through vertical spatial\nsegmentation and identifies passes that disrupt defensive lines within open\nplay. Beyond detection, we introduce several tactical metrics, including the\nspace build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and\nLBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or\nsustained attacking threats. We evaluate these metrics across teams and players\nin the 2022 FIFA World Cup, revealing stylistic differences in vertical\nprogression and structural disruption. The proposed methodology is explainable,\nscalable, and directly applicable to modern performance analysis and scouting\nworkflows.", "AI": {"tldr": "本文提出了一种基于聚类的无监督框架，用于检测和分析足球中的线突破传球（LBPs），并引入了量化其效果的战术指标。", "motivation": "线突破传球（LBPs）是足球中关键的战术行为，能够穿透防线并进入高价值区域，但缺乏系统化的分析方法。", "method": "通过垂直空间分割建模对手队形，并识别开放比赛中破坏防线的传球。引入了空间构建比（SBR）和两种链式指标（LBPCh^1和LBPCh^2）来量化LBPs的效果。", "result": "在2022年世界杯中评估了这些指标，揭示了不同球队和球员在垂直推进和结构破坏上的风格差异。", "conclusion": "该方法具有可解释性、可扩展性，可直接应用于现代比赛分析和球探工作流程。"}}
{"id": "2506.06970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06970", "abs": "https://arxiv.org/abs/2506.06970", "authors": ["Pengfei Zhao", "Rongbo Luan", "Wei Zhang", "Peng Wu", "Sifeng He"], "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment", "comment": null, "summary": "Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability\nto retrieve content across modalities, a substantial modality gap persists in\nits feature space. Intriguingly, we discover that off-the-shelf MLLMs\n(Multimodal Large Language Models) demonstrate powerful inherent modality\nalignment properties. While recent MLLM-based retrievers with unified\narchitectures partially mitigate this gap, their reliance on coarse modality\nalignment mechanisms fundamentally limits their potential. In this work, We\nintroduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel\nframework that leverages the fine grained alignment priors inherent in MLLM to\nguide cross modal representation learning. MAPLE formulates the learning\nprocess as reinforcement learning with two key components: (1) Automatic\npreference data construction using off-the-shelf MLLM, and (2) a new Relative\nPreference Alignment (RPA) loss, which adapts Direct Preference Optimization\n(DPO) to the embedding learning setting. Experimental results show that our\npreference-guided alignment achieves substantial gains in fine-grained\ncross-modal retrieval, underscoring its effectiveness in handling nuanced\nsemantic distinctions.", "AI": {"tldr": "MAPLE利用MLLM的细粒度对齐先验，通过强化学习框架和新的RPA损失函数，显著提升了跨模态检索的细粒度语义对齐能力。", "motivation": "CLIP在多模态检索中存在模态鸿沟问题，而现有MLLM虽具备对齐能力，但机制较粗糙。MAPLE旨在利用MLLM的细粒度对齐先验优化跨模态表示学习。", "method": "提出MAPLE框架，包含自动偏好数据构建和RPA损失函数，将学习过程建模为强化学习。", "result": "实验表明，MAPLE在细粒度跨模态检索中取得显著提升。", "conclusion": "MAPLE通过偏好引导的对齐机制，有效解决了跨模态检索中的细粒度语义对齐问题。"}}
{"id": "2506.06682", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06682", "abs": "https://arxiv.org/abs/2506.06682", "authors": ["Di Lin", "Wanjing Ren", "Xuanbin Li", "Rui Zhang"], "title": "Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics", "comment": null, "summary": "In graph self-supervised learning, masked autoencoders (MAE) and contrastive\nlearning (CL) are two prominent paradigms. MAE focuses on reconstructing masked\nelements, while CL maximizes similarity between augmented graph views. Recent\nstudies highlight their complementarity: MAE excels at local feature capture,\nand CL at global information extraction. Hybrid frameworks for homogeneous\ngraphs have been proposed, but face challenges in designing shared encoders to\nmeet the semantic requirements of both tasks. In semantically sparse scenarios,\nCL struggles with view construction, and gradient imbalance between positive\nand negative samples persists. This paper introduces HetCRF, a novel\ndual-channel self-supervised learning framework for heterogeneous graphs.\nHetCRF uses a two-stage aggregation strategy to adapt embedding semantics,\nmaking it compatible with both MAE and CL. To address semantic sparsity, it\nenhances encoder output for view construction instead of relying on raw\nfeatures, improving efficiency. Two positive sample augmentation strategies are\nalso proposed to balance gradient contributions. Node classification\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHetCRF outperforms state-of-the-art baselines. On datasets with missing node\nfeatures, such as Aminer and Freebase, at a 40% label rate in node\nclassification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%\nrespectively compared to the second-best baseline, validating its effectiveness\nand superiority.", "AI": {"tldr": "HetCRF是一种针对异构图的双通道自监督学习框架，结合了MAE和CL的优势，通过两阶段聚合策略和正样本增强策略，显著提升了节点分类性能。", "motivation": "现有混合框架在同构图中表现良好，但在异构图和语义稀疏场景中面临共享编码器设计、视图构建困难和梯度不平衡等挑战。", "method": "提出HetCRF框架，采用两阶段聚合策略适应嵌入语义，增强编码器输出以改进视图构建，并提出两种正样本增强策略平衡梯度贡献。", "result": "在四个真实异构图数据集上的节点分类实验中，HetCRF在缺失节点特征的情况下（如Aminer和Freebase），Macro-F1分数分别提升了2.75%和2.2%。", "conclusion": "HetCRF通过结合MAE和CL的优势，在异构图自监督学习中表现出色，验证了其有效性和优越性。"}}
{"id": "2506.06988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06988", "abs": "https://arxiv.org/abs/2506.06988", "authors": ["Binxiao Huang", "Zhihao Li", "Shiyong Liu", "Xiao Tang", "Jiajun Tang", "Jiaqi Lin", "Yuxin Cheng", "Zhenyu Chen", "Xiaofei Wu", "Ngai Wong"], "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction", "comment": null, "summary": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in\nimage-based 3D reconstruction and real-time rendering. However, regions with\ncomplex textures require numerous Gaussians to capture significant color\nvariations accurately, leading to inefficiencies in rendering speed. To address\nthis challenge, we introduce a hybrid representation for indoor scenes that\ncombines 3DGS with textured meshes. Our approach uses textured meshes to handle\ntexture-rich flat areas, while retaining Gaussians to model intricate\ngeometries. The proposed method begins by pruning and refining the extracted\nmesh to eliminate geometrically complex regions. We then employ a joint\noptimization for 3DGS and mesh, incorporating a warm-up strategy and\ntransmittance-aware supervision to balance their contributions\nseamlessly.Extensive experiments demonstrate that the hybrid representation\nmaintains comparable rendering quality and achieves superior frames per second\nFPS with fewer Gaussian primitives.", "AI": {"tldr": "论文提出了一种结合3D高斯点与纹理网格的混合表示方法，用于提升复杂纹理区域的渲染效率。", "motivation": "解决3D高斯点在复杂纹理区域需要大量高斯点导致渲染速度下降的问题。", "method": "通过修剪和优化网格处理平坦区域，保留高斯点建模复杂几何，采用联合优化和透射感知监督。", "result": "实验表明混合表示在保持渲染质量的同时，显著提升了FPS并减少了高斯点数量。", "conclusion": "混合表示方法在室内场景中实现了高效且高质量的渲染。"}}
{"id": "2506.06694", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.06694", "abs": "https://arxiv.org/abs/2506.06694", "authors": ["Yuan Yuan", "Yukun Liu", "Chonghua Han", "Jie Feng", "Yong Li"], "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning", "comment": null, "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.", "AI": {"tldr": "MoveGCL是一个隐私保护的移动性基础模型框架，通过生成式持续学习实现分散式模型进化，性能接近联合训练，优于联邦学习。", "motivation": "移动性数据隐私敏感且分散，难以构建通用基础模型。MoveGCL旨在填补这一空白。", "method": "采用生成式持续学习，通过冻结教师模型生成合成轨迹，结合知识蒸馏和Mixture-of-Experts Transformer。", "result": "在六个真实数据集上表现接近联合训练，显著优于联邦学习，同时保护隐私。", "conclusion": "MoveGCL为移动性基础模型提供了开放、可扩展且隐私保护的开发蓝图。"}}
{"id": "2506.06992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06992", "abs": "https://arxiv.org/abs/2506.06992", "authors": ["Yanting Gao", "Yepeng Liu", "Junming Liu", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization", "comment": "22 pages", "summary": "Exploring effective and transferable adversarial examples is vital for\nunderstanding the characteristics and mechanisms of Vision Transformers (ViTs).\nHowever, adversarial examples generated from surrogate models often exhibit\nweak transferability in black-box settings due to overfitting. Existing methods\nimprove transferability by diversifying perturbation inputs or applying uniform\ngradient regularization within surrogate models, yet they have not fully\nleveraged the shared and unique features of surrogate models trained on the\nsame task, leading to suboptimal transfer performance. Therefore, enhancing\nperturbations of common information shared by surrogate models and suppressing\nthose tied to individual characteristics offers an effective way to improve\ntransferability. Accordingly, we propose a commonality-oriented gradient\noptimization strategy (COGO) consisting of two components: Commonality\nEnhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low\nfrequency regions, leveraging the fact that ViTs trained on the same dataset\ntend to rely more on mid-to-low frequency information for classification. IS\nemploys adaptive thresholds to evaluate the correlation between backpropagated\ngradients and model individuality, assigning weights to gradients accordingly.\nExtensive experiments demonstrate that COGO significantly improves the transfer\nsuccess rates of adversarial attacks, outperforming current state-of-the-art\nmethods.", "AI": {"tldr": "论文提出了一种共性导向的梯度优化策略（COGO），通过增强共性信息和抑制个体特征，显著提高了对抗样本在黑盒设置中的迁移性能。", "motivation": "理解Vision Transformers（ViTs）的特性与机制需要有效的对抗样本，但现有方法因过拟合导致迁移性不足。", "method": "COGO包含共性增强（CE）和个体抑制（IS）两部分：CE扰动中低频区域，IS通过自适应阈值评估梯度相关性。", "result": "实验表明，COGO显著提升了对抗攻击的迁移成功率，优于现有方法。", "conclusion": "COGO通过优化共性信息与抑制个体特征，为提升对抗样本迁移性提供了有效途径。"}}
{"id": "2506.06699", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.06699", "abs": "https://arxiv.org/abs/2506.06699", "authors": ["Rajeev Bhatt Ambati", "James Lester", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "MarginSel : Max-Margin Demonstration Selection for LLMs", "comment": null, "summary": "Large Language Models (LLMs) excel at few-shot learning via in-context\nlearning (ICL). However, the effectiveness of ICL is often sensitive to the\nselection and ordering of demonstration examples. To address this, we present\nMarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that\nselects hard demonstration examples for the ICL prompt, adapting to each test\ninstance. Our approach achieves 2-7% absolute improvement in F1-score across\nclassification tasks, compared to a random selection of examples. We also\nprovide theoretical insights and empirical evidence showing that MarginSel\ninduces max-margin behavior in LLMs by effectively increasing the margin for\nhard examples, analogous to support vectors, thereby shifting the decision\nboundary in a beneficial direction.", "AI": {"tldr": "MarginSel是一种通过选择困难示例来优化ICL的方法，显著提升了LLM在分类任务中的表现。", "motivation": "ICL的效果对示例选择和顺序敏感，需要一种自适应的方法来优化演示示例的选择。", "method": "提出MarginSel，一种两步法，为每个测试实例选择困难的演示示例。", "result": "在分类任务中，F1分数绝对提升了2-7%。", "conclusion": "MarginSel通过增加困难示例的边界，有效改善了LLM的决策边界。"}}
{"id": "2506.06993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06993", "abs": "https://arxiv.org/abs/2506.06993", "authors": ["Cong Guan", "Jiacheng Ying", "Yuya Ieiri", "Osamu Yoshie"], "title": "DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching", "comment": null, "summary": "Dual-camera super-resolution is highly practical for smartphone photography\nthat primarily super-resolve the wide-angle images using the telephoto image as\na reference. In this paper, we propose DM$^3$Net, a novel dual-camera\nsuper-resolution network based on Domain Modulation and Multi-scale Matching.\nTo bridge the domain gap between the high-resolution domain and the degraded\ndomain, we learn two compressed global representations from image pairs\ncorresponding to the two domains. To enable reliable transfer of high-frequency\nstructural details from the reference image, we design a multi-scale matching\nmodule that conducts patch-level feature matching and retrieval across multiple\nreceptive fields to improve matching accuracy and robustness. Moreover, we also\nintroduce Key Pruning to achieve a significant reduction in memory usage and\ninference time with little model performance sacrificed. Experimental results\non three real-world datasets demonstrate that our DM$^3$Net outperforms the\nstate-of-the-art approaches.", "AI": {"tldr": "DM$^3$Net是一种基于域调制和多尺度匹配的双摄像头超分辨率网络，通过压缩全局表示和多尺度匹配模块提升性能，同时引入关键剪枝减少资源消耗。", "motivation": "解决智能手机摄影中广角图像与长焦图像之间的域差距问题，提升超分辨率效果。", "method": "学习两个压缩的全局表示以缩小域差距，设计多尺度匹配模块进行特征匹配，并引入关键剪枝优化资源使用。", "result": "在三个真实数据集上表现优于现有方法。", "conclusion": "DM$^3$Net在性能和效率上均优于现有技术。"}}
{"id": "2506.06701", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2506.06701", "abs": "https://arxiv.org/abs/2506.06701", "authors": ["Fudong Lin", "Wanrou Du", "Jinchan Liu", "Tarikul Milon", "Shelby Meche", "Wu Xu", "Xiaoqi Qin", "Xu Yuan"], "title": "Do Protein Transformers Have Biological Intelligence?", "comment": "Accepted by European Conference on Machine Learning and Principles\n  and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)", "summary": "Deep neural networks, particularly Transformers, have been widely adopted for\npredicting the functional properties of proteins. In this work, we focus on\nexploring whether Protein Transformers can capture biological intelligence\namong protein sequences. To achieve our goal, we first introduce a protein\nfunction dataset, namely Protein-FN, providing over 9000 protein data with\nmeaningful labels. Second, we devise a new Transformer architecture, namely\nSequence Protein Transformers (SPT), for computationally efficient protein\nfunction predictions. Third, we develop a novel Explainable Artificial\nIntelligence (XAI) technique called Sequence Score, which can efficiently\ninterpret the decision-making processes of protein models, thereby overcoming\nthe difficulty of deciphering biological intelligence bided in Protein\nTransformers. Remarkably, even our smallest SPT-Tiny model, which contains only\n5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%\non the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,\nall accomplished by training from scratch. Besides, our Sequence Score\ntechnique helps reveal that our SPT models can discover several meaningful\npatterns underlying the sequence structures of protein data, with these\npatterns aligning closely with the domain knowledge in the biology community.\nWe have officially released our Protein-FN dataset on Hugging Face Datasets\nhttps://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at\nhttps://github.com/fudong03/BioIntelligence.", "AI": {"tldr": "论文提出了一种新的Transformer架构（SPT）和可解释AI技术（Sequence Score），用于高效预测蛋白质功能并揭示其生物智能。", "motivation": "探索蛋白质Transformer是否能捕捉蛋白质序列中的生物智能。", "method": "1. 引入Protein-FN数据集；2. 设计SPT架构；3. 开发Sequence Score技术。", "result": "SPT-Tiny模型在AR和Protein-FN数据集上分别达到94.3%和99.6%的准确率，且Sequence Score揭示了与生物学知识一致的序列模式。", "conclusion": "SPT和Sequence Score技术成功捕捉蛋白质序列中的生物智能，并提供了可解释性。"}}
{"id": "2506.06995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06995", "abs": "https://arxiv.org/abs/2506.06995", "authors": ["Xiaoya Zhang"], "title": "Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems", "comment": "Winner of the GOOSE 3D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "This technical report presents the implementation details of the winning\nsolution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This\nchallenge focuses on semantic segmentation of 3D point clouds from diverse\nunstructured outdoor environments collected from multiple robotic platforms.\nThis problem was addressed by implementing Point Prompt Tuning (PPT) integrated\nwith Point Transformer v3 (PTv3) backbone, enabling adaptive processing of\nheterogeneous LiDAR data through platform-specific conditioning and\ncross-dataset class alignment strategies. The model is trained without\nrequiring additional external data. As a result, this approach achieved\nsubstantial performance improvements with mIoU increases of up to 22.59% on\nchallenging platforms compared to the baseline PTv3 model, demonstrating the\neffectiveness of adaptive point cloud understanding for field robotics\napplications.", "AI": {"tldr": "本文介绍了ICRA 2025 GOOSE 3D语义分割挑战赛的获胜方案，采用Point Prompt Tuning与Point Transformer v3结合的方法，显著提升了性能。", "motivation": "解决多机器人平台采集的异构LiDAR数据在3D点云语义分割中的挑战。", "method": "结合Point Prompt Tuning（PPT）和Point Transformer v3（PTv3）骨干网络，通过平台特定条件处理和跨数据集类别对齐策略。", "result": "相比基线PTv3模型，mIoU提升高达22.59%。", "conclusion": "该方法展示了自适应点云理解在野外机器人应用中的有效性。"}}
{"id": "2506.06715", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06715", "abs": "https://arxiv.org/abs/2506.06715", "authors": ["Minh-Duc Nguyen", "Dung D. Le"], "title": "A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks", "comment": "Paper is under review", "summary": "Pareto Set Learning (PSL) is popular as an efficient approach to obtaining\nthe complete optimal solution in Multi-objective Learning (MOL). A set of\noptimal solutions approximates the Pareto set, and its mapping is a set of\ndense points in the Pareto front in objective space. However, some current\nmethods face a challenge: how to make the Pareto solution is diverse while\nmaximizing the hypervolume value. In this paper, we propose a novel method to\naddress this challenge, which employs Stein Variational Gradient Descent (SVGD)\nto approximate the entire Pareto set. SVGD pushes a set of particles towards\nthe Pareto set by applying a form of functional gradient descent, which helps\nto converge and diversify optimal solutions. Additionally, we employ diverse\ngradient direction strategies to thoroughly investigate a unified framework for\nSVGD in multi-objective optimization and adapt this framework with an annealing\nschedule to promote stability. We introduce our method, SVH-MOL, and validate\nits effectiveness through extensive experiments on multi-objective problems and\nmulti-task learning, demonstrating its superior performance.", "AI": {"tldr": "PSL通过SVGD方法解决多目标学习中Pareto解的多样性与超体积最大化问题，提出SVH-MOL框架并通过实验验证其优越性。", "motivation": "当前方法在保持Pareto解多样性的同时最大化超体积值方面存在挑战。", "method": "采用SVGD逼近Pareto集，结合多样化梯度方向和退火策略提升稳定性。", "result": "SVH-MOL在多目标问题和多任务学习中表现出色。", "conclusion": "SVH-MOL有效解决了Pareto解多样性与超体积优化的平衡问题。"}}
{"id": "2506.06954", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.06954", "abs": "https://arxiv.org/abs/2506.06954", "authors": ["Clinton Enwerem", "Aniruddh G. Puranic", "John S. Baras", "Calin Belta"], "title": "Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression", "comment": "13 pages, 4 figures. Submission under review", "summary": "Mainstream approximate action-value iteration reinforcement learning (RL)\nalgorithms suffer from overestimation bias, leading to suboptimal policies in\nhigh-variance stochastic environments. Quantile-based action-value iteration\nmethods reduce this bias by learning a distribution of the expected cost-to-go\nusing quantile regression. However, ensuring that the learned policy satisfies\nsafety constraints remains a challenge when these constraints are not\nexplicitly integrated into the RL framework. Existing methods often require\ncomplex neural architectures or manual tradeoffs due to combined cost\nfunctions. To address this, we propose a risk-regularized quantile-based\nalgorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety\nwithout complex architectures. We also provide theoretical guarantees on the\ncontraction properties of the risk-sensitive distributional Bellman operator in\nWasserstein space, ensuring convergence to a unique cost distribution.\nSimulations of a mobile robot in a dynamic reach-avoid task show that our\napproach leads to more goal successes, fewer collisions, and better\nsafety-performance trade-offs compared to risk-neutral methods.", "AI": {"tldr": "提出一种基于分位数的风险正则化强化学习算法，通过CVaR整合安全性约束，避免复杂架构，并在动态任务中表现优于风险中性方法。", "motivation": "主流近似动作值迭代RL算法在高方差随机环境中存在高估偏差，导致策略次优，且现有方法难以确保安全性约束。", "method": "提出一种风险正则化的分位数方法，整合CVaR以强制安全性，无需复杂架构，并证明其在Wasserstein空间中的收敛性。", "result": "在动态避障任务中，该方法比风险中性方法实现更多目标成功、更少碰撞和更好的安全性能权衡。", "conclusion": "该方法有效解决了RL中的高估偏差和安全性约束问题，具有理论和实践优势。"}}
{"id": "2506.07002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07002", "abs": "https://arxiv.org/abs/2506.07002", "authors": ["Yunxiao Shi", "Hong Cai", "Jisoo Jeong", "Yinhao Zhu", "Shizhong Han", "Amin Ansari", "Fatih Porikli"], "title": "BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction", "comment": "Two-page abstract version available at CVPR 2025 Embodied AI Workshop", "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, BePo, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed BePo. Moreover,\nBePo also delivers competitive inference speed when compared to the latest\nefficient approaches.", "AI": {"tldr": "本文提出了一种结合BEV和稀疏点表示的3D占用预测方法BePo，通过双分支设计解决现有方法的不足。", "motivation": "现有3D占用预测方法计算成本高或存在信息丢失问题，BEV对小物体表现不佳，稀疏点对大物体或平面效率低。", "method": "采用双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息并融合输出。", "result": "在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，推理速度与最新高效方法相当。", "conclusion": "BePo通过结合BEV和稀疏点表示，有效解决了小物体和大物体的3D占用预测问题。"}}
{"id": "2506.06761", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06761", "abs": "https://arxiv.org/abs/2506.06761", "authors": ["Adrià Molina Rodríguez", "Oriol Ramos Terrades", "Josep Lladós"], "title": "The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing", "comment": null, "summary": "Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.", "AI": {"tldr": "提出了一种通过模型编辑增强低资源语言识别的方法，显著提升了跨域和跨字母表的迁移学习性能。", "motivation": "解决低资源语言（如古代手稿和非西方语言）在识别系统中代表性不足的问题，提升模型的泛化能力。", "method": "利用模型编辑技术，结合领域合并策略，优化稀疏数据分布下的学习效果。", "result": "在迁移学习和跨域评估中表现优异，特别是在历史密码文本和非拉丁字母表上。", "conclusion": "该方法为低资源语言的识别提供了一种高效解决方案，扩展了文档识别的适用范围。"}}
{"id": "2506.07286", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07286", "abs": "https://arxiv.org/abs/2506.07286", "authors": ["Aditya Chakravarty"], "title": "Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI", "comment": "Accepted in CVPR 2025 Embodied AI Workshop", "summary": "Diffusion models have shown remarkable flexibility for solving inverse\nproblems without task-specific retraining. However, existing approaches such as\nManifold Preserving Guided Diffusion (MPGD) apply only a single gradient update\nper denoising step, limiting restoration fidelity and robustness, especially in\nembedded or out-of-distribution settings. In this work, we introduce a\nmultistep optimization strategy within each denoising timestep, significantly\nenhancing image quality, perceptual accuracy, and generalization. Our\nexperiments on super-resolution and Gaussian deblurring demonstrate that\nincreasing the number of gradient updates per step improves LPIPS and PSNR with\nminimal latency overhead. Notably, we validate this approach on a Jetson Orin\nNano using degraded ImageNet and a UAV dataset, showing that MPGD, originally\ntrained on face datasets, generalizes effectively to natural and aerial scenes.\nOur findings highlight MPGD's potential as a lightweight, plug-and-play\nrestoration module for real-time visual perception in embodied AI agents such\nas drones and mobile robots.", "AI": {"tldr": "本文提出了一种在去噪步骤中采用多步优化的策略，显著提升了图像质量、感知准确性和泛化能力，验证了MPGD在自然和航空场景中的有效性。", "motivation": "现有方法（如MPGD）在去噪步骤中仅应用单次梯度更新，限制了恢复的保真度和鲁棒性，尤其是在嵌入式或分布外场景中。", "method": "在去噪的每个时间步中引入多步优化策略，增加梯度更新次数。", "result": "实验表明，该方法在超分辨率和高斯去模糊任务中提升了LPIPS和PSNR指标，且在Jetson Orin Nano上验证了其泛化能力。", "conclusion": "MPGD可作为轻量级、即插即用的恢复模块，适用于无人机和移动机器人等实时视觉感知任务。"}}
{"id": "2506.07013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07013", "abs": "https://arxiv.org/abs/2506.07013", "authors": ["Wentao Zhao", "Yihe Niu", "Yanbo Wang", "Tianchen Deng", "Shenghai Yuan", "Zhenli Wang", "Rui Guo", "Jingchuan Wang"], "title": "UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment", "comment": "15pages, 8 figures", "summary": "This work presents UNO, a unified monocular visual odometry framework that\nenables robust and adaptable pose estimation across diverse environments,\nplatforms, and motion patterns. Unlike traditional methods that rely on\ndeployment-specific tuning or predefined motion priors, our approach\ngeneralizes effectively across a wide range of real-world scenarios, including\nautonomous vehicles, aerial drones, mobile robots, and handheld devices. To\nthis end, we introduce a Mixture-of-Experts strategy for local state\nestimation, with several specialized decoders that each handle a distinct class\nof ego-motion patterns. Moreover, we introduce a fully differentiable\nGumbel-Softmax module that constructs a robust inter-frame correlation graph,\nselects the optimal expert decoder, and prunes erroneous estimates. These cues\nare then fed into a unified back-end that combines pre-trained,\nscale-independent depth priors with a lightweight bundling adjustment to\nenforce geometric consistency. We extensively evaluate our method on three\nmajor benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV\n(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating\nstate-of-the-art performance.", "AI": {"tldr": "UNO是一个统一的单目视觉里程计框架，能够在不同环境、平台和运动模式下实现鲁棒且自适应的位姿估计。", "motivation": "传统方法依赖于特定部署的调优或预定义的运动先验，而UNO旨在通用化地适应多种真实场景，如自动驾驶、无人机、移动机器人和手持设备。", "method": "采用专家混合策略进行局部状态估计，结合多个专用解码器处理不同运动模式，并使用可微分的Gumbel-Softmax模块构建帧间关联图、选择最优解码器并剔除错误估计。后端结合预训练的尺度无关深度先验和轻量级捆绑调整以确保几何一致性。", "result": "在KITTI、EuRoC-MAV和TUM-RGBD三个主要基准数据集上实现了最先进的性能。", "conclusion": "UNO框架在多样化的真实场景中表现出色，具有广泛的适用性和鲁棒性。"}}
{"id": "2506.06782", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06782", "abs": "https://arxiv.org/abs/2506.06782", "authors": ["Qinting Jiang", "Chuyang Ye", "Dongyan Wei", "Bingli Wang", "Yuan Xue", "Jingyan Jiang", "Zhi Wang"], "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World", "comment": null, "summary": "Despite progress, deep neural networks still suffer performance declines\nunder distribution shifts between training and test domains, leading to a\nsubstantial decrease in Quality of Experience (QoE) for applications. Existing\ntest-time adaptation (TTA) methods are challenged by dynamic, multiple test\ndistributions within batches. We observe that feature distributions across\ndifferent domains inherently cluster into distinct groups with varying means\nand variances. This divergence reveals a critical limitation of previous global\nnormalization strategies in TTA, which inevitably distort the original data\ncharacteristics. Based on this insight, we propose Feature-based Instance\nNeighbor Discovery (FIND), which comprises three key components: Layer-wise\nFeature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and\nSelective FABN (S-FABN). LFD stably captures features with similar\ndistributions at each layer by constructing graph structures. While FABN\noptimally combines source statistics with test-time distribution specific\nstatistics for robust feature representation. Finally, S-FABN determines which\nlayers require feature partitioning and which can remain unified, thereby\nenhancing inference efficiency. Extensive experiments demonstrate that FIND\nsignificantly outperforms existing methods, achieving a 30\\% accuracy\nimprovement in dynamic scenarios while maintaining computational efficiency.", "AI": {"tldr": "论文提出了一种名为FIND的新方法，通过特征实例邻居发现解决深度神经网络在测试时分布变化下的性能下降问题，显著提升了动态场景中的准确性。", "motivation": "深度神经网络在训练和测试域之间的分布变化下性能下降，影响应用体验。现有测试时适应方法难以应对动态、多测试分布的情况。", "method": "FIND包含三个关键组件：层间特征解耦（LFD）、特征感知批量归一化（FABN）和选择性FABN（S-FABN）。LFD通过构建图结构稳定捕捉相似分布特征，FABN结合源统计和测试分布统计优化特征表示，S-FABN决定哪些层需要分区。", "result": "FIND在动态场景中显著优于现有方法，准确性提升30%，同时保持计算效率。", "conclusion": "FIND通过特征解耦和动态归一化策略，有效解决了测试时分布变化问题，提升了模型的鲁棒性和效率。"}}
{"id": "2506.07338", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07338", "abs": "https://arxiv.org/abs/2506.07338", "authors": ["Yijie Deng", "Shuaihang Yuan", "Geeta Chandra Raju Bethala", "Anthony Tzes", "Yu-Shen Liu", "Yi Fang"], "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation", "comment": null, "summary": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify\nand navigate to a target object or location depicted in a reference image\ncaptured from any viewpoint. While recent methods leverage powerful novel view\nsynthesis (NVS) techniques, such as three-dimensional Gaussian splatting\n(3DGS), they typically rely on randomly sampling multiple viewpoints or\ntrajectories to ensure comprehensive coverage of discriminative visual cues.\nThis approach, however, creates significant redundancy through overlapping\nimage samples and lacks principled view selection, substantially increasing\nboth rendering and comparison overhead. In this paper, we introduce a novel IIN\nframework with a hierarchical scoring paradigm that estimates optimal\nviewpoints for target matching. Our approach integrates cross-level semantic\nscoring, utilizing CLIP-derived relevancy fields to identify regions with high\nsemantic similarity to the target object class, with fine-grained local\ngeometric scoring that performs precise pose estimation within promising\nregions. Extensive evaluations demonstrate that our method achieves\nstate-of-the-art performance on simulated IIN benchmarks and real-world\napplicability.", "AI": {"tldr": "提出了一种基于分层评分范式的实例图像目标导航框架，通过语义和几何评分优化视点选择，减少冗余，提升性能。", "motivation": "现有方法依赖随机采样视点或轨迹，导致冗余和效率低下，缺乏优化的视点选择机制。", "method": "结合跨层级语义评分（CLIP相似性）和局部几何评分（精确位姿估计），选择最优视点进行目标匹配。", "result": "在模拟和实际场景中均达到最先进性能。", "conclusion": "分层评分范式显著提升了导航效率和准确性。"}}
{"id": "2506.07015", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07015", "abs": "https://arxiv.org/abs/2506.07015", "authors": ["Qiyu Hou", "Jun Wang"], "title": "TABLET: Table Structure Recognition using Encoder-only Transformers", "comment": "ICDAR 2025", "summary": "To address the challenges of table structure recognition, we propose a novel\nSplit-Merge-based top-down model optimized for large, densely populated tables.\nOur approach formulates row and column splitting as sequence labeling tasks,\nutilizing dual Transformer encoders to capture feature interactions. The\nmerging process is framed as a grid cell classification task, leveraging an\nadditional Transformer encoder to ensure accurate and coherent merging. By\neliminating unstable bounding box predictions, our method reduces resolution\nloss and computational complexity, achieving high accuracy while maintaining\nfast processing speed. Extensive experiments on FinTabNet and PubTabNet\ndemonstrate the superiority of our model over existing approaches, particularly\nin real-world applications. Our method offers a robust, scalable, and efficient\nsolution for large-scale table recognition, making it well-suited for\nindustrial deployment.", "AI": {"tldr": "提出了一种基于Split-Merge的表格结构识别方法，通过双Transformer编码器优化行列分割，并利用额外编码器实现网格合并，显著提升了准确性和效率。", "motivation": "解决大规模密集表格结构识别的挑战，避免传统方法中不稳定的边界框预测问题。", "method": "将行列分割建模为序列标注任务，使用双Transformer编码器；合并过程作为网格分类任务，引入额外Transformer编码器。", "result": "在FinTabNet和PubTabNet上表现优异，准确率高且处理速度快，适合工业部署。", "conclusion": "该方法为大规模表格识别提供了鲁棒、可扩展且高效的解决方案。"}}
{"id": "2506.06784", "categories": ["cs.LG", "68T07 (Primary), 05C60, 05C85 (Secondary)", "I.2.6; G.2.2"], "pdf": "https://arxiv.org/pdf/2506.06784", "abs": "https://arxiv.org/abs/2506.06784", "authors": ["Marek Černý"], "title": "Caterpillar GNN: Replacing Message Passing with Efficient Aggregation", "comment": "40 pages, 9 figures, 3 tables", "summary": "Message-passing graph neural networks (MPGNNs) dominate modern graph\nlearning, typically prioritizing maximal expressive power. In contrast, we\nintroduce an \\emph{efficient aggregation} mechanism, deliberately trading off\nsome expressivity for stronger and more structured aggregation capabilities.\nOur approach allows seamless scaling between classical message-passing and\nsimpler methods based on colored or plain walks. We rigorously characterize the\nexpressive power at each intermediate step using homomorphism counts from a\nhierarchy of generalized \\emph{caterpillar graphs}. Based on this foundation,\nwe propose the \\emph{Caterpillar GNN}, whose robust graph-level aggregation\nenables it to successfully tackle synthetic graph-level task specifically\ndesigned to be challenging for classical MPGNNs. Moreover, we demonstrate that,\non real-world datasets, the Caterpillar GNN achieves comparable predictive\nperformance while significantly reducing the number of nodes in the hidden\nlayers of the computational graph.", "AI": {"tldr": "论文提出了一种高效的聚合机制（Caterpillar GNN），通过牺牲部分表达能力来增强聚合能力，并在合成和真实数据集上验证了其性能。", "motivation": "现代图学习中，消息传递图神经网络（MPGNNs）通常追求最大表达能力，但本文旨在通过高效的聚合机制在表达能力和聚合能力之间取得平衡。", "method": "引入了一种基于层次化毛虫图同态计数的高效聚合机制，并提出了Caterpillar GNN，能够在不同复杂度之间灵活调整。", "result": "在合成任务中，Caterpillar GNN成功解决了传统MPGNN难以处理的问题；在真实数据集上，其预测性能与传统方法相当，同时显著减少了计算图中的隐藏层节点数。", "conclusion": "Caterpillar GNN通过高效的聚合机制，在保持性能的同时降低了计算复杂度，为图学习提供了一种新的解决方案。"}}
{"id": "2506.07744", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07744", "abs": "https://arxiv.org/abs/2506.07744", "authors": ["Seungho Baek", "Taegeon Park", "Jongchan Park", "Seungjun Oh", "Yusung Kim"], "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning", "comment": "ICML 2025", "summary": "Existing offline hierarchical reinforcement learning methods rely on\nhigh-level policy learning to generate subgoal sequences. However, their\nefficiency degrades as task horizons increase, and they lack effective\nstrategies for stitching useful state transitions across different\ntrajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that\nformulates subgoal selection as a graph search problem rather than learning an\nexplicit high-level policy. By embedding states into a Temporal Distance\nRepresentation (TDR) space, GAS clusters semantically similar states from\ndifferent trajectories into unified graph nodes, enabling efficient transition\nstitching. A shortest-path algorithm is then applied to select subgoal\nsequences within the graph, while a low-level policy learns to reach the\nsubgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)\nmetric, which filters out noisy or inefficient transition states, significantly\nenhancing task performance. GAS outperforms prior offline HRL methods across\nlocomotion, navigation, and manipulation tasks. Notably, in the most\nstitching-critical task, it achieves a score of 88.3, dramatically surpassing\nthe previous state-of-the-art score of 1.0. Our source code is available at:\nhttps://github.com/qortmdgh4141/GAS.", "AI": {"tldr": "GAS提出了一种基于图搜索的子目标选择框架，替代传统的高层策略学习，通过TDR空间嵌入和TE指标优化，显著提升了离线分层强化学习的效率和性能。", "motivation": "现有离线分层强化学习方法在任务时间跨度增加时效率下降，且缺乏跨轨迹状态转移的有效策略。", "method": "GAS将子目标选择建模为图搜索问题，利用TDR空间嵌入聚类相似状态，并通过最短路径算法选择子目标序列，同时引入TE指标优化图质量。", "result": "GAS在运动、导航和操作任务中优于现有方法，尤其在关键任务中得分从1.0提升至88.3。", "conclusion": "GAS通过图搜索和状态聚类，有效解决了离线分层强化学习中的子目标选择和状态转移问题，性能显著提升。"}}
{"id": "2506.07016", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07016", "abs": "https://arxiv.org/abs/2506.07016", "authors": ["Sanjoy Chowdhury", "Mohamed Elmoghany", "Yohan Abeysinghe", "Junjie Fei", "Sayan Nag", "Salman Khan", "Mohamed Elhoseiny", "Dinesh Manocha"], "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks", "comment": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage", "summary": "Large multimodal models (LMMs) have shown remarkable progress in audio-visual\nunderstanding, yet they struggle with real-world scenarios that require complex\nreasoning across extensive video collections. Existing benchmarks for video\nquestion answering remain limited in scope, typically involving one clip per\nquery, which falls short of representing the challenges of large-scale,\naudio-visual retrieval and reasoning encountered in practical applications. To\nbridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal\nis to identify salient segments across different videos in response to a query\nand link them together to generate the most informative answer. To this end, we\npresent AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA\npairs designed to assess the capabilities of LMMs in multi-video retrieval and\ntemporal grounding task. Additionally, we propose a model-agnostic, multi-agent\nframework MAGNET to address this challenge, achieving up to 89% and 65%\nrelative improvements over baseline methods on BLEU@4 and GPT evaluation scores\nin QA task on our proposed AVHaystacks. To enable robust evaluation of\nmulti-video retrieval and temporal grounding for optimal response generation,\nwe introduce two new metrics, STEM, which captures alignment errors between a\nground truth and a predicted step sequence and MTGS, to facilitate balanced and\ninterpretable evaluation of segment-level grounding performance. Project:\nhttps://schowdhury671.github.io/magnet_project/", "AI": {"tldr": "论文提出AV-HaystacksQA任务和AVHaystacks基准，用于评估大型多模态模型在多视频检索和时序定位中的表现，并提出MAGNET框架和两个新指标STEM与MTGS。", "motivation": "现有视频问答基准局限于单一视频片段，无法满足实际应用中大规模音频-视觉检索和复杂推理的需求。", "method": "提出AVHaystacks基准和MAGNET多智能体框架，并引入STEM和MTGS指标。", "result": "MAGNET在BLEU@4和GPT评分上分别提升89%和65%。", "conclusion": "AVHaystacks和MAGNET为多视频检索和时序定位提供了有效解决方案和新评估标准。"}}
{"id": "2506.06787", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.06787", "abs": "https://arxiv.org/abs/2506.06787", "authors": ["Qiyun Zhao"], "title": "FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks", "comment": null, "summary": "As integrated circuit scale grows and design complexity rises, effective\ncircuit representation helps support logic synthesis, formal verification, and\nother automated processes in electronic design automation. And-Inverter Graphs\n(AIGs), as a compact and canonical structure, are widely adopted for\nrepresenting Boolean logic in these workflows. However, the increasing\ncomplexity and integration density of modern circuits introduce structural\nheterogeneity and global logic information loss in AIGs, posing significant\nchallenges to accurate circuit modeling. To address these issues, we propose\nFuncGNN, which integrates hybrid feature aggregation to extract\nmulti-granularity topological patterns, thereby mitigating structural\nheterogeneity and enhancing logic circuit representations. FuncGNN further\nintroduces gate-aware normalization that adapts to circuit-specific gate\ndistributions, improving robustness to structural heterogeneity. Finally,\nFuncGNN employs multi-layer integration to merge intermediate features across\nlayers, effectively synthesizing local and global semantic information for\ncomprehensive logic representations. Experimental results on two logic-level\nanalysis tasks (i.e., signal probability prediction and truth-table distance\nprediction) demonstrate that FuncGNN outperforms existing state-of-the-art\nmethods, achieving improvements of 2.06% and 18.71%, respectively, while\nreducing training time by approximately 50.6% and GPU memory usage by about\n32.8%.", "AI": {"tldr": "FuncGNN提出了一种新的电路表示方法，通过混合特征聚合、门感知归一化和多层集成，解决了AIGs中的结构异质性和全局信息丢失问题，显著提升了逻辑电路分析的性能。", "motivation": "现代电路复杂性和集成密度的增加导致AIGs中的结构异质性和全局逻辑信息丢失，需要更准确的电路建模方法。", "method": "FuncGNN结合了混合特征提取多粒度拓扑模式、门感知归一化和多层集成，以优化电路表示。", "result": "在信号概率预测和真值表距离预测任务中，FuncGNN性能分别提升2.06%和18.71%，训练时间和GPU内存使用分别减少50.6%和32.8%。", "conclusion": "FuncGNN通过多粒度特征提取和自适应归一化，有效提升了逻辑电路表示的准确性和效率。"}}
{"id": "2506.07045", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07045", "abs": "https://arxiv.org/abs/2506.07045", "authors": ["Yikun Ji", "Hong Yan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Qi Fan", "Liqing Zhang", "Jianfu Zhang"], "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "comment": null, "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.", "AI": {"tldr": "该论文提出了一种基于多模态大语言模型（MLLM）的方法，用于检测AI生成图像并提供可解释的视觉定位和文本解释。通过构建标注数据集和多阶段优化策略，显著提升了检测性能和解释能力。", "motivation": "现有AI生成图像检测方法多为黑箱，缺乏可解释性；MLLM虽具备分析能力，但在视觉与文本对齐上存在不足。", "method": "构建标注数据集，通过多阶段优化策略微调MLLM，平衡检测、定位和解释目标。", "result": "模型在检测和定位AI生成图像方面表现优异，显著优于基线方法。", "conclusion": "该方法为AI生成图像的检测和解释提供了有效解决方案，未来可进一步优化对齐能力。"}}
{"id": "2506.06793", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06793", "abs": "https://arxiv.org/abs/2506.06793", "authors": ["Zixuan Dong", "Yumi Omori", "Keith Ross"], "title": "Is Optimal Transport Necessary for Inverse Reinforcement Learning?", "comment": "19 pages, 10 tables", "summary": "Inverse Reinforcement Learning (IRL) aims to recover a reward function from\nexpert demonstrations. Recently, Optimal Transport (OT) methods have been\nsuccessfully deployed to align trajectories and infer rewards. While OT-based\nmethods have shown strong empirical results, they introduce algorithmic\ncomplexity, hyperparameter sensitivity, and require solving the OT optimization\nproblems. In this work, we challenge the necessity of OT in IRL by proposing\ntwo simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns\nrewards based on the nearest expert state regardless of temporal order; and (2)\nSegment-Matching Reward, which incorporates lightweight temporal alignment by\nmatching agent states to corresponding segments in the expert trajectory. These\nmethods avoid optimization, exhibit linear-time complexity, and are easy to\nimplement. Through extensive evaluations across 32 online and offline\nbenchmarks with three reinforcement learning algorithms, we show that our\nsimple rewards match or outperform recent OT-based approaches. Our findings\nsuggest that the core benefits of OT may arise from basic proximity alignment\nrather than its optimal coupling formulation, advocating for reevaluation of\ncomplexity in future IRL design.", "AI": {"tldr": "本文提出两种简单的启发式替代方法，挑战了逆强化学习中最优传输的必要性，并在实验中表现出与复杂方法相当或更好的性能。", "motivation": "最优传输方法在逆强化学习中虽有效，但存在算法复杂性和超参数敏感性问题，作者质疑其必要性。", "method": "提出两种无需优化的启发式方法：最小距离奖励和分段匹配奖励，具有线性时间复杂度和易实现性。", "result": "在32个在线和离线基准测试中，简单方法与最优传输方法性能相当或更优。", "conclusion": "最优传输的核心优势可能源于基本对齐而非复杂耦合，建议未来设计简化复杂性。"}}
{"id": "2506.07826", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07826", "abs": "https://arxiv.org/abs/2506.07826", "authors": ["William Ljungbergh", "Bernardo Taveira", "Wenzhao Zheng", "Adam Tonderski", "Chensheng Peng", "Fredrik Kahl", "Christoffer Petersson", "Michael Felsberg", "Kurt Keutzer", "Masayoshi Tomizuka", "Wei Zhan"], "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation", "comment": null, "summary": "Validating autonomous driving (AD) systems requires diverse and\nsafety-critical testing, making photorealistic virtual environments essential.\nTraditional simulation platforms, while controllable, are resource-intensive to\nscale and often suffer from a domain gap with real-world data. In contrast,\nneural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a\nscalable solution for creating photorealistic digital twins of real-world\ndriving scenes. However, they struggle with dynamic object manipulation and\nreusability as their per-scene optimization-based methodology tends to result\nin incomplete object models with integrated illumination effects. This paper\nintroduces R3D2, a lightweight, one-step diffusion model designed to overcome\nthese limitations and enable realistic insertion of complete 3D assets into\nexisting scenes by generating plausible rendering effects-such as shadows and\nconsistent lighting-in real time. This is achieved by training R3D2 on a novel\ndataset: 3DGS object assets are generated from in-the-wild AD data using an\nimage-conditioned 3D generative model, and then synthetically placed into\nneural rendering-based virtual environments, allowing R3D2 to learn realistic\nintegration. Quantitative and qualitative evaluations demonstrate that R3D2\nsignificantly enhances the realism of inserted assets, enabling use-cases like\ntext-to-3D asset insertion and cross-scene/dataset object transfer, allowing\nfor true scalability in AD validation. To promote further research in scalable\nand realistic AD simulation, we will release our dataset and code, see\nhttps://research.zenseact.com/publications/R3D2/.", "AI": {"tldr": "R3D2是一种轻量级扩散模型，用于在虚拟驾驶场景中插入逼真的3D资产，解决了传统神经重建方法在动态对象操作和可重用性上的不足。", "motivation": "验证自动驾驶系统需要多样化和安全关键的测试，传统仿真平台资源密集且与真实数据存在域差距，而神经重建方法（如3DGS）在动态对象操作和可重用性上表现不佳。", "method": "R3D2通过训练一个基于3DGS对象资产的数据集，利用一步扩散模型实现逼真的3D资产插入，生成阴影和一致光照效果。", "result": "R3D2显著提升了插入资产的真实感，支持文本到3D资产插入和跨场景对象转移，增强了自动驾驶验证的可扩展性。", "conclusion": "R3D2为自动驾驶仿真提供了可扩展且逼真的解决方案，未来将公开数据集和代码以促进研究。"}}
{"id": "2506.07050", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.07050", "abs": "https://arxiv.org/abs/2506.07050", "authors": ["Zheng Wang", "Kai Ying", "Bin Xu", "Chunjiao Wang", "Cong Bai"], "title": "From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion", "comment": null, "summary": "Accurate near-real-time precipitation retrieval has been enhanced by\nsatellite-based technologies. However, infrared-based algorithms have low\naccuracy due to weak relations with surface precipitation, whereas passive\nmicrowave and radar-based methods are more accurate but limited in range. This\nchallenge motivates the Precipitation Retrieval Expansion (PRE) task, which\naims to enable accurate, infrared-based full-disc precipitation retrievals\nbeyond the scanning swath. We introduce Multimodal Knowledge Expansion, a\ntwo-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling\nstage, PRE-Net transfers knowledge from a multimodal data integration model to\nan infrared-based model within the scanning swath via Coordinated Masking and\nWavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune\nrefines predictions across the full disc by balancing multimodal and full-disc\ninfrared knowledge. Experiments on the introduced PRE benchmark demonstrate\nthat PRE-Net significantly advanced precipitation retrieval performance,\noutperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code\nwill be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.", "AI": {"tldr": "论文提出了一种名为PRE-Net的两阶段模型，通过多模态知识扩展技术提升红外降水反演的准确性，解决了现有方法的范围限制问题。", "motivation": "现有红外降水反演算法精度低，而被动微波和雷达方法范围有限，因此需要一种能在全盘范围内实现高精度降水反演的技术。", "method": "采用两阶段流程：1) Swath-Distilling阶段，通过CoMWE技术将多模态知识迁移到红外模型；2) Full-Disc Adaptation阶段，利用Self-MaskTune平衡多模态和红外知识。", "result": "PRE-Net在PRE基准测试中显著优于PERSIANN-CCS、PDIR和IMERG等领先产品。", "conclusion": "PRE-Net通过多模态知识扩展实现了高精度的全盘降水反演，为卫星降水反演提供了新思路。"}}
{"id": "2506.06809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06809", "abs": "https://arxiv.org/abs/2506.06809", "authors": ["Di Lin", "Wanjing Ren", "Xuanbin Li", "Rui Zhang"], "title": "IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder", "comment": null, "summary": "Self-supervised learning (SSL) methods have been increasingly applied to\ndiverse downstream tasks due to their superior generalization capabilities and\nlow annotation costs. However, most existing heterogeneous graph SSL models\nconvert heterogeneous graphs into homogeneous ones via meta-paths for training,\nwhich only leverage information from nodes at both ends of meta-paths while\nunderutilizing the heterogeneous node information along the meta-paths. To\naddress this limitation, this paper proposes a novel framework named IMPA-HGAE\nto enhance target node embeddings by fully exploiting internal node information\nalong meta-paths. Experimental results validate that IMPA-HGAE achieves\nsuperior performance on heterogeneous datasets. Furthermore, this paper\nintroduce innovative masking strategies to strengthen the representational\ncapacity of generative SSL models on heterogeneous graph data. Additionally,\nthis paper discuss the interpretability of the proposed method and potential\nfuture directions for generative self-supervised learning in heterogeneous\ngraphs. This work provides insights into leveraging meta-path-guided structural\nsemantics for robust representation learning in complex graph scenarios.", "AI": {"tldr": "本文提出了一种名为IMPA-HGAE的新框架，通过充分利用元路径上的内部节点信息来增强目标节点嵌入，解决了现有异构图SSL模型信息利用不足的问题。", "motivation": "现有异构图SSL模型通过元路径将异构图转换为同构图进行训练，仅利用元路径两端节点的信息，而忽略了元路径上的异构节点信息。", "method": "提出了IMPA-HGAE框架，利用元路径上的内部节点信息增强目标节点嵌入，并引入了创新的掩码策略以提升生成式SSL模型在异构图数据上的表征能力。", "result": "实验证明IMPA-HGAE在异构数据集上表现优异。", "conclusion": "该工作为复杂图场景中利用元路径引导的结构语义进行鲁棒表示学习提供了见解，并探讨了生成式自监督学习在异构图中的未来方向。"}}
{"id": "2506.07857", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07857", "abs": "https://arxiv.org/abs/2506.07857", "authors": ["Zihui Zhang", "Weisheng Dai", "Hongtao Wen", "Bo Yang"], "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds", "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/LogoSP", "summary": "We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.", "AI": {"tldr": "LogoSP提出了一种无监督3D语义分割方法，通过结合局部和全局点特征学习语义信息，并在频域中根据全局模式分组超点，生成高精度伪标签用于训练。", "motivation": "现有方法仅依赖局部特征，缺乏发现更丰富语义先验的能力。", "method": "LogoSP通过频域中的全局模式分组超点，生成语义伪标签用于训练分割网络。", "result": "在两个室内和一个室外数据集上，LogoSP显著优于现有无监督方法，达到最先进性能。", "conclusion": "LogoSP证明了在无人类标注的情况下，全局模式能有效表示有意义的3D语义。"}}
{"id": "2506.07055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07055", "abs": "https://arxiv.org/abs/2506.07055", "authors": ["Tarique Dahri", "Zulfiqar Ali Memon", "Zhenyu Yu", "Mohd. Yamani Idna Idris", "Sheheryar Khan", "Sadiq Ahmad", "Maged Shoman", "Saddam Aziz", "Rizwan Qureshi"], "title": "A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge", "comment": null, "summary": "We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework\nfor training compact deep learning models. Unlike traditional methods that rely\non pre-trained teacher networks, our approach appends auxiliary classifiers to\nintermediate feature maps, generating diverse self-supervised knowledge and\nenabling one-to-one transfer across different network stages. Our method\nachieves an average improvement of 4.54\\% over the state-of-the-art PS-KD\nmethod and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on\nImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under\nfew-shot learning scenarios also achieve state-of-the-art results. These\nfindings demonstrate the effectiveness of our approach in enhancing model\ngeneralization and performance without the need for large over-parameterized\nteacher networks. Importantly, at the inference stage, all auxiliary\nclassifiers can be removed, yielding no extra computational cost. This makes\nour model suitable for deploying small language models on affordable\nlow-computing devices. Owing to its lightweight design and adaptability, our\nframework is particularly suitable for multimodal sensing and cyber-physical\nenvironments that require efficient and responsive inference. LSSKD facilitates\nthe development of intelligent agents capable of learning from limited sensory\ndata under weak supervision.", "AI": {"tldr": "LSSKD框架通过中间特征图的辅助分类器实现自监督知识蒸馏，无需依赖预训练教师网络，显著提升模型性能且无额外计算开销。", "motivation": "传统方法依赖预训练教师网络，而LSSKD旨在通过自监督知识蒸馏提升紧凑模型的泛化能力，适用于低计算设备。", "method": "在中间特征图上添加辅助分类器，生成多样化自监督知识，实现一对一跨网络阶段知识转移。", "result": "在CIFAR-100上平均提升4.54%，ImageNet上提升0.32%，并在小样本学习场景中达到SOTA。", "conclusion": "LSSKD高效轻量，适用于多模态感知和低计算环境，支持弱监督下有限数据学习。"}}
{"id": "2506.06815", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06815", "abs": "https://arxiv.org/abs/2506.06815", "authors": ["Max McGuinness", "Eirik Fladmark", "Francisco Vargas"], "title": "Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion", "comment": "6 pages. Presented at the OPT Workshop, NeurIPS 2024, Vancouver, CA", "summary": "We present an early investigation into the use of neural diffusion processes\nfor global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One\ncan use the Boltzmann distribution to formulate optimization as solving a\nSchr\\\"odinger bridge sampling problem, then apply Girsanov's theorem with a\nsimple (single-point) prior to frame it in stochastic control terms, and\ncompute the solution's integral terms via a neural approximation (a Fourier\nMLP). We provide theoretical bounds for this optimiser, results on toy\noptimisation tasks, and a summary of the stochastic theory motivating the\nmodel. Ultimately, we found the optimiser to display promising per-step\nperformance at optimisation tasks between 2 and 1,247 dimensions, but struggle\nto explore higher-dimensional spaces when faced with a 15.9k parameter model,\nindicating a need for work on adaptation in such environments.", "AI": {"tldr": "本文研究了神经扩散过程在全局优化中的应用，基于Zhang等人的Path Integral Sampler，通过Boltzmann分布将优化问题转化为Schrödinger桥采样问题，并利用Girsanov定理和神经近似（Fourier MLP）求解。实验表明该方法在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时存在困难。", "motivation": "探索神经扩散过程在全局优化中的潜力，特别是如何通过Boltzmann分布和Schrödinger桥采样问题来优化高维任务。", "method": "将优化问题转化为Schrödinger桥采样问题，应用Girsanov定理和单点先验，通过Fourier MLP进行神经近似求解。", "result": "在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时表现不佳。", "conclusion": "该方法在低维任务中表现优异，但在高维环境中需要进一步改进适应性。"}}
{"id": "2506.07865", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07865", "abs": "https://arxiv.org/abs/2506.07865", "authors": ["Jinxi Li", "Ziyang Song", "Siyuan Zhou", "Bo Yang"], "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity", "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/FreeGave", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.", "AI": {"tldr": "FreeGave是一种无需物体先验的方法，通过物理编码和无散度模块学习复杂动态3D场景的物理特性，优于现有方法。", "motivation": "现有方法在处理复杂物理运动或边界时表现不佳，且依赖物体先验信息（如掩码或类型）。FreeGave旨在解决这些问题。", "method": "引入物理编码和无散度模块，估计每高斯速度场，避免低效的PINN损失。", "result": "在三个公共数据集和新收集的真实数据集上表现优异，尤其在未来帧外推和运动分割任务中。", "conclusion": "FreeGave成功学习了无标签的3D物理运动模式，验证了其有效性。"}}
{"id": "2506.07056", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07056", "abs": "https://arxiv.org/abs/2506.07056", "authors": ["Zhenyu Liu", "Huizhi Liang", "Rajiv Ranjan", "Zhanxing Zhu", "Vaclav Snasel", "Varun Ojha"], "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness", "comment": null, "summary": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models.", "AI": {"tldr": "提出了一种双正则化损失（D2R Loss）和协作对抗生成（CAG）策略，用于增强深度神经网络模型的鲁棒性。", "motivation": "现有方法在目标模型的损失函数引导和对抗样本生成方面存在不足，需要改进。", "method": "D2R Loss通过对抗分布和干净分布优化增强模型鲁棒性；CAG通过梯度协作生成对抗样本。", "result": "在多个基准数据库和模型上实验表明，D2R Loss与CAG结合能显著提升模型鲁棒性。", "conclusion": "D2R Loss和CAG策略有效解决了现有方法的局限性，显著提升了模型的对抗鲁棒性。"}}
{"id": "2506.06853", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06853", "abs": "https://arxiv.org/abs/2506.06853", "authors": ["Ilya Kaufman Sirot", "Omri Azencot"], "title": "Curvature Enhanced Data Augmentation for Regression", "comment": "Accepted to ICML 2025", "summary": "Deep learning models with a large number of parameters, often referred to as\nover-parameterized models, have achieved exceptional performance across various\ntasks. Despite concerns about overfitting, these models frequently generalize\nwell to unseen data, thanks to effective regularization techniques, with data\naugmentation being among the most widely used. While data augmentation has\nshown great success in classification tasks using label-preserving\ntransformations, its application in regression problems has received less\nattention. Recently, a novel \\emph{manifold learning} approach for generating\nsynthetic data was proposed, utilizing a first-order approximation of the data\nmanifold. Building on this foundation, we present a theoretical framework and\npractical tools for approximating and sampling general data manifolds.\nFurthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)\nmethod for regression tasks. CEMS leverages a second-order representation of\nthe data manifold to enable efficient sampling and reconstruction of new data\npoints. Extensive evaluations across multiple datasets and comparisons with\nstate-of-the-art methods demonstrate that CEMS delivers superior performance in\nboth in-distribution and out-of-distribution scenarios, while introducing only\nminimal computational overhead. Code is available at\nhttps://github.com/azencot-group/CEMS.", "AI": {"tldr": "论文提出了一种基于二阶数据流形表示的CEMS方法，用于回归任务中的高效数据增强，显著提升了模型性能。", "motivation": "尽管数据增强在分类任务中表现优异，但在回归问题中应用较少。本文旨在填补这一空白，提出一种新的流形学习方法。", "method": "利用二阶数据流形表示，开发了Curvature-Enhanced Manifold Sampling (CEMS)方法，用于高效生成和重建数据点。", "result": "CEMS在多个数据集上表现出色，优于现有方法，且计算开销极小。", "conclusion": "CEMS为回归任务提供了一种高效的数据增强方法，具有广泛的应用潜力。"}}
{"id": "2506.07996", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.07996", "abs": "https://arxiv.org/abs/2506.07996", "authors": ["Ming-Feng Li", "Xin Yang", "Fu-En Wang", "Hritam Basak", "Yuyin Sun", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo"], "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References", "comment": "CVPR 2025", "summary": "6D object pose estimation has shown strong generalizability to novel objects.\nHowever, existing methods often require either a complete, well-reconstructed\n3D model or numerous reference images that fully cover the object. Estimating\n6D poses from partial references, which capture only fragments of an object's\nappearance and geometry, remains challenging. To address this, we propose\nUA-Pose, an uncertainty-aware approach for 6D object pose estimation and online\nobject completion specifically designed for partial references. We assume\naccess to either (1) a limited set of RGBD images with known poses or (2) a\nsingle 2D image. For the first case, we initialize a partial object 3D model\nbased on the provided images and poses, while for the second, we use\nimage-to-3D techniques to generate an initial object 3D model. Our method\nintegrates uncertainty into the incomplete 3D model, distinguishing between\nseen and unseen regions. This uncertainty enables confidence assessment in pose\nestimation and guides an uncertainty-aware sampling strategy for online object\ncompletion, enhancing robustness in pose estimation accuracy and improving\nobject completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and\nHO3D datasets, including RGBD sequences of YCB objects manipulated by robots\nand human hands. Experimental results demonstrate significant performance\nimprovements over existing methods, particularly when object observations are\nincomplete or partially captured. Project page:\nhttps://minfenli.github.io/UA-Pose/", "AI": {"tldr": "UA-Pose提出了一种不确定性感知的6D物体姿态估计方法，适用于部分参考数据，显著提升了在不完整观测下的性能。", "motivation": "现有方法需要完整3D模型或大量参考图像，而部分参考数据下的姿态估计仍具挑战性。", "method": "基于有限RGBD图像或单张2D图像初始化部分3D模型，并引入不确定性区分已知与未知区域，指导在线补全。", "result": "在YCB-Video等数据集上表现优于现有方法，特别是在不完整观测下。", "conclusion": "UA-Pose通过不确定性建模和在线补全，显著提升了部分参考数据下的6D姿态估计性能。"}}
{"id": "2506.07080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07080", "abs": "https://arxiv.org/abs/2506.07080", "authors": ["Anatol Garioud", "Sébastien Giordano", "Nicolas David", "Nicolas Gonthier"], "title": "FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping", "comment": null, "summary": "The growing availability of high-quality Earth Observation (EO) data enables\naccurate global land cover and crop type monitoring. However, the volume and\nheterogeneity of these datasets pose major processing and annotation\nchallenges. To address this, the French National Institute of Geographical and\nForest Information (IGN) is actively exploring innovative strategies to exploit\ndiverse EO data, which require large annotated datasets. IGN introduces\nFLAIR-HUB, the largest multi-sensor land cover dataset with\nvery-high-resolution (20 cm) annotations, covering 2528 km2 of France. It\ncombines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT\nimagery, topographic data, and historical aerial images. Extensive benchmarks\nevaluate multimodal fusion and deep learning models (CNNs, transformers) for\nland cover or crop mapping and also explore multi-task learning. Results\nunderscore the complexity of multimodal fusion and fine-grained classification,\nwith best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using\nnearly all modalities. FLAIR-HUB supports supervised and multimodal\npretraining, with data and code available at\nhttps://ignf.github.io/FLAIR/flairhub.", "AI": {"tldr": "FLAIR-HUB是一个多传感器土地覆盖数据集，结合六种对齐模态数据，用于全球土地覆盖和作物类型监测。通过多模态融合和深度学习模型评估，结果显示其复杂性，最佳性能达到78.2%准确率。", "motivation": "解决高分辨率地球观测数据的处理和标注挑战，支持土地覆盖和作物类型监测。", "method": "结合六种模态数据（如航空影像、Sentinel-1/2时间序列等），使用深度学习模型（CNN、transformer）进行多模态融合和多任务学习。", "result": "最佳土地覆盖分类性能为78.2%准确率和65.8% mIoU，几乎使用了所有模态数据。", "conclusion": "FLAIR-HUB为监督和多模态预训练提供了支持，数据与代码已公开。"}}
{"id": "2506.06858", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06858", "abs": "https://arxiv.org/abs/2506.06858", "authors": ["Ziwei Li", "Yuhan Duan", "Tianyu Xiong", "Yi-Tang Chen", "Wei-Lun Chao", "Han-Wei Shen"], "title": "High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations", "comment": null, "summary": "Effective surrogate models are critical for accelerating scientific\nsimulations. Implicit neural representations (INRs) offer a compact and\ncontinuous framework for modeling spatially structured data, but they often\nstruggle with complex scientific fields exhibiting localized, high-frequency\nvariations. Recent approaches address this by introducing additional features\nalong rigid geometric structures (e.g., grids), but at the cost of flexibility\nand increased model size. In this paper, we propose a simple yet effective\nalternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to\nan augmented memory bank to learn flexible feature representations, enabling\nadaptive allocation of model capacity based on data characteristics, rather\nthan rigid structural assumptions. To further improve scalability, we introduce\na coordinate-guided mixture of experts (MoE) that enhances the specialization\nand efficiency of feature representations. Experiments on three large-scale\nensemble simulation datasets show that FA-INR achieves state-of-the-art\nfidelity while significantly reducing model size, establishing a new trade-off\nfrontier between accuracy and compactness for INR-based surrogates.", "AI": {"tldr": "FA-INR提出了一种基于交叉注意力和记忆库的隐式神经表示方法，通过自适应特征分配和混合专家机制，显著提升了复杂科学场建模的精度和模型紧凑性。", "motivation": "现有隐式神经表示（INRs）在处理具有局部高频变化的复杂科学场时表现不佳，且传统方法依赖刚性几何结构导致灵活性不足和模型体积增大。", "method": "FA-INR利用交叉注意力机制和记忆库学习自适应特征表示，并引入坐标引导的混合专家（MoE）提升特征表示的专业化和效率。", "result": "在三个大规模集成仿真数据集上的实验表明，FA-INR在保持高精度的同时显著减小了模型体积。", "conclusion": "FA-INR为基于INR的代理模型建立了精度与紧凑性之间的新权衡前沿。"}}
{"id": "2506.07087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07087", "abs": "https://arxiv.org/abs/2506.07087", "authors": ["Weiqi Yan", "Lvhai Chen", "Huaijia Kou", "Shengchuan Zhang", "Yan Zhang", "Liujuan Cao"], "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning", "comment": "Accepted by CVPR 2025 (Hightlight)", "summary": "Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it\ndoesn't need to rely on extensive pixel-level labels. Existing UCOD methods\ntypically generate pseudo-labels using fixed strategies and train 1 x1\nconvolutional layers as a simple decoder, leading to low performance compared\nto fully-supervised methods. We emphasize two drawbacks in these approaches:\n1). The model is prone to fitting incorrect knowledge due to the pseudo-label\ncontaining substantial noise. 2). The simple decoder fails to capture and learn\nthe semantic features of camouflaged objects, especially for small-sized\nobjects, due to the low-resolution pseudo-labels and severe confusion between\nforeground and background pixels. To this end, we propose a UCOD method with a\nteacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,\nwhich contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial\n(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines\npseudo-labels generated by fixed strategies and the teacher model to prevent\nthe model from overfitting incorrect knowledge while preserving the ability for\nself-correction; the DBA decoder takes adversarial learning of different\nsegmentation objectives, guides the model to overcome the foreground-background\nconfusion of camouflaged objects, and the Look-Twice mechanism mimics the human\ntendency to zoom in on camouflaged objects and performs secondary refinement on\nsmall-sized objects. Extensive experiments show that our method demonstrates\noutstanding performance, even surpassing some existing fully supervised\nmethods. The code is available now.", "AI": {"tldr": "提出了一种基于动态伪标签学习的无监督伪装目标检测方法（UCOD-DPL），通过自适应伪标签模块、双分支对抗解码器和二次观察机制，显著提升了性能。", "motivation": "现有无监督伪装目标检测方法因伪标签噪声大和解码器简单导致性能较低，本文旨在解决这两个问题。", "method": "提出UCOD-DPL方法，包含自适应伪标签模块（APM）、双分支对抗解码器（DBA）和二次观察机制，动态优化伪标签并提升特征学习能力。", "result": "实验表明，该方法性能优异，甚至超过部分全监督方法。", "conclusion": "UCOD-DPL通过动态伪标签学习和多模块协作，显著提升了无监督伪装目标检测的性能。"}}
{"id": "2506.06861", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.06861", "abs": "https://arxiv.org/abs/2506.06861", "authors": ["Xizhi Tian", "Meng Ding", "Touming Tao", "Zihang Xiang", "Di Wang"], "title": "Differentially Private Sparse Linear Regression with Heavy-tailed Responses", "comment": "Accepted at ECML 2025", "summary": "As a fundamental problem in machine learning and differential privacy (DP),\nDP linear regression has been extensively studied. However, most existing\nmethods focus primarily on either regular data distributions or low-dimensional\ncases with irregular data. To address these limitations, this paper provides a\ncomprehensive study of DP sparse linear regression with heavy-tailed responses\nin high-dimensional settings. In the first part, we introduce the DP-IHT-H\nmethod, which leverages the Huber loss and private iterative hard thresholding\nto achieve an estimation error bound of \\(\n  \\tilde{O}\\biggl(\n  s^{* \\frac{1 }{2}}\n  \\cdot \\biggl(\\frac{\\log d}{n}\\biggr)^{\\frac{\\zeta}{1 + \\zeta}}\n  +\n  s^{* \\frac{1 + 2\\zeta}{2 + 2\\zeta}}\n  \\cdot \\biggl(\\frac{\\log^2 d}{n \\varepsilon}\\biggr)^{\\frac{\\zeta}{1 + \\zeta}}\n  \\biggr) \\) under the $(\\varepsilon, \\delta)$-DP model, where $n$ is the\nsample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,\nand $\\zeta \\in (0, 1]$ characterizes the tail heaviness of the data. In the\nsecond part, we propose DP-IHT-L, which further improves the error bound under\nadditional assumptions on the response and achieves \\(\n  \\tilde{O}\\Bigl(\\frac{(s^*)^{3/2} \\log d}{n \\varepsilon}\\Bigr). \\) Compared to\nthe first result, this bound is independent of the tail parameter $\\zeta$.\nFinally, through experiments on synthetic and real-world datasets, we\ndemonstrate that our methods outperform standard DP algorithms designed for\n``regular'' data.", "AI": {"tldr": "本文研究了高维设置下具有重尾响应的差分隐私稀疏线性回归问题，提出了两种方法（DP-IHT-H和DP-IHT-L），分别在不同假设下优化了误差界，并通过实验验证了其优越性。", "motivation": "现有方法主要针对常规数据分布或低维不规则数据，无法满足高维重尾数据的需求，因此需要研究更通用的差分隐私稀疏线性回归方法。", "method": "提出了两种方法：DP-IHT-H（基于Huber损失和私有迭代硬阈值）和DP-IHT-L（在额外响应假设下进一步优化误差界）。", "result": "DP-IHT-H在(ε, δ)-DP模型下实现了误差界，DP-IHT-L进一步优化了误差界且不依赖尾部参数ζ。实验表明两种方法优于常规DP算法。", "conclusion": "本文提出的方法在高维重尾数据下表现优异，为差分隐私稀疏线性回归提供了更通用的解决方案。"}}
{"id": "2506.07091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07091", "abs": "https://arxiv.org/abs/2506.07091", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui Jia"], "title": "SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model", "comment": null, "summary": "Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation\nof complex, interactive indoor scenes tailored to user prompt remains a\nformidable challenge. While existing methods achieve indoor scene synthesis,\nthey struggle with rigid editing constraints, physical incoherence, excessive\nhuman effort, single-room limitations, and suboptimal material quality. To\naddress these limitations, we propose SceneLCM, an end-to-end framework that\nsynergizes Large Language Model (LLM) for layout design with Latent Consistency\nModel(LCM) for scene optimization. Our approach decomposes scene generation\ninto four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D\nspatial reasoning to convert textual descriptions into parametric blueprints(3D\nlayout). And an iterative programmatic validation mechanism iteratively refines\nlayout parameters through LLM-mediated dialogue loops; (2) Furniture\nGeneration. SceneLCM employs Consistency Trajectory Sampling(CTS), a\nconsistency distillation sampling loss guided by LCM, to form fast,\nsemantically rich, and high-quality representations. We also offer two\ntheoretical justification to demonstrate that our CTS loss is equivalent to\nconsistency loss and its distillation error is bounded by the truncation error\nof the Euler solver; (3) Environment Optimization. We use a multiresolution\ntexture field to encode the appearance of the scene, and optimize via CTS loss.\nTo maintain cross-geometric texture coherence, we introduce a normal-aware\ncross-attention decoder to predict RGB by cross-attending to the anchors\nlocations in geometrically heterogeneous instance. (4)Physically Editing.\nSceneLCM supports physically editing by integrating physical simulation,\nachieved persistent physical realism. Extensive experiments validate SceneLCM's\nsuperiority over state-of-the-art techniques, showing its wide-ranging\npotential for diverse applications.", "AI": {"tldr": "SceneLCM是一个端到端框架，结合LLM进行布局设计和LCM进行场景优化，解决了现有室内场景生成方法的局限性。", "motivation": "现有方法在室内场景合成中存在编辑限制、物理不一致、人力需求高、单房间限制和材质质量差等问题。", "method": "SceneLCM通过四个模块化流程实现场景生成：布局生成（LLM指导）、家具生成（CTS损失）、环境优化（多分辨率纹理场）和物理编辑（物理模拟）。", "result": "实验验证SceneLCM优于现有技术，具有广泛的应用潜力。", "conclusion": "SceneLCM通过结合LLM和LCM，实现了高效、高质量的室内场景生成与编辑。"}}
{"id": "2506.06866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06866", "abs": "https://arxiv.org/abs/2506.06866", "authors": ["Dongyeop Lee", "Kwanhee Lee", "Jinseok Chung", "Namhoon Lee"], "title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning", "comment": "ICML 2025", "summary": "Sparsifying neural networks often suffers from seemingly inevitable\nperformance degradation, and it remains challenging to restore the original\nperformance despite much recent progress. Motivated by recent studies in robust\noptimization, we aim to tackle this problem by finding subnetworks that are\nboth sparse and flat at the same time. Specifically, we formulate pruning as a\nsparsity-constrained optimization problem where flatness is encouraged as an\nobjective. We solve it explicitly via an augmented Lagrange dual approach and\nextend it further by proposing a generalized projection operation, resulting in\nnovel pruning methods called SAFE and its extension, SAFE$^+$. Extensive\nevaluations on standard image classification and language modeling tasks reveal\nthat SAFE consistently yields sparse networks with improved generalization\nperformance, which compares competitively to well-established baselines. In\naddition, SAFE demonstrates resilience to noisy data, making it well-suited for\nreal-world conditions.", "AI": {"tldr": "论文提出了一种名为SAFE的剪枝方法，通过同时优化稀疏性和平坦性，显著提升了稀疏网络的泛化性能，并在噪声数据下表现出鲁棒性。", "motivation": "稀疏化神经网络通常会导致性能下降，难以恢复原始性能。受鲁棒优化研究的启发，作者希望通过同时寻找稀疏且平坦的子网络来解决这一问题。", "method": "将剪枝问题建模为稀疏性约束的优化问题，以平坦性为目标，采用增强拉格朗日对偶方法求解，并进一步提出广义投影操作，得到SAFE及其扩展SAFE$^+$。", "result": "在图像分类和语言建模任务中，SAFE能持续生成泛化性能更好的稀疏网络，且对噪声数据具有鲁棒性。", "conclusion": "SAFE方法在稀疏性和性能之间取得了平衡，适用于实际场景。"}}
{"id": "2506.07112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07112", "abs": "https://arxiv.org/abs/2506.07112", "authors": ["Changhong Fu", "Hua Lin", "Haobo Zuo", "Liangliang Yao", "Liguo Zhang"], "title": "EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring", "comment": null, "summary": "Text spotting for industrial panels is a key task for intelligent monitoring.\nHowever, achieving efficient and accurate text spotting for complex industrial\npanels remains challenging due to issues such as cross-scale localization and\nambiguous boundaries in dense text regions. Moreover, most existing methods\nprimarily focus on representing a single text shape, neglecting a comprehensive\nexploration of multi-scale feature information across different texts. To\naddress these issues, this work proposes a novel multi-scale dense text spotter\nfor edge AI-based vision system (EdgeSpotter) to achieve accurate and robust\nindustrial panel monitoring. Specifically, a novel Transformer with efficient\nmixer is developed to learn the interdependencies among multi-level features,\nintegrating multi-layer spatial and semantic cues. In addition, a new feature\nsampling with catmull-rom splines is designed, which explicitly encodes the\nshape, position, and semantic information of text, thereby alleviating missed\ndetections and reducing recognition errors caused by multi-scale or dense text\nregions. Furthermore, a new benchmark dataset for industrial panel monitoring\n(IPM) is constructed. Extensive qualitative and quantitative evaluations on\nthis challenging benchmark dataset validate the superior performance of the\nproposed method in different challenging panel monitoring tasks. Finally,\npractical tests based on the self-designed edge AI-based vision system\ndemonstrate the practicality of the method. The code and demo will be available\nat https://github.com/vision4robotics/EdgeSpotter.", "AI": {"tldr": "论文提出了一种名为EdgeSpotter的多尺度密集文本检测方法，用于工业面板监测，解决了跨尺度定位和密集文本区域模糊边界的问题。", "motivation": "工业面板的文本检测是智能监测的关键任务，但现有方法在复杂工业面板上效率低且准确性不足，尤其是多尺度特征信息的综合利用不足。", "method": "开发了一种新型Transformer结构，结合高效混合器学习多级特征的相互依赖关系，并设计了基于Catmull-Rom样条的特征采样方法，编码文本的形状、位置和语义信息。", "result": "在自建的工业面板监测基准数据集（IPM）上，方法表现出优越性能，并通过实际边缘AI视觉系统测试验证了实用性。", "conclusion": "EdgeSpotter方法在工业面板监测任务中实现了准确和鲁棒的文本检测，具有实际应用价值。"}}
{"id": "2506.06873", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06873", "abs": "https://arxiv.org/abs/2506.06873", "authors": ["Armin Behnamnia", "Gholamali Aminian", "Alireza Aghaei", "Chengchun Shi", "Vincent Y. F. Tan", "Hamid R. Rabiee"], "title": "Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning", "comment": "Accepted as spotlight poster in ICML 2025", "summary": "Off-policy learning and evaluation leverage logged bandit feedback datasets,\nwhich contain context, action, propensity score, and feedback for each data\npoint. These scenarios face significant challenges due to high variance and\npoor performance with low-quality propensity scores and heavy-tailed reward\ndistributions. We address these issues by introducing a novel estimator based\non the log-sum-exponential (LSE) operator, which outperforms traditional\ninverse propensity score estimators. Our LSE estimator demonstrates variance\nreduction and robustness under heavy-tailed conditions. For off-policy\nevaluation, we derive upper bounds on the estimator's bias and variance. In the\noff-policy learning scenario, we establish bounds on the regret -- the\nperformance gap between our LSE estimator and the optimal policy -- assuming\nbounded $(1+\\epsilon)$-th moment of weighted reward. Notably, we achieve a\nconvergence rate of $O(n^{-\\epsilon/(1+ \\epsilon)})$ for the regret bounds,\nwhere $\\epsilon \\in [0,1]$ and $n$ is the size of logged bandit feedback\ndataset. Theoretical analysis is complemented by comprehensive empirical\nevaluations in both off-policy learning and evaluation scenarios, confirming\nthe practical advantages of our approach. The code for our estimator is\navailable at the following link:\nhttps://github.com/armin-behnamnia/lse-offpolicy-learning.", "AI": {"tldr": "论文提出了一种基于log-sum-exponential（LSE）算子的新估计器，用于解决离线学习和评估中的高方差和低质量倾向得分问题，展示了其方差减少和对重尾奖励分布的鲁棒性。", "motivation": "离线学习和评估面临高方差和低质量倾向得分的挑战，尤其是在重尾奖励分布下性能较差。", "method": "引入基于LSE算子的新估计器，推导了其在离线评估中的偏差和方差上界，以及在离线学习中的遗憾上界。", "result": "LSE估计器在方差减少和鲁棒性方面优于传统方法，理论分析和实验验证了其优势。", "conclusion": "LSE估计器在离线学习和评估中表现出色，提供了理论保证和实际优势。"}}
{"id": "2506.07122", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2506.07122", "abs": "https://arxiv.org/abs/2506.07122", "authors": ["Prakriti Tripathi", "Theertha Biju", "Maniram Thota", "Rakesh Lingam"], "title": "Image segmentation and classification of E-waste for waste segregation", "comment": "4 pages, 7 figures. For code and link to dataset, see\n  https://github.com/prakriti16/Image-segmentation-and-classification-of-e-waste", "summary": "Industry partners provided a problem statement that involves classifying\nelectronic waste using machine learning models that will be used by\npick-and-place robots for waste segregation. We started by taking common\nelectronic waste items, such as a mouse and charger, unsoldering them, and\ntaking pictures to create a custom dataset. Then state-of-the art YOLOv11 model\nwas trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also\ntrained and achieved 41 mAP. The model will be further integrated with\npick-and-place robots to perform segregation of e-waste.", "AI": {"tldr": "论文提出了一种基于机器学习的电子废物分类方法，使用YOLOv11和Mask-RCNN模型，并计划与分拣机器人集成。", "motivation": "解决电子废物分类问题，以支持分拣机器人实现自动化废物分类。", "method": "创建自定义数据集，训练YOLOv11和Mask-RCNN模型。", "result": "YOLOv11达到70 mAP，Mask-RCNN达到41 mAP。", "conclusion": "模型将集成到分拣机器人中，用于电子废物分类。"}}
{"id": "2506.06884", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06884", "abs": "https://arxiv.org/abs/2506.06884", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "FREE: Fast and Robust Vision Language Models with Early Exits", "comment": "To appear at the Association of Computational Linguistics (ACL) 2025\n  Conference", "summary": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE.", "AI": {"tldr": "论文提出了一种名为FREE的对抗训练方法，用于在视觉语言模型（VLMs）中实现早期退出策略，以提高推理速度并减少性能损失。", "motivation": "视觉语言模型（VLMs）虽然性能优越，但模型体积大导致推理延迟问题，限制了实际应用。", "method": "采用基于GAN的对抗训练框架，每个退出点包含一个Transformer层和一个分类器，通过对抗训练使中间层特征接近最终层特征。", "result": "实验证明，该方法在保持性能的同时，推理速度提升超过1.51倍，并缓解了过度思考和中间危机现象。", "conclusion": "FREE方法有效提升了VLMs的推理效率和模型鲁棒性，适用于实际应用场景。"}}
{"id": "2506.07136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07136", "abs": "https://arxiv.org/abs/2506.07136", "authors": ["Huaize Liu", "Wenzhang Sun", "Qiyuan Zhang", "Donglin Di", "Biao Gong", "Hao Li", "Chen Wei", "Changqing Zou"], "title": "Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion", "comment": null, "summary": "Recent breakthroughs in video autoencoders (Video AEs) have advanced video\ngeneration, but existing methods fail to efficiently model spatio-temporal\nredundancies in dynamics, resulting in suboptimal compression factors. This\nshortfall leads to excessive training costs for downstream tasks. To address\nthis, we introduce Hi-VAE, an efficient video autoencoding framework that\nhierarchically encode coarse-to-fine motion representations of video dynamics\nand formulate the decoding process as a conditional generation task.\nSpecifically, Hi-VAE decomposes video dynamics into two latent spaces: Global\nMotion, capturing overarching motion patterns, and Detailed Motion, encoding\nhigh-frequency spatial details. Using separate self-supervised motion encoders,\nwe compress video latents into compact motion representations to reduce\nredundancy significantly. A conditional diffusion decoder then reconstructs\nvideos by combining hierarchical global and detailed motions, enabling\nhigh-fidelity video reconstructions. Extensive experiments demonstrate that\nHi-VAE achieves a high compression factor of 1428$\\times$, almost 30$\\times$\nhigher than baseline methods (e.g., Cosmos-VAE at 48$\\times$), validating the\nefficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction\nquality at such high compression rates and performs effectively in downstream\ngenerative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,\nproviding new perspectives for future exploration in video latent\nrepresentation and generation.", "AI": {"tldr": "Hi-VAE是一种高效的视频自编码框架，通过分层编码视频动态的粗到细运动表示，显著减少冗余，实现高压缩比和高保真重建。", "motivation": "现有视频自编码方法未能高效建模动态中的时空冗余，导致压缩效果不佳和训练成本高。", "method": "Hi-VAE将视频动态分解为全局运动和细节运动两个潜在空间，使用自监督运动编码器压缩表示，并通过条件扩散解码器重建视频。", "result": "实验表明，Hi-VAE的压缩比高达1428倍，远超基线方法（如Cosmos-VAE的48倍），同时保持高质量重建。", "conclusion": "Hi-VAE在高效压缩和高质量重建方面表现出色，具有可解释性和可扩展性，为视频潜在表示和生成提供了新视角。"}}
{"id": "2506.06891", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.06891", "abs": "https://arxiv.org/abs/2506.06891", "authors": ["Paulius Sasnauskas", "Yiğit Yalın", "Goran Radanović"], "title": "Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?", "comment": null, "summary": "We study the corruption-robustness of in-context reinforcement learning\n(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,\n2023). To address the challenge of reward poisoning attacks targeting the DPT,\nwe propose a novel adversarial training framework, called Adversarially Trained\nDecision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an\nattacker to minimize the true reward of the DPT by poisoning environment\nrewards, and a DPT model to infer optimal actions from the poisoned data. We\nevaluate the effectiveness of our approach against standard bandit algorithms,\nincluding robust baselines designed to handle reward contamination. Our results\nshow that the proposed method significantly outperforms these baselines in\nbandit settings, under a learned attacker. We additionally evaluate AT-DPT on\nan adaptive attacker, and observe similar results. Furthermore, we extend our\nevaluation to the MDP setting, confirming that the robustness observed in\nbandit scenarios generalizes to more complex environments.", "AI": {"tldr": "本文研究了上下文强化学习（ICRL）的鲁棒性，提出了一种对抗训练框架AT-DPT，以应对针对DPT的奖励投毒攻击。实验表明，该方法在多种环境下优于现有基线。", "motivation": "针对决策预训练变换器（DPT）在奖励投毒攻击下的脆弱性，研究如何提升其鲁棒性。", "method": "提出AT-DPT框架，同时训练攻击者和DPT模型，攻击者通过污染环境奖励来最小化DPT的真实奖励，而DPT则从污染数据中推断最优动作。", "result": "在强盗问题和MDP设置中，AT-DPT显著优于现有基线，且对自适应攻击者也表现良好。", "conclusion": "AT-DPT能有效提升DPT在奖励污染攻击下的鲁棒性，适用于复杂环境。"}}
{"id": "2506.07138", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.07138", "abs": "https://arxiv.org/abs/2506.07138", "authors": ["Hao Tang", "Chengchao Shen"], "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "comment": "The source code and trained weights are available at\n  https://github.com/visresearch/LLaVA-STF", "summary": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.", "AI": {"tldr": "论文提出了一种空间令牌融合（STF）和多块令牌融合（MBTF）方法，以减少视觉令牌序列长度并提升推理效率，同时保持多模态推理能力。", "motivation": "大型多模态模型（LMMs）因计算成本高和视觉令牌序列处理复杂度高而面临挑战，需要减少冗余令牌并提升效率。", "method": "通过STF融合空间相邻令牌以减少序列长度，并通过MBTF补充多粒度特征以平衡信息保留。", "result": "实验表明，该方法在仅使用25%视觉令牌的情况下，在8个基准测试中性能与基线相当或更优。", "conclusion": "STF和MBTF的结合有效提升了推理效率，同时保持了多模态推理能力。"}}
{"id": "2506.06895", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06895", "abs": "https://arxiv.org/abs/2506.06895", "authors": ["Jihao Andreas Lin", "Sebastian Ament", "Maximilian Balandat", "David Eriksson", "José Miguel Hernández-Lobato", "Eytan Bakshy"], "title": "Scalable Gaussian Processes with Latent Kronecker Structure", "comment": "International Conference on Machine Learning 2025", "summary": "Applying Gaussian processes (GPs) to very large datasets remains a challenge\ndue to limited computational scalability. Matrix structures, such as the\nKronecker product, can accelerate operations significantly, but their\napplication commonly entails approximations or unrealistic assumptions. In\nparticular, the most common path to creating a Kronecker-structured kernel\nmatrix is by evaluating a product kernel on gridded inputs that can be\nexpressed as a Cartesian product. However, this structure is lost if any\nobservation is missing, breaking the Cartesian product structure, which\nfrequently occurs in real-world data such as time series. To address this\nlimitation, we propose leveraging latent Kronecker structure, by expressing the\nkernel matrix of observed values as the projection of a latent Kronecker\nproduct. In combination with iterative linear system solvers and pathwise\nconditioning, our method facilitates inference of exact GPs while requiring\nsubstantially fewer computational resources than standard iterative methods. We\ndemonstrate that our method outperforms state-of-the-art sparse and variational\nGPs on real-world datasets with up to five million examples, including\nrobotics, automated machine learning, and climate applications.", "AI": {"tldr": "提出一种利用潜在Kronecker结构的方法，解决高斯过程在大规模数据集中的计算瓶颈，显著提升效率。", "motivation": "高斯过程在大规模数据集上的计算效率受限，现有方法（如Kronecker积）常需近似或假设，且对缺失数据敏感。", "method": "通过将观测值的核矩阵表示为潜在Kronecker积的投影，结合迭代线性系统求解和路径条件，实现精确高斯过程推断。", "result": "在多达500万样本的真实数据集（如机器人、自动机器学习和气候应用）上，性能优于现有稀疏和变分高斯过程方法。", "conclusion": "该方法显著减少计算资源需求，为大规模高斯过程推断提供了高效解决方案。"}}
{"id": "2506.07155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07155", "abs": "https://arxiv.org/abs/2506.07155", "authors": ["Van Nguyen Nguyen", "Christian Forster", "Sindi Shkodrani", "Vincent Lepetit", "Bugra Tekin", "Cem Keskin", "Tomas Hodan"], "title": "GoTrack: Generic 6DoF Object Pose Refinement and Tracking", "comment": null, "summary": "We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF\nobject pose refinement and tracking, which can handle diverse objects without\nany object-specific training. Unlike existing tracking methods that rely solely\non an analysis-by-synthesis approach for model-to-frame registration, GoTrack\nadditionally integrates frame-to-frame registration, which saves compute and\nstabilizes tracking. Both types of registration are realized by optical flow\nestimation. The model-to-frame registration is noticeably simpler than in\nexisting methods, relying only on standard neural network blocks (a transformer\nis trained on top of DINOv2) and producing reliable pose confidence scores\nwithout a scoring network. For the frame-to-frame registration, which is an\neasier problem as consecutive video frames are typically nearly identical, we\nemploy a light off-the-shelf optical flow model. We demonstrate that GoTrack\ncan be seamlessly combined with existing coarse pose estimation methods to\ncreate a minimal pipeline that reaches state-of-the-art RGB-only results on\nstandard benchmarks for 6DoF object pose estimation and tracking. Our source\ncode and trained models are publicly available at\nhttps://github.com/facebookresearch/gotrack", "AI": {"tldr": "GoTrack是一种基于CAD的高效6DoF物体姿态优化与跟踪方法，无需对象特定训练，结合了模型到帧和帧到帧的注册。", "motivation": "现有跟踪方法仅依赖分析-合成方法进行模型到帧注册，GoTrack通过结合帧到帧注册节省计算并稳定跟踪。", "method": "利用光流估计实现两种注册：模型到帧注册使用标准神经网络块（基于DINOv2的Transformer），帧到帧注册使用轻量级现成光流模型。", "result": "GoTrack与现有粗姿态估计方法结合，在标准6DoF姿态估计与跟踪基准测试中达到RGB-only的SOTA结果。", "conclusion": "GoTrack提供了一种高效、准确的姿态优化与跟踪方法，适用于多样化对象，且无需特定训练。"}}
{"id": "2506.06907", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06907", "abs": "https://arxiv.org/abs/2506.06907", "authors": ["Fred Xu", "Thomas Markovich"], "title": "Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations", "comment": null, "summary": "Graph Neural Networks have achieved impressive results across diverse network\nmodeling tasks, but accurately estimating uncertainty on graphs remains\ndifficult, especially under distributional shifts. Unlike traditional\nuncertainty estimation, graph-based uncertainty must account for randomness\narising from both the graph's structure and its label distribution, which adds\ncomplexity. In this paper, making an analogy between the evolution of a\nstochastic partial differential equation (SPDE) driven by Matern Gaussian\nProcess and message passing using GNN layers, we present a principled way to\ndesign a novel message passing scheme that incorporates spatial-temporal noises\nmotivated by the Gaussian Process approach to SPDE. Our method simultaneously\ncaptures uncertainty across space and time and allows explicit control over the\ncovariance kernel smoothness, thereby enhancing uncertainty estimates on graphs\nwith both low and high label informativeness. Our extensive experiments on\nOut-of-Distribution (OOD) detection on graph datasets with varying label\ninformativeness demonstrate the soundness and superiority of our model to\nexisting approaches.", "AI": {"tldr": "提出了一种基于高斯过程的图神经网络消息传递方法，用于提升图数据在分布偏移下的不确定性估计能力。", "motivation": "图神经网络在不确定性估计中面临挑战，尤其是分布偏移时，需同时考虑图结构和标签分布的随机性。", "method": "通过将随机偏微分方程（SPDE）与GNN消息传递类比，设计了一种结合时空噪声的消息传递方案，利用高斯过程控制核平滑度。", "result": "实验表明，该方法在低和高标签信息量的图数据上均能有效提升OOD检测性能，优于现有方法。", "conclusion": "该方法为图数据不确定性估计提供了新思路，尤其在分布偏移场景下表现优越。"}}
{"id": "2506.07164", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07164", "abs": "https://arxiv.org/abs/2506.07164", "authors": ["Qiong Chang", "Xinyuan Chen", "Xiang Li", "Weimin Wang", "Jun Miyazaki"], "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs", "comment": null, "summary": "The visual-based SLAM (Simultaneous Localization and Mapping) is a technology\nwidely used in applications such as robotic navigation and virtual reality,\nwhich primarily focuses on detecting feature points from visual images to\nconstruct an unknown environmental map and simultaneously determines its own\nlocation. It usually imposes stringent requirements on hardware power\nconsumption, processing speed and accuracy. Currently, the ORB (Oriented FAST\nand Rotated BRIEF)-based SLAM systems have exhibited superior performance in\nterms of processing speed and robustness. However, they still fall short of\nmeeting the demands for real-time processing on mobile platforms. This\nlimitation is primarily due to the time-consuming Oriented FAST calculations\naccounting for approximately half of the entire SLAM system. This paper\npresents two methods to accelerate the Oriented FAST feature detection on\nlow-end embedded GPUs. These methods optimize the most time-consuming steps in\nOriented FAST feature detection: FAST feature point detection and Harris corner\ndetection, which is achieved by implementing a binary-level encoding strategy\nto determine candidate points quickly and a separable Harris detection strategy\nwith efficient low-level GPU hardware-specific instructions. Extensive\nexperiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over\n7.3 times compared to widely used OpenCV with GPU support. This significant\nimprovement highlights its effectiveness and potential for real-time\napplications in mobile and resource-constrained environments.", "AI": {"tldr": "该论文提出了两种方法，用于在低端嵌入式GPU上加速Oriented FAST特征检测，显著提升了SLAM系统的实时处理能力。", "motivation": "由于Oriented FAST计算耗时占整个SLAM系统的一半，现有ORB-based SLAM系统在移动平台上难以满足实时处理需求。", "method": "通过二进制级编码策略快速确定候选点，以及利用高效的低级GPU硬件特定指令实现可分离的Harris角点检测策略。", "result": "在Jetson TX2嵌入式GPU上的实验表明，相比广泛使用的OpenCV（支持GPU），平均加速超过7.3倍。", "conclusion": "该方法显著提升了处理速度，展示了在移动和资源受限环境中实时应用的潜力。"}}
{"id": "2506.06917", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06917", "abs": "https://arxiv.org/abs/2506.06917", "authors": ["Shangjie Du", "Hui Wei", "Dong Yoon Lee", "Zhizhang Hu", "Shijia Pan"], "title": "Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data", "comment": "Accepted by ACM Transactions on Sensor Networks (TOSN) 2025", "summary": "This work introduces GraPhy, a graph-based, physics-guided learning framework\nfor high-resolution and accurate air quality modeling in urban areas with\nlimited monitoring data. Fine-grained air quality monitoring information is\nessential for reducing public exposure to pollutants. However, monitoring\nnetworks are often sparse in socioeconomically disadvantaged regions, limiting\nthe accuracy and resolution of air quality modeling. To address this, we\npropose a physics-guided graph neural network architecture called GraPhy with\nlayers and edge features designed specifically for low-resolution monitoring\ndata. Experiments using data from California's socioeconomically disadvantaged\nSan Joaquin Valley show that GraPhy achieves the overall best performance\nevaluated by mean squared error (MSE), mean absolute error (MAE), and R-square\nvalue (R2), improving the performance by 9%-56% compared to various baseline\nmodels. Moreover, GraPhy consistently outperforms baselines across different\nspatial heterogeneity levels, demonstrating the effectiveness of our model\ndesign.", "AI": {"tldr": "GraPhy是一种基于图的物理引导学习框架，用于高分辨率空气质量建模，适用于监测数据有限的地区。", "motivation": "解决社会经济弱势地区空气质量监测数据稀疏的问题，提高建模精度和分辨率。", "method": "提出物理引导的图神经网络架构GraPhy，专为低分辨率监测数据设计。", "result": "在加州圣华金谷的实验表明，GraPhy在MSE、MAE和R2上表现最佳，性能提升9%-56%。", "conclusion": "GraPhy在不同空间异质性水平下均优于基线模型，验证了其设计有效性。"}}
{"id": "2506.07177", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07177", "abs": "https://arxiv.org/abs/2506.07177", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Jaehong Yoon", "Soo Ye Kim", "Zhe Lin", "Sung Ju Hwang"], "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models", "comment": "Project page: https://frame-guidance-video.github.io/", "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.", "AI": {"tldr": "提出了一种无需训练的帧级信号引导方法（Frame Guidance），用于可控视频生成，显著减少内存使用并支持多样化任务。", "motivation": "现有方法依赖微调大规模视频模型，随着模型规模增长变得不切实际，因此需要一种无需训练的可控视频生成方法。", "method": "提出Frame Guidance，基于帧级信号（如关键帧、风格参考图等），采用简单的潜在处理方法减少内存，并设计新的潜在优化策略以实现全局一致性。", "result": "实验表明，Frame Guidance能高效生成高质量可控视频，适用于多种任务和输入信号。", "conclusion": "Frame Guidance是一种无需训练、兼容性强且高效的可控视频生成方法。"}}
{"id": "2506.06926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06926", "abs": "https://arxiv.org/abs/2506.06926", "authors": ["Wei Min Loh", "Jiaqi Shang", "Pascal Poupart"], "title": "Basis Transformers for Multi-Task Tabular Regression", "comment": null, "summary": "Dealing with tabular data is challenging due to partial information, noise,\nand heterogeneous structure. Existing techniques often struggle to\nsimultaneously address key aspects of tabular data such as textual information,\na variable number of columns, and unseen data without metadata besides column\nnames. We propose a novel architecture, \\textit{basis transformers},\nspecifically designed to tackle these challenges while respecting inherent\ninvariances in tabular data, including hierarchical structure and the\nrepresentation of numeric values. We evaluate our design on a multi-task\ntabular regression benchmark, achieving an improvement of 0.338 in the median\n$R^2$ score and the lowest standard deviation across 34 tasks from the\nOpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters\nthan the best-performing baseline and surpasses pretrained large language model\nbaselines -- even when initialized from randomized weights.", "AI": {"tldr": "论文提出了一种名为“basis transformers”的新架构，用于处理表格数据的部分信息、噪声和异构结构问题，并在多任务表格回归基准测试中表现优异。", "motivation": "现有技术难以同时处理表格数据的关键方面，如文本信息、可变列数和无元数据的新数据。", "method": "设计了“basis transformers”架构，尊重表格数据的固有不变性，包括层次结构和数值表示。", "result": "在OpenML-CTR23基准测试的34个任务中，中位R²分数提高了0.338，参数数量减少了五倍，且优于预训练大型语言模型基线。", "conclusion": "basis transformers在处理表格数据时表现出色，优于现有方法，且模型更轻量。"}}
{"id": "2506.07188", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2506.07188", "abs": "https://arxiv.org/abs/2506.07188", "authors": ["Ni Ding", "Lei He", "Shengbo Eben Li", "Keqiang Li"], "title": "Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks", "comment": "13 pages, 7 figures,", "summary": "End-to-end autonomous driving has emerged as a dominant paradigm, yet its\nhighly entangled black-box models pose significant challenges in terms of\ninterpretability and safety assurance. To improve model transparency and\ntraining flexibility, this paper proposes a hierarchical and decoupled\npost-training framework tailored for pretrained neural networks. By\nreconstructing intermediate feature maps from ground-truth labels, surrogate\nsupervisory signals are introduced at transitional layers to enable independent\ntraining of specific components, thereby avoiding the complexity and coupling\nof conventional end-to-end backpropagation and providing interpretable insights\ninto networks' internal mechanisms. To the best of our knowledge, this is the\nfirst method to formalize feature-level reverse computation as well-posed\noptimization problems, which we rigorously reformulate as systems of linear\nequations or least squares problems. This establishes a novel and efficient\ntraining paradigm that extends gradient backpropagation to feature\nbackpropagation. Extensive experiments on multiple standard image\nclassification benchmarks demonstrate that the proposed method achieves\nsuperior generalization performance and computational efficiency compared to\ntraditional training approaches, validating its effectiveness and potential.", "AI": {"tldr": "本文提出了一种分层解耦的后训练框架，通过重构中间特征图引入代理监督信号，提升模型透明度和训练灵活性。", "motivation": "解决端到端自动驾驶模型中黑箱特性带来的可解释性和安全性问题。", "method": "利用真实标签重构中间特征图，将特征级反向计算形式化为优化问题（线性方程组或最小二乘问题），实现独立组件训练。", "result": "在多个标准图像分类基准测试中表现出优越的泛化性能和计算效率。", "conclusion": "该方法为神经网络训练提供了新的高效范式，验证了其有效性和潜力。"}}
{"id": "2506.06933", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.06933", "abs": "https://arxiv.org/abs/2506.06933", "authors": ["Mahdi Salmani", "Alireza Abdollahpoorrostam", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry", "comment": null, "summary": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings.", "AI": {"tldr": "本文提出了一种针对图像分类器的非对称查询成本的决策型黑盒攻击框架，通过改进搜索策略和梯度估计过程，显著降低了总查询成本。", "motivation": "现有攻击方法假设所有查询成本相同，而实际中某些查询可能触发额外审查或惩罚，导致成本不对称。针对这一问题，本文提出非对称黑盒攻击框架。", "method": "提出非对称搜索（AS）和非对称梯度估计（AGREST），优化搜索策略和采样分布以减少高成本查询依赖，并设计高效算法平衡不同查询类型。", "result": "在标准图像分类基准测试中，该方法在不同成本机制下均显著降低总查询成本和扰动幅度，某些场景下提升达40%。", "conclusion": "本文提出的非对称黑盒攻击框架有效解决了查询成本不对称问题，为实际应用中的攻击方法提供了更优解决方案。"}}
{"id": "2506.07196", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07196", "abs": "https://arxiv.org/abs/2506.07196", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "comment": "11 pages, 4 figures", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.", "AI": {"tldr": "论文提出了SAP-Bench数据集和MLLM-SAP框架，用于评估多模态大语言模型（MLLMs）在手术动作规划任务中的表现，揭示了当前模型的性能差距。", "motivation": "现有基准无法充分评估手术决策任务中对原子视觉动作的区分和复杂长期程序的协调能力，因此需要高质量数据集和评估方法。", "method": "通过构建SAP-Bench数据集（包含1,226个临床验证的手术动作片段）和提出MLLM-SAP框架，利用MLLMs生成手术场景中的下一步动作推荐。", "result": "评估了七种先进MLLMs，发现它们在下一步动作预测任务中存在显著性能差距。", "conclusion": "SAP-Bench和MLLM-SAP为手术动作规划任务提供了有效的评估工具，并揭示了当前模型的局限性。"}}
{"id": "2506.06940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06940", "abs": "https://arxiv.org/abs/2506.06940", "authors": ["Geonhui Yoo", "Minhak Song", "Chulhee Yun"], "title": "Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More", "comment": "ICML 2025", "summary": "When training deep neural networks with gradient descent, sharpness often\nincreases -- a phenomenon known as progressive sharpening -- before saturating\nat the edge of stability. Although commonly observed in practice, the\nunderlying mechanisms behind progressive sharpening remain poorly understood.\nIn this work, we study this phenomenon using a minimalist model: a deep linear\nnetwork with a single neuron per layer. We show that this simple model\neffectively captures the sharpness dynamics observed in recent empirical\nstudies, offering a simple testbed to better understand neural network\ntraining. Moreover, we theoretically analyze how dataset properties, network\ndepth, stochasticity of optimizers, and step size affect the degree of\nprogressive sharpening in the minimalist model. We then empirically demonstrate\nhow these theoretical insights extend to practical scenarios. This study offers\na deeper understanding of sharpness dynamics in neural network training,\nhighlighting the interplay between depth, training data, and optimizers.", "AI": {"tldr": "本文通过研究深度线性网络中的渐进锐化现象，揭示了训练数据、网络深度和优化器对锐化动态的影响。", "motivation": "理解深度神经网络训练中常见的渐进锐化现象及其机制。", "method": "使用单神经元每层的深度线性网络作为简化模型，分析锐化动态。", "result": "理论分析表明数据集属性、网络深度和优化器影响锐化程度，实证验证了这些发现。", "conclusion": "研究提供了对神经网络训练中锐化动态的深入理解，强调了深度、数据和优化器的相互作用。"}}
{"id": "2506.07205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07205", "abs": "https://arxiv.org/abs/2506.07205", "authors": ["Min-Jung Kim", "Dongjin Kim", "Seokju Yun", "Jaegul Choo"], "title": "TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation", "comment": null, "summary": "Video editing has garnered increasing attention alongside the rapid progress\nof diffusion-based video generation models. As part of these advancements,\nthere is a growing demand for more accessible and controllable forms of video\nediting, such as prompt-based editing. Previous studies have primarily focused\non tasks such as style transfer, background replacement, object substitution,\nand attribute modification, while maintaining the content structure of the\nsource video. However, more complex tasks, including the addition of novel\nobjects and nonrigid transformations, remain relatively unexplored. In this\npaper, we present TV-LiVE, a Training-free and text-guided Video editing\nframework via Layerinformed Vitality Exploitation. We empirically identify\nvital layers within the video generation model that significantly influence the\nquality of generated outputs. Notably, these layers are closely associated with\nRotary Position Embeddings (RoPE). Based on this observation, our method\nenables both object addition and non-rigid video editing by selectively\ninjecting key and value features from the source model into the corresponding\nlayers of the target model guided by the layer vitality. For object addition,\nwe further identify prominent layers to extract the mask regions corresponding\nto the newly added target prompt. We found that the extracted masks from the\nprominent layers faithfully indicate the region to be edited. Experimental\nresults demonstrate that TV-LiVE outperforms existing approaches for both\nobject addition and non-rigid video editing. Project Page:\nhttps://emjay73.github.io/TV_LiVE/", "AI": {"tldr": "TV-LiVE是一种无需训练、基于文本引导的视频编辑框架，通过利用关键层信息实现复杂编辑任务，如添加新对象和非刚性变换。", "motivation": "现有视频编辑方法主要关注简单任务（如风格迁移、背景替换），而复杂任务（如添加新对象和非刚性变换）研究较少。TV-LiVE旨在填补这一空白。", "method": "通过识别视频生成模型中的关键层（与RoPE相关），选择性注入源模型的特征到目标模型，实现对象添加和非刚性编辑。", "result": "实验表明，TV-LiVE在对象添加和非刚性编辑任务上优于现有方法。", "conclusion": "TV-LiVE为复杂视频编辑任务提供了一种高效且可控的解决方案。"}}
{"id": "2506.07214", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.07214", "abs": "https://arxiv.org/abs/2506.07214", "authors": ["Zhiyuan Zhong", "Zhen Sun", "Yepang Liu", "Xinlei He", "Guanhong Tao"], "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation", "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment.", "AI": {"tldr": "论文提出了一种新的跨模态后门攻击方法BadSem，利用图像-文本语义不匹配作为隐式触发器，攻击视觉语言模型（VLMs）。实验表明其攻击成功率高达98%，且现有防御方法难以应对。", "motivation": "现有后门攻击主要依赖单模态触发器，忽略了VLMs的跨模态融合特性，因此研究跨模态语义不匹配作为触发器的攻击方法具有重要意义。", "method": "提出BadSem攻击方法，通过故意在训练时错配图像-文本对（如颜色和物体属性）植入后门，并构建SIMBad数据集支持攻击。", "result": "在四种主流VLMs上，BadSem平均攻击成功率（ASR）超过98%，且能泛化到分布外数据。可视化分析显示后门模型在语义敏感区域异常聚焦。", "conclusion": "BadSem揭示了VLMs的语义漏洞，现有防御方法无效，亟需研究更安全的跨模态模型部署方案。"}}
{"id": "2506.06977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06977", "abs": "https://arxiv.org/abs/2506.06977", "authors": ["Pengfei Hu", "Xiaoxue Han", "Fei Wang", "Yue Ning"], "title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare", "comment": null, "summary": "Domain generalization has become a critical challenge in clinical prediction,\nwhere patient cohorts often exhibit shifting data distributions that degrade\nmodel performance. Typical domain generalization approaches struggle in\nreal-world healthcare settings for two main reasons: (1) patient-specific\ndomain labels are typically unavailable, making domain discovery especially\ndifficult; (2) purely data-driven approaches overlook key clinical insights,\nleading to a gap in medical knowledge integration. To address these problems,\nwe leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to\ngroup diseases into higher-level categories and discover more flexible latent\ndomains. In this paper, we introduce UdonCare, a hierarchy-guided framework\nthat iteratively prunes fine-grained domains, encodes these refined domains,\nand applies a Siamese-type inference mechanism to separate domain-related\nsignals from patient-level features. Experimental results on clinical datasets\n(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher\nperformance compared to other domain generalization baselines when substantial\ndomain gaps presents, highlighting the untapped potential of medical knowledge\nfor enhancing domain generalization in practical healthcare applications.", "AI": {"tldr": "论文提出了一种基于医学本体层次结构的领域泛化框架UdonCare，用于解决临床预测中数据分布偏移的问题。", "motivation": "临床预测中患者数据分布偏移导致模型性能下降，且传统方法因缺乏领域标签和忽视医学知识而效果不佳。", "method": "利用ICD-9-CM层次结构分组疾病，提出UdonCare框架，通过迭代修剪细粒度领域、编码领域信息，并采用Siamese推理机制分离领域信号与患者特征。", "result": "在MIMIC-III和MIMIC-IV数据集上，UdonCare在领域差异显著时表现优于其他基线方法。", "conclusion": "医学知识在提升临床领域泛化能力方面具有潜力，UdonCare为此提供了有效解决方案。"}}
{"id": "2506.07216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07216", "abs": "https://arxiv.org/abs/2506.07216", "authors": ["Nada Aboudeshish", "Dmitry Ignatov", "Radu Timofte"], "title": "AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?", "comment": null, "summary": "Data augmentation is a crucial technique in deep learning, particularly for\ntasks with limited dataset diversity, such as skeleton-based datasets. This\npaper proposes a comprehensive data augmentation framework that integrates\ngeometric transformations, random cropping, rotation, zooming and\nintensity-based transformations, brightness and contrast adjustments to\nsimulate real-world variations. Random cropping ensures the preservation of\nspatio-temporal integrity while addressing challenges such as viewpoint bias\nand occlusions. The augmentation pipeline generates three augmented versions\nfor each sample in addition to the data set sample, thus quadrupling the data\nset size and enriching the diversity of gesture representations. The proposed\naugmentation strategy is evaluated on three models: multi-stream e2eET, FPPR\npoint cloud-based hand gesture recognition (HGR), and DD-Network. Experiments\nare conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.\nThe e2eET model, recognized as the state-of-the-art for hand gesture\nrecognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best\nperforming model on SHREC'17, excels in point cloud-based gesture recognition.\nDD-Net, a lightweight and efficient architecture for skeleton-based action\nrecognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).\nThe results underline the effectiveness and versatility of the proposed\naugmentation strategy, significantly improving model generalization and\nrobustness across diverse datasets and architectures. This framework not only\nestablishes state-of-the-art results on all three evaluated models but also\noffers a scalable solution to advance HGR and action recognition applications\nin real-world scenarios. The framework is available at\nhttps://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR", "AI": {"tldr": "本文提出了一种全面的数据增强框架，通过几何变换、随机裁剪、旋转等方法丰富骨架数据集的多样性，显著提升了手势识别和动作识别的性能。", "motivation": "解决骨架数据集中数据多样性不足的问题，通过模拟真实世界的变化提升模型的泛化能力和鲁棒性。", "method": "集成几何变换、随机裁剪、旋转、缩放和强度变换等方法，生成多样化的数据增强版本。", "result": "在多个基准数据集和模型上验证了框架的有效性，显著提升了性能，并实现了最先进的结果。", "conclusion": "该框架为手势识别和动作识别提供了一种可扩展的解决方案，适用于实际应用场景。"}}
{"id": "2506.06978", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06978", "abs": "https://arxiv.org/abs/2506.06978", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Near Optimal Non-asymptotic Sample Complexity of 1-Identification", "comment": null, "summary": "Motivated by an open direction in existing literature, we study the\n1-identification problem, a fundamental multi-armed bandit formulation on pure\nexploration. The goal is to determine whether there exists an arm whose mean\nreward is at least a known threshold $\\mu_0$, or to output None if it believes\nsuch an arm does not exist. The agent needs to guarantee its output is correct\nwith probability at least $1-\\delta$. Degenne & Koolen 2019 has established the\nasymptotically tight sample complexity for the 1-identification problem, but\nthey commented that the non-asymptotic analysis remains unclear. We design a\nnew algorithm Sequential-Exploration-Exploitation (SEE), and conduct\ntheoretical analysis from the non-asymptotic perspective. Novel to the\nliterature, we achieve near optimality, in the sense of matching upper and\nlower bounds on the pulling complexity. The gap between the upper and lower\nbounds is up to a polynomial logarithmic factor. The numerical result also\nindicates the effectiveness of our algorithm, compared to existing benchmarks.", "AI": {"tldr": "论文研究了1-identification问题，提出新算法SEE，填补了非渐进分析的空白，实现了接近最优的样本复杂度。", "motivation": "现有文献中关于1-identification问题的非渐进分析尚不明确，本文旨在填补这一空白。", "method": "设计了新算法Sequential-Exploration-Exploitation (SEE)，并从非渐进角度进行理论分析。", "result": "SEE算法在样本复杂度上实现了接近最优的上界和下界匹配，数值实验也验证了其有效性。", "conclusion": "SEE算法在1-identification问题上表现优异，填补了非渐进分析的空白。"}}
{"id": "2506.07227", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07227", "abs": "https://arxiv.org/abs/2506.07227", "authors": ["Tianyi Bai", "Yuxuan Fan", "Jiantao Qiu", "Fupeng Sun", "Jiayi Song", "Junlin Han", "Zichen Liu", "Conghui He", "Wentao Zhang", "Binhang Yuan"], "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks but still struggle with fine-grained visual differences,\nleading to hallucinations or missed semantic shifts. We attribute this to\nlimitations in both training data and learning objectives. To address these\nissues, we propose a controlled data generation pipeline that produces\nminimally edited image pairs with semantically aligned captions. Using this\npipeline, we construct the Micro Edit Dataset (MED), containing over 50K\nimage-text pairs spanning 11 fine-grained edit categories, including attribute,\ncount, position, and object presence changes. Building on MED, we introduce a\nsupervised fine-tuning (SFT) framework with a feature-level consistency loss\nthat promotes stable visual embeddings under small edits. We evaluate our\napproach on the Micro Edit Detection benchmark, which includes carefully\nbalanced evaluation pairs designed to test sensitivity to subtle visual\nvariations across the same edit categories. Our method improves difference\ndetection accuracy and reduces hallucinations compared to strong baselines,\nincluding GPT-4o. Moreover, it yields consistent gains on standard\nvision-language tasks such as image captioning and visual question answering.\nThese results demonstrate the effectiveness of combining targeted data and\nalignment objectives for enhancing fine-grained visual reasoning in MLLMs.", "AI": {"tldr": "论文提出了一种针对多模态大语言模型（MLLMs）在细粒度视觉差异任务中表现不佳的解决方案，通过构建微编辑数据集（MED）和引入特征级一致性损失的监督微调框架，显著提升了模型的性能。", "motivation": "MLLMs在视觉语言任务中表现优异，但在细粒度视觉差异（如属性、数量、位置等）上容易产生幻觉或遗漏语义变化，这主要源于训练数据和目标函数的局限性。", "method": "提出了一种可控数据生成流程，构建了包含50K图像-文本对的微编辑数据集（MED），并设计了基于特征级一致性损失的监督微调框架。", "result": "在微编辑检测基准测试中，该方法显著提升了差异检测准确性并减少了幻觉，同时在标准视觉语言任务（如图像描述和视觉问答）中表现一致优于基线模型（如GPT-4o）。", "conclusion": "通过结合针对性数据和目标对齐，可以有效增强MLLMs在细粒度视觉推理任务中的表现。"}}
{"id": "2506.06980", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.06980", "abs": "https://arxiv.org/abs/2506.06980", "authors": ["Sajib Acharjee Dip", "Uddip Acharjee Shuvo", "Dipanwita Mallick", "Abrar Rahman Abir", "Liqing Zhang"], "title": "MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification", "comment": "9 pages, 1 figure, 6 tables", "summary": "Cancer subtype classification is crucial for personalized treatment and\nprognostic assessment. However, effectively integrating multi-omic data remains\nchallenging due to the heterogeneous nature of genomic, epigenomic, and\ntranscriptomic features. In this work, we propose Modality-Aware\nCross-Attention MoXGATE, a novel deep-learning framework that leverages\ncross-attention and learnable modality weights to enhance feature fusion across\nmultiple omics sources. Our approach effectively captures inter-modality\ndependencies, ensuring robust and interpretable integration. Through\nexperiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)\ndatasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,\nachieving 95\\% classification accuracy. Ablation studies validate the\neffectiveness of cross-attention over simple concatenation and highlight the\nimportance of different omics modalities. Moreover, our model generalizes well\nto unseen cancer types e.g., breast cancer, underscoring its adaptability. Key\ncontributions include (1) a cross-attention-based multi-omic integration\nframework, (2) modality-weighted fusion for enhanced interpretability, (3)\napplication of focal loss to mitigate data imbalance, and (4) validation across\nmultiple cancer subtypes. Our results indicate that MoXGATE is a promising\napproach for multi-omic cancer subtype classification, offering improved\nperformance and biological generalizability.", "AI": {"tldr": "MoXGATE是一种新型深度学习框架，通过跨注意力和可学习模态权重整合多组学数据，显著提升癌症亚型分类性能。", "motivation": "癌症亚型分类对个性化治疗和预后评估至关重要，但多组学数据的异构性使其整合具有挑战性。", "method": "提出MoXGATE框架，利用跨注意力和模态加权融合技术，结合焦点损失处理数据不平衡问题。", "result": "在GIAC和BRCA数据集上达到95%的分类准确率，验证了跨注意力和模态加权的重要性。", "conclusion": "MoXGATE在多组学癌症亚型分类中表现出优越性能和生物通用性。"}}
{"id": "2506.07235", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07235", "abs": "https://arxiv.org/abs/2506.07235", "authors": ["Tianyi Bai", "Zengjie Hu", "Fupeng Sun", "Jiantao Qiu", "Yizhen Jiang", "Guangxin He", "Bohan Zeng", "Conghui He", "Binhang Yuan", "Wentao Zhang"], "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification", "comment": null, "summary": "Multi-modal large language models (MLLMs) have achieved remarkable\ncapabilities by integrating visual perception with language understanding,\nenabling applications such as image-grounded dialogue, visual question\nanswering, and scientific analysis. However, most MLLMs adopt a static\ninference paradigm, encoding the entire image into fixed visual tokens upfront,\nwhich limits their ability to iteratively refine understanding or adapt to\ncontext during inference. This contrasts sharply with human perception, which\nis dynamic, selective, and feedback-driven. In this work, we introduce a novel\nframework for inference-time visual token scaling that enables MLLMs to perform\niterative, verifier-guided reasoning over visual content. We formulate the\nproblem as a Markov Decision Process, involving a reasoner that proposes visual\nactions and a verifier, which is trained via multi-step Direct Preference\nOptimization (DPO), that evaluates these actions and determines when reasoning\nshould terminate. To support this, we present a new dataset, VTS, comprising\nsupervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning\ncomparisons (VTS-DPO). Our method significantly outperforms existing approaches\nacross diverse visual reasoning benchmarks, offering not only improved accuracy\nbut also more interpretable and grounded reasoning processes. These results\ndemonstrate the promise of dynamic inference mechanisms for enabling\nfine-grained, context-aware visual reasoning in next-generation MLLMs.", "AI": {"tldr": "论文提出了一种动态推理框架，通过迭代优化视觉令牌，提升多模态大语言模型（MLLMs）的视觉推理能力。", "motivation": "现有MLLMs采用静态推理范式，限制了其动态适应和迭代优化的能力，与人类感知的动态性不符。", "method": "提出基于马尔可夫决策过程的框架，结合推理器和验证器，通过多步直接偏好优化（DPO）训练验证器。", "result": "方法在多个视觉推理基准上显著优于现有方法，提高了准确性和可解释性。", "conclusion": "动态推理机制为下一代MLLMs实现细粒度、上下文感知的视觉推理提供了潜力。"}}
{"id": "2506.06985", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06985", "abs": "https://arxiv.org/abs/2506.06985", "authors": ["Anastasia Koloskova", "Youssef Allouah", "Animesh Jha", "Rachid Guerraoui", "Sanmi Koyejo"], "title": "Certified Unlearning for Neural Networks", "comment": null, "summary": "We address the problem of machine unlearning, where the goal is to remove the\ninfluence of specific training data from a model upon request, motivated by\nprivacy concerns and regulatory requirements such as the \"right to be\nforgotten.\" Unfortunately, existing methods rely on restrictive assumptions or\nlack formal guarantees. To this end, we propose a novel method for certified\nmachine unlearning, leveraging the connection between unlearning and privacy\namplification by stochastic post-processing. Our method uses noisy fine-tuning\non the retain data, i.e., data that does not need to be removed, to ensure\nprovable unlearning guarantees. This approach requires no assumptions about the\nunderlying loss function, making it broadly applicable across diverse settings.\nWe analyze the theoretical trade-offs in efficiency and accuracy and\ndemonstrate empirically that our method not only achieves formal unlearning\nguarantees but also performs effectively in practice, outperforming existing\nbaselines. Our code is available at\nhttps://github.com/stair-lab/certified-unlearningneural-networks-icml-2025", "AI": {"tldr": "提出了一种新的认证机器遗忘方法，通过噪声微调保留数据实现可证明的遗忘保证，无需对损失函数做假设，适用于多种场景。", "motivation": "解决机器遗忘问题，满足隐私需求和法规要求（如“被遗忘权”），现有方法假设严格或缺乏理论保证。", "method": "利用遗忘与随机后处理隐私放大的联系，通过噪声微调保留数据实现认证遗忘。", "result": "理论分析了效率与准确性的权衡，实验表明方法具有正式遗忘保证且优于现有基线。", "conclusion": "该方法无需限制性假设，适用性广，理论实践均表现优异。"}}
{"id": "2506.07280", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07280", "abs": "https://arxiv.org/abs/2506.07280", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models", "comment": "27 pages, 23 figures, 9 tables", "summary": "Video Diffusion Models (VDMs) have emerged as powerful generative tools,\ncapable of synthesizing high-quality spatiotemporal content. Yet, their\npotential goes far beyond mere video generation. We argue that the training\ndynamics of VDMs, driven by the need to model coherent sequences, naturally\npushes them to internalize structured representations and an implicit\nunderstanding of the visual world. To probe the extent of this internal\nknowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs\nfor new tasks using only a handful of examples. Our method transforms each task\ninto a visual transition, enabling the training of LoRA weights on short\ninput-output sequences without altering the generative interface of a frozen\nVDM. Despite minimal supervision, the model exhibits strong generalization\nacross diverse tasks, from low-level vision (for example, segmentation and pose\nestimation) to high-level reasoning (for example, on ARC-AGI). These results\nreframe VDMs as more than generative engines. They are adaptable visual\nlearners with the potential to serve as the backbone for future foundation\nmodels in vision.", "AI": {"tldr": "视频扩散模型（VDMs）不仅是强大的生成工具，还能通过少量样本微调适应新任务，展现出广泛的视觉理解能力。", "motivation": "探索VDMs在视频生成之外的潜力，验证其是否能够通过训练动态内化视觉世界的结构化表示。", "method": "提出一种少样本微调框架，将任务转化为视觉过渡，通过LoRA权重训练冻结的VDM。", "result": "模型在低层视觉任务（如分割、姿态估计）和高层推理任务（如ARC-AGI）中表现出强大的泛化能力。", "conclusion": "VDMs不仅是生成引擎，还是适应性强的视觉学习器，有望成为未来视觉基础模型的核心。"}}
{"id": "2506.06986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06986", "abs": "https://arxiv.org/abs/2506.06986", "authors": ["Austin Snyder", "Ryan Gallagher", "Boris Kovalerchuk"], "title": "Fully Explainable Classification Models Using Hyperblocks", "comment": "7 pages, 8 figures, 6 tables", "summary": "Building on existing work with Hyperblocks, which classify data using minimum\nand maximum bounds for each attribute, we focus on enhancing interpretability,\ndecreasing training time, and reducing model complexity without sacrificing\naccuracy. This system allows subject matter experts (SMEs) to directly inspect\nand understand the model's decision logic without requiring extensive machine\nlearning expertise. To reduce Hyperblock complexity while retaining\nperformance, we introduce a suite of algorithms for Hyperblock simplification.\nThese include removing redundant attributes, removing redundant blocks through\noverlap analysis, and creating disjunctive units. These methods eliminate\nunnecessary parameters, dramatically reducing model size without harming\nclassification power. We increase robustness by introducing an interpretable\nfallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not\ncovered by any block, ensuring complete data coverage while preserving model\ntransparency. Our results demonstrate that interpretable models can scale to\nhigh-dimensional, large-volume datasets while maintaining competitive accuracy.\nOn benchmark datasets such as WBC (9-D), we achieve strong predictive\nperformance with significantly reduced complexity. On MNIST (784-D), our method\ncontinues to improve through tuning and simplification, showing promise as a\ntransparent alternative to black-box models in domains where trust, clarity,\nand control are crucial.", "AI": {"tldr": "该论文提出了一种基于Hyperblocks的改进方法，旨在提升模型的可解释性、减少训练时间和复杂度，同时保持准确性。通过简化算法和引入k-NN回退机制，实现了高性能且透明的分类模型。", "motivation": "现有Hyperblocks方法虽然有效，但复杂且难以解释。论文旨在解决这一问题，使模型更易于理解且高效。", "method": "提出了一系列Hyperblock简化算法，包括冗余属性和块去除、重叠分析及创建分离单元，并引入k-NN回退机制确保数据全覆盖。", "result": "在WBC和MNIST等数据集上，模型在保持高准确性的同时显著降低了复杂度。", "conclusion": "该方法为高维大数据集提供了一种透明且高效的分类解决方案，适用于需要信任和清晰度的领域。"}}
{"id": "2506.06990", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06990", "abs": "https://arxiv.org/abs/2506.06990", "authors": ["Mingyi Li", "Michael R. Metel", "Akiko Takeda"], "title": "Modified K-means Algorithm with Local Optimality Guarantees", "comment": "ICML 2025", "summary": "The K-means algorithm is one of the most widely studied clustering algorithms\nin machine learning. While extensive research has focused on its ability to\nachieve a globally optimal solution, there still lacks a rigorous analysis of\nits local optimality guarantees. In this paper, we first present conditions\nunder which the K-means algorithm converges to a locally optimal solution.\nBased on this, we propose simple modifications to the K-means algorithm which\nensure local optimality in both the continuous and discrete sense, with the\nsame computational complexity as the original K-means algorithm. As the\ndissimilarity measure, we consider a general Bregman divergence, which is an\nextension of the squared Euclidean distance often used in the K-means\nalgorithm. Numerical experiments confirm that the K-means algorithm does not\nalways find a locally optimal solution in practice, while our proposed methods\nprovide improved locally optimal solutions with reduced clustering loss. Our\ncode is available at https://github.com/lmingyi/LO-K-means.", "AI": {"tldr": "本文分析了K-means算法的局部最优性，提出了改进方法以确保局部最优解，并通过实验验证了其有效性。", "motivation": "尽管K-means算法被广泛研究，但其局部最优性缺乏严格分析，本文旨在填补这一空白。", "method": "提出了简单的改进方法，确保在连续和离散情况下都能达到局部最优，同时保持与原算法相同的计算复杂度。", "result": "实验表明，改进方法能提供更好的局部最优解，并降低聚类损失。", "conclusion": "改进后的K-means算法在局部最优性方面表现更优，且计算效率不受影响。"}}
{"id": "2506.07304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07304", "abs": "https://arxiv.org/abs/2506.07304", "authors": ["Kavitha Viswanathan", "Vrinda Goel", "Shlesh Gholap", "Devayan Ghosh", "Madhav Gupta", "Dhruvi Ganatra", "Sanket Potdar", "Amit Sethi"], "title": "FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos", "comment": null, "summary": "Real-world surveillance often renders faces and license plates unrecognizable\nin individual low-resolution (LR) frames, hindering reliable identification. To\nadvance temporal recognition models, we present FANVID, a novel video-based\nbenchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63\nidentities and 49 license plates from three English-speaking countries. Each\nvideo includes distractor faces and plates, increasing task difficulty and\nrealism. The dataset contains 31,096 manually verified bounding boxes and\nlabels.\n  FANVID defines two tasks: (1) face matching -- detecting LR faces and\nmatching them to high-resolution mugshots, and (2) license plate recognition --\nextracting text from LR plates without a predefined database. Videos are\ndownsampled from high-resolution sources to ensure that faces and text are\nindecipherable in single frames, requiring models to exploit temporal\ninformation. We introduce evaluation metrics adapted from mean Average\nPrecision at IoU > 0.5, prioritizing identity correctness for faces and\ncharacter-level accuracy for text.\n  A baseline method with pre-trained video super-resolution, detection, and\nrecognition achieved performance scores of 0.58 (face matching) and 0.42 (plate\nrecognition), highlighting both the feasibility and challenge of the tasks.\nFANVID's selection of faces and plates balances diversity with recognition\nchallenge. We release the software for data access, evaluation, baseline, and\nannotation to support reproducibility and extension. FANVID aims to catalyze\ninnovation in temporal modeling for LR recognition, with applications in\nsurveillance, forensics, and autonomous vehicles.", "AI": {"tldr": "FANVID是一个新的视频基准数据集，用于低分辨率（LR）视频中的人脸匹配和车牌识别任务，包含1,463个视频片段和31,096个标注框。", "motivation": "现实监控中低分辨率视频难以识别个体身份，需开发利用时序信息的识别模型。", "method": "数据集包含多样化的低分辨率视频，任务包括人脸匹配和车牌识别，评估指标基于身份正确性和字符级准确性。", "result": "基线方法在两项任务中分别得分为0.58和0.42，表明任务可行但具挑战性。", "conclusion": "FANVID旨在推动低分辨率视频时序建模的创新，适用于监控、法医和自动驾驶领域。"}}
{"id": "2506.06999", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.06999", "abs": "https://arxiv.org/abs/2506.06999", "authors": ["Arun Sharma", "Mingzhou Yang", "Majid Farhadloo", "Subhankar Ghosh", "Bharat Jayaprakash", "Shashi Shekhar"], "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "comment": null, "summary": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "AI": {"tldr": "提出一种基于物理约束的扩散模型，用于检测异常轨迹，提高检测精度并降低误报率。", "motivation": "解决GPS欺骗导致的非法活动（如非法捕鱼和石油走私）问题，同时应对AI生成虚假轨迹和数据标注不足的挑战。", "method": "结合物理运动学约束的扩散模型，识别不符合物理规律的轨迹。", "result": "在真实数据集（海事和城市领域）上表现优于现有方法，检测精度更高，估计误差更低。", "conclusion": "物理约束的扩散模型能有效检测异常轨迹，为相关领域提供实用工具。"}}
{"id": "2506.07310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07310", "abs": "https://arxiv.org/abs/2506.07310", "authors": ["Adam W. Harley", "Yang You", "Xinglong Sun", "Yang Zheng", "Nikhil Raghuraman", "Yunqi Gu", "Sheldon Liang", "Wen-Hsuan Chu", "Achal Dave", "Pavel Tokmakov", "Suya You", "Rares Ambrus", "Katerina Fragkiadaki", "Leonidas J. Guibas"], "title": "AllTracker: Efficient Dense Point Tracking at High Resolution", "comment": null, "summary": "We introduce AllTracker: a model that estimates long-range point tracks by\nway of estimating the flow field between a query frame and every other frame of\na video. Unlike existing point tracking methods, our approach delivers\nhigh-resolution and dense (all-pixel) correspondence fields, which can be\nvisualized as flow maps. Unlike existing optical flow methods, our approach\ncorresponds one frame to hundreds of subsequent frames, rather than just the\nnext frame. We develop a new architecture for this task, blending techniques\nfrom existing work in optical flow and point tracking: the model performs\niterative inference on low-resolution grids of correspondence estimates,\npropagating information spatially via 2D convolution layers, and propagating\ninformation temporally via pixel-aligned attention layers. The model is fast\nand parameter-efficient (16 million parameters), and delivers state-of-the-art\npoint tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on\na 40G GPU). A benefit of our design is that we can train on a wider set of\ndatasets, and we find that doing so is crucial for top performance. We provide\nan extensive ablation study on our architecture details and training recipe,\nmaking it clear which details matter most. Our code and model weights are\navailable at https://alltracker.github.io .", "AI": {"tldr": "AllTracker是一种通过估计查询帧与视频中其他帧之间的流场来估计长距离点轨迹的模型，提供高分辨率和密集（全像素）的对应字段。", "motivation": "现有点跟踪方法无法提供高分辨率和密集对应字段，而现有光流方法仅能对应下一帧。AllTracker旨在填补这一空白。", "method": "结合光流和点跟踪技术，采用低分辨率网格的迭代推断，通过2D卷积层和像素对齐注意力层分别进行空间和时间信息传播。", "result": "模型参数高效（1600万参数），在768x1024像素分辨率下实现最先进的点跟踪精度，并在40G GPU上运行快速。", "conclusion": "AllTracker通过结合光流和点跟踪技术，实现了高效、高精度的长距离点跟踪，且训练数据集的多样性对性能至关重要。"}}
{"id": "2506.07003", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2506.07003", "abs": "https://arxiv.org/abs/2506.07003", "authors": ["Utkarsh Utkarsh", "Danielle C. Maddix", "Ruijun Ma", "Michael W. Mahoney", "Yuyang Wang"], "title": "End-to-End Probabilistic Framework for Learning with Hard Constraints", "comment": "46 pages, 5 figures, 10 tables", "summary": "We present a general purpose probabilistic forecasting framework,\nProbHardE2E, to learn systems that can incorporate operational/physical\nconstraints as hard requirements. ProbHardE2E enforces hard constraints by\nexploiting variance information in a novel way; and thus it is also capable of\nperforming uncertainty quantification (UQ) on the model. Our methodology uses a\nnovel differentiable probabilistic projection layer (DPPL) that can be combined\nwith a wide range of neural network architectures. This DPPL allows the model\nto learn the system in an end-to-end manner, compared to other approaches where\nthe constraints are satisfied either through a post-processing step or at\ninference. In addition, ProbHardE2E can optimize a strictly proper scoring\nrule, without making any distributional assumptions on the target, which\nenables it to obtain robust distributional estimates (in contrast to existing\napproaches that generally optimize likelihood-based objectives, which are\nheavily biased by their distributional assumptions and model choices); and it\ncan incorporate a range of non-linear constraints (increasing the power of\nmodeling and flexibility). We apply ProbHardE2E to problems in learning partial\ndifferential equations with uncertainty estimates and to probabilistic\ntime-series forecasting, showcasing it as a broadly applicable general setup\nthat connects these seemingly disparate domains.", "AI": {"tldr": "ProbHardE2E是一个通用概率预测框架，通过可微分概率投影层（DPPL）实现硬约束，支持端到端学习，适用于多种神经网络架构。", "motivation": "现有方法通常通过后处理或推理阶段满足约束，且依赖分布假设，限制了模型的灵活性和鲁棒性。", "method": "利用DPPL层结合神经网络，直接优化严格评分规则，支持非线性约束和不确定性量化。", "result": "在偏微分方程学习和时间序列预测中展示了广泛适用性。", "conclusion": "ProbHardE2E为硬约束和不确定性量化提供了一种通用且灵活的方法。"}}
{"id": "2506.07327", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.5.5; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.07327", "abs": "https://arxiv.org/abs/2506.07327", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "title": "\"CASE: Contrastive Activation for Saliency Estimation", "comment": "9 pages, 5 figures. Submitted to IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)", "summary": "Saliency methods are widely used to visualize which input features are deemed\nrelevant to a model's prediction. However, their visual plausibility can\nobscure critical limitations. In this work, we propose a diagnostic test for\nclass sensitivity: a method's ability to distinguish between competing class\nlabels on the same input. Through extensive experiments, we show that many\nwidely used saliency methods produce nearly identical explanations regardless\nof the class label, calling into question their reliability. We find that\nclass-insensitive behavior persists across architectures and datasets,\nsuggesting the failure mode is structural rather than model-specific. Motivated\nby these findings, we introduce CASE, a contrastive explanation method that\nisolates features uniquely discriminative for the predicted class. We evaluate\nCASE using the proposed diagnostic and a perturbation-based fidelity test, and\nshow that it produces faithful and more class-specific explanations than\nexisting methods.", "AI": {"tldr": "本文提出了一种诊断测试（class sensitivity）来评估显著性方法的可靠性，发现许多方法对类别标签不敏感，并提出了对比性解释方法CASE，其表现优于现有方法。", "motivation": "显著性方法在可视化模型预测时被广泛使用，但其视觉合理性可能掩盖了关键局限性，尤其是对类别标签的敏感性不足。", "method": "提出了一种诊断测试（class sensitivity），并通过实验验证了许多显著性方法的类别不敏感性。随后提出了对比性解释方法CASE，通过隔离对预测类别唯一具有区分性的特征来生成解释。", "result": "实验表明，许多显著性方法对类别标签不敏感，而CASE在诊断测试和基于扰动的保真度测试中表现更优，能生成更忠实且类别特定的解释。", "conclusion": "显著性方法的类别不敏感性是一个普遍问题，而CASE通过对比性解释提供了更可靠的解决方案。"}}
{"id": "2506.07014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07014", "abs": "https://arxiv.org/abs/2506.07014", "authors": ["Yutaro Nakagama", "Daisuke Ishii", "Kazuki Yoshizoe"], "title": "Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection", "comment": "8 pages, 3 figures, to be published at IV 2025", "summary": "Driver drowsiness detection (DDD) prevents road accidents caused by driver\nfatigue. Vehicle dynamics-based DDD has been proposed as a method that is both\neconomical and high performance. However, there are concerns about the\nreliability of performance metrics and the reproducibility of many of the\nexisting methods. For instance, some previous studies seem to have a data\nleakage issue among training and test datasets, and many do not openly provide\nthe datasets they used. To this end, this paper aims to compare the performance\nof representative vehicle dynamics-based DDD methods under a transparent and\nfair framework that uses a public dataset. We first develop a framework for\nextracting features from an open dataset by Aygun et al. and performing DDD\nwith lightweight ML models; the framework is carefully designed to support a\nvariety of onfigurations. Second, we implement three existing representative\nmethods and a concise random forest (RF)-based method in the framework.\nFinally, we report the results of experiments to verify the reproducibility and\nclarify the performance of DDD based on common metrics. Among the evaluated\nmethods, the RF-based method achieved the highest accuracy of 88 %. Our\nfindings imply the issues inherent in DDD methods developed in a non-standard\nmanner, and demonstrate a high performance method implemented appropriately.", "AI": {"tldr": "论文提出了一种透明公平的框架，用于比较基于车辆动力学的驾驶员疲劳检测方法，并使用公开数据集验证了随机森林方法的最高准确性（88%）。", "motivation": "解决现有驾驶员疲劳检测方法在性能指标可靠性和可复现性方面的问题，尤其是数据泄漏和数据集不公开的问题。", "method": "开发了一个框架，从公开数据集中提取特征，并使用轻量级机器学习模型进行疲劳检测；实现了三种现有方法和一种随机森林方法。", "result": "随机森林方法在实验中表现最佳，准确率达到88%，验证了其高性能和可复现性。", "conclusion": "研究揭示了非标准化开发的疲劳检测方法的问题，并展示了一种高性能且实现得当的方法。"}}
{"id": "2506.07022", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.07022", "abs": "https://arxiv.org/abs/2506.07022", "authors": ["Leheng Sheng", "Changshuo Shen", "Weixiang Zhao", "Junfeng Fang", "Xiaohao Liu", "Zhenkai Liang", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint", "comment": null, "summary": "As LLMs are increasingly deployed in real-world applications, ensuring their\nability to refuse malicious prompts, especially jailbreak attacks, is essential\nfor safe and reliable use. Recently, activation steering has emerged as an\neffective approach for enhancing LLM safety by adding a refusal direction\nvector to internal activations of LLMs during inference, which will further\ninduce the refusal behaviors of LLMs. However, indiscriminately applying\nactivation steering fundamentally suffers from the trade-off between safety and\nutility, since the same steering vector can also lead to over-refusal and\ndegraded performance on benign prompts. Although prior efforts, such as vector\ncalibration and conditional steering, have attempted to mitigate this\ntrade-off, their lack of theoretical grounding limits their robustness and\neffectiveness. To better address the trade-off between safety and utility, we\npresent a theoretically grounded and empirically effective activation steering\nmethod called AlphaSteer. Specifically, it considers activation steering as a\nlearnable process with two principled learning objectives: utility preservation\nand safety enhancement. For utility preservation, it learns to construct a\nnearly zero vector for steering benign data, with the null-space constraints.\nFor safety enhancement, it learns to construct a refusal direction vector for\nsteering malicious data, with the help of linear regression. Experiments across\nmultiple jailbreak attacks and utility benchmarks demonstrate the effectiveness\nof AlphaSteer, which significantly improves the safety of LLMs without\ncompromising general capabilities. Our codes are available at\nhttps://github.com/AlphaLab-USTC/AlphaSteer.", "AI": {"tldr": "AlphaSteer是一种基于理论的方法，通过优化激活引导向量来平衡LLMs的安全性和实用性，避免过度拒绝良性提示。", "motivation": "随着LLMs在现实应用中的部署增多，确保其能够拒绝恶意提示（如越狱攻击）至关重要，但现有方法在安全性和实用性之间存在权衡问题。", "method": "AlphaSteer将激活引导视为可学习过程，通过两个目标（实用性保留和安全性增强）优化引导向量，利用零空间约束和线性回归。", "result": "实验表明，AlphaSteer显著提升了LLMs的安全性，同时未损害其通用能力。", "conclusion": "AlphaSteer是一种理论支持且实证有效的方法，解决了安全性与实用性之间的权衡问题。"}}
{"id": "2506.07357", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07357", "abs": "https://arxiv.org/abs/2506.07357", "authors": ["Satvik Praveen", "Yoonsung Jung"], "title": "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms", "comment": null, "summary": "Object detection is vital in precision agriculture for plant monitoring,\ndisease detection, and yield estimation. However, models like YOLO struggle\nwith occlusions, irregular structures, and background noise, reducing detection\naccuracy. While Spatial Transformer Networks (STNs) improve spatial invariance\nthrough learned transformations, affine mappings are insufficient for non-rigid\ndeformations such as bent leaves and overlaps.\n  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)\ninto STNs for flexible, non-rigid spatial transformations that better align\nfeatures. Performance is further enhanced by the Convolutional Block Attention\nModule (CBAM), which suppresses background noise and emphasizes relevant\nspatial and channel-wise features.\n  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model\noutperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction\nin false positives, highlighting the benefits of improved spatial flexibility\nand attention-guided refinement. We also examine the impact of the TPS\nregularization parameter in balancing transformation smoothness and detection\nperformance.\n  This lightweight model improves spatial awareness and supports real-time edge\ndeployment, making it ideal for smart farming applications requiring accurate\nand efficient monitoring.", "AI": {"tldr": "论文提出了一种结合TPS和CBAM的改进YOLO模型（CBAM-STN-TPS-YOLO），用于解决农业场景中目标检测的遮挡和非刚性变形问题，显著提升了检测精度。", "motivation": "农业目标检测中，遮挡、不规则结构和背景噪声导致传统模型（如YOLO）性能下降。STN的仿射变换无法处理非刚性变形（如弯曲叶片和重叠）。", "method": "模型结合了TPS（用于非刚性空间变换）和CBAM（用于抑制背景噪声并增强关键特征），改进了STN-YOLO。", "result": "在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，假阳性减少12%。", "conclusion": "该轻量级模型提升了空间感知能力，适合实时边缘部署，适用于精准农业的智能监测。"}}
{"id": "2506.07033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07033", "abs": "https://arxiv.org/abs/2506.07033", "authors": ["Yung-Chien Wang", "Kuang-Da Wang", "Wei-Yao Wang", "Wen-Chih Peng"], "title": "Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression", "comment": "Preprint", "summary": "Tabular data serve as a fundamental and ubiquitous representation of\nstructured information in numerous real-world applications, e.g., finance and\nurban planning. In the realm of tabular imbalanced applications, data imbalance\nhas been investigated in classification tasks with insufficient instances in\ncertain labels, causing the model's ineffective generalizability. However, the\nimbalance issue of tabular regression tasks is underexplored, and yet is\ncritical due to unclear boundaries for continuous labels and simplifying\nassumptions in existing imbalance regression work, which often rely on known\nand balanced test distributions. Such assumptions may not hold in practice and\ncan lead to performance degradation. To address these issues, we propose MATI:\nMixture Experts with Test-Time Self-Supervised Aggregation for Tabular\nImbalance Regression, featuring two key innovations: (i) the Region-Aware\nMixture Expert, which adopts a Gaussian Mixture Model to capture the underlying\nrelated regions. The statistical information of each Gaussian component is then\nused to synthesize and train region-specific experts to capture the unique\ncharacteristics of their respective regions. (ii) Test-Time Self-Supervised\nExpert Aggregation, which dynamically adjusts region expert weights based on\ntest data features to reinforce expert adaptation across varying test\ndistributions. We evaluated MATI on four real-world tabular imbalance\nregression datasets, including house pricing, bike sharing, and age prediction.\nTo reflect realistic deployment scenarios, we adopted three types of test\ndistributions: a balanced distribution with uniform target frequencies, a\nnormal distribution that follows the training data, and an inverse distribution\nthat emphasizes rare target regions. On average across these three test\ndistributions, MATI achieved a 7.1% improvement in MAE compared to existing\nmethods.", "AI": {"tldr": "MATI提出了一种针对表格不平衡回归任务的新方法，通过区域感知混合专家和测试时自监督专家聚合，显著提升了模型性能。", "motivation": "表格数据在现实应用中广泛存在，但回归任务中的数据不平衡问题研究不足，现有方法依赖已知且平衡的测试分布假设，导致性能下降。", "method": "MATI采用高斯混合模型捕捉相关区域，训练区域特定专家，并通过测试时自监督动态调整专家权重以适应不同测试分布。", "result": "在四种真实数据集上，MATI在三种测试分布下平均MAE提升7.1%。", "conclusion": "MATI有效解决了表格不平衡回归问题，显著优于现有方法。"}}
{"id": "2506.07364", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07364", "abs": "https://arxiv.org/abs/2506.07364", "authors": ["Chengchao Shen", "Dawei Liu", "Jianxin Wang"], "title": "Multiple Object Stitching for Unsupervised Representation Learning", "comment": null, "summary": "Contrastive learning for single object centric images has achieved remarkable\nprogress on unsupervised representation, but suffering inferior performance on\nthe widespread images with multiple objects. In this paper, we propose a simple\nbut effective method, Multiple Object Stitching (MOS), to refine the\nunsupervised representation for multi-object images. Specifically, we construct\nthe multi-object images by stitching the single object centric ones, where the\nobjects in the synthesized multi-object images are predetermined. Hence,\ncompared to the existing contrastive methods, our method provides additional\nobject correspondences between multi-object images without human annotations.\nIn this manner, our method pays more attention to the representations of each\nobject in multi-object image, thus providing more detailed representations for\ncomplicated downstream tasks, such as object detection and semantic\nsegmentation. Experimental results on ImageNet, CIFAR and COCO datasets\ndemonstrate that our proposed method achieves the leading unsupervised\nrepresentation performance on both single object centric images and\nmulti-object ones. The source code is available at\nhttps://github.com/visresearch/MultipleObjectStitching.", "AI": {"tldr": "论文提出了一种名为MOS的方法，通过拼接单目标图像生成多目标图像，提升无监督表征在多目标图像上的性能。", "motivation": "现有的对比学习方法在多目标图像上表现不佳，需要改进无监督表征。", "method": "通过拼接单目标图像生成多目标图像，利用预定的对象对应关系优化表征。", "result": "在ImageNet、CIFAR和COCO数据集上，MOS方法在单目标和多目标图像上均取得领先的无监督表征性能。", "conclusion": "MOS方法简单有效，能显著提升多目标图像的无监督表征能力。"}}
{"id": "2506.07040", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07040", "abs": "https://arxiv.org/abs/2506.07040", "authors": ["Yang Xu", "Swetha Ganesh", "Vaneet Aggarwal"], "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning", "comment": "arXiv admin note: text overlap with arXiv:2502.16816", "summary": "We present the first $Q$-learning and actor-critic algorithms for robust\naverage reward Markov Decision Processes (MDPs) with non-asymptotic convergence\nunder contamination, TV distance and Wasserstein distance uncertainty sets. We\nshow that the robust $Q$ Bellman operator is a strict contractive mapping with\nrespect to a carefully constructed semi-norm with constant functions being\nquotiented out. This property supports a stochastic approximation update, that\nlearns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples.\nWe also show that the same idea can be used for robust $Q$ function estimation,\nwhich can be further used for critic estimation. Coupling it with theories in\nrobust policy mirror descent update, we present a natural actor-critic\nalgorithm that attains an $\\epsilon$-optimal robust policy in\n$\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of\ndistributionally robust reinforcement learning in the average reward setting.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07368", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07368", "abs": "https://arxiv.org/abs/2506.07368", "authors": ["Jiaying He", "Yitong Lin", "Jiahe Chen", "Honghui Xu", "Jianwei Zheng"], "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation", "comment": "6 pages, 4 figures, ICME2025", "summary": "For the immanent challenge of insufficiently annotated samples in the medical\nfield, semi-supervised medical image segmentation (SSMIS) offers a promising\nsolution. Despite achieving impressive results in delineating primary target\nareas, most current methodologies struggle to precisely capture the subtle\ndetails of boundaries. This deficiency often leads to significant diagnostic\ninaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised\nsegmentation model that synergistically integrates complementary competition\nand contrastive selection. This design significantly sharpens boundary\ndelineation and enhances overall precision. Specifically, we develop an\n$\\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining\nboundary localization. Additionally, we incorporate a $\\textit{Dynamic\nComplementary Competition}$ module that leverages two high-performing\nsub-networks to generate pseudo-labels, thereby further improving segmentation\nquality. The proposed C3S3 undergoes rigorous validation on two publicly\naccessible datasets, encompassing the practices of both MRI and CT scans. The\nresults demonstrate that our method achieves superior performance compared to\nprevious cutting-edge competitors. Especially, on the 95HD and ASD metrics, our\napproach achieves a notable improvement of at least $6\\%$, highlighting the\nsignificant advancements. The code is available at\nhttps://github.com/Y-TARL/C3S3.", "AI": {"tldr": "论文提出C3S3模型，通过互补竞争和对比选择提升半监督医学图像分割的边界细节捕捉能力。", "motivation": "解决医学图像分割中标注数据不足和边界细节捕捉不精确的问题。", "method": "结合结果驱动的对比学习模块和动态互补竞争模块，优化边界定位和伪标签生成。", "result": "在两个公开数据集上表现优于现有方法，95HD和ASD指标提升至少6%。", "conclusion": "C3S3模型显著提升了半监督医学图像分割的精度和边界细节表现。"}}
{"id": "2506.07049", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.07049", "abs": "https://arxiv.org/abs/2506.07049", "authors": ["Jake Robertson", "Noah Hollmann", "Samuel Müller", "Noor Awad", "Frank Hutter"], "title": "FairPFN: A Tabular Foundation Model for Causal Fairness", "comment": null, "summary": "Machine learning (ML) systems are utilized in critical sectors, such as\nhealthcare, law enforcement, and finance. However, these systems are often\ntrained on historical data that contains demographic biases, leading to ML\ndecisions that perpetuate or exacerbate existing social inequalities. Causal\nfairness provides a transparent, human-in-the-loop framework to mitigate\nalgorithmic discrimination, aligning closely with legal doctrines of direct and\nindirect discrimination. However, current causal fairness frameworks hold a key\nlimitation in that they assume prior knowledge of the correct causal model,\nrestricting their applicability in complex fairness scenarios where causal\nmodels are unknown or difficult to identify. To bridge this gap, we propose\nFairPFN, a tabular foundation model pre-trained on synthetic causal fairness\ndata to identify and mitigate the causal effects of protected attributes in its\npredictions. FairPFN's key contribution is that it requires no knowledge of the\ncausal model and still demonstrates strong performance in identifying and\nremoving protected causal effects across a diverse set of hand-crafted and\nreal-world scenarios relative to robust baseline methods. FairPFN paves the way\nfor promising future research, making causal fairness more accessible to a\nwider variety of complex fairness problems.", "AI": {"tldr": "FairPFN是一种无需因果模型先验知识的表格基础模型，用于识别和减轻受保护属性的因果效应，提升机器学习系统的公平性。", "motivation": "机器学习系统在关键领域（如医疗、执法和金融）中使用，但历史数据中的偏见可能导致不公平决策。因果公平框架需要已知因果模型，限制了其应用。", "method": "提出FairPFN，通过预训练合成因果公平数据的表格基础模型，无需因果模型知识即可识别和减轻受保护属性的因果效应。", "result": "FairPFN在多样化的手工和真实场景中表现优异，优于基线方法。", "conclusion": "FairPFN为复杂公平问题提供了更易用的因果公平解决方案，推动了未来研究。"}}
{"id": "2506.07369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07369", "abs": "https://arxiv.org/abs/2506.07369", "authors": ["Bolin Chen", "Shanzhi Yin", "Goluck Konuko", "Giuseppe Valenzise", "Zihan Zhang", "Shiqi Wang", "Yan Ye"], "title": "Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding", "comment": null, "summary": "The rise of deep generative models has greatly advanced video compression,\nreshaping the paradigm of face video coding through their powerful capability\nfor semantic-aware representation and lifelike synthesis. Generative Face Video\nCoding (GFVC) stands at the forefront of this revolution, which could\ncharacterize complex facial dynamics into compact latent codes for bitstream\ncompactness at the encoder side and leverages powerful deep generative models\nto reconstruct high-fidelity face signal from the compressed latent codes at\nthe decoder side. As such, this well-designed GFVC paradigm could enable\nhigh-fidelity face video communication at ultra-low bitrate ranges, far\nsurpassing the capabilities of the latest Versatile Video Coding (VVC)\nstandard. To pioneer foundational research and accelerate the evolution of\nGFVC, this paper presents the first comprehensive survey of GFVC technologies,\nsystematically bridging critical gaps between theoretical innovation and\nindustrial standardization. In particular, we first review a broad range of\nexisting GFVC methods with different feature representations and optimization\nstrategies, and conduct a thorough benchmarking analysis. In addition, we\nconstruct a large-scale GFVC-compressed face video database with subjective\nMean Opinion Scores (MOSs) based on human perception, aiming to identify the\nmost appropriate quality metrics tailored to GFVC. Moreover, we summarize the\nGFVC standardization potentials with a unified high-level syntax and develop a\nlow-complexity GFVC system which are both expected to push forward future\npractical deployments and applications. Finally, we envision the potential of\nGFVC in industrial applications and deliberate on the current challenges and\nfuture opportunities.", "AI": {"tldr": "GFVC利用深度生成模型实现超低码率下的高保真人脸视频压缩，超越传统VVC标准，并推动标准化与实际应用。", "motivation": "深度生成模型在视频压缩中的潜力激发了GFVC的研究，旨在通过语义感知表示和逼真合成提升人脸视频编码效率。", "method": "综述现有GFVC方法，构建大规模主观评分数据库，提出标准化语法和低复杂度系统。", "result": "GFVC在超低码率下实现高保真压缩，并识别出适合的质量评估指标。", "conclusion": "GFVC在工业应用中有广阔前景，但仍需解决当前挑战以推动未来发展。"}}
{"id": "2506.07054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07054", "abs": "https://arxiv.org/abs/2506.07054", "authors": ["Uri Koren", "Navdeep Kumar", "Uri Gadot", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "title": "Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead", "comment": null, "summary": "Classical policy gradient (PG) methods in reinforcement learning frequently\nconverge to suboptimal local optima, a challenge exacerbated in large or\ncomplex environments. This work investigates Policy Gradient with Tree Search\n(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance\npolicy optimization. We provide theoretical analysis demonstrating that\nincreasing the tree search depth $m$-monotonically reduces the set of\nundesirable stationary points and, consequently, improves the worst-case\nperformance of any resulting stationary policy. Critically, our analysis\naccommodates practical scenarios where policy updates are restricted to states\nvisited by the current policy, rather than requiring updates across the entire\nstate space. Empirical evaluations on diverse MDP structures, including Ladder,\nTightrope, and Gridworld environments, illustrate PGTS's ability to exhibit\n\"farsightedness,\" navigate challenging reward landscapes, escape local traps\nwhere standard PG fails, and achieve superior solutions.", "AI": {"tldr": "PGTS方法通过结合树搜索增强策略优化，减少不良稳定点，提升最差性能表现。", "motivation": "传统策略梯度方法易陷入局部最优，尤其在复杂环境中表现不佳。", "method": "提出PGTS方法，引入m步前瞻机制，理论分析表明增加搜索深度可减少不良稳定点。", "result": "实验证明PGTS能展现‘远见’，逃离局部陷阱，在多种MDP结构中表现优于标准PG。", "conclusion": "PGTS通过树搜索机制显著提升了策略优化的效果，尤其在复杂环境中表现突出。"}}
{"id": "2506.07371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07371", "abs": "https://arxiv.org/abs/2506.07371", "authors": ["Ruchit Rawal", "Reza Shirkavand", "Heng Huang", "Gowthami Somepalli", "Tom Goldstein"], "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs", "comment": "Project page with all the artifacts:\n  https://ruchitrawal.github.io/argus", "summary": "Video large language models have not yet been widely deployed, largely due to\ntheir tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on\nmultiple-choice questions. Unfortunately, VideoLLMs hallucinate far more\naggressively on freeform text generation tasks like video captioning than they\ndo on multiple choice verification tasks. To address this weakness, we propose\nARGUS, a VideoLLM benchmark that measures freeform video captioning\nperformance. By comparing VideoLLM outputs to human ground truth captions,\nARGUS quantifies dual metrics. First, we measure the rate of hallucinations in\nthe form of incorrect statements about video content or temporal relationships.\nSecond, we measure the rate at which the model omits important descriptive\ndetails. Together, these dual metrics form a comprehensive view of video\ncaptioning performance.", "AI": {"tldr": "ARGUS是一个针对VideoLLM的基准测试，通过自由文本生成任务（如视频字幕）评估模型性能，量化幻觉率和遗漏率。", "motivation": "由于VideoLLM在自由文本生成任务中幻觉问题严重，现有基准测试（如多选题）无法全面评估其性能。", "method": "提出ARGUS基准，通过比较模型生成的字幕与人工标注的真实字幕，量化幻觉（错误描述）和遗漏（重要细节缺失）率。", "result": "ARGUS提供了对视频字幕性能的双重评估指标，弥补了现有基准的不足。", "conclusion": "ARGUS为VideoLLM的性能评估提供了更全面的视角，有助于改进模型在自由文本生成任务中的表现。"}}
{"id": "2506.07375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07375", "abs": "https://arxiv.org/abs/2506.07375", "authors": ["Xunjie He", "Christina Dao Wen Lee", "Meiling Wang", "Chengran Yuan", "Zefan Huang", "Yufeng Yue", "Marcelo H. Ang Jr"], "title": "DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models", "comment": null, "summary": "Collaborative perception plays a crucial role in enhancing environmental\nunderstanding by expanding the perceptual range and improving robustness\nagainst sensor failures, which primarily involves collaborative 3D detection\nand tracking tasks. The former focuses on object recognition in individual\nframes, while the latter captures continuous instance tracklets over time.\nHowever, existing works in both areas predominantly focus on the vehicle\nsuperclass, lacking effective solutions for both multi-class collaborative\ndetection and tracking. This limitation hinders their applicability in\nreal-world scenarios, which involve diverse object classes with varying\nappearances and motion patterns. To overcome these limitations, we propose a\nmulti-class collaborative detection and tracking framework tailored for diverse\nroad users. We first present a detector with a global spatial attention fusion\n(GSAF) module, enhancing multi-scale feature learning for objects of varying\nsizes. Next, we introduce a tracklet RE-IDentification (REID) module that\nleverages visual semantics with a vision foundation model to effectively reduce\nID SWitch (IDSW) errors, in cases of erroneous mismatches involving small\nobjects like pedestrians. We further design a velocity-based adaptive tracklet\nmanagement (VATM) module that adjusts the tracking interval dynamically based\non object motion. Extensive experiments on the V2X-Real and OPV2V datasets show\nthat our approach significantly outperforms existing state-of-the-art methods\nin both detection and tracking accuracy.", "AI": {"tldr": "提出了一种多类别协同检测与跟踪框架，通过全局空间注意力融合模块和视觉语义重识别模块，显著提升了检测与跟踪的准确性。", "motivation": "现有研究主要集中在车辆类别，缺乏对多类别对象的有效解决方案，限制了实际应用。", "method": "设计了全局空间注意力融合模块（GSAF）增强多尺度特征学习，引入视觉语义重识别模块（REID）减少ID切换错误，并开发了基于速度的自适应轨迹管理模块（VATM）。", "result": "在V2X-Real和OPV2V数据集上的实验表明，该方法在检测和跟踪精度上显著优于现有方法。", "conclusion": "该框架为多类别道路用户提供了有效的协同感知解决方案，提升了实际场景的适用性。"}}
{"id": "2506.07085", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07085", "abs": "https://arxiv.org/abs/2506.07085", "authors": ["Uri Koren", "Yonatan Ashlag", "Mirco Mutti", "Esther Derman", "Pierre-Luc Bacon", "Shie Mannor"], "title": "State Entropy Regularization for Robust Reinforcement Learning", "comment": null, "summary": "State entropy regularization has empirically shown better exploration and\nsample complexity in reinforcement learning (RL). However, its theoretical\nguarantees have not been studied. In this paper, we show that state entropy\nregularization improves robustness to structured and spatially correlated\nperturbations. These types of variation are common in transfer learning but\noften overlooked by standard robust RL methods, which typically focus on small,\nuncorrelated changes. We provide a comprehensive characterization of these\nrobustness properties, including formal guarantees under reward and transition\nuncertainty, as well as settings where the method performs poorly. Much of our\nanalysis contrasts state entropy with the widely used policy entropy\nregularization, highlighting their different benefits. Finally, from a\npractical standpoint, we illustrate that compared with policy entropy, the\nrobustness advantages of state entropy are more sensitive to the number of\nrollouts used for policy evaluation.", "AI": {"tldr": "本文研究了状态熵正则化在强化学习中的鲁棒性优势，特别是在面对结构化扰动时的表现，并与策略熵正则化进行了对比。", "motivation": "状态熵正则化在强化学习中表现出更好的探索性和样本效率，但其理论保证尚未深入研究。本文旨在填补这一空白。", "method": "通过理论分析和实验验证，研究了状态熵正则化在奖励和转移不确定性下的鲁棒性，并与策略熵正则化进行对比。", "result": "状态熵正则化对结构化扰动具有更强的鲁棒性，但其优势对策略评估的采样次数更敏感。", "conclusion": "状态熵正则化在特定场景下优于策略熵正则化，但实际应用中需注意采样效率的影响。"}}
{"id": "2506.07376", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07376", "abs": "https://arxiv.org/abs/2506.07376", "authors": ["Jintao Tong", "Ran Ma", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation", "comment": "ICML 2025 Spotlight", "summary": "Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the\nmodel on a source-domain dataset with sufficient samples, and then transfer the\nmodel to target-domain datasets where only a few samples are available for\nefficient fine-tuning. There are majorly two challenges in this task: (1) the\ndomain gap and (2) fine-tuning with scarce data. To solve these challenges, we\nrevisit the adapter-based methods, and discover an intriguing insight not\nexplored in previous works: the adapter not only helps the fine-tuning of\ndownstream tasks but also naturally serves as a domain information decoupler.\nThen, we delve into this finding for an interpretation, and find the model's\ninherent structure could lead to a natural decoupling of domain information.\nBuilding upon this insight, we propose the Domain Feature Navigator (DFN),\nwhich is a structure-based decoupler instead of loss-based ones like current\nworks, to capture domain-specific information, thereby directing the model's\nattention towards domain-agnostic knowledge. Moreover, to prevent the potential\nexcessive overfitting of DFN during the source-domain training, we further\ndesign the SAM-SVN method to constrain DFN from learning sample-specific\nknowledge. On target domains, we freeze the model and fine-tune the DFN to\nlearn target-specific knowledge specific. Extensive experiments demonstrate\nthat our method surpasses the state-of-the-art method in CD-FSS significantly\nby 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.", "AI": {"tldr": "论文提出了一种基于适配器的域特征导航器（DFN）方法，用于跨域少样本分割任务，显著提升了性能。", "motivation": "解决跨域少样本分割中的域差距和数据稀缺问题。", "method": "提出DFN作为结构化解耦器，并结合SAM-SVN方法防止过拟合。", "result": "在1-shot和5-shot场景下，性能分别提升2.69%和4.68% MIoU。", "conclusion": "DFN能有效解耦域信息，提升跨域少样本分割性能。"}}
{"id": "2506.07088", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07088", "abs": "https://arxiv.org/abs/2506.07088", "authors": ["Ilja Kuzborskij", "Yasin Abbasi Yadkori"], "title": "Pointwise confidence estimation in the non-linear $\\ell^2$-regularized least squares", "comment": null, "summary": "We consider a high-probability non-asymptotic confidence estimation in the\n$\\ell^2$-regularized non-linear least-squares setting with fixed design. In\nparticular, we study confidence estimation for local minimizers of the\nregularized training loss. We show a pointwise confidence bound, meaning that\nit holds for the prediction on any given fixed test input $x$. Importantly, the\nproposed confidence bound scales with similarity of the test input to the\ntraining data in the implicit feature space of the predictor (for instance,\nbecoming very large when the test input lies far outside of the training data).\nThis desirable last feature is captured by the weighted norm involving the\ninverse-Hessian matrix of the objective function, which is a generalized\nversion of its counterpart in the linear setting, $x^{\\top} \\text{Cov}^{-1} x$.\nOur generalized result can be regarded as a non-asymptotic counterpart of the\nclassical confidence interval based on asymptotic normality of the MLE\nestimator. We propose an efficient method for computing the weighted norm,\nwhich only mildly exceeds the cost of a gradient computation of the loss\nfunction. Finally, we complement our analysis with empirical evidence showing\nthat the proposed confidence bound provides better coverage/width trade-off\ncompared to a confidence estimation by bootstrapping, which is a gold-standard\nmethod in many applications involving non-linear predictors such as neural\nnetworks.", "AI": {"tldr": "论文提出了一种在高概率非渐近情况下，针对固定设计的ℓ²正则化非线性最小二乘问题的置信度估计方法，特别关注局部最小化器的置信区间。", "motivation": "研究动机在于为非线性最小二乘问题的预测提供点式置信区间，并确保其随测试数据与训练数据的相似性动态调整。", "method": "方法基于隐式特征空间中测试数据与训练数据的相似性，通过加权范数（涉及目标函数的逆Hessian矩阵）构建置信区间。", "result": "结果表明，所提置信区间在覆盖率和宽度上优于传统的自助法，且计算效率接近梯度计算成本。", "conclusion": "结论指出，该方法为非渐近情况下MLE估计的经典置信区间提供了有效替代，适用于非线性预测器如神经网络。"}}
{"id": "2506.07399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07399", "abs": "https://arxiv.org/abs/2506.07399", "authors": ["Peiru Yang", "Jinhua Yin", "Haoran Zheng", "Xueying Bai", "Huili Wang", "Yufei Sun", "Xintian Li", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems", "comment": null, "summary": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses.", "AI": {"tldr": "本文提出了MrM，首个针对多模态RAG系统的黑盒成员推理攻击框架，通过多目标数据扰动和反事实攻击提升攻击效果。", "motivation": "多模态RAG系统可能泄露敏感信息，现有成员推理攻击方法主要针对文本模态，视觉模态研究不足。", "method": "提出多目标数据扰动框架和反事实掩码选择策略，通过统计建模提取成员信息。", "result": "在两个视觉数据集和八个主流视觉语言模型上验证了MrM的高效性和鲁棒性。", "conclusion": "MrM填补了视觉模态成员推理攻击的研究空白，为多模态RAG系统的隐私保护提供了新视角。"}}
{"id": "2506.07092", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07092", "abs": "https://arxiv.org/abs/2506.07092", "authors": ["Joydeb Kumar Sana", "Mohammad M. Masud", "M Sohel Rahman", "M Saifur Rahman"], "title": "Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data", "comment": "This paper presents a novel distributed patient similarity\n  computation (DPSC) technique based on data transformation (DT) methods,\n  utilizing an effective combination of time series and static data", "summary": "Patient similarity computation (PSC) is a fundamental problem in healthcare\ninformatics. The aim of the patient similarity computation is to measure the\nsimilarity among patients according to their historical clinical records, which\nhelps to improve clinical decision support. This paper presents a novel\ndistributed patient similarity computation (DPSC) technique based on data\ntransformation (DT) methods, utilizing an effective combination of time series\nand static data. Time series data are sensor-collected patients' information,\nincluding metrics like heart rate, blood pressure, Oxygen saturation,\nrespiration, etc. The static data are mainly patient background and demographic\ndata, including age, weight, height, gender, etc. Static data has been used for\nclustering the patients. Before feeding the static data to the machine learning\nmodel adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)\nmethods have been performed, which improve the prediction performances. In\naWOE-based patient similarity models, sensitive patient information has been\nprocessed using aWOE which preserves the data privacy of the trained models. We\nused the Dynamic Time Warping (DTW) approach, which is robust and very popular,\nfor time series similarity. However, DTW is not suitable for big data due to\nthe significant computational run-time. To overcome this problem, distributed\nDTW computation is used in this study. For Coronary Artery Disease, our DT\nbased approach boosts prediction performance by as much as 11.4%, 10.20%, and\n12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of\nCongestive Heart Failure (CHF), our proposed method achieves performance\nenhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.\nThe proposed method reduces the computation time by as high as 40%.", "AI": {"tldr": "本文提出了一种基于数据转换的分布式患者相似性计算（DPSC）技术，结合时间序列和静态数据，显著提升了预测性能和计算效率。", "motivation": "患者相似性计算（PSC）是医疗信息学中的核心问题，旨在通过患者历史临床记录衡量相似性，以支持临床决策。", "method": "采用数据转换方法（aWOE和Z-score）处理静态数据，并结合动态时间规整（DTW）进行时间序列相似性计算，同时通过分布式计算优化DTW的运行时间。", "result": "在冠状动脉疾病和充血性心力衰竭的预测中，AUC、准确率和F值分别提升高达11.4%、10.20%、12.6%和15.9%、10.5%、21.9%，计算时间减少40%。", "conclusion": "提出的DPSC方法在预测性能和计算效率上均表现出显著优势，为医疗决策提供了更高效的支持。"}}
{"id": "2506.07412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07412", "abs": "https://arxiv.org/abs/2506.07412", "authors": ["Changsheng Gao", "Wei Zhou", "Guosheng Lin", "Weisi Lin"], "title": "Compressed Feature Quality Assessment: Dataset and Baselines", "comment": null, "summary": "The widespread deployment of large models in resource-constrained\nenvironments has underscored the need for efficient transmission of\nintermediate feature representations. In this context, feature coding, which\ncompresses features into compact bitstreams, becomes a critical component for\nscenarios involving feature transmission, storage, and reuse. However, this\ncompression process introduces inherent semantic degradation that is\nnotoriously difficult to quantify with traditional metrics. To address this,\nthis paper introduces the research problem of Compressed Feature Quality\nAssessment (CFQA), which seeks to evaluate the semantic fidelity of compressed\nfeatures. To advance CFQA research, we propose the first benchmark dataset,\ncomprising 300 original features and 12000 compressed features derived from\nthree vision tasks and four feature codecs. Task-specific performance drops are\nprovided as true semantic distortion for the evaluation of CFQA metrics. We\nassess the performance of three widely used metrics (MSE, cosine similarity,\nand Centered Kernel Alignment) in capturing semantic degradation. The results\nunderscore the representativeness of the dataset and highlight the need for\nmore refined metrics capable of addressing the nuances of semantic distortion\nin compressed features. To facilitate the ongoing development of CFQA research,\nwe release the dataset and all accompanying source code at\n\\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.\nThis contribution aims to advance the field and provide a foundational resource\nfor the community to explore CFQA.", "AI": {"tldr": "论文提出了压缩特征质量评估（CFQA）问题，并创建了一个包含原始和压缩特征的基准数据集，评估了三种常用指标的性能，结果强调了数据集代表性和改进指标的必要性。", "motivation": "大规模模型在资源受限环境中的广泛部署需要高效传输中间特征表示，而特征压缩过程中的语义退化难以量化，因此需要研究CFQA。", "method": "提出了第一个CFQA基准数据集，包含300个原始特征和12000个压缩特征，评估了MSE、余弦相似度和中心核对齐三种指标的性能。", "result": "数据集具有代表性，但现有指标难以捕捉语义退化的细微差别，需要更精细的指标。", "conclusion": "论文为CFQA研究提供了基础资源，推动了该领域的发展。"}}
{"id": "2506.07099", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07099", "abs": "https://arxiv.org/abs/2506.07099", "authors": ["Wenying He", "Jieling Huang", "Junhua Gu", "Ji Zhang", "Yude Bai"], "title": "Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion", "comment": "9 pages,3 figures", "summary": "Missing data in spatiotemporal systems presents a significant challenge for\nmodern applications, ranging from environmental monitoring to urban traffic\nmanagement. The integrity of spatiotemporal data often deteriorates due to\nhardware malfunctions and software failures in real-world deployments. Current\napproaches based on machine learning and deep learning struggle to model the\nintricate interdependencies between spatial and temporal dimensions effectively\nand, more importantly, suffer from cumulative errors during the data imputation\nprocess, which propagate and amplify through iterations. To address these\nlimitations, we propose CoFILL, a novel Conditional Diffusion Model for\nspatiotemporal data imputation. CoFILL builds on the inherent advantages of\ndiffusion models to generate high-quality imputations without relying on\npotentially error-prone prior estimates. It incorporates an innovative\ndual-stream architecture that processes temporal and frequency domain features\nin parallel. By fusing these complementary features, CoFILL captures both rapid\nfluctuations and underlying patterns in the data, which enables more robust\nimputation. The extensive experiments reveal that CoFILL's noise prediction\nnetwork successfully transforms random noise into meaningful values that align\nwith the true data distribution. The results also show that CoFILL outperforms\nstate-of-the-art methods in imputation accuracy. The source code is publicly\navailable at https://github.com/joyHJL/CoFILL.", "AI": {"tldr": "CoFILL是一种基于条件扩散模型的新型时空数据填补方法，通过双流架构并行处理时空特征，显著提升了填补精度。", "motivation": "现有机器学习和深度学习方法在时空数据填补中存在累积误差和难以建模时空依赖的问题。", "method": "提出CoFILL，利用扩散模型生成高质量填补值，结合双流架构并行处理时空和频域特征。", "result": "实验表明CoFILL在填补精度上优于现有方法，并能有效捕捉数据中的快速波动和潜在模式。", "conclusion": "CoFILL为时空数据填补提供了一种高效且准确的解决方案。"}}
{"id": "2506.07414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07414", "abs": "https://arxiv.org/abs/2506.07414", "authors": ["Sheng-Kai Huang", "Jiun-Feng Chang", "Chun-Rong Huang"], "title": "DPFormer: Dynamic Prompt Transformer for Continual Learning", "comment": null, "summary": "In continual learning, solving the catastrophic forgetting problem may make\nthe models fall into the stability-plasticity dilemma. Moreover, inter-task\nconfusion will also occur due to the lack of knowledge exchanges between\ndifferent tasks. In order to solve the aforementioned problems, we propose a\nnovel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt\nschemes help the DPFormer memorize learned knowledge of previous classes and\ntasks, and keep on learning new knowledge from new classes and tasks under a\nsingle network structure with a nearly fixed number of model parameters.\nMoreover, they also provide discrepant information to represent different tasks\nto solve the inter-task confusion problem. Based on prompt schemes, a unified\nclassification module with the binary cross entropy loss, the knowledge\ndistillation loss and the auxiliary loss is proposed to train the whole model\nin an end-to-end trainable manner. Compared with state-of-the-art methods, our\nmethod achieves the best performance in the CIFAR-100, ImageNet100 and\nImageNet1K datasets under different class-incremental settings in continual\nlearning. The source code will be available at our GitHub after acceptance.", "AI": {"tldr": "论文提出了一种动态提示变换器（DPFormer），通过提示方案解决持续学习中的灾难性遗忘和任务间混淆问题，并在多个数据集上表现优异。", "motivation": "解决持续学习中的灾难性遗忘和稳定性-可塑性困境，以及任务间知识交换不足导致的混淆问题。", "method": "提出动态提示变换器（DPFormer）和提示方案，结合二进制交叉熵损失、知识蒸馏损失和辅助损失，以端到端方式训练模型。", "result": "在CIFAR-100、ImageNet100和ImageNet1K数据集上，在不同类增量设置下表现优于现有方法。", "conclusion": "DPFormer通过提示方案有效解决了持续学习中的关键问题，并在多个数据集上验证了其优越性。"}}
{"id": "2506.07431", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07431", "abs": "https://arxiv.org/abs/2506.07431", "authors": ["Jie He", "Minglang Chen", "Minying Lu", "Bocheng Liang", "Junming Wei", "Guiyan Peng", "Jiaxi Chen", "Ying Tan"], "title": "FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement", "comment": null, "summary": "Accurate ultrasound image segmentation is a prerequisite for precise\nbiometrics and accurate assessment. Relying on manual delineation introduces\nsignificant errors and is time-consuming. However, existing segmentation models\nare designed based on objects in natural scenes, making them difficult to adapt\nto ultrasound objects with high noise and high similarity. This is particularly\nevident in small object segmentation, where a pronounced jagged effect occurs.\nTherefore, this paper proposes a fetal femur and cranial ultrasound image\nsegmentation model based on feature perception and Mamba enhancement to address\nthese challenges. Specifically, a longitudinal and transverse independent\nviewpoint scanning convolution block and a feature perception module were\ndesigned to enhance the ability to capture local detail information and improve\nthe fusion of contextual information. Combined with the Mamba-optimized\nresidual structure, this design suppresses the interference of raw noise and\nenhances local multi-dimensional scanning. The system builds global information\nand local feature dependencies, and is trained with a combination of different\noptimizers to achieve the optimal solution. After extensive experimental\nvalidation, the FAMSeg network achieved the fastest loss reduction and the best\nsegmentation performance across images of varying sizes and orientations.", "AI": {"tldr": "提出了一种基于特征感知和Mamba增强的胎儿股骨和颅骨超声图像分割模型，解决了高噪声和高相似性超声图像分割的难题。", "motivation": "超声图像分割对精确生物测量和评估至关重要，但现有模型难以适应高噪声和高相似性的超声对象，尤其是小物体分割时锯齿效应明显。", "method": "设计了纵向和横向独立视角扫描卷积块和特征感知模块，结合Mamba优化的残差结构，抑制原始噪声干扰并增强局部多维扫描。", "result": "FAMSeg网络在实验中实现了最快的损失减少和最佳的分割性能，适用于不同大小和方向的图像。", "conclusion": "该模型有效提升了超声图像分割的精度和效率，尤其适用于复杂场景下的胎儿股骨和颅骨分割。"}}
{"id": "2506.07436", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.07436", "abs": "https://arxiv.org/abs/2506.07436", "authors": ["Nishi Chaudhary", "S M Jamil Uddin", "Sathvik Sharath Chandra", "Anto Ovid", "Alex Albert"], "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition", "comment": null, "summary": "The recent emergence of multimodal large language models (LLMs) has\nintroduced new opportunities for improving visual hazard recognition on\nconstruction sites. Unlike traditional computer vision models that rely on\ndomain-specific training and extensive datasets, modern LLMs can interpret and\ndescribe complex visual scenes using simple natural language prompts. However,\ndespite growing interest in their applications, there has been limited\ninvestigation into how different LLMs perform in safety-critical visual tasks\nwithin the construction domain. To address this gap, this study conducts a\ncomparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,\nGPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify\npotential hazards from real-world construction images. Each model was tested\nunder three prompting strategies: zero-shot, few-shot, and chain-of-thought\n(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated\nbasic safety context and a hazard source mnemonic, and CoT provided\nstep-by-step reasoning examples to scaffold model thinking. Quantitative\nanalysis was performed using precision, recall, and F1-score metrics across all\nconditions. Results reveal that prompting strategy significantly influenced\nperformance, with CoT prompting consistently producing higher accuracy across\nmodels. Additionally, LLM performance varied under different conditions, with\nGPT-4.5 and GPT-o3 outperforming others in most settings. The findings also\ndemonstrate the critical role of prompt design in enhancing the accuracy and\nconsistency of multimodal LLMs for construction safety applications. This study\noffers actionable insights into the integration of prompt engineering and LLMs\nfor practical hazard recognition, contributing to the development of more\nreliable AI-assisted safety systems.", "AI": {"tldr": "该研究比较了五种多模态大语言模型（LLMs）在建筑工地视觉危险识别中的表现，发现提示策略（如零样本、少样本和思维链）显著影响性能，其中思维链提示效果最佳。GPT-4.5和GPT-o3表现最优。", "motivation": "探索多模态LLMs在建筑安全领域的应用潜力，填补其在关键视觉任务中性能评估的研究空白。", "method": "对五种LLMs（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro）进行对比评估，采用三种提示策略（零样本、少样本和思维链），并使用精确率、召回率和F1分数进行定量分析。", "result": "思维链提示显著提升模型准确性，GPT-4.5和GPT-o3在多数场景中表现最佳。提示设计对多模态LLMs的性能至关重要。", "conclusion": "研究为建筑安全领域的多模态LLMs应用提供了实用见解，强调了提示工程在提升AI辅助安全系统可靠性中的重要性。"}}
{"id": "2506.07134", "categories": ["cs.LG", "cs.AI", "math.OC", "90C40 (Primary), 93E20, 68T05 (Secondary)", "I.2.6; G.3"], "pdf": "https://arxiv.org/pdf/2506.07134", "abs": "https://arxiv.org/abs/2506.07134", "authors": ["Eshwar S. R.", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning", "comment": "19 pages", "summary": "Despite decades of research, it remains challenging to correctly use\nReinforcement Learning (RL) algorithms with function approximation. A prime\nexample is policy iteration, whose fundamental guarantee of monotonic\nimprovement collapses even under linear function approximation. To address this\nissue, we introduce Reliable Policy Iteration (RPI). It replaces the common\nprojection or Bellman-error minimization during policy evaluation with a\nBellman-based constrained optimization. We prove that not only does RPI confer\ntextbook monotonicity on its value estimates but these estimates also lower\nbound the true return. Also, their limit partially satisfies the unprojected\nBellman equation, emphasizing RPI's natural fit within RL. RPI is the first\nalgorithm with such monotonicity and convergence guarantees under function\napproximation. For practical use, we provide a model-free variant of RPI that\namounts to a novel critic. It can be readily integrated into primary model-free\nPI implementations such as DQN and DDPG. In classical control tasks, such\nRPI-enhanced variants consistently maintain their lower-bound guarantee while\nmatching or surpassing the performance of all baseline methods.", "AI": {"tldr": "论文提出了Reliable Policy Iteration (RPI)算法，解决了传统强化学习中函数逼近下策略迭代单调性失效的问题，并提供了理论保证和实际应用。", "motivation": "传统强化学习在函数逼近下策略迭代的单调性保证失效，导致算法性能不稳定。", "method": "RPI通过基于Bellman的约束优化替代传统策略评估中的投影或Bellman误差最小化，确保单调性和收敛性。", "result": "RPI在经典控制任务中保持下界保证，性能优于或匹配基线方法。", "conclusion": "RPI是首个在函数逼近下具有单调性和收敛性保证的算法，适用于主流模型无关强化学习方法。"}}
{"id": "2506.07456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07456", "abs": "https://arxiv.org/abs/2506.07456", "authors": ["Wei Yao", "Yunlian Sun", "Chang Liu", "Hongwen Zhang", "Jinhui Tang"], "title": "PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation", "comment": null, "summary": "Driven by advancements in motion capture and generative artificial\nintelligence, leveraging large-scale MoCap datasets to train generative models\nfor synthesizing diverse, realistic human motions has become a promising\nresearch direction. However, existing motion-capture techniques and generative\nmodels often neglect physical constraints, leading to artifacts such as\ninterpenetration, sliding, and floating. These issues are exacerbated in\nmulti-person motion generation, where complex interactions are involved. To\naddress these limitations, we introduce physical mapping, integrated throughout\nthe human interaction generation pipeline. Specifically, motion imitation\nwithin a physics-based simulation environment is used to project target motions\ninto a physically valid space. The resulting motions are adjusted to adhere to\nreal-world physics constraints while retaining their original semantic meaning.\nThis mapping not only improves MoCap data quality but also directly informs\npost-processing of generated motions. Given the unique interactivity of\nmulti-person scenarios, we propose a tailored motion representation framework.\nMotion Consistency (MC) and Marker-based Interaction (MI) loss functions are\nintroduced to improve model performance. Experiments show our method achieves\nimpressive results in generated human motion quality, with a 3%-89% improvement\nin physical fidelity. Project page http://yw0208.github.io/physiinter", "AI": {"tldr": "提出了一种结合物理映射的运动生成方法，解决了多人生成运动中物理约束不足的问题，显著提升了运动质量和物理保真度。", "motivation": "现有运动捕捉技术和生成模型常忽略物理约束，导致运动中出现穿模、滑动等问题，尤其在多人交互中更为严重。", "method": "引入物理映射，通过物理模拟环境中的运动模仿将目标运动投影到物理有效空间，同时提出MC和MI损失函数优化模型。", "result": "实验显示，该方法在运动质量上表现优异，物理保真度提升了3%-89%。", "conclusion": "通过物理映射和定制化的运动表示框架，有效提升了多人运动生成的物理真实性和语义一致性。"}}
{"id": "2506.07165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07165", "abs": "https://arxiv.org/abs/2506.07165", "authors": ["Qi Liu", "Jingqing Ruan", "Hao Li", "Haodong Zhao", "Desheng Wang", "Jiansong Chen", "Wan Guanglu", "Xunliang Cai", "Zhi Zheng", "Tong Xu"], "title": "AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models", "comment": "Accepted by ACL 2025", "summary": "Existing multi-objective preference alignment methods for large language\nmodels (LLMs) face limitations: (1) the inability to effectively balance\nvarious preference dimensions, and (2) reliance on auxiliary reward/reference\nmodels introduces computational complexity. To address these challenges, we\npropose Adaptive Multi-objective Preference Optimization (AMoPO), a novel\nframework that achieves dynamic balance across preference dimensions. By\nintroducing the multi-objective optimization paradigm to use the\ndimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with\ndiverse preferences without additional reward models or reference models. We\nintroduce an adaptive weight assignment mechanism that models the generation\nspace as a Gaussian distribution, allowing dynamic prioritization of preference\ndimensions. Empirical results demonstrate that AMoPO outperforms\nstate-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B\nmodels reveal the scaling ability of AMoPO. Moreover, additional analysis of\nmultiple dimensions verifies its adaptability and effectiveness. These findings\nvalidate AMoPO's capability to achieve dimension-aware preference alignment,\nhighlighting its superiority. Our codes and datasets are available at\nhttps://github.com/Javkonline/AMoPO.", "AI": {"tldr": "AMoPO是一种新型框架，通过动态平衡多目标偏好优化，无需额外奖励模型，显著优于现有方法。", "motivation": "现有多目标偏好对齐方法无法有效平衡不同偏好维度，且依赖辅助模型增加计算复杂度。", "method": "引入多目标优化范式，利用维度感知生成指标作为隐式奖励，通过自适应权重分配机制动态调整偏好优先级。", "result": "AMoPO在7B、14B和32B模型上表现优异，优于基线方法28.5%，验证了其适应性和扩展性。", "conclusion": "AMoPO能够实现维度感知的偏好对齐，展现出显著优势，代码和数据集已开源。"}}
{"id": "2506.07460", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07460", "abs": "https://arxiv.org/abs/2506.07460", "authors": ["Taeryung Lee", "Hyeongjin Nam", "Gyeongsik Moon", "Kyoung Mu Lee"], "title": "GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning", "comment": null, "summary": "Sign language generation (SLG), or text-to-sign generation, bridges the gap\nbetween signers and non-signers. Despite recent progress in SLG, existing\nmethods still often suffer from incorrect lexical ordering and low semantic\naccuracy. This is primarily due to sentence-level condition, which encodes the\nentire sentence of the input text into a single feature vector as a condition\nfor SLG. This approach fails to capture the temporal structure of sign language\nand lacks the granularity of word-level semantics, often leading to disordered\nsign sequences and ambiguous motions. To overcome these limitations, we propose\nGLOS, a sign language generation framework with temporally aligned gloss-level\nconditioning. First, we employ gloss-level conditions, which we define as\nsequences of gloss embeddings temporally aligned with the motion sequence. This\nenables the model to access both the temporal structure of sign language and\nword-level semantics at each timestep. As a result, this allows for\nfine-grained control of signs and better preservation of lexical order. Second,\nwe introduce a condition fusion module, temporal alignment conditioning (TAC),\nto efficiently deliver the word-level semantic and temporal structure provided\nby the gloss-level condition to the corresponding motion timesteps. Our method,\nwhich is composed of gloss-level conditions and TAC, generates signs with\ncorrect lexical order and high semantic accuracy, outperforming prior methods\non CSL-Daily and Phoenix-2014T.", "AI": {"tldr": "论文提出GLOS框架，通过时间对齐的gloss级条件改进手语生成，解决了现有方法中词汇顺序错误和语义准确性低的问题。", "motivation": "现有手语生成方法因句子级条件无法捕捉时间结构和单词级语义，导致词汇顺序混乱和动作模糊。", "method": "采用gloss级条件（时间对齐的gloss嵌入序列）和条件融合模块TAC，实现精细控制和词汇顺序保持。", "result": "在CSL-Daily和Phoenix-2014T数据集上表现优于现有方法，生成的手语词汇顺序正确且语义准确。", "conclusion": "GLOS框架通过时间对齐的gloss级条件显著提升了手语生成的质量。"}}
{"id": "2506.07168", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07168", "abs": "https://arxiv.org/abs/2506.07168", "authors": ["Huanyi Xie", "Lijie Hu", "Lu Yu", "Tianhao Huang", "Longfei Li", "Meng Li", "Jun Zhou", "Huan Wang", "Di Wang"], "title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment", "comment": "23 pages", "summary": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural\nnetworks (GNNs) often fall short due to the complex textual information\nassociated with each node. Recent methods have improved node representations by\nleveraging large language models (LLMs) to enhance node text features, but\nthese approaches typically require extensive annotations or fine-tuning across\nall nodes, which is both time-consuming and costly. To overcome these\nchallenges, we introduce GAGA, an efficient framework for TAG representation\nlearning. GAGA reduces annotation time and cost by focusing on annotating only\nrepresentative nodes and edges. It constructs an annotation graph that captures\nthe topological relationships among these annotations. Furthermore, GAGA\nemploys a two-level alignment module to effectively integrate the annotation\ngraph with the TAG, aligning their underlying structures. Experiments show that\nGAGA achieves classification accuracies on par with or surpassing\nstate-of-the-art methods while requiring only 1% of the data to be annotated,\ndemonstrating its high efficiency.", "AI": {"tldr": "GAGA是一种高效的TAG表示学习框架，通过仅标注代表性节点和边来减少时间和成本，并通过两级对齐模块整合标注图与TAG。", "motivation": "传统GNN在TAG中因复杂文本信息表现不佳，现有方法需要大量标注或微调，耗时且昂贵。", "method": "GAGA通过标注代表性节点和边构建标注图，并采用两级对齐模块整合标注图与TAG。", "result": "实验表明，GAGA仅需1%的标注数据即可达到或超越现有方法分类精度。", "conclusion": "GAGA是一种高效且经济的TAG表示学习解决方案。"}}
{"id": "2506.07464", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07464", "abs": "https://arxiv.org/abs/2506.07464", "authors": ["Jinyoung Park", "Jeehye Na", "Jinyoung Kim", "Hyunwoo J. Kim"], "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO", "comment": "Work in progress", "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.", "AI": {"tldr": "论文探讨了GRPO在视频大语言模型中的应用问题，提出了Reg-GRPO和难度感知数据增强策略，显著提升了视频推理性能。", "motivation": "GRPO在视频大语言模型中的应用研究较少，且存在依赖安全措施和优势消失问题，需改进。", "method": "提出Reg-GRPO（将GRPO目标重构为回归任务）和难度感知数据增强策略。", "result": "DeepVideo-R1在多个视频推理基准测试中表现显著提升。", "conclusion": "Reg-GRPO和难度感知数据增强策略有效解决了GRPO在视频大语言模型中的学习问题。"}}
{"id": "2506.07179", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07179", "abs": "https://arxiv.org/abs/2506.07179", "authors": ["Kaiqi Wu", "Weiyang Kong", "Sen Zhang", "Yubao Liu", "Zitong Chen"], "title": "Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting", "comment": null, "summary": "Traffic prediction is a critical task in spatial-temporal forecasting with\nbroad applications in travel planning and urban management. Adaptive graph\nconvolution networks have emerged as mainstream solutions due to their ability\nto learn node embeddings in a data-driven manner and capture complex latent\ndependencies. However, existing adaptive graph learning methods for traffic\nforecasting often either ignore the regularization of node embeddings, which\naccount for a significant proportion of model parameters, or face scalability\nissues from expensive graph convolution operations. To address these\nchallenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.\nFirst, we introduce a regularized adaptive graph learning framework that\nsynergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via\na residual difference mechanism, achieving both embedding regularization and\nnoise suppression. Second, to ensure scalability on large road networks, we\ndevelop the Efficient Cosine Operator (ECO), which performs graph convolution\nbased on the cosine similarity of regularized embeddings with linear time\ncomplexity. Extensive experiments on four large-scale real-world traffic\ndatasets show that RAGL consistently outperforms state-of-the-art methods in\nterms of prediction accuracy and exhibits competitive computational efficiency.", "AI": {"tldr": "提出了一种名为RAGL的模型，通过正则化自适应图学习和高效余弦算子解决交通预测中的嵌入正则化和可扩展性问题。", "motivation": "现有自适应图学习方法在交通预测中常忽略节点嵌入的正则化或面临计算复杂度高的问题。", "method": "结合随机共享嵌入和自适应图卷积，提出正则化自适应图学习框架，并开发高效余弦算子以实现线性时间复杂度。", "result": "在四个大规模真实交通数据集上，RAGL在预测精度和计算效率上均优于现有方法。", "conclusion": "RAGL模型在交通预测任务中表现出色，解决了嵌入正则化和可扩展性问题。"}}
{"id": "2506.07471", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07471", "abs": "https://arxiv.org/abs/2506.07471", "authors": ["CH Cho", "WJ Moon", "W Jun", "MS Jung", "JP Heo"], "title": "Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval", "comment": "Accepted to AAAI 2025", "summary": "Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a\nspecific segment is relevant to a given text query. Typical training processes\nof PRVR assume a one-to-one relationship where each text query is relevant to\nonly one video. However, we point out the inherent ambiguity between text and\nvideo content based on their conceptual scope and propose a framework that\nincorporates this ambiguity into the model learning process. Specifically, we\npropose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous\ntext-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:\nuncertainty and similarity. Uncertainty represents whether instances include\ncommonly shared context across the dataset, while similarity indicates\npair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL\nhierarchically learns the semantic relationship via multi-positive contrastive\nlearning and dual triplet margin loss. Additionally, we delve into fine-grained\nrelationships within the video instances. Unlike typical training at the\ntext-video level, where pairwise information is provided, we address the\ninherent ambiguity within frames of the same untrimmed video, which often\ncontains multiple contexts. This allows us to further enhance learning at the\ntext-frame level. Lastly, we propose cross-model ambiguity detection to\nmitigate the error propagation that occurs when a single model is employed to\ndetect ambiguous pairs for its training. With all components combined, our\nproposed method demonstrates its effectiveness in PRVR.", "AI": {"tldr": "论文提出了一种处理文本与视频内容模糊性的框架（ARL），通过多正对比学习和双重三元组损失优化部分相关视频检索（PRVR）。", "motivation": "传统PRVR训练假设文本与视频一对一相关，忽略了文本与视频内容的模糊性，导致模型性能受限。", "method": "提出ARL框架，基于不确定性和相似性检测模糊对，通过多正对比学习和双重三元组损失学习语义关系，并探索视频内细粒度关系。", "result": "ARL框架在PRVR任务中表现出色，有效解决了文本与视频的模糊性问题。", "conclusion": "ARL通过检测和学习模糊性，提升了PRVR的性能，为多模态检索提供了新思路。"}}
{"id": "2506.07185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07185", "abs": "https://arxiv.org/abs/2506.07185", "authors": ["J. C. Husillos", "A. Gallego", "A. Roma", "A. Troncoso"], "title": "Learning based on neurovectors for tabular data: a new neural network approach", "comment": "Submitted to 25th IEEE International Conference on Data Mining (ICDM\n  2025)", "summary": "In this paper, we present a novel learning approach based on Neurovectors, an\ninnovative paradigm that structures information through interconnected nodes\nand vector relationships for tabular data processing. Unlike traditional\nartificial neural networks that rely on weight adjustment through\nbackpropagation, Neurovectors encode information by structuring data in vector\nspaces where energy propagation, rather than traditional weight updates, drives\nthe learning process, enabling a more adaptable and explainable learning\nprocess. Our method generates dynamic representations of knowledge through\nneurovectors, thereby improving both the interpretability and efficiency of the\npredictive model. Experimental results using datasets from well-established\nrepositories such as the UCI machine learning repository and Kaggle are\nreported both for classification and regression. To evaluate its performance,\nwe compare our approach with standard machine learning and deep learning\nmodels, showing that Neurovectors achieve competitive accuracy.", "AI": {"tldr": "提出了一种基于Neurovectors的新型学习方法，通过向量空间中的能量传播驱动学习，提高了模型的适应性和可解释性。", "motivation": "传统神经网络依赖反向传播调整权重，缺乏可解释性和适应性。Neurovectors旨在通过向量关系和信息结构化解决这些问题。", "method": "利用Neurovectors在向量空间中编码数据，通过能量传播而非权重更新驱动学习，生成动态知识表示。", "result": "在UCI和Kaggle数据集上的实验表明，Neurovectors在分类和回归任务中具有竞争力。", "conclusion": "Neurovectors提供了一种更适应性强且可解释的学习方法，性能与传统模型相当。"}}
{"id": "2506.07484", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.5.2"], "pdf": "https://arxiv.org/pdf/2506.07484", "abs": "https://arxiv.org/abs/2506.07484", "authors": ["Dasol Hong", "Wooju Lee", "Hyun Myung"], "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization", "comment": "8 pages, 5 figures; accepted at ICML 2025", "summary": "Prompt tuning, which adapts vision-language models by freezing model\nparameters and optimizing only the prompt, has proven effective for\ntask-specific adaptations. The core challenge in prompt tuning is improving\nspecialization for a specific task and generalization for unseen domains.\nHowever, frozen encoders often produce misaligned features, leading to\nconfusion between classes and limiting specialization. To overcome this issue,\nwe propose a confusion-aware loss (CoA-loss) that improves specialization by\nrefining the decision boundaries between confusing classes. Additionally, we\nmathematically demonstrate that a mixture model can enhance generalization\nwithout compromising specialization. This is achieved using confidence-aware\nweights (CoA-weights), which adjust the weights of each prediction in the\nmixture model based on its confidence within the class domains. Extensive\nexperiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,\noutperforms state-of-the-art methods by enhancing specialization and\ngeneralization. Our code is publicly available at\nhttps://github.com/url-kaist/CoCoA-Mix.", "AI": {"tldr": "论文提出了一种名为CoCoA-Mix的方法，通过混淆感知损失（CoA-loss）和置信感知权重（CoA-weights）提升视觉语言模型在特定任务上的专业化和泛化能力。", "motivation": "冻结编码器常导致特征不对齐，引发类别混淆，限制了模型的专化能力。", "method": "提出CoA-loss优化决策边界以减少混淆，并利用CoA-weights在混合模型中调整预测权重以增强泛化。", "result": "实验表明CoCoA-Mix在专业化和泛化方面优于现有方法。", "conclusion": "CoCoA-Mix通过结合CoA-loss和CoA-weights，有效解决了特征对齐和泛化问题。"}}
{"id": "2506.07191", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.07191", "abs": "https://arxiv.org/abs/2506.07191", "authors": ["Ramisa Farha", "Joshua O. Olukoya"], "title": "Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach", "comment": null, "summary": "This study employs a robust analytical framework to uncover patterns in\nsurvival outcomes among breast cancer patients from diverse racial and\ngeographical backgrounds. This research uses the SEER 2021 dataset to analyze\nbreast cancer survival outcomes to identify and comprehend dissimilarities. Our\napproach integrates exploratory data analysis (EDA), through this we identify\nkey variables that influence survival rates and employ survival analysis\ntechniques, including the Kaplan-Meier estimator and log-rank test and the\nadvanced modeling Cox Proportional Hazards model to determine how survival\nrates vary across racial groups and countries. Model validation and\ninterpretation are undertaken to ensure the reliability of our findings, which\nare documented comprehensively to inform policymakers and healthcare\nprofessionals. The outcome of this paper is a detailed version of statistical\nanalysis that not just highlights disparities in breast cancer treatment and\ncare but also serves as a foundational tool for developing targeted\ninterventions to address the inequalities effectively. Through this research,\nour aim is to contribute to the global efforts to improve breast cancer\noutcomes and reduce treatment disparities.", "AI": {"tldr": "该研究通过SEER 2021数据集分析乳腺癌患者生存率差异，结合EDA、Kaplan-Meier估计、log-rank检验和Cox比例风险模型，揭示种族和地理差异，为政策制定提供依据。", "motivation": "揭示乳腺癌患者生存率在不同种族和地理背景下的差异，为减少治疗不平等提供数据支持。", "method": "使用SEER 2021数据集，结合EDA、Kaplan-Meier估计、log-rank检验和Cox比例风险模型进行分析。", "result": "详细统计结果揭示了乳腺癌治疗和护理中的不平等现象。", "conclusion": "研究结果为制定针对性干预措施提供了基础，有助于改善乳腺癌治疗结果和减少不平等。"}}
{"id": "2506.07489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07489", "abs": "https://arxiv.org/abs/2506.07489", "authors": ["Yahao Shi", "Yang Liu", "Yanmin Wu", "Xing Liu", "Chen Zhao", "Jie Luo", "Bin Zhou"], "title": "Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video", "comment": "technical report", "summary": "We propose DriveAnyMesh, a method for driving mesh guided by monocular video.\nCurrent 4D generation techniques encounter challenges with modern rendering\nengines. Implicit methods have low rendering efficiency and are unfriendly to\nrasterization-based engines, while skeletal methods demand significant manual\neffort and lack cross-category generalization. Animating existing 3D assets,\ninstead of creating 4D assets from scratch, demands a deep understanding of the\ninput's 3D structure. To tackle these challenges, we present a 4D diffusion\nmodel that denoises sequences of latent sets, which are then decoded to produce\nmesh animations from point cloud trajectory sequences. These latent sets\nleverage a transformer-based variational autoencoder, simultaneously capturing\n3D shape and motion information. By employing a spatiotemporal,\ntransformer-based diffusion model, information is exchanged across multiple\nlatent frames, enhancing the efficiency and generalization of the generated\nresults. Our experimental results demonstrate that DriveAnyMesh can rapidly\nproduce high-quality animations for complex motions and is compatible with\nmodern rendering engines. This method holds potential for applications in both\nthe gaming and filming industries.", "AI": {"tldr": "DriveAnyMesh是一种基于单目视频驱动网格的方法，解决了现有4D生成技术在渲染效率和兼容性上的问题。", "motivation": "当前4D生成技术在现代渲染引擎中效率低且兼容性差，而骨骼动画需要大量手动工作且缺乏跨类别泛化能力。", "method": "使用4D扩散模型对潜在集序列去噪，并通过基于变压器的变分自编码器解码生成网格动画。", "result": "实验表明，DriveAnyMesh能快速生成高质量动画，且兼容现代渲染引擎。", "conclusion": "该方法在游戏和电影行业具有应用潜力。"}}
{"id": "2506.07198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07198", "abs": "https://arxiv.org/abs/2506.07198", "authors": ["Tianci Bu", "Chuanrui Wang", "Hao Ma", "Haoren Zheng", "Xin Lu", "Tailin Wu"], "title": "GGBall: Graph Generative Model on Poincaré Ball", "comment": "29 pages, 3 figures", "summary": "Generating graphs with hierarchical structures remains a fundamental\nchallenge due to the limitations of Euclidean geometry in capturing exponential\ncomplexity. Here we introduce \\textbf{GGBall}, a novel hyperbolic framework for\ngraph generation that integrates geometric inductive biases with modern\ngenerative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder\n(HVQVAE) with a Riemannian flow matching prior defined via closed-form\ngeodesics. This design enables flow-based priors to model complex latent\ndistributions, while vector quantization helps preserve the curvature-aware\nstructure of the hyperbolic space. We further develop a suite of hyperbolic GNN\nand Transformer layers that operate entirely within the manifold, ensuring\nstability and scalability. Empirically, our model reduces degree MMD by over\n75\\% on Community-Small and over 40\\% on Ego-Small compared to state-of-the-art\nbaselines, demonstrating an improved ability to preserve topological\nhierarchies. These results highlight the potential of hyperbolic geometry as a\npowerful foundation for the generative modeling of complex, structured, and\nhierarchical data domains. Our code is available at\n\\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.", "AI": {"tldr": "GGBall提出了一种双曲框架用于图生成，结合几何归纳偏置与现代生成范式，显著提升了拓扑层次结构的保留能力。", "motivation": "解决欧几里得几何在捕捉指数复杂度时的局限性，生成具有层次结构的图。", "method": "结合双曲向量量化自编码器（HVQVAE）和黎曼流匹配先验，开发双曲GNN和Transformer层。", "result": "在Community-Small和Ego-Small数据集上，度数MMD分别降低75%和40%。", "conclusion": "双曲几何为复杂、结构化、层次化数据的生成建模提供了强大基础。"}}
{"id": "2506.07491", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07491", "abs": "https://arxiv.org/abs/2506.07491", "authors": ["Yongsen Mao", "Junhao Zhong", "Chuan Fang", "Jia Zheng", "Rui Tang", "Hao Zhu", "Ping Tan", "Zihan Zhou"], "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "comment": null, "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.", "AI": {"tldr": "SpatialLM是一个处理3D点云数据并生成结构化3D场景理解输出的大型语言模型，性能优于现有方法。", "motivation": "提升现代LLM的空间理解能力，以应用于增强现实和机器人等领域。", "method": "基于开源LLM的多模态架构，通过大规模合成数据集进行微调。", "result": "在布局估计任务中达到最优性能，3D物体检测结果也具有竞争力。", "conclusion": "展示了增强LLM空间理解能力的可行路径。"}}
{"id": "2506.07218", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07218", "abs": "https://arxiv.org/abs/2506.07218", "authors": ["Tong Xiao", "Xin Xu", "Zhenya Huang", "Hongyu Gao", "Quan Liu", "Qi Liu", "Enhong Chen"], "title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "comment": null, "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language\nModels (MLLMs) is a challenging task that has attracted increasing attention in\nthe community. Recently, several studies have applied Reinforcement Learning\nwith Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the\nreasoning abilities of MLLMs. However, these works largely overlook the\nenhancement of multimodal perception capabilities in MLLMs, which serve as a\ncore prerequisite and foundational component of complex multimodal reasoning.\nThrough McNemar's test, we find that existing RLVR method fails to effectively\nenhance the multimodal perception capabilities of MLLMs, thereby limiting their\nfurther improvement in multimodal reasoning. To address this limitation, we\npropose Perception-R1, which introduces a novel visual perception reward that\nexplicitly encourages MLLMs to perceive the visual content accurately, thereby\ncan effectively incentivizing both their multimodal perception and reasoning\ncapabilities. Specifically, we first collect textual visual annotations from\nthe CoT trajectories of multimodal problems, which will serve as visual\nreferences for reward assignment. During RLVR training, we employ a judging LLM\nto assess the consistency between the visual annotations and the responses\ngenerated by MLLM, and assign the visual perception reward based on these\nconsistency judgments. Extensive experiments on several multimodal reasoning\nbenchmarks demonstrate the effectiveness of our Perception-R1, which achieves\nstate-of-the-art performance on most benchmarks using only 1,442 training data.", "AI": {"tldr": "论文提出Perception-R1方法，通过引入视觉感知奖励提升多模态大语言模型（MLLMs）的感知与推理能力。", "motivation": "现有基于可验证奖励的强化学习（RLVR）方法忽视了多模态感知能力的提升，限制了推理能力的进一步改进。", "method": "提出Perception-R1，利用视觉注释作为奖励参考，通过评判LLM评估一致性并分配视觉感知奖励。", "result": "在多个多模态推理基准测试中，Perception-R1仅用1,442训练数据即达到最优性能。", "conclusion": "Perception-R1有效提升了MLLMs的多模态感知与推理能力。"}}
{"id": "2506.07497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07497", "abs": "https://arxiv.org/abs/2506.07497", "authors": ["Xiangyu Guo", "Zhanqian Wu", "Kaixin Xiong", "Ziyang Xu", "Lijun Zhou", "Gangwei Xu", "Shaoqing Xu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency", "comment": null, "summary": "We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.", "AI": {"tldr": "Genesis是一个统一框架，用于联合生成多视角驾驶视频和LiDAR序列，具有时空和跨模态一致性。", "motivation": "解决多模态数据生成中时空和跨模态一致性的挑战。", "method": "采用两阶段架构，结合DiT视频扩散模型与3D-VAE编码，以及BEV感知的LiDAR生成器与NeRF渲染和自适应采样。", "result": "在nuScenes基准测试中表现优异（FVD 16.95, FID 4.24, Chamfer 0.611），并提升下游任务性能。", "conclusion": "Genesis在生成数据的语义保真度和实用性方面表现出色。"}}
{"id": "2506.07229", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07229", "abs": "https://arxiv.org/abs/2506.07229", "authors": ["Mateusz Gajewski", "Mikołaj Morzy", "Adam Karczmarz", "Piotr Sankowski"], "title": "VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution", "comment": null, "summary": "Existing feature attribution methods like SHAP often suffer from global\ndependence, failing to capture true local model behavior. This paper introduces\nVARSHAP, a novel model-agnostic local feature attribution method which uses the\nreduction of prediction variance as the key importance metric of features.\nBuilding upon Shapley value framework, VARSHAP satisfies the key Shapley\naxioms, but, unlike SHAP, is resilient to global data distribution shifts.\nExperiments on synthetic and real-world datasets demonstrate that VARSHAP\noutperforms popular methods such as KernelSHAP or LIME, both quantitatively and\nqualitatively.", "AI": {"tldr": "VARSHAP是一种新的局部特征归因方法，通过减少预测方差作为特征重要性指标，优于SHAP和LIME。", "motivation": "现有特征归因方法（如SHAP）存在全局依赖性，无法准确捕捉局部模型行为。", "method": "基于Shapley值框架，VARSHAP以预测方差减少为核心指标，对全局数据分布变化具有鲁棒性。", "result": "在合成和真实数据集上，VARSHAP在定量和定性上均优于KernelSHAP和LIME。", "conclusion": "VARSHAP是一种更有效的局部特征归因方法，适用于数据分布变化的场景。"}}
{"id": "2506.07533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07533", "abs": "https://arxiv.org/abs/2506.07533", "authors": ["Wei Tao", "Haocheng Lu", "Xiaoyang Qu", "Bin Zhang", "Kai Lu", "Jiguang Wan", "Jianzong Wang"], "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts", "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.", "AI": {"tldr": "MoQAE是一种新型混合精度量化方法，通过量化感知专家的混合来优化大语言模型的长上下文推理中的KV缓存内存消耗。", "motivation": "解决现有量化方法无法同时兼顾效果和效率的问题，优化KV缓存的高内存消耗。", "method": "1. 将不同量化位宽配置视为专家，采用MoE方法选择最优配置；2. 设计轻量级路由器微调过程，平衡精度与内存；3. 引入路由冻结和共享机制降低推理开销。", "result": "在多个基准数据集上，MoQAE在效率和效果上均优于现有KV缓存量化方法。", "conclusion": "MoQAE通过混合精度量化和优化路由机制，显著提升了KV缓存的效率和效果。"}}
{"id": "2506.07240", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.07240", "abs": "https://arxiv.org/abs/2506.07240", "authors": ["Roy Eisenstadt", "Itamar Zimerman", "Lior Wolf"], "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs", "comment": null, "summary": "Recently, techniques such as explicit structured reasoning have demonstrated\nstrong test-time scaling behavior by enforcing a separation between the model's\ninternal \"thinking\" process and the final response. A key factor influencing\nanswer quality in this setting is the length of the thinking stage. When the\nreasoning is too short, the model may fail to capture the complexity of the\ntask. Conversely, when it is too long, the model may overthink, leading to\nunnecessary computation and degraded performance. This paper explores and\nexploits the underlying mechanisms by which LLMs understand and regulate the\nlength of their reasoning during explicit thought processes. First, we show\nthat LLMs encode their progress through the reasoning process and introduce an\ninteractive progress bar visualization, which is then used to reveal insights\non the model's planning dynamics. Second, we manipulate the internal progress\nencoding during inference to reduce unnecessary steps and generate a more\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\nand reduces inference latency. Our code is publicly available.", "AI": {"tldr": "论文探讨了如何通过调节LLMs的推理长度来优化其表现，避免过短或过长的思考阶段。", "motivation": "研究LLMs在显式推理过程中如何理解和调节推理长度的机制，以提升答案质量和效率。", "method": "1. 展示LLMs如何编码推理进度，并引入交互式进度条可视化；2. 在推理过程中操纵内部进度编码以减少冗余步骤。", "result": "实验表明，该方法能减少过度思考，提高答案准确性，并降低推理延迟。", "conclusion": "通过调节推理长度，可以显著优化LLMs的表现，同时公开代码以供进一步研究。"}}
{"id": "2506.07539", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07539", "abs": "https://arxiv.org/abs/2506.07539", "authors": ["Xiaomeng Zhu", "Jacob Henningsson", "Duruo Li", "Pär Mårtensson", "Lars Hanson", "Mårten Björkman", "Atsuto Maki"], "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study", "comment": "This is accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), waiting for publication. 14 pages, 14 figures", "summary": "This paper addresses key aspects of domain randomization in generating\nsynthetic data for manufacturing object detection applications. To this end, we\npresent a comprehensive data generation pipeline that reflects different\nfactors: object characteristics, background, illumination, camera settings, and\npost-processing. We also introduce the Synthetic Industrial Parts Object\nDetection dataset (SIP15-OD) consisting of 15 objects from three industrial use\ncases under varying environments as a test bed for the study, while also\nemploying an industrial dataset publicly available for robotic applications. In\nour experiments, we present more abundant results and insights into the\nfeasibility as well as challenges of sim-to-real object detection. In\nparticular, we identified material properties, rendering methods,\npost-processing, and distractors as important factors. Our method, leveraging\nthese, achieves top performance on the public dataset with Yolov8 models\ntrained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics\ndataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,\nrespectively. The results showcase the effectiveness of the proposed domain\nrandomization, potentially covering the distribution close to real data for the\napplications.", "AI": {"tldr": "论文提出了一种用于制造业目标检测的合成数据生成方法，通过领域随机化技术生成多样化的数据，并在公开数据集上验证了其有效性。", "motivation": "解决制造业目标检测中合成数据生成的挑战，探索领域随机化在模拟到真实场景中的应用。", "method": "提出了一个综合数据生成流程，考虑对象特征、背景、光照等因素，并引入SIP15-OD数据集进行验证。", "result": "在Yolov8模型上，合成数据训练的模型在公开数据集上表现优异，mAP@50得分高达96.4%。", "conclusion": "领域随机化技术能有效生成接近真实数据分布的合成数据，为制造业目标检测提供了可行方案。"}}
{"id": "2506.07247", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07247", "abs": "https://arxiv.org/abs/2506.07247", "authors": ["Ngoc-Quan Pham", "Tuan Truong", "Quyen Tran", "Tan Nguyen", "Dinh Phung", "Trung Le"], "title": "Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models", "comment": "ICML 2025 (Poster)", "summary": "We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel\nBayesian inference framework that allows modeling the interactions between\nparticles, thereby enhancing ensemble quality through increased particle\ndiversity. IBDR is grounded in a generalized theoretical framework that\nconnects the distributional population loss with the approximate posterior,\nmotivating a practical dual optimization procedure that enforces distributional\nrobustness while fostering particle diversity. We evaluate IBDR's performance\nagainst various baseline methods using the VTAB-1K benchmark and the common\nreasoning language task. The results consistently show that IBDR outperforms\nthese baselines, underscoring its effectiveness in real-world applications.", "AI": {"tldr": "IBDR是一种新的贝叶斯推理框架，通过增强粒子多样性提升集合质量，并在VTAB-1K基准测试中表现优于基线方法。", "motivation": "提出IBDR以建模粒子间交互，提升集合质量。", "method": "基于广义理论框架，通过双优化程序实现分布鲁棒性和粒子多样性。", "result": "在VTAB-1K和语言推理任务中，IBDR表现优于基线方法。", "conclusion": "IBDR在实际应用中表现出色，验证了其有效性。"}}
{"id": "2506.07542", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07542", "abs": "https://arxiv.org/abs/2506.07542", "authors": ["Bowen Liu", "Weiyi Zhang", "Peranut Chotcomwongse", "Xiaolan Chen", "Ruoyu Chen", "Pawin Pakaymaskul", "Niracha Arjkongharn", "Nattaporn Vongsa", "Xuelian Cheng", "Zongyuan Ge", "Kun Huang", "Xiaohui Li", "Yiru Duan", "Zhenbang Wang", "BaoYe Xie", "Qiang Chen", "Huazhu Fu", "Michael A. Mahr", "Jiaqi Qu", "Wangyiyang Chen", "Shiye Wang", "Yubo Tan", "Yongjie Li", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs", "comment": null, "summary": "Optical Coherence Tomography (OCT) provides high-resolution, 3D, and\nnon-invasive visualization of retinal layers in vivo, serving as a critical\ntool for lesion localization and disease diagnosis. However, its widespread\nadoption is limited by equipment costs and the need for specialized operators.\nIn comparison, 2D color fundus photography offers faster acquisition and\ngreater accessibility with less dependence on expensive devices. Although\ngenerative artificial intelligence has demonstrated promising results in\nmedical image synthesis, translating 2D fundus images into 3D OCT images\npresents unique challenges due to inherent differences in data dimensionality\nand biological information between modalities. To advance generative models in\nthe fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society\n(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT\nGeneration from Fundus Images. This paper details the challenge framework\n(referred to as APTOS-2024 Challenge), including: the benchmark dataset,\nevaluation methodology featuring two fidelity metrics-image-based distance\n(pixel-level OCT B-scan similarity) and video-based distance (semantic-level\nvolumetric consistency), and analysis of top-performing solutions. The\nchallenge attracted 342 participating teams, with 42 preliminary submissions\nand 9 finalists. Leading methodologies incorporated innovations in hybrid data\npreprocessing or augmentation (cross-modality collaborative paradigms),\npre-training on external ophthalmic imaging datasets, integration of vision\nfoundation models, and model architecture improvement. The APTOS-2024 Challenge\nis the first benchmark demonstrating the feasibility of fundus-to-3D-OCT\nsynthesis as a potential solution for improving ophthalmic care accessibility\nin under-resourced healthcare settings, while helping to expedite medical\nresearch and clinical applications.", "AI": {"tldr": "APTOS-2024挑战赛探索了从2D眼底图像生成3D OCT图像的可行性，旨在提高眼科护理的可及性。", "motivation": "解决OCT设备成本高、操作复杂的问题，利用2D眼底图像的便捷性，通过生成式AI实现3D OCT图像的合成。", "method": "挑战赛提供了基准数据集和评估方法（图像和视频级指标），吸引了342个团队参与，采用混合数据预处理、预训练和模型架构改进等方法。", "result": "42个初步提交和9个决赛方案展示了可行性，创新方法包括跨模态协作和视觉基础模型集成。", "conclusion": "APTOS-2024挑战赛首次验证了2D到3D OCT合成的潜力，为资源匮乏地区提供了一种可能的解决方案。"}}
{"id": "2506.07254", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07254", "abs": "https://arxiv.org/abs/2506.07254", "authors": ["Kevin Frans", "Sergey Levine", "Pieter Abbeel"], "title": "A Stable Whitening Optimizer for Efficient Neural Network Training", "comment": null, "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.", "AI": {"tldr": "论文提出SPlus方法，解决了Shampoo算法的三个关键问题，包括稳定性、计算效率和参数噪声，并在多个任务中验证其性能优于Adam。", "motivation": "针对Shampoo算法在优化神经网络时存在的稳定性、计算效率和参数噪声问题，提出改进方法。", "method": "1. 引入有界更新结合历史特征基和瞬时归一化；2. 采用形状感知缩放实现学习率跨网络宽度迁移；3. 提出迭代平均方案减少参数噪声。", "result": "SPlus在验证性能上达到Adam的44%梯度步数和62%实际时间。", "conclusion": "SPlus方法显著提升了优化效率和稳定性，适用于多种任务。"}}
{"id": "2506.07555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07555", "abs": "https://arxiv.org/abs/2506.07555", "authors": ["Haoxiang Wang", "Zinan Lin", "Da Yu", "Huishuai Zhang"], "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries", "comment": null, "summary": "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.", "AI": {"tldr": "SPTI是一种通过文本中介生成高分辨率差分隐私（DP）图像的新方法，无需训练模型，仅使用现成模型即可实现高质量合成图像。", "motivation": "现有DP图像合成方法难以生成高分辨率且忠实于原始数据的图像，SPTI旨在解决这一问题。", "method": "SPTI将图像转换为文本描述，利用改进的Private Evolution算法生成DP文本，再通过文本到图像模型重建图像。", "result": "在LSUN Bedroom和MM CelebA HQ数据集上，SPTI的FID显著优于现有方法（如26.71 vs 40.36）。", "conclusion": "SPTI提供了一种资源高效且兼容专有模型的框架，显著扩展了对私有视觉数据的访问。"}}
{"id": "2506.07272", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07272", "abs": "https://arxiv.org/abs/2506.07272", "authors": ["Alex Clinton", "Thomas Zeng", "Yiding Chen", "Xiaojin Zhu", "Kirthevasan Kandasamy"], "title": "A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing", "comment": null, "summary": "Modern data marketplaces and data sharing consortia increasingly rely on\nincentive mechanisms to encourage agents to contribute data. However, schemes\nthat reward agents based on the quantity of submitted data are vulnerable to\nmanipulation, as agents may submit fabricated or low-quality data to inflate\ntheir rewards. Prior work has proposed comparing each agent's data against\nothers' to promote honesty: when others contribute genuine data, the best way\nto minimize discrepancy is to do the same. Yet prior implementations of this\nidea rely on very strong assumptions about the data distribution (e.g.\nGaussian), limiting their applicability. In this work, we develop reward\nmechanisms based on a novel, two-sample test inspired by the Cram\\'er-von Mises\nstatistic. Our methods strictly incentivize agents to submit more genuine data,\nwhile disincentivizing data fabrication and other types of untruthful\nreporting. We establish that truthful reporting constitutes a (possibly\napproximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We\ntheoretically instantiate our method in three canonical data sharing problems\nand show that it relaxes key assumptions made by prior work. Empirically, we\ndemonstrate that our mechanism incentivizes truthful data sharing via\nsimulations and on real-world language and image data.", "AI": {"tldr": "论文提出了一种基于Cramér-von Mises统计量的新型奖励机制，旨在激励真实数据共享，同时抑制数据伪造。", "motivation": "现有基于数据量的奖励机制易被操纵，而依赖强分布假设的对比方法适用性有限。", "method": "开发了一种基于两样本检验的奖励机制，无需强分布假设。", "result": "理论证明真实报告为纳什均衡，实验验证了机制在语言和图像数据上的有效性。", "conclusion": "新机制在放宽假设的同时，有效激励了真实数据共享。"}}
{"id": "2506.07559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07559", "abs": "https://arxiv.org/abs/2506.07559", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "comment": null, "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "AI": {"tldr": "提出了一种新的跨通道感知学习（CCPL）策略，用于解决H&E-to-IHC研究中忽略的细胞核与细胞膜跨通道相关性问题。", "motivation": "现有H&E-to-IHC研究常忽视细胞核与细胞膜的跨通道相关性，限制了病理图像分析与诊断的准确性。", "method": "CCPL将HER2免疫组化染色分解为Hematoxylin和DAB染色通道，利用Gigapath的Tile Encoder提取双通道特征并测量跨通道相关性，同时计算特征蒸馏损失和光学密度统计。", "result": "实验表明，CCPL在PSNR、SSIM、PCC和FID等指标上表现优异，且病理学家评估确认其能生成高质量虚拟染色图像。", "conclusion": "CCPL有效保留病理特征，为多媒体医疗数据的自动化病理诊断提供支持。"}}
{"id": "2506.07565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07565", "abs": "https://arxiv.org/abs/2506.07565", "authors": ["Jinlu Zhang", "Zixi Kang", "Yizhou Wang"], "title": "OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data", "comment": null, "summary": "Music-driven dance generation offers significant creative potential yet faces\nconsiderable challenges. The absence of fine-grained multimodal data and the\ndifficulty of flexible multi-conditional generation limit previous works on\ngeneration controllability and diversity in practice. In this paper, we build\nOpenDance5D, an extensive human dance dataset comprising over 101 hours across\n14 distinct genres. Each sample has five modalities to facilitate robust\ncross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and\nfine-grained textual descriptions from human arts. Furthermore, we propose\nOpenDanceNet, a unified masked modeling framework for controllable dance\ngeneration conditioned on music and arbitrary combinations of text prompts,\nkeypoints, or character positioning. Comprehensive experiments demonstrate that\nOpenDanceNet achieves high-fidelity and flexible controllability.", "AI": {"tldr": "论文提出了OpenDance5D数据集和OpenDanceNet框架，解决了音乐驱动舞蹈生成中的数据不足和可控性问题。", "motivation": "现有研究因缺乏细粒度多模态数据和灵活的多条件生成能力，导致生成舞蹈的多样性和可控性受限。", "method": "构建了OpenDance5D数据集（14种舞蹈风格，101小时数据，含5种模态），并提出OpenDanceNet框架，基于掩码建模实现多条件可控舞蹈生成。", "result": "实验表明OpenDanceNet能实现高保真和灵活可控的舞蹈生成。", "conclusion": "OpenDance5D和OpenDanceNet为音乐驱动舞蹈生成提供了数据和方法支持，提升了生成质量和可控性。"}}
{"id": "2506.07276", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07276", "abs": "https://arxiv.org/abs/2506.07276", "authors": ["Suho Shin", "Chenghao Yang", "Haifeng Xu", "Mohammad T. Hajiaghayi"], "title": "Tokenized Bandit for LLM Decoding and Alignment", "comment": "To appear at ICML 2025", "summary": "We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),\nvariants of linear and stochastic multi-armed bandit problems inspired by LLM\ndecoding and alignment. In these problems, at each round $t \\in [T]$, a user\nsubmits a query (context), and the decision maker (DM) sequentially selects a\ntoken irrevocably from a token set. Once the sequence is complete, the DM\nobserves a random utility from the user, whose expectation is presented by a\nsequence function mapping the chosen token sequence to a nonnegative real value\nthat depends on the query.\n  In both problems, we first show that learning is impossible without any\nstructure on the sequence function. We introduce a natural assumption,\ndiminishing distance with more commons (DDMC), and propose algorithms with\nregret $\\tilde{O}(L\\sqrt{T})$ and $\\tilde{O}(L\\sqrt{T^{2/3}})$ for TLB and\nTMAB, respectively. As a side product, we obtain an (almost) optimality of the\ngreedy decoding for LLM decoding algorithm under DDMC, which justifies the\nunresaonable effectiveness of greedy decoding in several tasks. This also has\nan immediate application to decoding-time LLM alignment, when the misaligned\nutility can be represented as the frozen LLM's utility and a linearly\nrealizable latent function. We finally validate our algorithm's performance\nempirically as well as verify our assumptions using synthetic and real-world\ndatasets.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07566", "abs": "https://arxiv.org/abs/2506.07566", "authors": ["Marco Peer", "Robert Sablatnig", "Florian Kleber"], "title": "Towards the Influence of Text Quantity on Writer Retrieval", "comment": "accepted for ICDAR2025", "summary": "This paper investigates the task of writer retrieval, which identifies\ndocuments authored by the same individual within a dataset based on handwriting\nsimilarities. While existing datasets and methodologies primarily focus on page\nlevel retrieval, we explore the impact of text quantity on writer retrieval\nperformance by evaluating line- and word level retrieval. We examine three\nstate-of-the-art writer retrieval systems, including both handcrafted and deep\nlearning-based approaches, and analyze their performance using varying amounts\nof text. Our experiments on the CVL and IAM dataset demonstrate that while\nperformance decreases by 20-30% when only one line of text is used as query and\ngallery, retrieval accuracy remains above 90% of full-page performance when at\nleast four lines are included. We further show that text-dependent retrieval\ncan maintain strong performance in low-text scenarios. Our findings also\nhighlight the limitations of handcrafted features in low-text scenarios, with\ndeep learning-based methods like NetVLAD outperforming traditional VLAD\nencoding.", "AI": {"tldr": "研究了基于手写相似性的作者检索任务，探讨了文本量对检索性能的影响，发现多行文本能保持较高准确率，深度学习方法在低文本场景中表现更优。", "motivation": "探索文本量（行和词级别）对作者检索性能的影响，填补现有研究主要关注页面级别检索的空白。", "method": "评估了三种先进作者检索系统（手工特征和深度学习方法），在不同文本量下的性能，使用CVL和IAM数据集进行实验。", "result": "实验表明，仅用一行文本时性能下降20-30%，但四行文本可保持90%以上的全页性能；深度学习方法在低文本场景中优于手工特征。", "conclusion": "多行文本能有效维持检索性能，深度学习方法在低文本场景中更具优势，手工特征在此场景中表现受限。"}}
{"id": "2506.07288", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07288", "abs": "https://arxiv.org/abs/2506.07288", "authors": ["Weijie Guan", "Haohui Wang", "Jian Kang", "Lihui Liu", "Dawei Zhou"], "title": "EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments", "comment": "KDD 2025", "summary": "Graph learning has been crucial to many real-world tasks, but they are often\nstudied with a closed-world assumption, with all possible labels of data known\na priori. To enable effective graph learning in an open and noisy environment,\nit is critical to inform the model users when the model makes a wrong\nprediction to in-distribution data of a known class, i.e., misclassification\ndetection or when the model encounters out-of-distribution from novel classes,\ni.e., out-of-distribution detection. This paper introduces Evidential Reasoning\nNetwork (EVINET), a framework that addresses these two challenges by\nintegrating Beta embedding within a subjective logic framework. EVINET includes\ntwo key modules: Dissonance Reasoning for misclassification detection and\nVacuity Reasoning for out-of-distribution detection. Extensive experiments\ndemonstrate that EVINET outperforms state-of-the-art methods across multiple\nmetrics in the tasks of in-distribution classification, misclassification\ndetection, and out-of-distribution detection. EVINET demonstrates the necessity\nof uncertainty estimation and logical reasoning for misclassification detection\nand out-of-distribution detection and paves the way for open-world graph\nlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.", "AI": {"tldr": "EVINET框架通过Beta嵌入和主观逻辑解决图学习中的误分类检测和分布外检测问题，性能优于现有方法。", "motivation": "解决开放和噪声环境中图学习的误分类和分布外检测问题，提升模型可靠性。", "method": "结合Beta嵌入和主观逻辑，设计Dissonance Reasoning和Vacuity Reasoning模块。", "result": "在多个任务和指标上优于现有方法，验证了不确定性和逻辑推理的必要性。", "conclusion": "EVINET为开放世界图学习提供了有效解决方案，代码和数据已开源。"}}
{"id": "2506.07570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07570", "abs": "https://arxiv.org/abs/2506.07570", "authors": ["Yixuan Yang", "Zhen Luo", "Tongsheng Ding", "Junru Lu", "Mingqi Gao", "Jinyu Yang", "Victor Sanchez", "Feng Zheng"], "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization", "comment": null, "summary": "Automatic indoor layout generation has attracted increasing attention due to\nits potential in interior design, virtual environment construction, and\nembodied AI. Existing methods fall into two categories: prompt-driven\napproaches that leverage proprietary LLM services (e.g., GPT APIs) and\nlearning-based methods trained on layout data upon diffusion-based models.\nPrompt-driven methods often suffer from spatial inconsistency and high\ncomputational costs, while learning-based methods are typically constrained by\ncoarse relational graphs and limited datasets, restricting their generalization\nto diverse room categories. In this paper, we revisit LLM-based indoor layout\ngeneration and present 3D-SynthPlace, a large-scale dataset that combines\nsynthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,\nupgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000\nscenes, covering four common room types -- bedroom, living room, kitchen, and\nbathroom -- enriched with diverse objects and high-level spatial annotations.\nWe further introduce OptiScene, a strong open-source LLM optimized for indoor\nlayout generation, fine-tuned based on our 3D-SynthPlace dataset through our\ntwo-stage training. For the warum-up stage I, we adopt supervised fine-tuning\n(SFT), which is taught to first generate high-level spatial descriptions then\nconditionally predict concrete object placements. For the reinforcing stage II,\nto better align the generated layouts with human design preferences, we apply\nmulti-turn direct preference optimization (DPO), which significantly improving\nlayout quality and generation success rates. Extensive experiments demonstrate\nthat OptiScene outperforms traditional prompt-driven and learning-based\nbaselines. Moreover, OptiScene shows promising potential in interactive tasks\nsuch as scene editing and robot navigation.", "AI": {"tldr": "论文提出3D-SynthPlace数据集和OptiScene模型，通过两阶段训练优化室内布局生成，显著优于现有方法。", "motivation": "现有方法在空间一致性和泛化能力上存在不足，需要更高效且通用的解决方案。", "method": "结合合成数据与人类检查构建3D-SynthPlace数据集，并通过SFT和DPO两阶段训练优化LLM模型OptiScene。", "result": "OptiScene在布局质量和生成成功率上优于基线方法，并展示出在交互任务中的潜力。", "conclusion": "3D-SynthPlace和OptiScene为室内布局生成提供了高效且通用的解决方案。"}}
{"id": "2506.07298", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07298", "abs": "https://arxiv.org/abs/2506.07298", "authors": ["Yijia Dai", "Zhaolin Gao", "Yahya Satter", "Sarah Dean", "Jennifer J. Sun"], "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context", "comment": null, "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.", "AI": {"tldr": "LLMs通过上下文学习（ICL）有效建模HMM生成的数据，接近理论最优预测精度，并在实际任务中表现优异。", "motivation": "解决HMM拟合真实数据计算复杂的问题，探索LLMs在建模HMM生成数据中的潜力。", "method": "利用预训练LLMs的ICL能力，在合成HMM数据和真实动物决策任务中测试性能。", "result": "LLMs在合成数据上接近理论最优，实际任务中与专家设计模型竞争。", "conclusion": "ICL能有效学习HMM生成序列，为复杂科学数据提供新工具。"}}
{"id": "2506.07572", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07572", "abs": "https://arxiv.org/abs/2506.07572", "authors": ["Yu Li", "Feng Xue", "Shujie Li", "Jinrui Zhang", "Shuang Yang", "Dan Guo", "Richang Hong"], "title": "Learning Speaker-Invariant Visual Features for Lipreading", "comment": null, "summary": "Lipreading is a challenging cross-modal task that aims to convert visual lip\nmovements into spoken text. Existing lipreading methods often extract visual\nfeatures that include speaker-specific lip attributes (e.g., shape, color,\ntexture), which introduce spurious correlations between vision and text. These\ncorrelations lead to suboptimal lipreading accuracy and restrict model\ngeneralization. To address this challenge, we introduce SIFLip, a\nspeaker-invariant visual feature learning framework that disentangles\nspeaker-specific attributes using two complementary disentanglement modules\n(Implicit Disentanglement and Explicit Disentanglement) to improve\ngeneralization. Specifically, since different speakers exhibit semantic\nconsistency between lip movements and phonetic text when pronouncing the same\nwords, our implicit disentanglement module leverages stable text embeddings as\nsupervisory signals to learn common visual representations across speakers,\nimplicitly decoupling speaker-specific features. Additionally, we design a\nspeaker recognition sub-task within the main lipreading pipeline to filter\nspeaker-specific features, then further explicitly disentangle these\npersonalized visual features from the backbone network via gradient reversal.\nExperimental results demonstrate that SIFLip significantly enhances\ngeneralization performance across multiple public datasets. Experimental\nresults demonstrate that SIFLip significantly improves generalization\nperformance across multiple public datasets, outperforming state-of-the-art\nmethods.", "AI": {"tldr": "SIFLip是一种新的唇读框架，通过解耦说话人特定特征，提升模型的泛化能力。", "motivation": "现有唇读方法提取的视觉特征包含说话人特定属性，导致虚假相关性，影响准确性和泛化能力。", "method": "SIFLip采用隐式解耦和显式解耦模块，结合文本嵌入和说话人识别子任务，分离说话人特定特征。", "result": "实验表明，SIFLip在多个公开数据集上显著优于现有方法。", "conclusion": "SIFLip通过解耦说话人特征，有效提升了唇读模型的泛化性能。"}}
{"id": "2506.07308", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07308", "abs": "https://arxiv.org/abs/2506.07308", "authors": ["Yizhuo Chen", "Chun-Fu", "Chen", "Hsiang Hsu", "Shaohan Hu", "Tarek Abdelzaher"], "title": "PASS: Private Attributes Protection with Stochastic Data Substitution", "comment": null, "summary": "The growing Machine Learning (ML) services require extensive collections of\nuser data, which may inadvertently include people's private information\nirrelevant to the services. Various studies have been proposed to protect\nprivate attributes by removing them from the data while maintaining the\nutilities of the data for downstream tasks. Nevertheless, as we theoretically\nand empirically show in the paper, these methods reveal severe vulnerability\nbecause of a common weakness rooted in their adversarial training based\nstrategies. To overcome this limitation, we propose a novel approach, PASS,\ndesigned to stochastically substitute the original sample with another one\naccording to certain probabilities, which is trained with a novel loss function\nsoundly derived from information-theoretic objective defined for\nutility-preserving private attributes protection. The comprehensive evaluation\nof PASS on various datasets of different modalities, including facial images,\nhuman activity sensory signals, and voice recording datasets, substantiates\nPASS's effectiveness and generalizability.", "AI": {"tldr": "论文提出了一种新方法PASS，通过概率替换原始数据样本以保护隐私属性，克服了现有对抗训练方法的脆弱性。", "motivation": "现有方法在保护隐私属性时因对抗训练策略的弱点而存在严重漏洞，需提出更有效的解决方案。", "method": "提出PASS方法，通过概率替换样本，并结合信息论目标设计的新型损失函数。", "result": "在多种数据集（如面部图像、活动传感器信号和语音记录）上验证了PASS的有效性和通用性。", "conclusion": "PASS在保护隐私属性的同时保持了数据实用性，优于现有方法。"}}
{"id": "2506.07575", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07575", "abs": "https://arxiv.org/abs/2506.07575", "authors": ["Ruiyang Zhang", "Hu Zhang", "Hao Fei", "Zhedong Zheng"], "title": "Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models", "comment": "Project page: https://uncertainty-o.github.io/", "summary": "Large Multimodal Models (LMMs), harnessing the complementarity among diverse\nmodalities, are often considered more robust than pure Language Large Models\n(LLMs); yet do LMMs know what they do not know? There are three key open\nquestions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a\nunified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to\nquantify uncertainty for downstream tasks. In an attempt to address these\nchallenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed\nto reveal uncertainty in LMMs regardless of their modalities, architectures, or\ncapabilities, (2) an empirical exploration of multimodal prompt perturbations\nto uncover LMM uncertainty, offering insights and findings, and (3) derive the\nformulation of multimodal semantic uncertainty, which enables quantifying\nuncertainty from multimodal responses. Experiments across 18 benchmarks\nspanning various modalities and 10 LMMs (both open- and closed-source)\ndemonstrate the effectiveness of Uncertainty-o in reliably estimating LMM\nuncertainty, thereby enhancing downstream tasks such as hallucination\ndetection, hallucination mitigation, and uncertainty-aware Chain-of-Thought\nreasoning.", "AI": {"tldr": "论文提出Uncertainty-o框架，用于评估和量化多模态模型（LMMs）的不确定性，解决统一评估、提示方法和下游任务量化问题。", "motivation": "尽管多模态模型被认为比纯语言模型更鲁棒，但其不确定性评估仍存在挑战，包括统一评估方法、提示设计和量化应用。", "method": "提出Uncertainty-o框架，包括模型无关的评估方法、多模态提示扰动实验和多模态语义不确定性量化公式。", "result": "在18个基准测试和10种LMMs上验证了Uncertainty-o的有效性，提升了幻觉检测、缓解和不确定性感知推理等下游任务。", "conclusion": "Uncertainty-o为LMMs的不确定性评估提供了统一且有效的解决方案，显著提升了相关下游任务的性能。"}}
{"id": "2506.07311", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07311", "abs": "https://arxiv.org/abs/2506.07311", "authors": ["Thomas Joshi", "Herman Saini", "Neil Dhillon", "Antoni Viros i Martin", "Kaoutar El Maghraoui"], "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference", "comment": null, "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.", "AI": {"tldr": "论文提出了一种结合PagedAttention和PyTorch FlexAttention的方法，解决了LLMs在长上下文推理中的内存效率问题，显著降低了推理延迟。", "motivation": "传统KV缓存的处理方式导致LLMs在长上下文推理中存在严重的内存效率问题。", "method": "通过将PagedAttention与PyTorch的FlexAttention结合，优化KV缓存的分配和碎片化问题。", "result": "在NVIDIA L4 GPU上的测试显示，推理延迟显著降低，且随序列长度线性增长。", "conclusion": "该方法为未来长上下文模型的部署提供了高效解决方案，并已开源实现。"}}
{"id": "2506.07576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07576", "abs": "https://arxiv.org/abs/2506.07576", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "comment": null, "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "AI": {"tldr": "提出了一种统一的超编码网络（SEN），通过递归关联多模态编码器，提升视频理解任务的表现。", "motivation": "现有多模态基础模型仅通过对比学习对齐不同模态的编码器，缺乏深层多模态交互，难以理解复杂视频场景中的目标运动。", "method": "将预训练的编码器视为“超级神经元”，设计递归关联（RA）块，逐步融合多模态信息，通过知识整合、分发和提示实现深层交互。", "result": "在跟踪、识别、聊天和编辑四项视频任务中表现显著提升，例如像素级跟踪的Jaccard指数提高2.7%，文本对齐提升6.4%。", "conclusion": "SEN通过递归关联多模态编码器，有效提升了视频理解的性能，为多模态交互提供了新思路。"}}
{"id": "2506.07312", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07312", "abs": "https://arxiv.org/abs/2506.07312", "authors": ["Yusuf Elnady"], "title": "Generative Modeling of Networked Time-Series via Transformer Architectures", "comment": null, "summary": "Many security and network applications require having large datasets to train\nthe machine learning models. Limited data access is a well-known problem in the\nsecurity domain. Recent studies have shown the potential of Transformer models\nto enlarge the size of data by synthesizing new samples, but the synthesized\nsamples don't improve the models over the real data. To address this issue, we\ndesign an efficient transformer-based model as a generative framework to\ngenerate time-series data, that can be used to boost the performance of\nexisting and new ML workflows. Our new transformer model achieves the SOTA\nresults. We style our model to be generalizable and work across different\ndatasets, and produce high-quality samples.", "AI": {"tldr": "提出了一种基于Transformer的高效生成模型，用于生成时间序列数据，以解决安全领域数据不足的问题，并提升机器学习模型的性能。", "motivation": "安全领域的数据访问受限，现有方法生成的合成数据未能有效提升模型性能。", "method": "设计了一种高效的Transformer生成框架，用于生成高质量的时间序列数据。", "result": "模型在生成数据方面达到了SOTA（当前最优）水平，且具有通用性和高质量样本生成能力。", "conclusion": "该Transformer模型能有效解决数据不足问题，并提升机器学习工作流的性能。"}}
{"id": "2506.07590", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07590", "abs": "https://arxiv.org/abs/2506.07590", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Huajie Shao", "Ashley Gao"], "title": "Explore the vulnerability of black-box models via diffusion models", "comment": null, "summary": "Recent advancements in diffusion models have enabled high-fidelity and\nphotorealistic image generation across diverse applications. However, these\nmodels also present security and privacy risks, including copyright violations,\nsensitive information leakage, and the creation of harmful or offensive content\nthat could be exploited maliciously. In this study, we uncover a novel security\nthreat where an attacker leverages diffusion model APIs to generate synthetic\nimages, which are then used to train a high-performing substitute model. This\nenables the attacker to execute model extraction and transfer-based adversarial\nattacks on black-box classification models with minimal queries, without\nneeding access to the original training data. The generated images are\nsufficiently high-resolution and diverse to train a substitute model whose\noutputs closely match those of the target model. Across the seven benchmarks,\nincluding CIFAR and ImageNet subsets, our method shows an average improvement\nof 27.37% over state-of-the-art methods while using just 0.01 times of the\nquery budget, achieving a 98.68% success rate in adversarial attacks on the\ntarget model.", "AI": {"tldr": "论文揭示了一种新的安全威胁，攻击者利用扩散模型API生成合成图像，训练高性能替代模型，从而对黑盒分类模型进行模型提取和对抗攻击。", "motivation": "扩散模型的高保真图像生成能力带来了安全和隐私风险，如版权侵犯、敏感信息泄露和恶意内容生成。", "method": "攻击者通过扩散模型API生成合成图像，训练替代模型，以最小查询量执行模型提取和对抗攻击。", "result": "在七个基准测试中，方法平均提升27.37%，仅使用0.01倍查询预算，对抗攻击成功率达98.68%。", "conclusion": "该方法高效且低成本，突显了扩散模型在安全领域的潜在威胁。"}}
{"id": "2506.07324", "categories": ["cs.LG", "physics.ao-ph", "35Q93 (Primary), 86A10, 65M75 (Secondary)", "I.2.6; I.6.3"], "pdf": "https://arxiv.org/pdf/2506.07324", "abs": "https://arxiv.org/abs/2506.07324", "authors": ["David Millard", "Arielle Carr", "Stéphane Gaudreault", "Ali Baheri"], "title": "DEF: Diffusion-augmented Ensemble Forecasting", "comment": "26 pages, 20 plots, journal paper", "summary": "We present DEF (\\textbf{\\ul{D}}iffusion-augmented \\textbf{\\ul{E}}nsemble\n\\textbf{\\ul{F}}orecasting), a novel approach for generating initial condition\nperturbations. Modern approaches to initial condition perturbations are\nprimarily designed for numerical weather prediction (NWP) solvers, limiting\ntheir applicability in the rapidly growing field of machine learning for\nweather prediction. Consequently, stochastic models in this domain are often\ndeveloped on a case-by-case basis. We demonstrate that a simple conditional\ndiffusion model can (1) generate meaningful structured perturbations, (2) be\napplied iteratively, and (3) utilize a guidance term to intuitivey control the\nlevel of perturbation. This method enables the transformation of any\ndeterministic neural forecasting system into a stochastic one. With our\nstochastic extended systems, we show that the model accumulates less error over\nlong-term forecasts while producing meaningful forecast distributions. We\nvalidate our approach on the 5.625$^\\circ$ ERA5 reanalysis dataset, which\ncomprises atmospheric and surface variables over a discretized global grid,\nspanning from the 1960s to the present. On this dataset, our method\ndemonstrates improved predictive performance along with reasonable spread\nestimates.", "AI": {"tldr": "DEF是一种通过扩散增强集成预测生成初始条件扰动的新方法，适用于机器学习天气预测领域，能将确定性神经预测系统转化为随机系统，提升长期预测准确性。", "motivation": "现代初始条件扰动方法主要针对数值天气预测（NWP）设计，限制了其在机器学习天气预测中的应用，因此需要一种更通用的方法。", "method": "使用简单的条件扩散模型生成结构化扰动，支持迭代应用和通过引导项直观控制扰动水平。", "result": "在ERA5再分析数据集上验证，DEF方法提升了预测性能并生成合理的分布估计，减少了长期预测误差。", "conclusion": "DEF方法为机器学习天气预测提供了一种有效的随机扰动生成方式，显著提升了预测性能和实用性。"}}
{"id": "2506.07600", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07600", "abs": "https://arxiv.org/abs/2506.07600", "authors": ["Nianbo Zeng", "Haowen Hou", "Fei Richard Yu", "Si Shi", "Ying Tiffany He"], "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding", "comment": null, "summary": "Despite recent advances in retrieval-augmented generation (RAG) for video\nunderstanding, effectively understanding long-form video content remains\nunderexplored due to the vast scale and high complexity of video data. Current\nRAG approaches typically segment videos into fixed-length chunks, which often\ndisrupts the continuity of contextual information and fails to capture\nauthentic scene boundaries. Inspired by the human ability to naturally organize\ncontinuous experiences into coherent scenes, we present SceneRAG, a unified\nframework that leverages large language models to segment videos into\nnarrative-consistent scenes by processing ASR transcripts alongside temporal\nmetadata. SceneRAG further sharpens these initial boundaries through\nlightweight heuristics and iterative correction. For each scene, the framework\nfuses information from both visual and textual modalities to extract entity\nrelations and dynamically builds a knowledge graph, enabling robust multi-hop\nretrieval and generation that account for long-range dependencies. Experiments\non the LongerVideos benchmark, featuring over 134 hours of diverse content,\nconfirm that SceneRAG substantially outperforms prior baselines, achieving a\nwin rate of up to 72.5 percent on generation tasks.", "AI": {"tldr": "SceneRAG是一个基于大语言模型的视频理解框架，通过自然分割视频场景并融合多模态信息，显著提升了长视频内容的理解能力。", "motivation": "当前基于固定长度分块的RAG方法破坏了视频的上下文连续性，无法捕捉真实场景边界，而人类能够自然组织连续经验为连贯场景。", "method": "利用大语言模型处理ASR转录和时间元数据，分割视频为叙事一致的场景，并通过轻量级启发式和迭代修正优化边界。融合视觉和文本模态信息，构建动态知识图谱以支持多跳检索和生成。", "result": "在LongerVideos基准测试中，SceneRAG以72.5%的胜率显著优于现有基线。", "conclusion": "SceneRAG通过自然场景分割和多模态融合，有效解决了长视频理解的挑战，为未来研究提供了新方向。"}}
{"id": "2506.07328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07328", "abs": "https://arxiv.org/abs/2506.07328", "authors": ["Jintao Yan", "Tan Chen", "Yuxuan Sun", "Zhaojun Nan", "Sheng Zhou", "Zhisheng Niu"], "title": "Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification", "comment": null, "summary": "Asynchronous Federated Learning (AFL) enables distributed model training\nacross multiple mobile devices, allowing each device to independently update\nits local model without waiting for others. However, device mobility introduces\nintermittent connectivity, which necessitates gradient sparsification and leads\nto model staleness, jointly affecting AFL convergence. This paper develops a\ntheoretical model to characterize the interplay among sparsification, model\nstaleness and mobility-induced contact patterns, and their joint impact on AFL\nconvergence. Based on the analysis, we propose a mobility-aware dynamic\nsparsification (MADS) algorithm that optimizes the sparsification degree based\non contact time and model staleness. Closed-form solutions are derived, showing\nthat under low-speed conditions, MADS increases the sparsification degree to\nenhance convergence, while under high-speed conditions, it reduces the\nsparsification degree to guarantee reliable uploads within limited contact\ntime. Experimental results validate the theoretical findings. Compared with the\nstate-of-the-art benchmarks, the MADS algorithm increases the image\nclassification accuracy on the CIFAR-10 dataset by 8.76% and reduces the\naverage displacement error in the Argoverse trajectory prediction dataset by\n9.46%.", "AI": {"tldr": "本文提出了一种移动感知的动态稀疏化（MADS）算法，用于优化异步联邦学习（AFL）中的梯度稀疏化，以应对设备移动性带来的连接问题。", "motivation": "设备移动性导致间歇性连接，影响AFL的收敛性，需要研究稀疏化、模型陈旧性和移动性接触模式的相互作用。", "method": "开发理论模型分析稀疏化、模型陈旧性和移动性接触模式对AFL收敛的影响，并提出MADS算法动态优化稀疏化程度。", "result": "实验表明，MADS在CIFAR-10数据集上提高分类准确率8.76%，在Argoverse轨迹预测数据集上减少平均位移误差9.46%。", "conclusion": "MADS算法通过动态调整稀疏化程度，有效提升了AFL在移动设备上的性能。"}}
{"id": "2506.07603", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07603", "abs": "https://arxiv.org/abs/2506.07603", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "comment": null, "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "AI": {"tldr": "SurgBench是一个统一的手术视频基准框架，包含预训练数据集SurgBench-P和评估基准SurgBench-E，旨在解决手术视频基础模型发展中的数据稀缺问题。", "motivation": "手术视频理解对自动化术中决策、技能评估和术后质量改进至关重要，但缺乏大规模多样化数据集阻碍了进展。", "method": "提出SurgBench框架，包含53百万帧的预训练数据集和覆盖72个任务的评估基准，用于测试模型的泛化能力。", "result": "实验表明，现有视频基础模型在多样化任务中泛化能力不足，而基于SurgBench-P的预训练显著提升了性能。", "conclusion": "SurgBench为手术视频分析提供了全面的数据集和评估标准，推动了基础模型的发展。"}}
{"id": "2506.07330", "categories": ["cs.LG", "cs.AI", "cs.CR", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.07330", "abs": "https://arxiv.org/abs/2506.07330", "authors": ["Yash Datta", "Sharath Rajasekar"], "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security", "comment": "16 pages, 1 Figure and 5 Tables", "summary": "We present JavelinGuard, a suite of low-cost, high-performance model\narchitectures designed for detecting malicious intent in Large Language Model\n(LLM) interactions, optimized specifically for production deployment. Recent\nadvances in transformer architectures, including compact BERT(Devlin et al.\n2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build\nhighly accurate classifiers with as few as approximately 400M parameters that\nachieve rapid inference speeds even on standard CPU hardware. We systematically\nexplore five progressively sophisticated transformer-based architectures:\nSharanga (baseline transformer classifier), Mahendra (enhanced\nattention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid\nneural ensemble architectures), and Raudra (an advanced multi-task framework\nwith specialized loss functions). Our models are rigorously benchmarked across\nnine diverse adversarial datasets, including popular sets like the NotInject\nseries, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly\nintroduced JavelinBench, specifically crafted to test generalization on\nchallenging borderline and hard-negative cases. Additionally, we compare our\narchitectures against leading open-source guardrail models as well as large\ndecoder-only LLMs such as gpt-4o, demonstrating superior cost-performance\ntrade-offs in terms of accuracy, and latency. Our findings reveal that while\nRaudra's multi-task design offers the most robust performance overall, each\narchitecture presents unique trade-offs in speed, interpretability, and\nresource requirements, guiding practitioners in selecting the optimal balance\nof complexity and efficiency for real-world LLM security applications.", "AI": {"tldr": "JavelinGuard是一套低成本、高性能的模型架构，用于检测大型语言模型（LLM）交互中的恶意意图，专为生产环境优化。", "motivation": "随着LLM的广泛应用，恶意交互检测成为关键需求，需要高效且低成本的解决方案。", "method": "系统探索了五种基于Transformer的架构，包括基础分类器、增强注意力池化、混合神经网络集成和多任务框架。", "result": "在九种对抗性数据集上测试，Raudra表现最稳健，但各架构在速度、可解释性和资源需求上有不同权衡。", "conclusion": "JavelinGuard提供了多种架构选择，帮助实践者在复杂性和效率之间找到最佳平衡，适用于实际LLM安全应用。"}}
{"id": "2506.07611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07611", "abs": "https://arxiv.org/abs/2506.07611", "authors": ["Yuan Zhou", "Junbao Zhou", "Qingshan Xu", "Kesen Zhao", "Yuxuan Wang", "Hao Fei", "Richang Hong", "Hanwang Zhang"], "title": "DragNeXt: Rethinking Drag-Based Image Editing", "comment": null, "summary": "Drag-Based Image Editing (DBIE), which allows users to manipulate images by\ndirectly dragging objects within them, has recently attracted much attention\nfrom the community. However, it faces two key challenges:\n(\\emph{\\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and\ndifficult to align with users' intentions; (\\emph{\\textcolor{magenta}{ii}})\ncurrent DBIE methods primarily rely on alternating between motion supervision\nand point tracking, which is not only cumbersome but also fails to produce\nhigh-quality results. These limitations motivate us to explore DBIE from a new\nperspective -- redefining it as deformation, rotation, and translation of\nuser-specified handle regions. Thereby, by requiring users to explicitly\nspecify both drag areas and types, we can effectively address the ambiguity\nissue. Furthermore, we propose a simple-yet-effective editing framework, dubbed\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}}. It unifies DBIE as a Latent Region\nOptimization (LRO) problem and solves it through Progressive Backward\nSelf-Intervention (PBSI), simplifying the overall procedure of DBIE while\nfurther enhancing quality by fully leveraging region-level structure\ninformation and progressive guidance from intermediate drag states. We validate\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}} on our NextBench, and extensive\nexperiments demonstrate that our proposed method can significantly outperform\nexisting approaches. Code will be released on github.", "AI": {"tldr": "论文提出DragNeXt，通过将拖拽编辑重新定义为变形、旋转和平移，解决了点拖拽的模糊性问题，并提出了一种简单高效的编辑框架。", "motivation": "当前拖拽图像编辑方法存在点拖拽模糊性和繁琐的交替监督问题，需要改进。", "method": "提出DragNeXt框架，将拖拽编辑统一为潜在区域优化问题，并通过渐进式反向自干预解决。", "result": "实验表明DragNeXt显著优于现有方法。", "conclusion": "DragNeXt简化了拖拽编辑流程并提升了质量，代码将开源。"}}
{"id": "2506.07334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07334", "abs": "https://arxiv.org/abs/2506.07334", "authors": ["Haoyu Wang", "Peihao Wang", "Mufei Li", "Shikun Liu", "Siqi Miao", "Zhangyang Wang", "Pan Li"], "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models", "comment": null, "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.", "AI": {"tldr": "Graph-KV通过利用KV缓存和结构归纳偏置，改进了大型语言模型在处理结构化数据时的性能，显著优于传统序列化方法。", "motivation": "现代大型语言模型（LLM）在处理结构化数据时，由于输入被序列化为扁平序列，无法充分利用结构依赖关系，限制了其在检索增强生成（RAG）和图结构数据推理等任务中的表现。", "method": "Graph-KV利用KV缓存作为文本段的压缩表示，并通过结构归纳偏置控制它们的交互。目标段仅选择性关注其指定的源段KV缓存，而非序列中的所有前驱段，从而引入图结构块掩码和消息传递机制。此外，通过策略性分配位置编码，减少位置偏见和上下文窗口消耗。", "result": "Graph-KV在三个场景（RAG基准测试、Arxiv-QA和论文主题分类）中显著优于基线方法，有效减少了位置偏见并利用了结构归纳偏置。", "conclusion": "Graph-KV通过结合KV缓存和结构归纳偏置，提升了LLM在处理结构化数据时的性能，为相关任务提供了更高效的解决方案。"}}
{"id": "2506.07612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07612", "abs": "https://arxiv.org/abs/2506.07612", "authors": ["Zikang Leng", "Archith Iyer", "Thomas Plötz"], "title": "Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques", "comment": null, "summary": "Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.", "AI": {"tldr": "论文比较了两种虚拟IMU数据生成方法（视频和语言）与传统数据增强技术，发现虚拟数据在有限数据条件下显著提升HAR性能。", "motivation": "解决HAR中标记数据稀缺问题，探索虚拟IMU数据的生成方法。", "method": "构建大规模虚拟IMU数据集，对比视频、语言和传统数据增强方法。", "result": "虚拟IMU数据显著优于真实或增强数据，尤其在数据有限时。", "conclusion": "提供选择数据生成策略的实用建议，并分析各方法的优缺点。"}}
{"id": "2506.07355", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.07355", "abs": "https://arxiv.org/abs/2506.07355", "authors": ["Yuya Okada", "Takayuki Nishio"], "title": "SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments", "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model\nadaptation framework for Split Computing under closed constraints, where the\nhead and tail networks are proprietary and inaccessible to users. In such\nclosed environments, conventional adaptation methods are infeasible since they\nrequire access to model parameters or architectures. SALT addresses this\nchallenge by introducing a compact, trainable adapter on the client side to\nrefine latent features from the head network, enabling user-specific adaptation\nwithout modifying the original models or increasing communication overhead. We\nevaluate SALT on user-specific classification tasks with CIFAR-10 and\nCIFAR-100, demonstrating improved accuracy with lower training latency compared\nto fine-tuning methods. Furthermore, SALT facilitates model adaptation for\nrobust inference over lossy networks, a common challenge in edge-cloud\nenvironments. With minimal deployment overhead, SALT offers a practical\nsolution for personalized inference in edge AI systems under strict system\nconstraints.", "AI": {"tldr": "SALT是一种轻量级模型适应框架，用于在封闭约束下的分割计算，通过客户端可训练适配器优化特征，无需修改原始模型。", "motivation": "在封闭环境中，传统适应方法因无法访问模型参数或架构而不可行，SALT旨在解决这一问题。", "method": "引入紧凑的可训练适配器，优化头部网络的潜在特征，支持用户特定适应，同时不增加通信开销。", "result": "在CIFAR-10和CIFAR-100上验证了更高的准确率和更低的训练延迟，适用于边缘AI系统。", "conclusion": "SALT为严格系统约束下的个性化推理提供了实用解决方案。"}}
{"id": "2506.07627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07627", "abs": "https://arxiv.org/abs/2506.07627", "authors": ["Haotong Qin", "Cheng Hu", "Michele Magno"], "title": "Event-Priori-Based Vision-Language Model for Efficient Visual Understanding", "comment": null, "summary": "Large Language Model (LLM)-based Vision-Language Models (VLMs) have\nsubstantially extended the boundaries of visual understanding capabilities.\nHowever, their high computational demands hinder deployment on\nresource-constrained edge devices. A key source of inefficiency stems from the\nVLM's need to process dense and redundant visual information. Visual inputs\ncontain significant regions irrelevant to text semantics, rendering the\nassociated computations ineffective for inference. This paper introduces a\nnovel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core\ncontribution is a novel mechanism leveraging motion priors derived from dynamic\nevent vision to enhance VLM efficiency. Inspired by human visual cognition,\nEP-VLM first employs event data to guide the patch-wise sparsification of RGB\nvisual inputs, progressively concentrating VLM computation on salient regions\nof the visual input. Subsequently, we construct a position-preserving\ntokenization strategy for the visual encoder within the VLM architecture. This\nstrategy processes the event-guided, unstructured, sparse visual input while\naccurately preserving positional understanding within the visual input.\nExperimental results demonstrate that EP-VLM achieves significant efficiency\nimprovements while maintaining nearly lossless accuracy compared to baseline\nmodels from the Qwen2-VL series. For instance, against the original\nQwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the\noriginal accuracy on the RealWorldQA dataset. This work demonstrates the\npotential of event-based vision priors for improving VLM inference efficiency,\npaving the way for creating more efficient and deployable VLMs for sustainable\nvisual understanding at the edge.", "AI": {"tldr": "EP-VLM利用动态事件视觉的运动先验，通过事件数据引导RGB视觉输入的稀疏化，显著提升VLM效率，同时保持高精度。", "motivation": "现有VLM计算密集且冗余，阻碍在资源受限设备上的部署。", "method": "提出EP-VLM，利用事件数据稀疏化视觉输入，并设计位置保留的标记化策略。", "result": "在Qwen2-VL-2B上实现50%计算量节省，精度仅损失2%。", "conclusion": "事件视觉先验可显著提升VLM效率，推动边缘设备部署。"}}
{"id": "2506.07366", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.07366", "abs": "https://arxiv.org/abs/2506.07366", "authors": ["Haiyue Ma", "Zhixu Du", "Yiran Chen"], "title": "MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing", "comment": null, "summary": "In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across\ndifferent GPUs, which creates load imbalance as each expert processes different\nnumber of tokens. Recent works improve MoE inference load balance by\ndynamically duplicating popular experts to more GPUs to process excessive\ntokens, which requires predicting the distribution before routing. In this\npaper, we discuss the tradeoff of prediction strategies, accuracies, overhead,\nand end-to-end system performance. We propose MoE-GPS, a framework that guides\nthe selection of the optimal predictor design under various system\nconfigurations, by quantifying the performance impact to system-level model\nruntime. Specifically, we advocate for Distribution-Only Prediction, a\nprediction strategy that only predicts overall token distribution which\nsignificantly reduces overhead compared to the traditional Token-to-Expert\nPrediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only\nPrediction which improves end-to-end inference performance by more than 23%\ncompared with Token-to-Expert Prediction.", "AI": {"tldr": "论文提出MoE-GPS框架，通过量化系统级模型运行时间的影响，指导选择最优预测器设计，显著提升多GPU MoE网络的推理性能。", "motivation": "多GPU MoE网络中专家分布不均导致负载不平衡，现有方法通过动态复制热门专家来平衡负载，但需预测路由前的分布。", "method": "提出MoE-GPS框架，量化预测策略对系统性能的影响，推荐仅预测总体令牌分布的Distribution-Only Prediction策略。", "result": "在Mixtral 8x7B MMLU数据集上，Distribution-Only Prediction比传统Token-to-Expert Prediction提升推理性能23%以上。", "conclusion": "MoE-GPS为多GPU MoE网络提供高效预测策略选择，显著优化系统性能。"}}
{"id": "2506.07628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07628", "abs": "https://arxiv.org/abs/2506.07628", "authors": ["Weronika Smolak-Dyżewska", "Dawid Malarz", "Grzegorz Wilczyński", "Rafał Tobiasz", "Joanna Waczyńska", "Piotr Borycki", "Przemysław Spurek"], "title": "HuSc3D: Human Sculpture dataset for 3D object reconstruction", "comment": null, "summary": "3D scene reconstruction from 2D images is one of the most important tasks in\ncomputer graphics. Unfortunately, existing datasets and benchmarks concentrate\non idealized synthetic or meticulously captured realistic data. Such benchmarks\nfail to convey the inherent complexities encountered in newly acquired\nreal-world scenes. In such scenes especially those acquired outside, the\nbackground is often dynamic, and by popular usage of cell phone cameras, there\nmight be discrepancies in, e.g., white balance. To address this gap, we present\nHuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D\nreconstruction models under realistic acquisition challenges. Our dataset\nuniquely features six highly detailed, fully white sculptures characterized by\nintricate perforations and minimal textural and color variation. Furthermore,\nthe number of images per scene varies significantly, introducing the additional\nchallenge of limited training data for some instances alongside scenes with a\nstandard number of views. By evaluating popular 3D reconstruction methods on\nthis diverse dataset, we demonstrate the distinctiveness of HuSc3D in\neffectively differentiating model performance, particularly highlighting the\nsensitivity of methods to fine geometric details, color ambiguity, and varying\ndata availability--limitations often masked by more conventional datasets.", "AI": {"tldr": "HuSc3D是一个专为3D重建模型在真实采集挑战下进行严格基准测试而设计的新数据集，填补了现有数据集在动态背景和颜色差异等现实问题上的不足。", "motivation": "现有数据集多集中于理想化合成或精细捕捉的真实数据，未能反映新采集现实场景中的复杂性，如动态背景和颜色差异。", "method": "HuSc3D数据集包含六个高度细节化的白色雕塑，具有复杂穿孔和最小纹理变化，且每场景图像数量差异显著。", "result": "评估显示HuSc3D能有效区分模型性能，突显方法对几何细节、颜色模糊和数据变化的敏感性。", "conclusion": "HuSc3D填补了现有数据集的不足，为3D重建模型在现实挑战下的评估提供了独特工具。"}}
{"id": "2506.07378", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07378", "abs": "https://arxiv.org/abs/2506.07378", "authors": ["Yuen Chen", "Haozhe Si", "Guojun Zhang", "Han Zhao"], "title": "Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization", "comment": "UAI 2025", "summary": "Domain generalization (DG) seeks to develop models that generalize well to\nunseen target domains, addressing the prevalent issue of distribution shifts in\nreal-world applications. One line of research in DG focuses on aligning\ndomain-level gradients and Hessians to enhance generalization. However,\nexisting methods are computationally inefficient and the underlying principles\nof these approaches are not well understood. In this paper, we develop the\ntheory of moment alignment for DG. Grounded in \\textit{transfer measure}, a\nprincipled framework for quantifying generalizability between two domains, we\nfirst extend the definition of transfer measure to domain generalization that\nincludes multiple source domains and establish a target error bound. Then, we\nprove that aligning derivatives across domains improves transfer measure both\nwhen the feature extractor induces an invariant optimal predictor across\ndomains and when it does not. Notably, moment alignment provides a unifying\nunderstanding of Invariant Risk Minimization, gradient matching, and Hessian\nmatching, three previously disconnected approaches to DG. We further connect\nfeature moments and derivatives of the classifier head, and establish the\nduality between feature learning and classifier fitting. Building upon our\ntheory, we introduce \\textbf{C}losed-Form \\textbf{M}oment \\textbf{A}lignment\n(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in\nclosed-form. Our method overcomes the computational inefficiencies of existing\ngradient and Hessian-based techniques by eliminating the need for repeated\nbackpropagation or sampling-based Hessian estimation. We validate the efficacy\nof our approach through two sets of experiments: linear probing and full\nfine-tuning. CMA demonstrates superior performance in both settings compared to\nEmpirical Risk Minimization and state-of-the-art algorithms.", "AI": {"tldr": "本文提出了一种基于矩对齐的域泛化理论，并通过闭式矩对齐（CMA）算法高效实现梯度与Hessian对齐，显著提升了模型在未见目标域上的泛化性能。", "motivation": "解决现有域泛化方法计算效率低且理论基础不足的问题，提出一种统一的理论框架。", "method": "基于传递度量理论，扩展其定义至多源域场景，并证明对齐导数能提升传递度量。提出CMA算法，闭式对齐梯度和Hessian。", "result": "CMA在线性探测和全微调实验中均优于经验风险最小化和现有先进算法。", "conclusion": "矩对齐为域泛化提供了统一的理论视角，CMA算法高效且性能优越。"}}
{"id": "2506.07637", "categories": ["cs.CV", "cs.LG", "68T07, 68T45", "I.2.10; I.4.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2506.07637", "abs": "https://arxiv.org/abs/2506.07637", "authors": ["Yuchong Long", "Wen Sun", "Ningxiao Sun", "Wenxiao Wang", "Chao Li", "Shan Yin"], "title": "HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition", "comment": "16 pages, 5 figures, 2 tables. The dataset at\n  https://www.kaggle.com/datasets/ayinven/hieraedgenetintegratesdatasets. The\n  models at\n  https://huggingface.co/datasets/AyinMostima/HieraEdgeNetintegratesdatasets.\n  The source code in at https://github.com/AyinMostima/PalynoKit", "summary": "Automated pollen recognition is vital to paleoclimatology, biodiversity\nmonitoring, and public health, yet conventional methods are hampered by\ninefficiency and subjectivity. Existing deep learning models often struggle to\nachieve the requisite localization accuracy for microscopic targets like\npollen, which are characterized by their minute size, indistinct edges, and\ncomplex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a\nmulti-scale edge-enhancement framework. The framework's core innovation is the\nintroduction of three synergistic modules: the Hierarchical Edge Module (HEM),\nwhich explicitly extracts a multi-scale pyramid of edge features that\ncorresponds to the semantic hierarchy at early network stages; the Synergistic\nEdge Fusion (SEF) module, for deeply fusing these edge priors with semantic\ninformation at each respective scale; and the Cross Stage Partial Omni-Kernel\nModule (CSPOKM), which maximally refines the most detail-rich feature layers\nusing an Omni-Kernel operator - comprising anisotropic large-kernel\nconvolutions and mixed-domain attention - all within a computationally\nefficient Cross-Stage Partial (CSP) framework. On a large-scale dataset\ncomprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision\n(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline\nmodels such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms\nthat our approach generates feature representations that are more precisely\nfocused on object boundaries. By systematically integrating edge information,\nHieraEdgeNet provides a robust and powerful solution for high-precision,\nhigh-efficiency automated detection of microscopic objects.", "AI": {"tldr": "HieraEdgeNet是一种多尺度边缘增强框架，显著提升了显微镜下微小目标（如花粉）的识别精度，优于现有模型。", "motivation": "传统花粉识别方法效率低且主观性强，现有深度学习模型对微小目标的定位精度不足。", "method": "提出HieraEdgeNet框架，包含三个模块：HEM提取多尺度边缘特征，SEF融合边缘与语义信息，CSPOKM优化细节特征。", "result": "在120类花粉数据集上，mAP@.5达0.9501，优于YOLOv12n和RT-DETR。", "conclusion": "HieraEdgeNet通过系统整合边缘信息，为高精度、高效率的微小目标检测提供了强大解决方案。"}}
{"id": "2506.07405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07405", "abs": "https://arxiv.org/abs/2506.07405", "authors": ["Zhongping Ji"], "title": "RiemannFormer: A Framework for Attention in Curved Spaces", "comment": "10 pages, 1 figure", "summary": "This research endeavors to offer insights into unlocking the further\npotential of transformer-based architectures. One of the primary motivations is\nto offer a geometric interpretation for the attention mechanism in\ntransformers. In our framework, the attention mainly involves metric tensors,\ntangent spaces, inner product, and how they relate to each other. These\nquantities and structures at discrete positions are intricately interconnected\nvia the parallel transport of tangent vectors. To make the learning process\nmore efficient, we reduce the number of parameters through ingenious predefined\nconfigurations. Moreover, we introduce an explicit mechanism to highlight a\nneighborhood by attenuating the remote values, given that transformers\ninherently neglect local inductive bias. Experimental results demonstrate that\nour modules deliver significant performance improvements relative to the\nbaseline. More evaluation experiments on visual and large language models will\nbe launched successively.", "AI": {"tldr": "该研究提出了一种基于几何视角的Transformer注意力机制解释，并通过减少参数和引入局部邻域机制提升性能。", "motivation": "为Transformer的注意力机制提供几何解释，并解决其忽视局部归纳偏置的问题。", "method": "利用度量张量、切空间和内积等几何概念构建框架，通过并行传输连接离散位置的结构，同时减少参数并引入局部邻域机制。", "result": "实验表明，该方法显著提升了基线模型的性能。", "conclusion": "该框架为Transformer提供了新的几何视角，并在性能上取得改进，未来将在视觉和大语言模型上进一步验证。"}}
{"id": "2506.07643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07643", "abs": "https://arxiv.org/abs/2506.07643", "authors": ["Jae Sung Park", "Zixian Ma", "Linjie Li", "Chenhao Zheng", "Cheng-Yu Hsieh", "Ximing Lu", "Khyathi Chandu", "Quan Kong", "Norimasa Kobori", "Ali Farhadi", "Yejin Choi", "Ranjay Krishna"], "title": "Synthetic Visual Genome", "comment": "CVPR 2025", "summary": "Reasoning over visual relationships-spatial, functional, interactional,\nsocial, etc.-is considered to be a fundamental component of human cognition.\nYet, despite the major advances in visual comprehension in multimodal language\nmodels (MLMs), precise reasoning over relationships and their generations\nremains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely\nannotated relationships capable of constructing high-quality dense scene graphs\nat scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by\ncompleting the missing relations of selected objects in existing scene graphs\nusing a teacher MLM and a carefully designed filtering process to ensure\nhigh-quality. To generate more accurate and rich scene graphs at scale for any\nimage, we introduce SG-EDIT: a self-distillation framework where GPT-4o further\nrefines ROBIN's predicted scene graphs by removing unlikely relations and/or\nsuggesting relevant ones. In total, our dataset contains 146K images and 5.6M\nrelationships for 2.6M objects. Results show that our ROBIN-3B model, despite\nbeing trained on less than 3 million instances, outperforms similar-size models\ntrained on over 300 million instances on relationship understanding benchmarks,\nand even surpasses larger models up to 13B parameters. Notably, it achieves\nstate-of-the-art performance in referring expression comprehension with a score\nof 88.9, surpassing the previous best of 87.4. Our results suggest that\ntraining on the refined scene graph data is crucial to maintaining high\nperformance across diverse visual reasoning task.", "AI": {"tldr": "论文提出ROBIN模型，通过密集标注的关系训练，能够生成高质量的场景图，并通过SG-EDIT框架进一步优化。ROBIN-3B模型在关系理解任务中表现优异，甚至超越更大规模的模型。", "motivation": "尽管多模态语言模型在视觉理解方面取得进展，但对视觉关系的精确推理和生成仍具挑战性。", "method": "使用合成场景图数据集SVG训练ROBIN模型，并通过SG-EDIT框架利用GPT-4o优化生成的场景图。", "result": "ROBIN-3B模型在关系理解任务中表现优于同类模型，并在指代表达理解任务中达到88.9分，刷新记录。", "conclusion": "通过高质量场景图数据训练对提升视觉推理任务性能至关重要。"}}
{"id": "2506.07406", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07406", "abs": "https://arxiv.org/abs/2506.07406", "authors": ["Yifan Luo", "Zhennan Zhou", "Bin Dong"], "title": "InverseScope: Scalable Activation Inversion for Interpreting Large Language Models", "comment": "18 pages, 8 figures", "summary": "Understanding the internal representations of large language models (LLMs) is\na central challenge in interpretability research. Existing feature\ninterpretability methods often rely on strong assumptions about the structure\nof representations that may not hold in practice. In this work, we introduce\nInverseScope, an assumption-light and scalable framework for interpreting\nneural activations via input inversion. Given a target activation, we define a\ndistribution over inputs that generate similar activations and analyze this\ndistribution to infer the encoded features. To address the inefficiency of\nsampling in high-dimensional spaces, we propose a novel conditional generation\narchitecture that significantly improves sample efficiency compared to previous\nmethods. We further introduce a quantitative evaluation protocol that tests\ninterpretability hypotheses using feature consistency rate computed over the\nsampled inputs. InverseScope scales inversion-based interpretability methods to\nlarger models and practical tasks, enabling systematic and quantitative\nanalysis of internal representations in real-world LLMs.", "AI": {"tldr": "InverseScope是一个轻假设、可扩展的框架，通过输入反演解释神经激活，显著提高了样本效率，并支持对大型语言模型内部表示的系统分析。", "motivation": "理解大型语言模型（LLMs）的内部表示是解释性研究的核心挑战，现有方法依赖的假设在实践中可能不成立。", "method": "提出InverseScope框架，通过定义目标激活的输入分布并分析其编码特征，结合条件生成架构提高采样效率。", "result": "InverseScope显著提升了样本效率，支持对大型语言模型内部表示的系统化和定量分析。", "conclusion": "InverseScope为解释大型语言模型的内部表示提供了一种高效、可扩展的方法。"}}
{"id": "2506.07652", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07652", "abs": "https://arxiv.org/abs/2506.07652", "authors": ["Hangbei Cheng", "Xiaorong Dong", "Xueyu Liu", "Jianan Zhang", "Xuetao Ma", "Mingqiang Wei", "Liansheng Wang", "Junxin Chen", "Yongfei Wu"], "title": "FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images", "comment": null, "summary": "Accurate lesion segmentation in histopathology images is essential for\ndiagnostic interpretation and quantitative analysis, yet it remains challenging\ndue to the limited availability of costly pixel-level annotations. To address\nthis, we propose FMaMIL, a novel two-stage framework for weakly supervised\nlesion segmentation based solely on image-level labels. In the first stage, a\nlightweight Mamba-based encoder is introduced to capture long-range\ndependencies across image patches under the MIL paradigm. To enhance spatial\nsensitivity and structural awareness, we design a learnable frequency-domain\nencoding module that supplements spatial-domain features with spectrum-based\ninformation. CAMs generated in this stage are used to guide segmentation\ntraining. In the second stage, we refine the initial pseudo labels via a\nCAM-guided soft-label supervision and a self-correction mechanism, enabling\nrobust training even under label noise. Extensive experiments on both public\nand private histopathology datasets demonstrate that FMaMIL outperforms\nstate-of-the-art weakly supervised methods without relying on pixel-level\nannotations, validating its effectiveness and potential for digital pathology\napplications.", "AI": {"tldr": "FMaMIL是一种基于图像级标签的弱监督病灶分割框架，通过两阶段方法实现高效分割，无需像素级标注。", "motivation": "解决组织病理学图像中病灶分割的挑战，尤其是像素级标注成本高的问题。", "method": "两阶段框架：第一阶段使用Mamba编码器和频域编码模块生成CAMs；第二阶段通过软标签监督和自校正机制优化伪标签。", "result": "在公共和私有数据集上表现优于现有弱监督方法。", "conclusion": "FMaMIL在数字病理学中具有潜力，无需依赖像素级标注。"}}
{"id": "2506.07407", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07407", "abs": "https://arxiv.org/abs/2506.07407", "authors": ["Yihong Jin", "Ze Yang", "Juntian Liu", "Xinhe Xu"], "title": "Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM", "comment": "Proceedings of 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the rapid development of multi-cloud environments, it is increasingly\nimportant to ensure the security and reliability of intelligent monitoring\nsystems. In this paper, we propose an anomaly detection and early warning\nmechanism for intelligent monitoring system in multi-cloud environment based on\nLarge-Scale Language Model (LLM). On the basis of the existing monitoring\nframework, the proposed model innovatively introduces a multi-level feature\nextraction method, which combines the natural language processing ability of\nLLM with traditional machine learning methods to enhance the accuracy of\nanomaly detection and improve the real-time response efficiency. By introducing\nthe contextual understanding capabilities of LLMs, the model dynamically adapts\nto different cloud service providers and environments, so as to more\neffectively detect abnormal patterns and predict potential failures.\nExperimental results show that the proposed model is significantly better than\nthe traditional anomaly detection system in terms of detection accuracy and\nlatency, and significantly improves the resilience and active management\nability of cloud infrastructure.", "AI": {"tldr": "提出了一种基于大语言模型（LLM）的多云环境智能监控系统异常检测与预警机制，结合多级特征提取方法，显著提升了检测精度和实时响应效率。", "motivation": "随着多云环境的快速发展，确保智能监控系统的安全性和可靠性变得日益重要。", "method": "在现有监控框架基础上，创新性地引入多级特征提取方法，结合LLM的自然语言处理能力和传统机器学习方法，动态适应不同云服务提供商和环境。", "result": "实验结果表明，该模型在检测精度和延迟方面显著优于传统异常检测系统，并显著提升了云基础设施的弹性和主动管理能力。", "conclusion": "该研究为多云环境下的智能监控系统提供了一种高效、可靠的异常检测与预警解决方案。"}}
{"id": "2506.07670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07670", "abs": "https://arxiv.org/abs/2506.07670", "authors": ["Xiaohan Lu", "Jiaye Fu", "Jiaqi Zhang", "Zetian Song", "Chuanmin Jia", "Siwei Ma"], "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views", "comment": null, "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising\nresults for novel view synthesis (NVS) from sparse input views, particularly\nunder narrow-baseline conditions. However, its performance significantly\ndegrades in wide-baseline scenarios due to limited texture details and\ngeometric inconsistencies across views. To address these challenges, in this\npaper, we propose ProSplat, a two-stage feed-forward framework designed for\nhigh-fidelity rendering under wide-baseline conditions. The first stage\ninvolves generating 3D Gaussian primitives via a 3DGS generator. In the second\nstage, rendered views from these primitives are enhanced through an improvement\nmodel. Specifically, this improvement model is based on a one-step diffusion\nmodel, further optimized by our proposed Maximum Overlap Reference view\nInjection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI\nsupplements missing texture and color by strategically selecting a reference\nview with maximum viewpoint overlap, while DWEA enforces geometric consistency\nusing epipolar constraints. Additionally, we introduce a divide-and-conquer\ntraining strategy that aligns data distributions between the two stages through\njoint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K\ndatasets under wide-baseline settings. Experimental results demonstrate that\nProSplat achieves an average improvement of 1 dB in PSNR compared to recent\nSOTA methods.", "AI": {"tldr": "ProSplat是一个两阶段前馈框架，用于在宽基线条件下实现高保真渲染，通过3D高斯基元生成和扩散模型增强视图，显著提升了性能。", "motivation": "解决3D高斯溅射在宽基线场景下因纹理细节不足和几何不一致导致的性能下降问题。", "method": "两阶段框架：1) 生成3D高斯基元；2) 通过扩散模型增强渲染视图，结合MORI和DWEA优化。", "result": "在RealEstate10K和DL3DV-10K数据集上，PSNR平均提升1 dB。", "conclusion": "ProSplat在宽基线条件下显著提升了渲染质量，优于现有方法。"}}
{"id": "2506.07408", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07408", "abs": "https://arxiv.org/abs/2506.07408", "authors": ["Xiaojun zhou", "Chunna Zhao", "Yaqun Huang", "Chengli Zhou", "Junjie Ye", "Kemeng Xiang"], "title": "Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks", "comment": null, "summary": "Fractional-order differentiation has many characteristics different from\ninteger-order differentiation. These characteristics can be applied to the\noptimization algorithms of artificial neural networks to obtain better results.\nHowever, due to insufficient theoretical research, at present, there is no\nfractional-order matrix differentiation method that is perfectly compatible\nwith automatic differentiation (Autograd) technology. Therefore, we propose a\nfractional-order matrix differentiation calculation method. This method is\nintroduced by the definition of the integer-order Jacobian matrix. We denote it\nas fractional-order Jacobian matrix differentiation (${{\\bf{J}}^\\alpha }$).\nThrough ${{\\bf{J}}^\\alpha }$, we can carry out the matrix-based\nfractional-order chain rule. Based on the Linear module and the\nfractional-order differentiation, we design the fractional-order Autograd\ntechnology to enable the use of fractional-order differentiation in hidden\nlayers, thereby enhancing the practicality of fractional-order differentiation\nin deep learning. In the experiment, according to the PyTorch framework, we\ndesign fractional-order Linear (FLinear) and replace nn.Linear in the\nmultilayer perceptron with FLinear. Through the qualitative analysis of the\ntraining set and validation set $Loss$, the quantitative analysis of the test\nset indicators, and the analysis of time consumption and GPU memory usage\nduring model training, we verify the superior performance of ${{\\bf{J}}^\\alpha\n}$ and prove that it is an excellent fractional-order gradient descent method\nin the field of deep learning.", "AI": {"tldr": "提出了一种基于分数阶雅可比矩阵的矩阵微分方法（${{\\bf{J}}^\\alpha }$），用于实现分数阶自动微分技术，提升深度学习中的分数阶微分实用性。", "motivation": "当前缺乏与自动微分技术完美兼容的分数阶矩阵微分方法，限制了分数阶微分在神经网络优化中的应用。", "method": "通过整数阶雅可比矩阵定义引入分数阶雅可比矩阵微分（${{\\bf{J}}^\\alpha }$），设计分数阶自动微分技术，并在PyTorch框架中实现分数阶线性模块（FLinear）。", "result": "实验验证了${{\\bf{J}}^\\alpha }$在训练集、验证集和测试集上的优越性能，以及其在时间和GPU内存消耗上的高效性。", "conclusion": "${{\\bf{J}}^\\alpha }$是一种优秀的分数阶梯度下降方法，适用于深度学习领域。"}}
{"id": "2506.07697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07697", "abs": "https://arxiv.org/abs/2506.07697", "authors": ["Jens Piekenbrinck", "Christian Schmidt", "Alexander Hermans", "Narunas Vaskevicius", "Timm Linder", "Bastian Leibe"], "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nneural scene reconstruction, offering high-quality novel view synthesis while\nmaintaining computational efficiency. In this paper, we extend the capabilities\nof 3DGS beyond pure scene representation by introducing an approach for\nopen-vocabulary 3D instance segmentation without requiring manual labeling,\ntermed OpenSplat3D. Our method leverages feature-splatting techniques to\nassociate semantic information with individual Gaussians, enabling fine-grained\nscene understanding. We incorporate Segment Anything Model instance masks with\na contrastive loss formulation as guidance for the instance features to achieve\naccurate instance-level segmentation. Furthermore, we utilize language\nembeddings of a vision-language model, allowing for flexible, text-driven\ninstance identification. This combination enables our system to identify and\nsegment arbitrary objects in 3D scenes based on natural language descriptions.\nWe show results on LERF-mask and LERF-OVS as well as the full ScanNet++\nvalidation set, demonstrating the effectiveness of our approach.", "AI": {"tldr": "OpenSplat3D扩展了3D高斯溅射（3DGS）的能力，实现了无需手动标注的开放词汇3D实例分割。", "motivation": "将3DGS从纯场景表示扩展到支持开放词汇的实例分割，以提升场景理解的细粒度。", "method": "结合特征溅射技术、Segment Anything Model实例掩码和对比损失，利用视觉语言模型的语言嵌入实现文本驱动的实例识别。", "result": "在LERF-mask、LERF-OVS和ScanNet++验证集上展示了方法的有效性。", "conclusion": "OpenSplat3D能够基于自然语言描述识别和分割3D场景中的任意对象。"}}
{"id": "2506.07413", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07413", "abs": "https://arxiv.org/abs/2506.07413", "authors": ["Ziwen Wang", "Jiajun Fan", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "Variational Supervised Contrastive Learning", "comment": null, "summary": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies.", "AI": {"tldr": "VarCon提出了一种变分监督对比学习方法，通过变分推断优化类感知匹配，解决了传统对比学习中的分布调控和泛化问题，并在多个数据集上实现了SOTA性能。", "motivation": "传统对比学习存在嵌入分布调控不足和泛化能力受限的问题，VarCon旨在通过变分推断优化类感知匹配和嵌入空间控制。", "method": "VarCon将监督对比学习重新表述为对潜在类变量的变分推断，最大化后验加权的ELBO，取代了耗尽的成对比较。", "result": "在CIFAR-10、CIFAR-100、ImageNet-100和ImageNet-1K上，VarCon实现了79.36%的Top-1准确率，收敛仅需200轮，且表现出更清晰的决策边界和语义组织。", "conclusion": "VarCon在性能、泛化能力和鲁棒性上均优于传统方法，为对比学习提供了更高效的框架。"}}
{"id": "2506.07698", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07698", "abs": "https://arxiv.org/abs/2506.07698", "authors": ["Yuxiao Yang", "Peihao Li", "Yuhong Zhang", "Junzhe Lu", "Xianglong He", "Minghan Qin", "Weitao Wang", "Haoqian Wang"], "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation", "comment": "8 pages, 7 figures, accepted by ICME 2025", "summary": "3D AI-generated content (AIGC) has made it increasingly accessible for anyone\nto become a 3D content creator. While recent methods leverage Score\nDistillation Sampling to distill 3D objects from pretrained image diffusion\nmodels, they often suffer from inadequate 3D priors, leading to insufficient\nmulti-view consistency. In this work, we introduce NOVA3D, an innovative\nsingle-image-to-3D generation framework. Our key insight lies in leveraging\nstrong 3D priors from a pretrained video diffusion model and integrating\ngeometric information during multi-view video fine-tuning. To facilitate\ninformation exchange between color and geometric domains, we propose the\nGeometry-Temporal Alignment (GTA) attention mechanism, thereby improving\ngeneralization and multi-view consistency. Moreover, we introduce the\nde-conflict geometry fusion algorithm, which improves texture fidelity by\naddressing multi-view inaccuracies and resolving discrepancies in pose\nalignment. Extensive experiments validate the superiority of NOVA3D over\nexisting baselines.", "AI": {"tldr": "NOVA3D是一个创新的单图像到3D生成框架，通过利用预训练视频扩散模型的强3D先验和几何信息，提升多视角一致性。", "motivation": "解决现有方法因缺乏3D先验导致的多视角一致性不足问题。", "method": "利用预训练视频扩散模型的3D先验，结合几何信息进行多视角视频微调，并提出了Geometry-Temporal Alignment (GTA)注意力机制和去冲突几何融合算法。", "result": "实验验证了NOVA3D在生成3D内容时优于现有基线方法。", "conclusion": "NOVA3D通过整合3D先验和几何信息，显著提升了多视角一致性和纹理保真度。"}}
{"id": "2506.07416", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07416", "abs": "https://arxiv.org/abs/2506.07416", "authors": ["Jin Huang", "Yuchao Jin", "Le An", "Josh Park"], "title": "LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments", "comment": null, "summary": "This paper introduces an efficient Vision-Language Model (VLM) pipeline\nspecifically optimized for deployment on embedded devices, such as those used\nin robotics and autonomous driving. The pipeline significantly reduces the\ncomputational overhead by jointly leveraging patch selection to filter\nirrelevant camera views, a token selection module to reduce input sequence\nlength for the LLM, and speculative decoding to accelerate token generation.\nEvaluation on the NVIDIA DRIVE Thor platform for automonous driving\napplication, our pipeline achieves $2.5\\times$ end-to-end latency reduction\nwithout compromising task accuracy. The speed-up further increases to\n$3.2\\times$ when applying FP8 post-training quantization. These results\ndemonstrate our pipeline as a viable solution for enabling real-time VLM\ndeployment in resource-constrained environments.", "AI": {"tldr": "提出了一种针对嵌入式设备优化的高效视觉语言模型（VLM）流水线，显著降低计算开销并提升实时性能。", "motivation": "解决在资源受限的嵌入式设备（如机器人和自动驾驶）上部署视觉语言模型时的高计算开销问题。", "method": "结合补丁选择过滤无关视图、令牌选择模块减少LLM输入序列长度，以及推测解码加速令牌生成。", "result": "在NVIDIA DRIVE Thor平台上，流水线实现2.5倍端到端延迟降低，FP8量化后提升至3.2倍，且不影响任务准确性。", "conclusion": "该流水线是资源受限环境中实时部署VLM的可行解决方案。"}}
{"id": "2506.07705", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.07705", "abs": "https://arxiv.org/abs/2506.07705", "authors": ["Weilei Wen", "Chunle Guo", "Wenqi Ren", "Hongpeng Wang", "Xiuli Shao"], "title": "Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations", "comment": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "summary": "Prior methodologies have disregarded the diversities among distinct\ndegradation types during image reconstruction, employing a uniform network\nmodel to handle multiple deteriorations. Nevertheless, we discover that\nprevalent degradation modalities, including sampling, blurring, and noise, can\nbe roughly categorized into two classes. We classify the first class as\nspatial-agnostic dominant degradations, less affected by regional changes in\nimage space, such as downsampling and noise degradation. The second class\ndegradation type is intimately associated with the spatial position of the\nimage, such as blurring, and we identify them as spatial-specific dominant\ndegradations. We introduce a dynamic filter network integrating global and\nlocal branches to address these two degradation types. This network can greatly\nalleviate the practical degradation problem. Specifically, the global dynamic\nfiltering layer can perceive the spatial-agnostic dominant degradation in\ndifferent images by applying weights generated by the attention mechanism to\nmultiple parallel standard convolution kernels, enhancing the network's\nrepresentation ability. Meanwhile, the local dynamic filtering layer converts\nfeature maps of the image into a spatially specific dynamic filtering operator,\nwhich performs spatially specific convolution operations on the image features\nto handle spatial-specific dominant degradations. By effectively integrating\nboth global and local dynamic filtering operators, our proposed method\noutperforms state-of-the-art blind super-resolution algorithms in both\nsynthetic and real image datasets.", "AI": {"tldr": "论文提出了一种动态滤波网络，通过全局和局部分支处理两种主要退化类型，显著提升了图像重建性能。", "motivation": "现有方法忽视了不同退化类型的多样性，采用统一网络模型处理多种退化，效果不佳。", "method": "引入动态滤波网络，分为全局和局部分支，分别处理空间无关和空间相关的退化类型。", "result": "在合成和真实图像数据集上，该方法优于现有盲超分辨率算法。", "conclusion": "动态滤波网络能有效处理多种退化类型，提升图像重建质量。"}}
{"id": "2506.07417", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07417", "abs": "https://arxiv.org/abs/2506.07417", "authors": ["Nan Sun", "Xixun Lin", "Zhiheng Zhou", "Yanmin Shang", "Zhenlin Cheng", "Yanan Cao"], "title": "Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs", "comment": "17 pages,5 figures", "summary": "Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims\nto identify whether incoming data deviates from the distribution of the\nin-distribution (ID) training set, has garnered considerable attention in\nsecurity-sensitive fields. Current OOD detection paradigms primarily focus on\nstatic graphs and confront two critical challenges: i) high bias and high\nvariance caused by single-point estimation, which makes the predictions\nsensitive to randomness in the data; ii) score homogenization resulting from\nthe lack of OOD training data, where the model only learns ID-specific\npatterns, resulting in overall low OOD scores and a narrow score gap between ID\nand OOD data. To tackle these issues, we first investigate OOD detection in\ndynamic graphs through the lens of Evidential Deep Learning (EDL).\nSpecifically, we propose EviSEC, an innovative and effective OOD detector via\nEvidential Spectrum-awarE Contrastive Learning. We design an evidential neural\nnetwork to redefine the output as the posterior Dirichlet distribution,\nexplaining the randomness of inputs through the uncertainty of distribution,\nwhich is overlooked by single-point estimation. Moreover, spectrum-aware\naugmentation module generates OOD approximations to identify patterns with high\nOOD scores, thereby widening the score gap between ID and OOD data and\nmitigating score homogenization. Extensive experiments on real-world datasets\ndemonstrate that EviSAC effectively detects OOD samples in dynamic graphs.", "AI": {"tldr": "论文提出EviSEC方法，通过证据深度学习和频谱感知对比学习解决动态图中OOD检测的高偏差、高方差和分数同质化问题。", "motivation": "动态图中的OOD检测在安全敏感领域备受关注，但现有方法主要针对静态图，存在高偏差、高方差和分数同质化问题。", "method": "提出EviSEC方法，结合证据神经网络和频谱感知增强模块，通过后验Dirichlet分布解释输入随机性，并生成OOD近似样本以扩大ID与OOD分数差距。", "result": "在真实数据集上的实验表明，EviSEC能有效检测动态图中的OOD样本。", "conclusion": "EviSEC通过结合证据深度学习和频谱感知对比学习，显著提升了动态图中OOD检测的性能。"}}
{"id": "2506.07713", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07713", "abs": "https://arxiv.org/abs/2506.07713", "authors": ["Ge Wang", "Songlin Fan", "Hangxu Liu", "Quanjian Song", "Hewei Wang", "Jinfeng Xu"], "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation", "comment": "16 pages, 12 figures", "summary": "With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.", "AI": {"tldr": "FlowV2V提出了一种基于光流的视频编辑方法，通过分解任务为第一帧编辑和条件I2V生成，解决了复杂运动建模的挑战，显著提升了时间一致性和样本质量。", "motivation": "现有方法难以建模复杂运动模式，尤其是在非刚性物体运动（如多物体和肖像编辑）任务中表现不佳。光流为复杂运动建模提供了新思路。", "method": "FlowV2V将视频编辑任务分解为第一帧编辑和条件I2V生成，通过模拟伪光流序列确保编辑过程中的时间一致性。", "result": "在DAVIS-EDIT数据集上，FlowV2V在DOVER和warping error指标上分别提升了13.67%和50.66%，表现出优越的时间一致性和样本质量。", "conclusion": "FlowV2V通过光流驱动的I2V生成方法，为复杂运动视频编辑提供了高效解决方案，并通过实验验证了其有效性。"}}
{"id": "2506.07440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07440", "abs": "https://arxiv.org/abs/2506.07440", "authors": ["Ruhan Wang", "Zhiyong Wang", "Chengkai Huang", "Rui Wang", "Tong Yu", "Lina Yao", "John C. S. Lui", "Dongruo Zhou"], "title": "Federated In-Context Learning: Iterative Refinement for Improved Answer Quality", "comment": "27 pages, 16 figures. Accepted to ICML 2025", "summary": "For question-answering (QA) tasks, in-context learning (ICL) enables language\nmodels to generate responses without modifying their parameters by leveraging\nexamples provided in the input. However, the effectiveness of ICL heavily\ndepends on the availability of high-quality examples, which are often scarce\ndue to data privacy constraints, annotation costs, and distribution\ndisparities. A natural solution is to utilize examples stored on client\ndevices, but existing approaches either require transmitting model parameters -\nincurring significant communication overhead - or fail to fully exploit local\ndatasets, limiting their effectiveness. To address these challenges, we propose\nFederated In-Context Learning (Fed-ICL), a general framework that enhances ICL\nthrough an iterative, collaborative process. Fed-ICL progressively refines\nresponses by leveraging multi-round interactions between clients and a central\nserver, improving answer quality without the need to transmit model parameters.\nWe establish theoretical guarantees for the convergence of Fed-ICL and conduct\nextensive experiments on standard QA benchmarks, demonstrating that our\nproposed approach achieves strong performance while maintaining low\ncommunication costs.", "AI": {"tldr": "论文提出了一种名为Fed-ICL的框架，通过多轮客户端与服务器交互提升问答任务中的上下文学习效果，避免了模型参数传输的高通信开销。", "motivation": "由于数据隐私、标注成本和分布差异，高质量上下文学习示例稀缺，现有方法要么通信开销大，要么未能充分利用本地数据。", "method": "提出Fed-ICL框架，通过客户端与服务器的多轮交互逐步优化回答，无需传输模型参数。", "result": "理论证明了Fed-ICL的收敛性，实验表明其在标准问答基准上表现优异且通信成本低。", "conclusion": "Fed-ICL是一种高效且低通信开销的上下文学习增强方法。"}}
{"id": "2506.07720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07720", "abs": "https://arxiv.org/abs/2506.07720", "authors": ["Yufei Guo", "Yuhan Zhang", "Zhou Jie", "Xiaode Liu", "Xin Tong", "Yuanpei Chen", "Weihang Peng", "Zhe Ma"], "title": "ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks", "comment": "Accpeted by ICML2024", "summary": "The Spiking Neural Network (SNN), a biologically inspired neural network\ninfrastructure, has garnered significant attention recently. SNNs utilize\nbinary spike activations for efficient information transmission, replacing\nmultiplications with additions, thereby enhancing energy efficiency. However,\nbinary spike activation maps often fail to capture sufficient data information,\nresulting in reduced accuracy. To address this challenge, we advocate reversing\nthe bit of the weight and activation for SNNs, called \\textbf{ReverB-SNN},\ninspired by recent findings that highlight greater accuracy degradation from\nquantizing activations compared to weights. Specifically, our method employs\nreal-valued spike activations alongside binary weights in SNNs. This preserves\nthe event-driven and multiplication-free advantages of standard SNNs while\nenhancing the information capacity of activations. Additionally, we introduce a\ntrainable factor within binary weights to adaptively learn suitable weight\namplitudes during training, thereby increasing network capacity. To maintain\nefficiency akin to vanilla \\textbf{ReverB-SNN}, our trainable binary weight\nSNNs are converted back to standard form using a re-parameterization technique\nduring inference. Extensive experiments across various network architectures\nand datasets, both static and dynamic, demonstrate that our approach\nconsistently outperforms state-of-the-art methods.", "AI": {"tldr": "论文提出了一种名为ReverB-SNN的方法，通过反转权重和激活的比特位，结合实数激活和二值权重，提升SNN的精度和效率。", "motivation": "解决SNN中二进制激活映射信息不足导致的精度下降问题。", "method": "使用实数激活和二值权重，引入可训练因子自适应学习权重幅度，并通过重参数化技术保持推理效率。", "result": "在多种网络架构和数据集上表现优于现有方法。", "conclusion": "ReverB-SNN在保持SNN优势的同时显著提升了精度。"}}
{"id": "2506.07448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07448", "abs": "https://arxiv.org/abs/2506.07448", "authors": ["T. Duy Nguyen-Hien", "Desi R. Ivanova", "Yee Whye Teh", "Wee Sun Lee"], "title": "Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs", "comment": null, "summary": "Although large language models (LLMs) are highly interactive and extendable,\ncurrent approaches to ensure reliability in deployments remain mostly limited\nto rejecting outputs with high uncertainty in order to avoid misinformation.\nThis conservative strategy reflects the current lack of tools to systematically\ndistinguish and respond to different sources of uncertainty. In this paper, we\nadvocate for the adoption of Bayesian Modeling of Experiments -- a framework\nthat provides a coherent foundation to reason about uncertainty and clarify the\nreducibility of uncertainty -- for managing and proactively addressing\nuncertainty that arises in LLM deployments. This framework enables LLMs and\ntheir users to take contextually appropriate steps, such as requesting\nclarification, retrieving external information, or refining inputs. By\nsupporting active resolution rather than passive avoidance, it opens the door\nto more reliable, transparent, and broadly applicable LLM systems, particularly\nin high-stakes, real-world settings.", "AI": {"tldr": "论文提出采用贝叶斯实验建模框架，以主动管理和解决LLM部署中的不确定性，而非被动避免。", "motivation": "当前LLM部署中缺乏系统区分和应对不确定性的工具，导致保守策略（如拒绝高不确定性输出）限制了可靠性。", "method": "引入贝叶斯实验建模框架，为不确定性提供一致的理论基础，并支持主动解决措施（如请求澄清或检索外部信息）。", "result": "该框架使LLM及其用户能采取情境适当的步骤，提升系统的可靠性和透明度。", "conclusion": "贝叶斯框架为高风险的现实场景提供了更可靠、广泛适用的LLM系统解决方案。"}}
{"id": "2506.07725", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07725", "abs": "https://arxiv.org/abs/2506.07725", "authors": ["Shadi Hamdan", "Chonghao Sima", "Zetong Yang", "Hongyang Li", "Fatma Güney"], "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", "comment": "ICCV 2025 submission. For code, see\n  https://github.com/opendrivelab/ETA", "summary": "How can we benefit from large models without sacrificing inference speed, a\ncommon dilemma in self-driving systems? A prevalent solution is a dual-system\narchitecture, employing a small model for rapid, reactive decisions and a\nlarger model for slower but more informative analyses. Existing dual-system\ndesigns often implement parallel architectures where inference is either\ndirectly conducted using the large model at each current frame or retrieved\nfrom previously stored inference results. However, these works still struggle\nto enable large models for a timely response to every online frame. Our key\ninsight is to shift intensive computations of the current frame to previous\ntime steps and perform a batch inference of multiple time steps to make large\nmodels respond promptly to each time step. To achieve the shifting, we\nintroduce Efficiency through Thinking Ahead (ETA), an asynchronous system\ndesigned to: (1) propagate informative features from the past to the current\nframe using future predictions from the large model, (2) extract current frame\nfeatures using a small model for real-time responsiveness, and (3) integrate\nthese dual features via an action mask mechanism that emphasizes\naction-critical image regions. Evaluated on the Bench2Drive CARLA\nLeaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with\na driving score of 69.53 while maintaining a near-real-time inference speed at\n50 ms.", "AI": {"tldr": "论文提出ETA系统，通过异步计算将大模型的计算任务提前到前一帧，结合小模型实时响应，提升自动驾驶系统的推理速度和性能。", "motivation": "解决自动驾驶系统中大模型推理速度慢的问题，同时不牺牲性能。", "method": "采用异步系统ETA，将大模型的计算任务提前到前一帧，结合小模型实时提取当前帧特征，并通过动作掩码机制整合双特征。", "result": "在Bench2Drive CARLA Leaderboard-v2基准测试中，ETA将驾驶分数提升8%，达到69.53，同时保持50ms的近实时推理速度。", "conclusion": "ETA系统成功实现了大模型的高效利用，同时保证了实时性，为自动驾驶系统提供了新的解决方案。"}}
{"id": "2506.07452", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.07452", "abs": "https://arxiv.org/abs/2506.07452", "authors": ["Yuxin Xiao", "Sana Tonekaboni", "Walter Gerych", "Vinith Suriyakumar", "Marzyeh Ghassemi"], "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment", "comment": null, "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in jailbreak queries. Although these\nstyle patterns are semantically unrelated to the malicious intents behind\njailbreak queries, their safety impact remains unclear. In this work, we seek\nto understand whether style patterns compromise LLM safety, how superficial\nstyle alignment increases model vulnerability, and how best to mitigate these\nrisks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,\nand find that malicious queries with style patterns inflate the attack success\nrate (ASR) for nearly all models. Notably, ASR inflation correlates with both\nthe length of style patterns and the relative attention an LLM exhibits on\nthem. We then investigate superficial style alignment, and find that\nfine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of\nthose same styles. Finally, we propose SafeStyle, a defense strategy that\nincorporates a small amount of safety training data augmented to match the\ndistribution of style patterns in the fine-tuning data. Across three LLMs and\nfive fine-tuning style settings, SafeStyle consistently outperforms baselines\nin maintaining LLM safety.", "AI": {"tldr": "研究发现，特定风格提示（如列表格式）会提高大型语言模型（LLM）对越狱攻击的脆弱性，攻击成功率（ASR）显著增加。作者提出SafeStyle防御策略，通过风格匹配的安全训练数据提升模型安全性。", "motivation": "探究风格提示对LLM安全性的影响，以及如何通过风格对齐缓解越狱攻击风险。", "method": "评估32个LLM在7个越狱基准测试中的表现，分析风格模式长度与注意力机制的关系，并提出SafeStyle防御策略。", "result": "风格提示显著增加ASR，且风格对齐会加剧模型脆弱性；SafeStyle在多种风格设置下优于基线方法。", "conclusion": "风格提示对LLM安全性有负面影响，SafeStyle能有效提升模型抗攻击能力。"}}
{"id": "2506.07737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07737", "abs": "https://arxiv.org/abs/2506.07737", "authors": ["Xuemei Chen", "Huamin Wang", "Hangchi Shen", "Shukai Duan", "Shiping Wen", "Tingwen Huang"], "title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding", "comment": null, "summary": "Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.", "AI": {"tldr": "提出了一种基于脉冲神经网络（SNN）的低功耗单目3D目标检测架构SpikeSMOKE，通过跨尺度门控编码机制（CSGC）和轻量级残差块提升性能并降低能耗。", "motivation": "解决3D目标检测中能耗高的问题，利用SNN的低功耗特性，同时克服SNN信号离散导致的信息损失问题。", "method": "提出CSGC机制增强特征表示，设计轻量级残差块减少计算量。", "result": "在KITTI数据集上性能显著提升，能耗降低72.2%，检测性能仅下降4%。", "conclusion": "SpikeSMOKE在低功耗3D目标检测中表现出色，为相关领域提供了新思路。"}}
{"id": "2506.07459", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.07459", "abs": "https://arxiv.org/abs/2506.07459", "authors": ["Ziwen Wang", "Jiajun Fan", "Ruihan Guo", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning", "comment": null, "summary": "Protein generative models have shown remarkable promise in protein design but\nstill face limitations in success rate, due to the scarcity of high-quality\nprotein datasets for supervised pretraining. We present ProteinZero, a novel\nframework that enables scalable, automated, and continuous self-improvement of\nthe inverse folding model through online reinforcement learning. To achieve\ncomputationally tractable online feedback, we introduce efficient proxy reward\nmodels based on ESM-fold and a novel rapid ddG predictor that significantly\naccelerates evaluation speed. ProteinZero employs a general RL framework\nbalancing multi-reward maximization, KL-divergence from a reference model, and\na novel protein-embedding level diversity regularization that prevents mode\ncollapse while promoting higher sequence diversity. Through extensive\nexperiments, we demonstrate that ProteinZero substantially outperforms existing\nmethods across every key metric in protein design, achieving significant\nimprovements in structural accuracy, designability, thermodynamic stability,\nand sequence diversity. Most impressively, ProteinZero reduces design failure\nrates by approximately 36% - 48% compared to widely-used methods like\nProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates\nexceeding 90% across diverse and complex protein folds. Notably, the entire RL\nrun on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,\nincluding reward computation. Our work establishes a new paradigm for protein\ndesign where models evolve continuously from their own generated outputs,\nopening new possibilities for exploring the vast protein design space.", "AI": {"tldr": "ProteinZero是一种通过在线强化学习实现蛋白质逆折叠模型自我改进的新框架，显著提高了蛋白质设计的成功率。", "motivation": "现有蛋白质生成模型因高质量数据集稀缺而成功率受限，需要一种可扩展且自动化的改进方法。", "method": "采用在线强化学习框架，结合高效代理奖励模型（ESM-fold和快速ddG预测器）和多目标优化策略（包括多样性正则化）。", "result": "ProteinZero在结构准确性、可设计性、热力学稳定性和序列多样性上均优于现有方法，设计失败率降低36%-48%，成功率超过90%。", "conclusion": "ProteinZero为蛋白质设计提供了持续自我改进的新范式，为探索蛋白质设计空间开辟了新可能。"}}
{"id": "2506.07738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07738", "abs": "https://arxiv.org/abs/2506.07738", "authors": ["Lanjiong Li", "Guanhua Zhao", "Lingting Zhu", "Zeyu Cai", "Lequan Yu", "Jian Zhang", "Zeyu Wang"], "title": "AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization", "comment": "SIGGRAPH 2025. 11 pages, 12 figures", "summary": "Recent research on generative models has primarily focused on creating\nproduct-ready visual outputs; however, designers often favor access to\nstandardized asset libraries, a domain that has yet to be significantly\nenhanced by generative capabilities. Although open-world scenes provide ample\nraw materials for designers, efficiently extracting high-quality, standardized\nassets remains a challenge. To address this, we introduce AssetDropper, the\nfirst framework designed to extract assets from reference images, providing\nartists with an open-world asset palette. Our model adeptly extracts a front\nview of selected subjects from input images, effectively handling complex\nscenarios such as perspective distortion and subject occlusion. We establish a\nsynthetic dataset of more than 200,000 image-subject pairs and a real-world\nbenchmark with thousands more for evaluation, facilitating the exploration of\nfuture research in downstream tasks. Furthermore, to ensure precise asset\nextraction that aligns well with the image prompts, we employ a pre-trained\nreward model to fulfill a closed-loop with feedback. We design the reward model\nto perform an inverse task that pastes the extracted assets back into the\nreference sources, which assists training with additional consistency and\nmitigates hallucination. Extensive experiments show that, with the aid of\nreward-driven optimization, AssetDropper achieves the state-of-the-art results\nin asset extraction. Project page: AssetDropper.github.io.", "AI": {"tldr": "AssetDropper是一个从参考图像中提取标准化资产的框架，解决了设计师从开放世界场景中高效提取高质量资产的挑战。", "motivation": "设计师需要标准化资产库，但现有生成模型未充分支持这一需求。", "method": "通过合成数据集和预训练奖励模型实现闭环反馈，优化资产提取。", "result": "AssetDropper在资产提取任务中达到最先进水平。", "conclusion": "AssetDropper为设计师提供了高效的开放世界资产提取工具，推动了相关研究。"}}
{"id": "2506.07467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07467", "abs": "https://arxiv.org/abs/2506.07467", "authors": ["Jie Peng", "Hongwei Yang", "Jing Zhao", "Hengji Dong", "Hui He", "Weizhe Zhang", "Haoyu He"], "title": "Circumventing Backdoor Space via Weight Symmetry", "comment": null, "summary": "Deep neural networks are vulnerable to backdoor attacks, where malicious\nbehaviors are implanted during training. While existing defenses can\neffectively purify compromised models, they typically require labeled data or\nspecific training procedures, making them difficult to apply beyond supervised\nlearning settings. Notably, recent studies have shown successful backdoor\nattacks across various learning paradigms, highlighting a critical security\nconcern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),\na novel backdoor purification defense that operates independently of data\nformat and requires only a small fraction of clean samples. Through theoretical\nanalysis, we prove that by leveraging permutation invariance in neural networks\nand quadratic mode connectivity, TSC amplifies the loss on poisoned samples\nwhile maintaining bounded clean accuracy. Experiments demonstrate that TSC\nachieves robust performance comparable to state-of-the-art methods in\nsupervised learning scenarios. Furthermore, TSC generalizes to self-supervised\nlearning frameworks, such as SimCLR and CLIP, maintaining its strong defense\ncapabilities. Our code is available at https://github.com/JiePeng104/TSC.", "AI": {"tldr": "论文提出了一种名为TSC的新型后门净化防御方法，适用于多种学习范式，无需标记数据或特定训练流程。", "motivation": "现有后门防御方法通常需要标记数据或特定训练流程，限制了其在监督学习之外的适用性，而TSC旨在填补这一空白。", "method": "TSC利用神经网络的排列不变性和二次模式连接性，通过两阶段对称连接性放大中毒样本的损失，同时保持干净的准确率。", "result": "实验表明，TSC在监督学习场景中性能与现有最优方法相当，并能推广到自监督学习框架（如SimCLR和CLIP）。", "conclusion": "TSC是一种通用且高效的后门净化防御方法，适用于多种学习范式。"}}
{"id": "2506.07739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07739", "abs": "https://arxiv.org/abs/2506.07739", "authors": ["Jing Zhong", "Jun Yin", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Shuai Lu", "Ran Luo"], "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models", "comment": null, "summary": "Architectural cultures across regions are characterized by stylistic\ndiversity, shaped by historical, social, and technological contexts in addition\nto geograph-ical conditions. Understanding architectural styles requires the\nability to describe and analyze the stylistic features of different architects\nfrom various regions through visual observations of architectural imagery.\nHowever, traditional studies of architectural culture have largely relied on\nsubjective expert interpretations and historical literature reviews, often\nsuffering from regional biases and limited ex-planatory scope. To address these\nchallenges, this study proposes three core contributions: (1) We construct a\nprofessional architectural style dataset named ArchDiffBench, which comprises\n1,765 high-quality architectural images and their corresponding style\nannotations, collected from different regions and historical periods. (2) We\npropose ArchiLense, an analytical framework grounded in Vision-Language Models\nand constructed using the ArchDiffBench dataset. By integrating ad-vanced\ncomputer vision techniques, deep learning, and machine learning algo-rithms,\nArchiLense enables automatic recognition, comparison, and precise\nclassi-fication of architectural imagery, producing descriptive language\noutputs that ar-ticulate stylistic differences. (3) Extensive evaluations show\nthat ArchiLense achieves strong performance in architectural style recognition,\nwith a 92.4% con-sistency rate with expert annotations and 84.5% classification\naccuracy, effec-tively capturing stylistic distinctions across images. The\nproposed approach transcends the subjectivity inherent in traditional analyses\nand offers a more objective and accurate perspective for comparative studies of\narchitectural culture.", "AI": {"tldr": "论文提出ArchDiffBench数据集和ArchiLense框架，利用视觉语言模型自动分析建筑风格，克服传统主观方法的局限性。", "motivation": "传统建筑文化研究依赖主观专家解读和历史文献，存在区域偏见和解释范围有限的问题。", "method": "构建ArchDiffBench数据集（1,765张高质量建筑图像），开发ArchiLense框架，结合计算机视觉和深度学习技术实现自动识别与分类。", "result": "ArchiLense在风格识别中表现优异，与专家标注一致性达92.4%，分类准确率为84.5%。", "conclusion": "该方法为建筑文化比较研究提供了更客观、准确的视角，超越了传统主观分析。"}}
{"id": "2506.07740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07740", "abs": "https://arxiv.org/abs/2506.07740", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "comment": null, "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "AI": {"tldr": "Flow-Anything 是一个从单视图图像生成大规模光学流训练数据的框架，通过3D表示和动态对象建模，显著提升了真实世界应用的性能。", "motivation": "解决现有光学流估计方法因依赖合成数据集而导致的真实世界应用性能受限问题。", "method": "1. 使用单目深度估计网络将单视图图像转换为3D表示；2. 开发对象无关的体积渲染和深度感知修复模块，生成动态对象的光学流数据。", "result": "生成的 FA-Flow 数据集在性能上超越了最先进的监督和无监督方法，并提升了多种下游视频任务的表现。", "conclusion": "Flow-Anything 首次证明了从大规模真实图像生成光学流数据的有效性，为视频任务提供了基础模型。"}}
{"id": "2506.07477", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.07477", "abs": "https://arxiv.org/abs/2506.07477", "authors": ["Thomas Zhu", "Joshua Clune", "Jeremy Avigad", "Albert Qiaochu Jiang", "Sean Welleck"], "title": "Premise Selection for a Lean Hammer", "comment": "LeanHammer is available at https://github.com/JOSHCLUNE/LeanHammer", "summary": "Neural methods are transforming automated reasoning for proof assistants, yet\nintegrating these advances into practical verification workflows remains\nchallenging. Hammers are tools that interface with external automatic theorem\nprovers to automate tedious reasoning steps. They have dramatically improved\nproductivity in proof assistants, but the Lean proof assistant still does not\nhave a hammer despite its growing popularity. We present LeanHammer, the first\nend-to-end domain-general hammer for Lean, built on a novel neural premise\nselection system for a hammer in dependent type theory. Unlike existing Lean\npremise selectors, our approach dynamically adapts to user-specific contexts\nand combines with symbolic proof search and reconstruction to create a\npractical hammer. With comprehensive evaluations, we show that our premise\nselector enables LeanHammer to solve 21\\% more goals relative to existing\npremise selectors, and generalize well to diverse domains. Our work bridges the\ngap between neural retrieval and symbolic reasoning, making formal verification\nmore accessible to researchers and practitioners.", "AI": {"tldr": "LeanHammer是第一个为Lean证明助手设计的端到端通用工具，结合神经前提选择和符号推理，显著提升验证效率。", "motivation": "尽管神经网络在自动推理中取得进展，但将其整合到实际验证工作流中仍具挑战性。Lean作为流行的证明助手，缺乏类似工具。", "method": "开发了动态适应用户上下文的神经前提选择系统，结合符号证明搜索和重构，构建LeanHammer。", "result": "评估显示，LeanHammer比现有前提选择器多解决21%的目标，并能泛化到多样领域。", "conclusion": "LeanHammer填补了神经检索与符号推理间的空白，使形式验证更易用。"}}
{"id": "2506.07750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07750", "abs": "https://arxiv.org/abs/2506.07750", "authors": ["Hyunsoo Kim", "Donghyun Kim", "Suhyun Kim"], "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation", "comment": "Published at CVPR 2025", "summary": "How can we generate an image B' that satisfies A:A'::B:B', given the input\nimages A,A' and B? Recent works have tackled this challenge through approaches\nlike visual in-context learning or visual instruction. However, these methods\nare typically limited to specific models (e.g. InstructPix2Pix. Inpainting\nmodels) rather than general diffusion models (e.g. Stable Diffusion, SDXL).\nThis dependency may lead to inherited biases or lower editing capabilities. In\nthis paper, we propose Difference Inversion, a method that isolates only the\ndifference from A and A' and applies it to B to generate a plausible B'. To\naddress model dependency, it is crucial to structure prompts in the form of a\n\"Full Prompt\" suitable for input to stable diffusion models, rather than using\nan \"Instruction Prompt\". To this end, we accurately extract the Difference\nbetween A and A' and combine it with the prompt of B, enabling a plug-and-play\napplication of the difference. To extract a precise difference, we first\nidentify it through 1) Delta Interpolation. Additionally, to ensure accurate\ntraining, we propose the 2) Token Consistency Loss and 3) Zero Initialization\nof Token Embeddings. Our extensive experiments demonstrate that Difference\nInversion outperforms existing baselines both quantitatively and qualitatively,\nindicating its ability to generate more feasible B' in a model-agnostic manner.", "AI": {"tldr": "提出了一种名为Difference Inversion的方法，通过提取A和A'之间的差异并应用于B，生成B'，解决了现有方法对特定模型的依赖问题。", "motivation": "现有方法通常局限于特定模型（如InstructPix2Pix），可能引入偏见或编辑能力受限，因此需要一种更通用的方法。", "method": "通过Delta插值提取差异，结合Token一致性损失和零初始化Token嵌入，构建适用于稳定扩散模型的完整提示。", "result": "实验表明，Difference Inversion在定量和定性上均优于现有基线，能够以模型无关的方式生成更可行的B'。", "conclusion": "Difference Inversion是一种高效且通用的方法，能够克服模型依赖性，提升图像编辑能力。"}}
{"id": "2506.07492", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07492", "abs": "https://arxiv.org/abs/2506.07492", "authors": ["Xiangkun Hu", "Lemin Kong", "Tong He", "David Wipf"], "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.09072", "summary": "The generated responses of large language models (LLMs) are often fine-tuned\nto human preferences through a process called reinforcement learning from human\nfeedback (RLHF). As RLHF relies on a challenging training sequence, whereby a\nseparate reward model is independently learned and then later applied to LLM\npolicy updates, ongoing research effort has targeted more straightforward\nalternatives. In this regard, direct preference optimization (DPO) and its many\noffshoots circumvent the need for a separate reward training step. Instead,\nthrough the judicious use of a reparameterization trick that induces an\n\\textit{implicit} reward, DPO and related methods consolidate learning to the\nminimization of a single loss function. And yet despite demonstrable success in\nsome real-world settings, we prove that DPO-based objectives are nonetheless\nsubject to sub-optimal regularization and counter-intuitive interpolation\nbehaviors, underappreciated artifacts of the reparameterizations upon which\nthey are based. To this end, we introduce an \\textit{explicit} preference\noptimization framework termed EXPO that requires no analogous\nreparameterization to achieve an implicit reward. Quite differently, we merely\nposit intuitively-appealing regularization factors from scratch that\ntransparently avoid the potential pitfalls of key DPO variants, provably\nsatisfying regularization desiderata that prior methods do not. Empirical\nresults serve to corroborate our analyses and showcase the efficacy of EXPO.", "AI": {"tldr": "论文提出了一种名为EXPO的显式偏好优化框架，解决了直接偏好优化（DPO）方法中的次优正则化和反直觉插值问题。", "motivation": "RLHF依赖复杂的训练序列，而DPO及其变体虽简化了流程，但仍存在正则化和插值问题，因此需要更优的替代方案。", "method": "通过引入显式偏好优化框架EXPO，避免DPO的隐式奖励重参数化，提出直观的正则化因子。", "result": "EXPO在理论上满足正则化需求，并在实验中验证了其有效性。", "conclusion": "EXPO提供了一种更透明、更优的替代方案，解决了DPO的局限性。"}}
{"id": "2506.07773", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07773", "abs": "https://arxiv.org/abs/2506.07773", "authors": ["Mohamed Djilani", "Nassim Ali Ousalah", "Nidhal Eddine Chenni"], "title": "Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity", "comment": null, "summary": "We introduce a trend-aware and visually-grounded fashion recommendation\nsystem that integrates deep visual representations, garment-aware segmentation,\nsemantic category similarity and user behavior simulation. Our pipeline\nextracts focused visual embeddings by masking non-garment regions via semantic\nsegmentation followed by feature extraction using pretrained CNN backbones\n(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we\ngenerate synthetic purchase histories influenced by user-specific trendiness\nand item popularity. Recommendations are computed using a weighted scoring\nfunction that fuses visual similarity, semantic coherence and popularity\nalignment. Experiments on the DeepFashion dataset demonstrate consistent gender\nalignment and improved category relevance, with ResNet-50 achieving 64.95%\ncategory similarity and lowest popularity MAE. An ablation study confirms the\ncomplementary roles of visual and popularity cues. Our method provides a\nscalable framework for personalized fashion recommendations that balances\nindividual style with emerging trends. Our implementation is available at\nhttps://github.com/meddjilani/FashionRecommender", "AI": {"tldr": "提出了一种结合视觉、语义和用户行为的时尚推荐系统，通过深度学习提取特征并模拟用户购物行为，实验表明其性能优于基线。", "motivation": "解决时尚推荐中视觉与语义的融合问题，同时模拟真实用户行为以提升个性化推荐效果。", "method": "使用语义分割提取服装区域特征，结合预训练CNN提取视觉嵌入，模拟用户购物行为生成推荐分数。", "result": "在DeepFashion数据集上，ResNet-50表现最佳，类别相似度达64.95%，且流行度误差最低。", "conclusion": "该方法为个性化时尚推荐提供了可扩展框架，平衡个人风格与流行趋势。"}}
{"id": "2506.07500", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.07500", "abs": "https://arxiv.org/abs/2506.07500", "authors": ["Shakir Yousefi", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks", "comment": null, "summary": "Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$.", "AI": {"tldr": "论文提出通过注入Gumbel噪声和直通估计器加速逻辑门网络（LGNs）训练，减少离散化差距，提高神经元利用率。", "motivation": "现代神经网络计算和能耗高，逻辑门网络（LGNs）训练慢且存在离散化差距，影响实际部署。", "method": "在训练中注入Gumbel噪声并使用直通估计器，理论分析表明其隐含Hessian正则化。", "result": "训练速度提升4.5倍，离散化差距减少98%，未使用门数量减少100%。", "conclusion": "该方法显著提升LGNs的训练效率和实际部署性能。"}}
{"id": "2506.07778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07778", "abs": "https://arxiv.org/abs/2506.07778", "authors": ["Yichang Xu", "Gaowen Liu", "Ramana Rao Kompella", "Sihao Hu", "Tiansheng Huang", "Fatih Ilhan", "Selim Furkan Tekin", "Zachary Yahn", "Ling Liu"], "title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning", "comment": null, "summary": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal visual-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date suffer from\ngeneralization performance. Inspired by recent development in LLMs for visual\nreasoning, this paper presents VLAgent, an AI system that can create a\nstep-by-step visual reasoning plan with an easy-to-understand script and\nexecute each step of the plan in real time by integrating planning script with\nexecution verifications via an automated process supported by VLAgent. In the\ntask planning phase, VLAgent fine-tunes an LLM through in-context learning to\ngenerate a step-by-step planner for each user-submitted text-visual reasoning\ntask. During the plan execution phase, VLAgent progressively refines the\ncomposition of neuro-symbolic executable modules to generate high-confidence\nreasoning results. VLAgent has three unique design characteristics: First, we\nimprove the quality of plan generation through in-context learning, improving\nlogic reasoning by reducing erroneous logic steps, incorrect programs, and LLM\nhallucinations. Second, we design a syntax-semantics parser to identify and\ncorrect additional logic errors of the LLM-generated planning script prior to\nlaunching the plan executor. Finally, we employ the ensemble method to improve\nthe generalization performance of our step-executor. Extensive experiments with\nfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent\nachieves significant performance enhancement for multimodal text-visual\nreasoning applications, compared to the exiting representative VLMs and LLM\nbased visual composition approaches like ViperGPT and VisProg, thanks to the\nnovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,\nOutput Verifiers). Code and data will be made available upon paper acceptance.", "AI": {"tldr": "VLAgent是一个结合规划脚本与执行验证的多模态视觉-文本推理系统，通过上下文学习优化计划生成，并利用神经符号模块提升推理性能。", "motivation": "现有视觉-语言模型在泛化性能上表现不足，VLAgent旨在通过分步规划和执行验证提升多模态推理能力。", "method": "VLAgent分两阶段：任务规划阶段通过上下文学习微调LLM生成分步计划；执行阶段通过神经符号模块逐步优化推理结果。", "result": "在GQA等四个基准测试中，VLAgent显著优于现有视觉-语言模型和基于LLM的视觉组合方法。", "conclusion": "VLAgent通过优化模块（如SS-Parser和Plan Repairer）提升了多模态推理性能，代码和数据将在论文接受后公开。"}}
{"id": "2506.07501", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07501", "abs": "https://arxiv.org/abs/2506.07501", "authors": ["Libo Wang"], "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning", "comment": "The relevant code has been uploaded to the publicly available GitHub\n  repository. The link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/GoCE", "summary": "In view of the problem that each subchain in the chain-of-model (CoM) relies\nonly on the information of the previous subchain and may lose long-range\ndependencies due to the causal mask blocking the global context flow between\nmulti-level subchains, this work proposes a graph of causal evolution (GoCE).\nIts core principle is to map the implicit token representation into a\ndifferentiable and sparse causal adjacency matrix, then permeate causal\nconstraints through each layer of calculation using causal-masked attention and\ncausal-MoE. By combining intervention consistency loss test and self-evolution\ngate, the dynamic balance between causal structure learning and adaptive\nupdating of transformer architecture is realized. The researcher built\nexperimental environments in sandboxes built with Claude Sonnet 4,\no4-mini-high, and DeepSeek R1 respectively with the transformer variant\narchitecture introduced in GoCE. It is evaluated on publicly available datasets\nincluding CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the\nbaseline LLMs. The finding proves that GoCE strengthens the transformer's\nability to capture long-range causal dependencies, while the ability to\nself-evolve is improved. It not only surpasses the design of CoM in terms of\ndesign principles, but also provides experience for future research on causal\nlearning and continuous adaptive improvement.", "AI": {"tldr": "论文提出了一种图因果演化（GoCE）方法，通过稀疏因果邻接矩阵和因果掩码注意力，解决了链式模型（CoM）中长距离依赖丢失的问题。", "motivation": "解决CoM中子链仅依赖前序信息导致的长距离依赖丢失问题。", "method": "将隐式令牌表示映射为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，通过干预一致性损失和自演化门实现动态平衡。", "result": "GoCE提升了Transformer捕捉长距离因果依赖的能力，并在多个数据集上优于基线模型。", "conclusion": "GoCE不仅优于CoM设计，还为因果学习和持续自适应改进提供了经验。"}}
{"id": "2506.07779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07779", "abs": "https://arxiv.org/abs/2506.07779", "authors": ["Beining Xu", "Junxian Li"], "title": "Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods", "comment": "11 pages, 13 figures", "summary": "Visible images offer rich texture details, while infrared images emphasize\nsalient targets. Fusing these complementary modalities enhances scene\nunderstanding, particularly for advanced vision tasks under challenging\nconditions. Recently, deep learning-based fusion methods have gained attention,\nbut current evaluations primarily rely on general-purpose metrics without\nstandardized benchmarks or downstream task performance. Additionally, the lack\nof well-developed dual-spectrum datasets and fair algorithm comparisons hinders\nprogress.\n  To address these gaps, we construct a high-quality dual-spectrum dataset\ncaptured in campus environments, comprising 1,369 well-aligned visible-infrared\nimage pairs across four representative scenarios: daytime, nighttime, smoke\nocclusion, and underpasses. We also propose a comprehensive and fair evaluation\nframework that integrates fusion speed, general metrics, and object detection\nperformance using the lang-segment-anything model to ensure fairness in\ndownstream evaluation.\n  Extensive experiments benchmark several state-of-the-art fusion algorithms\nunder this framework. Results demonstrate that fusion models optimized for\ndownstream tasks achieve superior performance in target detection, especially\nin low-light and occluded scenes. Notably, some algorithms that perform well on\ngeneral metrics do not translate to strong downstream performance, highlighting\nlimitations of current evaluation practices and validating the necessity of our\nproposed framework.\n  The main contributions of this work are: (1)a campus-oriented dual-spectrum\ndataset with diverse and challenging scenes; (2) a task-aware, comprehensive\nevaluation framework; and (3) thorough comparative analysis of leading fusion\nmethods across multiple datasets, offering insights for future development.", "AI": {"tldr": "论文提出了一种高质量的双光谱数据集和全面的评估框架，用于可见光和红外图像融合，并验证了融合模型在下游任务中的性能。", "motivation": "当前可见光和红外图像融合方法缺乏标准化评估和高质量数据集，限制了其在实际任务中的应用。", "method": "构建了一个包含1,369对对齐图像的数据集，并提出一个结合融合速度、通用指标和目标检测性能的评估框架。", "result": "实验表明，针对下游任务优化的融合模型在目标检测中表现更优，尤其是在低光和遮挡场景中。", "conclusion": "论文贡献包括高质量数据集、任务感知的评估框架以及对现有融合方法的全面分析，为未来研究提供了方向。"}}
{"id": "2506.07505", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07505", "abs": "https://arxiv.org/abs/2506.07505", "authors": ["Perry Dong", "Alec M. Lessing", "Annie S. Chen", "Chelsea Finn"], "title": "Reinforcement Learning via Implicit Imitation Guidance", "comment": null, "summary": "We study the problem of sample efficient reinforcement learning, where prior\ndata such as demonstrations are provided for initialization in lieu of a dense\nreward signal. A natural approach is to incorporate an imitation learning\nobjective, either as regularization during training or to acquire a reference\npolicy. However, imitation learning objectives can ultimately degrade long-term\nperformance, as it does not directly align with reward maximization. In this\nwork, we propose to use prior data solely for guiding exploration via noise\nadded to the policy, sidestepping the need for explicit behavior cloning\nconstraints. The key insight in our framework, Data-Guided Noise (DGN), is that\ndemonstrations are most useful for identifying which actions should be\nexplored, rather than forcing the policy to take certain actions. Our approach\nachieves up to 2-3x improvement over prior reinforcement learning from offline\ndata methods across seven simulated continuous control tasks.", "AI": {"tldr": "论文提出了一种名为DGN的方法，利用先验数据指导探索而非直接模仿，显著提升了样本效率。", "motivation": "研究如何在强化学习中高效利用先验数据（如演示），避免模仿学习目标对长期性能的负面影响。", "method": "提出Data-Guided Noise (DGN)框架，通过噪声引导探索，而非直接行为克隆。", "result": "在七个模拟连续控制任务中，性能比现有方法提升2-3倍。", "conclusion": "DGN通过探索导向的方式利用先验数据，避免了模仿学习的局限性，显著提升了强化学习效果。"}}
{"id": "2506.07785", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07785", "abs": "https://arxiv.org/abs/2506.07785", "authors": ["Qi Yang", "Chenghao Zhang", "Lubin Fan", "Kun Ding", "Jieping Ye", "Shiming Xiang"], "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger", "comment": "ICML 2025 Spotlight. 22 pages, 16 figures", "summary": "Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.", "AI": {"tldr": "本文提出了一种名为RCTS的多模态RAG框架，通过构建推理上下文丰富的知识库和树搜索重排序方法，解决了现有LVLMs在VQA任务中知识稀缺和响应不一致的问题。", "motivation": "现有的大视觉语言模型（LVLMs）在VQA任务中面临知识库中推理示例稀缺和检索知识响应不一致的挑战。", "method": "提出RCTS框架，包括构建推理上下文丰富的知识库和基于蒙特卡洛树搜索与启发式奖励（MCTS-HR）的重排序方法。", "result": "实验表明，RCTS在多个VQA数据集上实现了最先进的性能，显著优于ICL和Vanilla-RAG方法。", "conclusion": "RCTS框架通过高质量的知识库和重排序方法，显著提升了LVLMs的性能和一致性。"}}
{"id": "2506.07517", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.07517", "abs": "https://arxiv.org/abs/2506.07517", "authors": ["Shuqiang Zhang", "Yuchao Zhang", "Jinkun Chen", "Haochen Sui"], "title": "Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems", "comment": "In Proceedings of the 31st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining V.2 (KDD '25), August 3--7, 2025, Toronto, ON,\n  Canada", "summary": "Recommendation systems (RS) aim to provide personalized content, but they\nface a challenge in unbiased learning due to selection bias, where users only\ninteract with items they prefer. This bias leads to a distorted representation\nof user preferences, which hinders the accuracy and fairness of\nrecommendations. To address the issue, various methods such as error imputation\nbased, inverse propensity scoring, and doubly robust techniques have been\ndeveloped. Despite the progress, from the structural causal model perspective,\nprevious debiasing methods in RS assume the independence of the exogenous\nvariables. In this paper, we release this assumption and propose a learning\nalgorithm based on likelihood maximization to learn a prediction model. We\nfirst discuss the correlation and difference between unmeasured confounding and\nour scenario, then we propose a unified method that effectively handles latent\nexogenous variables. Specifically, our method models the data generation\nprocess with latent exogenous variables under mild normality assumptions. We\nthen develop a Monte Carlo algorithm to numerically estimate the likelihood\nfunction. Extensive experiments on synthetic datasets and three real-world\ndatasets demonstrate the effectiveness of our proposed method. The code is at\nhttps://github.com/WallaceSUI/kdd25-background-variable.", "AI": {"tldr": "论文提出了一种基于似然最大化的学习算法，解决推荐系统中因外生变量独立性假设导致的偏差问题。", "motivation": "推荐系统因选择偏差导致用户偏好表示失真，现有方法假设外生变量独立，限制了性能。", "method": "提出统一方法处理潜在外生变量，基于正态假设建模数据生成过程，并用蒙特卡洛算法估计似然函数。", "result": "在合成数据集和三个真实数据集上验证了方法的有效性。", "conclusion": "释放外生变量独立性假设，提出新算法显著提升推荐系统的准确性和公平性。"}}
{"id": "2506.07803", "categories": ["cs.CV", "68T10, 68T30, 68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2506.07803", "abs": "https://arxiv.org/abs/2506.07803", "authors": ["Eduard Allakhverdov", "Dmitrii Tarasov", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Image Reconstruction as a Tool for Feature Analysis", "comment": "23 pages, 14 figures", "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.", "AI": {"tldr": "论文提出了一种通过图像重建解释视觉特征的新方法，比较了SigLIP和SigLIP2模型，发现基于图像任务预训练的编码器保留更多图像信息，并揭示了特征空间的操作对重建图像的影响。", "motivation": "尽管视觉编码器在应用中表现出色，但其内部特征表示机制尚不明确，因此需要一种方法来解释这些特征。", "method": "通过图像重建比较不同视觉编码器（如SigLIP和SigLIP2），分析其特征表示的信息量，并探索特征空间的操作对重建图像的影响。", "result": "基于图像任务预训练的编码器保留更多图像信息；特征空间的正交旋转控制颜色编码。", "conclusion": "该方法适用于任何视觉编码器，揭示了特征空间的内部结构，为理解视觉编码器提供了新视角。"}}
{"id": "2506.07534", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07534", "abs": "https://arxiv.org/abs/2506.07534", "authors": ["Clément Bonet", "Christophe Vauthier", "Anna Korba"], "title": "Flowing Datasets with Wasserstein over Wasserstein Gradient Flows", "comment": "Accepted as an oral at ICML2025", "summary": "Many applications in machine learning involve data represented as probability\ndistributions. The emergence of such data requires radically novel techniques\nto design tractable gradient flows on probability distributions over this type\nof (infinite-dimensional) objects. For instance, being able to flow labeled\ndatasets is a core task for applications ranging from domain adaptation to\ntransfer learning or dataset distillation. In this setting, we propose to\nrepresent each class by the associated conditional distribution of features,\nand to model the dataset as a mixture distribution supported on these classes\n(which are themselves probability distributions), meaning that labeled datasets\ncan be seen as probability distributions over probability distributions. We\nendow this space with a metric structure from optimal transport, namely the\nWasserstein over Wasserstein (WoW) distance, derive a differential structure on\nthis space, and define WoW gradient flows. The latter enables to design\ndynamics over this space that decrease a given objective functional. We apply\nour framework to transfer learning and dataset distillation tasks, leveraging\nour gradient flow construction as well as novel tractable functionals that take\nthe form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels\nbetween probability distributions.", "AI": {"tldr": "论文提出了一种基于最优传输的Wasserstein over Wasserstein (WoW)距离，用于处理概率分布上的梯度流，并应用于迁移学习和数据集蒸馏任务。", "motivation": "机器学习中许多应用涉及概率分布数据，需要设计新的梯度流技术来处理这类无限维对象。", "method": "将每个类别表示为特征的条件分布，数据集建模为这些类别的混合分布，引入WoW距离和梯度流。", "result": "提出了WoW梯度流框架，并在迁移学习和数据集蒸馏任务中验证了其有效性。", "conclusion": "WoW梯度流为处理概率分布数据提供了新方法，并在实际应用中展示了潜力。"}}
{"id": "2506.07809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07809", "abs": "https://arxiv.org/abs/2506.07809", "authors": ["Weilei Wen", "Tianyi Zhang", "Qianqian Zhao", "Zhaohui Zheng", "Chunle Guo", "Xiuli Shao", "Chongyi Li"], "title": "Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution", "comment": null, "summary": "Recent advancements in codebook-based real image super-resolution (SR) have\nshown promising results in real-world applications. The core idea involves\nmatching high-quality image features from a codebook based on low-resolution\n(LR) image features. However, existing methods face two major challenges:\ninaccurate feature matching with the codebook and poor texture detail\nreconstruction. To address these issues, we propose a novel Uncertainty-Guided\nand Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key\ncomponents: (1) an uncertainty learning mechanism that guides the model to\nfocus on texture-rich regions, (2) a Top-k feature matching strategy that\nenhances feature matching accuracy by fusing multiple candidate features, and\n(3) an Align-Attention module that enhances the alignment of information\nbetween LR and HR features. Experimental results demonstrate significant\nimprovements in texture realism and reconstruction fidelity compared to\nexisting methods. We will release the code upon formal publication.", "AI": {"tldr": "提出了一种基于不确定性引导和Top-k代码书匹配的超分辨率框架（UGTSR），解决了现有方法中特征匹配不准确和纹理细节重建差的问题。", "motivation": "现有代码书超分辨率方法存在特征匹配不准确和纹理细节重建不足的挑战。", "method": "UGTSR框架包含不确定性学习机制、Top-k特征匹配策略和Align-Attention模块。", "result": "实验表明，UGTSR在纹理真实性和重建保真度上显著优于现有方法。", "conclusion": "UGTSR框架有效提升了超分辨率任务中的纹理细节重建和特征匹配精度。"}}
{"id": "2506.07549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07549", "abs": "https://arxiv.org/abs/2506.07549", "authors": ["Zhangchi Zhao", "Jun Shu", "Deyu Meng", "Zongben Xu"], "title": "Improving Memory Efficiency for Training KANs via Meta Learning", "comment": "ICML 2025", "summary": "Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel\nframework for function approximation by replacing traditional neural network\nweights with learnable univariate functions. This design demonstrates\nsignificant potential as an efficient and interpretable alternative to\ntraditional MLPs. However, KANs are characterized by a substantially larger\nnumber of trainable parameters, leading to challenges in memory efficiency and\nhigher training costs compared to MLPs. To address this limitation, we propose\nto generate weights for KANs via a smaller meta-learner, called MetaKANs. By\ntraining KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs\nachieve comparable or even superior performance while significantly reducing\nthe number of trainable parameters and maintaining promising interpretability.\nExtensive experiments on diverse benchmark tasks, including symbolic\nregression, partial differential equation solving, and image classification,\ndemonstrate the effectiveness of MetaKANs in improving parameter efficiency and\nmemory usage. The proposed method provides an alternative technique for\ntraining KANs, that allows for greater scalability and extensibility, and\nnarrows the training cost gap with MLPs stated in the original paper of KANs.\nOur code is available at https://github.com/Murphyzc/MetaKAN.", "AI": {"tldr": "MetaKANs提出了一种通过元学习生成KANs权重的方法，显著减少了可训练参数数量，同时保持性能与可解释性。", "motivation": "KANs作为函数逼近的新框架，虽高效且可解释，但参数过多导致内存和训练成本高。MetaKANs旨在解决这一问题。", "method": "通过较小的元学习器（MetaKANs）生成KANs的权重，并以端到端可微分的方式联合训练，减少参数数量。", "result": "在符号回归、偏微分方程求解和图像分类等任务中，MetaKANs在参数效率和内存使用上表现优异。", "conclusion": "MetaKANs为KANs提供了一种更高效且可扩展的训练方法，缩小了与MLPs的训练成本差距。"}}
{"id": "2506.07811", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07811", "abs": "https://arxiv.org/abs/2506.07811", "authors": ["Tieyuan Chen", "Huabin Liu", "Yi Wang", "Chaofan Gan", "Mingxi Lyu", "Gui Zou", "Weiyao Lin"], "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning", "comment": "Preprint", "summary": "Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the given video, with prior work primarily focusing on identifying the\nduration of relevant segments, referred to as explicit visual evidence.\nHowever, explicit visual evidence is not always directly available,\nparticularly when questions target symbolic meanings or deeper intentions,\nleading to significant performance degradation. To fill this gap, we introduce\na novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo\n$\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering\nquestions in scenarios where explicit visual evidence is inaccessible. Given an\nimplicit question and its corresponding video, I-VQA requires answering based\non the contextual visual cues present within the video. To tackle I-VQA, we\npropose a novel reasoning framework, IRM (Implicit Reasoning Model),\nincorporating dual-stream modeling of contextual actions and intent clues as\nimplicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the\nVisual Enhancement Module (VEM). AIM deduces and preserves question-related\ndual clues by generating clue candidates and performing relation deduction. VEM\nenhances contextual visual representation by leveraging key contextual clues.\nExtensive experiments validate the effectiveness of our IRM in I-VQA tasks,\noutperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$,\n$1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on\nsimilar implicit advertisement understanding and future prediction in\ntraffic-VQA. Datasets and codes are available for double-blind review in\nanonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.", "AI": {"tldr": "论文提出了一种新任务I-VQA，解决视频问答中隐含视觉证据的问题，并提出了IRM框架，性能优于现有模型。", "motivation": "现有视频问答任务依赖显式视觉证据，但在处理隐含问题时性能下降，因此需要新方法。", "method": "提出IRM框架，包含AIM和VEM模块，通过双流建模和上下文增强解决隐含问题。", "result": "IRM在I-VQA任务中性能优于GPT-4o等模型，提升幅度为0.76%至4.87%。", "conclusion": "IRM有效解决了隐含视觉证据的视频问答问题，并在相关任务中表现优异。"}}
{"id": "2506.07551", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07551", "abs": "https://arxiv.org/abs/2506.07551", "authors": ["Mengsong Wu", "YaFei Wang", "Yidong Ming", "Yuqi An", "Yuwei Wan", "Wenliang Chen", "Binbin Lin", "Yuqiang Li", "Tong Xie", "Dongzhan Zhou"], "title": "ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning", "comment": "15 pages, 6 figures", "summary": "Large language models (LLMs) have recently demonstrated promising\ncapabilities in chemistry tasks while still facing challenges due to outdated\npretraining knowledge and the difficulty of incorporating specialized chemical\nexpertise. To address these issues, we propose an LLM-based agent that\nsynergistically integrates 137 external chemical tools created ranging from\nbasic information retrieval to complex reaction predictions, and a dataset\ncuration pipeline to generate the dataset ChemToolBench that facilitates both\neffective tool selection and precise parameter filling during fine-tuning and\nevaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search\n(HE-MCTS) framework, enabling independent optimization of tool planning and\nexecution. By leveraging self-generated data, our approach supports step-level\nfine-tuning (FT) of the policy model and training task-adaptive PRM and ORM\nthat surpass GPT-4o. Experimental evaluations demonstrate that our approach\nsignificantly improves performance in Chemistry QA and discovery tasks,\noffering a robust solution to integrate specialized tools with LLMs for\nadvanced chemical applications. All datasets and code are available at\nhttps://github.com/AI4Chem/ChemistryAgent .", "AI": {"tldr": "提出了一种基于LLM的化学代理，整合外部工具和数据集ChemToolBench，通过HE-MCTS框架优化工具规划与执行，显著提升化学任务性能。", "motivation": "解决LLMs在化学任务中因知识过时和缺乏专业知识整合而面临的挑战。", "method": "结合137种外部化学工具和数据集ChemToolBench，采用HE-MCTS框架优化工具规划与执行，并通过自生成数据微调模型。", "result": "实验表明，该方法在化学问答和发现任务中性能显著优于GPT-4o。", "conclusion": "为LLMs与专业化学工具的结合提供了高效解决方案，推动了化学应用的进步。"}}
{"id": "2506.07813", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07813", "abs": "https://arxiv.org/abs/2506.07813", "authors": ["Junseo Bang", "Joonhee Lee", "Kyeonghyun Lee", "Haechang Lee", "Dong Un Kang", "Se Young Chun"], "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution", "comment": null, "summary": "Arbitrary-scale image super-resolution aims to upsample images to any desired\nresolution, offering greater flexibility than traditional fixed-scale\nsuper-resolution. Recent approaches in this domain utilize regression-based or\ngenerative models, but many of them are a single-stage upsampling process,\nwhich may be challenging to learn across a wide, continuous distribution of\nscaling factors. Progressive upsampling strategies have shown promise in\nmitigating this issue, yet their integration with diffusion models for flexible\nupscaling remains underexplored. Here, we present CasArbi, a novel\nself-cascaded diffusion framework for arbitrary-scale image super-resolution.\nCasArbi meets the varying scaling demands by breaking them down into smaller\nsequential factors and progressively enhancing the image resolution at each\nstep with seamless transitions for arbitrary scales. Our novel\ncoordinate-guided residual diffusion model allows for the learning of\ncontinuous image representations while enabling efficient diffusion sampling.\nExtensive experiments demonstrate that our CasArbi outperforms prior arts in\nboth perceptual and distortion performance metrics across diverse\narbitrary-scale super-resolution benchmarks.", "AI": {"tldr": "CasArbi是一种新型的自级联扩散框架，用于任意尺度图像超分辨率，通过逐步增强分辨率实现灵活上采样。", "motivation": "传统固定尺度超分辨率缺乏灵活性，而现有方法在连续尺度分布上学习困难。", "method": "采用自级联扩散框架，将大尺度分解为小尺度逐步处理，结合坐标引导残差扩散模型。", "result": "CasArbi在多种任意尺度超分辨率基准测试中，感知和失真性能均优于现有方法。", "conclusion": "CasArbi通过渐进式上采样和扩散模型，实现了高效且灵活的任意尺度超分辨率。"}}
{"id": "2506.07578", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07578", "abs": "https://arxiv.org/abs/2506.07578", "authors": ["Florian Andreas Marwitz", "Ralf Möller", "Magnus Bender", "Marcel Gehrke"], "title": "Denoising the Future: Top-p Distributions for Moving Through Time", "comment": null, "summary": "Inference in dynamic probabilistic models is a complex task involving\nexpensive operations. In particular, for Hidden Markov Models, the whole state\nspace has to be enumerated for advancing in time. Even states with negligible\nprobabilities are considered, resulting in computational inefficiency and\nincreased noise due to the propagation of unlikely probability mass. We propose\nto denoise the future and speed up inference by using only the top-p states,\ni.e., the most probable states with accumulated probability p. We show that the\nerror introduced by using only the top-p states is bound by p and the so-called\nminimal mixing rate of the underlying model. Moreover, in our empirical\nevaluation, we show that we can expect speedups of at least an order of\nmagnitude, while the error in terms of total variation distance is below 0.09.", "AI": {"tldr": "提出了一种通过仅使用最可能的状态（top-p）来加速动态概率模型推理的方法，同时证明了误差受限于p和模型的最小混合率，实验显示速度提升显著且误差较低。", "motivation": "动态概率模型推理复杂度高且计算效率低，尤其是隐马尔可夫模型中需要枚举整个状态空间，导致不必要的计算和噪声传播。", "method": "提出仅使用累积概率为p的最可能状态（top-p）来简化推理过程。", "result": "实验表明，该方法可实现至少一个数量级的速度提升，且总变差距离误差低于0.09。", "conclusion": "通过限制状态空间为top-p状态，显著提高了推理效率，同时误差可控。"}}
{"id": "2506.07814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07814", "abs": "https://arxiv.org/abs/2506.07814", "authors": ["Yongzhen Wang", "Yongjun Li", "Zhuoran Zheng", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration", "comment": "13 pages, 8 figures, 3 tables", "summary": "Natural images are often degraded by complex, composite degradations such as\nrain, snow, and haze, which adversely impact downstream vision applications.\nWhile existing image restoration efforts have achieved notable success, they\nare still hindered by two critical challenges: limited generalization across\ndynamically varying degradation scenarios and a suboptimal balance between\npreserving local details and modeling global dependencies. To overcome these\nchallenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based\nMamba-CNN fusion framework for efficient and robust all-in-one image\nrestoration. M2Restore introduces three key contributions: First, to boost the\nmodel's generalization across diverse degradation conditions, we exploit a\nCLIP-guided MoE gating mechanism that fuses task-conditioned prompts with\nCLIP-derived semantic priors. This mechanism is further refined via cross-modal\nfeature calibration, which enables precise expert selection for various\ndegradation types. Second, to jointly capture global contextual dependencies\nand fine-grained local details, we design a dual-stream architecture that\nintegrates the localized representational strength of CNNs with the long-range\nmodeling efficiency of Mamba. This integration enables collaborative\noptimization of global semantic relationships and local structural fidelity,\npreserving global coherence while enhancing detail restoration. Third, we\nintroduce an edge-aware dynamic gating mechanism that adaptively balances\nglobal modeling and local enhancement by reallocating computational attention\nto degradation-sensitive regions. This targeted focus leads to more efficient\nand precise restoration. Extensive experiments across multiple image\nrestoration benchmarks validate the superiority of M2Restore in both visual\nquality and quantitative performance.", "AI": {"tldr": "M2Restore提出了一种基于Mixture-of-Experts（MoE）和Mamba-CNN融合的框架，用于高效、鲁棒的全能图像恢复，解决了现有方法在动态退化场景中泛化能力不足和局部细节与全局依赖平衡不佳的问题。", "motivation": "自然图像常受复合退化（如雨、雪、雾）影响，现有方法在动态退化场景中泛化能力有限，且难以平衡局部细节与全局依赖。", "method": "1. 使用CLIP引导的MoE门控机制融合任务条件提示与CLIP语义先验；2. 设计双流架构结合CNN的局部表征与Mamba的长程建模；3. 引入边缘感知动态门控机制自适应平衡全局与局部。", "result": "在多个图像恢复基准测试中，M2Restore在视觉质量和定量性能上均表现出优越性。", "conclusion": "M2Restore通过创新的架构和机制，显著提升了图像恢复的泛化能力和细节保留效果。"}}
{"id": "2506.07581", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.07581", "abs": "https://arxiv.org/abs/2506.07581", "authors": ["Tan Chen", "Jintao Yan", "Yuxuan Sun", "Sheng Zhou", "Zhisheng Niu"], "title": "FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning", "comment": null, "summary": "Federated learning (FL) is a promising paradigm for multiple devices to\ncooperatively train a model. When applied in wireless networks, two issues\nconsistently affect the performance of FL, i.e., data heterogeneity of devices\nand limited bandwidth. Many papers have investigated device scheduling\nstrategies considering the two issues. However, most of them recognize data\nheterogeneity as a property of individual devices. In this paper, we prove that\nthe convergence speed of FL is affected by the sum of device-level and\nsample-level collective gradient divergence (CGD). The device-level CGD refers\nto the gradient divergence of the scheduled device group, instead of the sum of\nthe individual device divergence. The sample-level CGD is statistically upper\nbounded by sampling variance, which is inversely proportional to the total\nnumber of samples scheduled for local update. To derive a tractable form of the\ndevice-level CGD, we further consider a classification problem and transform it\ninto the weighted earth moving distance (WEMD) between the group distribution\nand the global distribution. Then we propose FedCGD algorithm to minimize the\nsum of multi-level CGDs by balancing WEMD and sampling variance, within\npolynomial time. Simulation shows that the proposed strategy increases\nclassification accuracy on the CIFAR-10 dataset by up to 4.2\\% while scheduling\n41.8\\% fewer devices, and flexibly switches between reducing WEMD and reducing\nsampling variance.", "AI": {"tldr": "该论文提出了一种联邦学习（FL）优化方法FedCGD，通过多级梯度发散（CGD）分析，平衡设备级和样本级的梯度差异，提升收敛速度和分类精度。", "motivation": "在无线网络中，联邦学习面临数据异构性和带宽限制问题，现有研究多关注设备个体差异，而忽略了设备组和样本级的集体梯度发散对收敛速度的影响。", "method": "论文证明了FL收敛速度受设备级和样本级CGD总和影响，并将设备级CGD转化为加权地动距离（WEMD），提出FedCGD算法在多项式时间内优化多级CGD。", "result": "实验表明，FedCGD在CIFAR-10数据集上分类精度提升4.2%，同时减少41.8%的设备调度，并能灵活调整WEMD和采样方差的优化。", "conclusion": "FedCGD通过多级梯度发散优化，显著提升了联邦学习的性能和效率。"}}
{"id": "2506.07584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07584", "abs": "https://arxiv.org/abs/2506.07584", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "comment": null, "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "AI": {"tldr": "MIRA是一个专为医疗时间序列设计的统一基础模型，通过创新方法解决了医疗数据的不规则性、异质采样率和缺失值问题，显著提升了预测准确性。", "motivation": "医疗时间序列数据具有不规则间隔、异质采样率和频繁缺失值等挑战，现有通用基础模型难以处理。MIRA旨在解决这些问题，减少标注负担，支持跨机构和任务的稳健迁移。", "method": "MIRA采用连续时间旋转位置编码、频率特定专家混合层和基于神经ODE的连续动态外推块，以建模不规则时间间隔和连续状态轨迹。", "result": "在大规模医疗数据上预训练后，MIRA在分布外和分布内场景中分别平均减少10%和7%的预测误差。", "conclusion": "MIRA为医疗时间序列建模提供了统一基础，并通过综合基准为未来研究奠定基础。"}}
{"id": "2506.07841", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07841", "abs": "https://arxiv.org/abs/2506.07841", "authors": ["Elizabeth Pavlova", "Xue-Xin Wei"], "title": "Diffusion models under low-noise regime", "comment": null, "summary": "Recent work on diffusion models proposed that they operate in two regimes:\nmemorization, in which models reproduce their training data, and\ngeneralization, in which they generate novel samples. While this has been\ntested in high-noise settings, the behavior of diffusion models as effective\ndenoisers when the corruption level is small remains unclear. To address this\ngap, we systematically investigated the behavior of diffusion models under\nlow-noise diffusion dynamics, with implications for model robustness and\ninterpretability. Using (i) CelebA subsets of varying sample sizes and (ii)\nanalytic Gaussian mixture benchmarks, we reveal that models trained on disjoint\ndata diverge near the data manifold even when their high-noise outputs\nconverge. We quantify how training set size, data geometry, and model objective\nchoice shape denoising trajectories and affect score accuracy, providing\ninsights into how these models actually learn representations of data\ndistributions. This work starts to address gaps in our understanding of\ngenerative model reliability in practical applications where small\nperturbations are common.", "AI": {"tldr": "研究了扩散模型在低噪声条件下的行为，揭示了训练数据规模、几何结构及目标函数对去噪轨迹的影响。", "motivation": "探索扩散模型在小噪声条件下的行为，填补对生成模型在实际应用中可靠性的理解空白。", "method": "使用CelebA子集和高斯混合基准，分析模型在低噪声扩散动力学下的表现。", "result": "发现模型在数据流形附近行为不同，训练集规模和目标函数影响去噪轨迹和分数准确性。", "conclusion": "为理解扩散模型如何学习数据分布提供了新视角，对实际应用中的模型可靠性有启示。"}}
{"id": "2506.07585", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07585", "abs": "https://arxiv.org/abs/2506.07585", "authors": ["Seokbin Yoon", "Keumjin Lee"], "title": "Aircraft Trajectory Dataset Augmentation in Latent Space", "comment": null, "summary": "Aircraft trajectory modeling plays a crucial role in Air Traffic Management\n(ATM) and is important for various downstream tasks, including conflict\ndetection and landing time prediction. Dataset augmentation through the\naddition of synthetically generated trajectory data is necessary to develop a\nmore robust aircraft trajectory model and ensure that the trajectory dataset is\nsufficient and balanced. In this work, we propose a novel framework called\nATRADA for aircraft trajectory dataset augmentation. In the proposed framework,\na Transformer encoder learns the underlying patterns in the original trajectory\ndataset and converts each data point into a context vector in the learned\nlatent space. The converted dataset in the latent space is projected into\nreduced dimensions using principal component analysis (PCA), and a Gaussian\nmixture model (GMM) is applied to fit the probability distribution of the data\npoints in the reduced-dimensional space. Finally, new samples are drawn from\nthe fitted GMM, the dimension of the samples is reverted to the original\ndimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several\nexperiments demonstrate that the framework effectively generates new,\nhigh-quality synthetic aircraft trajectory data, which were compared to the\nresults of several baselines.", "AI": {"tldr": "提出了一种名为ATRADA的框架，用于通过生成合成轨迹数据增强飞机轨迹数据集，提升模型的鲁棒性。", "motivation": "飞机轨迹建模对空中交通管理至关重要，但现有数据集往往不足或不平衡，需通过数据增强提升模型性能。", "method": "使用Transformer编码器学习轨迹数据的潜在模式，通过PCA降维和GMM拟合数据分布，生成新样本并用MLP解码。", "result": "实验表明，ATRADA能生成高质量的合成轨迹数据，优于多个基线方法。", "conclusion": "ATRADA为飞机轨迹数据增强提供了一种有效解决方案，有助于提升下游任务的性能。"}}
{"id": "2506.07847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07847", "abs": "https://arxiv.org/abs/2506.07847", "authors": ["Hengzhi Chen", "Liqian Feng", "Wenhua Wu", "Xiaogang Zhu", "Shawn Leo", "Kun Hu"], "title": "F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation", "comment": null, "summary": "Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery\nis critical for applications like environmental monitoring and urban planning\nbut faces computational and optimization challenges. Conventional methods\neither lose fine details through downsampling or fragment global context via\npatch processing. While multi-branch networks address this trade-off, they\nsuffer from computational inefficiency and conflicting gradient dynamics during\ntraining. We propose F2Net, a frequency-aware framework that decomposes UHR\nimages into high- and low-frequency components for specialized processing. The\nhigh-frequency branch preserves full-resolution structural details, while the\nlow-frequency branch processes downsampled inputs through dual sub-branches\ncapturing short- and long-range dependencies. A Hybrid-Frequency Fusion module\nintegrates these observations, guided by two novel objectives: Cross-Frequency\nAlignment Loss ensures semantic consistency between frequency components, and\nCross-Frequency Balance Loss regulates gradient magnitudes across branches to\nstabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net\nachieves state-of-the-art performance with mIoU of 80.22 and 83.39,\nrespectively. Our code will be publicly available.", "AI": {"tldr": "F2Net是一个频率感知框架，通过分解超高清遥感图像为高频和低频组件进行专门处理，解决了传统方法在细节丢失和全局上下文碎片化之间的权衡问题。", "motivation": "超高清遥感图像的语义分割在环境监测和城市规划中至关重要，但传统方法存在细节丢失或全局上下文碎片化的问题，多分支网络则存在计算效率低和梯度冲突的挑战。", "method": "F2Net将图像分解为高频和低频组件，高频分支保留全分辨率细节，低频分支通过双子分支捕获短程和长程依赖关系，并通过混合频率融合模块整合结果。", "result": "在DeepGlobe和Inria Aerial基准测试中，F2Net分别达到80.22和83.39的mIoU，表现最优。", "conclusion": "F2Net通过频率分解和融合模块，实现了高效且稳定的超高清遥感图像语义分割。"}}
{"id": "2506.07587", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07587", "abs": "https://arxiv.org/abs/2506.07587", "authors": ["Tongzhou Yu", "Zhuhao Zhang", "Guanghui Zhu", "Shen Jiang", "Meikang Qiu", "Yihua Huang"], "title": "PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs", "comment": null, "summary": "Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and\npromising approaches for fine-tuning pre-trained language models. Compared with\nFull parameter Fine-Tuning (FFT), PEFT achieved comparable task performance\nwith a substantial reduction of trainable parameters, which largely saved the\ntraining and storage costs. However, using the PEFT method requires considering\na vast design space, such as the type of PEFT modules and their insertion\nlayers. Inadequate configurations can lead to sub-optimal results. Conventional\nsolutions such as architectural search techniques, while effective, tend to\nintroduce substantial additional overhead. In this paper, we propose a novel\napproach, PrunePEFT, which formulates the PEFT strategy search as a pruning\nproblem and introduces a hybrid pruning strategy that capitalizes on the\nsensitivity of pruning methods to different PEFT modules. This method extends\ntraditional pruning techniques by iteratively removing redundant or conflicting\nPEFT modules, thereby optimizing the fine-tuned configuration. By efficiently\nidentifying the most relevant modules, our approach significantly reduces the\ncomputational burden typically associated with architectural search processes,\nmaking it a more scalable and efficient solution for fine-tuning large\npre-trained models.", "AI": {"tldr": "PrunePEFT将PEFT策略搜索转化为剪枝问题，通过混合剪枝策略优化模块配置，显著降低计算负担。", "motivation": "PEFT方法虽高效，但设计空间庞大，传统搜索方法开销大。", "method": "将PEFT策略搜索视为剪枝问题，采用混合剪枝策略迭代移除冗余模块。", "result": "显著减少计算负担，优化配置。", "conclusion": "PrunePEFT是一种高效、可扩展的PEFT策略搜索方法。"}}
{"id": "2506.07848", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07848", "abs": "https://arxiv.org/abs/2506.07848", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Jiangning Zhang", "Yuan Zhou", "Qinglin Lu", "Ran Yi"], "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement", "comment": null, "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.", "AI": {"tldr": "PolyVivid是一个多主体视频定制框架，通过文本-图像融合模块和3D-RoPE增强模块实现身份一致性和交互性，优于现有方法。", "motivation": "现有视频生成模型在多主体定制中缺乏细粒度可控性和身份一致性，PolyVivid旨在解决这一问题。", "method": "设计了VLLM-based文本-图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，结合MLLM-based数据管道提升生成质量。", "result": "实验表明PolyVivid在身份保真度、视频真实性和主体对齐方面优于现有开源和商业基线。", "conclusion": "PolyVivid通过多模块协同实现了高质量的多主体视频生成，具有显著优势。"}}
{"id": "2506.07595", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07595", "abs": "https://arxiv.org/abs/2506.07595", "authors": ["Hao Qiu", "Emmanuel Esposito", "Mengxiao Zhang"], "title": "Exploiting Curvature in Online Convex Optimization with Delayed Feedback", "comment": null, "summary": "In this work, we study the online convex optimization problem with curved\nlosses and delayed feedback. When losses are strongly convex, existing\napproaches obtain regret bounds of order $d_{\\max} \\ln T$, where $d_{\\max}$ is\nthe maximum delay and $T$ is the time horizon. However, in many cases, this\nguarantee can be much worse than $\\sqrt{d_{\\mathrm{tot}}}$ as obtained by a\ndelayed version of online gradient descent, where $d_{\\mathrm{tot}}$ is the\ntotal delay. We bridge this gap by proposing a variant of\nfollow-the-regularized-leader that obtains regret of order\n$\\min\\{\\sigma_{\\max}\\ln T, \\sqrt{d_{\\mathrm{tot}}}\\}$, where $\\sigma_{\\max}$ is\nthe maximum number of missing observations. We then consider exp-concave losses\nand extend the Online Newton Step algorithm to handle delays with an adaptive\nlearning rate tuning, achieving regret $\\min\\{d_{\\max} n\\ln T,\n\\sqrt{d_{\\mathrm{tot}}}\\}$ where $n$ is the dimension. To our knowledge, this\nis the first algorithm to achieve such a regret bound for exp-concave losses.\nWe further consider the problem of unconstrained online linear regression and\nachieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth\nforecaster with a clipping trick. Finally, we implement our algorithms and\nconduct experiments under various types of delay and losses, showing an\nimproved performance over existing methods.", "AI": {"tldr": "论文研究了具有弯曲损失和延迟反馈的在线凸优化问题，提出了改进算法以减少遗憾界。", "motivation": "现有方法在强凸损失下遗憾界为$d_{\\max} \\ln T$，可能不如延迟版本的在线梯度下降的$\\sqrt{d_{\\mathrm{tot}}}$。论文旨在填补这一差距。", "method": "提出了一种改进的跟随正则化领导者算法，适应性地调整学习率，并扩展了在线牛顿步算法以处理延迟。", "result": "实现了遗憾界$\\min\\{\\sigma_{\\max}\\ln T, \\sqrt{d_{\\mathrm{tot}}}\\}$和$\\min\\{d_{\\max} n\\ln T, \\sqrt{d_{\\mathrm{tot}}}\\}$，并在实验中表现优于现有方法。", "conclusion": "论文提出的算法在理论和实验上均优于现有方法，填补了遗憾界的差距。"}}
{"id": "2506.07850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07850", "abs": "https://arxiv.org/abs/2506.07850", "authors": ["Arash Rocky", "Q. M. Jonathan Wu"], "title": "SAM2Auto: Auto Annotation Using FLASH", "comment": null, "summary": "Vision-Language Models (VLMs) lag behind Large Language Models due to the\nscarcity of annotated datasets, as creating paired visual-textual annotations\nis labor-intensive and expensive. To address this bottleneck, we introduce\nSAM2Auto, the first fully automated annotation pipeline for video datasets\nrequiring no human intervention or dataset-specific training. Our approach\nconsists of two key components: SMART-OD, a robust object detection system that\ncombines automatic mask generation with open-world object detection\ncapabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a\nmulti-object real-time video instance segmentation (VIS) that maintains\nconsistent object identification across video frames even with intermittent\ndetection gaps. Unlike existing open-world detection methods that require\nframe-specific hyperparameter tuning and suffer from numerous false positives,\nour system employs statistical approaches to minimize detection errors while\nensuring consistent object tracking throughout entire video sequences.\nExtensive experimental validation demonstrates that SAM2Auto achieves\ncomparable accuracy to manual annotation while dramatically reducing annotation\ntime and eliminating labor costs. The system successfully handles diverse\ndatasets without requiring retraining or extensive parameter adjustments,\nmaking it a practical solution for large-scale dataset creation. Our work\nestablishes a new baseline for automated video annotation and provides a\npathway for accelerating VLM development by addressing the fundamental dataset\nbottleneck that has constrained progress in vision-language understanding.", "AI": {"tldr": "SAM2Auto是一种全自动视频数据集标注工具，无需人工干预或数据集特定训练，显著减少标注时间和成本。", "motivation": "解决视觉语言模型（VLMs）因标注数据集稀缺而发展受限的问题。", "method": "结合SMART-OD（自动掩码生成与开放世界目标检测）和FLASH（实时视频实例分割），通过统计方法减少错误并保持对象跟踪一致性。", "result": "实验证明其精度接近人工标注，且能处理多样化数据集而无需重新训练或参数调整。", "conclusion": "SAM2Auto为自动视频标注设定了新基准，加速了VLM的发展。"}}
{"id": "2506.07596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07596", "abs": "https://arxiv.org/abs/2506.07596", "authors": ["Torsten Krauß", "Hamid Dashtbani", "Alexandra Dmitrienko"], "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts", "comment": "26 pages, 25 tables, 13 figures, 2 algorithms, to appear in the 43th\n  USENIX Security Symposium (USENIX Security 2025)", "summary": "Machine learning is advancing rapidly, with applications bringing notable\nbenefits, such as improvements in translation and code generation. Models like\nChatGPT, powered by Large Language Models (LLMs), are increasingly integrated\ninto daily life. However, alongside these benefits, LLMs also introduce social\nrisks. Malicious users can exploit LLMs by submitting harmful prompts, such as\nrequesting instructions for illegal activities. To mitigate this, models often\ninclude a security mechanism that automatically rejects such harmful prompts.\nHowever, they can be bypassed through LLM jailbreaks. Current jailbreaks often\nrequire significant manual effort, high computational costs, or result in\nexcessive model modifications that may degrade regular utility.\n  We introduce TwinBreak, an innovative safety alignment removal method.\nBuilding on the idea that the safety mechanism operates like an embedded\nbackdoor, TwinBreak identifies and prunes parameters responsible for this\nfunctionality. By focusing on the most relevant model layers, TwinBreak\nperforms fine-grained analysis of parameters essential to model utility and\nsafety. TwinBreak is the first method to analyze intermediate outputs from\nprompts with high structural and content similarity to isolate safety\nparameters. We present the TwinPrompt dataset containing 100 such twin prompts.\nExperiments confirm TwinBreak's effectiveness, achieving 89% to 98% success\nrates with minimal computational requirements across 16 LLMs from five vendors.", "AI": {"tldr": "TwinBreak是一种创新的安全对齐移除方法，通过识别和修剪负责安全功能的参数，有效绕过LLMs的安全机制，成功率高达89%至98%。", "motivation": "尽管LLMs带来了显著好处，但也存在被恶意用户利用的风险。现有安全机制可能被绕过，而传统绕过方法成本高或影响模型性能。", "method": "TwinBreak通过分析中间输出，识别并修剪与安全功能相关的参数，同时保留模型实用性。", "result": "实验表明，TwinBreak在16个LLMs上成功率达到89%至98%，且计算成本低。", "conclusion": "TwinBreak提供了一种高效、低成本的方法来绕过LLMs的安全机制，凸显了现有安全措施的脆弱性。"}}
{"id": "2506.07616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07616", "abs": "https://arxiv.org/abs/2506.07616", "authors": ["Zhixin Geng", "Xu Fan", "Xiqiao Lu", "Yan Zhang", "Guangyuan Yu", "Cheng Huang", "Qian Wang", "Yuewu Li", "Weichun Ma", "Qi Yu", "Libo Wu", "Hao Li"], "title": "FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning", "comment": null, "summary": "Air pollution has emerged as a major public health challenge in megacities.\nNumerical simulations and single-site machine learning approaches have been\nwidely applied in air quality forecasting tasks. However, these methods face\nmultiple limitations, including high computational costs, low operational\nefficiency, and limited integration with observational data. With the rapid\nadvancement of artificial intelligence, there is an urgent need to develop a\nlow-cost, efficient air quality forecasting model for smart urban management.\nAn air quality forecasting model, named FuXi-Air, has been constructed in this\nstudy based on multimodal data fusion to support high-precision air quality\nforecasting and operated in typical megacities. The model integrates\nmeteorological forecasts, emission inventories, and pollutant monitoring data\nunder the guidance of air pollution mechanism. By combining an autoregressive\nprediction framework with a frame interpolation strategy, the model\nsuccessfully completes 72-hour forecasts for six major air pollutants at an\nhourly resolution across multiple monitoring sites within 25-30 seconds. In\nterms of both computational efficiency and forecasting accuracy, it outperforms\nthe mainstream numerical air quality models in operational forecasting work.\nAblation experiments concerning key influencing factors show that although\nmeteorological data contribute more to model accuracy than emission inventories\ndo, the integration of multimodal data significantly improves forecasting\nprecision and ensures that reliable predictions are obtained under differing\npollution mechanisms across megacities. This study provides both a technical\nreference and a practical example for applying multimodal data-driven models to\nair quality forecasting and offers new insights into building hybrid\nforecasting systems to support air pollution risk warning in smart city\nmanagement.", "AI": {"tldr": "FuXi-Air模型通过多模态数据融合，实现了高效、低成本的空气质量预测，优于主流数值模型。", "motivation": "解决传统空气质量预测方法的高计算成本、低效率和数据整合不足的问题。", "method": "结合气象预报、排放清单和污染物监测数据，采用自回归预测框架和帧插值策略。", "result": "在72小时内完成多站点、多污染物的预测，计算效率和准确性均优于主流模型。", "conclusion": "多模态数据驱动的模型为智能城市管理提供了技术参考和实践范例。"}}
{"id": "2506.07860", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07860", "abs": "https://arxiv.org/abs/2506.07860", "authors": ["Ivan Alberico", "Marco Cannici", "Giovanni Cioffi", "Davide Scaramuzza"], "title": "Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction", "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), Nashville (TN), USA, 2025; 5th International Workshop on\n  Event-Based Vision", "summary": "In this paper, we present a real-time egocentric trajectory prediction system\nfor table tennis using event cameras. Unlike standard cameras, which suffer\nfrom high latency and motion blur at fast ball speeds, event cameras provide\nhigher temporal resolution, allowing more frequent state updates, greater\nrobustness to outliers, and accurate trajectory predictions using just a short\ntime window after the opponent's impact. We collect a dataset of ping-pong game\nsequences, including 3D ground-truth trajectories of the ball, synchronized\nwith sensor data from the Meta Project Aria glasses and event streams. Our\nsystem leverages foveated vision, using eye-gaze data from the glasses to\nprocess only events in the viewer's fovea. This biologically inspired approach\nimproves ball detection performance and significantly reduces computational\nlatency, as it efficiently allocates resources to the most perceptually\nrelevant regions, achieving a reduction factor of 10.81 on the collected\ntrajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,\nincluding computation and perception - significantly lower than a frame-based\n30 FPS system, which, in the worst case, takes 66 ms solely for perception.\nFinally, we fit a trajectory prediction model to the estimated states of the\nball, enabling 3D trajectory forecasting in the future. To the best of our\nknowledge, this is the first approach to predict table tennis trajectories from\nan egocentric perspective using event cameras.", "AI": {"tldr": "提出了一种基于事件相机的实时乒乓球轨迹预测系统，利用高时间分辨率提升性能。", "motivation": "解决传统相机在高速度乒乓球运动中存在的延迟和运动模糊问题。", "method": "结合事件相机、眼动数据和生物启发式视觉处理，优化资源分配。", "result": "系统总延迟仅为4.5毫秒，比传统30 FPS系统快14.6倍。", "conclusion": "首次实现基于事件相机的第一视角乒乓球轨迹预测，性能显著提升。"}}
{"id": "2506.07619", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.07619", "abs": "https://arxiv.org/abs/2506.07619", "authors": ["Toby Boyne", "Juan S. Campos", "Becky D. Langdon", "Jixiang Qing", "Yilin Xie", "Shiqiang Zhang", "Calvin Tsay", "Ruth Misener", "Daniel W. Davies", "Kim E. Jelfs", "Sarah Boyall", "Thomas M. Dixon", "Linden Schrecker", "Jose Pablo Folch"], "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning", "comment": null, "summary": "Machine learning has promised to change the landscape of laboratory\nchemistry, with impressive results in molecular property prediction and\nreaction retro-synthesis. However, chemical datasets are often inaccessible to\nthe machine learning community as they tend to require cleaning, thorough\nunderstanding of the chemistry, or are simply not available. In this paper, we\nintroduce a novel dataset for yield prediction, providing the first-ever\ntransient flow dataset for machine learning benchmarking, covering over 1200\nprocess conditions. While previous datasets focus on discrete parameters, our\nexperimental set-up allow us to sample a large number of continuous process\nconditions, generating new challenges for machine learning models. We focus on\nsolvent selection, a task that is particularly difficult to model theoretically\nand therefore ripe for machine learning applications. We showcase benchmarking\nfor regression algorithms, transfer-learning approaches, feature engineering,\nand active learning, with important applications towards solvent replacement\nand sustainable manufacturing.", "AI": {"tldr": "论文介绍了一个用于产率预测的新型数据集，首次提供了用于机器学习基准测试的瞬态流动数据集，覆盖1200多种工艺条件，重点关注溶剂选择任务。", "motivation": "化学数据集通常难以获取或需要清洗，限制了机器学习的应用。本文旨在填补这一空白，提供首个瞬态流动数据集，推动机器学习在化学领域的应用。", "method": "通过实验设置采样大量连续工艺条件，生成新数据集，并用于回归算法、迁移学习、特征工程和主动学习的基准测试。", "result": "数据集覆盖1200多种工艺条件，为溶剂选择等任务提供了新的机器学习挑战和应用场景。", "conclusion": "该数据集为机器学习在化学领域的应用提供了重要资源，尤其在溶剂替换和可持续制造方面具有潜力。"}}
{"id": "2506.07863", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.07863", "abs": "https://arxiv.org/abs/2506.07863", "authors": ["Lev Novitskiy", "Viacheslav Vasilev", "Maria Kovaleva", "Vladimir Arkhipkin", "Denis Dimitrov"], "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "comment": null, "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer\nvision, yet their training is often plagued by artifacts that degrade\nreconstruction and generation quality. This paper introduces VIVAT, a\nsystematic approach to mitigating common artifacts in KL-VAE training without\nrequiring radical architectural changes. We present a detailed taxonomy of five\nprevalent artifacts - color shift, grid patterns, blur, corner and droplet\nartifacts - and analyze their root causes. Through straightforward\nmodifications, including adjustments to loss weights, padding strategies, and\nthe integration of Spatially Conditional Normalization, we demonstrate\nsignificant improvements in VAE performance. Our method achieves\nstate-of-the-art results in image reconstruction metrics (PSNR and SSIM) across\nmultiple benchmarks and enhances text-to-image generation quality, as evidenced\nby superior CLIP scores. By preserving the simplicity of the KL-VAE framework\nwhile addressing its practical challenges, VIVAT offers actionable insights for\nresearchers and practitioners aiming to optimize VAE training.", "AI": {"tldr": "VIVAT通过系统方法减少KL-VAE训练中的常见伪影，提升重建和生成质量，无需大幅架构改动。", "motivation": "解决KL-VAE训练中常见的伪影问题，如色彩偏移、网格模式等，以提升模型性能。", "method": "提出损失权重调整、填充策略改进和空间条件归一化等方法，优化训练过程。", "result": "在PSNR和SSIM指标上取得SOTA结果，文本到图像生成的CLIP分数也显著提升。", "conclusion": "VIVAT为优化VAE训练提供了实用解决方案，同时保持了KL-VAE的简洁性。"}}
{"id": "2506.07624", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07624", "abs": "https://arxiv.org/abs/2506.07624", "authors": ["Ali Hariri", "Álvaro Arroyo", "Alessio Gravina", "Moshe Eliasof", "Carola-Bibiane Schönlieb", "Davide Bacciu", "Kamyar Azizzadenesheli", "Xiaowen Dong", "Pierre Vandergheynst"], "title": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks", "comment": null, "summary": "ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by\nMessage Passing Neural Networks (MPNNs), which gained popularity for their\nsimplicity and effectiveness in capturing local graph structure. Despite their\nsuccess, MPNNs are limited in their ability to capture long-range dependencies\nbetween nodes. This has led researchers to adapt MPNNs through rewiring or make\nuse of Graph Transformers, which compromises the computational efficiency that\ncharacterized early spatial message-passing architectures, and typically\ndisregards the graph structure. Almost a decade after its original\nintroduction, we revisit ChebNet to shed light on its ability to model distant\nnode interactions. We find that out-of-box, ChebNet already shows competitive\nadvantages relative to classical MPNNs and GTs on long-range benchmarks, while\nmaintaining good scalability properties for high-order polynomials. However, we\nuncover that this polynomial expansion leads ChebNet to an unstable regime\nduring training. To address this limitation, we cast ChebNet as a stable and\nnon-dissipative dynamical system, which we coin Stable-ChebNet. Our\nStable-ChebNet model allows for stable information propagation, and has\ncontrollable dynamics which do not require the use of eigendecompositions,\npositional encodings, or graph rewiring. Across several benchmarks,\nStable-ChebNet achieves near state-of-the-art performance.", "AI": {"tldr": "ChebNet在长距离节点交互建模中表现优于MPNNs和Graph Transformers，但存在训练不稳定的问题。改进后的Stable-ChebNet解决了这一问题，并在多个基准测试中接近SOTA性能。", "motivation": "MPNNs和Graph Transformers在捕捉长距离依赖关系时存在局限性，而ChebNet虽然早期被忽视，但其在长距离建模中具有潜力。", "method": "通过将ChebNet建模为稳定的动态系统（Stable-ChebNet），解决了多项式扩展导致的训练不稳定问题。", "result": "Stable-ChebNet在长距离基准测试中表现优异，且无需复杂的图结构调整或特征分解。", "conclusion": "Stable-ChebNet是一种高效且稳定的方法，适用于长距离节点交互建模。"}}
{"id": "2506.07661", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07661", "abs": "https://arxiv.org/abs/2506.07661", "authors": ["Meir Feder", "Ruediger Urbanke", "Yaniv Fogel"], "title": "The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well", "comment": null, "summary": "A fundamental question in modern machine learning is why large,\nover-parameterized models, such as deep neural networks and transformers, tend\nto generalize well, even when their number of parameters far exceeds the number\nof training samples.\n  We investigate this phenomenon through the lens of information theory,\ngrounded in universal learning theory. Specifically, we study a Bayesian\nmixture learner with log-loss and (almost) uniform prior over an expansive\nhypothesis class.\n  Our key result shows that the learner's regret is not determined by the\noverall size of the hypothesis class, but rather by the cumulative probability\nof all models that are close, in Kullback-Leibler divergence distance, to the\ntrue data-generating process. We refer to this cumulative probability as the\nweight of the hypothesis.\n  This leads to a natural notion of model simplicity: simple models are those\nwith large weight and thus require fewer samples to generalize, while complex\nmodels have small weight and need more data. This perspective provides a\nrigorous and intuitive explanation for why over-parameterized models often\navoid overfitting: the presence of simple hypotheses allows the posterior to\nconcentrate on them when supported by the data.\n  We further bridge theory and practice by recalling that stochastic gradient\ndescent with Langevin dynamics samples from the correct posterior distribution,\nenabling our theoretical learner to be approximated using standard machine\nlearning methods combined with ensemble learning.\n  Our analysis yields non-uniform regret bounds and aligns with key practical\nconcepts such as flat minima and model distillation. The results apply broadly\nacross online, batch, and supervised learning settings, offering a unified and\nprincipled understanding of the generalization behavior of modern AI systems.", "AI": {"tldr": "论文通过信息论和贝叶斯混合学习框架，解释了过参数化模型泛化能力强的现象，提出假设权重概念，并证明其与模型简单性的关系。", "motivation": "研究过参数化模型（如深度神经网络）在参数远多于训练样本时仍能泛化良好的原因。", "method": "使用贝叶斯混合学习器，基于对数损失和均匀先验，分析假设类的累积概率（权重）对泛化的影响。", "result": "发现泛化能力取决于接近真实数据生成过程的假设权重，而非假设类规模，解释了过参数化模型避免过拟合的原因。", "conclusion": "理论框架与随机梯度下降实践结合，提供了对现代AI系统泛化行为的统一理解。"}}
{"id": "2506.07878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07878", "abs": "https://arxiv.org/abs/2506.07878", "authors": ["Muhammad Ahmed Humais", "Xiaoqian Huang", "Hussain Sajwani", "Sajid Javed", "Yahya Zweiri"], "title": "Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow", "comment": null, "summary": "Event cameras unlock new frontiers that were previously unthinkable with\nstandard frame-based cameras. One notable example is low-latency motion\nestimation (optical flow), which is critical for many real-time applications.\nIn such applications, the computational efficiency of algorithms is paramount.\nAlthough recent deep learning paradigms such as CNN, RNN, or ViT have shown\nremarkable performance, they often lack the desired computational efficiency.\nConversely, asynchronous event-based methods including SNNs and GNNs are\ncomputationally efficient; however, these approaches fail to capture sufficient\nspatio-temporal information, a powerful feature required to achieve better\nperformance for optical flow estimation. In this work, we introduce\nSpatio-Temporal State Space Model (STSSM) module along with a novel network\narchitecture to develop an extremely efficient solution with competitive\nperformance. Our STSSM module leverages state-space models to effectively\ncapture spatio-temporal correlations in event data, offering higher performance\nwith lower complexity compared to ViT, CNN-based architectures in similar\nsettings. Our model achieves 4.5x faster inference and 8x lower computations\ncompared to TMA and 2x lower computations compared to EV-FlowNet with\ncompetitive performance on the DSEC benchmark. Our code will be available at\nhttps://github.com/AhmedHumais/E-STMFlow", "AI": {"tldr": "论文提出了一种基于时空状态空间模型（STSSM）的高效事件相机运动估计算法，显著提升了计算效率和性能。", "motivation": "事件相机在低延迟运动估计中潜力巨大，但现有深度学习方法（如CNN、RNN、ViT）计算效率不足，而异步事件方法（如SNN、GNN）又无法充分捕捉时空信息。", "method": "引入STSSM模块和新型网络架构，利用状态空间模型高效捕捉事件数据的时空相关性。", "result": "模型在DSEC基准测试中表现优异，推理速度比TMA快4.5倍，计算量比EV-FlowNet低2倍。", "conclusion": "STSSM为事件相机的运动估计提供了一种高效且高性能的解决方案。"}}
{"id": "2506.07666", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07666", "abs": "https://arxiv.org/abs/2506.07666", "authors": ["Seyedhamidreza Mousavi", "Seyedali Mousavi", "Masoud Daneshtalab"], "title": "ProARD: progressive adversarial robustness distillation: provide wide range of robust students", "comment": null, "summary": "Adversarial Robustness Distillation (ARD) has emerged as an effective method\nto enhance the robustness of lightweight deep neural networks against\nadversarial attacks. Current ARD approaches have leveraged a large robust\nteacher network to train one robust lightweight student. However, due to the\ndiverse range of edge devices and resource constraints, current approaches\nrequire training a new student network from scratch to meet specific\nconstraints, leading to substantial computational costs and increased CO2\nemissions. This paper proposes Progressive Adversarial Robustness Distillation\n(ProARD), enabling the efficient one-time training of a dynamic network that\nsupports a diverse range of accurate and robust student networks without\nrequiring retraining. We first make a dynamic deep neural network based on\ndynamic layers by encompassing variations in width, depth, and expansion in\neach design stage to support a wide range of architectures. Then, we consider\nthe student network with the largest size as the dynamic teacher network.\nProARD trains this dynamic network using a weight-sharing mechanism to jointly\noptimize the dynamic teacher network and its internal student networks.\nHowever, due to the high computational cost of calculating exact gradients for\nall the students within the dynamic network, a sampling mechanism is required\nto select a subset of students. We show that random student sampling in each\niteration fails to produce accurate and robust students.", "AI": {"tldr": "ProARD提出了一种动态网络训练方法，通过一次训练支持多种轻量级学生网络，避免了重复训练的高成本和碳排放。", "motivation": "当前对抗鲁棒性蒸馏方法需要为不同资源限制的设备重新训练学生网络，导致高计算成本和碳排放。", "method": "基于动态层构建动态网络，通过权重共享机制联合优化动态教师网络及其内部学生网络，并采用采样机制选择部分学生以减少计算成本。", "result": "随机采样学生无法生成准确且鲁棒的学生网络。", "conclusion": "ProARD提供了一种高效的一次性训练方法，支持多样化的学生网络，同时避免了传统方法的高成本和碳排放。"}}
{"id": "2506.07885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07885", "abs": "https://arxiv.org/abs/2506.07885", "authors": ["Zubin Bhuyan", "Yuanchang Xie", "AngkeaReach Rith", "Xintong Yan", "Nasko Apostolov", "Jimi Oke", "Chengbo Ai"], "title": "CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing", "comment": null, "summary": "With the increasing availability of aerial and satellite imagery, deep\nlearning presents significant potential for transportation asset management,\nsafety analysis, and urban planning. This study introduces CrosswalkNet, a\nrobust and efficient deep learning framework designed to detect various types\nof pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet\nincorporates a novel detection approach that improves upon traditional object\ndetection strategies by utilizing oriented bounding boxes (OBB), enhancing\ndetection precision by accurately capturing crosswalks regardless of their\norientation. Several optimization techniques, including Convolutional Block\nAttention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine\nannealing, are implemented to maximize performance and efficiency. A\ncomprehensive dataset comprising over 23,000 annotated crosswalk instances is\nutilized to train and validate the proposed framework. The best-performing\nmodel achieves an impressive precision of 96.5% and a recall of 93.3% on aerial\nimagery from Massachusetts, demonstrating its accuracy and effectiveness.\nCrosswalkNet has also been successfully applied to datasets from New Hampshire,\nVirginia, and Maine without transfer learning or fine-tuning, showcasing its\nrobustness and strong generalization capability. Additionally, the crosswalk\ndetection results, processed using High-Performance Computing (HPC) platforms\nand provided in polygon shapefile format, have been shown to accelerate data\nprocessing and detection, supporting real-time analysis for safety and mobility\napplications. This integration offers policymakers, transportation engineers,\nand urban planners an effective instrument to enhance pedestrian safety and\nimprove urban mobility.", "AI": {"tldr": "CrosswalkNet是一种高效的深度学习框架，用于从高分辨率航拍图像中检测行人横道，通过优化技术和新型检测方法实现高精度和强泛化能力。", "motivation": "随着航拍和卫星图像的普及，深度学习在交通资产管理、安全分析和城市规划中具有巨大潜力。", "method": "采用定向边界框（OBB）和改进的检测策略，结合多种优化技术（如卷积块注意力和余弦退火），利用包含23,000多个标注样本的数据集进行训练。", "result": "在麻萨诸塞州的航拍图像上，模型精度达96.5%，召回率达93.3%，并在其他州数据集上表现出强泛化能力。", "conclusion": "CrosswalkNet为决策者和规划者提供了高效工具，支持实时分析和行人安全提升。"}}
{"id": "2506.07673", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07673", "abs": "https://arxiv.org/abs/2506.07673", "authors": ["Guanhua Zhang", "Florian E. Dorner", "Moritz Hardt"], "title": "How Benchmark Prediction from Fewer Data Misses the Mark", "comment": null, "summary": "Large language model (LLM) evaluation is increasingly costly, prompting\ninterest in methods that speed up evaluation by shrinking benchmark datasets.\nBenchmark prediction (also called efficient LLM evaluation) aims to select a\nsmall subset of evaluation points and predict overall benchmark performance\nfrom that subset. In this paper, we systematically assess the strengths and\nlimitations of 11 benchmark prediction methods across 19 diverse benchmarks.\nFirst, we identify a highly competitive baseline: Take a random sample and fit\na regression model on the sample to predict missing entries. Outperforming most\nexisting methods, this baseline challenges the assumption that careful subset\nselection is necessary for benchmark prediction. Second, we discover that all\nexisting methods crucially depend on model similarity. They work best when\ninterpolating scores among similar models. The effectiveness of benchmark\nprediction sharply declines when new models have higher accuracy than\npreviously seen models. In this setting of extrapolation, none of the previous\nmethods consistently beat a simple average over random samples. To improve over\nthe sample average, we introduce a new method inspired by augmented inverse\npropensity weighting. This method consistently outperforms the random sample\naverage even for extrapolation. However, its performance still relies on model\nsimilarity and the gains are modest in general. This shows that benchmark\nprediction fails just when it is most needed: at the evaluation frontier, where\nthe goal is to evaluate new models of unknown capabilities.", "AI": {"tldr": "论文研究了11种基准预测方法在19个不同基准上的表现，发现随机采样加回归模型优于多数现有方法，且现有方法依赖模型相似性。提出了一种新方法，在模型相似性条件下表现更好，但在前沿评估中仍有限。", "motivation": "大型语言模型（LLM）评估成本高，需通过缩小基准数据集加速评估。", "method": "系统评估11种基准预测方法，提出随机采样加回归模型作为基线，并引入新方法（基于增强逆倾向加权）。", "result": "现有方法依赖模型相似性，新方法在模型相似性条件下优于随机采样平均，但在前沿评估中效果有限。", "conclusion": "基准预测在评估前沿（新模型能力未知时）效果不佳，需进一步改进。"}}
{"id": "2506.07886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07886", "abs": "https://arxiv.org/abs/2506.07886", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "comment": null, "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "AI": {"tldr": "论文提出EgoM2P框架，通过高效的时间标记器和掩码建模解决多模态、多任务的自中心视觉问题，性能优于专用模型且速度更快。", "motivation": "自中心视觉的多模态信号理解对AR、机器人和人机交互至关重要，但数据异构性和缺失模态的伪标签生成困难限制了现有方法的扩展。", "method": "引入高效时间标记器，提出EgoM2P掩码建模框架，学习时间感知的多模态标记，支持多任务处理。", "result": "EgoM2P在注视预测、相机追踪和深度估计等任务中性能匹配或超越专用模型，速度提升一个数量级。", "conclusion": "EgoM2P为自中心4D理解提供通用解决方案，开源以推动研究。"}}
{"id": "2506.07706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07706", "abs": "https://arxiv.org/abs/2506.07706", "authors": ["Boris Martirosyan", "Alexey Karmanov"], "title": "Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation", "comment": null, "summary": "Latent diffusion models (LDMs) achieve state-of-the-art performance across\nvarious tasks, including image generation and video synthesis. However, they\ngenerally lack robustness, a limitation that remains not fully explored in\ncurrent research. In this paper, we propose several methods to address this\ngap. First, we hypothesize that the robustness of LDMs primarily should be\nmeasured without their text encoder, because if we take and explore the whole\narchitecture, the problems of image generator and text encoders wll be fused.\nSecond, we introduce novel data augmentation techniques designed to reveal\nrobustness shortcomings in LDMs when processing diverse textual prompts. We\nthen fine-tune Stable Diffusion 3 and Stable Diffusion XL models using\nDreambooth, incorporating these proposed augmentation methods across multiple\ntasks. Finally, we propose a novel evaluation pipeline specifically tailored to\nassess the robustness of LDMs fine-tuned via Dreambooth.", "AI": {"tldr": "本文提出改进潜在扩散模型（LDMs）鲁棒性的方法，包括分离文本编码器评估、新数据增强技术、DreamBooth微调及专用评估流程。", "motivation": "当前LDMs在鲁棒性方面存在不足，且研究不充分，本文旨在填补这一空白。", "method": "1. 分离文本编码器评估鲁棒性；2. 设计新数据增强技术；3. 用DreamBooth微调模型；4. 提出专用评估流程。", "result": "改进了Stable Diffusion 3和XL模型的鲁棒性。", "conclusion": "提出的方法有效提升了LDMs的鲁棒性，为未来研究提供了新方向。"}}
{"id": "2506.07891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07891", "abs": "https://arxiv.org/abs/2506.07891", "authors": ["Simone Facchiano", "Stefano Saravalle", "Matteo Migliarini", "Edoardo De Matteis", "Alessio Sampieri", "Andrea Pilzer", "Emanuele Rodolà", "Indro Spinelli", "Luca Franco", "Fabio Galasso"], "title": "Video Unlearning via Low-Rank Refusal Vector", "comment": null, "summary": "Video generative models democratize the creation of visual content through\nintuitive instruction following, but they also inherit the biases and harmful\nconcepts embedded within their web-scale training data. This inheritance\ncreates a significant risk, as users can readily generate undesirable and even\nillegal content. This work introduces the first unlearning technique tailored\nexplicitly for video diffusion models to address this critical issue. Our\nmethod requires 5 multi-modal prompt pairs only. Each pair contains a \"safe\"\nand an \"unsafe\" example that differ only by the target concept. Averaging their\nper-layer latent differences produces a \"refusal vector\", which, once\nsubtracted from the model parameters, neutralizes the unsafe concept. We\nintroduce a novel low-rank factorization approach on the covariance difference\nof embeddings that yields robust refusal vectors. This isolates the target\nconcept while minimizing collateral unlearning of other semantics, thus\npreserving the visual quality of the generated video. Our method preserves the\nmodel's generation quality while operating without retraining or access to the\noriginal training data. By embedding the refusal direction directly into the\nmodel's weights, the suppression mechanism becomes inherently more robust\nagainst adversarial bypass attempts compared to surface-level input-output\nfilters. In a thorough qualitative and quantitative evaluation, we show that we\ncan neutralize a variety of harmful contents, including explicit nudity,\ngraphic violence, copyrights, and trademarks. Project page:\nhttps://www.pinlab.org/video-unlearning.", "AI": {"tldr": "该论文提出了一种针对视频扩散模型的无学习技术，通过少量多模态提示对生成“拒绝向量”，用于消除模型中的有害概念，同时保持生成质量。", "motivation": "视频生成模型可能继承训练数据中的偏见和有害内容，导致用户生成不良或非法内容，亟需解决方案。", "method": "使用5对多模态提示（安全与不安全示例），计算其潜在差异生成“拒绝向量”，并通过低秩分解方法优化，直接嵌入模型权重。", "result": "方法有效中和多种有害内容（如裸露、暴力、版权问题等），同时保持视频生成质量，无需重新训练或原始数据。", "conclusion": "该技术为视频生成模型提供了一种高效、鲁棒的无学习方案，显著降低了有害内容生成的风险。"}}
{"id": "2506.07735", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07735", "abs": "https://arxiv.org/abs/2506.07735", "authors": ["Haizhao Jing", "Haokui Zhang", "Zhenhao Shang", "Rong Xiao", "Peng Wang", "Yanning Zhang"], "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "comment": "9 pages, 3 figures", "summary": "Neural Architecture Representation Learning aims to transform network models\ninto feature representations for predicting network attributes, playing a\ncrucial role in deploying and designing networks for real-world applications.\nRecently, inspired by the success of transformers, transformer-based models\nintegrated with Graph Neural Networks (GNNs) have achieved significant progress\nin representation learning. However, current methods still have some\nlimitations. First, existing methods overlook hardware attribute information,\nwhich conflicts with the current trend of diversified deep learning hardware\nand limits the practical applicability of models. Second, current encoding\napproaches rely on static adjacency matrices to represent topological\nstructures, failing to capture the structural differences between computational\nnodes, which ultimately compromises encoding effectiveness. In this paper, we\nintroduce LeDG-Former, an innovative framework that addresses these limitations\nthrough the synergistic integration of language-based semantic embedding and\ndynamic graph representation learning. Specifically, inspired by large language\nmodels (LLMs), we propose a language embedding framework where both neural\narchitectures and hardware platform specifications are projected into a unified\nsemantic space through tokenization and LLM processing, enabling zero-shot\nprediction across different hardware platforms for the first time. Then, we\npropose a dynamic graph-based transformer for modeling neural architectures,\nresulting in improved neural architecture modeling performance. On the NNLQP\nbenchmark, LeDG-Former surpasses previous methods, establishing a new SOTA\nwhile demonstrating the first successful cross-hardware latency prediction\ncapability. Furthermore, our framework achieves superior performance on the\ncell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "AI": {"tldr": "LeDG-Former结合语言嵌入和动态图表示学习，解决了现有方法忽略硬件属性和静态邻接矩阵的局限性，实现了跨硬件零样本预测和更优的神经网络建模。", "motivation": "现有方法忽视硬件属性信息且依赖静态邻接矩阵，限制了模型的实际应用和编码效果。", "method": "提出LeDG-Former框架，结合语言嵌入（通过LLM处理）和动态图Transformer，统一语义空间并改进建模。", "result": "在NNLQP基准上超越现有方法，实现跨硬件延迟预测，并在NAS-Bench数据集上表现优异。", "conclusion": "LeDG-Former通过创新框架解决了关键问题，为神经网络表示学习提供了新方向。"}}
{"id": "2506.07905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07905", "abs": "https://arxiv.org/abs/2506.07905", "authors": ["Jie Yang", "Feipeng Ma", "Zitian Wang", "Dacheng Yin", "Kang Rong", "Fengyun Rao", "Ruimao Zhang"], "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning", "comment": null, "summary": "Building on the success of text-based reasoning models like DeepSeek-R1,\nextending these capabilities to multimodal reasoning holds great promise. While\nrecent works have attempted to adapt DeepSeek-R1-style reinforcement learning\n(RL) training paradigms to multimodal large language models (MLLM), focusing on\ndomain-specific tasks like math and visual perception, a critical question\nremains: How can we achieve the general-purpose visual-language reasoning\nthrough RL? To address this challenge, we make three key efforts: (1) A novel\nScalable Multimodal QA Synthesis pipeline that autonomously generates\ncontext-aware, reasoning-centric question-answer (QA) pairs directly from the\ngiven images. (2) The open-source WeThink dataset containing over 120K\nmultimodal QA pairs with annotated reasoning paths, curated from 18 diverse\ndataset sources and covering various question domains. (3) A comprehensive\nexploration of RL on our dataset, incorporating a hybrid reward mechanism that\ncombines rule-based verification with model-based assessment to optimize RL\ntraining efficiency across various task domains. Across 14 diverse MLLM\nbenchmarks, we demonstrate that our WeThink dataset significantly enhances\nperformance, from mathematical reasoning to diverse general multimodal tasks.\nMoreover, we show that our automated data pipeline can continuously increase\ndata diversity to further improve model performance.", "AI": {"tldr": "论文提出了一种通过强化学习实现通用视觉-语言推理的方法，包括自动生成多模态QA对的数据合成流程、开源数据集WeThink，以及混合奖励机制的RL训练。", "motivation": "扩展文本推理模型（如DeepSeek-R1）到多模态领域，解决通用视觉-语言推理的挑战。", "method": "1. 开发可扩展的多模态QA合成流程；2. 构建包含12万QA对的开源数据集WeThink；3. 探索混合奖励机制的RL训练。", "result": "在14个多模态基准测试中，WeThink显著提升了模型性能，数据多样性持续优化。", "conclusion": "提出的方法有效提升了通用多模态推理能力，数据合成流程可扩展性强。"}}
{"id": "2506.07925", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.07925", "abs": "https://arxiv.org/abs/2506.07925", "authors": ["Yaxita Amin", "Naimisha S Trivedi", "Rashmi Bhattad"], "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images", "comment": null, "summary": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.", "AI": {"tldr": "本文对34篇论文进行了综合分析，比较了18种U-Net变体在遥感变化检测中的应用潜力，并评估了它们的优缺点。", "motivation": "填补U-Net架构在遥感变化检测领域应用的研究空白。", "method": "通过文献综述和比较分析，评估18种U-Net变体在遥感变化检测中的表现。", "result": "研究发现，Siamese Swin-U-Net等专为变化检测设计的变体表现突出，强调了处理多时相数据和长距离关系的重要性。", "conclusion": "本研究为选择U-Net变体进行遥感变化检测的研究者和实践者提供了有价值的参考。"}}
{"id": "2506.07747", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07747", "abs": "https://arxiv.org/abs/2506.07747", "authors": ["Adam Breuer"], "title": "E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time", "comment": "ICML 2025; Code available at: https://github.com/BreuerLabs/E- LDA", "summary": "In this paper, we provide the first practical algorithms with provable\nguarantees for the problem of inferring the topics assigned to each document in\nan LDA topic model. This is the primary inference problem for many applications\nof topic models in social science, data exploration, and causal inference\nsettings. We obtain this result by showing a novel non-gradient-based,\ncombinatorial approach to estimating topic models. This yields algorithms that\nconverge to near-optimal posterior probability in logarithmic parallel\ncomputation time (adaptivity) -- exponentially faster than any known LDA\nalgorithm. We also show that our approach can provide interpretability\nguarantees such that each learned topic is formally associated with a known\nkeyword. Finally, we show that unlike alternatives, our approach can maintain\nthe independence assumptions necessary to use the learned topic model for\ndownstream causal inference methods that allow researchers to study topics as\ntreatments. In terms of practical performance, our approach consistently\nreturns solutions of higher semantic quality than solutions from\nstate-of-the-art LDA algorithms, neural topic models, and LLM-based topic\nmodels across a diverse range of text datasets and evaluation parameters.", "AI": {"tldr": "本文提出了一种新颖的非梯度组合方法，用于推断LDA主题模型中每个文档的主题分配，其算法在并行计算时间上对数收敛，显著优于现有方法，并提供了解释性和因果推断的保证。", "motivation": "解决LDA主题模型在社会科学、数据探索和因果推断应用中的主要推断问题，提供高效且可解释的算法。", "method": "采用非梯度组合方法估计主题模型，实现对数并行计算时间的收敛。", "result": "算法在语义质量上优于现有LDA、神经主题模型和基于LLM的主题模型，并提供了解释性和因果推断的保证。", "conclusion": "该方法在效率和实用性上显著优于现有技术，适用于广泛的文本数据集和评估参数。"}}
{"id": "2506.07936", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07936", "abs": "https://arxiv.org/abs/2506.07936", "authors": ["Chengyue Huang", "Yuchen Zhu", "Sichen Zhu", "Jingyun Xiao", "Moises Andrade", "Shivang Chopra", "Zsolt Kira"], "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.", "AI": {"tldr": "研究发现视觉语言模型（VLMs）在多模态上下文学习（MM-ICL）中依赖浅层启发式方法，而非真正任务理解。通过分布偏移评估，发现性能随演示增加而下降，模型倾向于复制答案。提出的新方法加入推理步骤，但实验表明当前VLMs未有效利用演示信息。", "motivation": "探讨VLMs是否真正具备多模态上下文学习能力，而非依赖浅层启发式方法。", "method": "提出MM-ICL with Reasoning管道，为每个演示生成答案和推理依据，并在不同数据集和模型上进行实验。", "result": "性能对演示数量、检索方法等不敏感，表明当前VLMs未有效利用演示信息。", "conclusion": "当前VLMs在多模态上下文学习中表现有限，需进一步改进以提升任务理解能力。"}}
{"id": "2506.07754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07754", "abs": "https://arxiv.org/abs/2506.07754", "authors": ["Nicola Lavecchia", "Sid Fadanelli", "Federico Ricciuti", "Gennaro Aloe", "Enrico Bagli", "Pietro Giuffrida", "Daniele Vergari"], "title": "Comparing Credit Risk Estimates in the Gen-AI Era", "comment": null, "summary": "Generative AI technologies have demonstrated significant potential across\ndiverse applications. This study provides a comparative analysis of credit\nscore modeling techniques, contrasting traditional approaches with those\nleveraging generative AI. Our findings reveal that current generative AI models\nfall short of matching the performance of traditional methods, regardless of\nthe integration strategy employed. These results highlight the limitations in\nthe current capabilities of generative AI for credit risk scoring, emphasizing\nthe need for further research and development before the possibility of\napplying generative AI for this specific task, or equivalent ones.", "AI": {"tldr": "生成式AI在信用评分建模中表现不及传统方法，需进一步研究。", "motivation": "比较生成式AI与传统方法在信用评分建模中的表现。", "method": "对比分析生成式AI与传统信用评分建模技术。", "result": "生成式AI目前无法超越传统方法，表现较差。", "conclusion": "生成式AI在信用评分中仍有局限性，需更多研发。"}}
{"id": "2506.07943", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07943", "abs": "https://arxiv.org/abs/2506.07943", "authors": ["Yizhen Li", "Dell Zhang", "Xuelong Li", "Yiqing Shen"], "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "comment": null, "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "AI": {"tldr": "DTwinSeger提出了一种新的多模态视觉-文本任务方法，通过数字孪生（DT）表示将感知与推理解耦，利用LLM进行显式推理，显著提升了性能。", "motivation": "现有方法在图像标记化过程中破坏了对象的空间关系，限制了多模态推理能力。", "method": "采用两阶段方法：首先生成保留空间关系的DT表示，然后利用LLM进行推理。", "result": "在两个图像RS基准和三个图像参考分割基准上达到最优性能。", "conclusion": "DT表示是视觉与文本之间的有效桥梁，仅需LLM即可完成复杂多模态推理任务。"}}
{"id": "2506.07769", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07769", "abs": "https://arxiv.org/abs/2506.07769", "authors": ["Dekai Zhang", "Matthew Williams", "Francesca Toni"], "title": "Clustered Federated Learning via Embedding Distributions", "comment": "24 pages", "summary": "Federated learning (FL) is a widely used framework for machine learning in\ndistributed data environments where clients hold data that cannot be easily\ncentralised, such as for data protection reasons. FL, however, is known to be\nvulnerable to non-IID data. Clustered FL addresses this issue by finding more\nhomogeneous clusters of clients. We propose a novel one-shot clustering method,\nEMD-CFL, using the Earth Mover's distance (EMD) between data distributions in\nembedding space. We theoretically motivate the use of EMDs using results from\nthe domain adaptation literature and demonstrate empirically superior\nclustering performance in extensive comparisons against 16 baselines and on a\nrange of challenging datasets.", "AI": {"tldr": "提出了一种基于Earth Mover's distance（EMD）的单次聚类方法EMD-CFL，用于解决联邦学习中非独立同分布数据的问题。", "motivation": "联邦学习（FL）在分布式数据环境中广泛应用，但易受非独立同分布（non-IID）数据影响。", "method": "使用嵌入空间中数据分布的EMD进行一次性聚类。", "result": "在16种基线方法和多个挑战性数据集上表现出优越的聚类性能。", "conclusion": "EMD-CFL是一种有效解决FL中非IID数据问题的方法。"}}
{"id": "2506.07960", "categories": ["cs.CV", "I.4.6, J.5"], "pdf": "https://arxiv.org/pdf/2506.07960", "abs": "https://arxiv.org/abs/2506.07960", "authors": ["Ari Vesalainen", "Jenna Kanerva", "Aida Nitsch", "Kiia Korsu", "Ilari Larkiola", "Laura Ruotsalainen", "Filip Ginter"], "title": "Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920", "comment": null, "summary": "This article presents a large-scale effort to create a structured dataset of\ninternal migration in Finland between 1800 and 1920 using digitized church\nmoving records. These records, maintained by Evangelical-Lutheran parishes,\ndocument the migration of individuals and families and offer a valuable source\nfor studying historical demographic patterns. The dataset includes over six\nmillion entries extracted from approximately 200,000 images of handwritten\nmigration records.\n  The data extraction process was automated using a deep learning pipeline that\nincluded layout analysis, table detection, cell classification, and handwriting\nrecognition. The complete pipeline was applied to all images, resulting in a\nstructured dataset suitable for research.\n  The dataset can be used to study internal migration, urbanization, and family\nmigration, and the spread of disease in preindustrial Finland. A case study\nfrom the Elim\\\"aki parish shows how local migration histories can be\nreconstructed. The work demonstrates how large volumes of handwritten archival\nmaterial can be transformed into structured data to support historical and\ndemographic research.", "AI": {"tldr": "本文介绍了一个大规模项目，利用数字化教会迁移记录构建了芬兰1800年至1920年内部移民的结构化数据集。数据集包含600多万条记录，提取自约20万张手写迁移记录的图像。", "motivation": "研究动机是利用教会记录这一宝贵资源，研究历史人口模式，包括内部移民、城市化和疾病传播等。", "method": "方法是通过深度学习管道自动化数据提取，包括布局分析、表格检测、单元格分类和手写识别。", "result": "结果是一个结构化数据集，可用于研究历史人口学问题，并通过案例研究展示了如何重建地方移民历史。", "conclusion": "结论表明，大量手写档案材料可以转化为结构化数据，支持历史和人口学研究。"}}
{"id": "2506.07804", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07804", "abs": "https://arxiv.org/abs/2506.07804", "authors": ["Jie Bao", "Chuangyin Dang", "Rui Luo", "Hanwei Zhang", "Zhixin Zhou"], "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability", "comment": null, "summary": "As deep learning models are increasingly deployed in high-risk applications,\nrobust defenses against adversarial attacks and reliable performance guarantees\nbecome paramount. Moreover, accuracy alone does not provide sufficient\nassurance or reliable uncertainty estimates for these models. This study\nadvances adversarial training by leveraging principles from Conformal\nPrediction. Specifically, we develop an adversarial attack method, termed OPSA\n(OPtimal Size Attack), designed to reduce the efficiency of conformal\nprediction at any significance level by maximizing model uncertainty without\nrequiring coverage guarantees. Correspondingly, we introduce OPSA-AT\n(Adversarial Training), a defense strategy that integrates OPSA within a novel\nconformal training paradigm. Experimental evaluations demonstrate that our OPSA\nattack method induces greater uncertainty compared to baseline approaches for\nvarious defenses. Conversely, our OPSA-AT defensive model significantly\nenhances robustness not only against OPSA but also other adversarial attacks,\nand maintains reliable prediction. Our findings highlight the effectiveness of\nthis integrated approach for developing trustworthy and resilient deep learning\nmodels for safety-critical domains. Our code is available at\nhttps://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.", "AI": {"tldr": "该论文提出了一种基于Conformal Prediction的对抗攻击方法OPSA和防御策略OPSA-AT，旨在增强深度学习模型的鲁棒性和不确定性估计。实验表明，OPSA能有效增加模型不确定性，而OPSA-AT显著提升了模型对抗攻击的鲁棒性。", "motivation": "在深度学习模型应用于高风险领域时，对抗攻击的防御和不确定性估计变得至关重要。传统方法仅依赖准确性不足以保证模型的可靠性。", "method": "开发了OPSA攻击方法，通过最大化模型不确定性来降低Conformal Prediction的效率；提出了OPSA-AT防御策略，将OPSA融入新的Conformal训练范式。", "result": "OPSA攻击比基线方法更能增加模型不确定性；OPSA-AT防御模型显著提升了对抗攻击的鲁棒性，并保持了可靠的预测性能。", "conclusion": "该研究为安全关键领域开发可信赖且鲁棒的深度学习模型提供了一种有效方法。"}}
{"id": "2506.07964", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07964", "abs": "https://arxiv.org/abs/2506.07964", "authors": ["Wenxin Tang", "Jingyu Xiao", "Wenxuan Jiang", "Xi Xiao", "Yuhang Wang", "Xuxin Tang", "Qing Li", "Yuehe Ma", "Junliang Liu", "Shisong Tang", "Michael R. Lyu"], "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design", "comment": null, "summary": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.", "AI": {"tldr": "论文提出了Slide2Code基准和SlideCoder框架，用于从参考图像生成可编辑幻灯片，解决了现有方法在视觉和结构设计上的不足。", "motivation": "手动制作幻灯片耗时且需要专业知识，现有基于自然语言的LLM方法难以捕捉幻灯片设计的视觉和结构细节。", "method": "提出SlideCoder框架，结合颜色梯度分割算法和分层检索增强生成方法，并发布SlideMaster开源模型。", "result": "SlideCoder在布局保真度、执行准确性和视觉一致性上优于现有方法，最高提升40.5分。", "conclusion": "SlideCoder为幻灯片生成任务提供了高效解决方案，并开源了代码和模型。"}}
{"id": "2506.07806", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07806", "abs": "https://arxiv.org/abs/2506.07806", "authors": ["Avinash Kori", "Francesca Toni", "Ben Glocker"], "title": "Identifiable Object Representations under Spatial Ambiguities", "comment": null, "summary": "Modular object-centric representations are essential for *human-like\nreasoning* but are challenging to obtain under spatial ambiguities, *e.g. due\nto occlusions and view ambiguities*. However, addressing challenges presents\nboth theoretical and practical difficulties. We introduce a novel multi-view\nprobabilistic approach that aggregates view-specific slots to capture\n*invariant content* information while simultaneously learning disentangled\nglobal *viewpoint-level* information. Unlike prior single-view methods, our\napproach resolves spatial ambiguities, provides theoretical guarantees for\nidentifiability, and requires *no viewpoint annotations*. Extensive experiments\non standard benchmarks and novel complex datasets validate our method's\nrobustness and scalability.", "AI": {"tldr": "提出一种多视角概率方法，解决空间模糊性问题，无需视角标注，实现模块化对象中心表示。", "motivation": "模块化对象中心表示对人类推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以实现。", "method": "引入多视角概率方法，聚合视角特定槽以捕获不变内容信息，同时学习解耦的全局视角级信息。", "result": "在标准基准和新复杂数据集上的实验验证了方法的鲁棒性和可扩展性。", "conclusion": "该方法解决了空间模糊性，提供了可识别性理论保证，且无需视角标注。"}}
{"id": "2506.07966", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07966", "abs": "https://arxiv.org/abs/2506.07966", "authors": ["Ziyang Gong", "Wenhao Li", "Oliver Ma", "Songyuan Li", "Jiayi Ji", "Xue Yang", "Gen Luo", "Junchi Yan", "Rongrong Ji"], "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.", "AI": {"tldr": "SpaCE-10是一个用于评估多模态大语言模型（MLLMs）空间智能的综合基准，包含10种原子空间能力和8种组合能力，通过5k+ QA对和811个真实室内场景进行测试。", "motivation": "现有基准难以全面评估MLLMs从原子到组合层面的空间智能，因此需要SpaCE-10填补这一空白。", "method": "提出分层标注流程生成高质量QA对，并通过150+小时专家努力收集数据，覆盖点云输入和多选QA等场景。", "result": "先进MLLMs在SpaCE-10上表现仍远逊于人类，且计数能力不足限制了其组合空间能力。", "conclusion": "SpaCE-10为MLLM社区提供了重要发现和评估工具，揭示了现有模型的局限性。"}}
{"id": "2506.07822", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07822", "abs": "https://arxiv.org/abs/2506.07822", "authors": ["Xintong Duan", "Yutong He", "Fahim Tajwar", "Ruslan Salakhutdinov", "J. Zico Kolter", "Jeff Schneider"], "title": "Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation", "comment": null, "summary": "Although diffusion models have achieved strong results in decision-making\ntasks, their slow inference speed remains a key limitation. While the\nconsistency model offers a potential solution, its applications to\ndecision-making often struggle with suboptimal demonstrations or rely on\ncomplex concurrent training of multiple networks. In this work, we propose a\nnovel approach to consistency distillation for offline reinforcement learning\nthat directly incorporates reward optimization into the distillation process.\nOur method enables single-step generation while maintaining higher performance\nand simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and\nlong horizon planning demonstrate that our approach can achieve an 8.7%\nimprovement over previous state-of-the-art while offering up to 142x speedup\nover diffusion counterparts in inference time.", "AI": {"tldr": "提出了一种新的离线强化学习一致性蒸馏方法，将奖励优化直接融入蒸馏过程，实现单步生成，性能更高且训练更简单。", "motivation": "扩散模型在决策任务中表现优异，但推理速度慢；一致性模型虽能解决速度问题，但在决策任务中常受限于次优演示或需复杂多网络训练。", "method": "提出一种新颖的一致性蒸馏方法，将奖励优化直接融入蒸馏过程，实现单步生成。", "result": "在Gym MuJoCo基准测试和长时程规划中，性能提升8.7%，推理速度提升142倍。", "conclusion": "该方法在保持高性能的同时显著提升推理速度，简化训练过程。"}}
{"id": "2506.07971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07971", "abs": "https://arxiv.org/abs/2506.07971", "authors": ["Jiahao Meng", "Shuyang Sun", "Yue Tan", "Lu Qi", "Yunhai Tong", "Xiangtai Li", "Longyin Wen"], "title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding", "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.", "AI": {"tldr": "CyberV框架通过引入自适应系统，提升多模态大语言模型（MLLMs）在长视频理解中的性能，无需重新训练。", "motivation": "现有MLLMs在长视频理解中存在计算需求高、鲁棒性差和准确性不足的问题，尤其是参数较少的模型。", "method": "提出CyberV框架，包含MLLM推理系统、传感器和控制器，实现自我监控、纠正和动态资源分配。", "result": "实验显示CyberV显著提升模型性能，如Qwen2.5-VL-7B提升8.3%，InternVL3-8B提升5.5%，甚至接近人类专家水平。", "conclusion": "CyberV有效提升MLLMs在动态视频理解中的鲁棒性和准确性，具有广泛适用性。"}}
{"id": "2506.07829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07829", "abs": "https://arxiv.org/abs/2506.07829", "authors": ["Jan Corazza", "Hadi Partovi Aria", "Hyohun Kim", "Daniel Neider", "Zhe Xu"], "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information", "comment": null, "summary": "Reinforcement learning (RL) algorithms can find an optimal policy for a\nsingle agent to accomplish a particular task. However, many real-world problems\nrequire multiple agents to collaborate in order to achieve a common goal. For\nexample, a robot executing a task in a warehouse may require the assistance of\na drone to retrieve items from high shelves. In Decentralized Multi-Agent RL\n(DMARL), agents learn independently and then combine their policies at\nexecution time, but often must satisfy constraints on compatibility of local\npolicies to ensure that they can achieve the global task when combined. In this\npaper, we study how providing high-level symbolic knowledge to agents can help\naddress unique challenges of this setting, such as privacy constraints,\ncommunication limitations, and performance concerns. In particular, we extend\nthe formal tools used to check the compatibility of local policies with the\nteam task, making decentralized training with theoretical guarantees usable in\nmore scenarios. Furthermore, we empirically demonstrate that symbolic knowledge\nabout the temporal evolution of events in the environment can significantly\nexpedite the learning process in DMARL.", "AI": {"tldr": "论文研究了在去中心化多智能体强化学习（DMARL）中，如何利用高层符号知识解决隐私、通信和性能问题，并扩展了理论工具以确保局部策略的兼容性。", "motivation": "现实问题常需多智能体协作，但DMARL中局部策略的兼容性、隐私和通信限制是挑战。", "method": "引入高层符号知识，扩展理论工具以验证局部策略与团队任务的兼容性。", "result": "符号知识显著加速了DMARL的学习过程。", "conclusion": "符号知识能有效解决DMARL中的独特挑战，并提升学习效率。"}}
{"id": "2506.07977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07977", "abs": "https://arxiv.org/abs/2506.07977", "authors": ["Jingjing Chang", "Yixiao Fang", "Peng Xing", "Shuhan Wu", "Wei Cheng", "Rui Wang", "Xianfang Zeng", "Gang Yu", "Hai-Bao Chen"], "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation", "comment": null, "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.", "AI": {"tldr": "OneIG-Bench是一个全面的文本到图像（T2I）模型评估框架，专注于多维度细粒度评估，填补了现有评测系统的不足。", "motivation": "现有T2I模型的评测系统缺乏对推理能力、文本渲染和风格等维度的全面评估，无法满足最新模型的需求。", "method": "设计了OneIG-Bench框架，支持多维度（如提示-图像对齐、文本渲染精度、推理生成内容等）的灵活评估。", "result": "OneIG-Bench提供了细粒度的模型性能分析，帮助研究者识别图像生成流程中的优势和瓶颈。", "conclusion": "OneIG-Bench通过公开代码和数据集，促进了T2I研究社区的可重复评测和跨模型比较。"}}
{"id": "2506.07833", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07833", "abs": "https://arxiv.org/abs/2506.07833", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "title": "Improving large language models with concept-aware fine-tuning", "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm", "AI": {"tldr": "论文提出了一种名为CAFT的新方法，通过多令牌训练改进LLM的概念理解能力，显著提升了任务表现。", "motivation": "现有LLM的逐令牌预测范式限制了其对连贯高层次概念的理解，阻碍了智能系统的进一步发展。", "method": "引入Concept-Aware Fine-Tuning (CAFT)，一种多令牌训练方法，支持跨令牌序列学习。", "result": "实验表明，CAFT在文本摘要和蛋白质设计等任务中显著优于传统方法。", "conclusion": "CAFT首次将多令牌预测引入后训练阶段，为研究社区提供了更高效的工具，并暗示了更广泛的机器学习研究意义。"}}
{"id": "2506.07981", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07981", "abs": "https://arxiv.org/abs/2506.07981", "authors": ["Dmitrii Vorobev", "Artem Prosvetov", "Karim Elhadji Daou"], "title": "Real-time Localization of a Soccer Ball from a Single Camera", "comment": "13 pages, 4 figures", "summary": "We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.", "AI": {"tldr": "提出一种高效的单摄像头实时三维足球轨迹重建方法，通过多模式状态模型加速优化，保持厘米级精度，适用于复杂场景。", "motivation": "解决传统多摄像头系统成本高、复杂的问题，提供一种低成本、高效的3D足球轨迹跟踪方案。", "method": "引入多模式状态模型（W个离散模式），优化计算效率，适用于标准CPU，支持实时处理。", "result": "在6K分辨率俄罗斯超级联赛数据集上验证，性能媲美多摄像头系统，且无需昂贵设备。", "conclusion": "该方法为职业足球环境提供了一种实用、低成本且高精度的3D球轨迹跟踪方案。"}}
{"id": "2506.07843", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.07843", "abs": "https://arxiv.org/abs/2506.07843", "authors": ["Davide Carbone"], "title": "Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels", "comment": null, "summary": "Energy-Based Models (EBMs) provide a flexible framework for generative\nmodeling, but their training remains theoretically challenging due to the need\nto approximate normalization constants and efficiently sample from complex,\nmulti-modal distributions. Traditional methods, such as contrastive divergence\nand score matching, introduce biases that can hinder accurate learning. In this\nwork, we present a theoretical analysis of Jarzynski reweighting, a technique\nfrom non-equilibrium statistical mechanics, and its implications for training\nEBMs. We focus on the role of the choice of the kernel and we illustrate these\ntheoretical considerations in two key generative frameworks: (i) flow-based\ndiffusion models, where we reinterpret Jarzynski reweighting in the context of\nstochastic interpolants to mitigate discretization errors and improve sample\nquality, and (ii) Restricted Boltzmann Machines, where we analyze its role in\ncorrecting the biases of contrastive divergence. Our results provide insights\ninto the interplay between kernel choice and model performance, highlighting\nthe potential of Jarzynski reweighting as a principled tool for generative\nlearning.", "AI": {"tldr": "论文探讨了Jarzynski重加权在训练基于能量的模型（EBMs）中的应用，分析了其理论意义，并在两种生成框架中验证了其效果。", "motivation": "传统方法如对比散度和分数匹配在训练EBMs时存在偏差，Jarzynski重加权作为一种非平衡统计力学技术，可能提供更准确的训练方式。", "method": "通过理论分析Jarzynski重加权的机制，重点关注核选择的作用，并在流式扩散模型和受限玻尔兹曼机中验证其效果。", "result": "Jarzynski重加权能够减少离散化误差、提升样本质量，并纠正对比散度的偏差，核选择对模型性能有显著影响。", "conclusion": "Jarzynski重加权是一种有潜力的工具，可为生成学习提供更理论化的支持。"}}
{"id": "2506.07984", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07984", "abs": "https://arxiv.org/abs/2506.07984", "authors": ["Mingquan Lin", "Gregory Holste", "Song Wang", "Yiliang Zhou", "Yishu Wei", "Imon Banerjee", "Pengyi Chen", "Tianjie Dai", "Yuexi Du", "Nicha C. Dvornek", "Yuyan Ge", "Zuowei Guo", "Shouhei Hanaoka", "Dongkyun Kim", "Pablo Messina", "Yang Lu", "Denis Parra", "Donghyun Son", "Álvaro Soto", "Aisha Urooj", "René Vidal", "Yosuke Yamagishi", "Zefan Yang", "Ruichi Zhang", "Yang Zhou", "Leo Anthony Celi", "Ronald M. Summers", "Zhiyong Lu", "Hao Chen", "Adam Flanders", "George Shih", "Zhangyang Wang", "Yifan Peng"], "title": "CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray", "comment": "17 pages, 3 figures", "summary": "The CXR-LT series is a community-driven initiative designed to enhance lung\ndisease classification using chest X-rays (CXR). It tackles challenges in open\nlong-tailed lung disease classification and enhances the measurability of\nstate-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve\nthese goals by providing high-quality benchmark CXR data for model development\nand conducting comprehensive evaluations to identify ongoing issues impacting\nlung disease classification performance. Building on the success of CXR-LT\n2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45\ndisease labels, including 19 new rare disease findings. It also introduces a\nnew focus on zero-shot learning to address limitations identified in the\nprevious event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed\nclassification on a large, noisy test set, (ii) long-tailed classification on a\nmanually annotated \"gold standard\" subset, and (iii) zero-shot generalization\nto five previously unseen disease findings. This paper provides an overview of\nCXR-LT 2024, detailing the data curation process and consolidating\nstate-of-the-art solutions, including the use of multimodal models for rare\ndisease detection, advanced generative approaches to handle noisy labels, and\nzero-shot learning strategies for unseen diseases. Additionally, the expanded\ndataset enhances disease coverage to better represent real-world clinical\nsettings, offering a valuable resource for future research. By synthesizing the\ninsights and innovations of participating teams, we aim to advance the\ndevelopment of clinically realistic and generalizable diagnostic models for\nchest radiography.", "AI": {"tldr": "CXR-LT 2024是一个社区驱动的项目，旨在通过扩展数据集和改进任务设计，提升基于胸部X光的长尾疾病分类和零样本学习能力。", "motivation": "解决开放长尾疾病分类中的挑战，并提升现有技术的可测量性，以更好地模拟真实临床场景。", "method": "扩展数据集至377,110张胸部X光片和45种疾病标签，引入零样本学习任务，并采用多模态模型、生成方法和零样本学习策略。", "result": "提供了更全面的疾病覆盖和更高质量的基准数据，支持罕见疾病检测和噪声标签处理。", "conclusion": "CXR-LT 2024为开发临床现实和泛化性强的胸部X光诊断模型提供了重要资源，推动了相关研究的进展。"}}
{"id": "2506.07854", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07854", "abs": "https://arxiv.org/abs/2506.07854", "authors": ["Zheng Zhang", "Jie Bao", "Zhixin Zhou", "Nicolo Colombo", "Lixin Cheng", "Rui Luo"], "title": "Residual Reweighted Conformal Prediction for Graph Neural Networks", "comment": null, "summary": "Graph Neural Networks (GNNs) excel at modeling relational data but face\nsignificant challenges in high-stakes domains due to unquantified uncertainty.\nConformal prediction (CP) offers statistical coverage guarantees, but existing\nmethods often produce overly conservative prediction intervals that fail to\naccount for graph heteroscedasticity and structural biases. While residual\nreweighting CP variants address some of these limitations, they neglect graph\ntopology, cluster-specific uncertainties, and risk data leakage by reusing\ntraining sets. To address these issues, we propose Residual Reweighted GNN\n(RR-GNN), a framework designed to generate minimal prediction sets with\nprovable marginal coverage guarantees.\n  RR-GNN introduces three major innovations to enhance prediction performance.\nFirst, it employs Graph-Structured Mondrian CP to partition nodes or edges into\ncommunities based on topological features, ensuring cluster-conditional\ncoverage that reflects heterogeneity. Second, it uses Residual-Adaptive\nNonconformity Scores by training a secondary GNN on a held-out calibration set\nto estimate task-specific residuals, dynamically adjusting prediction intervals\naccording to node or edge uncertainty. Third, it adopts a Cross-Training\nProtocol, which alternates the optimization of the primary GNN and the residual\npredictor to prevent information leakage while maintaining graph dependencies.\nWe validate RR-GNN on 15 real-world graphs across diverse tasks, including node\nclassification, regression, and edge weight prediction. Compared to CP\nbaselines, RR-GNN achieves improved efficiency over state-of-the-art methods,\nwith no loss of coverage.", "AI": {"tldr": "RR-GNN通过结合图结构和残差自适应方法，改进了传统CP方法，提供了更精确的预测区间和统计覆盖保证。", "motivation": "现有CP方法在图数据中表现保守，未能考虑异方差性和结构偏差，RR-GNN旨在解决这些问题。", "method": "采用图结构Mondrian CP分区、残差自适应非一致性评分和交叉训练协议，优化预测区间。", "result": "在15个真实图数据上验证，RR-GNN在效率和覆盖性上优于现有方法。", "conclusion": "RR-GNN为高风险领域的图数据建模提供了更可靠的预测框架。"}}
{"id": "2506.07985", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07985", "abs": "https://arxiv.org/abs/2506.07985", "authors": ["Tuomas Oikarinen", "Ge Yan", "Akshay Kulkarni", "Tsui-Wei Weng"], "title": "Rethinking Crowd-Sourced Evaluation of Neuron Explanations", "comment": null, "summary": "Interpreting individual neurons or directions in activations space is an\nimportant component of mechanistic interpretability. As such, many algorithms\nhave been proposed to automatically produce neuron explanations, but it is\noften not clear how reliable these explanations are, or which methods produce\nthe best explanations. This can be measured via crowd-sourced evaluations, but\nthey can often be noisy and expensive, leading to unreliable results. In this\npaper, we carefully analyze the evaluation pipeline and develop a\ncost-effective and highly accurate crowdsourced evaluation strategy. In\ncontrast to previous human studies that only rate whether the explanation\nmatches the most highly activating inputs, we estimate whether the explanation\ndescribes neuron activations across all inputs. To estimate this effectively,\nwe introduce a novel application of importance sampling to determine which\ninputs are the most valuable to show to raters, leading to around 30x cost\nreduction compared to uniform sampling. We also analyze the label noise present\nin crowd-sourced evaluations and propose a Bayesian method to aggregate\nmultiple ratings leading to a further ~5x reduction in number of ratings\nrequired for the same accuracy. Finally, we use these methods to conduct a\nlarge-scale study comparing the quality of neuron explanations produced by the\nmost popular methods for two different vision models.", "AI": {"tldr": "本文提出了一种高效且准确的众包评估策略，用于评估神经元解释的可靠性，通过重要性采样和贝叶斯方法显著降低了成本。", "motivation": "现有神经元解释方法的可靠性评估通常依赖众包，但存在噪音大、成本高的问题，需要更高效的评估策略。", "method": "引入重要性采样选择最有价值的输入展示给评估者，并提出贝叶斯方法聚合多个评分，显著降低成本和噪音。", "result": "实现了约30倍的成本降低和约5倍的评分数量减少，同时保持高准确性。", "conclusion": "该方法为大规模比较神经元解释质量提供了高效工具，并验证了其在实际应用中的有效性。"}}
{"id": "2506.07861", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.07861", "abs": "https://arxiv.org/abs/2506.07861", "authors": ["Firas Laakom", "Haobo Chen", "Jürgen Schmidhuber", "Yuheng Bu"], "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective", "comment": "38 pages", "summary": "Despite substantial progress in promoting fairness in high-stake applications\nusing machine learning models, existing methods often modify the training\nprocess, such as through regularizers or other interventions, but lack formal\nguarantees that fairness achieved during training will generalize to unseen\ndata. Although overfitting with respect to prediction performance has been\nextensively studied, overfitting in terms of fairness loss has received far\nless attention. This paper proposes a theoretical framework for analyzing\nfairness generalization error through an information-theoretic lens. Our novel\nbounding technique is based on Efron-Stein inequality, which allows us to\nderive tight information-theoretic fairness generalization bounds with both\nMutual Information (MI) and Conditional Mutual Information (CMI). Our empirical\nresults validate the tightness and practical relevance of these bounds across\ndiverse fairness-aware learning algorithms. Our framework offers valuable\ninsights to guide the design of algorithms improving fairness generalization.", "AI": {"tldr": "本文提出了一种基于信息论的理论框架，用于分析公平性泛化误差，并通过Efron-Stein不等式推导出紧密的信息论公平性泛化边界。", "motivation": "现有方法缺乏对公平性在训练数据与未见数据之间泛化的形式化保证，本文旨在填补这一空白。", "method": "使用Efron-Stein不等式，结合互信息（MI）和条件互信息（CMI），推导公平性泛化边界。", "result": "实验验证了这些边界的紧密性和实际相关性，适用于多种公平性学习算法。", "conclusion": "该框架为设计提升公平性泛化的算法提供了有价值的指导。"}}
{"id": "2506.07986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07986", "abs": "https://arxiv.org/abs/2506.07986", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "comment": null, "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "AI": {"tldr": "论文提出了一种名为TACA的方法，通过动态调整跨模态注意力机制，解决了MM-DiT模型中文本与图像对齐的问题。", "motivation": "当前MM-DiT模型（如FLUX）在文本驱动视觉生成中，存在跨模态注意力不平衡和缺乏时间步感知的问题，导致文本与生成内容对齐不精确。", "method": "提出TACA方法，通过温度缩放和时间步依赖调整动态平衡多模态交互，并结合LoRA微调。", "result": "在T2I-CompBench基准测试中，TACA显著提升了文本-图像对齐效果，并在FLUX和SD3.5等模型中验证了其有效性。", "conclusion": "平衡跨模态注意力对提升文本到图像扩散模型的语义保真度至关重要，TACA是一种高效且计算开销低的方法。"}}
{"id": "2506.07864", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07864", "abs": "https://arxiv.org/abs/2506.07864", "authors": ["Mirko Paolo Barbato", "Giorgia Rigamonti", "Davide Marelli", "Paolo Napoletano"], "title": "Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes", "comment": null, "summary": "Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous\nmonitoring to prevent severe hypo- and hyperglycemic events. While continuous\nglucose monitoring has improved blood glucose management, deploying predictive\nmodels on wearable devices remains challenging due to computational and memory\nconstraints. To address this, we propose a novel Lightweight Sequential\nTransformer model designed for blood glucose prediction in T1D. By integrating\nthe strengths of Transformers' attention mechanisms and the sequential\nprocessing of recurrent neural networks, our architecture captures long-term\ndependencies while maintaining computational efficiency. The model is optimized\nfor deployment on resource-constrained edge devices and incorporates a balanced\nloss function to handle the inherent data imbalance in hypo- and hyperglycemic\nevents. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,\ndemonstrate that the proposed model outperforms state-of-the-art methods in\npredicting glucose levels and detecting adverse events. This work fills the gap\nbetween high-performance modeling and practical deployment, providing a\nreliable and efficient T1D management solution.", "AI": {"tldr": "提出了一种轻量级序列Transformer模型，用于T1D患者的血糖预测，解决了计算和内存限制问题，并在实验中表现优于现有方法。", "motivation": "T1D患者需要持续监测血糖，但现有预测模型在可穿戴设备上部署困难。", "method": "结合Transformer的注意力机制和RNN的序列处理能力，设计轻量级模型，优化边缘设备部署，并使用平衡损失函数处理数据不平衡。", "result": "在OhioT1DM和DiaTrend数据集上，模型在血糖预测和不良事件检测中优于现有方法。", "conclusion": "该模型填补了高性能建模与实际部署之间的空白，为T1D管理提供了高效可靠的解决方案。"}}
{"id": "2506.07992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07992", "abs": "https://arxiv.org/abs/2506.07992", "authors": ["Haoguang Lu", "Jiacheng Chen", "Zhenguo Yang", "Aurele Tohokantche Gnanha", "Fu Lee Wang", "Li Qing", "Xudong Mao"], "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing", "comment": null, "summary": "Recent advancements in text-guided image editing have achieved notable\nsuccess by leveraging natural language prompts for fine-grained semantic\ncontrol. However, certain editing semantics are challenging to specify\nprecisely using textual descriptions alone. A practical alternative involves\nlearning editing semantics from paired source-target examples. Existing\nexemplar-based editing methods still rely on text prompts describing the change\nwithin paired examples or learning implicit text-based editing instructions. In\nthis paper, we introduce PairEdit, a novel visual editing method designed to\neffectively learn complex editing semantics from a limited number of image\npairs or even a single image pair, without using any textual guidance. We\npropose a target noise prediction that explicitly models semantic variations\nwithin paired images through a guidance direction term. Moreover, we introduce\na content-preserving noise schedule to facilitate more effective semantic\nlearning. We also propose optimizing distinct LoRAs to disentangle the learning\nof semantic variations from content. Extensive qualitative and quantitative\nevaluations demonstrate that PairEdit successfully learns intricate semantics\nwhile significantly improving content consistency compared to baseline methods.\nCode will be available at https://github.com/xudonmao/PairEdit.", "AI": {"tldr": "PairEdit是一种无需文本指导的视觉编辑方法，通过少量图像对学习复杂编辑语义，显著提升内容一致性。", "motivation": "现有基于示例的编辑方法仍需依赖文本描述或隐式文本指令，而某些编辑语义难以用文本精确描述。", "method": "提出目标噪声预测和内容保持噪声调度，通过优化LoRAs解耦语义变化与内容学习。", "result": "PairEdit成功学习复杂语义，内容一致性显著优于基线方法。", "conclusion": "PairEdit为无需文本指导的图像编辑提供了高效解决方案。"}}
{"id": "2506.07871", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.07871", "abs": "https://arxiv.org/abs/2506.07871", "authors": ["Sigma Jahan", "Mohammad Masudur Rahman"], "title": "Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?", "comment": null, "summary": "As attention-based deep learning models scale in size and complexity,\ndiagnosing their faults becomes increasingly challenging. In this work, we\nconduct an empirical study to evaluate the potential of Hessian-based analysis\nfor diagnosing faults in attention-based models. Specifically, we use\nHessian-derived insights to identify fragile regions (via curvature analysis)\nand parameter interdependencies (via parameter interaction analysis) within\nattention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,\nDistilBERT), we show that Hessian-based metrics can localize instability and\npinpoint fault sources more effectively than gradients alone. Our empirical\nfindings suggest that these metrics could significantly improve fault diagnosis\nin complex neural architectures, potentially improving software debugging\npractices.", "AI": {"tldr": "研究通过Hessian矩阵分析诊断注意力模型的故障，比梯度更有效。", "motivation": "随着注意力模型的规模和复杂性增加，诊断其故障变得更具挑战性。", "method": "使用Hessian矩阵分析（曲率分析和参数交互分析）诊断注意力机制中的脆弱区域和参数依赖关系。", "result": "实验表明，Hessian指标能更有效地定位不稳定性和故障源。", "conclusion": "Hessian分析可显著改进复杂神经架构的故障诊断，提升调试效率。"}}
{"id": "2506.07883", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07883", "abs": "https://arxiv.org/abs/2506.07883", "authors": ["Rajat Rasal", "Avinash Kori", "Fabio De Sousa Ribeiro", "Tian Xia", "Ben Glocker"], "title": "Diffusion Counterfactual Generation with Semantic Abduction", "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada", "summary": "Counterfactual image generation presents significant challenges, including\npreserving identity, maintaining perceptual quality, and ensuring faithfulness\nto an underlying causal model. While existing auto-encoding frameworks admit\nsemantic latent spaces which can be manipulated for causal control, they\nstruggle with scalability and fidelity. Advancements in diffusion models\npresent opportunities for improving counterfactual image editing, having\ndemonstrated state-of-the-art visual quality, human-aligned perception and\nrepresentation learning capabilities. Here, we present a suite of\ndiffusion-based causal mechanisms, introducing the notions of spatial, semantic\nand dynamic abduction. We propose a general framework that integrates semantic\nrepresentations into diffusion models through the lens of Pearlian causality to\nedit images via a counterfactual reasoning process. To our knowledge, this is\nthe first work to consider high-level semantic identity preservation for\ndiffusion counterfactuals and to demonstrate how semantic control enables\nprincipled trade-offs between faithful causal control and identity\npreservation.", "AI": {"tldr": "论文提出了一种基于扩散模型的因果机制框架，用于反事实图像生成，解决了身份保留、感知质量和因果忠实性等问题。", "motivation": "现有自编码框架在扩展性和保真度方面存在不足，而扩散模型在视觉质量和表示学习方面表现出色，因此探索其在反事实图像编辑中的应用。", "method": "提出了一套扩散模型的因果机制，包括空间、语义和动态反演，并将语义表示通过Pearl因果理论整合到扩散模型中。", "result": "该方法首次实现了扩散模型中的高级语义身份保留，并在因果控制和身份保留之间取得了平衡。", "conclusion": "该框架为反事实图像生成提供了新的解决方案，展示了语义控制在扩散模型中的潜力。"}}
{"id": "2506.07999", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07999", "abs": "https://arxiv.org/abs/2506.07999", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "comment": null, "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "AI": {"tldr": "MADFormer是一种混合自回归（AR）和扩散模型的Transformer，通过空间块划分和垂直混合层优化高分辨率图像生成，显著提升性能和质量效率平衡。", "motivation": "现有混合模型缺乏系统指导如何分配AR和扩散模型的能力，需要探索更有效的结合方式。", "method": "MADFormer将图像生成分为空间块，AR层用于全局条件，扩散层用于局部细化，通过实验分析AR与扩散的权衡。", "result": "实验表明，块划分显著提升高分辨率图像性能，垂直混合AR和扩散层在有限计算下FID提升75%。", "conclusion": "MADFormer为未来混合生成模型提供了实用的设计原则。"}}
{"id": "2506.07884", "categories": ["cs.LG", "math.FA", "46B15", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.07884", "abs": "https://arxiv.org/abs/2506.07884", "authors": ["Anand Ganesh", "Babhrubahan Bose", "Anand Rajagopalan"], "title": "Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions", "comment": "9 pages", "summary": "We construct four Schauder bases for the space $C[0,1]$, one using ReLU\nfunctions, another using Softplus functions, and two more using sigmoidal\nversions of the ReLU and Softplus functions. This establishes the existence of\na basis using these functions for the first time, and improves on the universal\napproximation property associated with them.", "AI": {"tldr": "论文构建了四种Schauder基函数用于空间$C[0,1]$，分别基于ReLU、Softplus及其sigmoidal变体，首次证明了这些基函数的存在性，并改进了其通用逼近性质。", "motivation": "探索ReLU、Softplus及其sigmoidal变体在函数空间$C[0,1]$中作为Schauder基的可行性，填补相关理论空白。", "method": "通过构造四种不同的Schauder基函数（ReLU、Softplus及其sigmoidal版本），验证其在$C[0,1]$空间中的基函数性质。", "result": "首次证明了这些基函数的存在性，并提升了其通用逼近能力。", "conclusion": "研究为神经网络中常用激活函数的理论分析提供了新视角，并扩展了其在函数逼近中的应用潜力。"}}
{"id": "2506.08002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08002", "abs": "https://arxiv.org/abs/2506.08002", "authors": ["Aadarsh Sahoo", "Vansh Tibrewal", "Georgia Gkioxari"], "title": "Aligning Text, Images, and 3D Structure Token-by-Token", "comment": "Project webpage: https://glab-caltech.github.io/kyvo/", "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/", "AI": {"tldr": "论文提出了一种统一的LLM框架，用于对齐语言、图像和3D场景，并提供了实现最佳训练和性能的关键设计选择。", "motivation": "目标是帮助设计师构建和编辑3D环境，以及机器人导航和交互，通过研究自回归模型在结构化3D场景中的潜力。", "method": "采用统一的LLM框架，结合语言、图像和3D场景，并通过量化形状编码增强3D模态。", "result": "在四个核心3D任务（渲染、识别、指令跟随和问答）和四个3D数据集上评估了性能，并在真实世界3D物体识别任务中展示了有效性。", "conclusion": "该框架在3D场景理解和任务处理中表现出色，为未来研究提供了实用指南。"}}
{"id": "2506.07902", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07902", "abs": "https://arxiv.org/abs/2506.07902", "authors": ["Sifan Wang", "Zehao Dou", "Tong-Rui Liu", "Lu Lu"], "title": "FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling", "comment": "31 pages, 12 figures", "summary": "Recent advances in generative modeling -- particularly diffusion models and\nflow matching -- have achieved remarkable success in synthesizing discrete data\nsuch as images and videos. However, adapting these models to physical\napplications remains challenging, as the quantities of interest are continuous\nfunctions governed by complex physical laws. Here, we introduce\n$\\textbf{FunDiff}$, a novel framework for generative modeling in function\nspaces. FunDiff combines a latent diffusion process with a function autoencoder\narchitecture to handle input functions with varying discretizations, generate\ncontinuous functions evaluable at arbitrary locations, and seamlessly\nincorporate physical priors. These priors are enforced through architectural\nconstraints or physics-informed loss functions, ensuring that generated samples\nsatisfy fundamental physical laws. We theoretically establish minimax\noptimality guarantees for density estimation in function spaces, showing that\ndiffusion-based estimators achieve optimal convergence rates under suitable\nregularity conditions. We demonstrate the practical effectiveness of FunDiff\nacross diverse applications in fluid dynamics and solid mechanics. Empirical\nresults show that our method generates physically consistent samples with high\nfidelity to the target distribution and exhibits robustness to noisy and\nlow-resolution data. Code and datasets are publicly available at\nhttps://github.com/sifanexisted/fundiff.", "AI": {"tldr": "FunDiff是一个新颖的生成模型框架，用于函数空间中的生成建模，结合了潜在扩散过程和函数自动编码器架构，能够处理不同离散化的输入函数并生成连续函数。", "motivation": "适应生成模型（如扩散模型和流匹配）到物理应用中仍具挑战性，因为感兴趣的物理量是受复杂物理定律支配的连续函数。", "method": "FunDiff结合潜在扩散过程和函数自动编码器架构，通过架构约束或物理信息损失函数强制执行物理先验。", "result": "理论证明扩散基估计器在函数空间中实现最优密度估计收敛率，实验表明FunDiff在流体动力学和固体力学中生成物理一致的样本。", "conclusion": "FunDiff在生成连续函数和满足物理定律方面表现出色，适用于物理应用中的生成建模。"}}
{"id": "2506.08003", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08003", "abs": "https://arxiv.org/abs/2506.08003", "authors": ["Shuchen Weng", "Haojie Zheng", "Zheng Chang", "Si Li", "Boxin Shi", "Xinlong Wang"], "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control", "comment": null, "summary": "Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.", "AI": {"tldr": "MTV是一个用于音频同步视频生成的框架，通过分离音频轨道实现精细控制，并引入DEMIX数据集支持。", "motivation": "音频与视频的紧密同步在可控视频生成中至关重要，但现有方法在高质量和精确同步方面表现不足。", "method": "MTV框架将音频分为语音、效果和音乐轨道，分别控制唇部动作、事件时间和视觉氛围，并利用DEMIX数据集进行多阶段训练。", "result": "MTV在视频质量、文本-视频一致性和音频-视频对齐等六个标准指标上达到最优性能。", "conclusion": "MTV通过分离音频轨道和多阶段训练，实现了高质量且精确同步的视频生成。"}}
{"id": "2506.07903", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07903", "abs": "https://arxiv.org/abs/2506.07903", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "comment": "Accepted to ICML 2025. Code available at\n  https://github.com/KevinRojas1499/Diffuse-Everything", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "AI": {"tldr": "本文提出了一种新的多模态扩散模型框架，支持跨模态数据的原生生成，无需依赖外部预处理协议。", "motivation": "现有方法依赖外部预处理协议（如分词器和变分自编码器）统一多模态数据表示，但这对数据有限的应用存在问题。本文旨在解决这一限制。", "method": "提出了一种新颖的框架，支持在任意状态空间上构建多模态扩散模型，并引入解耦的噪声调度策略，实现无条件生成和模态条件生成。", "result": "在文本-图像生成和混合类型表格数据合成任务中，该方法表现出竞争力。", "conclusion": "该框架为多模态数据生成提供了一种灵活且高效的解决方案。"}}
{"id": "2506.08004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08004", "abs": "https://arxiv.org/abs/2506.08004", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "title": "Dynamic View Synthesis as an Inverse Problem", "comment": "Project Page: https://inverse-dvs.github.io/", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "AI": {"tldr": "通过重新设计预训练视频扩散模型的噪声初始化阶段，提出了一种无需权重更新或辅助模块的高保真动态视图合成方法。", "motivation": "解决单目视频动态视图合成的逆问题，避免训练依赖。", "method": "引入K阶递归噪声表示解决零终端信噪比问题，并采用随机潜在调制完成遮挡区域合成。", "result": "实验表明，通过噪声初始化阶段的潜在操作可有效实现动态视图合成。", "conclusion": "该方法在无需训练的情况下实现了高质量的动态视图合成。"}}
{"id": "2506.07918", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07918", "abs": "https://arxiv.org/abs/2506.07918", "authors": ["Vahid Balazadeh", "Hamidreza Kamkari", "Valentin Thomas", "Benson Li", "Junwei Ma", "Jesse C. Cresswell", "Rahul G. Krishnan"], "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning", "comment": null, "summary": "Causal effect estimation from observational data is fundamental across\nvarious applications. However, selecting an appropriate estimator from dozens\nof specialized methods demands substantial manual effort and domain expertise.\nWe present CausalPFN, a single transformer that amortizes this workflow:\ntrained once on a large library of simulated data-generating processes that\nsatisfy ignorability, it infers causal effects for new observational datasets\nout-of-the-box. CausalPFN combines ideas from Bayesian causal inference with\nthe large-scale training protocol of prior-fitted networks (PFNs), learning to\nmap raw observations directly to causal effects without any task-specific\nadjustment. Our approach achieves superior average performance on heterogeneous\nand average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).\nMoreover, it shows competitive performance for real-world policy making on\nuplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to\nsupport reliable decision-making based on Bayesian principles. This\nready-to-use model does not require any further training or tuning and takes a\nstep toward automated causal inference (https://github.com/vdblm/CausalPFN).", "AI": {"tldr": "CausalPFN是一个基于Transformer的模型，通过大规模模拟数据训练，能够直接推断新观测数据的因果效应，无需额外调整。", "motivation": "从观测数据中估计因果效应是许多应用的基础，但现有方法需要大量手动选择和专业知识。", "method": "结合贝叶斯因果推断和先验拟合网络（PFNs）的大规模训练协议，直接映射观测数据到因果效应。", "result": "在IHDP、Lalonde和ACIC等基准测试中表现优异，并在真实世界政策制定任务中具有竞争力。", "conclusion": "CausalPFN提供了一种无需额外训练或调参的自动化因果推断解决方案，支持可靠的决策。"}}
{"id": "2506.08005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08005", "abs": "https://arxiv.org/abs/2506.08005", "authors": ["Lei Lai", "Zekai Yin", "Eshed Ohn-Bar"], "title": "ZeroVO: Visual Odometry with Minimal Assumptions", "comment": null, "summary": "We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves\nzero-shot generalization across diverse cameras and environments, overcoming\nlimitations in existing methods that depend on predefined or static camera\ncalibration setups. Our approach incorporates three main innovations. First, we\ndesign a calibration-free, geometry-aware network structure capable of handling\nnoise in estimated depth and camera parameters. Second, we introduce a\nlanguage-based prior that infuses semantic information to enhance robust\nfeature extraction and generalization to previously unseen domains. Third, we\ndevelop a flexible, semi-supervised training paradigm that iteratively adapts\nto new scenes using unlabeled data, further boosting the models' ability to\ngeneralize across diverse real-world scenarios. We analyze complex autonomous\ndriving contexts, demonstrating over 30% improvement against prior methods on\nthree standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly\nintroduced, high-fidelity synthetic dataset derived from Grand Theft Auto\n(GTA). By not requiring fine-tuning or camera calibration, our work broadens\nthe applicability of VO, providing a versatile solution for real-world\ndeployment at scale.", "AI": {"tldr": "ZeroVO是一种新型视觉里程计算法，无需预定义或静态相机校准即可实现跨多样相机和环境的零样本泛化。", "motivation": "现有方法依赖预定义或静态相机校准，限制了泛化能力，ZeroVO旨在解决这一问题。", "method": "1. 设计无校准、几何感知的网络结构；2. 引入基于语言的先验增强特征提取；3. 开发半监督训练范式适应新场景。", "result": "在KITTI、nuScenes和Argoverse 2等标准基准上表现优于现有方法30%以上。", "conclusion": "ZeroVO无需微调或相机校准，为实际大规模部署提供了通用解决方案。"}}
{"id": "2506.07919", "categories": ["cs.LG", "cs.AI", "cs.CL", "nlin.CD", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.07919", "abs": "https://arxiv.org/abs/2506.07919", "authors": ["Manuel Brenner", "Georgia Koppe"], "title": "Uncovering the Functional Roles of Nonlinearity in Memory", "comment": "Preprint under review", "summary": "Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems.", "AI": {"tldr": "研究发现，在序列建模任务中，最小非线性通常足够且最优，简化模型并提高鲁棒性和可解释性。", "motivation": "探讨非线性在循环网络中的功能作用，明确其计算必要性和机制。", "method": "使用几乎线性循环神经网络（AL-RNNs）作为建模工具和内存机制探针。", "result": "最小非线性不仅足够，而且通常最优，模型更简单、鲁棒和可解释。", "conclusion": "为选择性引入非线性提供了原则性框架，连接动态系统理论与循环网络的功能需求。"}}
{"id": "2506.08006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08006", "abs": "https://arxiv.org/abs/2506.08006", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "comment": "Project Page: https://metadriverse.github.io/dreamland/", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "AI": {"tldr": "Dreamland是一个结合物理模拟器和生成模型的混合世界生成框架，通过分层抽象增强可控性，提升图像质量和可控性。", "motivation": "现有大规模视频生成模型缺乏元素级可控性，限制了其在场景编辑和AI代理训练中的应用。", "method": "设计分层世界抽象作为中间表示，结合物理模拟器和生成模型，构建D3Sim数据集支持训练和评估。", "result": "Dreamland在图像质量上提升50.8%，可控性增强17.9%，并支持现有和未来生成模型的即插即用。", "conclusion": "Dreamland在可控性和生成质量上显著优于基线方法，具有增强AI代理训练的潜力。"}}
{"id": "2506.07920", "categories": ["cs.LG", "eess.AS", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.07920", "abs": "https://arxiv.org/abs/2506.07920", "authors": ["Hossein Babaei", "Mel White", "Richard G. Baraniuk"], "title": "W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling", "comment": "10 pages, 2 figures, 3 tables", "summary": "State Space Models (SSMs) have emerged as powerful components for sequence\nmodeling, enabling efficient handling of long-range dependencies via linear\nrecurrence and convolutional computation. However, their effectiveness depends\nheavily on the choice and initialization of the state matrix. In this work, we\nbuild on the SaFARi framework and existing WaLRUS SSMs to introduce a new\nvariant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant\nwavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel\ncomputation without requiring low-rank approximations, making it both\ntheoretically grounded and computationally efficient. We show that WaLRUS\nretains information over long horizons significantly better than HiPPO-based\nSSMs, both in isolation and when integrated into deep architectures such as S4.\nOur experiments demonstrate consistent improvements across delay reconstruction\ntasks, classification benchmarks, and long-range sequence modeling, confirming\nthat high-quality, structured initialization enabled by wavelet-based state\ndynamic offers substantial advantages over existing alternatives. WaLRUS\nprovides a scalable and versatile foundation for the next generation of deep\nSSM-based models.", "AI": {"tldr": "论文提出了一种新的状态空间模型（SSM）变体W4S4，基于冗余小波框架构建，具有稳定对角化和快速核计算能力，优于现有HiPPO-based SSMs。", "motivation": "现有SSMs的性能高度依赖于状态矩阵的选择和初始化，因此需要一种更高效且理论支持的方法。", "method": "基于SaFARi框架和WaLRUS SSMs，引入W4S4，利用冗余小波框架构建，支持稳定对角化和快速核计算。", "result": "W4S4在延迟重构、分类基准和长序列建模任务中表现优于HiPPO-based SSMs。", "conclusion": "W4S4为下一代基于SSM的深度模型提供了可扩展且通用的基础。"}}
{"id": "2506.08008", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08008", "abs": "https://arxiv.org/abs/2506.08008", "authors": ["Stephanie Fu", "Tyler Bonnen", "Devin Guillory", "Trevor Darrell"], "title": "Hidden in plain sight: VLMs overlook their visual representations", "comment": "Project page: https://hidden-plain-sight.github.io/", "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.", "AI": {"tldr": "本文比较了视觉语言模型（VLMs）与其视觉编码器的性能，发现VLMs在视觉任务中表现显著较差，主要原因是未能有效整合视觉信息。", "motivation": "探索VLMs如何整合视觉与语言信息，并评估其在视觉任务中的表现。", "method": "通过一系列视觉基准测试（如深度估计、对应关系）比较VLMs与视觉编码器的性能，并分析其失败原因。", "result": "VLMs在视觉任务中表现接近随机水平，主要瓶颈在于未能有效利用视觉信息，且受语言先验影响。", "conclusion": "研究揭示了开源VLMs的失败模式，为未来改进视觉理解提供了评估方法。"}}
{"id": "2506.08009", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08009", "abs": "https://arxiv.org/abs/2506.08009", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "comment": "Project website: http://self-forcing.github.io/", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "AI": {"tldr": "Self Forcing是一种新的自回归视频扩散模型训练范式，解决了曝光偏差问题，通过自生成输出进行训练，并采用KV缓存和梯度截断策略提升效率。", "motivation": "解决自回归视频扩散模型在推理阶段因依赖自身不完美输出而产生的曝光偏差问题。", "method": "在训练时通过自回归展开和KV缓存，基于自生成输出生成帧，并使用视频级损失监督，同时采用梯度截断和滚动KV缓存机制优化效率。", "result": "实现了单GPU上的实时视频生成，延迟低于一秒，生成质量与更慢的非因果扩散模型相当或更优。", "conclusion": "Self Forcing通过自生成输出和高效训练策略，显著提升了视频生成的质量和效率。"}}
{"id": "2506.07933", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.07933", "abs": "https://arxiv.org/abs/2506.07933", "authors": ["Lev V. Utkin", "Semen P. Khomets", "Vlada A. Efremenko", "Andrei V. Konstantinov", "Natalya M. Verbova"], "title": "Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions", "comment": null, "summary": "Survival analysis predicts the time until an event of interest, such as\nfailure or death, but faces challenges due to censored data, where some events\nremain unobserved. Ensemble-based models, like random survival forests and\ngradient boosting, are widely used but can produce unstable predictions due to\nvariations in bootstrap samples. To address this, we propose SurvBESA (Survival\nBeran Estimators Self-Attended), a novel ensemble model that combines Beran\nestimators with a self-attention mechanism. Unlike traditional methods,\nSurvBESA applies self-attention to predicted survival functions, smoothing out\nnoise by adjusting each survival function based on its similarity to\nneighboring survival functions. We also explore a special case using Huber's\ncontamination model to define attention weights, simplifying training to a\nquadratic or linear optimization problem. Numerical experiments show that\nSurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is\npublicly available.", "AI": {"tldr": "SurvBESA是一种新型集成模型，结合Beran估计器和自注意力机制，通过调整生存函数的相似性来平滑噪声，优于现有方法。", "motivation": "生存分析因数据截尾问题面临挑战，传统集成模型因样本变化导致预测不稳定。", "method": "提出SurvBESA，结合Beran估计器和自注意力机制，调整生存函数相似性以平滑噪声。", "result": "数值实验表明SurvBESA优于现有方法。", "conclusion": "SurvBESA通过自注意力机制有效提升预测稳定性，公开实现可用。"}}
{"id": "2506.08010", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08010", "abs": "https://arxiv.org/abs/2506.08010", "authors": ["Nick Jiang", "Amil Dravid", "Alexei Efros", "Yossi Gandelsman"], "title": "Vision Transformers Don't Need Trained Registers", "comment": "Project page and code: https://avdravid.github.io/test-time-registers", "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.", "AI": {"tldr": "论文研究了Vision Transformers中高范数令牌导致噪声注意力图的机制，提出了一种无需重新训练的解决方案，通过转移高范数激活改善模型性能。", "motivation": "解决Vision Transformers中高范数令牌引起的噪声注意力图问题，避免重新训练模型的成本。", "method": "通过将高范数激活转移到额外的未训练令牌中，模拟注册令牌的效果。", "result": "方法能生成更清晰的注意力和特征图，提升下游视觉任务性能，效果接近显式训练注册令牌的模型。", "conclusion": "测试时注册令牌为预训练模型提供了一种无需训练的解决方案，提升了模型的可解释性。"}}
{"id": "2506.07948", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.07948", "abs": "https://arxiv.org/abs/2506.07948", "authors": ["Kasimir Schulz", "Kenneth Yeung", "Kieran Evans"], "title": "TokenBreak: Bypassing Text Classification Models Through Token Manipulation", "comment": null, "summary": "Natural Language Processing (NLP) models are used for text-related tasks such\nas classification and generation. To complete these tasks, input data is first\ntokenized from human-readable text into a format the model can understand,\nenabling it to make inferences and understand context. Text classification\nmodels can be implemented to guard against threats such as prompt injection\nattacks against Large Language Models (LLMs), toxic input and cybersecurity\nrisks such as spam emails. In this paper, we introduce TokenBreak: a novel\nattack that can bypass these protection models by taking advantage of the\ntokenization strategy they use. This attack technique manipulates input text in\nsuch a way that certain models give an incorrect classification. Importantly,\nthe end target (LLM or email recipient) can still understand and respond to the\nmanipulated text and therefore be vulnerable to the very attack the protection\nmodel was put in place to prevent. The tokenizer is tied to model architecture,\nmeaning it is possible to predict whether or not a model is vulnerable to\nattack based on family. We also present a defensive strategy as an added layer\nof protection that can be implemented without having to retrain the defensive\nmodel.", "AI": {"tldr": "本文介绍了TokenBreak，一种利用分词策略绕过文本分类保护模型的新型攻击方法，并提出了防御策略。", "motivation": "研究动机在于揭示现有文本分类保护模型（如防提示注入攻击、有毒输入等）可能因分词策略而被绕过，导致目标模型仍易受攻击。", "method": "提出TokenBreak攻击方法，通过操纵输入文本使保护模型错误分类，同时目标模型仍能理解并响应。", "result": "研究发现某些模型家族因分词策略易受攻击，并提出无需重新训练保护模型的防御策略。", "conclusion": "结论指出TokenBreak攻击的有效性，并强调防御策略的重要性以增强保护模型的鲁棒性。"}}
{"id": "2506.08011", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08011", "abs": "https://arxiv.org/abs/2506.08011", "authors": ["Yunfei Xie", "Yinsong Ma", "Shiyi Lan", "Alan Yuille", "Junfei Xiao", "Chen Wei"], "title": "Play to Generalize: Learning to Reason Through Game Play", "comment": "Project Page: https://yunfeixie233.github.io/ViGaL/", "summary": "Developing generalizable reasoning capabilities in multimodal large language\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\ndevelop out-of-domain generalization of multimodal reasoning through playing\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\nsignificantly enhances its downstream performance on multimodal math benchmarks\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\nworked solutions, equations, or diagrams during RL, suggesting the capture of\ntransferable reasoning skills. Remarkably, our model outperforms specialist\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\nwhile preserving the base model's performance on general visual benchmarks, a\nchallenge where specialist models often fall short. Our findings suggest a new\npost-training paradigm: synthetic, rule-based games can serve as controllable\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\nabilities in MLLMs.", "AI": {"tldr": "ViGaL是一种通过玩街机游戏提升多模态大语言模型（MLLM）泛化推理能力的新方法。", "motivation": "受认知科学启发，游戏能促进可迁移的认知技能，因此探索游戏是否能提升MLLM的多模态推理能力。", "method": "通过强化学习（RL）在简单街机游戏（如贪吃蛇）上对7B参数的MLLM进行后训练。", "result": "模型在多模态数学基准（如MathVista）和多学科问题（如MMMU）上表现显著提升，甚至超越专用模型。", "conclusion": "规则简单的游戏可作为可控、可扩展的预训练任务，解锁MLLM的通用多模态推理能力。"}}
{"id": "2506.07949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07949", "abs": "https://arxiv.org/abs/2506.07949", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "title": "Cost-Optimal Active AI Model Evaluation", "comment": null, "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "AI": {"tldr": "论文提出了一种成本感知方法，用于在生成式AI系统的评估中平衡低成本但可能不准确的弱评分者（如模型自动评分）和高成本但更准确的强评分者（如人工评分），以在有限预算下最大化统计效率。", "motivation": "生成式AI系统的评估通常依赖低成本但可能偏颇的合成数据，而高成本的人工评分虽准确但资源消耗大。如何在预算内平衡两者，提高评估效率是研究动机。", "method": "基于主动和预测驱动的统计推断，提出一系列成本最优策略，分配预算给弱评分者和强评分者，以最大化统计效率。", "result": "在合成和真实数据上验证，尤其在任务难度差异大的情况下，新策略能以更低预算达到与传统方法相同的估计精度。", "conclusion": "新方法在预算有限的情况下显著提高了生成式AI系统评估的效率，尤其适用于任务难度差异大的场景。"}}
{"id": "2506.08013", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08013", "abs": "https://arxiv.org/abs/2506.08013", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "comment": "Code is available at https://github.com/astra-vision/StableMTL", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "AI": {"tldr": "StableMTL利用扩散模型和任务注意力机制，在零样本设置下进行多任务学习，避免了全任务标注的需求，并在多个基准测试中表现优异。", "motivation": "解决多任务学习中需要大量全任务标注的问题，探索零样本设置下的多任务学习。", "method": "采用扩散模型进行潜在回归，结合任务编码和任务注意力机制，使用统一的潜在损失替代多任务损失。", "result": "在8个基准测试的7个任务中优于基线方法。", "conclusion": "StableMTL通过创新的任务注意力机制和潜在损失设计，实现了高效的多任务学习。"}}
{"id": "2506.07958", "categories": ["cs.LG", "math-ph", "math.AP", "math.MP", "math.SP"], "pdf": "https://arxiv.org/pdf/2506.07958", "abs": "https://arxiv.org/abs/2506.07958", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran"], "title": "Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs", "comment": null, "summary": "Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their\nChebyshev-based variants (cPIKANs), have recently emerged as promising models\nfor solving partial differential equations (PDEs). However, their training\ndynamics and convergence behavior remain largely unexplored both theoretically\nand numerically. In this work, we aim to advance the theoretical understanding\nof cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our\nobjective is to discern the evolution of kernel structure throughout\ngradient-based training and its subsequent impact on learning efficiency. We\nfirst derive the NTK of standard cKANs in a supervised setting, and then extend\nthe analysis to the physics-informed context. We analyze the spectral\nproperties of NTK matrices, specifically their eigenvalue distributions and\nspectral bias, for four representative PDEs: the steady-state Helmholtz\nequation, transient diffusion and Allen-Cahn equations, and forced vibrations\ngoverned by the Euler-Bernoulli beam equation. We also conduct an investigation\ninto the impact of various optimization strategies, e.g., first-order,\nsecond-order, and hybrid approaches, on the evolution of the NTK and the\nresulting learning dynamics. Results indicate a tractable behavior for NTK in\nthe context of cPIKANs, which exposes learning dynamics that standard\nphysics-informed neural networks (PINNs) cannot capture. Spectral trends also\nreveal when domain decomposition improves training, directly linking kernel\nbehavior to convergence rates under different setups. To the best of our\nknowledge, this is the first systematic NTK study of cPIKANs, providing\ntheoretical insight that clarifies and predicts their empirical performance.", "AI": {"tldr": "本文通过神经切线核（NTK）理论分析了基于Chebyshev的物理信息Kolmogorov-Arnold网络（cPIKANs）的训练动态和收敛行为，揭示了其与标准物理信息神经网络（PINNs）不同的学习效率。", "motivation": "cPIKANs在解决偏微分方程（PDEs）方面表现出潜力，但其训练动态和收敛行为缺乏理论和数值研究。本文旨在通过NTK理论填补这一空白。", "method": "使用NTK理论分析cPIKANs，推导其核结构在梯度训练中的演变，并研究NTK矩阵的谱特性（如特征值分布和谱偏差）对四种典型PDE的影响。", "result": "研究发现cPIKANs的NTK行为可预测，揭示了标准PINNs无法捕捉的学习动态，并明确了域分解对训练的改进条件。", "conclusion": "这是首次对cPIKANs进行系统的NTK研究，为理解其经验表现提供了理论支持。"}}
{"id": "2506.08015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08015", "abs": "https://arxiv.org/abs/2506.08015", "authors": ["Zhen Xu", "Zhengqin Li", "Zhao Dong", "Xiaowei Zhou", "Richard Newcombe", "Zhaoyang Lv"], "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos", "comment": "Project page: https://4dgt.github.io", "summary": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene\nreconstruction, trained entirely on real-world monocular posed videos. Using 4D\nGaussian as an inductive bias, 4DGT unifies static and dynamic components,\nenabling the modeling of complex, time-varying environments with varying object\nlifespans. We proposed a novel density control strategy in training, which\nenables our 4DGT to handle longer space-time input and remain efficient\nrendering at runtime. Our model processes 64 consecutive posed frames in a\nrolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike\noptimization-based methods, 4DGT performs purely feed-forward inference,\nreducing reconstruction time from hours to seconds and scaling effectively to\nlong video sequences. Trained only on large-scale monocular posed video\ndatasets, 4DGT can outperform prior Gaussian-based networks significantly in\nreal-world videos and achieve on-par accuracy with optimization-based methods\non cross-domain videos. Project page: https://4dgt.github.io", "AI": {"tldr": "4DGT是一种基于4D高斯的Transformer模型，用于动态场景重建，仅需单目视频输入，通过统一静态和动态组件处理复杂时变环境。", "motivation": "解决动态场景重建中复杂时变环境和对象生命周期变化的问题，同时减少传统优化方法的高耗时。", "method": "采用4D高斯作为归纳偏置，提出密度控制策略处理长时空输入，通过滚动窗口处理64帧连续视频，实现高效前馈推理。", "result": "在真实视频中显著优于其他高斯网络，跨域视频中与优化方法精度相当，重建时间从小时级降至秒级。", "conclusion": "4DGT通过高效前馈推理和密度控制策略，实现了动态场景的高效重建，适用于长视频序列。"}}
{"id": "2506.07969", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2506.07969", "abs": "https://arxiv.org/abs/2506.07969", "authors": ["Jacob Helwig", "Sai Sreeharsha Adavi", "Xuan Zhang", "Yuchao Lin", "Felix S. Chim", "Luke Takeshi Vizzini", "Haiyang Yu", "Muhammad Hasnain", "Saykat Kumar Biswas", "John J. Holloway", "Narendra Singh", "N. K. Anand", "Swagnik Guhathakurta", "Shuiwang Ji"], "title": "A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling", "comment": null, "summary": "We consider the problem of modeling high-speed flows using machine learning\nmethods. While most prior studies focus on low-speed fluid flows in which\nuniform time-stepping is practical, flows approaching and exceeding the speed\nof sound exhibit sudden changes such as shock waves. In such cases, it is\nessential to use adaptive time-stepping methods to allow a temporal resolution\nsufficient to resolve these phenomena while simultaneously balancing\ncomputational costs. Here, we propose a two-phase machine learning method,\nknown as ShockCast, to model high-speed flows with adaptive time-stepping. In\nthe first phase, we propose to employ a machine learning model to predict the\ntimestep size. In the second phase, the predicted timestep is used as an input\nalong with the current fluid fields to advance the system state by the\npredicted timestep. We explore several physically-motivated components for\ntimestep prediction and introduce timestep conditioning strategies inspired by\nneural ODE and Mixture of Experts. As ShockCast is the first framework for\nlearning high-speed flows, we evaluate our methods by generating two supersonic\nflow datasets, available at https://huggingface.co/datasets/divelab. Our code\nis publicly available as part of the AIRS library\n(https://github.com/divelab/AIRS).", "AI": {"tldr": "论文提出了一种名为ShockCast的两阶段机器学习方法，用于建模高速流动，并采用自适应时间步长策略。", "motivation": "现有研究多关注低速流体流动，而高速流动（如接近或超过音速）存在激波等突变现象，需要自适应时间步长方法。", "method": "ShockCast分为两阶段：第一阶段用机器学习预测时间步长，第二阶段将预测步长与当前流体场结合推进系统状态。", "result": "通过生成两个超音速流动数据集验证了方法的有效性，代码和数据已公开。", "conclusion": "ShockCast是首个学习高速流动的框架，为相关研究提供了新工具。"}}
{"id": "2506.07972", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.07972", "abs": "https://arxiv.org/abs/2506.07972", "authors": ["Hongzheng Chen", "Yingheng Wang", "Yaohui Cai", "Hins Hu", "Jiajie Li", "Shirley Huang", "Chenhui Deng", "Rongjian Liang", "Shufeng Kong", "Haoxing Ren", "Samitha Samaranayake", "Carla P. Gomes", "Zhiru Zhang"], "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.", "AI": {"tldr": "HeuriGym是一个评估LLM生成启发式算法能力的框架，通过代码执行反馈迭代优化，提出QYI指标量化性能，发现当前顶级模型表现仍远低于专家水平。", "motivation": "现有评估方法无法充分衡量LLM在组合优化问题中的能力，需更严谨的基准。", "method": "引入HeuriGym框架，让LLM生成启发式算法并通过代码执行反馈迭代优化，使用QYI指标评估性能。", "result": "测试9个顶级模型，QYI最高仅0.6，远低于专家基线1，显示LLM在工具使用、规划和自适应推理上的局限。", "conclusion": "HeuriGym为LLM在科学和工程领域的实际问题解决能力提供了开源基准，指导未来发展。"}}
{"id": "2506.07975", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07975", "abs": "https://arxiv.org/abs/2506.07975", "authors": ["Caleb Zheng", "Eli Shlizerman"], "title": "Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum", "comment": "26 pages, 3 figures", "summary": "A variety of pruning methods have been introduced for over-parameterized\nRecurrent Neural Networks to improve efficiency in terms of power consumption\nand storage utilization. These advances motivate a new paradigm, termed\n`hyperpruning', which seeks to identify the most suitable pruning strategy for\na given network architecture and application. Unlike conventional\nhyperparameter search, where the optimal configuration's accuracy remains\nuncertain, in the context of network pruning, the accuracy of the dense model\nsets the target for the accuracy of the pruned one. The goal, therefore, is to\ndiscover pruned variants that match or even surpass this established accuracy.\nHowever, exhaustive search over pruning configurations is computationally\nexpensive and lacks early performance guarantees. To address this challenge, we\npropose a novel Lyapunov Spectrum (LS)-based distance metric that enables early\ncomparison between pruned and dense networks, allowing accurate prediction of\npost-training performance. By integrating this LS-based distance with standard\nhyperparameter optimization algorithms, we introduce an efficient hyperpruning\nframework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an\norder of magnitude compared to conventional approaches relying on full\ntraining. Experiments on stacked LSTM and RHN architectures using the Penn\nTreebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under\nfixed training budgets and target pruning ratios, LSH consistently identifies\nsuperior pruned models. Remarkably, these pruned variants not only outperform\nthose selected by loss-based baseline but also exceed the performance of their\ndense counterpart.", "AI": {"tldr": "论文提出了一种基于Lyapunov谱（LS）的距离度量方法，用于高效比较剪枝网络与密集网络的性能，并结合超参数优化算法，实现了名为LSH的高效剪枝框架。", "motivation": "现有剪枝方法需要大量计算资源且缺乏早期性能保证，因此需要一种能快速预测剪枝网络性能的方法。", "method": "提出LS距离度量，结合超参数优化算法，构建LSH框架，显著减少搜索时间。", "result": "在多个网络架构和数据集上，LSH框架在固定训练预算和剪枝比例下，找到了性能优于密集网络的剪枝模型。", "conclusion": "LSH框架为网络剪枝提供了一种高效且性能优越的新方法。"}}
{"id": "2506.07976", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.07976", "abs": "https://arxiv.org/abs/2506.07976", "authors": ["Junhong Shen", "Hao Bai", "Lunjun Zhang", "Yifei Zhou", "Amrith Setlur", "Shengbang Tong", "Diego Caples", "Nan Jiang", "Tong Zhang", "Ameet Talwalkar", "Aviral Kumar"], "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction", "comment": null, "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.", "AI": {"tldr": "论文提出了一种新的测试时间扩展维度——交互扩展，通过增加代理的交互范围来支持复杂行为（如探索、回溯和动态重规划），并展示了在Web代理领域的成功应用。", "motivation": "当前测试时间扩展范式依赖于生成长推理轨迹（“多思考”）后再响应，但这种方法无法让代理从环境中获取新信息或随时间调整行为，因此需要探索交互扩展的可能性。", "method": "论文提出了TTI（测试时间交互），一种基于课程的在线强化学习方法，通过自适应调整代理的滚动长度来训练代理。", "result": "使用Gemma 3 12B模型，TTI在WebVoyager和WebArena基准测试中实现了最先进的性能，并展示了代理在探索与利用之间的自适应平衡能力。", "conclusion": "交互扩展是扩展每步计算能力的强大补充维度，为训练自适应代理提供了新途径。"}}
{"id": "2506.07980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.07980", "abs": "https://arxiv.org/abs/2506.07980", "authors": ["Alberto Bazán-Guillén", "Carlos Beis-Penedo", "Diego Cajaraville-Aboy", "Pablo Barbecho-Bautista", "Rebeca P. Díaz-Redondo", "Luis J. de la Cruz Llopis", "Ana Fernández-Vilas", "Mónica Aguilar Igartua", "Manuel Fernández-Veiga"], "title": "Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator", "comment": "21 pages, 7 figures", "summary": "Realistic urban traffic simulation is essential for sustainable urban\nplanning and the development of intelligent transportation systems. However,\ngenerating high-fidelity, time-varying traffic profiles that accurately reflect\nreal-world conditions, especially in large-scale scenarios, remains a major\nchallenge. Existing methods often suffer from limitations in accuracy,\nscalability, or raise privacy concerns due to centralized data processing. This\nwork introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a\nnovel framework that integrates Deep Reinforcement Learning (DRL) agents with\nthe SUMO simulator to generate realistic 24-hour traffic patterns. A key\ninnovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),\nwherein each traffic detector and its corresponding urban zone function as an\nindependent learning node. These nodes train local DRL models using minimal\nhistorical data and collaboratively refine their performance by exchanging\nmodel parameters with selected peers (e.g., geographically adjacent zones),\nwithout requiring a central coordinator. Evaluated using real-world data from\nthe city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as\nRouteSampler, as well as other centralized learning approaches, by delivering\nmore accurate and privacy-preserving traffic pattern generation.", "AI": {"tldr": "论文提出了一种名为DesRUTGe的去中心化框架，结合深度强化学习和SUMO模拟器，生成高保真的24小时交通模式，解决了现有方法在准确性、扩展性和隐私方面的问题。", "motivation": "现实城市交通模拟对可持续城市规划和智能交通系统发展至关重要，但现有方法在准确性、扩展性和隐私保护方面存在不足。", "method": "DesRUTGe框架利用去中心化联邦学习，每个交通检测器及其对应区域作为独立学习节点，通过局部DRL模型训练和参数交换协作优化性能。", "result": "基于巴塞罗那真实数据的评估表明，DesRUTGe在生成交通模式时比RouteSampler等工具更准确且保护隐私。", "conclusion": "DesRUTGe为大规模城市交通模拟提供了一种高效、隐私保护的新方法。"}}
{"id": "2506.07998", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.07998", "abs": "https://arxiv.org/abs/2506.07998", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "title": "Generative Modeling of Weights: Generalization or Memorization?", "comment": "Project page at https://boyazeng.github.io/weight_memorization", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "AI": {"tldr": "生成模型在图像和视频生成中取得成功后，被尝试用于合成神经网络权重。研究发现，现有方法主要通过记忆训练数据生成权重，无法产生新颖且高性能的模型。", "motivation": "探索生成模型在合成神经网络权重方面的潜力，评估其是否能生成新颖且高性能的权重。", "method": "研究了四种代表性方法，分析其生成权重的能力，并与简单基线方法（如添加噪声或权重集成）进行比较。", "result": "现有方法主要通过记忆训练数据生成权重，无法超越简单基线方法。修改建模因素或数据增强也无法有效缓解记忆问题。", "conclusion": "研究揭示了当前生成模型在新领域的局限性，强调了对生成模型评估的重要性。"}}
{"id": "2506.08001", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08001", "abs": "https://arxiv.org/abs/2506.08001", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Schölkopf", "Weiyang Liu"], "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "comment": "Technical report v1 (36 pages, 24 figures, project page:\n  https://spherelab.ai/poet-site/)", "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.", "AI": {"tldr": "POET是一种新型的重新参数化训练算法，通过正交等价变换优化神经元，提升大语言模型的训练效果和泛化能力。", "motivation": "大语言模型（LLMs）的训练是AI领域的重要挑战，POET旨在解决这一问题。", "method": "POET通过两个可学习的正交矩阵和一个固定随机权重矩阵重新参数化神经元，并利用高效近似方法实现灵活性和可扩展性。", "result": "实验验证了POET在训练LLMs中的有效性和可扩展性。", "conclusion": "POET通过正交等价变换优化神经元，显著提升了训练稳定性和泛化能力。"}}
{"id": "2506.06276", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.06276", "abs": "https://arxiv.org/abs/2506.06276", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "comment": "TLDR: We show for the first time that normalizing flows can be scaled\n  for high-resolution and text-conditioned image synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "AI": {"tldr": "STARFlow是一种基于归一化流的高分辨率图像生成模型，结合了Transformer的自回归能力，通过理论验证和多项创新实现了高效扩展。", "motivation": "解决高分辨率图像生成中归一化流模型的扩展性和性能问题。", "method": "提出TARFlow结合归一化流和Transformer，采用深度-浅层设计、潜在空间建模和新型引导算法。", "result": "在类条件和文本条件图像生成任务中表现优异，接近扩散模型的样本质量。", "conclusion": "STARFlow首次证明了归一化流在大规模高分辨率图像生成中的有效性。"}}
