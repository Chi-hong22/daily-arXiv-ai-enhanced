{"id": "2506.08200", "categories": ["cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.08200", "abs": "https://arxiv.org/abs/2506.08200", "authors": ["Kat R. Agres", "Adyasha Dash", "Phoebe Chua", "Stefan K. Ehrlich"], "title": "AffectMachine-Pop: A controllable expert system for real-time pop music generation", "comment": null, "summary": "Music is a powerful medium for influencing listeners' emotional states, and\nthis capacity has driven a surge of research interest in AI-based affective\nmusic generation in recent years. Many existing systems, however, are a black\nbox which are not directly controllable, thus making these systems less\nflexible and adaptive to users. We present \\textit{AffectMachine-Pop}, an\nexpert system capable of generating retro-pop music according to arousal and\nvalence values, which can either be pre-determined or based on a listener's\nreal-time emotion states. To validate the efficacy of the system, we conducted\na listening study demonstrating that AffectMachine-Pop is capable of generating\naffective music at target levels of arousal and valence. The system is tailored\nfor use either as a tool for generating interactive affective music based on\nuser input, or for incorporation into biofeedback or neurofeedback systems to\nassist users with emotion self-regulation."}
{"id": "2506.08294", "categories": ["cs.HC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.08294", "abs": "https://arxiv.org/abs/2506.08294", "authors": ["Ruanqianqian Huang", "Ayana Monroe", "Peli de Halleux", "Sorin Lerner", "Nikolaj Bj√∏rner"], "title": "Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling", "comment": null, "summary": "Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from\nbudgeting for grocery shopping to verifying software behavior. Logic modeling\nhelps solve CSPs programmatically using SMT solvers. Despite its importance in\nmany Computer Science disciplines, resources for teaching and learning logic\nmodeling are scarce and scattered, and challenges remain in designing\neducational environments for logic modeling that are accessible and meet the\nneeds of teachers and students. This paper explores how to design such an\nenvironment and probes the impact of the design on the learning experience.\nFrom a need-finding interview study and a design iteration with teachers of\nlogic modeling, we curated 10 design guidelines spanning three main\nrequirements: providing easy access, supporting various educational modalities,\nand allowing extensions for customized pedagogical needs. We implemented nine\nguidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a\nlogic modeling learning workshop with more than 100 students, we gathered\npositive feedback on its support for learning and identified opportunities for\nfuture improvements."}
{"id": "2506.08303", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08303", "abs": "https://arxiv.org/abs/2506.08303", "authors": ["Thomas M. Kwok", "Hilary HY Cheng", "Wai Tuck Chow"], "title": "EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation", "comment": "Accepted by the Workshop on Human-Robot Contact and Manipulation\n  (HRCM 2025) at RSS Conference 2025", "summary": "In this work, we introduce HJ-Pal, a lightweight wearable haptic device that\nleverages EMG-driven honeycomb jamming to render muscle activation as\nkinesthetic feedback, enabling remote palpation for small muscle assessment in\ntelerehabilitation."}
{"id": "2506.08443", "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.08443", "abs": "https://arxiv.org/abs/2506.08443", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition."}
{"id": "2506.08043", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications."}
{"id": "2506.08438", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08438", "abs": "https://arxiv.org/abs/2506.08438", "authors": ["Yuchen Wu", "Xinyi Zhong", "Zhuoran Yang"], "title": "Learning to Lead: Incentivizing Strategic Agents in the Dark", "comment": "81 pages, 7 figures", "summary": "We study an online learning version of the generalized principal-agent model,\nwhere a principal interacts repeatedly with a strategic agent possessing\nprivate types, private rewards, and taking unobservable actions. The agent is\nnon-myopic, optimizing a discounted sum of future rewards and may strategically\nmisreport types to manipulate the principal's learning. The principal,\nobserving only her own realized rewards and the agent's reported types, aims to\nlearn an optimal coordination mechanism that minimizes strategic regret. We\ndevelop the first provably sample-efficient algorithm for this challenging\nsetting. Our approach features a novel pipeline that combines (i) a delaying\nmechanism to incentivize approximately myopic agent behavior, (ii) an\ninnovative reward angle estimation framework that uses sector tests and a\nmatching procedure to recover type-dependent reward functions, and (iii) a\npessimistic-optimistic LinUCB algorithm that enables the principal to explore\nefficiently while respecting the agent's incentive constraints. We establish a\nnear optimal $\\tilde{O}(\\sqrt{T}) $ regret bound for learning the principal's\noptimal policy, where $\\tilde{O}(\\cdot) $ omits logarithmic factors. Our\nresults open up new avenues for designing robust online learning algorithms for\na wide range of game-theoretic settings involving private types and strategic\nagents."}
{"id": "2506.08138", "categories": ["cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.08138", "abs": "https://arxiv.org/abs/2506.08138", "authors": ["William Gebhardt", "Alexander G. Ororbia", "Nathan McDonald", "Clare Thiem", "Jack Lombardi"], "title": "A Practical Guide to Tuning Spiking Neuronal Dynamics", "comment": null, "summary": "In this work, we examine fundamental elements of spiking neural networks\n(SNNs) as well as how to tune them. Concretely, we focus on two different\nfoundational neuronal units utilized in SNNs -- the leaky integrate-and-fire\n(LIF) and the resonate-and-fire (RAF) neuron. We explore key equations and how\nhyperparameter values affect behavior. Beyond hyperparameters, we discuss other\nimportant design elements of SNNs -- the choice of input encoding and the setup\nfor excitatory-inhibitory populations -- and how these impact LIF and RAF\ndynamics."}
{"id": "2506.08039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08039", "abs": "https://arxiv.org/abs/2506.08039", "authors": ["Ray Wai Man Kong"], "title": "AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production", "comment": "12 pages, 9 Figures", "summary": "Efficiency, speed, and precision are essential in modern manufacturing. AI\nMaglev Conveyor system, combining magnetic levitation (maglev) technology with\nartificial intelligence (AI), revolutionizes automated production processes.\nThis system reduces maintenance costs and downtime by eliminating friction,\nenhancing operational efficiency. It transports goods swiftly with minimal\nenergy consumption, optimizing resource use and supporting sustainability. AI\nintegration enables real-time monitoring and adaptive control, allowing\nbusinesses to respond to production demand fluctuations and streamline supply\nchain operations.\n  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse\nproduct types and sizes for flexible manufacturing without extensive\nreconfiguration. AI algorithms optimize routing, reduce cycle times, and\nimprove throughput, creating an agile production line adaptable to market\nchanges.\n  This applied research paper introduces the Maglev Conveyor system, featuring\nan electromagnetic controller and multiple movers to enhance automation. It\noffers cost savings as an alternative to setups using six-axis robots or linear\nmotors, with precise adjustments for robotic arm loading. Operating at high\nspeeds minimizes treatment time for delicate components while maintaining\nprecision. Its adaptable design accommodates various materials, facilitating\nintegration of processing stations alongside electronic product assembly.\nPositioned between linear-axis and robotic systems in cost, the Maglev Conveyor\nis ideal for flat parts requiring minimal travel, transforming production\nefficiency across industries. It explores its technical advantages,\nflexibility, cost reductions, and overall benefits."}
{"id": "2506.08048", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08048", "abs": "https://arxiv.org/abs/2506.08048", "authors": ["Zheng Han", "Jun Zhou", "Jialun Pei", "Jing Qin", "Yingfang Fan", "Qi Dou"], "title": "Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts", "comment": null, "summary": "In augmented reality (AR)-guided surgical navigation, preoperative organ\nmodels are superimposed onto the patient's intraoperative anatomy to visualize\ncritical structures such as vessels and tumors. Accurate deformation modeling\nis essential to maintain the reliability of AR overlays by ensuring alignment\nbetween preoperative models and the dynamically changing anatomy. Although the\nfinite element method (FEM) offers physically plausible modeling, its high\ncomputational cost limits intraoperative applicability. Moreover, existing\nalgorithms often fail to handle large anatomical changes, such as those induced\nby pneumoperitoneum or ligament dissection, leading to inaccurate anatomical\ncorrespondences and compromised AR guidance. To address these challenges, we\npropose a data-driven biomechanics algorithm that preserves FEM-level accuracy\nwhile improving computational efficiency. In addition, we introduce a novel\nhuman-in-the-loop mechanism into the deformation modeling process. This enables\nsurgeons to interactively provide prompts to correct anatomical misalignments,\nthereby incorporating clinical expertise and allowing the model to adapt\ndynamically to complex surgical scenarios. Experiments on a publicly available\ndataset demonstrate that our algorithm achieves a mean target registration\nerror of 3.42 mm. Incorporating surgeon prompts through the interactive\nframework further reduces the error to 2.78 mm, surpassing state-of-the-art\nmethods in volumetric accuracy. These results highlight the ability of our\nframework to deliver efficient and accurate deformation modeling while\nenhancing surgeon-algorithm collaboration, paving the way for safer and more\nreliable computer-assisted surgeries."}
{"id": "2506.08018", "categories": ["cs.LG", "cs.AI", "03B65 ((Primary))", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.08018", "abs": "https://arxiv.org/abs/2506.08018", "authors": ["Fei Li", "Song Liu", "Weiguo Wu", "Shiqiang Nie", "Jinyu Wang"], "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache", "comment": "14 pages, 8 figures, 4 tables", "summary": "The high memory demands of the Key-Value (KV) Cache during the inference of\nLarge Language Models (LLMs) severely restrict their deployment in\nresource-constrained platforms. Quantization can effectively alleviate the\nmemory pressure caused by KV Cache. However, existing methods either rely on\nstatic one-size-fits-all precision allocation or fail to dynamically prioritize\ncritical KV in long-context tasks, forcing memory-accuracy-throughput\ntradeoffs. In this work, we propose a novel mixed-precision quantization method\nfor KV Cache named KVmix. KVmix leverages gradient-based importance analysis to\nevaluate how individual Key and Value projection matrices affect the model\nloss, enabling layer-specific bit-width allocation for mix-precision\nquantization. It dynamically prioritizes higher precision for important layers\nwhile aggressively quantizing less influential ones, achieving a tunable\nbalance between accuracy and efficiency. KVmix also introduces a dynamic\nlong-context optimization strategy that adaptively keeps full-precision KV\npairs for recent pivotal tokens and compresses older ones, achieving\nhigh-quality sequence generation with low memory usage. Additionally, KVmix\nprovides efficient low-bit quantization and CUDA kernels to optimize\ncomputational overhead. On LLMs such as Llama and Mistral, KVmix achieves\nnear-lossless inference performance with extremely low quantization\nconfiguration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x\nmemory compression and a 5.3x speedup in inference throughput."}
{"id": "2506.08028", "categories": ["eess.SY", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.08028", "abs": "https://arxiv.org/abs/2506.08028", "authors": ["Huy Truong-Ba", "Jacky Chin", "Michael E. Cholette", "Pietro Borghesani"], "title": "Sensor Fusion for Track Geometry Monitoring: Integrating On-Board Data and Degradation Models via Kalman Filtering", "comment": null, "summary": "Track geometry monitoring is essential for maintaining the safety and\nefficiency of railway operations. While Track Recording Cars (TRCs) provide\naccurate measurements of track geometry indicators, their limited availability\nand high operational costs restrict frequent monitoring across large rail\nnetworks. Recent advancements in on-board sensor systems installed on\nin-service trains offer a cost-effective alternative by enabling\nhigh-frequency, albeit less accurate, data collection. This study proposes a\nmethod to enhance the reliability of track geometry predictions by integrating\nlow-accuracy sensor signals with degradation models through a Kalman filter\nframework. An experimental campaign using a low-cost sensor system mounted on a\nTRC evaluates the proposed approach. The results demonstrate that incorporating\nfrequent sensor data significantly reduces prediction uncertainty, even when\nthe data is noisy. The study also investigates how the frequency of data\nrecording influences the size of the credible prediction interval, providing\nguidance on the optimal deployment of on-board sensors for effective track\nmonitoring and maintenance planning."}
{"id": "2506.08507", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08507", "abs": "https://arxiv.org/abs/2506.08507", "authors": ["Kuo Yang", "Xingjie Yang", "Linhui Yu", "Qing Xu", "Yan Fang", "Xu Wang", "Zhengyang Zhou", "Yang Wang"], "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning", "comment": null, "summary": "Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently\nemerged as a powerful paradigm for tackling complex real-world tasks. However,\nexisting Mas construction methods typically rely on manually crafted\ninteraction mechanisms or heuristic rules, introducing human biases and\nconstraining the autonomous ability. Even with recent advances in adaptive Mas\nconstruction, existing systems largely remain within the paradigm of\nsemi-autonomous patterns. In this work, we propose MasHost, a Reinforcement\nLearning (RL)-based framework for autonomous and query-adaptive Mas design. By\nformulating Mas construction as a graph search problem, our proposed MasHost\njointly samples agent roles and their interactions through a unified\nprobabilistic sampling mechanism. Beyond the accuracy and efficiency objectives\npursued in prior works, we introduce component rationality as an additional and\nnovel design principle in Mas. To achieve this multi-objective optimization, we\npropose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy\nthat collaboratively integrates group-relative advantages and action-wise\nrewards. To our knowledge, our proposed MasHost is the first RL-driven\nframework for autonomous Mas graph construction. Extensive experiments on six\nbenchmarks demonstrate that MasHost consistently outperforms most competitive\nbaselines, validating its effectiveness, efficiency, and structure rationality."}
{"id": "2506.08346", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08346", "abs": "https://arxiv.org/abs/2506.08346", "authors": ["Wenhan Yao", "Fen Xiao", "Xiarun Chen", "Jia Liu", "YongQiang He", "Weiping Wen"], "title": "SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models", "comment": "Accepted by IJCNN 2025", "summary": "Deep speech classification tasks, including keyword spotting and speaker\nverification, are vital in speech-based human-computer interaction. Recently,\nthe security of these technologies has been revealed to be susceptible to\nbackdoor attacks. Specifically, attackers use noisy disruption triggers and\nspeech element triggers to produce poisoned speech samples that train models to\nbecome vulnerable. However, these methods typically create only a limited\nnumber of backdoors due to the inherent constraints of the trigger function. In\nthis paper, we propose that speech backdoor attacks can strategically focus on\nspeech elements such as timbre and emotion, leveraging the Speech Large\nLanguage Model (SLLM) to generate diverse triggers. Increasing the number of\ntriggers may disproportionately elevate the poisoning rate, resulting in higher\nattack costs and a lower success rate per trigger. We introduce the Multiple\nGradient Descent Algorithm (MGDA) as a mitigation strategy to address this\nchallenge. The proposed attack is called the Speech Prompt Backdoor Attack\n(SPBA). Building on this foundation, we conducted attack experiments on two\nspeech classification tasks, demonstrating that SPBA shows significant trigger\neffectiveness and achieves exceptional performance in attack metrics."}
{"id": "2506.08467", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08467", "abs": "https://arxiv.org/abs/2506.08467", "authors": ["Prakash Shukla", "Suchismita Naik", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parson"], "title": "Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education", "comment": "8 pages, EduCHI 2025: 7th Annual Symposium on HCI Education,\n  Bloomington, IN, USA, July 2025", "summary": "The growing integration of AI tools in student design projects presents an\nunresolved challenge in HCI education: how should AI-generated content be cited\nand documented? Traditional citation frameworks -- grounded in credibility,\nretrievability, and authorship -- struggle to accommodate the dynamic and\nephemeral nature of AI outputs. In this paper, we examine how undergraduate\nstudents in a UX design course approached AI usage and citation when given the\nfreedom to integrate generative tools into their design process. Through\nqualitative analysis of 35 team projects and reflections from 175 students, we\nidentify varied citation practices ranging from formal attribution to indirect\nor absent acknowledgment. These inconsistencies reveal gaps in existing\nframeworks and raise questions about authorship, assessment, and pedagogical\ntransparency. We argue for rethinking AI citation as a reflective and\npedagogical practice; one that supports metacognitive engagement by prompting\nstudents to critically evaluate how and why they used AI throughout the design\nprocess. We propose alternative strategies -- such as AI contribution\nstatements and process-aware citation models that better align with the\niterative and reflective nature of design education. This work invites\neducators to reconsider how citation practices can support meaningful\nstudent--AI collaboration."}
{"id": "2506.08064", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08064", "abs": "https://arxiv.org/abs/2506.08064", "authors": ["Livio Tenze", "Enrique Canessa"], "title": "A Real-time 3D Desktop Display", "comment": "10 pages, 5 figures", "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0"}
{"id": "2506.08484", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.08484", "abs": "https://arxiv.org/abs/2506.08484", "authors": ["Cen Shipeng", "Tan Ying"], "title": "Efficient Fireworks Algorithm Equipped with an Explosion Mechanism based on Student's T-distribution", "comment": null, "summary": "Many real-world problems can be transformed into optimization problems, which\ncan be classified into convex and non-convex. Although convex problems are\nalmost completely studied in theory, many related algorithms to many non-convex\nproblems do not work well and we need more optimization techniques. As a swarm\nintelligence optimization algorithm, the Fireworks Algorithm(FWA) has been\nwidely studied and applied to many real-world scenarios, even including large\nlanguage model fine-tuning. But the current fireworks algorithm still has a\nnumber of problems. Firstly, as a heuristic algorithm, its performance on\nconvex problems cannot match the SOTA results, and can even be said to be\nunsatisfactory; secondly, the sampling methods (explosion) of most FWA variants\nare still uniform sampling, which is actually inefficient in high dimensional\ncases. This work of ours proposes a new student's t-distribution based\nFWA(TFWA) with a solid theoretical foundation, which fully utilizes the\nadvantage that student's t-distribution can adjust the parameters (degrees of\nfreedom) and thus adjust the exploitation capability. We have fully\nexperimented on mainstream benchmarks CEC2013 and CEC2017, which proves that\nTFWA not only becomes the strongest variant of the fireworks algorithm, but\nalso achieves results comparable to SOTA on the test set, and its performance\nis far superior to that of the SOTA algorithm in some scenarios with a large\nnumber of extreme points."}
{"id": "2506.08045", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08045", "abs": "https://arxiv.org/abs/2506.08045", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs", "comment": "40 pages, 6 Figures", "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence,\nintegrating perception, decision-making, memory, and collaborative planning to\noperate adaptively in complex, real-world environments. Driven by recent\nadvances in Agentic AI, these systems surpass traditional UAVs by exhibiting\ngoal-driven behavior, contextual reasoning, and interactive autonomy. We\nprovide a comprehensive foundation for understanding the architectural\ncomponents and enabling technologies that distinguish Agentic UAVs from\ntraditional autonomous UAVs. Furthermore, a detailed comparative analysis\nhighlights advancements in autonomy with AI agents, learning, and mission\nflexibility. This study explores seven high-impact application domains\nprecision agriculture, construction & mining, disaster response, environmental\nmonitoring, infrastructure inspection, logistics, security, and wildlife\nconservation, illustrating the broad societal value of agentic aerial\nintelligence. Furthermore, we identify key challenges in technical constraints,\nregulatory limitations, and data-model reliability, and we present emerging\nsolutions across hardware innovation, learning architectures, and human-AI\ninteraction. Finally, a future roadmap is proposed, outlining pathways toward\nself-evolving aerial ecosystems, system-level collaboration, and sustainable,\nequitable deployments. This survey establishes a foundational framework for the\nfuture development, deployment, and governance of agentic aerial systems\n(Agentic UAVs) across diverse societal and industrial domains."}
{"id": "2506.08052", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08052", "abs": "https://arxiv.org/abs/2506.08052", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "comment": null, "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS."}
{"id": "2506.08019", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.08019", "abs": "https://arxiv.org/abs/2506.08019", "authors": ["Andrew Wells", "Geraldine Henningsen", "Brice Bolane Tchinde Kengne"], "title": "Gridding Forced Displacement using Semi-Supervised Learning", "comment": null, "summary": "We present a semi-supervised approach that disaggregates refugee statistics\nfrom administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan\nAfrican countries. By integrating UNHCR's ProGres registration data with\nsatellite-derived building footprints from Google Open Buildings and location\ncoordinates from OpenStreetMap Populated Places, our label spreading algorithm\ncreates spatially explicit refugee statistics at high granularity.This\nmethodology achieves 92.9% average accuracy in placing over 10 million refugee\nobservations into appropriate grid cells, enabling the identification of\nlocalized displacement patterns previously obscured in broader regional and\nnational statistics. The resulting high-resolution dataset provides a\nfoundation for a deeper understanding of displacement drivers."}
{"id": "2506.08029", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08029", "abs": "https://arxiv.org/abs/2506.08029", "authors": ["Jiayu Li", "Masood Mortazavi", "Ning Yan", "Yihong Ma", "Reza Zafarani"], "title": "Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning", "comment": "A briefer version of this paper was accepted as a Work-in-Progress\n  (WIP) at the Design Automation Conference (DAC) 2024", "summary": "The goal of inverse design in distributed circuits is to generate\nnear-optimal designs that meet a desirable transfer function specification.\nExisting design exploration methods use some combination of strategies\ninvolving artificial grids, differentiable evaluation procedures, and specific\ntemplate topologies. However, real-world design practices often require\nnon-differentiable evaluation procedures, varying topologies, and\nnear-continuous placement spaces. In this paper, we propose DCIDA, a design\nexploration framework that learns a near-optimal design sampling policy for a\ntarget transfer function. DCIDA decides all design factors in a compound\nsingle-step action by sampling from a set of jointly-trained conditional\ndistributions generated by the policy. Utilizing an injective interdependent\n``map\", DCIDA transforms raw sampled design ``actions\" into uniquely equivalent\nphysical representations, enabling the framework to learn the conditional\ndependencies among joint ``raw'' design decisions. Our experiments demonstrate\nDCIDA's Transformer-based policy network achieves significant reductions in\ndesign error compared to state-of-the-art approaches, with significantly better\nfit in cases involving more complex transfer functions."}
{"id": "2506.08038", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08038", "abs": "https://arxiv.org/abs/2506.08038", "authors": ["Chen Huang", "Dingxuan Wang", "Ronghui Hou"], "title": "Joint Routing and Control Optimization in VANET", "comment": "11 pages; 10 figures", "summary": "In this paper, we introduce DynaRoute, an adaptive joint optimization\nframework for dynamic vehicular networks that simultaneously addresses platoon\ncontrol and data transmission through trajectory-aware routing and\nsafety-constrained vehicle coordination. DynaRoute guarantees continuous\nvehicle movement via platoon safety control with optimizing transmission paths\nthrough real-time trajectory prediction and ensuring reliable data. Our\nsolution achieves three key objectives: (1) maintaining platoon stability\nthrough accurate data transmission, (2) enabling adaptive routing based on\nvehicle movement patterns, and (3) enhancing overall intelligent transportation\nsystem performance. DynaRoute equires predefined traffic models and adapts to\ndynamic network conditions using local vehicle state information. We present\ncomprehensive simulation results demonstrating that DynaRoute maintains control\nand transmission performance in multiple complex scenarios while significantly\nimproving throughput and reliability compared to traditional approaches."}
{"id": "2506.08348", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08348", "abs": "https://arxiv.org/abs/2506.08348", "authors": ["Wenhan Yao", "Fen Xiao", "Xiarun Chen", "Jia Liu", "YongQiang He", "Weiping Wen"], "title": "Pureformer-VC: Non-parallel Voice Conversion with Pure Stylized Transformer Blocks and Triplet Discriminative Training", "comment": "Accepted by IJCNN 2025", "summary": "As a foundational technology for intelligent human-computer interaction,\nvoice conversion (VC) seeks to transform speech from any source timbre into any\ntarget timbre. Traditional voice conversion methods based on Generative\nAdversarial Networks (GANs) encounter significant challenges in precisely\nencoding diverse speech elements and effectively synthesising these elements\ninto natural-sounding converted speech. To overcome these limitations, we\nintroduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer\nblocks to build a disentangled encoder and employs Zipformer blocks to create a\nstyle transfer decoder. We adopt a variational decoupled training approach to\nisolate speech components using a Variational Autoencoder (VAE), complemented\nby triplet discriminative training to enhance the speaker's discriminative\ncapabilities. Furthermore, we incorporate the Attention Style Transfer\nMechanism (ASTM) with Zipformer's shared weights to improve the style transfer\nperformance in the decoder. We conducted experiments on two multi-speaker\ndatasets. The experimental results demonstrate that the proposed model achieves\ncomparable subjective evaluation scores while significantly enhancing objective\nmetrics compared to existing approaches in many-to-many and many-to-one VC\nscenarios."}
{"id": "2506.08517", "categories": ["cs.HC", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.08517", "abs": "https://arxiv.org/abs/2506.08517", "authors": ["Mayar Elfares", "Salma Younis", "Pascal Reisert", "Ralf K√ºsters", "Tobias Renner", "Andreas Bulling"], "title": "Guidelines for Gaze-based Neural Preliminary Diagnosis", "comment": null, "summary": "Neural disorders refer to any condition affecting the nervous system and that\ninfluence how individuals perceive and interact with the world. Traditional\nneural diagnoses rely on cumbersome, time-consuming, or subjective methods,\nsuch as clinical interviews, behavioural observations, or medical imaging. Eye\ntracking is an attractive alternative because analysing eye movements, such as\nfixations and saccades, can provide more objective insights into brain function\nand cognitive processing by capturing non-verbal and unconscious responses.\nDespite its potential, existing gaze-based studies presented seemingly\ncontradictory findings. They are dispersed across diverse fields, requiring\nfurther research to standardise protocols and expand their application,\nparticularly as a preliminary indicator of neural processes for differential\ndiagnosis. Therefore, this paper outlines the main agreed-upon findings and\nprovides a systematisation of knowledge and key guidelines towards advancing\ngaze-based neural preliminary diagnosis."}
{"id": "2506.08161", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.08161", "abs": "https://arxiv.org/abs/2506.08161", "authors": ["Jakub Bok≈°ansk√Ω", "Daniel Meister", "Carsten Benthin"], "title": "GATE: Geometry-Aware Trained Encoding", "comment": null, "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08071", "abs": "https://arxiv.org/abs/2506.08071", "authors": ["Aniket Rege", "Zinnia Nie", "Mahesh Ramesh", "Unmesh Raskar", "Zhuoran Yu", "Aditya Kusupati", "Yong Jae Lee", "Ramya Korlakai Vinayak"], "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems", "comment": "41 pages, 22 figures, 17 tables", "summary": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is\nheavily Amero and Euro-centric, underrepresenting the cultures of the Global\nSouth. To analyze these biases, we introduce CuRe, a novel and scalable\nbenchmarking and scoring suite for cultural representativeness that leverages\nthe marginal utility of attribute specification to T2I systems as a proxy for\nhuman judgments. Our CuRe benchmark dataset has a novel categorical hierarchy\nbuilt from the crowdsourced Wikimedia knowledge graph, with 300 cultural\nartifacts across 32 cultural subcategories grouped into six broad cultural axes\n(food, art, fashion, architecture, celebrations, and people). Our dataset's\ncategorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing\ntheir response to increasing the informativeness of text conditioning, enabling\nfine-grained cultural comparisons. We empirically observe much stronger\ncorrelations of our class of scorers to human judgments of perceptual\nsimilarity, image-text alignment, and cultural diversity across image encoders\n(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,\nGemini 2.0 Flash) and state-of-the-art text-to-image systems, including three\nvariants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,\nand DALL-E 3. The code and dataset is open-sourced and available at\nhttps://aniketrege.github.io/cure/."}
{"id": "2506.08020", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08020", "abs": "https://arxiv.org/abs/2506.08020", "authors": ["Zi-Ying Chen", "Chuan-Xian Ren", "Hong Yan"], "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation", "comment": null, "summary": "Partial domain adaptation (PDA) problem requires aligning cross-domain\nsamples while distinguishing the outlier classes for accurate knowledge\ntransfer. The widely used weighting framework tries to address the outlier\nclasses by introducing the reweighed source domain with a similar label\ndistribution to the target domain. However, the empirical modeling of weights\ncan only characterize the sample-wise relations, which leads to insufficient\nexploration of cluster structures, and the weights could be sensitive to the\ninaccurate prediction and cause confusion on the outlier classes. To tackle\nthese issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model\nto simultaneously characterize the sample-wise and class-wise relations in a\nunified transport framework. Specifically, a cooperation mechanism between\nsample-level and class-level transport is introduced, where the sample-level\ntransport provides essential structure information for the class-level\nknowledge transfer, while the class-level transport supplies discriminative\ninformation for the outlier identification. The bi-level transport plan\nprovides guidance for the alignment process. By incorporating the label-aware\ntransport cost, the local transport structure is ensured and a fast computation\nformulation is derived to improve the efficiency. Extensive experiments on\nbenchmark datasets validate the competitiveness of BUOT."}
{"id": "2506.08032", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.08032", "abs": "https://arxiv.org/abs/2506.08032", "authors": ["Jakub Ka≈°par", "V√≠t Fanta", "Vladim√≠r Havlena"], "title": "Tram Positioning with Map-Enabled GNSS Data Reconciliation", "comment": "Submitted to European Control Conference 2025", "summary": "This paper presents an approach to tackle the problem of tram localization\nthrough utilizing a custom processing of Global Navigation Satellite System\n(GNSS) observables and the track map. The method is motivated by suboptimal\nperformance in dense urban environments where the direct line of sight to GNSS\nsatellites is often obscured which leads to multipath propagation of GNSS\nsignals. The presented concept is based upon the iterated extended Kalman\nfilter (IEKF) and has linear complexity (with respect to the number of GNSS\nmeasurements) as opposed to some other techniques mitigating the multipath\nsignal propagation. The technique is demonstrated both on a simulated example\nand real data. The root-mean-squared errors from the simulated ground truth\npositions show that the presented solution is able to improve performance\ncompared to a baseline localization approach. Similar result is achieved for\nthe experiment with real data, while treating orthogonal projections onto the\ntram track as the true position, which is unavailable in the realistic\nscenario. This proof-of-concept shows results which may be further improved\nwith implementation of a bank-of-models method or $\\chi^2$-based rejection of\noutlying GNSS pseudorange measurements."}
{"id": "2506.09046", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09046", "abs": "https://arxiv.org/abs/2506.09046", "authors": ["Xiaowen Ma", "Chenyang Lin", "Yao Zhang", "Volker Tresp", "Yunpu Ma"], "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation", "comment": null, "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework."}
{"id": "2506.08357", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08357", "abs": "https://arxiv.org/abs/2506.08357", "authors": ["Franck Meyer", "Kyunghoon Hur", "Edward Choi"], "title": "MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion", "comment": "Main paper (16 pages, 5 figures). Paper submitted for review. Code\n  available at https://github.com/fr-meyer/MD-ViSCo", "summary": "Despite the remarkable progress of deep-learning methods generating a target\nvital sign waveform from a source vital sign waveform, most existing models are\ndesigned exclusively for a specific source-to-target pair. This requires\ndistinct model architectures, optimization procedures, and pre-processing\npipelines, resulting in multiple models that hinder usability in clinical\nsettings. To address this limitation, we propose the Multi-Directional\nVital-Sign Converter (MD-ViSCo), a unified framework capable of generating any\ntarget waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or\narterial blood pressure (ABP) from any single input waveform with a single\nmodel. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin\nTransformer that leverages Adaptive Instance Normalization (AdaIN) to capture\ndistinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct\nmulti-directional waveform generation on two publicly available datasets. Our\nframework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average\nacross all waveform types, lowering Mean absolute error (MAE) by 8.8% and\nimproving Pearson correlation (PC) by 4.9% over two datasets. In addition, the\ngenerated ABP waveforms satisfy the Association for the Advancement of Medical\nInstrumentation (AAMI) criterion and achieve Grade B on the British\nHypertension Society (BHS) standard, outperforming all baselines. By\neliminating the need for developing a distinct model for each task, we believe\nthat this work offers a unified framework that can deal with any kind of vital\nsign waveforms with a single model in healthcare monitoring."}
{"id": "2506.08549", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08549", "abs": "https://arxiv.org/abs/2506.08549", "authors": ["Rajan Das Gupta", "Ashikur Rahman", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "title": "Exploring the Convergence of HCI and Evolving Technologies in Information Systems", "comment": "Accepted in CITIC 2025", "summary": "Modern technology driven information systems are part of our daily lives.\nHowever, this deep integration poses new challenges to the human computer\ninteraction (HCI) professionals. With the rapid growth of mobile and cloud\ncomputing and the Internet of Things (IoT), the demand for HCI specialists to\ndesign user-friendly and adaptable interfaces has never been more pressing.\nEspecially for diverse user groups such as children, the elderly and people\nwith disabilities who need interfaces tailored to their needs regardless of\ntime and location. This study reviewed 50 recent papers on HCI interface design\nfor modern information systems. The goal is to see how well these methods\naddress the demands of current technology. The findings show that most HCI\ndesign methods are still based on old desktop models and do not support mobile\nusers and location-based services well. Most existing interface design\nguidelines do not align with the flexibility and dynamism of emerging\ntechnologies. The goal of this study is to improve interface design by\ncombining agile methodologies with human-centered design principles. Future\nstudies should also incorporate both qualitative and quantitative approaches,\nparticularly in the context of cloud-based technologies and organizational\ninformation systems. This approach aims to bridge the gap between current\ninterface design practices and the changing technological landscape."}
{"id": "2506.08237", "categories": ["cs.GR", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.08237", "abs": "https://arxiv.org/abs/2506.08237", "authors": ["Bailey Miller", "Rohan Sawhney", "Keenan Crane", "Ioannis Gkioulekas"], "title": "Solving partial differential equations in participating media", "comment": "SIGGRAPH 2025. Project page\n  https://imaging.cs.cmu.edu/volumetric_walk_on_spheres", "summary": "We consider the problem of solving partial differential equations (PDEs) in\ndomains with complex microparticle geometry that is impractical, or\nintractable, to model explicitly. Drawing inspiration from volume rendering, we\npropose tackling this problem by treating the domain as a participating medium\nthat models microparticle geometry stochastically, through aggregate\nstatistical properties (e.g., particle density). We first introduce the problem\nsetting of PDE simulation in participating media. We then specialize to\nexponential media and describe the properties that make them an attractive\nmodel of microparticle geometry for PDE simulation problems. We use these\nproperties to develop two new algorithms, volumetric walk on spheres and\nvolumetric walk on stars, that generalize previous Monte Carlo algorithms to\nenable efficient and discretization-free simulation of linear elliptic PDEs\n(e.g., Laplace) in participating media. We demonstrate experimentally that our\nalgorithms can solve Laplace boundary value problems with complex microparticle\ngeometry more accurately and more efficiently than previous approaches, such as\nensemble averaging and homogenization."}
{"id": "2506.08149", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}."}
{"id": "2506.08137", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08137", "abs": "https://arxiv.org/abs/2506.08137", "authors": ["Oishee Bintey Hoque", "Abhijin Adiga", "Aniruddha Adiga", "Siddharth Chaudhary", "Madhav V. Marathe", "S. S. Ravi", "Kirti Rajagopalan", "Amanda Wilson", "Samarth Swarup"], "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation", "comment": null, "summary": "Accurate canal network mapping is essential for water management, including\nirrigation planning and infrastructure maintenance. State-of-the-art semantic\nsegmentation models for infrastructure mapping, such as roads, rely on large,\nwell-annotated remote sensing datasets. However, incomplete or inadequate\nground truth can hinder these learning approaches. Many infrastructure networks\nhave graph-level properties such as reachability to a source (like canals) or\nconnectivity (roads) that can be leveraged to improve these existing ground\ntruth. This paper develops a novel iterative framework IGraSS, combining a\nsemantic segmentation module-incorporating RGB and additional modalities (NDWI,\nDEM)-with a graph-based ground-truth refinement module. The segmentation module\nprocesses satellite imagery patches, while the refinement module operates on\nthe entire data viewing the infrastructure network as a graph. Experiments show\nthat IGraSS reduces unreachable canal segments from around 18% to 3%, and\ntraining with refined ground truth significantly improves canal identification.\nIGraSS serves as a robust framework for both refining noisy ground truth and\nmapping canal networks from remote sensing imagery. We also demonstrate the\neffectiveness and generalizability of IGraSS using road networks as an example,\napplying a different graph-theoretic constraint to complete road networks."}
{"id": "2506.08021", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2506.08021", "abs": "https://arxiv.org/abs/2506.08021", "authors": ["Weihao Zou", "Weibing Feng", "Pin Wu"], "title": "FlowBERT: Prompt-tuned BERT for variable flow field prediction", "comment": null, "summary": "This study proposes a universal flow field prediction framework based on\nknowledge transfer\n  from large language model (LLM), addressing the high computational costs of\ntraditional\n  computational fluid dynamics (CFD) methods and the limited cross-condition\ntransfer capability\n  of existing deep learning models. The framework innovatively integrates\nProper Orthogonal\n  Decomposition (POD) dimensionality reduction with fine-tuning strategies for\npretrained LLM,\n  where POD facilitates compressed representation of flow field features while\nthe fine-tuned model\n  learns to encode system dynamics in state space. To enhance the model's\nadaptability to flow field\n  data, we specifically designed fluid dynamics-oriented text templates that\nimprove predictive\n  performance through enriched contextual semantic information. Experimental\nresults demonstrate\n  that our framework outperforms conventional Transformer models in few-shot\nlearning scenarios while\n  exhibiting exceptional generalization across various inflow conditions and\nairfoil geometries.\n  Ablation studies reveal the contributions of key components in the FlowBERT\narchitecture. Compared\n  to traditional Navier-Stokes equation solvers requiring hours of computation,\nour approach reduces\n  prediction time to seconds while maintaining over 90% accuracy. The developed\nknowledge transfer\n  paradigm establishes a new direction for rapid fluid dynamics prediction,\nwith potential\n  applications extending to aerodynamic optimization, flow control, and other\nengineering domains."}
{"id": "2506.08033", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08033", "abs": "https://arxiv.org/abs/2506.08033", "authors": ["Axel TahmasebiMoradi", "Vincent Ren", "Benjamin Le-Creurer", "Chetra Mang"], "title": "Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases", "comment": null, "summary": "Aiming to reduce the computational cost of numerical simulations, a\nconvolutional neural network (CNN) and a multi-layer perceptron (MLP) are\nintroduced to build a surrogate model to approximate radiative heat transfer\nsolutions in a 2-D walled domain with participative gases. The originality of\nthis work lays in the adaptation of the inputs of the problem (gas and wall\nproperties) in order to fit with the CNN architecture, more commonly used for\nimage processing. Two precision datasets have been created with the classical\nsolver, ICARUS2D, that uses the discrete transfer radiation method with the\nstatistical narrow bands model. The performance of the CNN architecture is\ncompared to a more classical MLP architecture in terms of speed and accuracy.\nThanks to Optuna, all results are obtained using the optimized hyper parameters\nnetworks. The results show a significant speedup with industrially acceptable\nrelative errors compared to the classical solver for both architectures.\nAdditionally, the CNN outperforms the MLP in terms of precision and is more\nrobust and stable to changes in hyper-parameters. A performance analysis on the\ndataset size of the samples have also been carried out to gain a deeper\nunderstanding of the model behavior."}
{"id": "2506.08372", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08372", "abs": "https://arxiv.org/abs/2506.08372", "authors": ["Rishabh Ranjan", "Likhith Ayinala", "Mayank Vatsa", "Richa Singh"], "title": "Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages", "comment": "Accepted in Interpseech 2025", "summary": "This paper introduces a novel multimodal framework for hate speech detection\nin deepfake audio, excelling even in zero-shot scenarios. Unlike previous\napproaches, our method uses contrastive learning to jointly align audio and\ntext representations across languages. We present the first benchmark dataset\nwith 127,290 paired text and synthesized speech samples in six languages:\nEnglish and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil,\nTelugu). Our model learns a shared semantic embedding space, enabling robust\ncross-lingual and cross-modal classification. Experiments on two multilingual\ntest sets show our approach outperforms baselines, achieving accuracies of\n0.819 and 0.701, and generalizes well to unseen languages. This demonstrates\nthe advantage of combining modalities for hate speech detection in synthetic\nmedia, especially in low-resource settings where unimodal models falter. The\nDataset is available at https://www.iab-rubric.org/resources."}
{"id": "2506.08634", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08634", "abs": "https://arxiv.org/abs/2506.08634", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills."}
{"id": "2506.08334", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08334", "abs": "https://arxiv.org/abs/2506.08334", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly."}
{"id": "2506.08291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08291", "abs": "https://arxiv.org/abs/2506.08291", "authors": ["Won Kyung Do", "Matthew Strong", "Aiden Swann", "Boshu Lei", "Monroe Kennedy III"], "title": "TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation", "comment": null, "summary": "Advanced dexterous manipulation involving multiple simultaneous contacts\nacross different surfaces, like pinching coins from ground or manipulating\nintertwined objects, remains challenging for robotic systems. Such tasks exceed\nthe capabilities of vision and proprioception alone, requiring high-resolution\ntactile sensing with calibrated physical metrics. Raw optical tactile sensor\nimages, while information-rich, lack interpretability and cross-sensor\ntransferability, limiting their real-world utility. TensorTouch addresses this\nchallenge by integrating finite element analysis with deep learning to extract\ncomprehensive contact information from optical tactile sensors, including\nstress tensors, deformation fields, and force distributions at pixel-level\nresolution. The TensorTouch framework achieves sub-millimeter position accuracy\nand precise force estimation while supporting large sensor deformations crucial\nfor manipulating soft objects. Experimental validation demonstrates 90% success\nin selectively grasping one of two strings based on detected motion, enabling\nnew contact-rich manipulation capabilities previously inaccessible to robotic\nsystems."}
{"id": "2506.08163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08163", "abs": "https://arxiv.org/abs/2506.08163", "authors": ["Harshvardhan Takawale", "Nirupam Roy"], "title": "Spectral Domain Neural Reconstruction for Passband FMCW Radars", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.23313", "summary": "We present SpINRv2, a neural framework for high-fidelity volumetric\nreconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.\nExtending our prior work (SpINR), this version introduces enhancements that\nallow accurate learning under high start frequencies-where phase aliasing and\nsub-bin ambiguity become prominent. Our core contribution is a fully\ndifferentiable frequency-domain forward model that captures the complex radar\nresponse using closed-form synthesis, paired with an implicit neural\nrepresentation (INR) for continuous volumetric scene modeling. Unlike\ntime-domain baselines, SpINRv2 directly supervises the complex frequency\nspectrum, preserving spectral fidelity while drastically reducing computational\noverhead. Additionally, we introduce sparsity and smoothness regularization to\ndisambiguate sub-bin ambiguities that arise at fine range resolutions.\nExperimental results show that SpINRv2 significantly outperforms both classical\nand learning-based baselines, especially under high-frequency regimes,\nestablishing a new benchmark for neural radar-based 3D imaging."}
{"id": "2506.08022", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations."}
{"id": "2506.08034", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08034", "abs": "https://arxiv.org/abs/2506.08034", "authors": ["Michael Sebek"], "title": "Exploring Noncommutative Polynomial Equation Methods for Discrete-Time Quaternionic Control", "comment": "5 pages, 1 figure, to be presented at the 33rd Mediterranean\n  Conference on Control and Automation, Tangier, Morocco, June 10-13, 2025", "summary": "We present new polynomial-based methods for discrete-time quaternionic\nsystems, highlighting how noncommutative multiplication modifies classical\ncontrol approaches. Defining quaternionic polynomials via a backward-shift\noperator, we examine left and right fraction representations of transfer\nfunctions, showing that right zeros correspond to similarity classes of\nquaternionic matrix right eigenvalues. We then propose a feedback design\nprocedure that generalizes pole placement to quaternions - a first approach\nusing a genuine quaternionic polynomial equation."}
{"id": "2506.08457", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08457", "abs": "https://arxiv.org/abs/2506.08457", "authors": ["Ge Zhu", "Yutong Wen", "Zhiyao Duan"], "title": "A Review on Score-based Generative Models for Audio Applications", "comment": null, "summary": "Diffusion models have emerged as powerful deep generative techniques,\nproducing high-quality and diverse samples in applications in various domains\nincluding audio. These models have many different design choices suitable for\ndifferent applications, however, existing reviews lack in-depth discussions of\nthese design choices. The audio diffusion model literature also lacks\nprincipled guidance for the implementation of these design choices and their\ncomparisons for different applications. This survey provides a comprehensive\nreview of diffusion model design with an emphasis on design principles for\nquality improvement and conditioning for audio applications. We adopt the score\nmodeling perspective as a unifying framework that accommodates various\ninterpretations, including recent approaches like flow matching. We\nsystematically examine the training and sampling procedures of diffusion\nmodels, and audio applications through different conditioning mechanisms. To\naddress the lack of audio diffusion model codebases and to promote reproducible\nresearch and rapid prototyping, we introduce an open-source codebase at\nhttps://github.com/gzhu06/AudioDiffuser that implements our reviewed framework\nfor various audio applications. We demonstrate its capabilities through three\ncase studies: audio generation, speech enhancement, and text-to-speech\nsynthesis, with benchmark evaluations on standard datasets."}
{"id": "2506.08725", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08725", "abs": "https://arxiv.org/abs/2506.08725", "authors": ["Hyeon Jeon", "Jeongin Park", "Sungbok Shin", "Jinwook Seo"], "title": "Stop Misusing t-SNE and UMAP for Visual Analytics", "comment": "9 pages", "summary": "Misuses of t-SNE and UMAP in visual analytics have become increasingly\ncommon. For example, although t-SNE and UMAP projections often do not\nfaithfully reflect true distances between clusters, practitioners frequently\nuse them to investigate inter-cluster relationships. In this paper, we bring\nthis issue to the surface and comprehensively investigate why such misuse\noccurs and how to prevent it. We conduct a literature review of 114 papers to\nverify the prevalence of the misuse and analyze the reasonings behind it. We\nthen execute an interview study to uncover practitioners' implicit motivations\nfor using these techniques -- rationales often undisclosed in the literature.\nOur findings indicate that misuse of t-SNE and UMAP primarily stems from\nlimited discourse on their appropriate use in visual analytics. We conclude by\nproposing future directions and concrete action items to promote more\nreasonable use of DR."}
{"id": "2506.08350", "categories": ["cs.GR", "cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Ak≈üit"], "title": "Complex-Valued Holographic Radiance Fields", "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations."}
{"id": "2506.08296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08296", "abs": "https://arxiv.org/abs/2506.08296", "authors": ["Hongjun Wu", "Heng Zhang", "Pengsong Zhang", "Jin Wang", "Cong Wang"], "title": "HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation", "comment": "31 pages,5 figures", "summary": "Recent advances in multimodal vision-language-action (VLA) models have\nrevolutionized traditional robot learning, enabling systems to interpret\nvision, language, and action in unified frameworks for complex task planning.\nHowever, mastering complex manipulation tasks remains an open challenge,\nconstrained by limitations in persistent contextual memory, multi-agent\ncoordination under uncertainty, and dynamic long-horizon planning across\nvariable sequences. To address this challenge, we propose \\textbf{HiBerNAC}, a\n\\textbf{Hi}erarchical \\textbf{B}rain-\\textbf{e}mulated \\textbf{r}obotic\n\\textbf{N}eural \\textbf{A}gent \\textbf{C}ollective, inspired by breakthroughs\nin neuroscience, particularly in neural circuit mechanisms and hierarchical\ndecision-making. Our framework combines: (1) multimodal VLA planning and\nreasoning with (2) neuro-inspired reflection and multi-agent mechanisms,\nspecifically designed for complex robotic manipulation tasks. By leveraging\nneuro-inspired functional modules with decentralized multi-agent collaboration,\nour approach enables robust and enhanced real-time execution of complex\nmanipulation tasks. In addition, the agentic system exhibits scalable\ncollective intelligence via dynamic agent specialization, adapting its\ncoordination strategy to variable task horizons and complexity. Through\nextensive experiments on complex manipulation tasks compared with\nstate-of-the-art VLA models, we demonstrate that \\textbf{HiBerNAC} reduces\naverage long-horizon task completion time by 23\\%, and achieves non-zero\nsuccess rates (12\\textendash 31\\%) on multi-path tasks where prior\nstate-of-the-art VLA models consistently fail. These results provide indicative\nevidence for bridging biological cognition and robotic learning mechanisms."}
{"id": "2506.08185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08185", "abs": "https://arxiv.org/abs/2506.08185", "authors": ["Huixin Zhan", "Jason H. Moore"], "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework", "comment": null, "summary": "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting."}
{"id": "2506.08027", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.08027", "abs": "https://arxiv.org/abs/2506.08027", "authors": ["Asit Mishra", "Dusan Stosic", "Simon Layton"], "title": "Recipes for Pre-training LLMs with MXFP8", "comment": null, "summary": "Precision scaling - using fewer bits to represent model parameters and\nrelated tensors during pre-training - has emerged as a compelling technique for\nimproving GPU efficiency without sacrificing accuracy. Microscaling (MX)\nformats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling\nthis precision scaling aspect. These formats combine narrow floating-point data\ntypes with per-block scaling factors, offering a fine-grained approach to\nquantizing tensors.\n  Although MX-formats offer the promise of improved numeric stability compared\nto other reduced-precision representations, in practice they must be used\ncarefully in order to successfully converge an LLM on a multi-trillion token\ndataset. In this paper, we show that the rounding mode suggested in OCP\nspecification can lead to divergence when pre-training an LLM. We show an\nimproved rounding mode, which uses round-to-infinity to compute scaling\nfactors, enables successful pre-training in MXFP8 for an 8B model on 15T\ntokens."}
{"id": "2506.08036", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08036", "abs": "https://arxiv.org/abs/2506.08036", "authors": ["Rahul Bhadani"], "title": "Followstopper Revisited: Phase-space Lagrangian Controller for Traffic Decongestion", "comment": "10 figures, 11 pages", "summary": "This paper revisits Followerstopper, a phase-space-based control system that\nhad demonstrated its ability to mitigate emergent traffic jams due to\nstop-and-go traffic during rush hour in the mixed-autonomy setting.\nFollowerstopper was deployed on an autonomous vehicle. The controller\nattenuates the emanant traffic waves by regulating its velocity according to\nthe relative distance and velocity of the leader car. While regulating the\nvelocity, the controller also prevents the collision of the ego vehicle with\nthe lead vehicle within the range specified by the controller's design\nparameter. The controller design is based on a configurable quadratic curve on\nrelative distance-relative velocity phase-space that allows the transition of\nthe regulated velocity from (i) no modification of input, (ii) decelerating to\nmatch the leader's velocity (iii) braking to avoid any imminent collision. In\nthis paper, we explore the phase-space properties of Followerstopper and\nprovide a detailed description of a nonlinear control law that regulates the\nreference input to Followerstopper within the physics-informed boundaries. We\nalso provide a new discussion on the nominal control law that regulates the\nreference speed to Followerstopper to avoid unrealistic and unsafe\nacceleration."}
{"id": "2506.08471", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.08471", "abs": "https://arxiv.org/abs/2506.08471", "authors": ["Tal I. Sommer", "Ori Katz"], "title": "Passive acoustic non-line-of-sight localization without a relay surface", "comment": null, "summary": "The detection and localization of a source hidden outside the Line-of-Sight\n(LOS) traditionally rely on the acquisition of indirect signals, such as those\nreflected from visible relay surfaces such as floors or walls. These reflected\nsignals are then utilized to reconstruct the obscured scene. In this study, we\npresent an approach that utilize signals diffracted from an edge of an obstacle\nto achieve three-dimensional (3D) localization of an acoustic point source\nsituated outside the LOS. We address two scenarios - a doorway and a convex\ncorner - and propose a localization method for each of them. For the first\nscenario, we utilize the two edges of the door as virtual detector arrays. For\nthe second scenario, we exploit the spectral signature of a knife-edge\ndiffraction, inspired by the human perception of sound location by the\nhead-related transfer function (HRTF). In both methods, knife-edge diffraction\nis utilized to extend the capabilities of non-line-of-sight (NLOS) acoustic\nsensing, enabling localization in environments where conventional relay-surface\nbased approaches may be limited."}
{"id": "2506.08805", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08805", "abs": "https://arxiv.org/abs/2506.08805", "authors": ["Stina Klein", "Pooja Prajod", "Katharina Weitz", "Matteo Lavit Nicora", "Dimitra Tsovaltzi", "Elisabeth Andr√©"], "title": "Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration", "comment": "Accepted LBW at CHIWORK 2025", "summary": "The integration of collaborative robots (cobots) in industrial settings\nraises concerns about worker well-being, particularly due to reduced social\ninteractions. Avatars - designed to facilitate worker interactions and\nengagement - are promising solutions to enhance the human-robot collaboration\n(HRC) experience. However, real-world perspectives on avatar-supported HRC\nremain unexplored. To address this gap, we conducted a focus group study with\nemployees from a German manufacturing company that uses cobots. Before the\ndiscussion, participants engaged with a scripted, industry-like HRC demo in a\nlab setting. This qualitative approach provided valuable insights into the\navatar's potential roles, improvements to its behavior, and practical\nconsiderations for deploying them in industrial workcells. Our findings also\nemphasize the importance of personalized communication and task assistance.\nAlthough our study's limitations restrict its generalizability, it serves as an\ninitial step in recognizing the potential of adaptive, context-aware avatar\ninteractions in real-world industrial environments."}
{"id": "2506.09023", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Ne≈üet √únver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Ta≈ükƒ±n Padƒ±r"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08189", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08189", "abs": "https://arxiv.org/abs/2506.08189", "authors": ["Amartya Dutta", "Kazi Sajeed Mehrab", "Medha Sawhney", "Abhilash Neog", "Mridul Khurana", "Sepideh Fatemi", "Aanish Pradhan", "M. Maruf", "Ismini Lourentzou", "Arka Daw", "Anuj Karpatne"], "title": "Open World Scene Graph Generation using Vision Language Models", "comment": "Accepted in CVPR 2025 Workshop (CVinW)", "summary": "Scene-Graph Generation (SGG) seeks to recognize objects in an image and\ndistill their salient pairwise relationships. Most methods depend on\ndataset-specific supervision to learn the variety of interactions, restricting\ntheir usefulness in open-world settings, involving novel objects and/or\nrelations. Even methods that leverage large Vision Language Models (VLMs)\ntypically require benchmark-specific fine-tuning. We introduce Open-World SGG,\na training-free, efficient, model-agnostic framework that taps directly into\nthe pretrained knowledge of VLMs to produce scene graphs with zero additional\nlearning. Casting SGG as a zero-shot structured-reasoning problem, our method\ncombines multimodal prompting, embedding alignment, and a lightweight\npair-refinement strategy, enabling inference over unseen object vocabularies\nand relation sets. To assess this setting, we formalize an Open-World\nevaluation protocol that measures performance when no SGG-specific data have\nbeen observed either in terms of objects and relations. Experiments on Visual\nGenome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate\nthe capacity of pretrained VLMs to perform relational understanding without\ntask-level training."}
{"id": "2506.08051", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08051", "abs": "https://arxiv.org/abs/2506.08051", "authors": ["Mahmuda Sultana Mimi", "Md Monzurul Islam", "Anannya Ghosh Tusti", "Shriyank Somvanshi", "Subasish Das"], "title": "ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity", "comment": null, "summary": "Understanding the spatial and temporal dynamics of automated vehicle (AV)\ncrash severity is critical for advancing urban mobility safety and\ninfrastructure planning. In this work, we introduce ST-GraphNet, a\nspatio-temporal graph neural network framework designed to model and predict AV\ncrash severity by using both fine-grained and region-aggregated spatial graphs.\nUsing a balanced dataset of 2,352 real-world AV-related crash reports from\nTexas (2024), including geospatial coordinates, crash timestamps, SAE\nautomation levels, and narrative descriptions, we construct two complementary\ngraph representations: (1) a fine-grained graph with individual crash events as\nnodes, where edges are defined via spatio-temporal proximity; and (2) a\ncoarse-grained graph where crashes are aggregated into Hexagonal Hierarchical\nSpatial Indexing (H3)-based spatial cells, connected through hexagonal\nadjacency. Each node in the graph is enriched with multimodal data, including\nsemantic, spatial, and temporal attributes, including textual embeddings from\ncrash narratives using a pretrained Sentence-BERT model. We evaluate various\ngraph neural network (GNN) architectures, such as Graph Convolutional Networks\n(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN\n(DSTGCN), to classify crash severity and predict high-risk regions. Our\nproposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3\ngraph, achieves a test accuracy of 97.74\\%, substantially outperforming the\nbest fine-grained model (64.7\\% test accuracy). These findings highlight the\neffectiveness of spatial aggregation, dynamic message passing, and multi-modal\nfeature integration in capturing the complex spatio-temporal patterns\nunderlying AV crash severity."}
{"id": "2506.08038", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08038", "abs": "https://arxiv.org/abs/2506.08038", "authors": ["Chen Huang", "Dingxuan Wang", "Ronghui Hou"], "title": "Joint Routing and Control Optimization in VANET", "comment": "11 pages; 10 figures", "summary": "In this paper, we introduce DynaRoute, an adaptive joint optimization\nframework for dynamic vehicular networks that simultaneously addresses platoon\ncontrol and data transmission through trajectory-aware routing and\nsafety-constrained vehicle coordination. DynaRoute guarantees continuous\nvehicle movement via platoon safety control with optimizing transmission paths\nthrough real-time trajectory prediction and ensuring reliable data. Our\nsolution achieves three key objectives: (1) maintaining platoon stability\nthrough accurate data transmission, (2) enabling adaptive routing based on\nvehicle movement patterns, and (3) enhancing overall intelligent transportation\nsystem performance. DynaRoute equires predefined traffic models and adapts to\ndynamic network conditions using local vehicle state information. We present\ncomprehensive simulation results demonstrating that DynaRoute maintains control\nand transmission performance in multiple complex scenarios while significantly\nimproving throughput and reliability compared to traditional approaches."}
{"id": "2506.08524", "categories": ["cs.SD", "cs.AI", "cs.MM", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08524", "abs": "https://arxiv.org/abs/2506.08524", "authors": ["Weiguo Wang", "Andy Nie", "Wenrui Zhou", "Yi Kai", "Chengchen Hu"], "title": "Teaching Physical Awareness to LLMs through Sounds", "comment": "ICML 2025", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in text and\nmultimodal processing, yet they fundamentally lack physical\nawareness--understanding of real-world physical phenomena. In this work, we\npresent ACORN, a framework that teaches LLMs physical awareness through sound,\nfocusing on fundamental physical phenomena like the Doppler effect, multipath\neffect, and spatial relationships. To overcome data scarcity, ACORN introduce a\nphysics-based simulator combining real-world sound sources with controlled\nphysical channels to generate diverse training data. Using this simulator, we\nbuild AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an\naudio encoder that processes both magnitude and phase information. By\nconnecting our audio encoder to state-of-the-art LLMs, we demonstrate\nreasonable results in both simulated and real-world tasks, such as\nline-of-sight detection, Doppler effect estimation, and Direction-of-Arrival\nestimation, paving the way for enabling LLMs to understand physical world."}
{"id": "2506.08881", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08881", "abs": "https://arxiv.org/abs/2506.08881", "authors": ["Nicolas Grelier", "Johannes Pfau", "Nicolas Mathieu", "St√©phane Kaufmann"], "title": "From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags", "comment": null, "summary": "The video game industry deals with a fast-paced, competitive and almost\nunpredictable market. Trends of genres, settings and modalities change on a\nperpetual basis, studios are often one big hit or miss away from surviving or\nperishing, and hitting the pulse of the time has become one of the greatest\nchallenges for industrials, investors and other stakeholders. In this work, we\naim to support the understanding of video game trends over time based on\ndata-driven analysis, visualization and interpretation of Steam tag evolutions.\nWe confirm underlying groundwork that trends can be categorized in short-lived\nfads, contemporary fashions, or stable classics, and derived that the surge of\na trend averages at about four years in the realm of video games. After using\nindustrial experts to validate our findings, we deliver visualizations,\ninsights and an open approach of deciphering shifts in video game trends."}
{"id": "2506.08416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08416", "abs": "https://arxiv.org/abs/2506.08416", "authors": ["Bolin Li", "Linwei Sun", "Xuecong Huang", "Yuzhi Jiang", "Lijun Zhu"], "title": "Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots", "comment": null, "summary": "This paper presents a periodic bipedal gait learning method using reward\ncomposition, integrated with a real-time gait planner for humanoid robots.\nFirst, we introduce a novel gait planner that incorporates dynamics to design\nthe desired joint trajectory. In the gait design process, the 3D robot model is\ndecoupled into two 2D models, which are then approximated as hybrid inverted\npendulums (H-LIP) for trajectory planning. The gait planner operates in\nparallel in real time within the robot's learning environment. Second, based on\nthis gait planner, we design three effective reward functions within a\nreinforcement learning framework, forming a reward composition to achieve\nperiodic bipedal gait. This reward composition reduces the robot's learning\ntime and enhances locomotion performance. Finally, a gait design example and\nperformance comparison are presented to demonstrate the effectiveness of the\nproposed method."}
{"id": "2506.08191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08191", "abs": "https://arxiv.org/abs/2506.08191", "authors": ["Antoni Nowinowski", "Krzysztof Krawiec"], "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes", "comment": null, "summary": "This study builds on the architecture of the Disentangler of Visual Priors\n(DVP), a type of autoencoder that learns to interpret scenes by decomposing the\nperceived objects into independent visual aspects of shape, size, orientation,\nand color appearance. These aspects are expressed as latent parameters which\ncontrol a differentiable renderer that performs image reconstruction, so that\nthe model can be trained end-to-end with gradient using reconstruction loss. In\nthis study, we extend the original DVP so that it can handle multiple objects\nin a scene. We also exploit the interpretability of its latent by using the\ndecoder to sample additional training examples and devising alternative\ntraining modes that rely on loss functions defined not only in the image space,\nbut also in the latent space. This significantly facilitates training, which is\notherwise challenging due to the presence of extensive plateaus in the\nimage-space reconstruction loss. To examine the performance of this approach,\nwe propose a new benchmark featuring multiple 2D objects, which subsumes the\npreviously proposed Multi-dSprites dataset while being more parameterizable. We\ncompare the DVP extended in these ways with two baselines (MONet and LIVE) and\ndemonstrate its superiority in terms of reconstruction quality and capacity to\ndecompose overlapping objects. We also analyze the gradients induced by the\nconsidered loss functions, explain how they impact the efficacy of training,\nand discuss the limitations of differentiable rendering in autoencoders and the\nways in which they can be addressed."}
{"id": "2506.08054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08054", "abs": "https://arxiv.org/abs/2506.08054", "authors": ["Yiming Wang", "Hao Peng", "Senzhang Wang", "Haohua Du", "Chunyang Liu", "Jia Wu", "Guanlin Wu"], "title": "STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation", "comment": "10 pages, 5 figures, 3 tables. Extended version of paper accepted at\n  IJCAI 2025", "summary": "Traffic data imputation is fundamentally important to support various\napplications in intelligent transportation systems such as traffic flow\nprediction. However, existing time-to-space sequential methods often fail to\neffectively extract features in block-wise missing data scenarios. Meanwhile,\nthe static graph structure for spatial feature propagation significantly\nconstrains the models flexibility in handling the distribution shift issue for\nthe nonstationary traffic data. To address these issues, this paper proposes a\nSpatioTemporal Attention Mixture of experts network named STAMImputer for\ntraffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)\nframework to capture latent spatio-temporal features and their influence\nweights, effectively imputing block missing. A novel Low-rank guided Sampling\nGraph ATtention (LrSGAT) mechanism is designed to dynamically balance the local\nand global correlations across road networks. The sampled attention vectors are\nutilized to generate dynamic graphs that capture real-time spatial\ncorrelations. Extensive experiments are conducted on four traffic datasets for\nevaluation. The result shows STAMImputer achieves significantly performance\nimprovement compared with existing SOTA approaches. Our codes are available at\nhttps://github.com/RingBDStack/STAMImupter."}
{"id": "2506.08042", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08042", "abs": "https://arxiv.org/abs/2506.08042", "authors": ["Mohammad Mirtaba", "Ankit Goel"], "title": "Continuous-Time Output Feedback Adaptive Control for Stabilization and Tracking with Experimental Results", "comment": null, "summary": "This paper presents a continuous-time output feedback adaptive control\ntechnique for stabilization and tracking control problems. The adaptive\ncontroller is motivated by the classical discrete-time retrospective cost\nadaptive control algorithm. The particle swarm optimization framework automates\nthe adaptive algorithm's hyper-parameter tuning. The proposed controller is\nnumerically validated in the tracking problems of a double integrator and a\nbicopter system and is experimentally validated in an attitude stabilization\nproblem. Numerical and experimental results show that the proposed controller\nis an effective technique for model-free output feedback control."}
{"id": "2506.08540", "categories": ["cs.SD", "eess.AS", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2506.08540", "abs": "https://arxiv.org/abs/2506.08540", "authors": ["Dima Mrad", "Sara Najem"], "title": "Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas and Partitas: Topological and Geometrical Explorations", "comment": null, "summary": "Music is inherently complex, with structures and interactions that unfold\nacross multiple layers. Complex networks have emerged as powerful structures\nfor the quantitative analysis of Western classical music, revealing significant\nfeatures of its harmonic and structural organization. Although notable works\nhave used these approaches to study music, dyadic representations of\ninteractions fall short in conveying the underlying complexity and depth. In\nrecent years, the limitations of traditional graph representations have been\nquestioned and challenged in the context of interactions that could be\nhigher-dimensional. Effective musical analysis requires models that capture\nhigher-order interactions and a framework that simultaneously captures\ntransitions between them. Subsequently, in this paper, we present a topological\nframework for analyzing J. S. Bach's Solo Violin Sonatas and Partitas that uses\nhigher-order networks where single notes are vertices, two-note chords are\nedges, three-notes are triangles, etc. We subsequently account for the flow of\nmusic, by modeling transitions between successive notes. We identify\ngenre-specific patterns in the works' geometric and topological properties. In\nparticular, we find signatures in the trends of the evolution of the Euler\ncharacteristic and curvature, as well as examining adherence to the\nGauss-Bonnet theorem across different movement types. The distinctions are\nrevealed between slow movements, Fugues, and Baroque dance movements through\ntheir simplicial complex representation."}
{"id": "2506.08892", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08892", "abs": "https://arxiv.org/abs/2506.08892", "authors": ["Tauhid Tanjim", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "The human-robot interaction (HRI) field has recognized the importance of\nenabling robots to interact with teams. Human teams rely on effective\ncommunication for successful collaboration in time-sensitive environments.\nRobots can play a role in enhancing team coordination through real-time\nassistance. Despite significant progress in human-robot teaming research, there\nremains an essential gap in how robots can effectively communicate with action\nteams using multimodal interaction cues in time-sensitive environments. This\nstudy addresses this knowledge gap in an experimental in-lab study to\ninvestigate how multimodal robot communication in action teams affects workload\nand human perception of robots. We explore team collaboration in a medical\ntraining scenario where a robotic crash cart (RCC) provides verbal and\nnon-verbal cues to help users remember to perform iterative tasks and search\nfor supplies. Our findings show that verbal cues for object search tasks and\nvisual cues for task reminders reduce team workload and increase perceived ease\nof use and perceived usefulness more effectively than a robot with no feedback.\nOur work contributes to multimodal interaction research in the HRI field,\nhighlighting the need for more human-robot teaming research to understand best\npractices for integrating collaborative robots in time-sensitive environments\nsuch as in hospitals, search and rescue, and manufacturing applications."}
{"id": "2506.08434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08434", "abs": "https://arxiv.org/abs/2506.08434", "authors": ["Rui Zhao", "Xingjian Zhang", "Yuhong Cao", "Yizhuo Wang", "Guillaume Sartoretti"], "title": "Attention-based Learning for 3D Informative Path Planning", "comment": null, "summary": "In this work, we propose an attention-based deep reinforcement learning\napproach to address the adaptive informative path planning (IPP) problem in 3D\nspace, where an aerial robot equipped with a downward-facing sensor must\ndynamically adjust its 3D position to balance sensing footprint and accuracy,\nand finally obtain a high-quality belief of an underlying field of interest\nover a given domain (e.g., presence of specific plants, hazardous gas,\ngeological structures, etc.). In adaptive IPP tasks, the agent is tasked with\nmaximizing information collected under time/distance constraints, continuously\nadapting its path based on newly acquired sensor data. To this end, we leverage\nattention mechanisms for their strong ability to capture global spatial\ndependencies across large action spaces, allowing the agent to learn an\nimplicit estimation of environmental transitions. Our model builds a contextual\nbelief representation over the entire domain, guiding sequential movement\ndecisions that optimize both short- and long-term search objectives.\nComparative evaluations against state-of-the-art planners demonstrate that our\napproach significantly reduces environmental uncertainty within constrained\nbudgets, thus allowing the agent to effectively balance exploration and\nexploitation. We further show our model generalizes well to environments of\nvarying sizes, highlighting its potential for many real-world applications."}
{"id": "2506.08194", "categories": ["cs.CV", "68T45", "I.5.4; I.2.10; I.3.5"], "pdf": "https://arxiv.org/pdf/2506.08194", "abs": "https://arxiv.org/abs/2506.08194", "authors": ["Mateusz Michalkiewicz", "Anekha Sokhal", "Tadeusz Michalkiewicz", "Piotr Pawlikowski", "Mahsa Baktashmotlagh", "Varun Jampani", "Guha Balakrishnan"], "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "comment": "15 pages, 4 figures", "summary": "Monocular 3D reconstruction methods and vision-language models (VLMs)\ndemonstrate impressive results on standard benchmarks, yet their true\nunderstanding of geometric properties remains unclear. We introduce GIQ , a\ncomprehensive benchmark specifically designed to evaluate the geometric\nreasoning capabilities of vision and vision-language foundation models. GIQ\ncomprises synthetic and real-world images of 224 diverse polyhedra - including\nPlatonic, Archimedean, Johnson, and Catalan solids, as well as stellations and\ncompound shapes - covering varying levels of complexity and symmetry. Through\nsystematic experiments involving monocular 3D reconstruction, 3D symmetry\ndetection, mental rotation tests, and zero-shot shape classification tasks, we\nreveal significant shortcomings in current models. State-of-the-art\nreconstruction algorithms trained on extensive 3D datasets struggle to\nreconstruct even basic geometric forms accurately. While foundation models\neffectively detect specific 3D symmetry elements via linear probing, they\nfalter significantly in tasks requiring detailed geometric differentiation,\nsuch as mental rotation. Moreover, advanced vision-language assistants exhibit\nremarkably low accuracy on complex polyhedra, systematically misinterpreting\nbasic properties like face geometry, convexity, and compound structures. GIQ is\npublicly available, providing a structured platform to highlight and address\ncritical gaps in geometric intelligence, facilitating future progress in\nrobust, geometry-aware representation learning."}
{"id": "2506.08060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08060", "abs": "https://arxiv.org/abs/2506.08060", "authors": ["Asankhaya Sharma"], "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques", "comment": null, "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log\n\\frac{m}{\\delta} \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l\n\\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ suffice to approximate\nfine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$\nis the vocabulary size and $\\delta$ is the failure probability. For linear\nclassification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon}\n\\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log\n\\frac{1}{\\delta} \\right)$ are sufficient, where $d$ is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications."}
{"id": "2506.08211", "categories": ["eess.SY", "cs.SY", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.08211", "abs": "https://arxiv.org/abs/2506.08211", "authors": ["Romeo Ortega", "Jose Guadalupe Romero", "Stanislav Aranovskiy", "Gang Tao"], "title": "Standard LSParameter Estimators Ensure Finite Convergence Time for Linear Regression Equations Under an Interval Excitation Assumption", "comment": null, "summary": "In this brief note we recall the little-known fact that, for linear\nregression equations (LRE) with intervally excited (IE) regressors, standard\nLeast Square (LS) parameter estimators ensure finite convergence time (FCT) of\nthe estimated parameters. The convergence time being equal to the time length\nneeded to comply with the IE assumption. As is well-known, IE is necessary and\nsufficient for the identifiability of the LRE-hence, it is the weakest\nassumption for the on-or off-line solution of the parameter estimation problem."}
{"id": "2506.08570", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08570", "abs": "https://arxiv.org/abs/2506.08570", "authors": ["Or Tal", "Felix Kreuk", "Yossi Adi"], "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation", "comment": null, "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM"}
{"id": "2506.08911", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08911", "abs": "https://arxiv.org/abs/2506.08911", "authors": ["Petar Jaku≈°", "Hrvoje D≈æapo"], "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "comment": "4 pages", "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms."}
{"id": "2506.08440", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO"}
{"id": "2506.08210", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills."}
{"id": "2506.08062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08062", "abs": "https://arxiv.org/abs/2506.08062", "authors": ["Woosung Kim", "Jinho Lee", "Jongmin Lee", "Byung-Jun Lee"], "title": "FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning", "comment": "Multi-objective Reinforcement Learning", "summary": "Multi-objective reinforcement learning (MORL) aims to optimize policies in\nthe presence of conflicting objectives, where linear scalarization is commonly\nused to reduce vector-valued returns into scalar signals. While effective for\ncertain preferences, this approach cannot capture fairness-oriented goals such\nas Nash social welfare or max-min fairness, which require nonlinear and\nnon-additive trade-offs. Although several online algorithms have been proposed\nfor specific fairness objectives, a unified approach for optimizing nonlinear\nwelfare criteria in the offline setting-where learning must proceed from a\nfixed dataset-remains unexplored. In this work, we present FairDICE, the first\noffline MORL framework that directly optimizes nonlinear welfare objective.\nFairDICE leverages distribution correction estimation to jointly account for\nwelfare maximization and distributional regularization, enabling stable and\nsample-efficient learning without requiring explicit preference weights or\nexhaustive weight search. Across multiple offline benchmarks, FairDICE\ndemonstrates strong fairness-aware performance compared to existing baselines."}
{"id": "2506.08319", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08319", "abs": "https://arxiv.org/abs/2506.08319", "authors": ["Ao Jin", "Qinyi Wang", "Sijie Wen", "Ya Liu", "Ganghui Shen", "Panfeng Huang", "Fan Zhang"], "title": "DEKC: Data-Enable Control for Tethered Space Robot Deployment in the Presence of Uncertainty via Koopman Operator Theory", "comment": "12 pages", "summary": "This work focuses the deployment of tethered space robot in the presence of\nunknown uncertainty. A data-enable framework called DEKC which contains offline\ntraining part and online execution part is proposed to deploy tethered space\nrobot in the presence of uncertainty. The main idea of this work is modeling\nthe unknown uncertainty as a dynamical system, which enables high accuracy and\nconvergence of capturing uncertainty. The core part of proposed framework is a\nproxy model of uncertainty, which is derived from data-driven Koopman theory\nand is separated with controller design. In the offline stage, the lifting\nfunctions associated with Koopman operator are parameterized with deep neural\nnetworks. Then by solving an optimization problem, the lifting functions are\nlearned from sampling data. In the online execution stage, the proxy model\ncooperates the learned lifting functions obtained in the offline phase to\ncapture the unknown uncertainty. Then the output of proxy model is compensated\nto the baseline controller such that the effect of uncertainty can be\nattenuated or even eliminated. Furthermore, considering some scenarios in which\nthe performance of proxy model may weaken, a receding-horizon scheme is\nproposed to update the proxy model online. Finally, the extensive numerical\nsimulations demonstrate the effectiveness of our proposed framework. The\nimplementation of proposed DEKC framework is publicly available at\nhttps://github.com/NPU-RCIR/DEKC.git."}
{"id": "2506.08967", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08967", "abs": "https://arxiv.org/abs/2506.08967", "authors": ["Ailin Huang", "Bingxin Li", "Bruce Wang", "Boyong Wu", "Chao Yan", "Chengli Feng", "Heng Wang", "Hongyu Zhou", "Hongyuan Wang", "Jingbei Li", "Jianjian Sun", "Joanna Wang", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Shilei Jiang", "Tian Fei", "Wang You", "Xi Chen", "Xuerui Yang", "Yechang Huang", "Yuxiang Zhang", "Zheng Ge", "Zheng Gong", "Zhewei Huang", "Zixin Zhang", "Bin Wang", "Bo Li", "Buyun Ma", "Changxin Miao", "Changyi Wan", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Hanpeng Hu", "Haonan Jia", "Jiahao Gong", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Junzhe Lin", "Kaixiang Li", "Lei Xia", "Longlong Gu", "Ming Li", "Nie Hao", "Ranchen Ming", "Shaoliang Pang", "Siqi Liu", "Song Yuan", "Tiancheng Cao", "Wen Li", "Wenqing He", "Xu Zhao", "Xuelin Zhang", "Yanbo Yu", "Yinmin Zhong", "Yu Zhou", "Yuanwei Liang", "Yuanwei Lu", "Yuxiang Yang", "Zidong Yang", "Zili Zhang", "Binxing Jiao", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Xinhao Zhang", "Yibo Zhu", "Daxin Jiang", "Shuchang Zhou", "Chen Hu"], "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model", "comment": "12 pages, 3 figures", "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks."}
{"id": "2506.08555", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08555", "abs": "https://arxiv.org/abs/2506.08555", "authors": ["Xinyue Niu", "Akira Furui"], "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement", "comment": "6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025", "summary": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems."}
{"id": "2506.08459", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections."}
{"id": "2506.08214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08214", "abs": "https://arxiv.org/abs/2506.08214", "authors": ["Ioannis Iakovidis", "Zahra Kalantari", "Amir Hossein Payberah", "Fernando Jaramillo", "Francisco Pena Escobar"], "title": "Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation", "comment": "16 pages, 9 figures", "summary": "In recent years the wide availability of high-resolution radar satellite\nimages along with the advancement of computer vision models have enabled the\nremote monitoring of the surface area of wetlands. However, these models\nrequire large amounts of manually annotated satellite images, which are slow\nand expensive to produce. To overcome this problem, self-supervised training\nmethods have been deployed to train models without using annotated data. In\nthis paper we use a combination of deep clustering and negative sampling to\ntrain a model to segment radar satellite images into areas that separate water\nfrom land without the use of any manual annotations. Furthermore, we implement\nan ensemble version of the model to reduce variance and improve performance.\nCompared to a single fully-supervised model using the same architecture, our\nensemble of self-supervised models achieves a 0.02 improvement in the\nIntersection Over Union metric over our test dataset."}
{"id": "2506.08063", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08063", "abs": "https://arxiv.org/abs/2506.08063", "authors": ["Songqiao Hu", "Zeyi Liu", "Xiao He"], "title": "Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift", "comment": "6 pages, 4 figures, accepted by the 2025 CAA Symposium on Fault\n  Detection, Supervision and Safety for Technical Processes (SAFEPROCESS 2025)", "summary": "The change in data distribution over time, also known as concept drift, poses\na significant challenge to the reliability of online learning methods. Existing\nmethods typically require model retraining or drift detection, both of which\ndemand high computational costs and are often unsuitable for real-time\napplications. To address these limitations, a lightweight, fast and efficient\nrandom vector functional-link network termed Lite-RVFL is proposed, capable of\nadapting to concept drift without drift detection and retraining. Lite-RVFL\nintroduces a novel objective function that assigns weights exponentially\nincreasing to new samples, thereby emphasizing recent data and enabling timely\nadaptation. Theoretical analysis confirms the feasibility of this objective\nfunction for drift adaptation, and an efficient incremental update rule is\nderived. Experimental results on a real-world safety assessment task validate\nthe efficiency, effectiveness in adapting to drift, and potential to capture\ntemporal patterns of Lite-RVFL. The source code is available at\nhttps://github.com/songqiaohu/Lite-RVFL."}
{"id": "2506.08366", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08366", "abs": "https://arxiv.org/abs/2506.08366", "authors": ["Renjie Ma", "Su Zhang", "Wenjie Liu", "Zhijian Hu", "Peng Shi"], "title": "Learning event-triggered controllers for linear parameter-varying systems from data", "comment": "13 pages, 5 figures", "summary": "Nonlinear dynamical behaviours in engineering applications can be\napproximated by linear-parameter varying (LPV) representations, but obtaining\nprecise model knowledge to develop a control algorithm is difficult in\npractice. In this paper, we develop the data-driven control strategies for\nevent-triggered LPV systems with stability verifications. First, we provide the\ntheoretical analysis of ${\\theta}$-persistence of excitation for LPV systems,\nwhich leads to the feasible data-based representations. Then, in terms of the\navailable perturbed data, we derive the stability certificates for\nevent-triggered LPV systems with the aid of Petersen's lemma in the sense of\nrobust control, resulting in the computationally tractable semidefinite\nprogrammings, the feasible solutions of which yields the optimal gain\nschedulings. Besides, we generalize the data-driven eventtriggered LPV control\nmethods to the scenario of reference trajectory tracking, and discuss the\nrobust tracking stability accordingly. Finally, we verify the effectiveness of\nour theoretical derivations by numerical simulations."}
{"id": "2506.08911", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08911", "abs": "https://arxiv.org/abs/2506.08911", "authors": ["Petar Jaku≈°", "Hrvoje D≈æapo"], "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "comment": "4 pages", "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms."}
{"id": "2506.08890", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08578", "abs": "https://arxiv.org/abs/2506.08578", "authors": ["Boyang Chen", "Xizhe Zang", "Chao Song", "Yue Zhang", "Xuehe Zhang", "Jie Zhao"], "title": "Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot", "comment": null, "summary": "The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design\ninspired by the rollover shape of the human foot, significantly enhances the\nenergy efficiency of the robot walking gait. However, due to the tilt of the\nsupporting leg, the error of the contact model are amplified, making robot\nstate estimation more challenging. Therefore, this paper focuses on the noise\nanalysis and state estimation for robot walking with the ESVC foot. First,\nthrough physical robot experiments, we investigate the effect of the ESVC foot\non robot measurement noise and process noise. and a noise-time regression model\nusing sliding window strategy is developed. Then, a hierarchical adaptive state\nestimator for biped robots with the ESVC foot is proposed. The state estimator\nconsists of two stages: pre-estimation and post-estimation. In the\npre-estimation stage, a data fusion-based estimation is employed to process the\nsensory data. During post-estimation, the acceleration of center of mass is\nfirst estimated, and then the noise covariance matrices are adjusted based on\nthe regression model. Following that, an EKF(Extended Kalman Filter) based\napproach is applied to estimate the centroid state during robot walking.\nPhysical experiments demonstrate that the proposed adaptive state estimator for\nbiped robot walking with the ESVC foot not only provides higher precision than\nboth EKF and Adaptive EKF, but also converges faster under varying noise\nconditions."}
{"id": "2506.08220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08220", "abs": "https://arxiv.org/abs/2506.08220", "authors": ["Octave Mariotti", "Zhipeng Du", "Yash Bhalgat", "Oisin Mac Aodha", "Hakan Bilen"], "title": "Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence", "comment": null, "summary": "Semantic correspondence (SC) aims to establish semantically meaningful\nmatches across different instances of an object category. We illustrate how\nrecent supervised SC methods remain limited in their ability to generalize\nbeyond sparsely annotated training keypoints, effectively acting as keypoint\ndetectors. To address this, we propose a novel approach for learning dense\ncorrespondences by lifting 2D keypoints into a canonical 3D space using\nmonocular depth estimation. Our method constructs a continuous canonical\nmanifold that captures object geometry without requiring explicit 3D\nsupervision or camera annotations. Additionally, we introduce SPair-U, an\nextension of SPair-71k with novel keypoint annotations, to better assess\ngeneralization. Experiments not only demonstrate that our model significantly\noutperforms supervised baselines on unseen keypoints, highlighting its\neffectiveness in learning robust correspondences, but that unsupervised\nbaselines outperform supervised counterparts when generalized across different\ndatasets."}
{"id": "2506.08070", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08070", "abs": "https://arxiv.org/abs/2506.08070", "authors": ["Ziheng Qin", "Hailun Xu", "Wei Chee Yew", "Qi Jia", "Yang Luo", "Kanchan Sarkar", "Danhui Guan", "Kai Wang", "Yang You"], "title": "Info-Coevolution: An Efficient Framework for Data Model Coevolution", "comment": "V1", "summary": "Machine learning relies heavily on data, yet the continuous growth of\nreal-world data poses challenges for efficient dataset construction and\ntraining. A fundamental yet unsolved question is: given our current model and\ndata, does a new data (sample/batch) need annotation/learning? Conventional\napproaches retain all available data, leading to non-optimal data and training\nefficiency. Active learning aims to reduce data redundancy by selecting a\nsubset of samples to annotate, while it increases pipeline complexity and\nintroduces bias. In this work, we propose Info-Coevolution, a novel framework\nthat efficiently enables models and data to coevolve through online selective\nannotation with no bias. Leveraging task-specific models (and open-source\nmodels), it selectively annotates and integrates online and web data to improve\ndatasets efficiently. For real-world datasets like ImageNet-1K,\nInfo-Coevolution reduces annotation and training costs by 32\\% without\nperformance loss. It is able to automatically give the saving ratio without\ntuning the ratio. It can further reduce the annotation ratio to 50\\% with\nsemi-supervised learning. We also explore retrieval-based dataset enhancement\nusing unlabeled open-source data. Code is available at\nhttps://github.com/NUS-HPC-AI-Lab/Info-Coevolution/."}
{"id": "2506.08404", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08404", "abs": "https://arxiv.org/abs/2506.08404", "authors": ["Yanpei Shi", "Jingxuan Zhang", "Zhuo Shi", "Chenyao Zhang", "Yuze Guo", "Rui Feng"], "title": "Compact Amplified Laser Power Stabilization Using Robust Active Disturbance Rejection Control with Sensor Noise Decoupling", "comment": null, "summary": "Laser power instability, encompassing random jitter and slow drift, severely\nlimits the performance of optically pumped magnetometers (OPMs) in detecting\nultra-weak magnetic fields, especially in large-scale OPM arrays for\nmagnetoencephalography. Although a unified amplified laser (AL) architecture\nimproves integration, fluctuations in the pump beam progressively degrade\nperformance across all channels, exacerbated by environmental disturbances and\nsystem uncertainties. To address this challenge, this paper presents a compact\nAL power stabilization approach based on an innovative dual-loop active\ndisturbance rejection control (DLADRC) strategy, while integrating a\ncomprehensive quantitative stability analysis through novel exponential decay\nestimates for extended state observers (ESOs) and control error dynamics. As\nvalidated through physical experimental results, the proposed method\nsignificantly improves AL's long-term stability with sensor noise decoupling,\nachieving an over 85.7% reduction in 1-hour power instability and a tenfold\ndecrease in Allan variance for correlation times 10^2 s--10^3 s, compared to\nstandard ADRC. Crucially, the strategy demonstrates robust effectiveness across\ndiverse operating scenarios, enabling AL-based OPM systems to achieve their\nfull potential in high-sensitivity biomagnetic field detection."}
{"id": "2506.08639", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08639", "abs": "https://arxiv.org/abs/2506.08639", "authors": ["Amir Hossein Barjini", "Seyed Adel Alizadeh Kolagar", "Sadeq Yaqubi", "Jouni Mattila"], "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators", "comment": null, "summary": "This article presents a motion planning and control framework for flexible\nrobotic manipulators, integrating deep reinforcement learning (DRL) with a\nnonlinear partial differential equation (PDE) controller. Unlike conventional\napproaches that focus solely on control, we demonstrate that the desired\ntrajectory significantly influences endpoint vibrations. To address this, a DRL\nmotion planner, trained using the soft actor-critic (SAC) algorithm, generates\noptimized trajectories that inherently minimize vibrations. The PDE nonlinear\ncontroller then computes the required torques to track the planned trajectory\nwhile ensuring closed-loop stability using Lyapunov analysis. The proposed\nmethodology is validated through both simulations and real-world experiments,\ndemonstrating superior vibration suppression and tracking accuracy compared to\ntraditional methods. The results underscore the potential of combining\nlearning-based motion planning with model-based control for enhancing the\nprecision and stability of flexible robotic manipulators."}
{"id": "2506.08227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08227", "abs": "https://arxiv.org/abs/2506.08227", "authors": ["Vishaal Udandarao", "Mehdi Cherti", "Shyamgopal Karthik", "Jenia Jitsev", "Samuel Albanie", "Matthias Bethge"], "title": "A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks", "comment": null, "summary": "We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for\nmeasuring compositional understanding capabilities of vision-language models\n(VLMs). We scrutinize design choices in their construction, including data\nsource (e.g. MS-COCO) and curation procedures (e.g. constructing negative\nimages/captions), uncovering several inherent biases across most benchmarks. We\nfind that blind heuristics (e.g. token-length, log-likelihood under a language\nmodel) perform on par with CLIP models, indicating that these benchmarks do not\neffectively measure compositional understanding. We demonstrate that the\nunderlying factor is a distribution asymmetry between positive and negative\nimages/captions, induced by the benchmark construction procedures. To mitigate\nthese issues, we provide a few key recommendations for constructing more robust\nvision-language compositional understanding benchmarks, that would be less\nprone to such simple attacks."}
{"id": "2506.08113", "categories": ["cs.LG", "cs.AI", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2506.08113", "abs": "https://arxiv.org/abs/2506.08113", "authors": ["Timoth√©e Hornek Amir Sartipi", "Igor Tchappi", "Gilbert Fridgen"], "title": "Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting", "comment": null, "summary": "Accurate electricity price forecasting (EPF) is crucial for effective\ndecision-making in power trading on the spot market. While recent advances in\ngenerative artificial intelligence (GenAI) and pre-trained large language\nmodels (LLMs) have inspired the development of numerous time series foundation\nmodels (TSFMs) for time series forecasting, their effectiveness in EPF remains\nuncertain. To address this gap, we benchmark several state-of-the-art\npretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and\nTimeGPT--against established statistical and machine learning (ML) methods for\nEPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,\nFrance, the Netherlands, Austria, and Belgium, we generate daily forecasts with\na one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the\nTSFMs, performing on par with traditional models. However, the biseasonal MSTL\nmodel, which captures daily and weekly seasonality, stands out for its\nconsistent performance across countries and evaluation metrics, with no TSFM\nstatistically outperforming it."}
{"id": "2506.08414", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08414", "abs": "https://arxiv.org/abs/2506.08414", "authors": ["Nurullah Sevim", "Mostafa Ibrahim", "Sabit Ekin", "Theodore S. Rappaport"], "title": "Theoretical Foundations of Waste Factor and Waste Figure with Applications to Fixed Wireless Access and Relay Systems", "comment": null, "summary": "The exponential rise in energy consumption across wireless communication\nsystems, particularly in anticipation of next-generation wireless systems,\nnecessitates rigorous frameworks for evaluating and optimizing energy\nefficiency. This paper revisits and expands the concept of the Waste Factor\n(W), or Waste Figure (WF) in decibel scale, as a unifying metric that captures\nboth utilized and wasted power in cascaded communication systems. Building upon\nits foundation in system-level power modeling, we integrate the Waste Factor\ninto a refined formulation of the Consumption Factor (CF), the ratio of data\nrate to total consumed power, linking it directly to Shannon's theoretical\nlimit on energy per bit. This analysis introduces additive energy waste into\nthe classical energy-per-bit derivation through the Waste Factor term.\n  We derive closed-form expressions for energy-per-bit expenditure in both\ndirect and relay-assisted links and develop a decision rule to determine which\ncommunication path is more energy efficient under given conditions. While not\nmodeled explicitly, Reflective Intelligent Surfaces (RIS) can be interpreted as\na special case of relay-based architectures within this unified formulation,\nsuggesting broader applicability of the Waste Factor framework to emerging 6G\nuse cases. The framework is then extended to a Fixed Wireless Access (FWA)\nscenario, where uplink and downlink asymmetries, traffic directionality, and\ncomponent inefficiencies are jointly considered to analyze energy-optimal\ndeployment strategies."}
{"id": "2506.08706", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.08706", "abs": "https://arxiv.org/abs/2506.08706", "authors": ["Tomasz Winiarski", "Jan Kaniuka", "Daniel Gie≈Çdowski", "Jakub Ostrysz", "Krystian Radlak", "Dmytro Kushnir"], "title": "ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel", "comment": "19 pages", "summary": "As robotic systems grow increasingly complex, heterogeneous, and\nsafety-critical, the need for structured development methodologies becomes\nparamount. Although frameworks like the Robot Operating System (ROS) and\nModel-Based Systems Engineering (MBSE) offer foundational tools, they often\nlack integration when used together. This paper addresses that gap by aligning\nthe widely recognized V-model development paradigm with the MeROS metamodel\nSysML-based modeling language tailored for ROS-based systems.\n  We propose a domain-specific methodology that bridges ROS-centric modelling\nwith systems engineering practices. Our approach formalises the structure,\nbehaviour, and validation processes of robotic systems using MeROS, while\nextending it with a generalized, adaptable V-model compatible with both ROS and\nROS 2. Rather than prescribing a fixed procedure, the approach supports\nproject-specific flexibility and reuse, offering guidance across all stages of\ndevelopment.\n  The approach is validated through a comprehensive case study on HeROS, a\nheterogeneous multi-robot platform comprising manipulators, mobile units, and\ndynamic test environments. This example illustrates how the MeROS-compatible\nV-model enhances traceability and system consistency while remaining accessible\nand extensible for future adaptation. The work contributes a structured,\ntool-agnostic foundation for developers and researchers seeking to apply MBSE\npractices in ROS-based projects."}
{"id": "2506.08257", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08257", "abs": "https://arxiv.org/abs/2506.08257", "authors": ["L. Lao Beyer", "T. Li", "X. Chen", "S. Karaman", "K. He"], "title": "Highly Compressed Tokenizer Can Generate Without Training", "comment": "Main manuscript: 9 pages, 7 figures. Appendix: 8 pages, 9 figures. To\n  appear in the Proceedings of the 42nd International Conference on Machine\n  Learning", "summary": "Commonly used image tokenizers produce a 2D grid of spatially arranged\ntokens. In contrast, so-called 1D image tokenizers represent images as highly\ncompressed one-dimensional sequences of as few as 32 discrete tokens. We find\nthat the high degree of compression achieved by a 1D tokenizer with vector\nquantization enables image editing and generative capabilities through\nheuristic manipulation of tokens, demonstrating that even very crude\nmanipulations -- such as copying and replacing tokens between latent\nrepresentations of images -- enable fine-grained image editing by transferring\nappearance and semantic attributes. Motivated by the expressivity of the 1D\ntokenizer's latent space, we construct an image generation pipeline leveraging\ngradient-based test-time optimization of tokens with plug-and-play loss\nfunctions such as reconstruction or CLIP similarity. Our approach is\ndemonstrated for inpainting and text-guided image editing use cases, and can\ngenerate diverse and realistic samples without requiring training of any\ngenerative model."}
{"id": "2506.08125", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08125", "abs": "https://arxiv.org/abs/2506.08125", "authors": ["Hanbing Liu", "Lang Cao", "Yuanyi Ren", "Mengyu Zhou", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning", "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities,\nyet they often suffer from inefficiencies due to unnecessarily verbose or\nredundant outputs. While many works have explored reinforcement learning (RL)\nto enhance reasoning abilities, most primarily focus on improving accuracy,\nwith limited attention to reasoning efficiency. Some existing approaches\nintroduce direct length-based rewards to encourage brevity, but this often\nleads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL\nframework that advances length-based reward design to boost efficient\nreasoning. Bingo incorporates two key mechanisms: a significance-aware length\nreward, which gradually guides the model to reduce only insignificant tokens,\nand a dynamic length reward, which initially encourages elaborate reasoning for\nhard questions but decays over time to improve overall efficiency. Experiments\nacross multiple reasoning benchmarks show that Bingo improves both accuracy and\nefficiency. It outperforms the vanilla reward and several other length-based\nreward baselines in RL, achieving a favorable trade-off between accuracy and\nefficiency. These results underscore the potential of training LLMs explicitly\nfor efficient reasoning."}
{"id": "2506.08509", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08509", "abs": "https://arxiv.org/abs/2506.08509", "authors": ["Chaoqun Ma", "Zhiyong Zhang"], "title": "Predictive reinforcement learning based adaptive PID controller", "comment": null, "summary": "Purpose: This study aims to address the challenges of controlling unstable\nand nonlinear systems by proposing an adaptive PID controller based on\npredictive reinforcement learning (PRL-PID), where the PRL-PID combines the\nadvantages of both data-driven and model-driven approaches.\nDesign/methodology/approach: A predictive reinforcement learning framework is\nintroduced, incorporating action smooth strategy to suppress overshoot and\noscillations, and a hierarchical reward function to support training. Findings:\nExperimental results show that the PRL-PID controller achieves superior\nstability and tracking accuracy in nonlinear, unstable, and strongly coupled\nsystems, consistently outperforming existing RL-tuned PID methods while\nmaintaining excellent robustness and adaptability across diverse operating\nconditions. Originality/Value: By adopting predictive learning, the proposed\nPRL-PID integrates system model priors into data-driven control, enhancing both\nthe control framework's training efficiency and the controller's stability. As\na result, PRL-PID provides a balanced blend of model-based and data-driven\napproaches, delivering robust, high-performance control."}
{"id": "2506.08708", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08279", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08279", "abs": "https://arxiv.org/abs/2506.08279", "authors": ["Aditi Sundararaman", "Amogh Adishesha", "Andrew Jaegle", "Dan Bigioi", "Hyoung-Kyu Song", "Jon Kyl", "Justin Mao", "Kevin Lan", "Mojtaba Komeili", "ShahRukh Athar", "Sheila Babayan", "Stanislau Beliasau", "William Buchwalter"], "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage", "comment": "Technical report website: mirage.app/research/seeing-voices, product\n  website: mirage.app", "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links)."}
{"id": "2506.08139", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08139", "abs": "https://arxiv.org/abs/2506.08139", "authors": ["Aviad Susman", "Mayte Su√°rez-Fari√±as", "Joseph T Colonel"], "title": "Nearness of Neighbors Attention for Regression in Supervised Finetuning", "comment": null, "summary": "It is common in supervised machine learning to combine the feature extraction\ncapabilities of neural networks with the predictive power of traditional\nalgorithms, such as k-nearest neighbors (k-NN) or support vector machines. This\nprocedure involves performing supervised fine-tuning (SFT) on a\ndomain-appropriate feature extractor, followed by training a traditional\npredictor on the resulting SFT embeddings. When used in this manner,\ntraditional predictors often deliver increased performance over the SFT model\nitself, despite the fine-tuned feature extractor yielding embeddings\nspecifically optimized for prediction by the neural network's final dense\nlayer. This suggests that directly incorporating traditional algorithms into\nSFT as prediction layers may further improve performance. However, many\ntraditional algorithms have not been implemented as neural network layers due\nto their non-differentiable nature and their unique optimization requirements.\nAs a step towards solving this problem, we introduce the Nearness of Neighbors\nAttention (NONA) regression layer. NONA uses the mechanics of neural network\nattention and a novel learned attention-masking scheme to yield a\ndifferentiable proxy of the k-NN regression algorithm. Results on multiple\nunstructured datasets show improved performance over both dense layer\nprediction and k-NN on SFT embeddings for regression."}
{"id": "2506.08530", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08530", "abs": "https://arxiv.org/abs/2506.08530", "authors": ["Tao Li", "Yi Li", "Lulin Zhang", "Jiuxiang Dong"], "title": "The Invariant Zonotopic Set-Membership Filter for State Estimation on Groups", "comment": null, "summary": "The invariant filtering theory based on the group theory has been successful\nin statistical filtering methods. However, there exists a class of state\nestimation problems with unknown statistical properties of noise disturbances,\nand it is worth discussing whether the invariant observer still has performance\nadvantages. In this paper, considering the problem of state estimation with\nunknown but bounded noise disturbances, an Invariant Zonotopic Set-Membership\nFilter (InZSMF) method on groups is innovatively proposed, which extends the\ninvariant filtering theory to the field of non-statistical filtering\nrepresented by set-membership filtering. Firstly, the InZSMF method transforms\nthe state space from the traditional Euclidean vector space to the Lie group\nspace to construct group affine discrete systems with unknown but bounded noise\nuncertainty defined by the zonotope on groups. Secondly, the nonlinear observer\non the group is defined and the corresponding linearized estimation error is\nderived. Then, two observer gain tuning algorithms under the InZSMF method are\nproposed, respectively, the pole configuration method and the F-radius\noptimization method. Finally, through simulation experiments, it is shown that\nthe InZSMF state estimation method is generally superior to the traditional\nZonotopic Set-Membership Filter (ZSMF) state estimation method. Especially,\nwhen the initial estimations are imprecise, the convergence speed of state\nestimation, the accuracy of set-membership center estimation, and the average\ninterval area of zonotopic estimation of the InZSMF method are significantly\nbetter than those of the ZSMF method."}
{"id": "2506.08756", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08756", "abs": "https://arxiv.org/abs/2506.08756", "authors": ["Octavio Arriaga", "Rebecca Adam", "Melvin Laux", "Lisa Gutzeit", "Marco Ragni", "Jan Peters", "Frank Kirchner"], "title": "Bayesian Inverse Physics for Neuro-Symbolic Robot Learning", "comment": null, "summary": "Real-world robotic applications, from autonomous exploration to assistive\ntechnologies, require adaptive, interpretable, and data-efficient learning\nparadigms. While deep learning architectures and foundation models have driven\nsignificant advances in diverse robotic applications, they remain limited in\ntheir ability to operate efficiently and reliably in unknown and dynamic\nenvironments. In this position paper, we critically assess these limitations\nand introduce a conceptual framework for combining data-driven learning with\ndeliberate, structured reasoning. Specifically, we propose leveraging\ndifferentiable physics for efficient world modeling, Bayesian inference for\nuncertainty-aware decision-making, and meta-learning for rapid adaptation to\nnew tasks. By embedding physical symbolic reasoning within neural models,\nrobots could generalize beyond their training data, reason about novel\nsituations, and continuously expand their knowledge. We argue that such hybrid\nneuro-symbolic architectures are essential for the next generation of\nautonomous systems, and to this end, we provide a research roadmap to guide and\naccelerate their development."}
{"id": "2506.08297", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08297", "abs": "https://arxiv.org/abs/2506.08297", "authors": ["Nhat Thanh Tran", "Fanghui Xue", "Shuai Zhang", "Jiancheng Lyu", "Yunling Zheng", "Yingyong Qi", "Jack Xin"], "title": "SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging", "comment": "15 pages, figures 3", "summary": "Attention is the critical component of a transformer. Yet the quadratic\ncomputational complexity of vanilla full attention in the input size and the\ninability of its linear attention variant to focus have been challenges for\ncomputer vision tasks. We provide a mathematical definition of generalized\nattention and formulate both vanilla softmax attention and linear attention\nwithin the general framework. We prove that generalized attention disperses,\nthat is, as the number of keys tends to infinity, the query assigns equal\nweights to all keys. Motivated by the dispersion property and recent\ndevelopment of Mamba form of attention, we design Scalable and Efficient Mamba\nlike Attention (SEMA) which utilizes token localization to avoid dispersion and\nmaintain focusing, complemented by theoretically consistent arithmetic\naveraging to capture global aspect of attention. We support our approach on\nImagenet-1k where classification results show that SEMA is a scalable and\neffective alternative beyond linear attention, outperforming recent vision\nMamba models on increasingly larger scales of images at similar model parameter\nsizes."}
{"id": "2506.08140", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08140", "abs": "https://arxiv.org/abs/2506.08140", "authors": ["Yifei Li", "Hanane Nour Moussa", "Ziru Chen", "Shijie Chen", "Botao Yu", "Mingyi Xue", "Benjamin Burns", "Tzu-Yao Chiu", "Vishal Dey", "Zitong Lu", "Chen Wei", "Qianheng Zhang", "Tianyu Zhang", "Song Gao", "Xuhui Huang", "Xia Ning", "Nesreen K. Ahmed", "Ali Payani", "Huan Sun"], "title": "AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists", "comment": null, "summary": "Despite long-standing efforts in accelerating scientific discovery with AI,\nbuilding AI co-scientists remains challenging due to limited high-quality data\nfor training and evaluation. To tackle this data scarcity issue, we present\nAutoSDT, an automatic pipeline that collects high-quality coding tasks in\nreal-world data-driven discovery workflows. AutoSDT leverages the coding\ncapabilities and parametric knowledge of LLMs to search for diverse sources,\nselect ecologically valid tasks, and synthesize accurate task instructions and\ncode solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404\ncoding tasks for data-driven discovery that covers four scientific disciplines\nand 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the\nonly automatically collected and the largest open dataset for data-driven\nscientific discovery. Expert feedback on a subset of 256 tasks shows the\neffectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,\nand 92.2% of the synthesized programs are functionally correct. Trained on\nAutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show\nsubstantial improvement on two challenging data-driven discovery benchmarks,\nScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches\nthe same level of performance as GPT-4o on ScienceAgentBench with a success\nrate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it\nlifts the hypothesis matching score to 8.1, bringing a 17.4% relative\nimprovement and closing the gap between open-weight models and GPT-4o."}
{"id": "2506.08579", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08579", "abs": "https://arxiv.org/abs/2506.08579", "authors": ["Guiyang Luo", "Jinglin Li", "Qixun Zhang", "Zhiyong Feng", "Quan Yuan", "Yijing Lin", "Hui Zhang", "Nan Cheng", "Ping Zhang"], "title": "Toward Low-Altitude Airspace Management and UAV Operations: Requirements, Architecture and Enabling Technologies", "comment": null, "summary": "The low-altitude economy (LAE) is rapidly advancing toward intelligence,\nconnectivity, and coordination, bringing new challenges in dynamic airspace\nmanagement, unmanned aerial vehicle (UAV) operation, and security management.\nExisting systems remain fragmented and lack effective coordination. To bridge\nthese gaps, we propose UTICN (Ubiquitous and Trusted Intelligent\nCellular-native Network) for LAE, a unified cellular-native architecture that\nintegrates multi-domain sensing, high-precision positioning, intelligent\naircraft-to-everything communication, dynamic airspace management, and UAV\noperational services. UTICN introduces key technologies such as integrated\nsensing and communication (ISAC), passive and active positioning, intelligent\nmachine communication, swarm coordination, and control-data decoupled\nmanagement frameworks. We demonstrate UTICN's feasibility through two use\ncases, i.e., a city-level LAE management platform and a multi-frequency\ncollaborative ISAC system. This work provides a fundamental reference for\nbuilding a unified operational foundation and airspace management architecture\nfor the LAE."}
{"id": "2506.08795", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08795", "abs": "https://arxiv.org/abs/2506.08795", "authors": ["Kaijie Shi", "Wanglong Lu", "Hanli Zhao", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning", "comment": null, "summary": "Limb loss affects millions globally, impairing physical function and reducing\nquality of life. Most traditional surface electromyographic (sEMG) and\nsemi-autonomous methods require users to generate myoelectric signals for each\ncontrol, imposing physically and mentally taxing demands. This study aims to\ndevelop a fully autonomous control system that enables a prosthetic hand to\nautomatically grasp and release objects of various shapes using only a camera\nattached to the wrist. By placing the hand near an object, the system will\nautomatically execute grasping actions with a proper grip force in response to\nthe hand's movements and the environment. To release the object being grasped,\njust naturally place the object close to the table and the system will\nautomatically open the hand. Such a system would provide individuals with limb\nloss with a very easy-to-use prosthetic control interface and greatly reduce\nmental effort while using. To achieve this goal, we developed a teleoperation\nsystem to collect human demonstration data for training the prosthetic hand\ncontrol model using imitation learning, which mimics the prosthetic hand\nactions from human. Through training the model using only a few objects' data\nfrom one single participant, we have shown that the imitation learning\nalgorithm can achieve high success rates, generalizing to more individuals and\nunseen objects with a variation of weights. The demonstrations are available at\n\\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}"}
{"id": "2506.08299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08299", "abs": "https://arxiv.org/abs/2506.08299", "authors": ["Kangning Yang", "Ling Ouyang", "Huiming Sun", "Jie Cai", "Lan Fu", "Jiaming Ding", "Chiu Man Ho", "Zibo Meng"], "title": "OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal", "comment": null, "summary": "Reflection removal technology plays a crucial role in photography and\ncomputer vision applications. However, existing techniques are hindered by the\nlack of high-quality in-the-wild datasets. In this paper, we propose a novel\nparadigm for collecting reflection datasets from a fresh perspective. Our\napproach is convenient, cost-effective, and scalable, while ensuring that the\ncollected data pairs are of high quality, perfectly aligned, and represent\nnatural and diverse scenarios. Following this paradigm, we collect a\nReal-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which\ncontains 1,000 high-quality transmission-reflection image pairs collected in\nthe wild. Through the analysis of several reflection removal methods and\nbenchmark evaluation experiments on our dataset, we demonstrate its\neffectiveness in improving robustness in challenging real-world environments.\nOur dataset is available at https://github.com/caijie0620/OpenRR-1k."}
{"id": "2506.08143", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08143", "abs": "https://arxiv.org/abs/2506.08143", "authors": ["Francesco Tonin", "Alex Lambert", "Johan A. K. Suykens", "Volkan Cevher"], "title": "Accelerating Spectral Clustering under Fairness Constraints", "comment": "ICML 2025", "summary": "Fairness of decision-making algorithms is an increasingly important issue. In\nthis paper, we focus on spectral clustering with group fairness constraints,\nwhere every demographic group is represented in each cluster proportionally as\nin the general population. We present a new efficient method for fair spectral\nclustering (Fair SC) by casting the Fair SC problem within the difference of\nconvex functions (DC) framework. To this end, we introduce a novel variable\naugmentation strategy and employ an alternating direction method of multipliers\ntype of algorithm adapted to DC problems. We show that each associated\nsubproblem can be solved efficiently, resulting in higher computational\nefficiency compared to prior work, which required a computationally expensive\neigendecomposition. Numerical experiments demonstrate the effectiveness of our\napproach on both synthetic and real-world benchmarks, showing significant\nspeedups in computation time over prior art, especially as the problem size\ngrows. This work thus represents a considerable step forward towards the\nadoption of fair clustering in real-world applications."}
{"id": "2506.08608", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08608", "abs": "https://arxiv.org/abs/2506.08608", "authors": ["Yang Lv", "Rong Hu", "Bin Qian", "Jian-Bo Yang"], "title": "Q-learning-based Hierarchical Cooperative Local Search for Steelmaking-continuous Casting Scheduling Problem", "comment": null, "summary": "The steelmaking continuous casting scheduling problem (SCCSP) is a critical\nand complex challenge in modern steel production, requiring the coordinated\nassignment and sequencing of steel charges across multiple production stages.\nEfficient scheduling not only enhances productivity but also significantly\nreduces energy consumption. However, both traditional heuristics (e.g.,\ntwo-stage local search) and recent metaheuristics often struggle to adapt to\nthe dynamic characteristics of practical SCCSP instances. To address these\nlimitations, this paper introduces a novel Q learning based hierarchical\ncooperative local search framework, termed HierC_Q, aimed at minimizing the\nweighted sum of the maximum completion time and the average waiting time in\nSCCSP. The core contributions of HierC_Q are twofold. First, considering the\nintrinsic coupling properties of the SCCSP, a dedicated reward function is\nproposed based on a novel coupling measure (CM), guiding the exploration\nprocess towards promising regions of the solution space. Second, a hierarchical\narchitecture is devised, comprising two distinct tiers: the learn to improve\n(L2I) tier and the \"disturb to renovate\" (D2R) tier. The L2I tier performs deep\nexploitation within promising regions using two independent Q-learning-based\nlocal search frameworks (QLSFs) tailored for subproblems, along with a synergy\nQLSF designed for the main problem. To enhance the effectiveness of local\nsearch, a validity evaluation approach and a speed-up evaluation method are\nalso intro-duced, grounded in a detailed study of the problem's structure.\nMeanwhile, the D2R tier incorporates a perturbation and construction based\nsolution renewal strategy to mitigate the risk of premature convergence. The\nsuperiority and effectiveness of HierC_Q are demonstrated through extensive\ncomparisons with eleven local search frameworks and nine state-of-the-art\nalgorithms."}
{"id": "2506.08822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08822", "abs": "https://arxiv.org/abs/2506.08822", "authors": ["Yifei Su", "Ning Liu", "Dong Chen", "Zhen Zhao", "Kun Wu", "Meng Li", "Zhiyuan Xu", "Zhengping Che", "Jian Tang"], "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency", "comment": null, "summary": "Generative modeling-based visuomotor policies have been widely adopted in\nrobotic manipulation attributed to their ability to model multimodal action\ndistributions. However, the high inference cost of multi-step sampling limits\ntheir applicability in real-time robotic systems. To address this issue,\nexisting approaches accelerate the sampling process in generative\nmodeling-based visuomotor policies by adapting acceleration techniques\noriginally developed for image generation. Despite this progress, a major\ndistinction remains: image generation typically involves producing independent\nsamples without temporal dependencies, whereas robotic manipulation involves\ngenerating time-series action trajectories that require continuity and temporal\ncoherence. To effectively exploit temporal information in robotic manipulation,\nwe propose FreqPolicy, a novel approach that first imposes frequency\nconsistency constraints on flow-based visuomotor policies. Our work enables the\naction model to capture temporal structure effectively while supporting\nefficient, high-quality one-step action generation. We introduce a frequency\nconsistency constraint that enforces alignment of frequency-domain action\nfeatures across different timesteps along the flow, thereby promoting\nconvergence of one-step action generation toward the target distribution. In\naddition, we design an adaptive consistency loss to capture structural temporal\nvariations inherent in robotic manipulation tasks. We assess FreqPolicy on 53\ntasks across 3 simulation benchmarks, proving its superiority over existing\none-step action generators. We further integrate FreqPolicy into the\nvision-language-action (VLA) model and achieve acceleration without performance\ndegradation on the 40 tasks of Libero. Besides, we show efficiency and\neffectiveness in real-world robotic scenarios with an inference frequency\n93.5Hz. The code will be publicly available."}
{"id": "2506.08324", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08324", "abs": "https://arxiv.org/abs/2506.08324", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating", "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15155,\n  arXiv:2504.13045, arXiv:2503.23472", "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more effectively extract\nand fuse spatial context with fine spectral information in hyperspectral image\n(HSI) classification, this paper proposes a novel network architecture called\nSTNet. The core advantage of STNet stems from the dual innovative design of its\nSpatial-Spectral Transformer module: first, the fundamental explicit decoupling\nof spatial and spectral attention ensures targeted capture of key information\nin HSI; second, two functionally distinct gating mechanisms perform intelligent\nregulation at both the fusion level of attention flows (adaptive attention\nfusion gating) and the internal level of feature transformation (GFFN). This\ncharacteristic demonstrates superior feature extraction and fusion capabilities\ncompared to traditional convolutional neural networks, while reducing\noverfitting risks in small-sample and high-noise scenarios. STNet enhances\nmodel representation capability without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches."}
{"id": "2506.08146", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.08146", "abs": "https://arxiv.org/abs/2506.08146", "authors": ["Vahidullah Ta√ß", "Amirhossein Amiri-Hezaveh", "Manuel K. Rausch", "Grace N. Bechtel", "Francisco Sahli Costabal", "Adrian Buganza Tepole"], "title": "Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields", "comment": null, "summary": "We propose a new framework for identifying mechanical properties of\nheterogeneous materials without a closed-form constitutive equation. Given a\nfull-field measurement of the displacement field, for instance as obtained from\ndigital image correlation (DIC), a continuous approximation of the strain field\nis obtained by training a neural network that incorporates Fourier features to\neffectively capture sharp gradients in the data. A physics-based data-driven\nmethod built upon ordinary neural differential equations (NODEs) is employed to\ndiscover constitutive equations. The NODE framework can represent arbitrary\nmaterials while satisfying constraints in the theory of constitutive equations\nby default. To account for heterogeneity, a hyper-network is defined, where the\ninput is the material coordinate system, and the output is the NODE-based\nconstitutive equation. The parameters of the hyper-network are optimized by\nminimizing a multi-objective loss function that includes penalty terms for\nviolations of the strong form of the equilibrium equations of elasticity and\nthe associated Neumann boundary conditions. We showcase the framework with\nseveral numerical examples, including heterogeneity arising from variations in\nmaterial parameters, spatial transitions from isotropy to anisotropy, material\nidentification in the presence of noise, and, ultimately, application to\nexperimental data. As the numerical results suggest, the proposed approach is\nrobust and general in identifying the mechanical properties of heterogeneous\nmaterials with very few assumptions, making it a suitable alternative to\nclassical inverse methods."}
{"id": "2506.08676", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08676", "abs": "https://arxiv.org/abs/2506.08676", "authors": ["Alicia Beneyto-Rodriguez", "Gregorio I. Sainz-Palmero", "Marta Galende-Hern√°ndez", "Mar√≠a J. Fuente"], "title": "Linguistic Ordered Weighted Averaging based deep learning pooling for fault diagnosis in a wastewater treatment plant", "comment": null, "summary": "Nowadays, water reuse is a serious challenge to help address water shortages.\nHere, the wastewater treatment plants (WWTP) play a key role, and its proper\noperation is mandatory. So, fault diagnosis is a key activity for these plants.\nTheir high complexity and large-scale require of smart methodologies for that\nfault diagnosis and safety operation. All these large-scale and complex\nindustrial processes are monitored, allowing the data collection about the\nplant operation, so data driven approaches for fault diagnosis can be applied.\nA popular approach to fault diagnosis is deep learning-based methodologies.\nHere, a fault diagnosis methodology is proposed for a WWTP using a new\nlinguistic Ordered Weighted Averaging (OWA) pooling based Deep Convolutional\nNeural Network (DCNN) and a sliding and overlapping time window. This window\nslides over input data based on the monitoring sampling time, then the\ndiagnosis is carried out by the linguistic OWA pooling based DCNN. This\nalternative linguistic pooling uses well-known linguistic OWA quantifiers,\nwhich permit terms such as \\textsl{Most, AtLeast, etc.}, supplying new\nintuitive options for the pooling tasks. This sliding time window and the OWA\npooling based network permit a better and earlier fault diagnosis, at each\nsampling time, using a few monitoring samples and a fewer learning iterations\nthan DCNN standard pooling. Several linguistic OWA operators have been checked\nwith a benchmark for WWTPs. A set of 5 fault types has been used, taking into\naccount 140 variables sampled at 15 minutes time intervals. The performance has\nbeen over $91\\%$ for $Accuracy$, $Recall$ or $F1-Score$, and better than other\ncompetitive methodologies. Moreover, these linguistic OWA operators for DCNN\npooling have shown a better performance than the standard \\textsl{Max} and\n\\textsl{Average} options."}
{"id": "2506.08840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08840", "abs": "https://arxiv.org/abs/2506.08840", "authors": ["Dewei Wang", "Xinmiao Wang", "Xinzhe Liu", "Jiyuan Shi", "Yingnan Zhao", "Chenjia Bai", "Xuelong Li"], "title": "MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains", "comment": "9 pages, 5 figures", "summary": "Humanoid robots have demonstrated robust locomotion capabilities using\nReinforcement Learning (RL)-based approaches. Further, to obtain human-like\nbehaviors, existing methods integrate human motion-tracking or motion prior in\nthe RL framework. However, these methods are limited in flat terrains with\nproprioception only, restricting their abilities to traverse challenging\nterrains with human-like gaits. In this work, we propose a novel framework\nusing a mixture of latent residual experts with multi-discriminators to train\nan RL policy, which is capable of traversing complex terrains in controllable\nlifelike gaits with exteroception. Our two-stage training pipeline first\nteaches the policy to traverse complex terrains using a depth camera, and then\nenables gait-commanded switching between human-like gait patterns. We also\ndesign gait rewards to adjust human-like behaviors like robot base height.\nSimulation and real-world experiments demonstrate that our framework exhibits\nexceptional performance in traversing complex terrains, and achieves seamless\ntransitions between multiple human-like gait patterns."}
{"id": "2506.08327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08327", "abs": "https://arxiv.org/abs/2506.08327", "authors": ["Yuto Kase", "Kai Ishibe", "Ryoma Yasuda", "Yudai Washida", "Sakiko Hashimoto"], "title": "Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera", "comment": "17 pages, 10 figures, 3 tables", "summary": "In racket sports, such as tennis, locating the ball's position at impact is\nimportant in clarifying player and equipment characteristics, thereby aiding in\npersonalized equipment design. High-speed cameras are used to measure the\nimpact location; however, their excessive memory consumption limits prolonged\nscene capture, and manual digitization for position detection is time-consuming\nand prone to human error. These limitations make it difficult to effectively\ncapture the entire playing scene, hindering the ability to analyze the player's\nperformance. We propose a method for locating the tennis ball impact on the\nracket in real time using an event camera. Event cameras efficiently measure\nbrightness changes (called `events') with microsecond accuracy under high-speed\nmotion while using lower memory consumption. These cameras enable users to\ncontinuously monitor their performance over extended periods. Our method\nconsists of three identification steps: time range of swing, timing at impact,\nand contours of ball and racket. Conventional computer vision techniques are\nutilized along with an original event-based processing to detect the timing at\nimpact (PATS: the amount of polarity asymmetry in time symmetry). The results\nof the experiments were within the permissible range for measuring tennis\nplayers' performance. Moreover, the computation time was sufficiently short for\nreal-time applications."}
{"id": "2506.08164", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08164", "abs": "https://arxiv.org/abs/2506.08164", "authors": ["Hadi Reisizadeh", "Jinghan Jia", "Zhiqi Bu", "Bhanukiran Vinzamuri", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Sijia Liu", "Mingyi Hong"], "title": "BLUR: A Bi-Level Optimization Approach for LLM Unlearning", "comment": null, "summary": "Enabling large language models (LLMs) to unlearn knowledge and capabilities\nacquired during training has proven vital for ensuring compliance with data\nregulations and promoting ethical practices in generative AI. Although there\nare growing interests in developing various unlearning algorithms, it remains\nunclear how to best formulate the unlearning problem. The most popular\nformulation uses a weighted sum of forget and retain loss, but it often leads\nto performance degradation due to the inherent trade-off between forget and\nretain losses. In this work, we argue that it is important to model the\nhierarchical structure of the unlearning problem, where the forget problem\n(which \\textit{unlearns} certain knowledge and/or capabilities) takes priority\nover the retain problem (which preserves model utility). This hierarchical\nstructure naturally leads to a bi-level optimization formulation where the\nlower-level objective focuses on minimizing the forget loss, while the\nupper-level objective aims to maintain the model's utility. Based on this new\nformulation, we propose a novel algorithm, termed Bi-Level UnleaRning\n(\\texttt{BLUR}), which not only possesses strong theoretical guarantees but\nmore importantly, delivers superior performance. In particular, our extensive\nexperiments demonstrate that \\texttt{BLUR} consistently outperforms all the\nstate-of-the-art algorithms across various unlearning tasks, models, and\nmetrics. Codes are available at\nhttps://github.com/OptimAI-Lab/BLURLLMUnlearning."}
{"id": "2506.08689", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08689", "abs": "https://arxiv.org/abs/2506.08689", "authors": ["Eduardo Figueiredo", "Steven Adams", "Peyman Mohajerin Esfahani", "Luca Laurenti"], "title": "Efficient Uncertainty Propagation with Guarantees in Wasserstein Distance", "comment": null, "summary": "In this paper, we consider the problem of propagating an uncertain\ndistribution by a possibly non-linear function and quantifying the resulting\nuncertainty. We measure the uncertainty using the Wasserstein distance, and for\na given input set of distributions close in the Wasserstein distance, we\ncompute a set of distributions centered at a discrete distribution that is\nguaranteed to contain the pushforward of any distribution in the input set. Our\napproach is based on approximating a nominal distribution from the input set to\na discrete support distribution for which the exact computation of the\npushforward distribution is tractable, thus guaranteeing computational\nefficiency to our approach. Then, we rely on results from semi-discrete optimal\ntransport and distributional robust optimization to show that for any $\\epsilon\n> 0$ the error introduced by our approach can be made smaller than $\\epsilon$.\nCritically, in the context of dynamical systems, we show how our results allow\none to efficiently approximate the distribution of a stochastic dynamical\nsystem with a discrete support distribution for a possibly infinite horizon\nwhile bounding the resulting approximation error. We empirically investigate\nthe effectiveness of our framework on various benchmarks, including a 10-D\nnon-linear system, showing the effectiveness of our approach in quantifying\nuncertainty in linear and non-linear stochastic systems."}
{"id": "2506.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08851", "abs": "https://arxiv.org/abs/2506.08851", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments."}
{"id": "2506.08351", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod."}
{"id": "2506.08167", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.08167", "abs": "https://arxiv.org/abs/2506.08167", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data", "comment": null, "summary": "Federated Learning (FL) often suffers from severe performance degradation\nwhen faced with non-IID data, largely due to local classifier bias. Traditional\nremedies such as global model regularization or layer freezing either incur\nhigh computational costs or struggle to adapt to feature shifts. In this work,\nwe propose UniVarFL, a novel FL framework that emulates IID-like training\ndynamics directly at the client level, eliminating the need for global model\ndependency. UniVarFL leverages two complementary regularization strategies\nduring local training: Classifier Variance Regularization, which aligns\nclass-wise probability distributions with those expected under IID conditions,\neffectively mitigating local classifier bias; and Hyperspherical Uniformity\nRegularization, which encourages a uniform distribution of feature\nrepresentations across the hypersphere, thereby enhancing the model's ability\nto generalize under diverse data distributions. Extensive experiments on\nmultiple benchmark datasets demonstrate that UniVarFL outperforms existing\nmethods in accuracy, highlighting its potential as a highly scalable and\nefficient solution for real-world FL deployments, especially in\nresource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL"}
{"id": "2506.08719", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08719", "abs": "https://arxiv.org/abs/2506.08719", "authors": ["Yongpeng Zhao", "Maik Pfefferkorn", "Maximilian Templer", "Rolf Findeisen"], "title": "Efficient Learning of Vehicle Controller Parameters via Multi-Fidelity Bayesian Optimization: From Simulation to Experiment", "comment": "8 pages, 8 figures, accepted for IEEE IV 2025", "summary": "Parameter tuning for vehicle controllers remains a costly and time-intensive\nchallenge in automotive development. Traditional approaches rely on extensive\nreal-world testing, making the process inefficient. We propose a multi-fidelity\nBayesian optimization approach that efficiently learns optimal controller\nparameters by leveraging both low-fidelity simulation data and a very limited\nnumber of real-world experiments. Our approach significantly reduces the need\nfor manual tuning and expensive field testing while maintaining the standard\ntwo-stage development workflow used in industry. The core contribution is the\nintegration of an auto-regressive multi-fidelity Gaussian process model into\nBayesian optimization, enabling knowledge transfer between different fidelity\nlevels without requiring additional low-fidelity evaluations during real-world\ntesting. We validate our approach through both simulation studies and realworld\nexperiments. The results demonstrate that our method achieves high-quality\ncontroller performance with only very few real-world experiments, highlighting\nits potential as a practical and scalable solution for intelligent vehicle\ncontrol tuning in industrial applications."}
{"id": "2506.08856", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08856", "abs": "https://arxiv.org/abs/2506.08856", "authors": ["Jonathan P. King", "Harnoor Ahluwalia", "Michael Zhang", "Nancy S. Pollard"], "title": "Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation", "comment": "Submitted to IEEE Conference on Humanoid Robots", "summary": "This work presents a fast anytime algorithm for computing globally optimal\nindependent contact regions (ICRs). ICRs are regions such that one contact\nwithin each region enables a valid grasp. Locations of ICRs can provide\nguidance for grasp and manipulation planning, learning, and policy transfer.\nHowever, ICRs for modern applications have been little explored, in part due to\nthe expense of computing them, as they have a search space exponential in the\nnumber of contacts. We present a divide and conquer algorithm based on\nincremental n-dimensional Delaunay triangulation that produces results with\nbounded suboptimality in times sufficient for real-time planning. This paper\npresents the base algorithm for grasps where contacts lie within a plane. Our\nexperiments show substantial benefits over competing grasp quality metrics and\nspeedups of 100X and more for competing approaches to computing ICRs. We\nexplore robustness of a policy guided by ICRs and outline a path to general 3D\nimplementation. Code will be released on publication to facilitate further\ndevelopment and applications."}
{"id": "2506.08356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08356", "abs": "https://arxiv.org/abs/2506.08356", "authors": ["Shivang Chopra", "Lingchao Mao", "Gabriela Sanchez-Rodriguez", "Andrew J Feola", "Jing Li", "Zsolt Kira"], "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding", "comment": null, "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems."}
{"id": "2506.08169", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.08169", "abs": "https://arxiv.org/abs/2506.08169", "authors": ["Jingqiao Tang", "Ryan Bausback", "Feng Bao", "Richard Archibald"], "title": "Federated Learning on Stochastic Neural Networks", "comment": "25 pages, 19 figures, Submitted to Journal of Machine Learning for\n  Modeling and Computing", "summary": "Federated learning is a machine learning paradigm that leverages edge\ncomputing on client devices to optimize models while maintaining user privacy\nby ensuring that local data remains on the device. However, since all data is\ncollected by clients, federated learning is susceptible to latent noise in\nlocal datasets. Factors such as limited measurement capabilities or human\nerrors may introduce inaccuracies in client data. To address this challenge, we\npropose the use of a stochastic neural network as the local model within the\nfederated learning framework. Stochastic neural networks not only facilitate\nthe estimation of the true underlying states of the data but also enable the\nquantification of latent noise. We refer to our federated learning approach,\nwhich incorporates stochastic neural networks as local models, as Federated\nstochastic neural networks. We will present numerical experiments demonstrating\nthe performance and effectiveness of our method, particularly in handling\nnon-independent and identically distributed data."}
{"id": "2506.08720", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08720", "abs": "https://arxiv.org/abs/2506.08720", "authors": ["Fr√©d√©ric Zheng", "Yassir Jedra", "Alexandre Prouti√®re"], "title": "Minimal Order Recovery through Rank-adaptive Identification", "comment": null, "summary": "This paper addresses the problem of identifying linear systems from noisy\ninput-output trajectories. We introduce Thresholded Ho-Kalman, an algorithm\nthat leverages a rank-adaptive procedure to estimate a Hankel-like matrix\nassociated with the system. This approach optimally balances the trade-off\nbetween accurately inferring key singular values and minimizing approximation\nerrors for the rest. We establish finite-sample Frobenius norm error bounds for\nthe estimated Hankel matrix. Our algorithm further recovers both the system\norder and its Markov parameters, and we provide upper bounds for the sample\ncomplexity required to identify the system order and finite-time error bounds\nfor estimating the Markov parameters. Interestingly, these bounds match those\nachieved by state-of-the-art algorithms that assume prior knowledge of the\nsystem order."}
{"id": "2506.08868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08868", "abs": "https://arxiv.org/abs/2506.08868", "authors": ["Marco Ruggia"], "title": "MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation", "comment": "12 pages, 12 figures, preprint", "summary": "MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone\nthat is fully actuated, meaning it can control its orientation independently of\nits position. MOMAV is also highly symmetrical, making its flight efficiency\nlargely unaffected by its current orientation. These characteristics are\nachieved by a novel drone design where six rotor arms align with the vertices\nof an octahedron, and where each arm can actively rotate along its long axis.\nVarious standout features of MOMAV are presented: The high flight efficiency\ncompared to arm configuration of other fully-actuated drones, the design of an\noriginal rotating arm assembly featuring slip-rings used to enable continuous\narm rotation, and a novel control allocation algorithm based on sequential\nquadratic programming (SQP) used to calculate throttle and arm-angle setpoints\nin flight. Flight tests have shown that MOMAV is able to achieve remarkably low\nmean position/orientation errors of 6.6mm, 2.1{\\deg} ({\\sigma}: 3.0mm,\n1.0{\\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\\deg} ({\\sigma}:\n8.6mm, 2.0{\\deg}) when sweeping orientation setpoints."}
{"id": "2506.08361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08361", "abs": "https://arxiv.org/abs/2506.08361", "authors": ["Yanting Mei", "Zhilu Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "title": "Image Demoir√©ing Using Dual Camera Fusion on Mobile Phones", "comment": "ICME 2025", "summary": "When shooting electronic screens, moir\\'e patterns usually appear in captured\nimages, which seriously affects the image quality. Existing image demoir\\'eing\nmethods face great challenges in removing large and heavy moir\\'e. To address\nthe issue, we propose to utilize Dual Camera fusion for Image Demoir\\'eing\n(DCID), \\ie, using the ultra-wide-angle (UW) image to assist the moir\\'e\nremoval of wide-angle (W) image. This is inspired by two motivations: (1) the\ntwo lenses are commonly equipped with modern smartphones, (2) the UW image\ngenerally can provide normal colors and textures when moir\\'e exists in the W\nimage mainly due to their different focal lengths. In particular, we propose an\nefficient DCID method, where a lightweight UW image encoder is integrated into\nan existing demoir\\'eing network and a fast two-stage image alignment manner is\npresent. Moreover, we construct a large-scale real-world dataset with diverse\nmobile phones and monitors, containing about 9,000 samples. Experiments on the\ndataset show our method performs better than state-of-the-art methods. Code and\ndataset are available at https://github.com/Mrduckk/DCID."}
{"id": "2506.08176", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.08176", "abs": "https://arxiv.org/abs/2506.08176", "authors": ["Anh V Nguyen", "Diego Klabjan"], "title": "FedGA-Tree: Federated Decision Tree using Genetic Algorithm", "comment": null, "summary": "In recent years, with rising concerns for data privacy, Federated Learning\nhas gained prominence, as it enables collaborative training without the\naggregation of raw data from participating clients. However, much of the\ncurrent focus has been on parametric gradient-based models, while nonparametric\ncounterparts such as decision tree are relatively understudied. Existing\nmethods for adapting decision trees to Federated Learning generally combine a\ngreedy tree-building algorithm with differential privacy to produce a global\nmodel for all clients. These methods are limited to classification trees and\ncategorical data due to the constraints of differential privacy. In this paper,\nwe explore an alternative approach that utilizes Genetic Algorithm to\nfacilitate the construction of personalized decision trees and accommodate\ncategorical and numerical data, thus allowing for both classification and\nregression trees. Comprehensive experiments demonstrate that our method\nsurpasses decision trees trained solely on local data and a benchmark\nalgorithm."}
{"id": "2506.08724", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08724", "abs": "https://arxiv.org/abs/2506.08724", "authors": ["Lorenzo Zapparoli", "Alfredo Oneto", "Mar√≠a Parajeles Herrera", "Blazhe Gjorgiev", "Gabriela Hug", "Giovanni Sansavini"], "title": "Future Deployment and Flexibility of Distributed Energy Resources in the Distribution Grids of Switzerland", "comment": "The dataset can be accessed here:\n  https://doi.org/10.5281/zenodo.15056134", "summary": "The decarbonization goals worldwide drive the energy transition of power\ndistribution grids, which operate under increasingly volatile conditions and\ncloser to their technical limits. In this context, localized operational data\nwith high temporal and spatial resolution is essential for their effective\nplanning and regulation. Nevertheless, information on grid-connected\ndistributed energy resources, such as electric vehicles, photovoltaic systems,\nand heat pumps, is often fragmented, inconsistent, and unavailable. This work\nintroduces a comprehensive database of distributed energy resources and\nnon-controllable loads allocated in Switzerland's medium- and low-voltage\ndistribution grid models, covering over 2 million points of connection.\nRemarkably, this data specifies the flexibility capabilities of the\ncontrollable devices, with a set of projections aligned with national forecasts\nfor 2030, 2040, and 2050. The database supports studies on flexibility\nprovision of distributed energy resources, distribution grid resilience, and\nnational energy policy, among other topics. Importantly, its modular structure\nallows users to extract national- and local-scale information across medium-\nand low-voltage systems, enabling broad applicability across locations."}
{"id": "2506.08890", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08391", "abs": "https://arxiv.org/abs/2506.08391", "authors": ["Woohyeon Park", "Woojin Kim", "Jaeik Kim", "Jaeyoung Do"], "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding", "comment": null, "summary": "Despite significant advancements in Vision-Language Models (VLMs), the\nperformance of existing VLMs remains hindered by object hallucination, a\ncritical challenge to achieving accurate visual understanding. To address this\nissue, we propose SECOND: Selective and Contrastive Decoding, a novel approach\nthat enables VLMs to effectively leverage multi-scale visual information with\nan object-centric manner, closely aligning with human visual perception. SECOND\nprogressively selects and integrates multi-scale visual information,\nfacilitating a more precise interpretation of images. By contrasting these\nvisual information iteratively, SECOND significantly reduces perceptual\nhallucinations and outperforms a wide range of benchmarks. Our theoretical\nanalysis and experiments highlight the largely unexplored potential of\nmulti-scale application in VLMs, showing that prioritizing and contrasting\nacross scales outperforms existing methods."}
{"id": "2506.08201", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.08201", "abs": "https://arxiv.org/abs/2506.08201", "authors": ["Krishna Pillutla", "Jalaj Upadhyay", "Christopher A. Choquette-Choo", "Krishnamurthy Dvijotham", "Arun Ganesh", "Monika Henzinger", "Jonathan Katz", "Ryan McKenna", "H. Brendan McMahan", "Keith Rush", "Thomas Steinke", "Abhradeep Thakurta"], "title": "Correlated Noise Mechanisms for Differentially Private Learning", "comment": "212 pages", "summary": "This monograph explores the design and analysis of correlated noise\nmechanisms for differential privacy (DP), focusing on their application to\nprivate training of AI and machine learning models via the core primitive of\nestimation of weighted prefix sums. While typical DP mechanisms inject\nindependent noise into each step of a stochastic gradient (SGD) learning\nalgorithm in order to protect the privacy of the training data, a growing body\nof recent research demonstrates that introducing (anti-)correlations in the\nnoise can significantly improve privacy-utility trade-offs by carefully\ncanceling out some of the noise added on earlier steps in subsequent steps.\nSuch correlated noise mechanisms, known variously as matrix mechanisms,\nfactorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when\napplied to learning algorithms, have also been influential in practice, with\nindustrial deployment at a global scale."}
{"id": "2506.08861", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08861", "abs": "https://arxiv.org/abs/2506.08861", "authors": ["Hiya Gada", "Rupamathi Jaddivada", "Marija Ilic"], "title": "Distributed component-level modeling and control of energy dynamics in electric power systems", "comment": "Submitted to Automatica for possible publication", "summary": "The widespread deployment of power electronic-based technologies is\ntransforming modern power systems into fast, nonlinear, and heterogeneous\nsystems. Conventional modeling and control approaches, rooted in quasi-static\nanalysis and centralized control, are inadequate for these converter-dominated\nsystems, which operate on fast timescales and involve proprietary models of\ndiverse components. This paper adopts and extends a previously introduced\nenergy space modeling framework grounded in energy conservation principles to\naddress these challenges. We generalize the notion of a port interaction\nvariable, which encodes energy exchange between interconnected, heterogeneous\ncomponents in a unified and physically intuitive manner. A multilayered\ndistributed control architecture is proposed, wherein the nonlinear physical\ndynamics of each component are lifted to a higher-level linear energy space\nthrough well-defined mappings. Distributed controllers are designed in this\nenergy space using only local states and minimal neighbor information via port\ninteraction variables. Two control designs, energy-based feedback linearizing\ncontrol (FBLC) and sliding mode control (SMC), are proven to achieve asymptotic\nconvergence to reference outputs. The approach is validated on two systems: an\ninverter-controlled RLC circuit and a synchronous generator connected to a\nload. In both cases, energy-based control improves transient response and\nreduces control effort."}
{"id": "2506.08931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08931", "abs": "https://arxiv.org/abs/2506.08931", "authors": ["Yixuan Li", "Yutang Lin", "Jieming Cui", "Tengyu Liu", "Wei Liang", "Yixin Zhu", "Siyuan Huang"], "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks", "comment": "18 pages, 13 figures", "summary": "Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks."}
{"id": "2506.08418", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.08418", "abs": "https://arxiv.org/abs/2506.08418", "authors": ["Taiqin Chen", "Zikun Zhou", "Zheng Fang", "Wenzhen Zou", "Kanjun Liu", "Ke Chen", "Yongbing Zhang", "Yaowei Wang"], "title": "RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation", "comment": null, "summary": "The radio map represents the spatial distribution of spectrum resources\nwithin a region, supporting efficient resource allocation and interference\nmitigation. However, it is difficult to construct a dense radio map as a\nlimited number of samples can be measured in practical scenarios. While\nexisting works have used deep learning to estimate dense radio maps from sparse\nsamples, they are hard to integrate with the physical characteristics of the\nradio map. To address this challenge, we cast radio map estimation as the\nsparse signal recovery problem. A physical propagation model is further\nincorporated to decompose the problem into multiple factor optimization\nsub-problems, thereby reducing recovery complexity. Inspired by the existing\ncompressive sensing methods, we propose the Radio Deep Unfolding Network\n(RadioDUN) to unfold the optimization process, achieving adaptive parameter\nadjusting and prior fitting in a learnable manner. To account for the radio\npropagation characteristics, we develop a dynamic reweighting module (DRM) to\nadaptively model the importance of each factor for the radio map. Inspired by\nthe shadowing factor in the physical propagation model, we integrate\nobstacle-related factors to express the obstacle-induced signal stochastic\ndecay. The shadowing loss is further designed to constrain the factor\nprediction and act as a supplementary supervised objective, which enhances the\nperformance of RadioDUN. Extensive experiments have been conducted to\ndemonstrate that the proposed method outperforms the state-of-the-art methods.\nOur code will be made publicly available upon publication."}
{"id": "2506.08205", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2506.08205", "abs": "https://arxiv.org/abs/2506.08205", "authors": ["Shadab Anwar Shaikh", "Kranthi Balusu", "Ayoub Soulami"], "title": "A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts", "comment": null, "summary": "Residual stresses, which remain within a component after processing, can\ndeteriorate performance. Accurately determining their full-field distributions\nis essential for optimizing the structural integrity and longevity. However,\nthe experimental effort required for full-field characterization is\nimpractical. Given these challenges, this work proposes a machine learning (ML)\nbased Residual Stress Generator (RSG) to infer full-field stresses from limited\nmeasurements. An extensive dataset was initially constructed by performing\nnumerous process simulations with a diverse parameter set. A ML model based on\nU-Net architecture was then trained to learn the underlying structure through\nsystematic hyperparameter tuning. Then, the model's ability to generate\nsimulated stresses was evaluated, and it was ultimately tested on actual\ncharacterization data to validate its effectiveness. The model's prediction of\nsimulated stresses shows that it achieved excellent predictive accuracy and\nexhibited a significant degree of generalization, indicating that it\nsuccessfully learnt the latent structure of residual stress distribution. The\nRSG's performance in predicting experimentally characterized data highlights\nthe feasibility of the proposed approach in providing a comprehensive\nunderstanding of residual stress distributions from limited measurements,\nthereby significantly reducing experimental efforts."}
{"id": "2506.08903", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08903", "abs": "https://arxiv.org/abs/2506.08903", "authors": ["Luca Vaccino", "Alana K. Lund", "Shirley J. Dyke", "Mohsen Azimi", "Ethan Vallerga"], "title": "HabSim: Architecture for modelling disruptions, propagation, detection and repair in deep space habitats", "comment": null, "summary": "Establishing long-term human settlements in deep space presents significant\nchallenges. Harsh environmental conditions, such as extreme temperature\nfluctuations, micrometeorite impacts, seismic activity, and exposure to solar\nand cosmic radiation pose obstacles to the design and operation of habitat\nsystems. Prolonged mission duration and the vast distances from Earth introduce\nfurther complications in the form of delayed communication and limited\nresources, making autonomy especially desirable.\n  Enabling simulation of the consequences of disruptions and their propagation\nthrough the various habitat subsystems is important for the development of\nautonomous and resilient space habitats. While existing simulation tools can\nassist in modeling some of these aspects, the integration of damage\npropagation, detection and repair in a computational model is rarely\nconsidered. This paper introduces and demonstrates a simulation architecture\ndesigned to model these aspects efficiently. By combining physics-based and\nphenomenological models, our approach balances computational efficiency with\nmodel fidelity. Furthermore, by coordinating subsystems operating at different\ntime scales, we achieve real-time simulation capabilities.\n  After describing the architecture, we demonstrate its application within\nHabSim, a space habitat system model developed by the NASA-funded Resilient\nExtraterrestrial Habitat Institute (RETHi). In these scenarios we consider fire\nhazard propagation within a lunar habitat to illustrate both how our\narchitecture supports the modeling of disruption propagation, detection, and\nrepair in a simulation environment and how the HabSim model can be leveraged\nfor through stochastic simulations to support resilience assessment. The\narchitecture developed herein is efficient and scalable, enabling researchers\nto gain insight into resilience, autonomy and decision-making."}
{"id": "2506.08043", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications."}
{"id": "2506.08429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08429", "abs": "https://arxiv.org/abs/2506.08429", "authors": ["Mingjie Xu", "Andrew Estornell", "Hongzheng Yang", "Yuzhi Zhao", "Zhaowei Zhu", "Qi Xuan", "Jiaheng Wei"], "title": "Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring", "comment": null, "summary": "The application of visual instruction tuning and other post-training\ntechniques has significantly enhanced the capabilities of Large Language Models\n(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with\nmore comprehensive visual language datasets. However, the effectiveness of VLMs\nis highly dependent on large-scale, high-quality datasets that ensure precise\nrecognition and accurate reasoning. Two key challenges hinder progress: (1)\nnoisy alignments between images and the corresponding text, which leads to\nmisinterpretation, and (2) ambiguous or misleading text, which obscures visual\ncontent. To address these challenges, we propose SCALE (Single modality data\nquality and Cross modality Alignment Evaluation), a novel quality-driven data\nselection pipeline for VLM instruction tuning datasets. Specifically, SCALE\nintegrates a cross-modality assessment framework that first assigns each data\nentry to its appropriate vision-language task, generates general and\ntask-specific captions (covering scenes, objects, style, etc.), and evaluates\nthe alignment, clarity, task rarity, text coherence, and image clarity of each\nentry based on the generated captions. We reveal that: (1) current unimodal\nquality assessment methods evaluate one modality while overlooking the rest,\nwhich can underestimate samples essential for specific tasks and discard the\nlower-quality instances that help build model robustness; and (2) appropriately\ngenerated image captions provide an efficient way to transfer the image-text\nmultimodal task into a unified text modality."}
{"id": "2506.08216", "categories": ["cs.LG", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.08216", "abs": "https://arxiv.org/abs/2506.08216", "authors": ["Shahaf Bassan", "Guy Amir", "Meirav Zehavi", "Guy Katz"], "title": "What makes an Ensemble (Un) Interpretable?", "comment": "To appear in ICML 2025", "summary": "Ensemble models are widely recognized in the ML community for their limited\ninterpretability. For instance, while a single decision tree is considered\ninterpretable, ensembles of trees (e.g., boosted trees) are often treated as\nblack-boxes. Despite this folklore recognition, there remains a lack of\nrigorous mathematical understanding of what particularly makes an ensemble\n(un)-interpretable, including how fundamental factors like the (1) *number*,\n(2) *size*, and (3) *type* of base models influence its interpretability. In\nthis work, we seek to bridge this gap by applying concepts from computational\ncomplexity theory to study the challenges of generating explanations for\nvarious ensemble configurations. Our analysis uncovers nuanced complexity\npatterns influenced by various factors. For example, we demonstrate that under\nstandard complexity assumptions like P$\\neq$NP, interpreting ensembles remains\nintractable even when base models are of constant size. Surprisingly, the\ncomplexity changes drastically with the number of base models: small ensembles\nof decision trees are efficiently interpretable, whereas interpreting ensembles\nwith even a constant number of linear models remains intractable. We believe\nthat our findings provide a more robust foundation for understanding the\ninterpretability of ensembles, emphasizing the benefits of examining it through\na computational complexity lens."}
{"id": "2506.08975", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08975", "abs": "https://arxiv.org/abs/2506.08975", "authors": ["Masoud Behbahani", "Alireza Fereidunian"], "title": "Quantitative Indices for Improving Metro Load Curve, Using Distributed Generation", "comment": "Preprint of the accepted paper for the EPDC 2014", "summary": "This paper promises the idea of using DG (Distributed Generation) to improve\nthe Metro load curve. Public transportation systems are often based on gasoline\nand diesel. However, with the gradual development in usage of the Metro and\nmonorail, a new load with heavy demand, inappropriate load curve and middle LF\n(Load factor) is added to the electricity grid. In addition to supply problem\nof this massive consumer, the Metro load curve is another problem, which has a\nrelatively low LF. Furthermore, Metro load peak hours coincide with the peaks\nof national grid. Improvement of the load curve is well-known in electrical\nengineering literature, which depending on the type of load curve, offers\ngeneral recommendations in three approaches; DSM (Demand Side Management), DS\n(Distributed Storage) and DG. In this paper, to achieve quantitative indices of\nimprovement for Metro load curve using DG, firstly based on the analysis of\nvolume and consumption pattern of the main loads in Metro, the typical load\ncurve has been extracted. Using this curve, the result of using DG is shown by\nquantitative parameters which represent the significant improvement in load\ncurve. These parameters can be used to calculate economic indicators such as\ninitial cost and ROI (Return of Investment)."}
{"id": "2506.08048", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08048", "abs": "https://arxiv.org/abs/2506.08048", "authors": ["Zheng Han", "Jun Zhou", "Jialun Pei", "Jing Qin", "Yingfang Fan", "Qi Dou"], "title": "Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts", "comment": null, "summary": "In augmented reality (AR)-guided surgical navigation, preoperative organ\nmodels are superimposed onto the patient's intraoperative anatomy to visualize\ncritical structures such as vessels and tumors. Accurate deformation modeling\nis essential to maintain the reliability of AR overlays by ensuring alignment\nbetween preoperative models and the dynamically changing anatomy. Although the\nfinite element method (FEM) offers physically plausible modeling, its high\ncomputational cost limits intraoperative applicability. Moreover, existing\nalgorithms often fail to handle large anatomical changes, such as those induced\nby pneumoperitoneum or ligament dissection, leading to inaccurate anatomical\ncorrespondences and compromised AR guidance. To address these challenges, we\npropose a data-driven biomechanics algorithm that preserves FEM-level accuracy\nwhile improving computational efficiency. In addition, we introduce a novel\nhuman-in-the-loop mechanism into the deformation modeling process. This enables\nsurgeons to interactively provide prompts to correct anatomical misalignments,\nthereby incorporating clinical expertise and allowing the model to adapt\ndynamically to complex surgical scenarios. Experiments on a publicly available\ndataset demonstrate that our algorithm achieves a mean target registration\nerror of 3.42 mm. Incorporating surgeon prompts through the interactive\nframework further reduces the error to 2.78 mm, surpassing state-of-the-art\nmethods in volumetric accuracy. These results highlight the ability of our\nframework to deliver efficient and accurate deformation modeling while\nenhancing surgeon-algorithm collaboration, paving the way for safer and more\nreliable computer-assisted surgeries."}
{"id": "2506.08456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08456", "abs": "https://arxiv.org/abs/2506.08456", "authors": ["June Suk Choi", "Kyungmin Lee", "Sihyun Yu", "Yisol Choi", "Jinwoo Shin", "Kimin Lee"], "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance", "comment": "Preprint. Under review. Project page available at\n  http://choi403.github.io/ALG", "summary": "Recent text-to-video (T2V) models have demonstrated strong capabilities in\nproducing high-quality, dynamic videos. To improve the visual controllability,\nrecent works have considered fine-tuning pre-trained T2V models to support\nimage-to-video (I2V) generation. However, such adaptation frequently suppresses\nmotion dynamics of generated outputs, resulting in more static videos compared\nto their T2V counterparts. In this work, we analyze this phenomenon and\nidentify that it stems from the premature exposure to high-frequency details in\nthe input image, which biases the sampling process toward a shortcut trajectory\nthat overfits to the static appearance of the reference image. To address this,\nwe propose adaptive low-pass guidance (ALG), a simple fix to the I2V model\nsampling procedure to generate more dynamic videos without compromising\nper-frame image quality. Specifically, ALG adaptively modulates the frequency\ncontent of the conditioning image by applying low-pass filtering at the early\nstage of denoising. Extensive experiments demonstrate that ALG significantly\nimproves the temporal dynamics of generated videos, while preserving image\nfidelity and text alignment. Especially, under VBench-I2V test suite, ALG\nachieves an average improvement of 36% in dynamic degree without a significant\ndrop in video quality or image fidelity."}
{"id": "2506.08226", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08226", "abs": "https://arxiv.org/abs/2506.08226", "authors": ["Arthur Feeney", "Kuei-Hsiang Huang", "Aparna Chandramowlishwaran"], "title": "Mondrian: Transformer Operators via Domain Decomposition", "comment": "26 pages, 7 figures", "summary": "Operator learning enables data-driven modeling of partial differential\nequations (PDEs) by learning mappings between function spaces. However, scaling\ntransformer-based operator models to high-resolution, multiscale domains\nremains a challenge due to the quadratic cost of attention and its coupling to\ndiscretization. We introduce \\textbf{Mondrian}, transformer operators that\ndecompose a domain into non-overlapping subdomains and apply attention over\nsequences of subdomain-restricted functions. Leveraging principles from domain\ndecomposition, Mondrian decouples attention from discretization. Within each\nsubdomain, it replaces standard layers with expressive neural operators, and\nattention across subdomains is computed via softmax-based inner products over\nfunctions. The formulation naturally extends to hierarchical windowed and\nneighborhood attention, supporting both local and global interactions. Mondrian\nachieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating\nresolution scaling without retraining. These results highlight the promise of\ndomain-decomposed attention for scalable and general-purpose neural operators."}
{"id": "2506.08983", "categories": ["eess.SY", "cs.SY", "math.OA"], "pdf": "https://arxiv.org/pdf/2506.08983", "abs": "https://arxiv.org/abs/2506.08983", "authors": ["Yue Wu", "Jianfu Cao", "Ye Cao"], "title": "Online Learning Control Strategies for Industrial Processes with Application for Loosening and Conditioning", "comment": "19pages,6figures", "summary": "This paper proposes a novel adaptive Koopman Model Predictive Control (MPC)\nframework, termed HPC-AK-MPC, designed to address the dual challenges of\ntime-varying dynamics and safe operation in complex industrial processes. The\nframework integrates two core strategies: online learning and\nhistorically-informed safety constraints. To contend with process\ntime-variance, a Recursive Extended Dynamic Mode Decomposition (rEDMDc)\ntechnique is employed to construct an adaptive Koopman model capable of\nupdating its parameters from real-time data, endowing the controller with the\nability to continuously learn and track dynamic changes. To tackle the critical\nissue of safe operation under model uncertainty, we introduce a novel\nHistorical Process Constraint (HPC) mechanism. This mechanism mines successful\noperational experiences from a historical database and, by coupling them with\nthe confidence level of the online model, generates a dynamic \"safety corridor\"\nfor the MPC optimization problem. This approach transforms implicit expert\nknowledge into explicit, adaptive constraints, establishing a dynamic balance\nbetween pursuing optimal performance and ensuring robust safety. The proposed\nHPC-AK-MPC method is applied to a real-world tobacco loosening and conditioning\nprocess and systematically validated using an \"advisor mode\" simulation\nframework with industrial data. Experimental results demonstrate that, compared\nto historical operations, the proposed method significantly improves the\nProcess Capability Index (Cpk) for key quality variables across all tested\nbatches, proving its substantial potential in enhancing control performance\nwhile guaranteeing operational safety."}
{"id": "2506.08052", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08052", "abs": "https://arxiv.org/abs/2506.08052", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "comment": null, "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS."}
{"id": "2506.08470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08470", "abs": "https://arxiv.org/abs/2506.08470", "authors": ["Siyuan Shen", "Ziheng Wang", "Xingyue Peng", "Suan Xia", "Ruiqian Li", "Shiying Li", "Jingyi Yu"], "title": "MARMOT: Masked Autoencoder for Modeling Transient Imaging", "comment": null, "summary": "Pretrained models have demonstrated impressive success in many modalities\nsuch as language and vision. Recent works facilitate the pretraining paradigm\nin imaging research. Transients are a novel modality, which are captured for an\nobject as photon counts versus arrival times using a precisely time-resolved\nsensor. In particular for non-line-of-sight (NLOS) scenarios, transients of\nhidden objects are measured beyond the sensor's direct line of sight. Using\nNLOS transients, the majority of previous works optimize volume density or\nsurfaces to reconstruct the hidden objects and do not transfer priors learned\nfrom datasets. In this work, we present a masked autoencoder for modeling\ntransient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a\nself-supervised model pretrianed on massive and diverse NLOS transient\ndatasets. Using a Transformer-based encoder-decoder, MARMOT learns features\nfrom partially masked transients via a scanning pattern mask (SPM), where the\nunmasked subset is functionally equivalent to arbitrary sampling, and predicts\nfull measurements. Pretrained on TransVerse-a synthesized transient dataset of\n500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature\ntransfer or decoder finetuning. Comprehensive experiments are carried out in\ncomparisons with state-of-the-art methods. Quantitative and qualitative results\ndemonstrate the efficiency of our MARMOT."}
{"id": "2506.08228", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08228", "abs": "https://arxiv.org/abs/2506.08228", "authors": ["Mustafa Baniodeh", "Kratarth Goel", "Scott Ettinger", "Carlos Fuertes", "Ari Seff", "Tim Shen", "Cole Gulino", "Chenjie Yang", "Ghassen Jerfel", "Dokook Choe", "Rui Wang", "Vinutha Kallem", "Sergio Casas", "Rami Al-Rfou", "Benjamin Sapp", "Dragomir Anguelov"], "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report", "comment": null, "summary": "We study the empirical scaling laws of a family of encoder-decoder\nautoregressive transformer models on the task of joint motion forecasting and\nplanning in the autonomous driving domain. Using a 500 thousand hours driving\ndataset, we demonstrate that, similar to language modeling, model performance\nimproves as a power-law function of the total compute budget, and we observe a\nstrong correlation between model training loss and model evaluation metrics.\nMost interestingly, closed-loop metrics also improve with scaling, which has\nimportant implications for the suitability of open-loop metrics for model\ndevelopment and hill climbing. We also study the optimal scaling of the number\nof transformer parameters and the training data size for a training\ncompute-optimal model. We find that as the training compute budget grows,\noptimal scaling requires increasing the model size 1.5x as fast as the dataset\nsize. We also study inference-time compute scaling, where we observe that\nsampling and clustering the output of smaller models makes them competitive\nwith larger models, up to a crossover point beyond which a larger models\nbecomes more inference-compute efficient. Overall, our experimental results\ndemonstrate that optimizing the training and inference-time scaling properties\nof motion forecasting and planning models is a key lever for improving their\nperformance to address a wide variety of driving scenarios. Finally, we briefly\nstudy the utility of training on general logged driving data of other agents to\nimprove the performance of the ego-agent, an important research area to address\nthe scarcity of robotics data for large capacity models training."}
{"id": "2506.08061", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08061", "abs": "https://arxiv.org/abs/2506.08061", "authors": ["Ali Abedi", "Fernando Cladera", "Mohsen Farajijalal", "Reza Ehsani"], "title": "Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards", "comment": "5 pages, 3 figures, Accepted to the Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots IEEE ICRA Workshop - 2025", "summary": "We present a real-time system for per-tree canopy volume estimation using\nmobile LiDAR data collected during routine robotic navigation. Unlike prior\napproaches that rely on static scans or assume uniform orchard structures, our\nmethod adapts to varying field geometries via an integrated pipeline of\nLiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.\nWe evaluate the system across two commercial orchards, one pistachio orchard\nwith regular spacing and one almond orchard with dense, overlapping crowns. A\nhybrid clustering strategy combining DBSCAN and spectral clustering enables\nrobust per-tree segmentation, achieving 93% success in pistachio and 80% in\nalmond, with strong agreement to drone derived canopy volume estimates. This\nwork advances scalable, non-intrusive tree monitoring for structurally diverse\norchard environments."}
{"id": "2506.08228", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08228", "abs": "https://arxiv.org/abs/2506.08228", "authors": ["Mustafa Baniodeh", "Kratarth Goel", "Scott Ettinger", "Carlos Fuertes", "Ari Seff", "Tim Shen", "Cole Gulino", "Chenjie Yang", "Ghassen Jerfel", "Dokook Choe", "Rui Wang", "Vinutha Kallem", "Sergio Casas", "Rami Al-Rfou", "Benjamin Sapp", "Dragomir Anguelov"], "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report", "comment": null, "summary": "We study the empirical scaling laws of a family of encoder-decoder\nautoregressive transformer models on the task of joint motion forecasting and\nplanning in the autonomous driving domain. Using a 500 thousand hours driving\ndataset, we demonstrate that, similar to language modeling, model performance\nimproves as a power-law function of the total compute budget, and we observe a\nstrong correlation between model training loss and model evaluation metrics.\nMost interestingly, closed-loop metrics also improve with scaling, which has\nimportant implications for the suitability of open-loop metrics for model\ndevelopment and hill climbing. We also study the optimal scaling of the number\nof transformer parameters and the training data size for a training\ncompute-optimal model. We find that as the training compute budget grows,\noptimal scaling requires increasing the model size 1.5x as fast as the dataset\nsize. We also study inference-time compute scaling, where we observe that\nsampling and clustering the output of smaller models makes them competitive\nwith larger models, up to a crossover point beyond which a larger models\nbecomes more inference-compute efficient. Overall, our experimental results\ndemonstrate that optimizing the training and inference-time scaling properties\nof motion forecasting and planning models is a key lever for improving their\nperformance to address a wide variety of driving scenarios. Finally, we briefly\nstudy the utility of training on general logged driving data of other agents to\nimprove the performance of the ego-agent, an important research area to address\nthe scarcity of robotics data for large capacity models training."}
{"id": "2506.08493", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.08493", "abs": "https://arxiv.org/abs/2506.08493", "authors": ["Qilin Yin", "Wei Lu", "Xiangyang Luo", "Xiaochun Cao"], "title": "Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization", "comment": null, "summary": "Most research efforts in the multimedia forensics domain have focused on\ndetecting forgery audio-visual content and reached sound achievements. However,\nthese works only consider deepfake detection as a classification task and\nignore the case where partial segments of the video are tampered with. Temporal\nforgery localization (TFL) of small fake audio-visual clips embedded in real\nvideos is still challenging and more in line with realistic application\nscenarios. To resolve this issue, we propose a universal context-aware\ncontrastive learning framework (UniCaCLF) for TFL. Our approach leverages\nsupervised contrastive learning to discover and identify forged instants by\nmeans of anomaly detection, allowing for the precise localization of temporal\nforged segments. To this end, we propose a novel context-aware perception layer\nthat utilizes a heterogeneous activation operation and an adaptive context\nupdater to construct a context-aware contrastive objective, which enhances the\ndiscriminability of forged instant features by contrasting them with genuine\ninstant features in terms of their distances to the global context. An\nefficient context-aware contrastive coding is introduced to further push the\nlimit of instant feature distinguishability between genuine and forged instants\nin a supervised sample-by-sample manner, suppressing the cross-sample influence\nto improve temporal forgery localization performance. Extensive experimental\nresults over five public datasets demonstrate that our proposed UniCaCLF\nsignificantly outperforms the state-of-the-art competing algorithms."}
{"id": "2506.08231", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.08231", "abs": "https://arxiv.org/abs/2506.08231", "authors": ["Melissa Estevez", "Nisha Singh", "Lauren Dyson", "Blythe Adamson", "Qianyu Yuan", "Megan W. Hildner", "Erin Fidyk", "Olive Mbah", "Farhad Khan", "Kathi Seidl-Rathkopf", "Aaron B. Cohen"], "title": "Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework", "comment": "18 pages, 3 tables, 1 figure", "summary": "Large language models (LLMs) are increasingly used to extract clinical data\nfrom electronic health records (EHRs), offering significant improvements in\nscalability and efficiency for real-world data (RWD) curation in oncology.\nHowever, the adoption of LLMs introduces new challenges in ensuring the\nreliability, accuracy, and fairness of extracted data, which are essential for\nresearch, regulatory, and clinical applications. Existing quality assurance\nframeworks for RWD and artificial intelligence do not fully address the unique\nerror modes and complexities associated with LLM-extracted data. In this paper,\nwe propose a comprehensive framework for evaluating the quality of clinical\ndata extracted by LLMs. The framework integrates variable-level performance\nbenchmarking against expert human abstraction, automated verification checks\nfor internal consistency and plausibility, and replication analyses comparing\nLLM-extracted data to human-abstracted datasets or external standards. This\nmultidimensional approach enables the identification of variables most in need\nof improvement, systematic detection of latent errors, and confirmation of\ndataset fitness-for-purpose in real-world research. Additionally, the framework\nsupports bias assessment by stratifying metrics across demographic subgroups.\nBy providing a rigorous and transparent method for assessing LLM-extracted RWD,\nthis framework advances industry standards and supports the trustworthy use of\nAI-powered evidence generation in oncology research and practice."}
{"id": "2506.08063", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08063", "abs": "https://arxiv.org/abs/2506.08063", "authors": ["Songqiao Hu", "Zeyi Liu", "Xiao He"], "title": "Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift", "comment": "6 pages, 4 figures, accepted by the 2025 CAA Symposium on Fault\n  Detection, Supervision and Safety for Technical Processes (SAFEPROCESS 2025)", "summary": "The change in data distribution over time, also known as concept drift, poses\na significant challenge to the reliability of online learning methods. Existing\nmethods typically require model retraining or drift detection, both of which\ndemand high computational costs and are often unsuitable for real-time\napplications. To address these limitations, a lightweight, fast and efficient\nrandom vector functional-link network termed Lite-RVFL is proposed, capable of\nadapting to concept drift without drift detection and retraining. Lite-RVFL\nintroduces a novel objective function that assigns weights exponentially\nincreasing to new samples, thereby emphasizing recent data and enabling timely\nadaptation. Theoretical analysis confirms the feasibility of this objective\nfunction for drift adaptation, and an efficient incremental update rule is\nderived. Experimental results on a real-world safety assessment task validate\nthe efficiency, effectiveness in adapting to drift, and potential to capture\ntemporal patterns of Lite-RVFL. The source code is available at\nhttps://github.com/songqiaohu/Lite-RVFL."}
{"id": "2506.08319", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08319", "abs": "https://arxiv.org/abs/2506.08319", "authors": ["Ao Jin", "Qinyi Wang", "Sijie Wen", "Ya Liu", "Ganghui Shen", "Panfeng Huang", "Fan Zhang"], "title": "DEKC: Data-Enable Control for Tethered Space Robot Deployment in the Presence of Uncertainty via Koopman Operator Theory", "comment": "12 pages", "summary": "This work focuses the deployment of tethered space robot in the presence of\nunknown uncertainty. A data-enable framework called DEKC which contains offline\ntraining part and online execution part is proposed to deploy tethered space\nrobot in the presence of uncertainty. The main idea of this work is modeling\nthe unknown uncertainty as a dynamical system, which enables high accuracy and\nconvergence of capturing uncertainty. The core part of proposed framework is a\nproxy model of uncertainty, which is derived from data-driven Koopman theory\nand is separated with controller design. In the offline stage, the lifting\nfunctions associated with Koopman operator are parameterized with deep neural\nnetworks. Then by solving an optimization problem, the lifting functions are\nlearned from sampling data. In the online execution stage, the proxy model\ncooperates the learned lifting functions obtained in the offline phase to\ncapture the unknown uncertainty. Then the output of proxy model is compensated\nto the baseline controller such that the effect of uncertainty can be\nattenuated or even eliminated. Furthermore, considering some scenarios in which\nthe performance of proxy model may weaken, a receding-horizon scheme is\nproposed to update the proxy model online. Finally, the extensive numerical\nsimulations demonstrate the effectiveness of our proposed framework. The\nimplementation of proposed DEKC framework is publicly available at\nhttps://github.com/NPU-RCIR/DEKC.git."}
{"id": "2506.08512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08512", "abs": "https://arxiv.org/abs/2506.08512", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Zihao Liu", "Linlin Yang"], "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (VTG), which aims to localize video clips\ncorresponding to natural language queries, is a fundamental yet challenging\ntask in video understanding. Existing Transformer-based methods often suffer\nfrom redundant attention and suboptimal multi-modal alignment. To address these\nlimitations, we propose MLVTG, a novel framework that integrates two key\nmodules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba\nblocks as a backbone instead of Transformers to model temporal dependencies and\nextract robust video representations for multi-modal alignment. LLMRefiner\nleverages the specific frozen layer of a pre-trained Large Language Model (LLM)\nto implicitly transfer semantic priors, enhancing multi-modal alignment without\nfine-tuning. This dual alignment strategy, temporal modeling via structured\nstate-space dynamics and semantic purification via textual priors, enables more\nprecise localization. Extensive experiments on QVHighlights, Charades-STA, and\nTVSum demonstrate that MLVTG achieves state-of-the-art performance and\nsignificantly outperforms existing baselines."}
{"id": "2506.08240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08240", "abs": "https://arxiv.org/abs/2506.08240", "authors": ["Dongkyu Cho", "Rumi Chunara"], "title": "Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations", "comment": "12 pages, 6 figures", "summary": "Data augmentation is a promising tool for enhancing out-of-distribution\ngeneralization, where the key is to produce diverse, challenging variations of\nthe source domain via costly targeted augmentations that maximize its\ngeneralization effect. Conversely, random augmentation is inexpensive but is\ndeemed suboptimal due to its limited effect. In this paper, we revisit random\naugmentation and explore methods to address its shortcomings. We show that the\nstochastic nature of random augmentation can produce a set of colliding\naugmentations that distorts the learned features, similar to catastrophic\nforgetting. We propose a simple solution that improves the generalization\neffect of random augmentation by addressing forgetting, which displays strong\ngeneralization performance across various single source domain generalization\n(sDG) benchmarks."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Ne≈üet √únver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Ta≈ükƒ±n Padƒ±r"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08460", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08460", "abs": "https://arxiv.org/abs/2506.08460", "authors": ["Yihong Guo", "Yu Yang", "Pan Xu", "Anqi Liu"], "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning", "comment": null, "summary": "We study the off-dynamics offline reinforcement learning problem, where the\ngoal is to learn a policy from offline datasets collected from source and\ntarget domains with mismatched transition. Existing off-dynamics offline RL\nmethods typically either filter source transitions that resemble those of the\ntarget domain or apply reward augmentation to source data, both constrained by\nthe limited transitions available from the target domain. As a result, the\nlearned policy is unable to explore target domain beyond the offline datasets.\nWe propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that\naddresses this limitation by enabling exploration of the target domain via\nlearned dynamics. MOBODY generates new synthetic transitions in the target\ndomain through model rollouts, which are used as data augmentation during\noffline policy learning. Unlike existing model-based methods that learn\ndynamics from a single domain, MOBODY tackles the challenge of mismatched\ndynamics by leveraging both source and target datasets. Directly merging these\ndatasets can bias the learned model toward source dynamics. Instead, MOBODY\nlearns target dynamics by discovering a shared latent representation of states\nand transitions across domains through representation learning. To stabilize\ntraining, MOBODY incorporates a behavior cloning loss that regularizes the\npolicy. Specifically, we introduce a Q-weighted behavior cloning loss that\nregularizes the policy toward actions with high target-domain Q-values, rather\nthan uniformly imitating all actions in the dataset. These Q-values are learned\nfrom an enhanced target dataset composed of offline target data, augmented\nsource data, and rollout data from the learned target dynamics. We evaluate\nMOBODY on MuJoCo benchmarks and show that it significantly outperforms\nstate-of-the-art baselines, with especially pronounced improvements in\nchallenging scenarios."}
{"id": "2506.08526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08526", "abs": "https://arxiv.org/abs/2506.08526", "authors": ["Zhongtao Tian", "Wenhao Huang", "Zhidong Chen", "Xiao Wei Sun"], "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer", "comment": null, "summary": "Visual localization remains challenging in dynamic environments where\nfluctuating lighting, adverse weather, and moving objects disrupt appearance\ncues. Despite advances in feature representation, current absolute pose\nregression methods struggle to maintain consistency under varying conditions.\nTo address this challenge, we propose a framework that synergistically combines\nmulti-scale feature learning with semantic scene understanding. Our approach\nemploys a hierarchical Transformer with cross-scale attention to fuse geometric\ndetails and contextual cues, preserving spatial precision while adapting to\nenvironmental changes. We improve the performance of this architecture with\nsemantic supervision via neural scene representation during training, guiding\nthe network to learn view-invariant features that encode persistent structural\ninformation while suppressing complex environmental interference. Experiments\non TartanAir demonstrate that our approach outperforms existing pose regression\nmethods in challenging scenarios with dynamic objects, illumination changes,\nand occlusions. Our findings show that integrating multi-scale processing with\nsemantic guidance offers a promising strategy for robust visual localization in\nreal-world dynamic environments."}
{"id": "2506.08243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08243", "abs": "https://arxiv.org/abs/2506.08243", "authors": ["Zhenjiang Mao", "Artem Bisliouk", "Rohith Reddy Nama", "Ivan Ruchkin"], "title": "Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive performance in\nmathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.\nHowever, they tend to produce highly confident yet incorrect outputs, which\nposes significant risks in domains like education, where users may lack the\nexpertise to assess reasoning steps. To address this, we propose a structured\nframework that models stepwise confidence as a temporal signal and evaluates it\nusing Signal Temporal Logic (STL). In particular, we define formal STL-based\nconstraints to capture desirable temporal properties and compute robustness\nscores that serve as structured, interpretable confidence estimates. Our\napproach also introduces a set of uncertainty reshaping strategies to enforce\nsmoothness, monotonicity, and causal consistency across the reasoning\ntrajectory. Experiments show that our approach consistently improves\ncalibration metrics and provides more reliable uncertainty estimates than\nconventional confidence aggregation and post-hoc calibration."}
{"id": "2506.08441", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08441", "abs": "https://arxiv.org/abs/2506.08441", "authors": ["Anh N. Nhu", "Sanghyun Son", "Ming Lin"], "title": "Time-Aware World Model for Adaptive Prediction and Control", "comment": "Paper accepted to ICML 2025", "summary": "In this work, we introduce the Time-Aware World Model (TAWM), a model-based\napproach that explicitly incorporates temporal dynamics. By conditioning on the\ntime-step size, {\\Delta}t, and training over a diverse range of {\\Delta}t\nvalues -- rather than sampling at a fixed time-step -- TAWM learns both high-\nand low-frequency task dynamics across diverse control problems. Grounded in\nthe information-theoretic insight that the optimal sampling rate depends on a\nsystem's underlying dynamics, this time-aware formulation improves both\nperformance and data efficiency. Empirical evaluations show that TAWM\nconsistently outperforms conventional models across varying observation rates\nin a variety of control tasks, using the same number of training samples and\niterations. Our code can be found online at:\ngithub.com/anh-nn01/Time-Aware-World-Model."}
{"id": "2506.08463", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08463", "abs": "https://arxiv.org/abs/2506.08463", "authors": ["Zhishuai Liu", "Yu Yang", "Ruhan Wang", "Pan Xu", "Dongruo Zhou"], "title": "How to Provably Improve Return Conditioned Supervised Learning?", "comment": "25 pages, 4 figures, 12 tables", "summary": "In sequential decision-making problems, Return-Conditioned Supervised\nLearning (RCSL) has gained increasing recognition for its simplicity and\nstability in modern decision-making tasks. Unlike traditional offline\nreinforcement learning (RL) algorithms, RCSL frames policy learning as a\nsupervised learning problem by taking both the state and return as input. This\napproach eliminates the instability often associated with temporal difference\n(TD) learning in offline RL. However, RCSL has been criticized for lacking the\nstitching property, meaning its performance is inherently limited by the\nquality of the policy used to generate the offline dataset. To address this\nlimitation, we propose a principled and simple framework called Reinforced\nRCSL. The key innovation of our framework is the introduction of a concept we\ncall the in-distribution optimal return-to-go. This mechanism leverages our\npolicy to identify the best achievable in-dataset future return based on the\ncurrent state, avoiding the need for complex return augmentation techniques.\nOur theoretical analysis demonstrates that Reinforced RCSL can consistently\noutperform the standard RCSL approach. Empirical results further validate our\nclaims, showing significant performance improvements across a range of\nbenchmarks."}
{"id": "2506.08529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08529", "abs": "https://arxiv.org/abs/2506.08529", "authors": ["Xijun Wang", "Xin Li", "Bingchen Li", "Zhibo Chen"], "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\\times$RTX 4090s", "comment": "Project page: https://kopperx.github.io/projects/liftvsr", "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."}
{"id": "2506.08244", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08244", "abs": "https://arxiv.org/abs/2506.08244", "authors": ["Riccardo Ali", "Pietro Li√≤", "Jamie Vicary"], "title": "Parameter-free approximate equivariance for tasks with finite group symmetry", "comment": null, "summary": "Equivariant neural networks incorporate symmetries through group actions,\nembedding them as an inductive bias to improve performance on a wide variety of\ntasks. However, existing equivariant methods can be computationally intensive,\nwith high parameter counts, and are often tied to a specific architecture. We\npropose a simple zero-parameter approach that imposes approximate equivariance\nfor a finite group in the latent representation, as an additional term in the\nloss function. We conduct experiments which allow the network to learn a group\nrepresentation on the latent space, and show in every case it prefers to learn\nthe regular representation. Fixing this action on the latent space, this yields\na simple method to impose approximate equivariance as an additional loss\npenalty. We benchmark our approach on three datasets and compare it against\nseveral existing equivariant methods, showing that in many cases it achieves\nsimilar or better performance for a fraction of the parameters."}
{"id": "2506.08524", "categories": ["cs.SD", "cs.AI", "cs.MM", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08524", "abs": "https://arxiv.org/abs/2506.08524", "authors": ["Weiguo Wang", "Andy Nie", "Wenrui Zhou", "Yi Kai", "Chengchen Hu"], "title": "Teaching Physical Awareness to LLMs through Sounds", "comment": "ICML 2025", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in text and\nmultimodal processing, yet they fundamentally lack physical\nawareness--understanding of real-world physical phenomena. In this work, we\npresent ACORN, a framework that teaches LLMs physical awareness through sound,\nfocusing on fundamental physical phenomena like the Doppler effect, multipath\neffect, and spatial relationships. To overcome data scarcity, ACORN introduce a\nphysics-based simulator combining real-world sound sources with controlled\nphysical channels to generate diverse training data. Using this simulator, we\nbuild AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an\naudio encoder that processes both magnitude and phase information. By\nconnecting our audio encoder to state-of-the-art LLMs, we demonstrate\nreasonable results in both simulated and real-world tasks, such as\nline-of-sight detection, Doppler effect estimation, and Direction-of-Arrival\nestimation, paving the way for enabling LLMs to understand physical world."}
{"id": "2506.08541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08541", "abs": "https://arxiv.org/abs/2506.08541", "authors": ["Qi Yan", "Brian Zhang", "Yutong Zhang", "Daniel Yang", "Joshua White", "Di Chen", "Jiachao Liu", "Langechuan Liu", "Binnan Zhuang", "Shaoshuai Shi", "Renjie Liao"], "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching", "comment": null, "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and\ninformed decision-making in autonomous driving, particularly under dynamic\nreal-world conditions that necessitate multi-modal forecasts. We introduce\nTrajFlow, a novel flow matching-based motion prediction framework that\naddresses the scalability and efficiency challenges of existing generative\ntrajectory prediction methods. Unlike conventional generative approaches that\nemploy i.i.d. sampling and require multiple inference passes to capture diverse\noutcomes, TrajFlow predicts multiple plausible future trajectories in a single\npass, significantly reducing computational overhead while maintaining coherence\nacross predictions. Moreover, we propose a ranking loss based on the\nPlackett-Luce distribution to improve uncertainty estimation of predicted\ntrajectories. Additionally, we design a self-conditioning training technique\nthat reuses the model's own predictions to construct noisy inputs during a\nsecond forward pass, thereby improving generalization and accelerating\ninference. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across\nvarious key metrics, underscoring its effectiveness for safety-critical\nautonomous driving applications. The code and other details are available on\nthe project website https://traj-flow.github.io/."}
{"id": "2506.08255", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.08255", "abs": "https://arxiv.org/abs/2506.08255", "authors": ["Patryk Krukowski", "≈Åukasz Gorczyca", "Piotr Helm", "Kamil KsiƒÖ≈ºek", "Przemys≈Çaw Spurek"], "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense", "comment": null, "summary": "Traditional deep neural networks suffer from several limitations, including\ncatastrophic forgetting. When models are adapted to new datasets, they tend to\nquickly forget previously learned knowledge. Another significant issue is the\nlack of robustness to even small perturbations in the input data. In practice,\nwe can often easily perform adversarial attacks and change the network's\npredictions, adding minimal noise to the input. Dedicated architectures and\ntraining procedures can solve each of the above problems separately.\nUnfortunately, currently, no model can simultaneously address both catastrophic\nforgetting and vulnerability to adversarial attacks. We introduce SHIELD\n(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel\napproach that integrates a hypernetwork-based continual learning approach with\ninterval arithmetic. SHIELD use the hypernetwork to transfer trainable task\nembedding vectors into the weights of a target model dedicated to specific\ndata. This paradigm allows for the dynamic generation of separate networks for\neach subtask, while the hypernetwork aggregates and analyzes information across\nall tasks. The target model takes in the input a data sample with a defined\ninterval range, and by creating a hypercube, produces a prediction for the\ngiven range. Therefore, such target models provide strict guarantees against\nall possible attacks for data samples within the interval range. Our approach\nenhances security without sacrificing network adaptability, addressing the\noverlooked challenge of safety in continual learning."}
{"id": "2506.08719", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08719", "abs": "https://arxiv.org/abs/2506.08719", "authors": ["Yongpeng Zhao", "Maik Pfefferkorn", "Maximilian Templer", "Rolf Findeisen"], "title": "Efficient Learning of Vehicle Controller Parameters via Multi-Fidelity Bayesian Optimization: From Simulation to Experiment", "comment": "8 pages, 8 figures, accepted for IEEE IV 2025", "summary": "Parameter tuning for vehicle controllers remains a costly and time-intensive\nchallenge in automotive development. Traditional approaches rely on extensive\nreal-world testing, making the process inefficient. We propose a multi-fidelity\nBayesian optimization approach that efficiently learns optimal controller\nparameters by leveraging both low-fidelity simulation data and a very limited\nnumber of real-world experiments. Our approach significantly reduces the need\nfor manual tuning and expensive field testing while maintaining the standard\ntwo-stage development workflow used in industry. The core contribution is the\nintegration of an auto-regressive multi-fidelity Gaussian process model into\nBayesian optimization, enabling knowledge transfer between different fidelity\nlevels without requiring additional low-fidelity evaluations during real-world\ntesting. We validate our approach through both simulation studies and realworld\nexperiments. The results demonstrate that our method achieves high-quality\ncontroller performance with only very few real-world experiments, highlighting\nits potential as a practical and scalable solution for intelligent vehicle\ncontrol tuning in industrial applications."}
{"id": "2506.08543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08543", "abs": "https://arxiv.org/abs/2506.08543", "authors": ["Bowei Tian", "Xuntao Lyu", "Meng Liu", "Hongyi Wang", "Ang Li"], "title": "Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs", "comment": "arXiv admin note: text overlap with arXiv:2503.22720", "summary": "High-level representations have become a central focus in enhancing AI\ntransparency and control, shifting attention from individual neurons or\ncircuits to structured semantic directions that align with human-interpretable\nconcepts. Motivated by the Linear Representation Hypothesis (LRH), we propose\nthe Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned\ndirections originate in the input space and are selectively amplified with\nincreasing depth. We then introduce the Spectral Principal Path (SPP)\nframework, which formalizes how deep networks progressively distill linear\nrepresentations along a small set of dominant spectral directions. Building on\nthis framework, we further demonstrate the multimodal robustness of these\nrepresentations in Vision-Language Models (VLMs). By bridging theoretical\ninsights with empirical validation, this work advances a structured theory of\nrepresentation formation in deep networks, paving the way for improving AI\nrobustness, fairness, and transparency."}
{"id": "2506.08266", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.08266", "abs": "https://arxiv.org/abs/2506.08266", "authors": ["Yaswanth Chittepu", "Blossom Metevier", "Will Schwarzer", "Austin Hoag", "Scott Niekum", "Philip S. Thomas"], "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints", "comment": "20 pages, 6 figures, 4 tables, Second Reinforcement Learning\n  Conference (RLC 2025)", "summary": "Existing approaches to language model alignment often treat safety as a\ntradeoff against helpfulness, which can lead to unacceptable responses in\nsensitive domains. To ensure reliable performance in such settings, we propose\nHigh-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a\nmethod that provides high-confidence safety guarantees while maximizing\nhelpfulness. Similar to previous methods, HC-RLHF explicitly decouples human\npreferences into helpfulness and harmlessness (safety), which are learned by\ntraining a reward model and a cost model, respectively. It then employs a\ntwo-step process to find safe solutions. In the first step, it optimizes the\nreward function under an intentionally pessimistic version of the cost\nconstraint. In the second step, the trained model undergoes a safety test to\nverify whether its performance stays within an upper-confidence bound of the\nactual cost constraint. We provide a theoretical analysis of HC-RLHF, including\nproof that it will not return an unsafe solution with a probability greater\nthan a user-specified threshold. For our empirical analysis, we apply HC-RLHF\nto align three different language models (Qwen2-1.5B, Qwen2.5-3B, and\nLLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF\nproduces safe models with high probability and can improve harmlessness and\nhelpfulness compared to previous methods."}
{"id": "2506.08805", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08805", "abs": "https://arxiv.org/abs/2506.08805", "authors": ["Stina Klein", "Pooja Prajod", "Katharina Weitz", "Matteo Lavit Nicora", "Dimitra Tsovaltzi", "Elisabeth Andr√©"], "title": "Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration", "comment": "Accepted LBW at CHIWORK 2025", "summary": "The integration of collaborative robots (cobots) in industrial settings\nraises concerns about worker well-being, particularly due to reduced social\ninteractions. Avatars - designed to facilitate worker interactions and\nengagement - are promising solutions to enhance the human-robot collaboration\n(HRC) experience. However, real-world perspectives on avatar-supported HRC\nremain unexplored. To address this gap, we conducted a focus group study with\nemployees from a German manufacturing company that uses cobots. Before the\ndiscussion, participants engaged with a scripted, industry-like HRC demo in a\nlab setting. This qualitative approach provided valuable insights into the\navatar's potential roles, improvements to its behavior, and practical\nconsiderations for deploying them in industrial workcells. Our findings also\nemphasize the importance of personalized communication and task assistance.\nAlthough our study's limitations restrict its generalizability, it serves as an\ninitial step in recognizing the potential of adaptive, context-aware avatar\ninteractions in real-world industrial environments."}
{"id": "2506.08553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08553", "abs": "https://arxiv.org/abs/2506.08553", "authors": ["Agnese Taluzzi", "Davide Gesualdi", "Riccardo Santambrogio", "Chiara Plizzari", "Francesca Palermo", "Simone Mentasti", "Matteo Matteucci"], "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge", "comment": "Technical report for the HD-EPIC VQA Challenge 2025 (1st place)", "summary": "This report presents SceneNet and KnowledgeNet, our approaches developed for\nthe HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with\na multi-modal large language model (MLLM) to capture fine-grained object\ninteractions, spatial relationships, and temporally grounded events. In\nparallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge\nto introduce high-level semantic connections between entities, enabling\nreasoning beyond directly observable visual evidence. Each method demonstrates\ndistinct strengths across the seven categories of the HD-EPIC benchmark, and\ntheir combination within our framework results in an overall accuracy of 44.21%\non the challenge, highlighting its effectiveness for complex egocentric VQA\ntasks."}
{"id": "2506.08267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08267", "abs": "https://arxiv.org/abs/2506.08267", "authors": ["Mansooreh Montazerin", "Majd Al Aawar", "Antonio Ortega", "Ajitesh Srivastava"], "title": "Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression", "comment": null, "summary": "Symbolic regression (SR) aims to discover closed-form mathematical\nexpressions that accurately describe data, offering interpretability and\nanalytical insight beyond standard black-box models. Existing SR methods often\nrely on population-based search or autoregressive modeling, which struggle with\nscalability and symbolic consistency. We introduce LIES (Logarithm, Identity,\nExponential, Sine), a fixed neural network architecture with interpretable\nprimitive activations that are optimized to model symbolic expressions. We\ndevelop a framework to extract compact formulae from LIES networks by training\nwith an appropriate oversampling strategy and a tailored loss function to\npromote sparsity and to prevent gradient instability. After training, it\napplies additional pruning strategies to further simplify the learned\nexpressions into compact formulae. Our experiments on SR benchmarks show that\nthe LIES framework consistently produces sparse and accurate symbolic formulae\noutperforming all baselines. We also demonstrate the importance of each design\ncomponent through ablation studies."}
{"id": "2506.08892", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08892", "abs": "https://arxiv.org/abs/2506.08892", "authors": ["Tauhid Tanjim", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams", "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "The human-robot interaction (HRI) field has recognized the importance of\nenabling robots to interact with teams. Human teams rely on effective\ncommunication for successful collaboration in time-sensitive environments.\nRobots can play a role in enhancing team coordination through real-time\nassistance. Despite significant progress in human-robot teaming research, there\nremains an essential gap in how robots can effectively communicate with action\nteams using multimodal interaction cues in time-sensitive environments. This\nstudy addresses this knowledge gap in an experimental in-lab study to\ninvestigate how multimodal robot communication in action teams affects workload\nand human perception of robots. We explore team collaboration in a medical\ntraining scenario where a robotic crash cart (RCC) provides verbal and\nnon-verbal cues to help users remember to perform iterative tasks and search\nfor supplies. Our findings show that verbal cues for object search tasks and\nvisual cues for task reminders reduce team workload and increase perceived ease\nof use and perceived usefulness more effectively than a robot with no feedback.\nOur work contributes to multimodal interaction research in the HRI field,\nhighlighting the need for more human-robot teaming research to understand best\npractices for integrating collaborative robots in time-sensitive environments\nsuch as in hospitals, search and rescue, and manufacturing applications."}
{"id": "2506.08555", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.08555", "abs": "https://arxiv.org/abs/2506.08555", "authors": ["Xinyue Niu", "Akira Furui"], "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement", "comment": "6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025", "summary": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems."}
{"id": "2506.08270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08270", "abs": "https://arxiv.org/abs/2506.08270", "authors": ["Zitong Huang", "Mansooreh Montazerin", "Ajitesh Srivastava"], "title": "SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space", "comment": null, "summary": "Designing neural networks typically relies on manual trial and error or a\nneural architecture search (NAS) followed by weight training. The former is\ntime-consuming and labor-intensive, while the latter often discretizes\narchitecture search and weight optimization. In this paper, we propose a\nfundamentally different approach that simultaneously optimizes both the\narchitecture and the weights of a neural network. Our framework first trains a\nuniversal multi-scale autoencoder that embeds both architectural and parametric\ninformation into a continuous latent space, where functionally similar neural\nnetworks are mapped closer together. Given a dataset, we then randomly\ninitialize a point in the embedding space and update it via gradient descent to\nobtain the optimal neural network, jointly optimizing its structure and\nweights. The optimization process incorporates sparsity and compactness\npenalties to promote efficient models. Experiments on synthetic regression\ntasks demonstrate that our method effectively discovers sparse and compact\nneural networks with strong performance."}
{"id": "2506.08979", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08979", "abs": "https://arxiv.org/abs/2506.08979", "authors": ["Longyu Yang", "Ping Hu", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu"], "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather", "comment": null, "summary": "LiDAR segmentation has emerged as an important task to enrich multimedia\nexperiences and analysis. Range-view-based methods have gained popularity due\nto their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation."}
{"id": "2506.08562", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08562", "abs": "https://arxiv.org/abs/2506.08562", "authors": ["Duc Thanh Pham", "Hong Dang Nguyen", "Nhat Minh Nguyen Quoc", "Linh Ngo Van", "Sang Dinh Viet", "Duc Anh Nguyen"], "title": "Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection", "comment": null, "summary": "Recently, object detection models have witnessed notable performance\nimprovements, particularly with transformer-based models. However, new objects\nfrequently appear in the real world, requiring detection models to continually\nlearn without suffering from catastrophic forgetting. Although Incremental\nObject Detection (IOD) has emerged to address this challenge, these existing\nmodels are still not practical due to their limited performance and prolonged\ninference time. In this paper, we introduce a novel framework for IOD, called\nHier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both\nefficiency and competitive performance by leveraging Neural Collapse for\nimbalance dataset and Hierarchical relation of classes' labels."}
{"id": "2506.08272", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.08272", "abs": "https://arxiv.org/abs/2506.08272", "authors": ["Tarushri N. S."], "title": "Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids", "comment": null, "summary": "Universal Differential Equations (UDEs), which blend neural networks with\nphysical differential equations, have emerged as a powerful framework for\nscientific machine learning (SciML), enabling data-efficient, interpretable,\nand physically consistent modeling. In the context of smart grid systems,\nmodeling node-wise battery dynamics remains a challenge due to the\nstochasticity of solar input and variability in household load profiles.\nTraditional approaches often struggle with generalization and fail to capture\nunmodeled residual dynamics. This work proposes a UDE-based approach to learn\nnode-specific battery evolution by embedding a neural residual into a\nphysically inspired battery ODE. Synthetic yet realistic solar generation and\nload demand data are used to simulate battery dynamics over time. The neural\ncomponent learns to model unobserved or stochastic corrections arising from\nheterogeneity in node demand and environmental conditions. Comprehensive\nexperiments reveal that the trained UDE aligns closely with ground truth\nbattery trajectories, exhibits smooth convergence behavior, and maintains\nstability in long-term forecasts. These findings affirm the viability of\nUDE-based SciML approaches for battery modeling in decentralized energy\nnetworks and suggest broader implications for real-time control and\noptimization in renewable-integrated smart grids."}
{"id": "2506.08997", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08997", "abs": "https://arxiv.org/abs/2506.08997", "authors": ["Fabian Immel", "Jan-Hendrik Pauls", "Richard Fehler", "Frank Bieder", "Jonas Merkert", "Christoph Stiller"], "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction", "comment": null, "summary": "Autonomous vehicles rely on detailed and accurate environmental information\nto operate safely. High definition (HD) maps offer a promising solution, but\ntheir high maintenance cost poses a significant barrier to scalable deployment.\nThis challenge is addressed by online HD map construction methods, which\ngenerate local HD maps from live sensor data. However, these methods are\ninherently limited by the short perception range of onboard sensors. To\novercome this limitation and improve general performance, recent approaches\nhave explored the use of standard definition (SD) maps as prior, which are\nsignificantly easier to maintain. We propose SDTagNet, the first online HD map\nconstruction method that fully utilizes the information of widely available SD\nmaps, like OpenStreetMap, to enhance far range detection accuracy. Our approach\nintroduces two key innovations. First, in contrast to previous work, we\nincorporate not only polyline SD map data with manually selected classes, but\nadditional semantic information in the form of textual annotations. In this\nway, we enrich SD vector map tokens with NLP-derived features, eliminating the\ndependency on predefined specifications or exhaustive class taxonomies. Second,\nwe introduce a point-level SD map encoder together with orthogonal element\nidentifiers to uniformly integrate all types of map elements. Experiments on\nArgoverse 2 and nuScenes show that this boosts map perception performance by up\nto +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP\n(+20%) w.r.t. previous approaches that already use SD map priors. Code is\navailable at https://github.com/immel-f/SDTagNet"}
{"id": "2506.08566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08566", "abs": "https://arxiv.org/abs/2506.08566", "authors": ["Yibo Cui", "Liang Xie", "Yu Zhao", "Jiawei Sun", "Erwei Yin"], "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations", "comment": null, "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate\nenvironments by integrating visual perception and natural language\ninstructions, yet faces significant challenges due to the scarcity of\nfine-grained cross-modal alignment annotations. Existing datasets primarily\nfocus on global instruction-trajectory matching, neglecting\nsub-instruction-level and entity-level alignments critical for accurate\nnavigation action decision-making. To address this limitation, we propose\nFCA-NIG, a generative framework that automatically constructs navigation\ninstructions with dual-level fine-grained cross-modal annotations. In this\nframework, an augmented trajectory is first divided into sub-trajectories,\nwhich are then processed through GLIP-based landmark detection, crafted\ninstruction construction, OFA-Speaker based R2R-like instruction generation,\nand CLIP-powered entity selection, generating sub-instruction-trajectory pairs\nwith entity-landmark annotations. Finally, these sub-pairs are aggregated to\nform a complete instruction-trajectory pair. The framework generates the\nFCA-R2R dataset, the first large-scale augmentation dataset featuring precise\nsub-instruction-sub-trajectory and entity-landmark alignments. Extensive\nexperiments demonstrate that training with FCA-R2R significantly improves the\nperformance of multiple state-of-the-art VLN agents, including SF, EnvDrop,\nRecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances\nagents' state awareness and decision accuracy, while entity-landmark alignment\nfurther boosts navigation performance and generalization. These results\nhighlight the effectiveness of FCA-NIG in generating high-quality, scalable\ntraining data without manual annotation, advancing fine-grained cross-modal\nlearning in complex navigation tasks."}
{"id": "2506.08274", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08274", "abs": "https://arxiv.org/abs/2506.08274", "authors": ["Jo√£o Manoel Herrera Pinheiro", "Suzana Vilas Boas de Oliveira", "Thiago Henrique Segreto Silva", "Pedro Antonio Rabelo Saraiva", "Enzo Ferreira de Souza", "Leonardo Andr√© Ambrosio", "Marcelo Becker"], "title": "The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks", "comment": "27 pages", "summary": "This research addresses the critical lack of comprehensive studies on feature\nscaling by systematically evaluating 12 scaling techniques - including several\nless common transformations - across 14 different Machine Learning algorithms\nand 16 datasets for classification and regression tasks. We meticulously\nanalyzed impacts on predictive performance (using metrics such as accuracy,\nMAE, MSE, and $R^2$) and computational costs (training time, inference time,\nand memory usage). Key findings reveal that while ensemble methods (such as\nRandom Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)\ndemonstrate robust performance largely independent of scaling, other widely\nused models such as Logistic Regression, SVMs, TabNet, and MLPs show\nsignificant performance variations highly dependent on the chosen scaler. This\nextensive empirical analysis, with all source code, experimental results, and\nmodel parameters made publicly available to ensure complete transparency and\nreproducibility, offers model-specific crucial guidance to practitioners on the\nneed for an optimal selection of feature scaling techniques."}
{"id": "2506.08591", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.08591", "abs": "https://arxiv.org/abs/2506.08591", "authors": ["Chengchao Shen", "Hourun Zhu", "Gongfan Fang", "Jianxin Wang", "Xinchao Wang"], "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers", "comment": null, "summary": "Transformer models achieve excellent scaling property, where the performance\nis improved with the increment of model capacity. However, large-scale model\nparameters lead to an unaffordable cost of computing and memory. We analyze\npopular transformer architectures and find that multilayer perceptron (MLP)\nmodules take up the majority of model parameters. To this end, we focus on the\nrecoverability of the compressed models and propose a Diversity-Guided MLP\nReduction (DGMR) method to significantly reduce the parameters of large vision\ntransformers with only negligible performance degradation. Specifically, we\nconduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons\nof MLP hidden layer, while preserving weight diversity for better performance\nrecover during distillation. Compared to the model trained from scratch, our\npruned model only requires 0.06\\% data of LAION-2B (for the training of large\nvision transformers) without labels (ImageNet-1K) to recover the original\nperformance. Experimental results on several state-of-the-art large vision\ntransformers demonstrate that our method achieves a more than 57.0\\% parameter\nand FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),\nour method accomplishes a 71.5\\% parameter and FLOPs reduction without\nperformance degradation. The source code and trained weights are available at\nhttps://github.com/visresearch/DGMR."}
{"id": "2506.08292", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08292", "abs": "https://arxiv.org/abs/2506.08292", "authors": ["Xie Yi", "Zhanke Zhou", "Chentao Cao", "Qiyu Niu", "Tongliang Liu", "Bo Han"], "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium", "comment": "Accepted by ICML 2025", "summary": "Multi-agent frameworks can substantially boost the reasoning power of large\nlanguage models (LLMs), but they typically incur heavy computational costs and\nlack convergence guarantees. To overcome these challenges, we recast multi-LLM\ncoordination as an incomplete-information game and seek a Bayesian Nash\nequilibrium (BNE), in which each agent optimally responds to its probabilistic\nbeliefs about the strategies of others. We introduce Efficient Coordination via\nNash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that\nmarries distributed reasoning with centralized final output. Under ECON, each\nLLM independently selects responses that maximize its expected reward,\nconditioned on its beliefs about co-agents, without requiring costly\ninter-agent exchanges. We mathematically prove that ECON attains a markedly\ntighter regret bound than non-equilibrium multi-agent schemes. Empirically,\nECON outperforms existing multi-LLM approaches by 11.2% on average across six\nbenchmarks spanning complex reasoning and planning tasks. Further experiments\ndemonstrate ECON's ability to flexibly incorporate additional models,\nconfirming its scalability and paving the way toward larger, more powerful\nmulti-LLM ensembles. The code is publicly available at:\nhttps://github.com/tmlr-group/ECON."}
{"id": "2506.08596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08596", "abs": "https://arxiv.org/abs/2506.08596", "authors": ["Guyang Zhang", "Waleed Abdulla"], "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems", "comment": null, "summary": "Transformers have become the architecture of choice for learning long-range\ndependencies, yet their adoption in hyperspectral imaging (HSI) is still\nemerging. We reviewed more than 300 papers published up to 2025 and present the\nfirst end-to-end survey dedicated to Transformer-based HSI classification. The\nstudy categorizes every stage of a typical pipeline-pre-processing, patch or\npixel tokenization, positional encoding, spatial-spectral feature extraction,\nmulti-head self-attention variants, skip connections, and loss design-and\ncontrasts alternative design choices with the unique spatial-spectral\nproperties of HSI. We map the field's progress against persistent obstacles:\nscarce labeled data, extreme spectral dimensionality, computational overhead,\nand limited model explainability. Finally, we outline a research agenda\nprioritizing valuable public data sets, lightweight on-edge models,\nillumination and sensor shifts robustness, and intrinsically interpretable\nattention mechanisms. Our goal is to guide researchers in selecting, combining,\nor extending Transformer components that are truly fit for purpose for\nnext-generation HSI applications."}
{"id": "2506.08295", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08295", "abs": "https://arxiv.org/abs/2506.08295", "authors": ["Zhanke Zhou", "Xiao Feng", "Zhaocheng Zhu", "Jiangchao Yao", "Sanmi Koyejo", "Bo Han"], "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?", "comment": "Accepted by ICML 2025", "summary": "While existing benchmarks probe the reasoning abilities of large language\nmodels (LLMs) across diverse domains, they predominantly assess passive\nreasoning, providing models with all the information needed to reach a\nsolution. By contrast, active reasoning-where an LLM must interact with\nexternal systems to acquire missing evidence or data-has received little\nsystematic attention. To address this shortfall, we present AR-Bench, a novel\nbenchmark designed explicitly to evaluate an LLM's active reasoning skills.\nAR-Bench comprises three task families-detective cases, situation puzzles, and\nguessing numbers-that together simulate real-world, agentic scenarios and\nmeasure performance across commonsense, logical, and symbolic reasoning\nchallenges. Empirical evaluation on AR-Bench demonstrates that contemporary\nLLMs exhibit pronounced difficulties with active reasoning: they frequently\nfail to acquire or leverage the information needed to solve tasks. This gap\nhighlights a stark divergence between their passive and active reasoning\nabilities. Moreover, ablation studies indicate that even advanced strategies,\nsuch as tree-based searching or post-training approaches, yield only modest\ngains and fall short of the levels required for real-world deployment.\nCollectively, these findings highlight the critical need to advance methodology\nfor active reasoning, e.g., incorporating interactive learning, real-time\nfeedback loops, and environment-aware objectives for training. The benchmark is\npublicly available at: https://github.com/tmlr-group/AR-Bench."}
{"id": "2506.08611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08611", "abs": "https://arxiv.org/abs/2506.08611", "authors": ["Shiji Zhao", "Chi Chen", "Ranjie Duan", "Xizhe Wang", "Xingxing Wei"], "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation", "comment": "arXiv admin note: text overlap with arXiv:2312.05508", "summary": "Adversarial Training (AT) is widely recognized as an effective approach to\nenhance the adversarial robustness of Deep Neural Networks. As a variant of AT,\nAdversarial Robustness Distillation (ARD) has shown outstanding performance in\nenhancing the robustness of small models. However, both AT and ARD face robust\nfairness issue: these models tend to display strong adversarial robustness\nagainst some classes (easy classes) while demonstrating weak adversarial\nrobustness against others (hard classes). This paper explores the underlying\nfactors of this problem and points out the smoothness degree of soft labels for\ndifferent classes significantly impacts the robust fairness from both empirical\nobservation and theoretical analysis. Based on the above exploration, we\npropose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge\nDistillation framework to enhance the adversarial robust fairness.\nSpecifically, ABSLD adaptively reduces the student's error risk gap between\ndifferent classes, which is accomplished by adjusting the class-wise smoothness\ndegree of teacher's soft labels during the training process, and the adjustment\nis managed by assigning varying temperatures to different classes.\nAdditionally, as a label-based approach, ABSLD is highly adaptable and can be\nintegrated with the sample-based methods. Extensive experiments demonstrate\nABSLD outperforms state-of-the-art methods on the comprehensive performance of\nrobustness and fairness."}
{"id": "2506.08298", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.08298", "abs": "https://arxiv.org/abs/2506.08298", "authors": ["Trung-Kien Nguyen", "Heng Ping", "Shixuan Li", "Peiyu Zhang", "Nikos Kanakaris", "Nicholas Kotov", "Paul Bogdan"], "title": "H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs", "comment": null, "summary": "The growing interests and applications of graph learning in diverse domains\nhave propelled the development of a unified model generalizing well across\ndifferent graphs and tasks, known as the Graph Foundation Model (GFM). Existing\nresearch has leveraged text-attributed graphs (TAGs) to tackle the\nheterogeneity in node features among graphs. However, they primarily focus on\nhomogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple\ntypes of nodes/edges reside, underexplored. To enhance the capabilities and\napplications of GFM, we introduce H$^2$GFM, a novel framework designed to\ngeneralize across both HoTAGs and HeTAGs. Our model projects diverse\nmeta-relations among graphs under a unified textual space, and employs a\ncontext encoding to capture spatial and higher-order semantic relationships. To\nachieve robust node representations, we propose a novel context-adaptive graph\ntransformer (CGT), effectively capturing information from both context\nneighbors and their relationships. Furthermore, we employ a mixture of CGT\nexperts to capture the heterogeneity in structural patterns among graph types.\nComprehensive experiments on a wide range of HoTAGs and HeTAGs as well as\nlearning scenarios demonstrate the effectiveness of our model."}
{"id": "2506.08612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08612", "abs": "https://arxiv.org/abs/2506.08612", "authors": ["Robert-Jan Bruintjes", "Attila Lengyel", "Osman Semih Kayhan", "Davide Zambrano", "Nergis T√∂men", "Hadi Jamali-Rad", "Jan van Gemert"], "title": "Data-Efficient Challenges in Visual Inductive Priors: A Retrospective", "comment": null, "summary": "Deep Learning requires large amounts of data to train models that work well.\nIn data-deficient settings, performance can be degraded. We investigate which\nDeep Learning methods benefit training models in a data-deficient setting, by\norganizing the \"VIPriors: Visual Inductive Priors for Data-Efficient Deep\nLearning\" workshop series, featuring four editions of data-impaired challenges.\nThese challenges address the problem of training deep learning models for\ncomputer vision tasks with limited data. Participants are limited to training\nmodels from scratch using a low number of training samples and are not allowed\nto use any form of transfer learning. We aim to stimulate the development of\nnovel approaches that incorporate prior knowledge to improve the data\nefficiency of deep learning models. Successful challenge entries make use of\nlarge model ensembles that mix Transformers and CNNs, as well as heavy data\naugmentation. Novel prior knowledge-based methods contribute to success in some\nentries."}
{"id": "2506.08309", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08309", "abs": "https://arxiv.org/abs/2506.08309", "authors": ["Katherine Tieu", "Dongqi Fu", "Zihao Li", "Ross Maciejewski", "Jingrui He"], "title": "Learnable Spatial-Temporal Positional Encoding for Link Prediction", "comment": "Accepted by ICML 2025. 28 pages, 1 figures, 22 tables", "summary": "Accurate predictions rely on the expressiveness power of graph deep learning\nframeworks like graph neural networks and graph transformers, where a\npositional encoding mechanism has become much more indispensable in recent\nstate-of-the-art works to record the canonical position information. However,\nthe current positional encoding is limited in three aspects: (1) most\npositional encoding methods use pre-defined, and fixed functions, which are\ninadequate to adapt to the complex attributed graphs; (2) a few pioneering\nworks proposed the learnable positional encoding but are still limited to the\nstructural information, not considering the real-world time-evolving\ntopological and feature information; (3) most positional encoding methods are\nequipped with transformers' attention mechanism to fully leverage their\ncapabilities, where the dense or relational attention is often unaffordable on\nlarge-scale structured data. Hence, we aim to develop Learnable\nSpatial-Temporal Positional Encoding in an effective and efficient manner and\npropose a simple temporal link prediction model named L-STEP. Briefly, for\nL-STEP, we (1) prove the proposed positional learning scheme can preserve the\ngraph property from the spatial-temporal spectral viewpoint, (2) verify that\nMLPs can fully exploit the expressiveness and reach transformers' performance\non that encoding, (3) change different initial positional encoding inputs to\nshow robustness, (4) analyze the theoretical complexity and obtain less\nempirical running time than SOTA, and (5) demonstrate its temporal link\nprediction out-performance on 13 classic datasets and with 10 algorithms in\nboth transductive and inductive settings using 3 different sampling strategies.\nAlso, \\name\\ obtains the leading performance in the newest large-scale TGB\nbenchmark. Our code is available at https://github.com/kthrn22/L-STEP."}
{"id": "2506.08613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08613", "abs": "https://arxiv.org/abs/2506.08613", "authors": ["Joost van Dalen", "Yuki M. Asano", "Marc Russwurm"], "title": "SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything", "comment": null, "summary": "This work proposes SAMSelect, an algorithm to obtain a salient three-channel\nvisualization for multispectral images. We develop SAMSelect and show its use\nfor marine scientists visually interpreting floating marine debris in\nSentinel-2 imagery. These debris are notoriously difficult to visualize due to\ntheir compositional heterogeneity in medium-resolution imagery. Out of these\ndifficulties, a visual interpretation of imagery showing marine debris remains\na common practice by domain experts, who select bands and spectral indices on a\ncase-by-case basis informed by common practices and heuristics. SAMSelect\nselects the band or index combination that achieves the best classification\naccuracy on a small annotated dataset through the Segment Anything Model. Its\ncentral assumption is that the three-channel visualization achieves the most\naccurate segmentation results also provide good visual information for\nphoto-interpretation.\n  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine\ndebris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets\nfrom the Plastic Litter Project. This reveals the potential of new previously\nunused band combinations (e.g., a normalized difference index of B8, B2), which\ndemonstrate improved performance compared to literature-based indices. We\ndescribe the algorithm in this paper and provide an open-source code repository\nthat will be helpful for domain scientists doing visual photo interpretation,\nespecially in the marine field."}
{"id": "2506.08312", "categories": ["cs.LG", "cs.CR", "cs.DS", "math.PR", "math.ST", "stat.TH", "68P27 (Primary) 68Q32, 68Q87, 60B10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2506.08312", "abs": "https://arxiv.org/abs/2506.08312", "authors": ["Tom√°s Gonz√°lez", "Giulia Fanti", "Aaditya Ramdas"], "title": "Private Evolution Converges", "comment": null, "summary": "Private Evolution (PE) is a promising training-free method for differentially\nprivate (DP) synthetic data generation. While it achieves strong performance in\nsome domains (e.g., images and text), its behavior in others (e.g., tabular\ndata) is less consistent. To date, the only theoretical analysis of the\nconvergence of PE depends on unrealistic assumptions about both the algorithm's\nbehavior and the structure of the sensitive dataset. In this work, we develop a\nnew theoretical framework to explain PE's practical behavior and identify\nsufficient conditions for its convergence. For $d$-dimensional sensitive\ndatasets with $n$ data points from a bounded domain, we prove that PE produces\nan $(\\epsilon, \\delta)$-DP synthetic dataset with expected 1-Wasserstein\ndistance of order $\\tilde{O}(d(n\\epsilon)^{-1/d})$ from the original,\nestablishing worst-case convergence of the algorithm as $n \\to \\infty$. Our\nanalysis extends to general Banach spaces as well. We also connect PE to the\nPrivate Signed Measure Mechanism, a method for DP synthetic data generation\nthat has thus far not seen much practical adoption. We demonstrate the\npractical relevance of our theoretical findings in simulations."}
{"id": "2506.08619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08619", "abs": "https://arxiv.org/abs/2506.08619", "authors": ["Gon√ßalo Dias Pais", "Valter Piedade", "Moitreya Chatterjee", "Marcus Greiff", "Pedro Miraldo"], "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering", "comment": "Accepted in ECCV 2024", "summary": "Several variants of Neural Radiance Fields (NeRFs) have significantly\nimproved the accuracy of synthesized images and surface reconstruction of 3D\nscenes/objects. In all of these methods, a key characteristic is that none can\ntrain the neural network with every possible input data, specifically, every\npixel and potential 3D point along the projection rays due to scalability\nissues. While vanilla NeRFs uniformly sample both the image pixels and 3D\npoints along the projection rays, some variants focus only on guiding the\nsampling of the 3D points along the projection rays. In this paper, we leverage\nthe implicit surface representation of the foreground scene and model a\nprobability density function in a 3D image projection space to achieve a more\ntargeted sampling of the rays toward regions of interest, resulting in improved\nrendering. Additionally, a new surface reconstruction loss is proposed for\nimproved performance. This new loss fully explores the proposed 3D image\nprojection space model and incorporates near-to-surface and empty space\ncomponents. By integrating our novel sampling strategy and novel loss into\ncurrent state-of-the-art neural implicit surface renderers, we achieve more\naccurate and detailed 3D reconstructions and improved image rendering,\nespecially for the regions of interest in any given scene."}
{"id": "2506.08316", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08316", "abs": "https://arxiv.org/abs/2506.08316", "authors": ["Alan N. Amin", "Nate Gruver", "Andrew Gordon Wilson"], "title": "Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion", "comment": "Code available at: https://github.com/AlanNawzadAmin/SCUD", "summary": "Discrete diffusion models, like continuous diffusion models, generate\nhigh-quality samples by gradually undoing noise applied to datapoints with a\nMarkov process. Gradual generation in theory comes with many conceptual\nbenefits; for example, inductive biases can be incorporated into the noising\nMarkov process, and access to improved sampling algorithms. In practice,\nhowever, the consistently best performing discrete diffusion model is,\nsurprisingly, masking diffusion, which does not denoise gradually. Here we\nexplain the superior performance of masking diffusion by noting that it makes\nuse of a fundamental difference between continuous and discrete Markov\nprocesses: discrete Markov processes evolve by discontinuous jumps at a fixed\nrate and, unlike other discrete diffusion models, masking diffusion builds in\nthe known distribution of jump times and only learns where to jump to. We show\nthat we can similarly bake in the known distribution of jump times into any\ndiscrete diffusion model. The resulting models - schedule-conditioned discrete\ndiffusion (SCUD) - generalize classical discrete diffusion and masking\ndiffusion. By applying SCUD to models with noising processes that incorporate\ninductive biases on images, text, and protein data, we build models that\noutperform masking."}
{"id": "2506.08629", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08629", "abs": "https://arxiv.org/abs/2506.08629", "authors": ["Feixiang Du", "Shengkun Wu"], "title": "ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network", "comment": "16 pages, 2 figures, 4 tables", "summary": "In the past decade, Convolutional Neural Networks (CNNs) and Transformers\nhave achieved wide applicaiton in semantic segmentation tasks. Although CNNs\nwith Transformer models greatly improve performance, the global context\nmodeling remains inadequate. Recently, Mamba achieved great potential in vision\ntasks, showing its advantages in modeling long-range dependency. In this paper,\nwe propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,\ndubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based\nframework to address their complementary weaknesses. Specifically, We design a\nEnhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to\nimprove the representations ability of feature, We devise a Multi-Scale\nAttention Unit (MSAU) to integrate multi-scale feature aggregation, spatial\naggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion\nModule (FFM) merges diverse level feature, significantly enhancing segmented\naccuracy. Extensive experiments on two representative datasets demonstrate that\nthe proposed model excels in accuracy and efficiency balance, achieving 70.6%\nmIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M\nparameters and 8.27G FLOPs on a single RTX 3090 GPU platform."}
{"id": "2506.08326", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08326", "abs": "https://arxiv.org/abs/2506.08326", "authors": ["Xingbo Fu", "Zehong Wang", "Zihan Chen", "Jiazheng Li", "Yaochen Zhu", "Zhenyu Lei", "Cong Shen", "Yanfang Ye", "Chuxu Zhang", "Jundong Li"], "title": "Graph Prompting for Graph Learning Models: Recent Advances and Future Directions", "comment": "Accepted by KDD 2025 Tutorial/Survey Track", "summary": "Graph learning models have demonstrated great prowess in learning expressive\nrepresentations from large-scale graph data in a wide variety of real-world\nscenarios. As a prevalent strategy for training powerful graph learning models,\nthe \"pre-training, adaptation\" scheme first pre-trains graph learning models on\nunlabeled graph data in a self-supervised manner and then adapts them to\nspecific downstream tasks. During the adaptation phase, graph prompting emerges\nas a promising approach that learns trainable prompts while keeping the\npre-trained graph learning models unchanged. In this paper, we present a\nsystematic review of recent advancements in graph prompting. First, we\nintroduce representative graph pre-training methods that serve as the\nfoundation step of graph prompting. Next, we review mainstream techniques in\ngraph prompting and elaborate on how they design learnable prompts for graph\nprompting. Furthermore, we summarize the real-world applications of graph\nprompting from different domains. Finally, we discuss several open challenges\nin existing studies with promising future directions in this field."}
{"id": "2506.08632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08632", "abs": "https://arxiv.org/abs/2506.08632", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "comment": null, "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning."}
{"id": "2506.08337", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08337", "abs": "https://arxiv.org/abs/2506.08337", "authors": ["Juhyeok Choi", "Chenglin Fan"], "title": "A Simple Analysis of Discretization Error in Diffusion Models", "comment": null, "summary": "Diffusion models, formulated as discretizations of stochastic differential\nequations (SDEs), achieve state-of-the-art generative performance. However,\nexisting analyses of their discretization error often rely on complex\nprobabilistic tools. In this work, we present a simplified theoretical\nframework for analyzing the Euler--Maruyama discretization of\nvariance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models\n(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion\nprocess. Our approach leverages Gr\\\"onwall's inequality to derive a convergence\nrate of $ \\mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly\nstreamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise\nin the discretization can be replaced by a discrete random variable (e.g.,\nRademacher or uniform noise) without sacrificing convergence guarantees-an\ninsight with practical implications for efficient sampling. Experiments\nvalidate our theory, showing that (1) the error scales as predicted, (2)\ndiscrete noise achieves comparable sample quality to Gaussian noise, and (3)\nincorrect noise scaling degrades performance. By unifying simplified analysis\nand discrete noise substitution, our work bridges theoretical rigor with\npractical efficiency in diffusion-based generative modeling."}
{"id": "2506.08635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08635", "abs": "https://arxiv.org/abs/2506.08635", "authors": ["Siddhant Ranade", "Gon√ßalo Dias Pais", "Ross Tyler Whitaker", "Jacinto C. Nascimento", "Pedro Miraldo", "Srikumar Ramalingam"], "title": "SurfR: Surface Reconstruction with Multi-scale Attention", "comment": "Accepted in 3DV 2025", "summary": "We propose a fast and accurate surface reconstruction algorithm for\nunorganized point clouds using an implicit representation. Recent learning\nmethods are either single-object representations with small neural models that\nallow for high surface details but require per-object training or generalized\nrepresentations that require larger models and generalize to newer shapes but\nlack details, and inference is slow. We propose a new implicit representation\nfor general 3D shapes that is faster than all the baselines at their optimum\nresolution, with only a marginal loss in performance compared to the\nstate-of-the-art. We achieve the best accuracy-speed trade-off using three key\ncontributions. Many implicit methods extract features from the point cloud to\nclassify whether a query point is inside or outside the object. First, to speed\nup the reconstruction, we show that this feature extraction does not need to\nuse the query point at an early stage (lazy query). Second, we use a parallel\nmulti-scale grid representation to develop robust features for different noise\nlevels and input resolutions. Finally, we show that attention across scales can\nprovide improved reconstruction results."}
{"id": "2506.08340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08340", "abs": "https://arxiv.org/abs/2506.08340", "authors": ["Emo Todorov"], "title": "Dynamical System Optimization", "comment": null, "summary": "We develop an optimization framework centered around a core idea: once a\n(parametric) policy is specified, control authority is transferred to the\npolicy, resulting in an autonomous dynamical system. Thus we should be able to\noptimize policy parameters without further reference to controls or actions,\nand without directly using the machinery of approximate Dynamic Programming and\nReinforcement Learning. Here we derive simpler algorithms at the autonomous\nsystem level, and show that they compute the same quantities as policy\ngradients and Hessians, natural gradients, proximal methods. Analogs to\napproximate policy iteration and off-policy learning are also available. Since\npolicy parameters and other system parameters are treated uniformly, the same\nalgorithms apply to behavioral cloning, mechanism design, system\nidentification, learning of state estimators. Tuning of generative AI models is\nnot only possible, but is conceptually closer to the present framework than to\nReinforcement Learning."}
{"id": "2506.08640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08640", "abs": "https://arxiv.org/abs/2506.08640", "authors": ["Yichong Lu", "Yuzhuo Tian", "Zijin Jiang", "Yikun Zhao", "Yuanbo Yang", "Hao Ouyang", "Haoji Hu", "Huimin Yu", "Yujun Shen", "Yiyi Liao"], "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned", "comment": "Project Page: https://xdimlab.github.io/Orientation_Matters", "summary": "Humans intuitively perceive object shape and orientation from a single image,\nguided by strong priors about canonical poses. However, existing 3D generative\nmodels often produce misaligned results due to inconsistent training data,\nlimiting their usability in downstream tasks. To address this gap, we introduce\nthe task of orientation-aligned 3D object generation: producing 3D objects from\nsingle images with consistent orientations across categories. To facilitate\nthis, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D\nmodels spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two\nrepresentative 3D generative models based on multi-view diffusion and 3D\nvariational autoencoder frameworks to produce aligned objects that generalize\nwell to unseen objects across various categories. Experimental results\ndemonstrate the superiority of our method over post-hoc alignment approaches.\nFurthermore, we showcase downstream applications enabled by our aligned object\ngeneration, including zero-shot object orientation estimation via\nanalysis-by-synthesis and efficient arrow-based object rotation manipulation."}
{"id": "2506.08347", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.08347", "abs": "https://arxiv.org/abs/2506.08347", "authors": ["Yinan Huang", "Haoteng Ying", "Eli Chien", "Rongzhe Wei", "Pan Li"], "title": "Differentially Private Relational Learning with Entity-level Privacy Guarantees", "comment": null, "summary": "Learning with relational and network-structured data is increasingly vital in\nsensitive domains where protecting the privacy of individual entities is\nparamount. Differential Privacy (DP) offers a principled approach for\nquantifying privacy risks, with DP-SGD emerging as a standard mechanism for\nprivate model training. However, directly applying DP-SGD to relational\nlearning is challenging due to two key factors: (i) entities often participate\nin multiple relations, resulting in high and difficult-to-control sensitivity;\nand (ii) relational learning typically involves multi-stage, potentially\ncoupled (interdependent) sampling procedures that make standard privacy\namplification analyses inapplicable. This work presents a principled framework\nfor relational learning with formal entity-level DP guarantees. We provide a\nrigorous sensitivity analysis and introduce an adaptive gradient clipping\nscheme that modulates clipping thresholds based on entity occurrence frequency.\nWe also extend the privacy amplification results to a tractable subclass of\ncoupled sampling, where the dependence arises only through sample sizes. These\ncontributions lead to a tailored DP-SGD variant for relational data with\nprovable privacy guarantees. Experiments on fine-tuning text encoders over\ntext-attributed network-structured relational data demonstrate the strong\nutility-privacy trade-offs of our approach. Our code is available at\nhttps://github.com/Graph-COM/Node_DP."}
{"id": "2506.08649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08649", "abs": "https://arxiv.org/abs/2506.08649", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Youwei Lu"], "title": "Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization", "comment": null, "summary": "Video memorability refers to the ability of videos to be recalled after\nviewing, playing a crucial role in creating content that remains memorable.\nExisting models typically focus on extracting multimodal features to predict\nvideo memorability scores but often fail to fully utilize motion cues. The\nrepresentation of motion features is compromised during the fine-tuning phase\nof the motion feature extractor due to a lack of labeled data. In this paper,\nwe introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal\nvideo memorability prediction model designed to enhance the representation of\nmotion features. We tackle the challenge of improving motion feature\nrepresentation by leveraging text description similarities across videos to\nestablish positive and negative motion sample sets for a given target. This\nenhancement allows the model to learn similar feature representations for\nsemantically related motion content, resulting in more accurate memorability\npredictions. Our model achieves state-of-the-art performance on two video\nmemorability prediction datasets. Moreover, the potential applications of video\nmemorability prediction have been underexplored. To address this gap, we\npresent Memorability Weighted Correction for Video Summarization (MWCVS), using\nvideo memorability prediction to reduce subjectivity in video summarization\nlabels. Experimental results on two video summarization datasets demonstrate\nthe effectiveness of MWCVS, showcasing the promising applications of video\nmemorability prediction."}
{"id": "2506.08353", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08353", "abs": "https://arxiv.org/abs/2506.08353", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "An Adaptive Method Stabilizing Activations for Enhanced Generalization", "comment": null, "summary": "We introduce AdaAct, a novel optimization algorithm that adjusts learning\nrates according to activation variance. Our method enhances the stability of\nneuron outputs by incorporating neuron-wise adaptivity during the training\nprocess, which subsequently leads to better generalization -- a complementary\napproach to conventional activation regularization methods. Experimental\nresults demonstrate AdaAct's competitive performance across standard image\nclassification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing\nit with other state-of-the-art methods. Importantly, AdaAct effectively bridges\nthe gap between the convergence speed of Adam and the strong generalization\ncapabilities of SGD, all while maintaining competitive execution times. Code is\navailable at https://github.com/hseung88/adaact."}
{"id": "2506.08650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08650", "abs": "https://arxiv.org/abs/2506.08650", "authors": ["Peter Gr√∂nquist", "Stepan Tulyakov", "Dengxin Dai"], "title": "Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping", "comment": null, "summary": "Achieving consistent color reproduction across multiple cameras is essential\nfor seamless image fusion and Image Processing Pipeline (ISP) compatibility in\nmodern devices, but it is a challenging task due to variations in sensors and\noptics. Existing raw-to-raw conversion methods face limitations such as poor\nadaptability to changing illumination, high computational costs, or impractical\nrequirements such as simultaneous camera operation and overlapping\nfields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,\nphysically-informed approach that simulates raw images under specified\nillumination to estimate transformations between devices. The NPM effectively\nadapts to varying illumination conditions, can be initialized with physical\nmeasurements, and supports training with or without paired data. Experiments on\npublic datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent\nstate-of-the-art methods, providing robust chromatic consistency across\ndifferent sensors and optical systems."}
{"id": "2506.08360", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08360", "abs": "https://arxiv.org/abs/2506.08360", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation", "comment": null, "summary": "Adaptive gradient methods are computationally efficient and converge quickly,\nbut they often suffer from poor generalization. In contrast, second-order\nmethods enhance convergence and generalization but typically incur high\ncomputational and memory costs. In this work, we introduce NysAct, a scalable\nfirst-order gradient preconditioning method that strikes a balance between\nstate-of-the-art first-order and second-order optimization methods. NysAct\nleverages an eigenvalue-shifted Nystrom method to approximate the activation\ncovariance matrix, which is used as a preconditioning matrix, significantly\nreducing time and memory complexities with minimal impact on test accuracy. Our\nexperiments show that NysAct not only achieves improved test accuracy compared\nto both first-order and second-order methods but also demands considerably less\ncomputational resources than existing second-order methods. Code is available\nat https://github.com/hseung88/nysact."}
{"id": "2506.08666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08666", "abs": "https://arxiv.org/abs/2506.08666", "authors": ["Wenzhuo Liu", "Fei Zhu", "Haiyang Guo", "Longhui Wei", "Cheng-Lin Liu"], "title": "LLaVA-c: Continual Improved Visual Instruction Tuning", "comment": null, "summary": "Multimodal models like LLaVA-1.5 achieve state-of-the-art visual\nunderstanding through visual instruction tuning on multitask datasets, enabling\nstrong instruction-following and multimodal performance. However, multitask\nlearning faces challenges such as task balancing, requiring careful adjustment\nof data proportions, and expansion costs, where new tasks risk catastrophic\nforgetting and need costly retraining. Continual learning provides a promising\nalternative to acquiring new knowledge incrementally while preserving existing\ncapabilities. However, current methods prioritize task-specific performance,\nneglecting base model degradation from overfitting to specific instructions,\nwhich undermines general capabilities. In this work, we propose a simple but\neffective method with two modifications on LLaVA-1.5: spectral-aware\nconsolidation for improved task balance and unsupervised inquiry regularization\nto prevent base model degradation. We evaluate both general and task-specific\nperformance across continual pretraining and fine-tuning. Experiments\ndemonstrate that LLaVA-c consistently enhances standard benchmark performance\nand preserves general capabilities. For the first time, we show that\ntask-by-task continual learning can achieve results that match or surpass\nmultitask joint learning. The code will be publicly released."}
{"id": "2506.08365", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2506.08365", "abs": "https://arxiv.org/abs/2506.08365", "authors": ["Cheng Tan", "Zhenxiao Cao", "Zhangyang Gao", "Siyuan Li", "Yufei Huang", "Stan Z. Li"], "title": "AlphaFold Database Debiasing for Robust Inverse Folding", "comment": "Under review", "summary": "The AlphaFold Protein Structure Database (AFDB) offers unparalleled\nstructural coverage at near-experimental accuracy, positioning it as a valuable\nresource for data-driven protein design. However, its direct use in training\ndeep models that are sensitive to fine-grained atomic geometry, such as inverse\nfolding, exposes a critical limitation. Comparative analysis of structural\nfeature distributions reveals that AFDB structures exhibit distinct statistical\nregularities, reflecting a systematic geometric bias that deviates from the\nconformational diversity found in experimentally determined structures from the\nProtein Data Bank (PDB). While AFDB structures are cleaner and more idealized,\nPDB structures capture the intrinsic variability and physical realism essential\nfor generalization in downstream tasks. To address this discrepancy, we\nintroduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct\nnative-like conformations from intentionally corrupted backbone geometries. By\ntraining the model to recover plausible structural states, DeSAE implicitly\ncaptures a more robust and natural structural manifold. At inference, applying\nDeSAE to AFDB structures produces debiased structures that significantly\nimprove inverse folding performance across multiple benchmarks. This work\nhighlights the critical impact of subtle systematic biases in predicted\nstructures and presents a principled framework for debiasing, significantly\nboosting the performance of structure-based learning tasks like inverse\nfolding."}
{"id": "2506.08678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08678", "abs": "https://arxiv.org/abs/2506.08678", "authors": ["Juan Yeo", "Soonwoo Cha", "Jiwoo Song", "Hyunbin Jin", "Taesup Kim"], "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction", "comment": null, "summary": "Vision-language models such as CLIP have recently propelled open-vocabulary\ndense prediction tasks by enabling recognition of a broad range of visual\nconcepts. However, CLIP still struggles with fine-grained, region-level\nunderstanding, hindering its effectiveness on these dense prediction tasks. We\nidentify two pivotal factors required to address this limitation: semantic\ncoherence and fine-grained vision-language alignment. Current adaptation\nmethods often improve fine-grained alignment at the expense of semantic\ncoherence, and often rely on extra modules or supervised fine-tuning. To\novercome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel\napproach that simultaneously enhances semantic coherence and fine-grained\nalignment by leveraging own knowledge of a model across all representation\nlevels. Unlike prior methods, ATAS uses only unlabeled images and an internal\nself-distillation process to refine representations of CLIP vision encoders,\npreserving local semantic consistency while sharpening local detail\nrecognition. On open-vocabulary object detection and semantic segmentation\nbenchmarks, ATAS achieves substantial performance gains, outperforming baseline\nCLIP models. These results validate the effectiveness of our approach and\nunderscore the importance of jointly maintaining semantic coherence and\nfine-grained alignment for advanced open-vocabulary dense prediction."}
{"id": "2506.08379", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08379", "abs": "https://arxiv.org/abs/2506.08379", "authors": ["Yurun Yuan", "Tengyang Xie"], "title": "Reinforce LLM Reasoning through Multi-Agent Reflection", "comment": "International Conference on Machine Learning (ICML), 2025", "summary": "Leveraging more test-time computation has proven to be an effective way to\nboost the reasoning capabilities of large language models (LLMs). Among various\nmethods, the verify-and-improve paradigm stands out for enabling dynamic\nsolution exploration and feedback incorporation. However, existing approaches\noften suffer from restricted feedback spaces and lack of coordinated training\nof different parties, leading to suboptimal performance. To address this, we\nmodel this multi-turn refinement process as a Markov Decision Process and\nintroduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement\nlearning algorithm that trains an actor-critic LLM system to iteratively refine\nanswers via direct preference learning on self-generated data. Theoretically,\nDPSDP can match the performance of any policy within the training distribution.\nEmpirically, we instantiate DPSDP with various base models and show\nimprovements on both in- and out-of-distribution benchmarks. For example, on\nbenchmark MATH 500, majority voting over five refinement steps increases\nfirst-turn accuracy from 58.2% to 63.2% with Ministral-based models. An\nablation study further confirms the benefits of multi-agent collaboration and\nout-of-distribution generalization."}
{"id": "2506.08690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08690", "abs": "https://arxiv.org/abs/2506.08690", "authors": ["Hugo Porta", "Emanuele Dalsasso", "Jessica L. McCarty", "Devis Tuia"], "title": "CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities", "comment": "34 pages, 11 figures", "summary": "Canada experienced in 2023 one of the most severe wildfire seasons in recent\nhistory, causing damage across ecosystems, destroying communities, and emitting\nlarge quantities of CO2. This extreme wildfire season is symptomatic of a\nclimate-change-induced increase in the length and severity of the fire season\nthat affects the boreal ecosystem. Therefore, it is critical to empower\nwildfire management in boreal communities with better mitigation solutions.\nWildfire probability maps represent an important tool for understanding the\nlikelihood of wildfire occurrence and the potential severity of future\nwildfires. The massive increase in the availability of Earth observation data\nhas enabled the development of deep learning-based wildfire forecasting models,\naiming at providing precise wildfire probability maps at different spatial and\ntemporal scales. A main limitation of such methods is their reliance on\ncoarse-resolution environmental drivers and satellite products, leading to\nwildfire occurrence prediction of reduced resolution, typically around $\\sim\n0.1${\\deg}. This paper presents a benchmark dataset: CanadaFireSat, and\nbaseline methods for high-resolution: 100 m wildfire forecasting across Canada,\nleveraging multi-modal data from high-resolution multi-spectral satellite\nimages (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and\nenvironmental factors (ERA5 reanalysis data). Our experiments consider two\nmajor deep learning architectures. We observe that using multi-modal temporal\ninputs outperforms single-modal temporal inputs across all metrics, achieving a\npeak performance of 60.3% in F1 score for the 2023 wildfire season, a season\nnever seen during model training. This demonstrates the potential of\nmulti-modal deep learning models for wildfire forecasting at high-resolution\nand continental scale."}
{"id": "2506.08383", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.08383", "abs": "https://arxiv.org/abs/2506.08383", "authors": ["Jiaqi Chen", "Rongbin Ye"], "title": "Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest", "comment": null, "summary": "With the rapid expansion of Internet of Things (IoT) networks, detecting\nmalicious traffic in real-time has become a critical cybersecurity challenge.\nThis research addresses the detection challenges by presenting a comprehensive\nempirical analysis of machine learning techniques for malware detection using\nthe IoT-23 dataset provided by the Stratosphere Laboratory. We address the\nsignificant class imbalance within the dataset through three resampling\nstrategies. We implement and compare a few machine learning techniques. Our\nfindings demonstrate that the combination of appropriate imbalance treatment\ntechniques with ensemble methods, particularly gcForest, achieves better\ndetection performance compared to traditional approaches. This work contributes\nsignificantly to the development of more intelligent and efficient automated\nthreat detection systems for IoT environments, helping to secure critical\ninfrastructure against sophisticated cyber attacks while optimizing\ncomputational resource usage."}
{"id": "2506.08691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08691", "abs": "https://arxiv.org/abs/2506.08691", "authors": ["Congzhi Zhang", "Jiawei Peng", "Zhenglin Wang", "Yilong Lai", "Haowen Sun", "Heng Chang", "Fei Ma", "Weijiang Yu"], "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism", "comment": "Accepted by ACL 2025 main", "summary": "Large Vision-Language Models (LVLMs) have shown exceptional performance in\nmultimodal tasks, but their effectiveness in complex visual reasoning is still\nconstrained, especially when employing Chain-of-Thought prompting techniques.\nIn this paper, we propose VReST, a novel training-free approach that enhances\nReasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.\nVReST meticulously traverses the reasoning landscape by establishing a search\ntree, where each node encapsulates a reasoning step, and each path delineates a\ncomprehensive reasoning sequence. Our innovative multimodal Self-Reward\nmechanism assesses the quality of reasoning steps by integrating the utility of\nsub-questions, answer correctness, and the relevance of vision-language clues,\nall without the need for additional models. VReST surpasses current prompting\nmethods and secures state-of-the-art performance across three multimodal\nmathematical reasoning benchmarks. Furthermore, it substantiates the efficacy\nof test-time scaling laws in multimodal tasks, offering a promising direction\nfor future research."}
{"id": "2506.08388", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08388", "abs": "https://arxiv.org/abs/2506.08388", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "title": "Reinforcement Learning Teachers of Test Time Scaling", "comment": "Preprint", "summary": "Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework."}
{"id": "2506.08694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08694", "abs": "https://arxiv.org/abs/2506.08694", "authors": ["Mohammadreza Salehi", "Shashanka Venkataramanan", "Ioana Simion", "Efstratios Gavves", "Cees G. M. Snoek", "Yuki M Asano"], "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning", "comment": "preprint", "summary": "Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main"}
{"id": "2506.08397", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08397", "abs": "https://arxiv.org/abs/2506.08397", "authors": ["Vamshika Sutar", "Amandeep Singh", "Rohitash Chandra"], "title": "Spatiotemporal deep learning models for detection of rapid intensification in cyclones", "comment": null, "summary": "Cyclone rapid intensification is the rapid increase in cyclone wind\nintensity, exceeding a threshold of 30 knots, within 24 hours. Rapid\nintensification is considered an extreme event during a cyclone, and its\noccurrence is relatively rare, contributing to a class imbalance in the\ndataset. A diverse array of factors influences the likelihood of a cyclone\nundergoing rapid intensification, further complicating the task for\nconventional machine learning models. In this paper, we evaluate deep learning,\nensemble learning and data augmentation frameworks to detect cyclone rapid\nintensification based on wind intensity and spatial coordinates. We note that\nconventional data augmentation methods cannot be utilised for generating\nspatiotemporal patterns replicating cyclones that undergo rapid\nintensification. Therefore, our framework employs deep learning models to\ngenerate spatial coordinates and wind intensity that replicate cyclones to\naddress the class imbalance problem of rapid intensification. We also use a\ndeep learning model for the classification module within the data augmentation\nframework to differentiate between rapid and non-rapid intensification events\nduring a cyclone. Our results show that data augmentation improves the results\nfor rapid intensification detection in cyclones, and spatial coordinates play a\ncritical role as input features to the given models. This paves the way for\nresearch in synthetic data generation for spatiotemporal data with extreme\nevents."}
{"id": "2506.08699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08699", "abs": "https://arxiv.org/abs/2506.08699", "authors": ["Frederik Hagelskjaer"], "title": "ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds", "comment": "6 pages, 5 figures, 4 tables", "summary": "This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose\nestimation network for colorless point clouds. The pose estimation is\ncalculated from center and top points of the object, predicted by the neural\nnetwork. The network is trained on synthetic data, and tested on a benchmark\ndataset, where it demonstrates state-of-the-art performance and outperforms all\ncolorless methods. The network is able to run inference in only 250\nmilliseconds making it usable in many scenarios. Project page with code at\narrowpose.github.io"}
{"id": "2506.08409", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.08409", "abs": "https://arxiv.org/abs/2506.08409", "authors": ["Fred Xu", "Song Jiang", "Zijie Huang", "Xiao Luo", "Shichang Zhang", "Adrian Chen", "Yizhou Sun"], "title": "FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion", "comment": null, "summary": "Taxonomy Expansion, which models complex concepts and their relations, can be\nformulated as a set representation learning task. The generalization of set,\nfuzzy set, incorporates uncertainty and measures the information within a\nsemantic concept, making it suitable for concept modeling. Existing works\nusually model sets as vectors or geometric objects such as boxes, which are not\nclosed under set operations. In this work, we propose a sound and efficient\nformulation of set representation learning based on its volume approximation as\na fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),\nsatisfies all set operations and compactly approximates the underlying fuzzy\nset, hence preserving information while being efficient to learn, relying on\nminimum neural architecture. We empirically demonstrate the power of FUSE on\nthe task of taxonomy expansion, where FUSE achieves remarkable improvements up\nto 23% compared with existing baselines. Our work marks the first attempt to\nunderstand and efficiently compute the embeddings of fuzzy sets."}
{"id": "2506.08704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08704", "abs": "https://arxiv.org/abs/2506.08704", "authors": ["Xiaohan Zhang", "Sitong Wang", "Yushen Yan", "Yi Yang", "Mingda Xu", "Qi Liu"], "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering", "comment": null, "summary": "High-quality novel view synthesis for large-scale scenes presents a\nchallenging dilemma in 3D computer vision. Existing methods typically partition\nlarge scenes into multiple regions, reconstruct a 3D representation using\nGaussian splatting for each region, and eventually merge them for novel view\nrendering. They can accurately render specific scenes, yet they do not\ngeneralize effectively for two reasons: (1) rigid spatial partition techniques\nstruggle with arbitrary camera trajectories, and (2) the merging of regions\nresults in Gaussian overlap to distort texture details. To address these\nchallenges, we propose TraGraph-GS, leveraging a trajectory graph to enable\nhigh-precision rendering for arbitrarily large-scale scenes. We present a\nspatial partitioning method for large-scale scenes based on graphs, which\nincorporates a regularization constraint to enhance the rendering of textures\nand distant objects, as well as a progressive rendering strategy to mitigate\nartifacts caused by Gaussian overlap. Experimental results demonstrate its\nsuperior performance both on four aerial and four ground datasets and highlight\nits remarkable efficiency: our method achieves an average improvement of 1.86\ndB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to\nstate-of-the-art approaches."}
{"id": "2506.08412", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08412", "abs": "https://arxiv.org/abs/2506.08412", "authors": ["Saraa Ali", "Aleksandr Khizhik", "Stepan Svirin", "Artem Ryzhikov", "Denis Derkach"], "title": "Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics", "comment": null, "summary": "The application of machine learning (ML) algorithms in the intelligent\ndiagnosis of three-phase engines has the potential to significantly enhance\ndiagnostic performance and accuracy. Traditional methods largely rely on\nsignature analysis, which, despite being a standard practice, can benefit from\nthe integration of advanced ML techniques. In our study, we innovate by\ncombining ML algorithms with a novel unsupervised anomaly generation\nmethodology that takes into account the engine physics model. We propose\nSignature-Guided Data Augmentation (SGDA), an unsupervised framework that\nsynthesizes physically plausible faults directly in the frequency domain of\nhealthy current signals. Guided by Motor Current Signature Analysis, SGDA\ncreates diverse and realistic anomalies without resorting to computationally\nintensive simulations. This hybrid approach leverages the strengths of both\nsupervised ML and unsupervised signature analysis, achieving superior\ndiagnostic accuracy and reliability along with wide industrial application. The\nfindings highlight the potential of our approach to contribute significantly to\nthe field of engine diagnostics, offering a robust and efficient solution for\nreal-world applications."}
{"id": "2506.08710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08710", "abs": "https://arxiv.org/abs/2506.08710", "authors": ["Mengjiao Ma", "Qi Ma", "Yue Li", "Jiahuan Cheng", "Runyi Yang", "Bin Ren", "Nikola Popovic", "Mingqiang Wei", "Nicu Sebe", "Luc Van Gool", "Theo Gevers", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting", "comment": "15 pages, codes, data and benchmark will be released", "summary": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient\nencoding of scene geometry, appearance, and semantics. Moreover, grounding\nlanguage in 3D scenes has proven to be an effective strategy for 3D scene\nunderstanding. Current Language Gaussian Splatting line of work fall into three\nmain groups: (i) per-scene optimization-based, (ii) per-scene\noptimization-free, and (iii) generalizable approach. However, most of them are\nevaluated only on rendered 2D views of a handful of scenes and viewpoints close\nto the training views, limiting ability and insight into holistic 3D\nunderstanding. To address this gap, we propose the first large-scale benchmark\nthat systematically assesses these three groups of methods directly in 3D\nspace, evaluating on 1060 scenes across three indoor datasets and one outdoor\ndataset. Benchmark results demonstrate a clear advantage of the generalizable\nparadigm, particularly in relaxing the scene-specific limitation, enabling fast\nfeed-forward inference on novel scenes, and achieving superior segmentation\nperformance. We further introduce GaussianWorld-49K a carefully curated 3DGS\ndataset comprising around 49K diverse indoor and outdoor scenes obtained from\nmultiple sources, with which we demonstrate the generalizable approach could\nharness strong data priors. Our codes, benchmark, and datasets will be made\npublic to accelerate research in generalizable 3DGS scene understanding."}
{"id": "2506.08415", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.08415", "abs": "https://arxiv.org/abs/2506.08415", "authors": ["Licong Lin", "Jingfeng Wu", "Peter L. Bartlett"], "title": "Improved Scaling Laws in Linear Regression via Data Reuse", "comment": null, "summary": "Neural scaling laws suggest that the test error of large language models\ntrained online decreases polynomially as the model size and data size increase.\nHowever, such scaling can be unsustainable when running out of new data. In\nthis work, we show that data reuse can improve existing scaling laws in linear\nregression. Specifically, we derive sharp test error bounds on $M$-dimensional\nlinear models trained by multi-pass stochastic gradient descent (multi-pass\nSGD) on $N$ data with sketched features. Assuming that the data covariance has\na power-law spectrum of degree $a$, and that the true parameter follows a prior\nwith an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show\nthat multi-pass SGD achieves a test error of $\\Theta(M^{1-b} + L^{(1-b)/a})$,\nwhere $L \\lesssim N^{a/b}$ is the number of iterations. In the same setting,\none-pass SGD only attains a test error of $\\Theta(M^{1-b} + N^{(1-b)/a})$ (see\ne.g., Lin et al., 2024). This suggests an improved scaling law via data reuse\n(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are\nalso provided to verify our theoretical findings."}
{"id": "2506.08729", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08729", "abs": "https://arxiv.org/abs/2506.08729", "authors": ["Dieuwertje Alblas", "Patryk Rygiel", "Julian Suk", "Kaj O. Kappe", "Marieke Hofman", "Christoph Brune", "Kak Khee Yeung", "Jelmer M. Wolterink"], "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces", "comment": null, "summary": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the\nabdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current\nclinical guidelines recommend elective surgical repair when the maximum AAA\ndiameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet\nthese criteria are periodically monitored, with surveillance intervals based on\nthe maximum AAA diameter. However, this diameter does not take into account the\ncomplex relation between the 3D AAA shape and its growth, making standardized\nintervals potentially unfit. Personalized AAA growth predictions could improve\nmonitoring strategies. We propose to use an SE(3)-symmetric transformer model\nto predict AAA growth directly on the vascular model surface enriched with\nlocal, multi-physical features. In contrast to other works which have\nparameterized the AAA shape, this representation preserves the vascular\nsurface's anatomical structure and geometric fidelity. We train our model using\na longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24\nAAA patients at irregularly sampled intervals. After training, our model\npredicts AAA growth to the next scan moment with a median diameter error of\n1.18 mm. We further demonstrate our model's utility to identify whether a\npatient will become eligible for elective repair within two years (acc = 0.93).\nFinally, we evaluate our model's generalization on an external validation set\nconsisting of 25 CTAs from 7 AAA patients from a different hospital. Our\nresults show that local directional AAA growth prediction from the vascular\nsurface is feasible and may contribute to personalized surveillance strategies."}
{"id": "2506.08417", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08417", "abs": "https://arxiv.org/abs/2506.08417", "authors": ["Qingmao Yao", "Zhichao Lei", "Tianyuan Chen", "Ziyue Yuan", "Xuefan Chen", "Jianxiang Liu", "Faguo Wu", "Xiao Zhang"], "title": "Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood", "comment": "ICLR 2025", "summary": "Offline Reinforcement Learning (RL) struggles with distributional shifts,\nleading to the $Q$-value overestimation for out-of-distribution (OOD) actions.\nExisting methods address this issue by imposing constraints; however, they\noften become overly conservative when evaluating OOD regions, which constrains\nthe $Q$-function generalization. This over-constraint issue results in poor\n$Q$-value estimation and hinders policy improvement. In this paper, we\nintroduce a novel approach to achieve better $Q$-value estimation by enhancing\n$Q$-function generalization in OOD regions within Convex Hull and its\nNeighborhood (CHN). Under the safety generalization guarantees of the CHN, we\npropose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by\nsmoothing them with neighboring in-sample $Q$-values. We theoretically show\nthat SBO approximates true $Q$-values for both in-sample and OOD actions within\nthe CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),\nempirically alleviates the over-constraint issue, achieving near-accurate\n$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing\nstate-of-the-art methods in both performance and computational efficiency."}
{"id": "2506.08735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08735", "abs": "https://arxiv.org/abs/2506.08735", "authors": ["Yuhang Wang", "Jun Li", "Zhijian Wu", "Jianhua Xu"], "title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba", "comment": null, "summary": "Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba."}
{"id": "2506.08419", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08419", "abs": "https://arxiv.org/abs/2506.08419", "authors": ["Ruichen Jiang", "Ali Kavis", "Aryan Mokhtari"], "title": "Online Learning-guided Learning Rate Adaptation via Gradient Alignment", "comment": "24 pages, 5 figures", "summary": "The performance of an optimizer on large-scale deep learning models depends\ncritically on fine-tuning the learning rate, often requiring an extensive grid\nsearch over base learning rates, schedules, and other hyperparameters. In this\npaper, we propose a principled framework called GALA (Gradient Alignment-based\nLearning rate Adaptation), which dynamically adjusts the learning rate by\ntracking the alignment between consecutive gradients and using a local\ncurvature estimate. Guided by the convergence analysis, we formulate the\nproblem of selecting the learning rate as a one-dimensional online learning\nproblem. When paired with an online learning algorithm such as\nFollow-the-Regularized-Leader, our method produces a flexible, adaptive\nlearning rate schedule that tends to increase when consecutive gradients are\naligned and decrease otherwise. We establish a data-adaptive convergence rate\nfor normalized SGD equipped with GALA in the smooth, nonconvex setting.\nEmpirically, common optimizers such as SGD and Adam, when augmented with GALA,\ndemonstrate robust performance across a wide range of initial learning rates\nand perform competitively without the need for tuning."}
{"id": "2506.08772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08772", "abs": "https://arxiv.org/abs/2506.08772", "authors": ["Jiayi Song", "Kaiyu Li", "Xiangyong Cao", "Deyu Meng"], "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation", "comment": null, "summary": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nWe propose that Vision Foundation Models (VFMs), pre-trained on vast and\ndiverse datasets, possess robust generalization capabilities that can\neffectively bridge this distribution gap and provide strong semantic priors for\nSSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and\nFusion), a novel framework that leverages the powerful semantic knowledge\nembedded in VFMs to guide semi-supervised learning in remote sensing.\nSpecifically, RS-MTDF employs multiple frozen VFMs (\\textit{e.g.}, DINOv2 and\nCLIP) as expert teachers, utilizing feature-level distillation to align student\nfeatures with their robust representations. To further enhance discriminative\npower, the distilled knowledge is seamlessly fused into the student decoder.\nExtensive experiments on three challenging remote sensing datasets (ISPRS\nPotsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves\nstate-of-the-art performance. Notably, our method outperforms existing\napproaches across various label ratios on LoveDA and secures the highest IoU in\nthe majority of semantic categories. These results underscore the efficacy of\nmulti-teacher VFM guidance in significantly enhancing both generalization and\nsemantic understanding for remote sensing segmentation. Ablation studies\nfurther validate the contribution of each proposed module."}
{"id": "2506.08426", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.08426", "abs": "https://arxiv.org/abs/2506.08426", "authors": ["Zheng Lin", "Zhe Chen", "Xianhao Chen", "Wei Ni", "Yue Gao"], "title": "HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems", "comment": "16 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2403.13101", "summary": "Split federated learning (SFL) has emerged as a promising paradigm to\ndemocratize machine learning (ML) on edge devices by enabling layer-wise model\npartitioning. However, existing SFL approaches suffer significantly from the\nstraggler effect due to the heterogeneous capabilities of edge devices. To\naddress the fundamental challenge, we propose adaptively controlling batch\nsizes (BSs) and model splitting (MS) for edge devices to overcome resource\nheterogeneity. We first derive a tight convergence bound of SFL that quantifies\nthe impact of varied BSs and MS on learning performance. Based on the\nconvergence bound, we propose HASFL, a heterogeneity-aware SFL framework\ncapable of adaptively controlling BS and MS to balance communication-computing\nlatency and training convergence in heterogeneous edge networks. Extensive\nexperiments with various datasets validate the effectiveness of HASFL and\ndemonstrate its superiority over state-of-the-art benchmarks."}
{"id": "2506.08777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08777", "abs": "https://arxiv.org/abs/2506.08777", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "comment": null, "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a\ncornerstone for many 3D vision tasks, enabling effective learning from\nlarge-scale unannotated data. At the scene level, existing SSL methods often\nincorporate volume rendering into the pre-training framework, using RGB-D\nimages as reconstruction signals to facilitate cross-modal learning. This\nstrategy promotes alignment between 2D and 3D modalities and enables the model\nto benefit from rich visual cues in the RGB-D inputs. However, these approaches\nare limited by their reliance on implicit scene representations and high memory\ndemands. Furthermore, since their reconstruction objectives are applied only in\n2D space, they often fail to capture underlying 3D geometric structures. To\naddress these challenges, we propose Gaussian2Scene, a novel scene-level SSL\nframework that leverages the efficiency and explicit nature of 3D Gaussian\nSplatting (3DGS) for pre-training. The use of 3DGS not only alleviates the\ncomputational burden associated with volume rendering but also supports direct\n3D scene reconstruction, thereby enhancing the geometric understanding of the\nbackbone network. Our approach follows a progressive two-stage training\nstrategy. In the first stage, a dual-branch masked autoencoder learns both 2D\nand 3D scene representations. In the second stage, we initialize training with\nreconstructed point clouds and further supervise learning using the geometric\nlocations of Gaussian primitives and rendered RGB images. This process\nreinforces both geometric and cross-modal learning. We demonstrate the\neffectiveness of Gaussian2Scene across several downstream 3D object detection\ntasks, showing consistent improvements over existing pre-training methods."}
{"id": "2506.08435", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08435", "abs": "https://arxiv.org/abs/2506.08435", "authors": ["Mingyuan Fan", "Fuyi Wang", "Cen Chen", "Jianying Zhou"], "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings", "comment": "Accepted to Usenix Security 2025", "summary": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods."}
{"id": "2506.08780", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08780", "abs": "https://arxiv.org/abs/2506.08780", "authors": ["Isaac Corley", "Lakshay Sharma", "Ruth Crasto"], "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models", "comment": null, "summary": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L."}
{"id": "2506.08438", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08438", "abs": "https://arxiv.org/abs/2506.08438", "authors": ["Yuchen Wu", "Xinyi Zhong", "Zhuoran Yang"], "title": "Learning to Lead: Incentivizing Strategic Agents in the Dark", "comment": "81 pages, 7 figures", "summary": "We study an online learning version of the generalized principal-agent model,\nwhere a principal interacts repeatedly with a strategic agent possessing\nprivate types, private rewards, and taking unobservable actions. The agent is\nnon-myopic, optimizing a discounted sum of future rewards and may strategically\nmisreport types to manipulate the principal's learning. The principal,\nobserving only her own realized rewards and the agent's reported types, aims to\nlearn an optimal coordination mechanism that minimizes strategic regret. We\ndevelop the first provably sample-efficient algorithm for this challenging\nsetting. Our approach features a novel pipeline that combines (i) a delaying\nmechanism to incentivize approximately myopic agent behavior, (ii) an\ninnovative reward angle estimation framework that uses sector tests and a\nmatching procedure to recover type-dependent reward functions, and (iii) a\npessimistic-optimistic LinUCB algorithm that enables the principal to explore\nefficiently while respecting the agent's incentive constraints. We establish a\nnear optimal $\\tilde{O}(\\sqrt{T}) $ regret bound for learning the principal's\noptimal policy, where $\\tilde{O}(\\cdot) $ omits logarithmic factors. Our\nresults open up new avenues for designing robust online learning algorithms for\na wide range of game-theoretic settings involving private types and strategic\nagents."}
{"id": "2506.08784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08784", "abs": "https://arxiv.org/abs/2506.08784", "authors": ["Jongyub Seok", "Chanjin Kang"], "title": "HomographyAD: Deep Anomaly Detection Using Self Homography Learning", "comment": null, "summary": "Anomaly detection (AD) is a task that distinguishes normal and abnormal data,\nwhich is important for applying automation technologies of the manufacturing\nfacilities. For MVTec dataset that is a representative AD dataset for\nindustrial environment, many recent works have shown remarkable performances.\nHowever, the existing anomaly detection works have a limitation of showing good\nperformance for fully-aligned datasets only, unlike real-world industrial\nenvironments. To solve this limitation, we propose HomographyAD, a novel deep\nanomaly detection methodology based on the ImageNet-pretrained network, which\nis specially designed for actual industrial dataset. Specifically, we first\nsuggest input foreground alignment using the deep homography estimation method.\nIn addition, we fine-tune the model by self homography learning to learn\nadditional shape information from normal samples. Finally, we conduct anomaly\ndetection based on the measure of how far the feature of test sample is from\nthe distribution of the extracted normal features. By applying our proposed\nmethod to various existing AD approaches, we show performance enhancement\nthrough extensive experiments."}
{"id": "2506.08441", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08441", "abs": "https://arxiv.org/abs/2506.08441", "authors": ["Anh N. Nhu", "Sanghyun Son", "Ming Lin"], "title": "Time-Aware World Model for Adaptive Prediction and Control", "comment": "Paper accepted to ICML 2025", "summary": "In this work, we introduce the Time-Aware World Model (TAWM), a model-based\napproach that explicitly incorporates temporal dynamics. By conditioning on the\ntime-step size, {\\Delta}t, and training over a diverse range of {\\Delta}t\nvalues -- rather than sampling at a fixed time-step -- TAWM learns both high-\nand low-frequency task dynamics across diverse control problems. Grounded in\nthe information-theoretic insight that the optimal sampling rate depends on a\nsystem's underlying dynamics, this time-aware formulation improves both\nperformance and data efficiency. Empirical evaluations show that TAWM\nconsistently outperforms conventional models across varying observation rates\nin a variety of control tasks, using the same number of training samples and\niterations. Our code can be found online at:\ngithub.com/anh-nn01/Time-Aware-World-Model."}
{"id": "2506.08793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.08793", "abs": "https://arxiv.org/abs/2506.08793", "authors": ["Zhuoran Zheng"], "title": "A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory", "comment": "report", "summary": "This paper presents a novel partial differential equation (PDE) framework for\nsingle-image dehazing. By integrating the atmospheric scattering model with\nnonlocal regularization and dark channel prior, we propose the improved PDE: \\[\n-\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A) \\]\nwhere $D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$ is the edge-preserving\ndiffusion coefficient, $G(u)$ is the Gaussian convolution operator, and\n$\\lambda(t)$ is the adaptive regularization parameter based on transmission map\n$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\\Omega)$\nusing Lax-Milgram theorem, and implement an efficient fixed-point iteration\nscheme accelerated by PyTorch GPU computation. The experimental results\ndemonstrate that this method is a promising deghazing solution that can be\ngeneralized to the deep model paradigm."}
{"id": "2506.08460", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08460", "abs": "https://arxiv.org/abs/2506.08460", "authors": ["Yihong Guo", "Yu Yang", "Pan Xu", "Anqi Liu"], "title": "MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning", "comment": null, "summary": "We study the off-dynamics offline reinforcement learning problem, where the\ngoal is to learn a policy from offline datasets collected from source and\ntarget domains with mismatched transition. Existing off-dynamics offline RL\nmethods typically either filter source transitions that resemble those of the\ntarget domain or apply reward augmentation to source data, both constrained by\nthe limited transitions available from the target domain. As a result, the\nlearned policy is unable to explore target domain beyond the offline datasets.\nWe propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that\naddresses this limitation by enabling exploration of the target domain via\nlearned dynamics. MOBODY generates new synthetic transitions in the target\ndomain through model rollouts, which are used as data augmentation during\noffline policy learning. Unlike existing model-based methods that learn\ndynamics from a single domain, MOBODY tackles the challenge of mismatched\ndynamics by leveraging both source and target datasets. Directly merging these\ndatasets can bias the learned model toward source dynamics. Instead, MOBODY\nlearns target dynamics by discovering a shared latent representation of states\nand transitions across domains through representation learning. To stabilize\ntraining, MOBODY incorporates a behavior cloning loss that regularizes the\npolicy. Specifically, we introduce a Q-weighted behavior cloning loss that\nregularizes the policy toward actions with high target-domain Q-values, rather\nthan uniformly imitating all actions in the dataset. These Q-values are learned\nfrom an enhanced target dataset composed of offline target data, augmented\nsource data, and rollout data from the learned target dynamics. We evaluate\nMOBODY on MuJoCo benchmarks and show that it significantly outperforms\nstate-of-the-art baselines, with especially pronounced improvements in\nchallenging scenarios."}
{"id": "2506.08796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08796", "abs": "https://arxiv.org/abs/2506.08796", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "comment": null, "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm."}
{"id": "2506.08463", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08463", "abs": "https://arxiv.org/abs/2506.08463", "authors": ["Zhishuai Liu", "Yu Yang", "Ruhan Wang", "Pan Xu", "Dongruo Zhou"], "title": "How to Provably Improve Return Conditioned Supervised Learning?", "comment": "25 pages, 4 figures, 12 tables", "summary": "In sequential decision-making problems, Return-Conditioned Supervised\nLearning (RCSL) has gained increasing recognition for its simplicity and\nstability in modern decision-making tasks. Unlike traditional offline\nreinforcement learning (RL) algorithms, RCSL frames policy learning as a\nsupervised learning problem by taking both the state and return as input. This\napproach eliminates the instability often associated with temporal difference\n(TD) learning in offline RL. However, RCSL has been criticized for lacking the\nstitching property, meaning its performance is inherently limited by the\nquality of the policy used to generate the offline dataset. To address this\nlimitation, we propose a principled and simple framework called Reinforced\nRCSL. The key innovation of our framework is the introduction of a concept we\ncall the in-distribution optimal return-to-go. This mechanism leverages our\npolicy to identify the best achievable in-dataset future return based on the\ncurrent state, avoiding the need for complex return augmentation techniques.\nOur theoretical analysis demonstrates that Reinforced RCSL can consistently\noutperform the standard RCSL approach. Empirical results further validate our\nclaims, showing significant performance improvements across a range of\nbenchmarks."}
{"id": "2506.08797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08797", "abs": "https://arxiv.org/abs/2506.08797", "authors": ["Ziyao Huang", "Zixiang Zhou", "Juan Cao", "Yifeng Ma", "Yi Chen", "Zejing Rao", "Zhiyong Xu", "Hongmei Wang", "Qin Lin", "Yuan Zhou", "Qinglin Lu", "Fan Tang"], "title": "HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation", "comment": null, "summary": "To address key limitations in human-object interaction (HOI) video generation\n-- specifically the reliance on curated motion data, limited generalization to\nnovel objects/scenarios, and restricted accessibility -- we introduce\nHunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.\nHunyuanVideo-HOMA enhances controllability and reduces dependency on precise\ninputs through sparse, decoupled motion guidance. It encodes appearance and\nmotion signals into the dual input space of a multimodal diffusion transformer\n(MMDiT), fusing them within a shared context space to synthesize temporally\nconsistent and physically plausible interactions. To optimize training, we\nintegrate a parameter-space HOI adapter initialized from pretrained MMDiT\nweights, preserving prior knowledge while enabling efficient adaptation, and a\nfacial cross-attention adapter for anatomically accurate audio-driven lip\nsynchronization. Extensive experiments confirm state-of-the-art performance in\ninteraction naturalness and generalization under weak supervision. Finally,\nHunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and\ninteractive object manipulation, supported by a user-friendly demo interface.\nThe project page is at https://anonymous.4open.science/w/homa-page-0FBE/."}
{"id": "2506.08464", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08464", "abs": "https://arxiv.org/abs/2506.08464", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature", "comment": null, "summary": "Second-order optimization methods for training neural networks, such as KFAC,\nexhibit superior convergence by utilizing curvature information of loss\nlandscape. However, it comes at the expense of high computational burden. In\nthis work, we analyze the two components that constitute the layer-wise Fisher\ninformation matrix (FIM) used in KFAC: the Kronecker factors related to\nactivations and pre-activation gradients. Based on empirical observations on\ntheir eigenspectra, we propose efficient approximations for them, resulting in\na computationally efficient optimization method called MAC. To the best of our\nknowledge, MAC is the first algorithm to apply the Kronecker factorization to\nthe FIM of attention layers used in transformers and explicitly integrate\nattention scores into the preconditioning. We also study the convergence\nproperty of MAC on nonlinear neural networks and provide two conditions under\nwhich it converges to global minima. Our extensive evaluations on various\nnetwork architectures and datasets show that the proposed method outperforms\nKFAC and other state-of-the-art methods in terms of accuracy, end-to-end\ntraining time, and memory usage. Code is available at\nhttps://github.com/hseung88/mac."}
{"id": "2506.08809", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.08809", "abs": "https://arxiv.org/abs/2506.08809", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "comment": null, "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions."}
{"id": "2506.08473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08473", "abs": "https://arxiv.org/abs/2506.08473", "authors": ["Shuo Yang", "Qihui Zhang", "Yuyang Liu", "Yue Huang", "Xiaojun Jia", "Kunpeng Ning", "Jiayu Yao", "Jigang Wang", "Hailiang Dai", "Yibing Song", "Li Yuan"], "title": "AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin", "comment": null, "summary": "Large language models (LLMs) are vulnerable to safety risks during\nfine-tuning, where small amounts of malicious or harmless data can compromise\nsafeguards. In this paper, building on the concept of alignment direction --\ndefined by the weight difference between aligned and unaligned models -- we\nobserve that perturbations along this direction preserve model safety. In\ncontrast, perturbations along directions orthogonal to this alignment are\nstrongly linked to harmful direction perturbations, rapidly degrading safety\nand framing the parameter space as a narrow safety basin. Based on this\ninsight, we propose a methodology for safety fine-tuning called AsFT (Anchoring\nSafety in Fine-Tuning), which integrates a regularization term into the\ntraining objective. This term uses the alignment direction as an anchor to\nsuppress updates in harmful directions, ensuring that fine-tuning is\nconstrained within the narrow safety basin. Extensive experiments on multiple\ndatasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by\n7.60 percent, improving model performance by 3.44 percent, and maintaining\nrobust performance across various experimental settings. Code is available at\nhttps://github.com/PKU-YuanGroup/AsFT"}
{"id": "2506.08817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08817", "abs": "https://arxiv.org/abs/2506.08817", "authors": ["Shuyi Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Pengwei Wang", "Zhongyuan Wang", "Hongxuan Ma", "Shanghang Zhang"], "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought", "comment": null, "summary": "Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ ."}
{"id": "2506.08475", "categories": ["cs.LG", "cs.CE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.08475", "abs": "https://arxiv.org/abs/2506.08475", "authors": ["Xiaolong He", "Yeonjong Shin", "Anthony Gruber", "Sohyeon Jung", "Kookjin Lee", "Youngsoo Choi"], "title": "Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems", "comment": null, "summary": "We propose an efficient thermodynamics-informed latent space dynamics\nidentification (tLaSDI) framework for the reduced-order modeling of parametric\nnonlinear dynamical systems. This framework integrates autoencoders for\ndimensionality reduction with newly developed parametric GENERIC\nformalism-informed neural networks (pGFINNs), which enable efficient learning\nof parametric latent dynamics while preserving key thermodynamic principles\nsuch as free energy conservation and entropy generation across the parameter\nspace. To further enhance model performance, a physics-informed active learning\nstrategy is incorporated, leveraging a greedy, residual-based error indicator\nto adaptively sample informative training data, outperforming uniform sampling\nat equivalent computational cost. Numerical experiments on the Burgers'\nequation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed\nmethod achieves up to 3,528x speed-up with 1-3% relative errors, and\nsignificant reduction in training (50-90%) and inference (57-61%) cost.\nMoreover, the learned latent space dynamics reveal the underlying thermodynamic\nbehavior of the system, offering valuable insights into the physical-space\ndynamics."}
{"id": "2506.08835", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08835", "abs": "https://arxiv.org/abs/2506.08835", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Sta≈Ñczak", "Aishwarya Agrawal"], "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "comment": null, "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies."}
{"id": "2506.08505", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2506.08505", "abs": "https://arxiv.org/abs/2506.08505", "authors": ["Shahaf Bassan", "Yizhak Yisrael Elboher", "Tobias Ladner", "Matthias Althoff", "Guy Katz"], "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations", "comment": "To appear in ICML 2025", "summary": "Despite significant advancements in post-hoc explainability techniques for\nneural networks, many current methods rely on heuristics and do not provide\nformally provable guarantees over the explanations provided. Recent work has\nshown that it is possible to obtain explanations with formal guarantees by\nidentifying subsets of input features that are sufficient to determine that\npredictions remain unchanged using neural network verification techniques.\nDespite the appeal of these explanations, their computation faces significant\nscalability challenges. In this work, we address this gap by proposing a novel\nabstraction-refinement technique for efficiently computing provably sufficient\nexplanations of neural network predictions. Our method abstracts the original\nlarge neural network by constructing a substantially reduced network, where a\nsufficient explanation of the reduced network is also provably sufficient for\nthe original network, hence significantly speeding up the verification process.\nIf the explanation is in sufficient on the reduced network, we iteratively\nrefine the network size by gradually increasing it until convergence. Our\nexperiments demonstrate that our approach enhances the efficiency of obtaining\nprovably sufficient explanations for neural network predictions while\nadditionally providing a fine-grained interpretation of the network's\npredictions across different abstraction levels."}
{"id": "2506.08849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08849", "abs": "https://arxiv.org/abs/2506.08849", "authors": ["Jingguo Qu", "Xinyang Han", "Tonghuan Xiao", "Jia Ai", "Juan Wu", "Tong Zhao", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Yingƒ±nst"], "title": "Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis", "comment": null, "summary": "Medical ultrasonography is an essential imaging technique for examining\nsuperficial organs and tissues, including lymph nodes, breast, and thyroid. It\nemploys high-frequency ultrasound waves to generate detailed images of the\ninternal structures of the human body. However, manually contouring regions of\ninterest in these images is a labor-intensive task that demands expertise and\noften results in inconsistent interpretations among individuals.\nVision-language foundation models, which have excelled in various computer\nvision applications, present new opportunities for enhancing ultrasound image\nanalysis. Yet, their performance is hindered by the significant differences\nbetween natural and medical imaging domains. This research seeks to overcome\nthese challenges by developing domain adaptation methods for vision-language\nfoundation models. In this study, we explore the fine-tuning pipeline for\nvision-language foundation models by utilizing large language model as text\nrefiner with special-designed adaptation strategies and task-driven heads. Our\napproach has been extensively evaluated on six ultrasound datasets and two\ntasks: segmentation and classification. The experimental results show that our\nmethod can effectively improve the performance of vision-language foundation\nmodels for ultrasound image analysis, and outperform the existing\nstate-of-the-art vision-language and pure foundation models. The source code of\nthis study is available at\n\\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}."}
{"id": "2506.08514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08514", "abs": "https://arxiv.org/abs/2506.08514", "authors": ["Jacob Piland", "Chris Sweet", "Adam Czakja"], "title": "DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training", "comment": null, "summary": "Class Activation Mapping (CAM) and its gradient-based variants (e.g.,\nGradCAM) have become standard tools for explaining Convolutional Neural Network\n(CNN) predictions. However, these approaches typically focus on individual\nlogits, while for neural networks using softmax, the class membership\nprobability estimates depend \\textit{only} on the \\textit{differences} between\nlogits, not on their absolute values. This disconnect leaves standard CAMs\nvulnerable to adversarial manipulation, such as passive fooling, where a model\nis trained to produce misleading CAMs without affecting decision performance.\nWe introduce \\textbf{Salience-Hoax Activation Maps (SHAMs)}, an\n\\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM\nrobustness under adversarial conditions. To address the passive fooling\nvulnerability, we then propose \\textbf{DiffGradCAM}, a novel, lightweight, and\ncontrastive approach to class activation mapping that is both non-suceptible to\npassive fooling, but also matches the output of standard CAM methods such as\nGradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a\nnew framework for probing and improving the robustness of saliency-based\nexplanations. We validate both contributions across multi-class tasks with few\nand many classes."}
{"id": "2506.08854", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08854", "abs": "https://arxiv.org/abs/2506.08854", "authors": ["Junzhuo Liu", "Markus Eckstein", "Zhixiang Wang", "Friedrich Feuerhake", "Dorit Merhof"], "title": "Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning", "comment": "20 pages, 7 figures", "summary": "Spatial transcriptomics is a technology that captures gene expression levels\nat different spatial locations, widely used in tumor microenvironment analysis\nand molecular profiling of histopathology, providing valuable insights into\nresolving gene expression and clinical diagnosis of cancer. Due to the high\ncost of data acquisition, large-scale spatial transcriptomics data remain\nchallenging to obtain. In this study, we develop a contrastive learning-based\ndeep learning method to predict spatially resolved gene expression from\nwhole-slide images. Evaluation across six different disease datasets\ndemonstrates that, compared to existing studies, our method improves Pearson\nCorrelation Coefficient (PCC) in the prediction of highly expressed genes,\nhighly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%\nrespectively. Further analysis indicates that our method preserves gene-gene\ncorrelations and applies to datasets with limited samples. Additionally, our\nmethod exhibits potential in cancer tissue localization based on biomarker\nexpression."}
{"id": "2506.08516", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08516", "abs": "https://arxiv.org/abs/2506.08516", "authors": ["Mouadh Yagoubi", "David Danan", "Milad Leyli-Abadi", "Ahmed Mazari", "Jean-Patrick Brunet", "Abbas Kabalan", "Fabien Casenave", "Yuxin Ma", "Giovanni Catalani", "Jean Fesquet", "Jacob Helwig", "Xuan Zhang", "Haiyang Yu", "Xavier Bertrand", "Frederic Tost", "Michael Baurheim", "Joseph Morlier", "Shuiwang Ji"], "title": "NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis", "comment": null, "summary": "The integration of machine learning (ML) into the physical sciences is\nreshaping computational paradigms, offering the potential to accelerate\ndemanding simulations such as computational fluid dynamics (CFD). Yet,\npersistent challenges in accuracy, generalization, and physical consistency\nhinder the practical deployment of ML models in scientific domains. To address\nthese limitations and systematically benchmark progress, we organized the\nML4CFD competition, centered on surrogate modeling for aerodynamic simulations\nover two-dimensional airfoils. The competition attracted over 240 teams, who\nwere provided with a curated dataset generated via OpenFOAM and evaluated\nthrough a multi-criteria framework encompassing predictive accuracy, physical\nfidelity, computational efficiency, and out-of-distribution generalization.\nThis retrospective analysis reviews the competition outcomes, highlighting\nseveral approaches that outperformed baselines under our global evaluation\nscore. Notably, the top entry exceeded the performance of the original OpenFOAM\nsolver on aggregate metrics, illustrating the promise of ML-based surrogates to\noutperform traditional solvers under tailored criteria. Drawing from these\nresults, we analyze the key design principles of top submissions, assess the\nrobustness of our evaluation framework, and offer guidance for future\nscientific ML challenges."}
{"id": "2506.08862", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08862", "abs": "https://arxiv.org/abs/2506.08862", "authors": ["Zike Wu", "Qi Yan", "Xuanyu Yi", "Lele Wang", "Renjie Liao"], "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams", "comment": null, "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat."}
{"id": "2506.08523", "categories": ["cs.LG", "cond-mat.dis-nn", "nlin.CD", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2506.08523", "abs": "https://arxiv.org/abs/2506.08523", "authors": ["Pedro Jim√©nez-Gonz√°lez", "Miguel C. Soriano", "Lucas Lacasa"], "title": "Leveraging chaos in the training of artificial neural networks", "comment": null, "summary": "Traditional algorithms to optimize artificial neural networks when confronted\nwith a supervised learning task are usually exploitation-type relaxational\ndynamics such as gradient descent (GD). Here, we explore the dynamics of the\nneural network trajectory along training for unconventionally large learning\nrates. We show that for a region of values of the learning rate, the GD\noptimization shifts away from purely exploitation-like algorithm into a regime\nof exploration-exploitation balance, as the neural network is still capable of\nlearning but the trajectory shows sensitive dependence on initial conditions --\nas characterized by positive network maximum Lyapunov exponent --.\nInterestingly, the characteristic training time required to reach an acceptable\naccuracy in the test set reaches a minimum precisely in such learning rate\nregion, further suggesting that one can accelerate the training of artificial\nneural networks by locating at the onset of chaos. Our results -- initially\nillustrated for the MNIST classification task -- qualitatively hold for a range\nof supervised learning tasks, learning architectures and other hyperparameters,\nand showcase the emergent, constructive role of transient chaotic dynamics in\nthe training of artificial neural networks."}
{"id": "2506.08887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08887", "abs": "https://arxiv.org/abs/2506.08887", "authors": ["Leqi Shen", "Guoqiang Gong", "Tianxiang Hao", "Tao He", "Yifeng Zhang", "Pengzhang Liu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval", "comment": "CVPR 2025", "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA."}
{"id": "2506.08533", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.08533", "abs": "https://arxiv.org/abs/2506.08533", "authors": ["Nihal Acharya Adde", "Alexandra Gianzina", "Hanno Gottschalk", "Andreas Ebert"], "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)", "comment": "Published at ESANN 2025 Conference", "summary": "This paper introduces Evolutionary Multi-Objective Network Architecture\nSearch (EMNAS) for the first time to optimize neural network architectures in\nlarge-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses\ngenetic algorithms to automate network design, tailored to enhance rewards and\nreduce model size without compromising performance. Additionally,\nparallelization techniques are employed to accelerate the search, and\nteacher-student methodologies are implemented to ensure scalable optimization.\nThis research underscores the potential of transfer learning as a robust\nframework for optimizing performance across iterative learning processes by\neffectively leveraging knowledge from earlier generations to enhance learning\nefficiency and stability in subsequent generations. Experimental results\ndemonstrate that tailored EMNAS outperforms manually designed models, achieving\nhigher rewards with fewer parameters. The findings of these strategies\ncontribute positively to EMNAS for RL in autonomous driving, advancing the\nfield toward better-performing networks suitable for real-world scenarios."}
{"id": "2506.08894", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08894", "abs": "https://arxiv.org/abs/2506.08894", "authors": ["Yunzhi Zhang", "Carson Murtuza-Lanier", "Zizhang Li", "Yilun Du", "Jiajun Wu"], "title": "Product of Experts for Visual Generation", "comment": "Project page: https://product-of-experts.github.io/", "summary": "Modern neural models capture rich priors and have complementary knowledge\nover shared data domains, e.g., images and videos. Integrating diverse\nknowledge from multiple sources -- including visual generative models, visual\nlanguage models, and sources with human-crafted knowledge such as graphics\nengines and physics simulators -- remains under-explored. We propose a Product\nof Experts (PoE) framework that performs inference-time knowledge composition\nfrom heterogeneous models. This training-free approach samples from the product\ndistribution across experts via Annealed Importance Sampling (AIS). Our\nframework shows practical benefits in image and video synthesis tasks, yielding\nbetter controllability than monolithic methods and additionally providing\nflexible user interfaces for specifying visual generation goals."}
{"id": "2506.08551", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08551", "abs": "https://arxiv.org/abs/2506.08551", "authors": ["Panlong Wu", "Ting Wang", "Yifei Zhong", "Haoqi Zhang", "Zitong Wang", "Fangxin Wang"], "title": "DeepForm: Reasoning Large Language Model for Communication System Formulation", "comment": null, "summary": "Communication system formulation is critical for advancing 6G and future\nwireless technologies, yet it remains a complex, expertise-intensive task.\nWhile Large Language Models (LLMs) offer potential, existing general-purpose\nmodels often lack the specialized domain knowledge, nuanced reasoning\ncapabilities, and access to high-quality, domain-specific training data\nrequired for adapting a general LLM into an LLM specially for communication\nsystem formulation. To bridge this gap, we introduce DeepForm, the first\nreasoning LLM specially for automated communication system formulation. We\npropose the world-first large-scale, open-source dataset meticulously curated\nfor this domain called Communication System Formulation Reasoning Corpus\n(CSFRC). Our framework employs a two-stage training strategy: first, Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;\nsecond, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based\non ReMax, to cultivate advanced modeling capabilities and elicit sophisticated\nreasoning patterns like self-correction and verification. Extensive experiments\ndemonstrate that our model achieves state-of-the-art performance, significantly\noutperforming larger proprietary LLMs on diverse senerios. We will release\nrelated resources to foster further research in this area after the paper is\naccepted."}
{"id": "2506.08896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08896", "abs": "https://arxiv.org/abs/2506.08896", "authors": ["Negin Ghamsarian", "Raphael Sznitman", "Klaus Schoeffmann", "Jens Kowal"], "title": "WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos", "comment": "9 pages, 6 figures", "summary": "To meet the growing demand for systematic surgical training, wetlab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wetlab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wetlab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wetlab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse https://www.synapse.org/Synapse:syn66401174/files."}
{"id": "2506.08572", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08572", "abs": "https://arxiv.org/abs/2506.08572", "authors": ["Waiss Azizian", "Michael Kirchhof", "Eugene Ndiaye", "Louis Bethune", "Michal Klein", "Pierre Ablin", "Marco Cuturi"], "title": "The Geometries of Truth Are Orthogonal Across Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive generalization\ncapabilities across various tasks, but their claim to practical relevance is\nstill mired by concerns on their reliability. Recent works have proposed\nexamining the activations produced by an LLM at inference time to assess\nwhether its answer to a question is correct. Some works claim that a \"geometry\nof truth\" can be learned from examples, in the sense that the activations that\ngenerate correct answers can be distinguished from those leading to mistakes\nwith a linear classifier. In this work, we underline a limitation of these\napproaches: we observe that these \"geometries of truth\" are intrinsically\ntask-dependent and fail to transfer across tasks. More precisely, we show that\nlinear classifiers trained across distinct tasks share little similarity and,\nwhen trained with sparsity-enforcing regularizers, have almost disjoint\nsupports. We show that more sophisticated approaches (e.g., using mixtures of\nprobes and tasks) fail to overcome this limitation, likely because activation\nvectors commonly used to classify answers form clearly separated clusters when\nexamined across tasks."}
{"id": "2506.08900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08900", "abs": "https://arxiv.org/abs/2506.08900", "authors": ["Jos√© Morano", "Botond Fazekas", "Emese S√ºkei", "Ronald Fecso", "Taha Emre", "Markus Gumpinger", "Georg Faustmann", "Marzieh Oghbaie", "Ursula Schmidt-Erfurth", "Hrvoje Bogunoviƒá"], "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis", "comment": null, "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE."}
{"id": "2506.08574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08574", "abs": "https://arxiv.org/abs/2506.08574", "authors": ["Alvise Dei Rossi", "Matteo Metaldi", "Michal Bechny", "Irina Filchenko", "Julia van der Meer", "Markus H. Schmidt", "Claudio L. A. Bassetti", "Athina Tzovara", "Francesca D. Faraci", "Luigi Fiorillo"], "title": "SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models", "comment": "41 pages, 4 Figures, 7 Tables", "summary": "Despite advances in deep learning for automatic sleep staging, clinical\nadoption remains limited due to challenges in fair model evaluation,\ngeneralization across diverse datasets, model bias, and variability in human\nannotations. We present SLEEPYLAND, an open-source sleep staging evaluation\nframework designed to address these barriers. It includes more than 22'0000\nhours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain\n(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,\nand hardware setups. We release pre-trained models based on high-performing SoA\narchitectures and evaluate them under standardized conditions across single-\nand multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble\ncombining models across architectures and channel setups via soft voting.\nSOMNUS achieves robust performance across twenty-four different datasets, with\nmacro-F1 scores between 68.7% and 87.2%, outperforming individual models in\n94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including\ncases where compared models were trained ID while SOMNUS treated the same data\nas OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked\nto age, gender, AHI, and PLMI, showing that while ensemble improves robustness,\nno model architecture consistently minimizes bias in performance and clinical\nmarkers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,\nDOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on\nDOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus\nthan any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA\ncohorts). Finally, we introduce ensemble disagreement metrics - entropy and\ninter-model divergence based - predicting regions of scorer disagreement with\nROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty."}
{"id": "2506.08906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08906", "abs": "https://arxiv.org/abs/2506.08906", "authors": ["Peilin Yu", "Yuwei Wu", "Zhi Gao", "Xiaomeng Fan", "Shuo Yang", "Yunde Jia"], "title": "Hyperbolic Dual Feature Augmentation for Open-Environment", "comment": "arXiv admin note: text overlap with arXiv:2207.03824,\n  arXiv:2304.11855 by other authors", "summary": "Feature augmentation generates novel samples in the feature space, providing\nan effective way to enhance the generalization ability of learning algorithms\nwith hyperbolic geometry. Most hyperbolic feature augmentation is confined to\nclosed-environment, assuming the number of classes is fixed (\\emph{i.e.}, seen\nclasses) and generating features only for these classes. In this paper, we\npropose a hyperbolic dual feature augmentation method for open-environment,\nwhich augments features for both seen and unseen classes in the hyperbolic\nspace. To obtain a more precise approximation of the real data distribution for\nefficient training, (1) we adopt a neural ordinary differential equation\nmodule, enhanced by meta-learning, estimating the feature distributions of both\nseen and unseen classes; (2) we then introduce a regularizer to preserve the\nlatent hierarchical structures of data in the hyperbolic space; (3) we also\nderive an upper bound for the hyperbolic dual augmentation loss, allowing us to\ntrain a hyperbolic model using infinite augmentations for seen and unseen\nclasses. Extensive experiments on five open-environment tasks:\nclass-incremental learning, few-shot open-set recognition, few-shot learning,\nzero-shot learning, and general image classification, demonstrate that our\nmethod effectively enhances the performance of hyperbolic algorithms in\nopen-environment."}
{"id": "2506.08577", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08577", "abs": "https://arxiv.org/abs/2506.08577", "authors": ["Nicholas A. Pearson", "Francesca Cairoli", "Luca Bortolussi", "Davide Russo", "Francesca Zanello"], "title": "Diffusion-based Time Series Forecasting for Sewerage Systems", "comment": "Accepted for presentation at the 13th Urban Drainage Modelling\n  Conference, Innsbruck (Austria), September 2025", "summary": "We introduce a novel deep learning approach that harnesses the power of\ngenerative artificial intelligence to enhance the accuracy of contextual\nforecasting in sewerage systems. By developing a diffusion-based model that\nprocesses multivariate time series data, our system excels at capturing complex\ncorrelations across diverse environmental signals, enabling robust predictions\neven during extreme weather events. To strengthen the model's reliability, we\nfurther calibrate its predictions with a conformal inference technique,\ntailored for probabilistic time series data, ensuring that the resulting\nprediction intervals are statistically reliable and cover the true target\nvalues with a desired confidence level. Our empirical tests on real sewerage\nsystem data confirm the model's exceptional capability to deliver reliable\ncontextual predictions, maintaining accuracy even under severe weather\nconditions."}
{"id": "2506.08908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08908", "abs": "https://arxiv.org/abs/2506.08908", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "comment": null, "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released."}
{"id": "2506.08600", "categories": ["cs.LG", "cs.SC", "math.AC"], "pdf": "https://arxiv.org/pdf/2506.08600", "abs": "https://arxiv.org/abs/2506.08600", "authors": ["Hiroshi Kera", "Shun Arakawa", "Yuta Sato"], "title": "CALT: A Library for Computer Algebra with Transformer", "comment": "ISSAC 2025 Short Communications", "summary": "Recent advances in artificial intelligence have demonstrated the learnability\nof symbolic computation through end-to-end deep learning. Given a sufficient\nnumber of examples of symbolic expressions before and after the target\ncomputation, Transformer models - highly effective learners of\nsequence-to-sequence functions - can be trained to emulate the computation.\nThis development opens up several intriguing challenges and new research\ndirections, which require active contributions from the symbolic computation\ncommunity. In this work, we introduce Computer Algebra with Transformer (CALT),\na user-friendly Python library designed to help non-experts in deep learning\ntrain models for symbolic computation tasks."}
{"id": "2506.08915", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08915", "abs": "https://arxiv.org/abs/2506.08915", "authors": ["Ananthu Aniraj", "Cassio F. Dantas", "Dino Ienco", "Diego Marcos"], "title": "Inherently Faithful Attention Maps for Vision Transformers", "comment": null, "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds."}
{"id": "2506.08604", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.08604", "abs": "https://arxiv.org/abs/2506.08604", "authors": ["Giacomo Baldan", "Qiang Liu", "Alberto Guardone", "Nils Thuerey"], "title": "Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation", "comment": null, "summary": "Generative machine learning methods, such as diffusion models and flow\nmatching, have shown great potential in modeling complex system behaviors and\nbuilding efficient surrogate models. However, these methods typically learn the\nunderlying physics implicitly from data. We propose Physics-Based Flow Matching\n(PBFM), a novel generative framework that explicitly embeds physical\nconstraints, both PDE residuals and algebraic relations, into the flow matching\nobjective. We also introduce temporal unrolling at training time that improves\nthe accuracy of the final, noise-free sample prediction. Our method jointly\nminimizes the flow matching loss and the physics-based residual loss without\nrequiring hyperparameter tuning of their relative weights. Additionally, we\nanalyze the role of the minimum noise level, $\\sigma_{\\min}$, in the context of\nphysical constraints and evaluate a stochastic sampling strategy that helps to\nreduce physical residuals. Through extensive benchmarks on three representative\nPDE problems, we show that our approach yields up to an $8\\times$ more accurate\nphysical residuals compared to FM, while clearly outperforming existing\nalgorithms in terms of distributional accuracy. PBFM thus provides a principled\nand efficient framework for surrogate modeling, uncertainty quantification, and\naccelerated simulation in physics and engineering applications."}
{"id": "2506.08927", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08927", "abs": "https://arxiv.org/abs/2506.08927", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "comment": null, "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts."}
{"id": "2506.08607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08607", "abs": "https://arxiv.org/abs/2506.08607", "authors": ["Kiran Purohit", "V Venktesh", "Sourangshu Bhattacharya", "Avishek Anand"], "title": "Sample Efficient Demonstration Selection for In-Context Learning", "comment": "Accepted at ICML 2025 , 24 pages", "summary": "The in-context learning paradigm with LLMs has been instrumental in advancing\na wide range of natural language processing tasks. The selection of few-shot\nexamples (exemplars / demonstration samples) is essential for constructing\neffective prompts under context-length budget constraints. In this paper, we\nformulate the exemplar selection task as a top-m best arms identification\nproblem. A key challenge in this setup is the exponentially large number of\narms that need to be evaluated to identify the m-best arms. We propose CASE\n(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient\nselective exploration strategy that maintains a shortlist of \"challenger\" arms,\nwhich are current candidates for the top-m arms. In each iteration, only one of\nthe arms from this shortlist or the current topm set is pulled, thereby\nreducing sample complexity and, consequently, the number of LLM evaluations.\nFurthermore, we model the scores of exemplar subsets (arms) using a\nparameterized linear scoring function, leading to stochastic linear bandits\nsetting. CASE achieves remarkable efficiency gains of up to 7x speedup in\nruntime while requiring 7x fewer LLM calls (87% reduction) without sacrificing\nperformance compared to state-of-the-art exemplar selection methods. We release\nour code and data at https://github.com/kiranpurohit/CASE"}
{"id": "2506.08933", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08933", "abs": "https://arxiv.org/abs/2506.08933", "authors": ["Wendong Bu", "Yang Wu", "Qifan Yu", "Minghe Gao", "Bingchen Miao", "Zhenkui Zhang", "Kaihang Pan", "Yunfei Li", "Mengze Li", "Wei Ji", "Juncheng Li", "Siliang Tang", "Yueting Zhuang"], "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities", "comment": "Accepted by ICML 2025 (Oral)", "summary": "As multimodal large language models (MLLMs) advance, MLLM-based virtual\nagents have demonstrated remarkable performance. However, existing benchmarks\nface significant limitations, including uncontrollable task complexity,\nextensive manual annotation with limited scenarios, and a lack of\nmultidimensional evaluation. In response to these challenges, we introduce\nOmniBench, a self-generating, cross-platform, graph-based benchmark with an\nautomated pipeline for synthesizing tasks of controllable complexity through\nsubtask composition. To evaluate the diverse capabilities of virtual agents on\nthe graph, we further present OmniEval, a multidimensional evaluation framework\nthat includes subtask-level evaluation, graph-based metrics, and comprehensive\ntests across 10 capabilities. Our synthesized dataset contains 36k\ngraph-structured tasks across 20 scenarios, achieving a 91\\% human acceptance\nrate. Training on our graph-structured data shows that it can more efficiently\nguide agents compared to manually annotated data. We conduct multidimensional\nevaluations for various open-source and closed-source models, revealing their\nperformance across various capabilities and paving the way for future\nadvancements. Our project is available at https://omni-bench.github.io/."}
{"id": "2506.08618", "categories": ["cs.LG", "cond-mat.mes-hall", "cond-mat.other", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08618", "abs": "https://arxiv.org/abs/2506.08618", "authors": ["Xianquan Yan", "Hakan Akg√ºn", "Kenji Kawaguchi", "N. Duane Loh", "Ching Hua Lee"], "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset", "comment": "39 pages, 13 figures, 3 tables. Code & pipeline:\n  [https://github.com/sarinstein-yan/Poly2Graph] Dataset:\n  [https://github.com/sarinstein-yan/HSG-12M] Dataset released under CC BY 4.0", "summary": "Existing graph benchmarks assume non-spatial, simple edges, collapsing\nphysically distinct paths into a single link. We introduce HSG-12M, the first\nlarge-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a\nmetric space where multiple geometrically distinct trajectories between two\nnodes are retained as separate edges. HSG-12M contains 11.6 million static and\n5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401\ncharacteristic-polynomial classes, derived from 177 TB of spectral potential\ndata. Each graph encodes the full geometry of a 1-D crystal's energy spectrum\non the complex plane, producing diverse, physics-grounded topologies that\ntranscend conventional node-coordinate datasets. To enable future extensions,\nwe release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that\nmaps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with\npopular GNNs expose new challenges in learning from multi-edge geometry at\nscale. Beyond its practical utility, we show that spectral graphs serve as\nuniversal topological fingerprints of polynomials, vectors, and matrices,\nforging a new algebra-to-graph link. HSG-12M lays the groundwork for\ngeometry-aware graph learning and new opportunities of data-driven scientific\ndiscovery in condensed matter physics and beyond."}
{"id": "2506.08949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08949", "abs": "https://arxiv.org/abs/2506.08949", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "comment": null, "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS."}
{"id": "2506.08641", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08641", "abs": "https://arxiv.org/abs/2506.08641", "authors": ["Simon Roschmann", "Quentin Bouniot", "Vasilii Feofanov", "Ievgen Redko", "Zeynep Akata"], "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers", "comment": null, "summary": "Time series classification is a fundamental task in healthcare and industry,\nyet the development of time series foundation models (TSFMs) remains limited by\nthe scarcity of publicly available time series datasets. In this work, we\npropose Time Vision Transformer (TiViT), a framework that converts time series\ninto images to leverage the representational power of frozen Vision\nTransformers (ViTs) pretrained on large-scale image datasets. First, we\ntheoretically motivate our approach by analyzing the 2D patching of ViTs for\ntime series, showing that it can increase the number of label-relevant tokens\nand reduce the sample complexity. Second, we empirically demonstrate that TiViT\nachieves state-of-the-art performance on standard time series classification\nbenchmarks by utilizing the hidden representations of large OpenCLIP models. We\nexplore the structure of TiViT representations and find that intermediate\nlayers with high intrinsic dimension are the most effective for time series\nclassification. Finally, we assess the alignment between TiViT and TSFM\nrepresentation spaces and identify a strong complementarity, with further\nperformance gains achieved by combining their features. Our findings reveal yet\nanother direction for reusing vision representations in a non-visual domain."}
{"id": "2506.08953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08953", "abs": "https://arxiv.org/abs/2506.08953", "authors": ["Anirudh Nanduri", "Siyuan Huang", "Rama Chellappa"], "title": "Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF", "comment": null, "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nwide range of biometric tasks, including face and body recognition. In this\nwork, we adapt a ViT model pretrained on visible (VIS) imagery to the\nchallenging problem of cross-spectral body recognition, which involves matching\nimages captured in the visible and infrared (IR) domains. Recent ViT\narchitectures have explored incorporating additional embeddings beyond\ntraditional positional embeddings. Building on this idea, we integrate Side\nInformation Embedding (SIE) and examine the impact of encoding domain and\ncamera information to enhance cross-spectral matching. Surprisingly, our\nresults show that encoding only camera information - without explicitly\nincorporating domain information - achieves state-of-the-art performance on the\nLLCM dataset. While occlusion handling has been extensively studied in\nvisible-spectrum person re-identification (Re-ID), occlusions in\nvisible-infrared (VI) Re-ID remain largely underexplored - primarily because\nexisting VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly\nfeature full-body, unoccluded images. To address this gap, we analyze the\nimpact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain\nFace (IJB-MDF) dataset, which provides a diverse set of visible and infrared\nimages captured at various distances, enabling cross-range, cross-spectral\nevaluations."}
{"id": "2506.08644", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08644", "abs": "https://arxiv.org/abs/2506.08644", "authors": ["Woosung Kim", "JunHo Seo", "Jongmin Lee", "Byung-Jun Lee"], "title": "Semi-gradient DICE for Offline Constrained Reinforcement Learning", "comment": "Constrained Offline Reinforcement Learning", "summary": "Stationary Distribution Correction Estimation (DICE) addresses the mismatch\nbetween the stationary distribution induced by a policy and the target\ndistribution required for reliable off-policy evaluation (OPE) and policy\noptimization. DICE-based offline constrained RL particularly benefits from the\nflexibility of DICE, as it simultaneously maximizes return while estimating\ncosts in offline settings. However, we have observed that recent approaches\ndesigned to enhance the offline RL performance of the DICE framework\ninadvertently undermine its ability to perform OPE, making them unsuitable for\nconstrained RL scenarios. In this paper, we identify the root cause of this\nlimitation: their reliance on a semi-gradient optimization, which solves a\nfundamentally different optimization problem and results in failures in cost\nestimation. Building on these insights, we propose a novel method to enable OPE\nand constrained RL through semi-gradient DICE. Our method ensures accurate cost\nestimation and achieves state-of-the-art performance on the offline constrained\nRL benchmark, DSRL."}
{"id": "2506.08955", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08955", "abs": "https://arxiv.org/abs/2506.08955", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "title": "Segment Concealed Objects with Incomplete Supervision", "comment": "IEEE TPAMI", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models."}
{"id": "2506.08645", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08645", "abs": "https://arxiv.org/abs/2506.08645", "authors": ["Youqi Wu", "Jingwei Zhang", "Farzan Farnia"], "title": "Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach", "comment": null, "summary": "Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved\npromising results in aligning representations across modalities. However, these\nembeddings could underperform compared to state-of-the-art single-modality\nembeddings on modality-specific tasks. On the other hand, single-modality\nembeddings excel in their domains but lack cross-modal alignment capabilities.\nIn this work, we focus on the problem of unifying cross-modality and\nsingle-modality embeddings to achieve the performance of modality-expert\nembedding within individual modalities while preserving cross-modal alignment.\nTo this end, we propose RP-KrossFuse, a method that leverages a random\nprojection-based Kronecker product to integrate cross-modal embeddings with\nsingle-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise\nsimilarity scores of the fused embeddings and operates efficiently in a\nspecified kernel space and supports scalable implementations via random Fourier\nfeatures for shift-invariant kernels such as the Gaussian kernel. We\ndemonstrate the effectiveness of RP-KrossFuse through several numerical\nexperiments, combining CLIP embeddings with uni-modal image and text\nembeddings. Our numerical results indicate that RP-KrossFuse achieves\ncompetitive modality-specific performance while retaining cross-modal\nalignment, bridging the gap between cross-modal and single-modality embeddings."}
{"id": "2506.08956", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08956", "abs": "https://arxiv.org/abs/2506.08956", "authors": ["DaeEun Yoon", "Semin Kim", "SangWook Yoo", "Jongha Lee"], "title": "Data Augmentation For Small Object using Fast AutoAugment", "comment": "Accepted and published in the USB Proceedings of the 20th\n  International Conference on Modeling Decisions for Artificial Intelligence\n  (MDAI 2023), Ume{\\aa}, Sweden, June 19--22, 2023, ISBN 978-91-527-7293-5,\n  pp.\\ 12--21", "summary": "In recent years, there has been tremendous progress in object detection\nperformance. However, despite these advances, the detection performance for\nsmall objects is significantly inferior to that of large objects. Detecting\nsmall objects is one of the most challenging and important problems in computer\nvision. To improve the detection performance for small objects, we propose an\noptimal data augmentation method using Fast AutoAugment. Through our proposed\nmethod, we can quickly find optimal augmentation policies that can overcome\ndegradation when detecting small objects, and we achieve a 20% performance\nimprovement on the DOTA dataset."}
{"id": "2506.08652", "categories": ["cs.LG", "cs.AI", "20-XX, 08A02", "F.4.1; I.2"], "pdf": "https://arxiv.org/pdf/2506.08652", "abs": "https://arxiv.org/abs/2506.08652", "authors": ["Mahesh Godavarti"], "title": "JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset", "comment": null, "summary": "Transformers have demonstrated remarkable success in sequence modeling, yet\neffectively incorporating positional information remains a challenging and\nactive area of research. In this paper, we introduce JoFormer, a journey-based\nTransformer architecture grounded in a recently proposed non-commutative\nalgebra for composing transformations across positions. JoFormer represents\nrelative positions through learnable directional transforms that are\nsequentially composed along the input, thereby extending and generalizing\nexisting approaches based on relative position representations. We derive the\nJoFormer attention mechanism from first principles and show that it subsumes\nstandard methods such as rotary transformations as special cases. To evaluate\nits effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny\nShakespeare character-level language modeling task. Our results demonstrate\nthat\n  JoFormer consistently achieves lower perplexity and faster convergence,\nhighlighting the advantages of its more expressive, journey-based treatment of\nposition. Notably, the per-token JoFormer is still a primitive, conceptual\nvariant with layer-independent angles, yet it already demonstrates strong\nperformance-underscoring its promise as a proof of concept for more expressive\narchitectures. We conclude by discussing how JoFormer offers a principled\napproach to integrating positional structure into Transformer architectures.\nThe code used in this work is available at\nhttps://github.com/mahesh-godavarti/joformer."}
{"id": "2506.08964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08964", "abs": "https://arxiv.org/abs/2506.08964", "authors": ["Jinwoo Kim", "Sangmin Han", "Jinho Jeong", "Jiwoo Choi", "Dongyoung Kim", "Seon Joo Kim"], "title": "ORIDa: Object-centric Real-world Image Composition Dataset", "comment": "Accepted at CVPR 2025", "summary": "Object compositing, the task of placing and harmonizing objects in images of\ndiverse visual scenes, has become an important task in computer vision with the\nrise of generative models. However, existing datasets lack the diversity and\nscale required to comprehensively explore real-world scenarios. We introduce\nORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,\nreal-captured dataset containing over 30,000 images featuring 200 unique\nobjects, each of which is presented across varied positions and scenes. ORIDa\nhas two types of data: factual-counterfactual sets and factual-only scenes. The\nfactual-counterfactual sets consist of four factual images showing an object in\ndifferent positions within a scene and a single counterfactual (or background)\nimage of the scene without the object, resulting in five images per scene. The\nfactual-only scenes include a single image containing an object in a specific\ncontext, expanding the variety of environments. To our knowledge, ORIDa is the\nfirst publicly available dataset with its scale and complexity for real-world\nimage composition. Extensive analysis and experiments highlight the value of\nORIDa as a resource for advancing further research in object compositing."}
{"id": "2506.08655", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.08655", "abs": "https://arxiv.org/abs/2506.08655", "authors": ["Kamil Jerabek", "Jan Luxemburk", "Richard Plny", "Josef Koumar", "Jaroslav Pesek", "Karel Hynek"], "title": "When Simple Model Just Works: Is Network Traffic Classification in Crisis?", "comment": null, "summary": "Machine learning has been applied to network traffic classification (TC) for\nover two decades. While early efforts used shallow models, the latter 2010s saw\na shift toward complex neural networks, often reporting near-perfect accuracy.\nHowever, it was recently revealed that a simple k-NN baseline using packet\nsequences metadata (sizes, times, and directions) can be on par or even\noutperform more complex methods. In this paper, we investigate this phenomenon\nfurther and evaluate this baseline across 12 datasets and 15 TC tasks, and\ninvestigate why it performs so well. Our analysis shows that most datasets\ncontain over 50% redundant samples (identical packet sequences), which\nfrequently appear in both training and test sets due to common splitting\npractices. This redundancy can lead to overestimated model performance and\nreduce the theoretical maximum accuracy when identical flows have conflicting\nlabels. Given its distinct characteristics, we further argue that standard\nmachine learning practices adapted from domains like NLP or computer vision may\nbe ill-suited for TC. Finally, we propose new directions for task formulation\nand evaluation to address these challenges and help realign the field."}
{"id": "2506.08968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08968", "abs": "https://arxiv.org/abs/2506.08968", "authors": ["Amirreza Rouhi", "Solmaz Arezoomandan", "Knut Peterson", "Joseph T. Woods", "David K. Han"], "title": "ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations", "comment": null, "summary": "Object detection models typically rely on predefined categories, limiting\ntheir ability to identify novel objects in open-world scenarios. To overcome\nthis constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,\na training-free, self-refining framework for open-world object labeling. ADAM\nleverages large language models (LLMs) to generate candidate labels for unknown\nobjects based on contextual information from known entities within a scene.\nThese labels are paired with visual embeddings from CLIP to construct an\nEmbedding-Label Repository (ELR) that enables inference without category\nsupervision. For a newly encountered unknown object, ADAM retrieves visually\nsimilar instances from the ELR and applies frequency-based voting and\ncross-modal re-ranking to assign a robust label. To further enhance\nconsistency, we introduce a self-refinement loop that re-evaluates repository\nlabels using visual cohesion analysis and k-nearest-neighbor-based majority\nre-labeling. Experimental results on the COCO and PASCAL datasets demonstrate\nthat ADAM effectively annotates novel categories using only visual and\ncontextual signals, without requiring any fine-tuning or retraining."}
{"id": "2506.08660", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08660", "abs": "https://arxiv.org/abs/2506.08660", "authors": ["Jinkwan Jang", "Hyungjin Park", "Jinmyeong Choi", "Taesup Kim"], "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness", "comment": null, "summary": "Real-world time series data are inherently multivariate, often exhibiting\ncomplex inter-channel dependencies. Each channel is typically sampled at its\nown period and is prone to missing values due to various practical and\noperational constraints. These characteristics pose fundamental challenges\nrelated to channel dependency, sampling asynchrony, and missingness, all of\nwhich must be addressed to enable robust and reliable forecasting in practical\nsettings. However, most existing architectures are built on oversimplified\nassumptions, such as identical sampling periods across channels and fully\nobserved inputs at test time, which often do not hold in real-world scenarios.\nTo bridge this gap, we propose ChannelTokenFormer, a Transformer-based\nforecasting model with a flexible architecture designed to explicitly capture\ncross-channel interactions, accommodate channel-wise asynchronous sampling, and\neffectively handle missing values. Extensive experiments on three benchmark\ndatasets modified to reflect practical settings, along with one real-world\nindustrial dataset, demonstrate the superior robustness and accuracy of\nChannelTokenFormer under challenging real-world conditions."}
{"id": "2506.08979", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08979", "abs": "https://arxiv.org/abs/2506.08979", "authors": ["Longyu Yang", "Ping Hu", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu"], "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather", "comment": null, "summary": "LiDAR segmentation has emerged as an important task to enrich multimedia\nexperiences and analysis. Range-view-based methods have gained popularity due\nto their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation."}
{"id": "2506.08662", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.08662", "abs": "https://arxiv.org/abs/2506.08662", "authors": ["Florian Borzechowski", "Michael Sch√§fer", "Heiko Schwarz", "Jonathan Pfaff", "Detlev Marpe", "Thomas Wiegand"], "title": "Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization", "comment": "Accepted at ICIP2024, the IEEE International Conference on Image\n  Processing", "summary": "The continuous improvements on image compression with variational\nautoencoders have lead to learned codecs competitive with conventional\napproaches in terms of rate-distortion efficiency. Nonetheless, taking the\nquantization into account during the training process remains a problem, since\nit produces zero derivatives almost everywhere and needs to be replaced with a\ndifferentiable approximation which allows end-to-end optimization. Though there\nare different methods for approximating the quantization, none of them model\nthe quantization noise correctly and thus, result in suboptimal networks.\nHence, we propose an additional finetuning training step: After conventional\nend-to-end training, parts of the network are retrained on quantized latents\nobtained at the inference stage. For entropy-constraint quantizers like\nTrellis-Coded Quantization, the impact of the quantizer is particularly\ndifficult to approximate by rounding or adding noise as the quantized latents\nare interdependently chosen through a trellis search based on both the entropy\nmodel and a distortion measure. We show that retraining on correctly quantized\ndata consistently yields additional coding gain for both uniform scalar and\nespecially for entropy-constraint quantization, without increasing inference\ncomplexity. For the Kodak test set, we obtain average savings between 1% and\n2%, and for the TecNick test set up to 2.2% in terms of Bj{\\o}ntegaard-Delta\nbitrate."}
{"id": "2506.08990", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08990", "abs": "https://arxiv.org/abs/2506.08990", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Dongyun Liang", "Jing Qin", "Liansheng Wang"], "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "comment": "TMI 2025", "summary": "Medical vision-language alignment through cross-modal contrastive learning\nshows promising performance in image-text matching tasks, such as retrieval and\nzero-shot classification. However, conventional cross-modal contrastive\nlearning (CLIP-based) methods suffer from suboptimal visual representation\ncapabilities, which also limits their effectiveness in vision-language\nalignment. In contrast, although the models pretrained via multimodal masked\nmodeling struggle with direct cross-modal matching, they excel in visual\nrepresentation. To address this contradiction, we propose ALTA (ALign Through\nAdapting), an efficient medical vision-language alignment method that utilizes\nonly about 8% of the trainable parameters and less than 1/5 of the\ncomputational consumption required for masked record modeling. ALTA achieves\nsuperior performance in vision-language matching tasks like retrieval and\nzero-shot classification by adapting the pretrained vision model from masked\nrecord modeling. Additionally, we integrate temporal-multiview radiograph\ninputs to enhance the information consistency between radiographs and their\ncorresponding descriptions in reports, further improving the vision-language\nalignment. Experimental evaluations show that ALTA outperforms the\nbest-performing counterpart by over 4% absolute points in text-to-image\naccuracy and approximately 6% absolute points in image-to-text retrieval\naccuracy. The adaptation of vision-language models during efficient alignment\nalso promotes better vision and language understanding. Code is publicly\navailable at https://github.com/DopamineLcy/ALTA."}
{"id": "2506.08669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08669", "abs": "https://arxiv.org/abs/2506.08669", "authors": ["Dongge Han", "Menglin Xia", "Daniel Madrigal Diaz", "Samuel Kessler", "Ankur Mallick", "Xuchao Zhang", "Mirian Del Carmen Hipolito Garcia", "Jin Xu", "Victor R√ºhle", "Saravan Rajmohan"], "title": "Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search", "comment": "TTODLer-FM Workshop@ICML'25 (Tiny Titans: The next wave of On-Device\n  Learning for Foundational Models)", "summary": "Small language models (SLMs) offer promising and efficient alternatives to\nlarge language models (LLMs). However, SLMs' limited capacity restricts their\nreasoning capabilities and makes them sensitive to prompt variations. To\naddress these challenges, we propose a novel framework that enhances SLM\nreasoning capabilities through LLM generated blueprints. The blueprints provide\nstructured, high-level reasoning guides that help SLMs systematically tackle\nrelated problems. Furthermore, our framework integrates a prompt template\nsearch mechanism to mitigate the SLMs' sensitivity to prompt variations. Our\nframework demonstrates improved SLM performance across various tasks, including\nmath (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves\nthe reasoning capabilities of SLMs without increasing model size or requiring\nadditional training, offering a lightweight and deployment-friendly solution\nfor on-device or resource-constrained environments."}
{"id": "2506.08991", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.08991", "abs": "https://arxiv.org/abs/2506.08991", "authors": ["Anudeep Das", "Gurjot Singh", "Prach Chantasantitam", "N. Asokan"], "title": "Do Concept Replacement Techniques Really Erase Unacceptable Concepts?", "comment": null, "summary": "Generative models, particularly diffusion-based text-to-image (T2I) models,\nhave demonstrated astounding success. However, aligning them to avoid\ngenerating content with unacceptable concepts (e.g., offensive or copyrighted\ncontent, or celebrity likenesses) remains a significant challenge. Concept\nreplacement techniques (CRTs) aim to address this challenge, often by trying to\n\"erase\" unacceptable concepts from models. Recently, model providers have\nstarted offering image editing services which accept an image and a text prompt\nas input, to produce an image altered as specified by the prompt. These are\nknown as image-to-image (I2I) models. In this paper, we first use an I2I model\nto empirically demonstrate that today's state-of-the-art CRTs do not in fact\nerase unacceptable concepts. Existing CRTs are thus likely to be ineffective in\nemerging I2I scenarios, despite their proven ability to remove unwanted\nconcepts in T2I pipelines, highlighting the need to understand this discrepancy\nbetween T2I and I2I settings. Next, we argue that a good CRT, while replacing\nunacceptable concepts, should preserve other concepts specified in the inputs\nto generative models. We call this fidelity. Prior work on CRTs have neglected\nfidelity in the case of unacceptable concepts. Finally, we propose the use of\ntargeted image-editing techniques to achieve both effectiveness and fidelity.\nWe present such a technique, AntiMirror, and demonstrate its viability."}
{"id": "2506.08673", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.08673", "abs": "https://arxiv.org/abs/2506.08673", "authors": ["Diptarka Chakraborty", "Kushagra Chatterjee", "Debarati Das", "Tien Long Nguyen", "Romina Nobahari"], "title": "Towards Fair Representation: Clustering and Consensus", "comment": "The paper has been accepted at the Conference on Learning Theory\n  (COLT) 2025", "summary": "Consensus clustering, a fundamental task in machine learning and data\nanalysis, aims to aggregate multiple input clusterings of a dataset,\npotentially based on different non-sensitive attributes, into a single\nclustering that best represents the collective structure of the data. In this\nwork, we study this fundamental problem through the lens of fair clustering, as\nintroduced by Chierichetti et al. [NeurIPS'17], which incorporates the\ndisparate impact doctrine to ensure proportional representation of each\nprotected group in the dataset within every cluster. Our objective is to find a\nconsensus clustering that is not only representative but also fair with respect\nto specific protected attributes. To the best of our knowledge, we are the\nfirst to address this problem and provide a constant-factor approximation.\n  As part of our investigation, we examine how to minimally modify an existing\nclustering to enforce fairness -- an essential postprocessing step in many\nclustering applications that require fair representation. We develop an optimal\nalgorithm for datasets with equal group representation and near-linear time\nconstant factor approximation algorithms for more general scenarios with\ndifferent proportions of two group sizes. We complement our approximation\nresult by showing that the problem is NP-hard for two unequal-sized groups.\nGiven the fundamental nature of this problem, we believe our results on Closest\nFair Clustering could have broader implications for other clustering problems,\nparticularly those for which no prior approximation guarantees exist for their\nfair variants."}
{"id": "2506.08997", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08997", "abs": "https://arxiv.org/abs/2506.08997", "authors": ["Fabian Immel", "Jan-Hendrik Pauls", "Richard Fehler", "Frank Bieder", "Jonas Merkert", "Christoph Stiller"], "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction", "comment": null, "summary": "Autonomous vehicles rely on detailed and accurate environmental information\nto operate safely. High definition (HD) maps offer a promising solution, but\ntheir high maintenance cost poses a significant barrier to scalable deployment.\nThis challenge is addressed by online HD map construction methods, which\ngenerate local HD maps from live sensor data. However, these methods are\ninherently limited by the short perception range of onboard sensors. To\novercome this limitation and improve general performance, recent approaches\nhave explored the use of standard definition (SD) maps as prior, which are\nsignificantly easier to maintain. We propose SDTagNet, the first online HD map\nconstruction method that fully utilizes the information of widely available SD\nmaps, like OpenStreetMap, to enhance far range detection accuracy. Our approach\nintroduces two key innovations. First, in contrast to previous work, we\nincorporate not only polyline SD map data with manually selected classes, but\nadditional semantic information in the form of textual annotations. In this\nway, we enrich SD vector map tokens with NLP-derived features, eliminating the\ndependency on predefined specifications or exhaustive class taxonomies. Second,\nwe introduce a point-level SD map encoder together with orthogonal element\nidentifiers to uniformly integrate all types of map elements. Experiments on\nArgoverse 2 and nuScenes show that this boosts map perception performance by up\nto +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP\n(+20%) w.r.t. previous approaches that already use SD map priors. Code is\navailable at https://github.com/immel-f/SDTagNet"}
{"id": "2506.08681", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08681", "abs": "https://arxiv.org/abs/2506.08681", "authors": ["Phuc Minh Nguyen", "Ngoc-Hieu Nguyen", "Duy H. M. Nguyen", "Anji Liu", "An Mai", "Binh T. Nguyen", "Daniel Sonntag", "Khoa D. Doan"], "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling", "comment": "First version", "summary": "Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization\n(DPO) have emerged as alternatives to the standard Reinforcement Learning from\nHuman Feedback (RLHF) for aligning large language models (LLMs) with human\nvalues. However, these methods are more susceptible to over-optimization, in\nwhich the model drifts away from the reference policy, leading to degraded\nperformance as training progresses. This paper proposes a novel\nimportance-sampling approach to mitigate the over-optimization problem of\noffline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective\nwith an importance ratio that accounts for the reference policy distribution.\nIS-DAAs additionally avoid the high variance issue associated with importance\nsampling by clipping the importance ratio to a maximum value. Our extensive\nexperiments demonstrate that IS-DAAs can effectively mitigate\nover-optimization, especially under low regularization strength, and achieve\nbetter performance than other methods designed to address this problem. Our\nimplementations are provided publicly at this link."}
{"id": "2506.09022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09022", "abs": "https://arxiv.org/abs/2506.09022", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "title": "Do MIL Models Transfer?", "comment": "ICML 2025 (Spotlight). 20 pages, 8 figures", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab"}
{"id": "2506.08698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08698", "abs": "https://arxiv.org/abs/2506.08698", "authors": ["Boyu Xie", "Tangtang Xie"], "title": "Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data", "comment": "9 pages, 2 figures", "summary": "With the development of smart grids, High-Dimensional and Incomplete (HDI)\nPower Load Monitoring (PLM) data challenges the performance of Power Load\nForecasting (PLF) models. In this paper, we propose a potential\ncharacterization model VAE-LF based on Variational Autoencoder (VAE) for\nefficiently representing and complementing PLM missing data. VAE-LF learns a\nlow-dimensional latent representation of the data using an Encoder-Decoder\nstructure by splitting the HDI PLM data into vectors and feeding them\nsequentially into the VAE-LF model, and generates the complementary data.\nExperiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark\nmodels in both 5% and 10% sparsity test cases, with significantly lower RMSE\nand MAE, and especially outperforms on low sparsity ratio data. The method\nprovides an efficient data-completion solution for electric load management in\nsmart grids."}
{"id": "2506.09024", "categories": ["cs.CV", "cs.LG", "I.2.11; I.4.9; I.4.9; J.3; I.2.0"], "pdf": "https://arxiv.org/pdf/2506.09024", "abs": "https://arxiv.org/abs/2506.09024", "authors": ["Felix Wagner", "Pramit Saha", "Harry Anthony", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "comment": null, "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard training data\nafter deployment or assume that test samples and training data are centrally\nstored together, an assumption that rarely holds in real-world settings. This\nis because shipping training data with the deployed model is usually impossible\ndue to the size of training databases, as well as proprietary or privacy\nconstraints. We introduce the Isolation Network, an OOD detection framework\nthat quantifies the difficulty of separating a target test sample from the\ntraining data by solving a binary classification task. We then propose\nDecentralized Isolation Networks (DIsoN), which enables the comparison of\ntraining and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code will be available upon\nacceptance at: *****"}
{"id": "2506.08727", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.08727", "abs": "https://arxiv.org/abs/2506.08727", "authors": ["Samarth Sikand", "Rohit Mehra", "Priyavanshi Pathania", "Nikhil Bamby", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs", "comment": "5 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "summary": "While Generative AI stands to be one of the fastest adopted technologies\never, studies have made evident that the usage of Large Language Models (LLMs)\nputs significant burden on energy grids and our environment. It may prove a\nhindrance to the Sustainability goals of any organization. A crucial step in\nany Sustainability strategy is monitoring or estimating the energy consumption\nof various components. While there exist multiple tools for monitoring energy\nconsumption, there is a dearth of tools/frameworks for estimating the\nconsumption or carbon emissions. Current drawbacks of both monitoring and\nestimation tools include high input data points, intrusive nature, high error\nmargin, etc. We posit that leveraging emerging LLM benchmarks and related data\npoints can help overcome aforementioned challenges while balancing accuracy of\nthe emission estimations. To that extent, we discuss the challenges of current\napproaches and present our evolving framework, R-ICE, which estimates prompt\nlevel inference carbon emissions by leveraging existing state-of-the-art(SOTA)\nbenchmark. This direction provides a more practical and non-intrusive way to\nenable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our\npromising validation results suggest that benchmark-based modelling holds great\npotential for inference emission estimation and warrants further exploration\nfrom the scientific community."}
{"id": "2506.09027", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09027", "abs": "https://arxiv.org/abs/2506.09027", "authors": ["Runqian Wang", "Kaiming He"], "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "comment": null, "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning."}
{"id": "2506.08737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08737", "abs": "https://arxiv.org/abs/2506.08737", "authors": ["Haozhe Ma", "Guoji Fu", "Zhengding Luo", "Jiele Wu", "Tze-Yun Leong"], "title": "Exploration by Random Reward Perturbation", "comment": null, "summary": "We introduce Random Reward Perturbation (RRP), a novel exploration strategy\nfor reinforcement learning (RL). Our theoretical analyses demonstrate that\nadding zero-mean noise to environmental rewards effectively enhances policy\ndiversity during training, thereby expanding the range of exploration. RRP is\nfully compatible with the action-perturbation-based exploration strategies,\nsuch as $\\epsilon$-greedy, stochastic policies, and entropy regularization,\nproviding additive improvements to exploration effects. It is general,\nlightweight, and can be integrated into existing RL algorithms with minimal\nimplementation effort and negligible computational overhead. RRP establishes a\ntheoretical connection between reward shaping and noise-driven exploration,\nhighlighting their complementary potential. Experiments show that RRP\nsignificantly boosts the performance of Proximal Policy Optimization and Soft\nActor-Critic, achieving higher sample efficiency and escaping local optima\nacross various tasks, under both sparse and dense reward scenarios."}
{"id": "2506.09035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09035", "abs": "https://arxiv.org/abs/2506.09035", "authors": ["Karhan Kayan", "Stamatis Alexandropoulos", "Rishabh Jain", "Yiming Zuo", "Erich Liang", "Jia Deng"], "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose", "comment": null, "summary": "We introduce Princeton365, a large-scale diverse dataset of 365 videos with\naccurate camera pose. Our dataset bridges the gap between accuracy and data\ndiversity in current SLAM benchmarks by introducing a novel ground truth\ncollection framework that leverages calibration boards and a 360-camera. We\ncollect indoor, outdoor, and object scanning videos with synchronized monocular\nand stereo RGB video outputs as well as IMU. We further propose a new scene\nscale-aware evaluation metric for SLAM based on the the optical flow induced by\nthe camera pose estimation error. In contrast to the current metrics, our new\nmetric allows for comparison between the performance of SLAM methods across\nscenes as opposed to existing metrics such as Average Trajectory Error (ATE),\nallowing researchers to analyze the failure modes of their methods. We also\npropose a challenging Novel View Synthesis benchmark that covers cases not\ncovered by current NVS benchmarks, such as fully non-Lambertian scenes with\n360-degree camera trajectories. Please visit\nhttps://princeton365.cs.princeton.edu for the dataset, code, videos, and\nsubmission."}
{"id": "2506.08740", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08740", "abs": "https://arxiv.org/abs/2506.08740", "authors": ["Sidhika Balachandar", "Shuvom Sadhuka", "Bonnie Berger", "Emma Pierson", "Nikhil Garg"], "title": "Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports", "comment": null, "summary": "Graph neural networks (GNNs) are widely used in urban spatiotemporal\nforecasting, such as predicting infrastructure problems. In this setting,\ngovernment officials wish to know in which neighborhoods incidents like\npotholes or rodent issues occur. The true state of incidents (e.g., street\nconditions) for each neighborhood is observed via government inspection\nratings. However, these ratings are only conducted for a sparse set of\nneighborhoods and incident types. We also observe the state of incidents via\ncrowdsourced reports, which are more densely observed but may be biased due to\nheterogeneous reporting behavior. First, for such settings, we propose a\nmultiview, multioutput GNN-based model that uses both unbiased rating data and\nbiased reporting data to predict the true latent state of incidents. Second, we\ninvestigate a case study of New York City urban incidents and collect,\nstandardize, and make publicly available a dataset of 9,615,863 crowdsourced\nreports and 1,041,415 government inspection ratings over 3 years and across 139\ntypes of incidents. Finally, we show on both real and semi-synthetic data that\nour model can better predict the latent state compared to models that use only\nreporting data or models that use only rating data, especially when rating data\nis sparse and reports are predictive of ratings. We also quantify demographic\nbiases in crowdsourced reporting, e.g., higher-income neighborhoods report\nproblems at higher rates. Our analysis showcases a widely applicable approach\nfor latent state prediction using heterogeneous, sparse, and biased data."}
{"id": "2506.09040", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR."}
{"id": "2506.08764", "categories": ["cs.LG", "68T07, 60B20"], "pdf": "https://arxiv.org/pdf/2506.08764", "abs": "https://arxiv.org/abs/2506.08764", "authors": ["Benjamin Dadoun", "Soufiane Hayou", "Hanan Salam", "Mohamed El Amine Seddik", "Pierre Youssef"], "title": "On the Stability of the Jacobian Matrix in Deep Neural Networks", "comment": "16 pages, 26 figures", "summary": "Deep neural networks are known to suffer from exploding or vanishing\ngradients as depth increases, a phenomenon closely tied to the spectral\nbehavior of the input-output Jacobian. Prior work has identified critical\ninitialization schemes that ensure Jacobian stability, but these analyses are\ntypically restricted to fully connected networks with i.i.d. weights. In this\nwork, we go significantly beyond these limitations: we establish a general\nstability theorem for deep neural networks that accommodates sparsity (such as\nthat introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.\ninduced by training). Our results rely on recent advances in random matrix\ntheory, and provide rigorous guarantees for spectral stability in a much\nbroader class of network models. This extends the theoretical foundation for\ninitialization schemes in modern neural networks with structured and dependent\nrandomness."}
{"id": "2506.09042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09042", "abs": "https://arxiv.org/abs/2506.09042", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "comment": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao: Equal contribution.\n  Only the core contributors are listed. The full list of contributors can be\n  found in Appendix A of this paper", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams"}
{"id": "2506.08837", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.08837", "abs": "https://arxiv.org/abs/2506.08837", "authors": ["Luca Beurer-Kellner", "Beat Buesser Ana-Maria Cre≈£u", "Edoardo Debenedetti", "Daniel Dobos", "Daniel Fabian", "Marc Fischer", "David Froelicher", "Kathrin Grosse", "Daniel Naeff", "Ezinwanne Ozoani", "Andrew Paverd", "Florian Tram√®r", "V√°clav Volhejn"], "title": "Design Patterns for Securing LLM Agents against Prompt Injections", "comment": null, "summary": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies."}
{"id": "2506.09045", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09045", "abs": "https://arxiv.org/abs/2506.09045", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "comment": "Project Page: https://zehong-ma.github.io/MagCache", "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."}
{"id": "2506.08844", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2506.08844", "abs": "https://arxiv.org/abs/2506.08844", "authors": ["Siyi Sun", "David Antony Selby", "Yunchuan Huang", "Sebastian Vollmer", "Seth Flaxman", "Anisoara Calinescu"], "title": "IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)", "comment": null, "summary": "Missing data imputation in tabular datasets remains a pivotal challenge in\ndata science and machine learning, particularly within socioeconomic research.\nHowever, real-world socioeconomic datasets are typically subject to strict data\nprotection protocols, which often prohibit public sharing, even for synthetic\nderivatives. This severely limits the reproducibility and accessibility of\nbenchmark studies in such settings. Further, there are very few publicly\navailable synthetic datasets. Thus, there is limited availability of benchmarks\nfor systematic evaluation of imputation methods on socioeconomic datasets,\nwhether real or synthetic. In this study, we utilize the World Bank's publicly\navailable synthetic dataset, Synthetic Data for an Imaginary Country, which\nclosely mimics a real World Bank household survey while being fully public,\nenabling broad access for methodological research. With this as a starting\npoint, we derived the IMAGIC-500 dataset: we select a subset of 500k\nindividuals across approximately 100k households with 19 socioeconomic\nfeatures, designed to reflect the hierarchical structure of real-world\nhousehold surveys. This paper introduces a comprehensive missing data\nimputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,\nMNAR) and missingness ratios (10\\%, 20\\%, 30\\%, 40\\%, 50\\%). Our evaluation\nconsiders the imputation accuracy for continuous and categorical variables,\ncomputational efficiency, and impact on downstream predictive tasks, such as\nestimating educational attainment at the individual level. The results\nhighlight the strengths and weaknesses of statistical, traditional machine\nlearning, and deep learning imputation techniques, including recent\ndiffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate\nthe development of robust imputation algorithms and foster reproducible social\nscience research."}
{"id": "2506.08019", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.08019", "abs": "https://arxiv.org/abs/2506.08019", "authors": ["Andrew Wells", "Geraldine Henningsen", "Brice Bolane Tchinde Kengne"], "title": "Gridding Forced Displacement using Semi-Supervised Learning", "comment": null, "summary": "We present a semi-supervised approach that disaggregates refugee statistics\nfrom administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan\nAfrican countries. By integrating UNHCR's ProGres registration data with\nsatellite-derived building footprints from Google Open Buildings and location\ncoordinates from OpenStreetMap Populated Places, our label spreading algorithm\ncreates spatially explicit refugee statistics at high granularity.This\nmethodology achieves 92.9% average accuracy in placing over 10 million refugee\nobservations into appropriate grid cells, enabling the identification of\nlocalized displacement patterns previously obscured in broader regional and\nnational statistics. The resulting high-resolution dataset provides a\nfoundation for a deeper understanding of displacement drivers."}
{"id": "2506.08850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08850", "abs": "https://arxiv.org/abs/2506.08850", "authors": ["Amin Avan", "Akramul Azim", "Qusay Mahmoud"], "title": "Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing", "comment": null, "summary": "Soft real-time applications are becoming increasingly complex, posing\nsignificant challenges for scheduling offloaded tasks in edge computing\nenvironments while meeting task timing constraints. Moreover, the exponential\ngrowth of the search space, presence of multiple objectives and parameters, and\nhighly dynamic nature of edge computing environments further exacerbate the\ncomplexity of task scheduling. As a result, schedulers based on heuristic and\nmetaheuristic algorithms frequently encounter difficulties in generating\noptimal or near-optimal task schedules due to their constrained ability to\nadapt to the dynamic conditions and complex environmental characteristics of\nedge computing. Accordingly, reinforcement learning algorithms have been\nincorporated into schedulers to address the complexity and dynamic conditions\ninherent in task scheduling in edge computing. However, a significant\nlimitation of reinforcement learning algorithms is the prolonged learning time\nrequired to adapt to new environments and to address medium- and large-scale\nproblems. This challenge arises from the extensive global action space and\nfrequent random exploration of irrelevant actions. Therefore, this study\nproposes Agile Reinforcement learning (aRL), in which the RL-agent performs\ninformed exploration and executes only relevant actions. Consequently, the\npredictability of the RL-agent is enhanced, leading to rapid adaptation and\nconvergence, which positions aRL as a suitable candidate for scheduling the\ntasks of soft real-time applications in edge computing. The experiments\ndemonstrate that the combination of informed exploration and action-masking\nmethods enables aRL to achieve a higher hit-ratio and converge faster than the\nbaseline approaches."}
{"id": "2506.08020", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08020", "abs": "https://arxiv.org/abs/2506.08020", "authors": ["Zi-Ying Chen", "Chuan-Xian Ren", "Hong Yan"], "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation", "comment": null, "summary": "Partial domain adaptation (PDA) problem requires aligning cross-domain\nsamples while distinguishing the outlier classes for accurate knowledge\ntransfer. The widely used weighting framework tries to address the outlier\nclasses by introducing the reweighed source domain with a similar label\ndistribution to the target domain. However, the empirical modeling of weights\ncan only characterize the sample-wise relations, which leads to insufficient\nexploration of cluster structures, and the weights could be sensitive to the\ninaccurate prediction and cause confusion on the outlier classes. To tackle\nthese issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model\nto simultaneously characterize the sample-wise and class-wise relations in a\nunified transport framework. Specifically, a cooperation mechanism between\nsample-level and class-level transport is introduced, where the sample-level\ntransport provides essential structure information for the class-level\nknowledge transfer, while the class-level transport supplies discriminative\ninformation for the outlier identification. The bi-level transport plan\nprovides guidance for the alignment process. By incorporating the label-aware\ntransport cost, the local transport structure is ensured and a fast computation\nformulation is derived to improve the efficiency. Extensive experiments on\nbenchmark datasets validate the competitiveness of BUOT."}
{"id": "2506.08871", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.08871", "abs": "https://arxiv.org/abs/2506.08871", "authors": ["Victor M. Tenorio", "Madeline Navarro", "Samuel Rey", "Santiago Segarra", "Antonio G. Marques"], "title": "Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery", "comment": null, "summary": "Graph Neural Networks (GNNs) often struggle with heterophilic data, where\nconnected nodes may have dissimilar labels, as they typically assume homophily\nand rely on local message passing. To address this, we propose creating\nalternative graph structures by linking nodes with similar structural\nattributes (e.g., role-based or global), thereby fostering higher label\nhomophily on these new graphs. We theoretically prove that GNN performance can\nbe improved by utilizing graphs with fewer false positive edges (connections\nbetween nodes of different classes) and that considering multiple graph views\nincreases the likelihood of finding such beneficial structures. Building on\nthese insights, we introduce Structure-Guided GNN (SG-GNN), an architecture\nthat processes the original graph alongside the newly created structural\ngraphs, adaptively learning to weigh their contributions. Extensive experiments\non various benchmark datasets, particularly those with heterophilic\ncharacteristics, demonstrate that our SG-GNN achieves state-of-the-art or\nhighly competitive performance, highlighting the efficacy of exploiting\nstructural information to guide GNNs."}
{"id": "2506.08022", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations."}
{"id": "2506.08882", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08882", "abs": "https://arxiv.org/abs/2506.08882", "authors": ["Dimitrios Amaxilatis", "Themistoklis Sarantakos", "Ioannis Chatzigiannakis", "Georgios Mylonas"], "title": "Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data", "comment": null, "summary": "In this work, we explore the application of recent data imputation techniques\nto enhance monitoring and management of water distribution networks using smart\nwater meters, based on data derived from a real-world IoT water grid monitoring\ndeployment. Despite the detailed data produced by such meters, data gaps due to\ntechnical issues can significantly impact operational decisions and efficiency.\nOur results, by comparing various imputation methods, such as k-Nearest\nNeighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate\nthat effective data imputation can substantially enhance the quality of the\ninsights derived from water consumption data as we study their effect on\naccuracy and reliability of water metering data to provide solutions in\napplications like leak detection and predictive maintenance scheduling."}
{"id": "2506.08043", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications."}
{"id": "2506.08884", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.08884", "abs": "https://arxiv.org/abs/2506.08884", "authors": ["Shiqin Tang", "Shujian Yu"], "title": "InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis", "comment": "accepted by UAI-25, code is available at\n  \\url{https://github.com/marcusstang/InfoDPCCA}", "summary": "Extracting meaningful latent representations from high-dimensional sequential\ndata is a crucial challenge in machine learning, with applications spanning\nnatural science and engineering. We introduce InfoDPCCA, a dynamic\nprobabilistic Canonical Correlation Analysis (CCA) framework designed to model\ntwo interdependent sequences of observations. InfoDPCCA leverages a novel\ninformation-theoretic objective to extract a shared latent representation that\ncaptures the mutual structure between the data streams and balances\nrepresentation compression and predictive sufficiency while also learning\nseparate latent components that encode information specific to each sequence.\nUnlike prior dynamic CCA models, such as DPCCA, our approach explicitly\nenforces the shared latent space to encode only the mutual information between\nthe sequences, improving interpretability and robustness. We further introduce\na two-step training scheme to bridge the gap between information-theoretic\nrepresentation learning and generative modeling, along with a residual\nconnection mechanism to enhance training stability. Through experiments on\nsynthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool\nfor representation learning. Code of InfoDPCCA is available at\nhttps://github.com/marcusstang/InfoDPCCA."}
{"id": "2506.08064", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08064", "abs": "https://arxiv.org/abs/2506.08064", "authors": ["Livio Tenze", "Enrique Canessa"], "title": "A Real-time 3D Desktop Display", "comment": "10 pages, 5 figures", "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0"}
{"id": "2506.08889", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08889", "abs": "https://arxiv.org/abs/2506.08889", "authors": ["Yizhao Gao", "Shuming Guo", "Shijie Cao", "Yuqing Xia", "Yu Cheng", "Lei Wang", "Lingxiao Ma", "Yutao Sun", "Tianzhu Ye", "Li Dong", "Hayden Kwok-Hay So", "Yu Hua", "Ting Cao", "Fan Yang", "Mao Yang"], "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning", "comment": null, "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention."}
{"id": "2506.08167", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.08167", "abs": "https://arxiv.org/abs/2506.08167", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data", "comment": null, "summary": "Federated Learning (FL) often suffers from severe performance degradation\nwhen faced with non-IID data, largely due to local classifier bias. Traditional\nremedies such as global model regularization or layer freezing either incur\nhigh computational costs or struggle to adapt to feature shifts. In this work,\nwe propose UniVarFL, a novel FL framework that emulates IID-like training\ndynamics directly at the client level, eliminating the need for global model\ndependency. UniVarFL leverages two complementary regularization strategies\nduring local training: Classifier Variance Regularization, which aligns\nclass-wise probability distributions with those expected under IID conditions,\neffectively mitigating local classifier bias; and Hyperspherical Uniformity\nRegularization, which encourages a uniform distribution of feature\nrepresentations across the hypersphere, thereby enhancing the model's ability\nto generalize under diverse data distributions. Extensive experiments on\nmultiple benchmark datasets demonstrate that UniVarFL outperforms existing\nmethods in accuracy, highlighting its potential as a highly scalable and\nefficient solution for real-world FL deployments, especially in\nresource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL"}
{"id": "2506.08902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08902", "abs": "https://arxiv.org/abs/2506.08902", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "title": "Intention-Conditioned Flow Occupancy Models", "comment": null, "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom"}
{"id": "2506.08334", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08334", "abs": "https://arxiv.org/abs/2506.08334", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly."}
{"id": "2506.08916", "categories": ["cs.LG", "math.DS", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2506.08916", "abs": "https://arxiv.org/abs/2506.08916", "authors": ["Maria-Veronica Ciocanel", "John T. Nardini", "Kevin B. Flores", "Erica M. Rutter", "Suzanne S. Sindi", "Alexandria Volkening"], "title": "Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)", "comment": "31 pages, 10 figures", "summary": "Agent-based modeling (ABM) is a powerful tool for understanding\nself-organizing biological systems, but it is computationally intensive and\noften not analytically tractable. Equation learning (EQL) methods can derive\ncontinuum models from ABM data, but they typically require extensive\nsimulations for each parameter set, raising concerns about generalizability. In\nthis work, we extend EQL to Multi-experiment equation learning (ME-EQL) by\nintroducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns\nindividual models for each parameter set and connects them via interpolation,\nand embedded structure ME-EQL (ES ME-EQL), which builds a unified model library\nacross parameters. We demonstrate these methods using a birth--death mean-field\nmodel and an on-lattice agent-based model of birth, death, and migration with\nspatial structure. Our results show that both methods significantly reduce the\nrelative error in recovering parameters from agent-based simulations, with OAT\nME-EQL offering better generalizability across parameter space. Our findings\nhighlight the potential of equation learning from multiple experiments to\nenhance the generalizability and interpretability of learned models for complex\nbiological systems."}
{"id": "2506.08350", "categories": ["cs.GR", "cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Ak≈üit"], "title": "Complex-Valued Holographic Radiance Fields", "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations."}
{"id": "2506.08928", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.08928", "abs": "https://arxiv.org/abs/2506.08928", "authors": ["Zhongyuan Liang", "Zachary T. Rewolinski", "Abhineet Agarwal", "Tiffany M. Tang", "Bin Yu"], "title": "Local MDI+: Local Feature Importances for Tree-Based Models", "comment": null, "summary": "Tree-based ensembles such as random forests remain the go-to for tabular data\nover deep learning models due to their prediction performance and computational\nefficiency. These advantages have led to their widespread deployment in\nhigh-stakes domains, where interpretability is essential for ensuring\ntrustworthy predictions. This has motivated the development of popular local\n(i.e. sample-specific) feature importance (LFI) methods such as LIME and\nTreeSHAP. However, these approaches rely on approximations that ignore the\nmodel's internal structure and instead depend on potentially unstable\nperturbations. These issues are addressed in the global setting by MDI+, a\nfeature importance method which exploits an equivalence between decision trees\nand linear models on a transformed node basis. However, the global MDI+ scores\nare not able to explain predictions when faced with heterogeneous individual\ncharacteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel\nextension of the MDI+ framework to the sample specific setting. LMDI+\noutperforms existing baselines LIME and TreeSHAP in identifying\ninstance-specific signal features, averaging a 10% improvement in downstream\ntask performance across twelve real-world benchmark datasets. It further\ndemonstrates greater stability by consistently producing similar instance-level\nfeature importance rankings across multiple random forest fits. Finally, LMDI+\nenables local interpretability use cases, including the identification of\ncloser counterfactuals and the discovery of homogeneous subgroups."}
{"id": "2506.08353", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08353", "abs": "https://arxiv.org/abs/2506.08353", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "An Adaptive Method Stabilizing Activations for Enhanced Generalization", "comment": null, "summary": "We introduce AdaAct, a novel optimization algorithm that adjusts learning\nrates according to activation variance. Our method enhances the stability of\nneuron outputs by incorporating neuron-wise adaptivity during the training\nprocess, which subsequently leads to better generalization -- a complementary\napproach to conventional activation regularization methods. Experimental\nresults demonstrate AdaAct's competitive performance across standard image\nclassification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing\nit with other state-of-the-art methods. Importantly, AdaAct effectively bridges\nthe gap between the convergence speed of Adam and the strong generalization\ncapabilities of SGD, all while maintaining competitive execution times. Code is\navailable at https://github.com/hseung88/adaact."}
{"id": "2506.08936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08936", "abs": "https://arxiv.org/abs/2506.08936", "authors": ["Amina Mollaysa", "Artem Moskale", "Pushpak Pati", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models", "comment": "Proceedings of ICML 2025 Workshop on Multi-modal Foundation\n  Proceedings of ICML 2025 Workshop on Multi-modal Foundation Proceedings of\n  ICML 2025 Workshop on Multi-modal Foundation Models and Large Language Models\n  for Life Sciences", "summary": "We present BioLangFusion, a simple approach for integrating pre-trained DNA,\nmRNA, and protein language models into unified molecular representations.\nMotivated by the central dogma of molecular biology (information flow from gene\nto transcript to protein), we align per-modality embeddings at the biologically\nmeaningful codon level (three nucleotides encoding one amino acid) to ensure\ndirect cross-modal correspondence. BioLangFusion studies three standard fusion\ntechniques: (i) codon-level embedding concatenation, (ii) entropy-regularized\nattention pooling inspired by multiple-instance learning, and (iii) cross-modal\nmulti-head attention -- each technique providing a different inductive bias for\ncombining modality-specific signals. These methods require no additional\npre-training or modification of the base models, allowing straightforward\nintegration with existing sequence-based foundation models. Across five\nmolecular property prediction tasks, BioLangFusion outperforms strong unimodal\nbaselines, showing that even simple fusion of pre-trained models can capture\ncomplementary multi-omic information with minimal overhead."}
{"id": "2506.08435", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08435", "abs": "https://arxiv.org/abs/2506.08435", "authors": ["Mingyuan Fan", "Fuyi Wang", "Cen Chen", "Jianying Zhou"], "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings", "comment": "Accepted to Usenix Security 2025", "summary": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods."}
{"id": "2506.08939", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08939", "abs": "https://arxiv.org/abs/2506.08939", "authors": ["Hang Ye", "Gaoxiang Duan", "Haoran Zeng", "Yangxin Zhu", "Lingxue Meng", "Xiaoying Zheng", "Yongxin Zhu"], "title": "KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting", "comment": "10 pages,3 figures, published to WASA2025", "summary": "Multivariate long-term and efficient time series forecasting is a key\nrequirement for a variety of practical applications, and there are complex\ninterleaving time dynamics in time series data that require decomposition\nmodeling. Traditional time series decomposition methods are single and rely on\nfixed rules, which are insufficient for mining the potential information of the\nseries and adapting to the dynamic characteristics of complex series. On the\nother hand, the Transformer-based models for time series forecasting struggle\nto effectively model long sequences and intricate dynamic relationships due to\ntheir high computational complexity. To overcome these limitations, we\nintroduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to\ndynamically extract trend and seasonal components. It further integrates a\nHybrid Frequency-Time Decomposition module (HFTD) to further decompose Series\ninto frequency-domain and time-domain. These components are coupled with\nmulti-scale Mamba-based KarmaBlock to efficiently process global and local\ninformation in a coordinated manner. Experiments on eight real-world datasets\nfrom diverse domains well demonstrated that KARMA significantly outperforms\nmainstream baseline methods in both predictive accuracy and computational\nefficiency. Code and full results are available at this repository:\nhttps://github.com/yedadasd/KARMA"}
{"id": "2506.08443", "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.08443", "abs": "https://arxiv.org/abs/2506.08443", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition."}
{"id": "2506.08961", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08961", "abs": "https://arxiv.org/abs/2506.08961", "authors": ["Chenxu Wang", "Huaping Liu"], "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation", "comment": null, "summary": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have\nbeen widely studied in various threat models; however, few consider\nenvironmental state perturbations, which are natural in embodied scenarios. To\nimprove the robustness of DRL agents, we formulate the problem of environmental\nstate perturbation, introducing a preliminary non-targeted attack method as a\ncalibration adversary, and then propose a defense framework, named Boosted\nAdversarial Training (BAT), which first tunes the agents via supervised\nlearning to avoid catastrophic failure and subsequently adversarially trains\nthe agent with reinforcement learning. Extensive experimental results\nsubstantiate the vulnerability of mainstream agents under environmental state\nperturbations and the effectiveness of our proposed attack. The defense results\ndemonstrate that while existing robust reinforcement learning algorithms may\nnot be suitable, our BAT framework can significantly enhance the robustness of\nagents against environmental state perturbations across various situations."}
{"id": "2506.08618", "categories": ["cs.LG", "cond-mat.mes-hall", "cond-mat.other", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08618", "abs": "https://arxiv.org/abs/2506.08618", "authors": ["Xianquan Yan", "Hakan Akg√ºn", "Kenji Kawaguchi", "N. Duane Loh", "Ching Hua Lee"], "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset", "comment": "39 pages, 13 figures, 3 tables. Code & pipeline:\n  [https://github.com/sarinstein-yan/Poly2Graph] Dataset:\n  [https://github.com/sarinstein-yan/HSG-12M] Dataset released under CC BY 4.0", "summary": "Existing graph benchmarks assume non-spatial, simple edges, collapsing\nphysically distinct paths into a single link. We introduce HSG-12M, the first\nlarge-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a\nmetric space where multiple geometrically distinct trajectories between two\nnodes are retained as separate edges. HSG-12M contains 11.6 million static and\n5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401\ncharacteristic-polynomial classes, derived from 177 TB of spectral potential\ndata. Each graph encodes the full geometry of a 1-D crystal's energy spectrum\non the complex plane, producing diverse, physics-grounded topologies that\ntranscend conventional node-coordinate datasets. To enable future extensions,\nwe release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that\nmaps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with\npopular GNNs expose new challenges in learning from multi-edge geometry at\nscale. Beyond its practical utility, we show that spectral graphs serve as\nuniversal topological fingerprints of polynomials, vectors, and matrices,\nforging a new algebra-to-graph link. HSG-12M lays the groundwork for\ngeometry-aware graph learning and new opportunities of data-driven scientific\ndiscovery in condensed matter physics and beyond."}
{"id": "2506.08965", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08965", "abs": "https://arxiv.org/abs/2506.08965", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "title": "GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO", "comment": null, "summary": "The ability to train high-performing reward models with few-shot data is\ncritical for enhancing the efficiency and scalability of Reinforcement Learning\nfrom Human Feedback (RLHF). We propose a data augmentation and expansion\nframework that enables generative reward models trained on small datasets to\nachieve comparable performance to those trained on large-scale datasets.\nTraditional methods to train a generative reward model, such as Direct\nPreference Optimization (DPO), are constrained by inefficiencies in sample\npairing and limited data diversity. This work introduces preference refinement,\nwhich employs Chain-of-Thought (CoT) sampling to uncover diverse and\nhigh-quality preference relationships. It also incorporates a perplexity-based\nscoring mechanism to assign nuanced preference levels and utilizes Multi-level\nDirect Preference Optimization (M-DPO) to enable the model to capture\nfiner-grained preference differences between samples. Experimental results\ndemonstrate that the proposed method significantly enhances data efficiency and\nmodel performance, enabling reward models trained in a few-shot setting to\nachieve results on par with those trained on large-scale datasets. This study\nunderscores the potential of data-efficient strategies in advancing reward\nmodel optimization, offering a robust solution for low-resource RLHF\napplications."}
{"id": "2506.08634", "categories": ["cs.HC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08634", "abs": "https://arxiv.org/abs/2506.08634", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills."}
{"id": "2506.08977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08977", "abs": "https://arxiv.org/abs/2506.08977", "authors": ["Victoria Hankemeier", "Malte Schilling"], "title": "Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data", "comment": "Accepted at IJCNN25, Code: https://github.com/vicky-hnk/time-flex", "summary": "Developments in Deep Learning have significantly improved time series\nforecasting by enabling more accurate modeling of complex temporal dependencies\ninherent in sequential data. The effectiveness of such models is often\ndemonstrated on limited sets of specific real-world data. Although this allows\nfor comparative analysis, it still does not demonstrate how specific data\ncharacteristics align with the architectural strengths of individual models.\nOur research aims at uncovering clear connections between time series\ncharacteristics and particular models. We introduce a novel dataset generated\nusing Gaussian Processes, specifically designed to display distinct, known\ncharacteristics for targeted evaluations of model adaptability to them.\nFurthermore, we present TimeFlex, a new model that incorporates a modular\narchitecture tailored to handle diverse temporal dynamics, including trends and\nperiodic patterns. This model is compared to current state-of-the-art models,\noffering a deeper understanding of how models perform under varied time series\nconditions."}
{"id": "2506.08641", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08641", "abs": "https://arxiv.org/abs/2506.08641", "authors": ["Simon Roschmann", "Quentin Bouniot", "Vasilii Feofanov", "Ievgen Redko", "Zeynep Akata"], "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers", "comment": null, "summary": "Time series classification is a fundamental task in healthcare and industry,\nyet the development of time series foundation models (TSFMs) remains limited by\nthe scarcity of publicly available time series datasets. In this work, we\npropose Time Vision Transformer (TiViT), a framework that converts time series\ninto images to leverage the representational power of frozen Vision\nTransformers (ViTs) pretrained on large-scale image datasets. First, we\ntheoretically motivate our approach by analyzing the 2D patching of ViTs for\ntime series, showing that it can increase the number of label-relevant tokens\nand reduce the sample complexity. Second, we empirically demonstrate that TiViT\nachieves state-of-the-art performance on standard time series classification\nbenchmarks by utilizing the hidden representations of large OpenCLIP models. We\nexplore the structure of TiViT representations and find that intermediate\nlayers with high intrinsic dimension are the most effective for time series\nclassification. Finally, we assess the alignment between TiViT and TSFM\nrepresentation spaces and identify a strong complementarity, with further\nperformance gains achieved by combining their features. Our findings reveal yet\nanother direction for reusing vision representations in a non-visual domain."}
{"id": "2506.08978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.08978", "abs": "https://arxiv.org/abs/2506.08978", "authors": ["Anna Langedijk", "Jaap Jumelet", "Willem Zuidema"], "title": "Propositional Logic for Probing Generalization in Neural Networks", "comment": null, "summary": "The extent to which neural networks are able to acquire and represent\nsymbolic rules remains a key topic of research and debate. Much current work\nfocuses on the impressive capabilities of large language models, as well as\ntheir often ill-understood failures on a wide range of reasoning tasks. In this\npaper, in contrast, we investigate the generalization behavior of three key\nneural architectures (Transformers, Graph Convolution Networks and LSTMs) in a\ncontrolled task rooted in propositional logic. The task requires models to\ngenerate satisfying assignments for logical formulas, making it a structured\nand interpretable setting for studying compositionality. We introduce a\nbalanced extension of an existing dataset to eliminate superficial patterns and\nenable testing on unseen operator combinations. Using this dataset, we evaluate\nthe ability of the three architectures to generalize beyond the training\ndistribution. While all models perform well in-distribution, we find that\ngeneralization to unseen patterns, particularly those involving negation,\nremains a significant challenge. Transformers fail to apply negation\ncompositionally, unless structural biases are introduced. Our findings\nhighlight persistent limitations in the ability of standard architectures to\nlearn systematic representations of logical operators, suggesting the need for\nstronger inductive biases to support robust rule-based reasoning."}
{"id": "2506.08708", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08982", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08982", "abs": "https://arxiv.org/abs/2506.08982", "authors": ["Ivan Rubachev", "Akim Kotelnikov", "Nikolay Kartashev"], "title": "On Finetuning Tabular Foundation Models", "comment": null, "summary": "Foundation models are an emerging research direction in tabular deep\nlearning. Notably, TabPFNv2 recently claimed superior performance over\ntraditional GBDT-based methods on small-scale datasets using an in-context\nlearning paradigm, which does not adapt model parameters to target datasets.\nHowever, the optimal finetuning approach for adapting tabular foundational\nmodels, and how this adaptation reshapes their internal mechanisms, remains\nunderexplored. While prior works studied finetuning for earlier foundational\nmodels, inconsistent findings and TabPFNv2's unique architecture necessitate\nfresh investigation. To address these questions, we first systematically\nevaluate various finetuning strategies on diverse datasets. Our findings\nestablish full finetuning as the most practical solution for TabPFNv2 in terms\nof time-efficiency and effectiveness. We then investigate how finetuning alters\nTabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.\nWe reveal that the success of finetuning stems from the fact that after\ngradient-based adaptation, the dot products of the query-representations of\ntest objects and the key-representations of in-context training objects more\naccurately reflect their target similarity. This improved similarity allows\nfinetuned TabPFNv2 to better approximate target dependency by appropriately\nweighting relevant in-context samples, improving the retrieval-based prediction\nlogic. From the practical perspective, we managed to finetune TabPFNv2 on\ndatasets with up to 50K objects, observing performance improvements on almost\nall tasks. More precisely, on academic datasets with I.I.D. splits, finetuning\nallows TabPFNv2 to achieve state-of-the-art results, while on datasets with\ngradual temporal shifts and rich feature sets, TabPFNv2 is less stable and\nprior methods remain better."}
{"id": "2506.09023", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels."}
{"id": "2506.08989", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.08989", "abs": "https://arxiv.org/abs/2506.08989", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Yeyun Gong", "Yang Wang", "Hengyuan Zhang", "Yelong Shen", "Ying Nian Wu", "Weizhu Chen"], "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "comment": "Reinforcement Learning; Large Language Models; LLM Reasoning", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor training large language models (LLMs) on complex reasoning tasks, such as\nmathematical problem solving. A prerequisite for the scalability of RLVR is a\nhigh-quality problem set with precise and verifiable answers. However, the\nscarcity of well-crafted human-labeled math problems and limited-verification\nanswers in existing distillation-oriented synthetic datasets limit their\neffectiveness in RL. Additionally, most problem synthesis strategies\nindiscriminately expand the problem set without considering the model's\ncapabilities, leading to low efficiency in generating useful questions. To\nmitigate this issue, we introduce a Self-aware Weakness-driven problem\nSynthesis framework (SwS) that systematically identifies model deficiencies and\nleverages them for problem augmentation. Specifically, we define weaknesses as\nquestions that the model consistently fails to learn through its iterative\nsampling during RL training. We then extract the core concepts from these\nfailure cases and synthesize new problems to strengthen the model's weak areas\nin subsequent augmented training, enabling it to focus on and gradually\novercome its weaknesses. Without relying on external knowledge distillation,\nour framework enables robust generalization byempowering the model to\nself-identify and address its weaknesses in RL, yielding average performance\ngains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning\nbenchmarks."}
{"id": "2506.09007", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09007", "abs": "https://arxiv.org/abs/2506.09007", "authors": ["Sophia Tang", "Yinuo Zhang", "Alexander Tong", "Pranam Chatterjee"], "title": "Branched Schr√∂dinger Bridge Matching", "comment": null, "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations."}
{"id": "2506.09010", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09010", "abs": "https://arxiv.org/abs/2506.09010", "authors": ["Sebastian Schmidt", "Prasanga Dhungel", "Christoffer L√∂ffler", "Bj√∂rn Nieth", "Stephan G√ºnnemann", "Leo Schwinn"], "title": "Effective Data Pruning through Score Extrapolation", "comment": null, "summary": "Training advanced machine learning models demands massive datasets, resulting\nin prohibitive computational costs. To address this challenge, data pruning\ntechniques identify and remove redundant training samples while preserving\nmodel performance. Yet, existing pruning techniques predominantly require a\nfull initial training pass to identify removable samples, negating any\nefficiency benefits for single training runs. To overcome this limitation, we\nintroduce a novel importance score extrapolation framework that requires\ntraining on only a small subset of data. We present two initial approaches in\nthis framework - k-nearest neighbors and graph neural networks - to accurately\npredict sample importance for the entire dataset using patterns learned from\nthis minimal subset. We demonstrate the effectiveness of our approach for 2\nstate-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different\ndatasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training\nparadigms (supervised, unsupervised, and adversarial). Our results indicate\nthat score extrapolation is a promising direction to scale expensive score\ncalculation methods, such as pruning, data attribution, or other tasks."}
{"id": "2506.09016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09016", "abs": "https://arxiv.org/abs/2506.09016", "authors": ["Ruiqi Zhang", "Daman Arora", "Song Mei", "Andrea Zanette"], "title": "SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning", "comment": "pre-print", "summary": "Training large language models with reinforcement learning (RL) against\nverifiable rewards significantly enhances their reasoning abilities, yet\nremains computationally expensive due to inefficient uniform prompt sampling.\nWe introduce Selective Prompting with Efficient Estimation of Difficulty\n(SPEED), an adaptive online RL curriculum that selectively chooses training\nexamples of intermediate difficulty to maximize learning efficiency.\nTheoretically, we establish that intermediate-difficulty prompts improve the\ngradient estimator's signal-to-noise ratio, accelerating convergence.\nEmpirically, our efficient implementation leads to 2x to 6x faster training\nwithout degrading accuracy, requires no manual tuning, and integrates\nseamlessly into standard RL algorithms."}
{"id": "2506.09018", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09018", "abs": "https://arxiv.org/abs/2506.09018", "authors": ["Marton Havasi", "Brian Karrer", "Itai Gat", "Ricky T. Q. Chen"], "title": "Edit Flows: Flow Matching with Edit Operations", "comment": null, "summary": "Autoregressive generative models naturally generate variable-length\nsequences, while non-autoregressive models struggle, often imposing rigid,\ntoken-wise structures. We propose Edit Flows, a non-autoregressive model that\novercomes these limitations by defining a discrete flow over sequences through\nedit operations-insertions, deletions, and substitutions. By modeling these\noperations within a Continuous-time Markov Chain over the sequence space, Edit\nFlows enable flexible, position-relative generation that aligns more closely\nwith the structure of sequence data. Our training method leverages an expanded\nstate space with auxiliary variables, making the learning process efficient and\ntractable. Empirical results show that Edit Flows outperforms both\nautoregressive and mask models on image captioning and significantly\noutperforms the mask construction in text and code generation."}
{"id": "2506.09026", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09026", "abs": "https://arxiv.org/abs/2506.09026", "authors": ["Amrith Setlur", "Matthew Y. R. Yang", "Charlie Snell", "Jeremy Greer", "Ian Wu", "Virginia Smith", "Max Simchowitz", "Aviral Kumar"], "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs", "comment": null, "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model."}
{"id": "2506.09034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09034", "abs": "https://arxiv.org/abs/2506.09034", "authors": ["Sizhe Dang", "Yangyang Guo", "Yanjun Zhao", "Haishan Ye", "Xiaodong Zheng", "Guang Dai", "Ivor Tsang"], "title": "FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed", "comment": null, "summary": "Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:\nthe backward pass of first-order optimizers like Adam increases memory usage to\nmore than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order\n(ZO) optimizers avoid this cost by estimating gradients only from forward\npasses, yet existing methods like MeZO usually require many more steps to\nconverge. Can this trade-off between speed and memory in ZO be fundamentally\nimproved? Normalized-SGD demonstrates strong empirical performance with greater\nmemory efficiency than Adam. In light of this, we introduce FZOO, a Fast\nZeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward\npasses needed for convergence by employing batched one-sided estimates that\nadapt step sizes based on the standard deviation of batch losses. It also\naccelerates per-batch computation through the use of Rademacher random vector\nperturbations coupled with CUDA's parallel processing. Extensive experiments on\ndiverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,\nacross 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms\nMeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For\nRoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy\nand an 18 times reduction in forward passes compared to MeZO, achieving\nconvergence speeds comparable to Adam. We also provide theoretical analysis\nproving FZOO's formal equivalence to a normalized-SGD update rule and its\nconvergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling\neven larger memory savings. Overall, our results make single-GPU, high-speed,\nfull-parameter fine-tuning practical and point toward future work on\nmemory-efficient pre-training."}
{"id": "2506.09044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09044", "abs": "https://arxiv.org/abs/2506.09044", "authors": ["Javier Sanguino", "Thomas Kehrenberg", "Jose A. Lozano", "Novi Quadrianto"], "title": "The Decoupled Risk Landscape in Performative Prediction", "comment": null, "summary": "Performative Prediction addresses scenarios where deploying a model induces a\ndistribution shift in the input data, such as individuals modifying their\nfeatures and reapplying for a bank loan after rejection. Literature has had a\ntheoretical perspective giving mathematical guarantees for convergence (either\nto the stable or optimal point). We believe that visualization of the loss\nlandscape can complement this theoretical advances with practical insights.\nTherefore, (1) we introduce a simple decoupled risk visualization method\ninspired in the two-step process that performative prediction is. Our approach\nvisualizes the risk landscape with respect to two parameter vectors: model\nparameters and data parameters. We use this method to propose new properties of\nthe interest points, to examine how existing algorithms traverse the risk\nlandscape and perform under more realistic conditions, including strategic\nclassification with non-linear models. (2) Building on this decoupled risk\nvisualization, we introduce a novel setting - extended Performative Prediction\n- which captures scenarios where the distribution reacts to a model different\nfrom the decision-making one, reflecting the reality that agents often lack\nfull access to the deployed model."}
{"id": "2506.09046", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09046", "abs": "https://arxiv.org/abs/2506.09046", "authors": ["Xiaowen Ma", "Chenyang Lin", "Yao Zhang", "Volker Tresp", "Yunpu Ma"], "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation", "comment": null, "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework."}
{"id": "2506.09048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09048", "abs": "https://arxiv.org/abs/2506.09048", "authors": ["Yuxin Dong", "Jiachen Jiang", "Zhihui Zhu", "Xia Ning"], "title": "Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations", "comment": null, "summary": "Task vectors offer a compelling mechanism for accelerating inference in\nin-context learning (ICL) by distilling task-specific information into a\nsingle, reusable representation. Despite their empirical success, the\nunderlying principles governing their emergence and functionality remain\nunclear. This work proposes the Linear Combination Conjecture, positing that\ntask vectors act as single in-context demonstrations formed through linear\ncombinations of the original ones. We provide both theoretical and empirical\nsupport for this conjecture. First, we show that task vectors naturally emerge\nin linear transformers trained on triplet-formatted prompts through loss\nlandscape analysis. Next, we predict the failure of task vectors on\nrepresenting high-rank mappings and confirm this on practical LLMs. Our\nfindings are further validated through saliency analyses and parameter\nvisualization, suggesting an enhancement of task vectors by injecting multiple\nones into few-shot prompts. Together, our results advance the understanding of\ntask vectors and shed light on the mechanisms underlying ICL in\ntransformer-based models."}
{"id": "2506.08029", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08029", "abs": "https://arxiv.org/abs/2506.08029", "authors": ["Jiayu Li", "Masood Mortazavi", "Ning Yan", "Yihong Ma", "Reza Zafarani"], "title": "Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning", "comment": "A briefer version of this paper was accepted as a Work-in-Progress\n  (WIP) at the Design Automation Conference (DAC) 2024", "summary": "The goal of inverse design in distributed circuits is to generate\nnear-optimal designs that meet a desirable transfer function specification.\nExisting design exploration methods use some combination of strategies\ninvolving artificial grids, differentiable evaluation procedures, and specific\ntemplate topologies. However, real-world design practices often require\nnon-differentiable evaluation procedures, varying topologies, and\nnear-continuous placement spaces. In this paper, we propose DCIDA, a design\nexploration framework that learns a near-optimal design sampling policy for a\ntarget transfer function. DCIDA decides all design factors in a compound\nsingle-step action by sampling from a set of jointly-trained conditional\ndistributions generated by the policy. Utilizing an injective interdependent\n``map\", DCIDA transforms raw sampled design ``actions\" into uniquely equivalent\nphysical representations, enabling the framework to learn the conditional\ndependencies among joint ``raw'' design decisions. Our experiments demonstrate\nDCIDA's Transformer-based policy network achieves significant reductions in\ndesign error compared to state-of-the-art approaches, with significantly better\nfit in cases involving more complex transfer functions."}
{"id": "2506.08033", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.08033", "abs": "https://arxiv.org/abs/2506.08033", "authors": ["Axel TahmasebiMoradi", "Vincent Ren", "Benjamin Le-Creurer", "Chetra Mang"], "title": "Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases", "comment": null, "summary": "Aiming to reduce the computational cost of numerical simulations, a\nconvolutional neural network (CNN) and a multi-layer perceptron (MLP) are\nintroduced to build a surrogate model to approximate radiative heat transfer\nsolutions in a 2-D walled domain with participative gases. The originality of\nthis work lays in the adaptation of the inputs of the problem (gas and wall\nproperties) in order to fit with the CNN architecture, more commonly used for\nimage processing. Two precision datasets have been created with the classical\nsolver, ICARUS2D, that uses the discrete transfer radiation method with the\nstatistical narrow bands model. The performance of the CNN architecture is\ncompared to a more classical MLP architecture in terms of speed and accuracy.\nThanks to Optuna, all results are obtained using the optimized hyper parameters\nnetworks. The results show a significant speedup with industrially acceptable\nrelative errors compared to the classical solver for both architectures.\nAdditionally, the CNN outperforms the MLP in terms of precision and is more\nrobust and stable to changes in hyper-parameters. A performance analysis on the\ndataset size of the samples have also been carried out to gain a deeper\nunderstanding of the model behavior."}
{"id": "2506.08043", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications."}
{"id": "2506.08210", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills."}
{"id": "2506.08279", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08279", "abs": "https://arxiv.org/abs/2506.08279", "authors": ["Aditi Sundararaman", "Amogh Adishesha", "Andrew Jaegle", "Dan Bigioi", "Hyoung-Kyu Song", "Jon Kyl", "Justin Mao", "Kevin Lan", "Mojtaba Komeili", "ShahRukh Athar", "Sheila Babayan", "Stanislau Beliasau", "William Buchwalter"], "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage", "comment": "Technical report website: mirage.app/research/seeing-voices, product\n  website: mirage.app", "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links)."}
{"id": "2506.08344", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Ne≈üet √únver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Ta≈ükƒ±n Padƒ±r"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism."}
{"id": "2506.08507", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08507", "abs": "https://arxiv.org/abs/2506.08507", "authors": ["Kuo Yang", "Xingjie Yang", "Linhui Yu", "Qing Xu", "Yan Fang", "Xu Wang", "Zhengyang Zhou", "Yang Wang"], "title": "MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning", "comment": null, "summary": "Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently\nemerged as a powerful paradigm for tackling complex real-world tasks. However,\nexisting Mas construction methods typically rely on manually crafted\ninteraction mechanisms or heuristic rules, introducing human biases and\nconstraining the autonomous ability. Even with recent advances in adaptive Mas\nconstruction, existing systems largely remain within the paradigm of\nsemi-autonomous patterns. In this work, we propose MasHost, a Reinforcement\nLearning (RL)-based framework for autonomous and query-adaptive Mas design. By\nformulating Mas construction as a graph search problem, our proposed MasHost\njointly samples agent roles and their interactions through a unified\nprobabilistic sampling mechanism. Beyond the accuracy and efficiency objectives\npursued in prior works, we introduce component rationality as an additional and\nnovel design principle in Mas. To achieve this multi-objective optimization, we\npropose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy\nthat collaboratively integrates group-relative advantages and action-wise\nrewards. To our knowledge, our proposed MasHost is the first RL-driven\nframework for autonomous Mas graph construction. Extensive experiments on six\nbenchmarks demonstrate that MasHost consistently outperforms most competitive\nbaselines, validating its effectiveness, efficiency, and structure rationality."}
{"id": "2506.08570", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08570", "abs": "https://arxiv.org/abs/2506.08570", "authors": ["Or Tal", "Felix Kreuk", "Yossi Adi"], "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation", "comment": null, "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM"}
{"id": "2506.08591", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.08591", "abs": "https://arxiv.org/abs/2506.08591", "authors": ["Chengchao Shen", "Hourun Zhu", "Gongfan Fang", "Jianxin Wang", "Xinchao Wang"], "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers", "comment": null, "summary": "Transformer models achieve excellent scaling property, where the performance\nis improved with the increment of model capacity. However, large-scale model\nparameters lead to an unaffordable cost of computing and memory. We analyze\npopular transformer architectures and find that multilayer perceptron (MLP)\nmodules take up the majority of model parameters. To this end, we focus on the\nrecoverability of the compressed models and propose a Diversity-Guided MLP\nReduction (DGMR) method to significantly reduce the parameters of large vision\ntransformers with only negligible performance degradation. Specifically, we\nconduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons\nof MLP hidden layer, while preserving weight diversity for better performance\nrecover during distillation. Compared to the model trained from scratch, our\npruned model only requires 0.06\\% data of LAION-2B (for the training of large\nvision transformers) without labels (ImageNet-1K) to recover the original\nperformance. Experimental results on several state-of-the-art large vision\ntransformers demonstrate that our method achieves a more than 57.0\\% parameter\nand FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),\nour method accomplishes a 71.5\\% parameter and FLOPs reduction without\nperformance degradation. The source code and trained weights are available at\nhttps://github.com/visresearch/DGMR."}
{"id": "2506.08725", "categories": ["cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08725", "abs": "https://arxiv.org/abs/2506.08725", "authors": ["Hyeon Jeon", "Jeongin Park", "Sungbok Shin", "Jinwook Seo"], "title": "Stop Misusing t-SNE and UMAP for Visual Analytics", "comment": "9 pages", "summary": "Misuses of t-SNE and UMAP in visual analytics have become increasingly\ncommon. For example, although t-SNE and UMAP projections often do not\nfaithfully reflect true distances between clusters, practitioners frequently\nuse them to investigate inter-cluster relationships. In this paper, we bring\nthis issue to the surface and comprehensively investigate why such misuse\noccurs and how to prevent it. We conduct a literature review of 114 papers to\nverify the prevalence of the misuse and analyze the reasonings behind it. We\nthen execute an interview study to uncover practitioners' implicit motivations\nfor using these techniques -- rationales often undisclosed in the literature.\nOur findings indicate that misuse of t-SNE and UMAP primarily stems from\nlimited discourse on their appropriate use in visual analytics. We conclude by\nproposing future directions and concrete action items to promote more\nreasonable use of DR."}
{"id": "2506.08780", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08780", "abs": "https://arxiv.org/abs/2506.08780", "authors": ["Isaac Corley", "Lakshay Sharma", "Ruth Crasto"], "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models", "comment": null, "summary": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L."}
{"id": "2506.08862", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08862", "abs": "https://arxiv.org/abs/2506.08862", "authors": ["Zike Wu", "Qi Yan", "Xuanyu Yi", "Lele Wang", "Renjie Liao"], "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams", "comment": null, "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat."}
{"id": "2506.08911", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.08911", "abs": "https://arxiv.org/abs/2506.08911", "authors": ["Petar Jaku≈°", "Hrvoje D≈æapo"], "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "comment": "4 pages", "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms."}
{"id": "2506.08955", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08955", "abs": "https://arxiv.org/abs/2506.08955", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "title": "Segment Concealed Objects with Incomplete Supervision", "comment": "IEEE TPAMI", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models."}
{"id": "2506.08956", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08956", "abs": "https://arxiv.org/abs/2506.08956", "authors": ["DaeEun Yoon", "Semin Kim", "SangWook Yoo", "Jongha Lee"], "title": "Data Augmentation For Small Object using Fast AutoAugment", "comment": "Accepted and published in the USB Proceedings of the 20th\n  International Conference on Modeling Decisions for Artificial Intelligence\n  (MDAI 2023), Ume{\\aa}, Sweden, June 19--22, 2023, ISBN 978-91-527-7293-5,\n  pp.\\ 12--21", "summary": "In recent years, there has been tremendous progress in object detection\nperformance. However, despite these advances, the detection performance for\nsmall objects is significantly inferior to that of large objects. Detecting\nsmall objects is one of the most challenging and important problems in computer\nvision. To improve the detection performance for small objects, we propose an\noptimal data augmentation method using Fast AutoAugment. Through our proposed\nmethod, we can quickly find optimal augmentation policies that can overcome\ndegradation when detecting small objects, and we achieve a 20% performance\nimprovement on the DOTA dataset."}
{"id": "2506.08990", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.08990", "abs": "https://arxiv.org/abs/2506.08990", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Dongyun Liang", "Jing Qin", "Liansheng Wang"], "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "comment": "TMI 2025", "summary": "Medical vision-language alignment through cross-modal contrastive learning\nshows promising performance in image-text matching tasks, such as retrieval and\nzero-shot classification. However, conventional cross-modal contrastive\nlearning (CLIP-based) methods suffer from suboptimal visual representation\ncapabilities, which also limits their effectiveness in vision-language\nalignment. In contrast, although the models pretrained via multimodal masked\nmodeling struggle with direct cross-modal matching, they excel in visual\nrepresentation. To address this contradiction, we propose ALTA (ALign Through\nAdapting), an efficient medical vision-language alignment method that utilizes\nonly about 8% of the trainable parameters and less than 1/5 of the\ncomputational consumption required for masked record modeling. ALTA achieves\nsuperior performance in vision-language matching tasks like retrieval and\nzero-shot classification by adapting the pretrained vision model from masked\nrecord modeling. Additionally, we integrate temporal-multiview radiograph\ninputs to enhance the information consistency between radiographs and their\ncorresponding descriptions in reports, further improving the vision-language\nalignment. Experimental evaluations show that ALTA outperforms the\nbest-performing counterpart by over 4% absolute points in text-to-image\naccuracy and approximately 6% absolute points in image-to-text retrieval\naccuracy. The adaptation of vision-language models during efficient alignment\nalso promotes better vision and language understanding. Code is publicly\navailable at https://github.com/DopamineLcy/ALTA."}
{"id": "2506.09024", "categories": ["cs.CV", "cs.LG", "I.2.11; I.4.9; I.4.9; J.3; I.2.0"], "pdf": "https://arxiv.org/pdf/2506.09024", "abs": "https://arxiv.org/abs/2506.09024", "authors": ["Felix Wagner", "Pramit Saha", "Harry Anthony", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "comment": null, "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard training data\nafter deployment or assume that test samples and training data are centrally\nstored together, an assumption that rarely holds in real-world settings. This\nis because shipping training data with the deployed model is usually impossible\ndue to the size of training databases, as well as proprietary or privacy\nconstraints. We introduce the Isolation Network, an OOD detection framework\nthat quantifies the difficulty of separating a target test sample from the\ntraining data by solving a binary classification task. We then propose\nDecentralized Isolation Networks (DIsoN), which enables the comparison of\ntraining and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code will be available upon\nacceptance at: *****"}
{"id": "2506.09027", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09027", "abs": "https://arxiv.org/abs/2506.09027", "authors": ["Runqian Wang", "Kaiming He"], "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "comment": null, "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning."}
