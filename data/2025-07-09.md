<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 81]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 29]
- [eess.SY](#eess.SY) [Total: 9]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.LG](#cs.LG) [Total: 70]
- [cs.SD](#cs.SD) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: 通过结构化标注提升生成模型与文本的匹配度。


<details>
  <summary>Details</summary>
Motivation: 生成模型在文本到图像任务中因数据噪声大而难以准确匹配提示，需依赖复杂的提示工程。

Method: 使用结构化标注（主题、场景、美学、相机细节）训练模型，并对比结构化与非结构化标注的效果。

Result: 结构化标注显著提高了文本-图像对齐分数。

Conclusion: 结构化标注是提升生成模型可控性和对齐性的有效方法。

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [2] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: 提出了一种名为CorrDetail的视觉细节增强自校正框架，用于可解释的人脸伪造检测，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，面部深度伪造对安全领域构成重大挑战，亟需有效的检测方法。现有方法存在解释性不足或幻觉问题。

Method: 设计了CorrDetail框架，通过错误引导提问校正伪造细节，并引入视觉细粒度细节增强模块和融合决策策略。

Result: 实验表明，CorrDetail在性能上优于最新方法，能准确识别伪造细节，并具备强泛化能力。

Conclusion: CorrDetail为可解释的人脸伪造检测提供了有效解决方案，兼具高性能和鲁棒性。

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [3] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: YOLO-APD是一种改进的深度学习架构，专为复杂道路上的行人检测设计，结合多种创新模块，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决在几何复杂的道路上（如Type-S曲面）RGB摄像头方法在行人检测中的局限性。

Method: 引入YOLO-APD架构，整合SimAM注意力机制、C3Ghost模块、SimSPPF模块、Mish激活函数和IGD模块，并利用车辆转向动态进行自适应区域处理。

Result: 在CARLA数据集上达到77.7% mAP@0.5:0.95和96%的召回率，实时处理能力为100 FPS。

Conclusion: YOLO-APD在复杂环境中表现出高精度和高效性，为低成本传感器的感知系统提供了新方向。

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [4] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Spotlight是一种虚拟染色方法，通过引导模型关注细胞结构，改进了形态学表示。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟染色方法对所有像素平等处理，导致背景噪声和伪影被复制，而非聚焦于生物信号。

Method: Spotlight使用基于直方图的前景估计来屏蔽像素级损失，并通过软阈值预测计算Dice损失以实现形状感知学习。

Result: 在3D基准数据集上，Spotlight改善了形态学表示，同时保持了像素级精度。

Conclusion: Spotlight生成的虚拟染色更适合下游任务，如分割和分析。

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [5] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: 提出了一种新的激活函数SoftReMish，用于提升CNN在图像分类任务中的性能，实验表明其优于ReLU、Tanh和Mish。


<details>
  <summary>Details</summary>
Motivation: 改进CNN在图像分类任务中的性能，探索更优的激活函数。

Method: 在标准CNN架构中使用SoftReMish替换其他激活函数，并在MNIST数据集上进行评估。

Result: SoftReMish取得了最低训练损失（3.14e-8）和最高验证准确率（99.41%），优于其他激活函数。

Conclusion: SoftReMish具有更好的收敛性和泛化能力，适用于视觉识别任务。

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [6] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: 论文探讨了基础模型在农业监测中的应用潜力，提出了农业基础模型（CropFM）的需求框架，并评估了现有通用基础模型在农业任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着人口增长和气候变化加剧，粮食安全成为全球关注的问题，需要创新解决方案来提高农业生产力。基础模型在遥感和气候科学中的成功应用为农业监测提供了新机会。

Method: 论文定量评估了现有基础模型在农业任务中的有效性，提出了CropFM的需求框架，并比较和评估了两类通用基础模型在三个代表性农业任务中的表现。

Result: 研究发现现有通用基础模型在农业任务中表现有限，强调了开发专门针对农业的基础模型的必要性。

Conclusion: 论文呼吁开发专门为农业设计的基础模型（CropFM），以更好地满足农业监测的需求。

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [7] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的水下图像增强方法，结合人类主观评估训练生成对抗网络（GAN），显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像处理领域取得显著进展，但水下图像质量提升仍面临挑战，需结合人类主观评估以优化效果。

Method: 1. 训练分类器网络区分高/低质量图像；2. 使用GAN基于多种增强标准优化低质量图像。

Result: 模型在PSNR、SSIM和UIQM等指标及定性分析中表现优异，尤其在颜色保真度和清晰度方面提升显著。

Conclusion: 结合人类主观评估的深度学习模型能有效提升水下图像质量，为实际应用提供可靠解决方案。

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [8] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: pFedMMA是一种个性化联邦学习框架，利用多模态适配器优化视觉语言任务，在个性化和泛化之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化联邦学习中往往牺牲泛化能力，pFedMMA旨在解决这一问题。

Method: 通过多模态适配器和不对称优化策略，客户端可本地适应个性化数据分布，同时协作训练共享投影以提升泛化。

Result: 在11个数据集上的实验表明，pFedMMA在个性化和泛化之间取得了最佳平衡。

Conclusion: pFedMMA在视觉语言任务中表现优异，代码已开源。

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [9] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: LoongX是一种基于多模态神经生理信号的无手图像编辑方法，利用扩散模型和脑机接口技术，性能接近文本驱动方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像编辑依赖手动操作，对运动或语言能力有限的人不友好。LoongX旨在通过神经信号实现无障碍编辑。

Method: 结合扩散模型和多模态神经信号（EEG、fNIRS等），使用CS3和DGF模块处理信号异质性，并通过对比学习预训练编码器。

Result: LoongX性能接近文本驱动方法（如CLIP-I和DINO），且在结合语音时表现更优。

Conclusion: 神经驱动生成模型为无障碍图像编辑提供了新方向，支持认知驱动的创意技术发展。

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [10] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: 该论文综述了运动生成领域的最新进展，重点分析了不同生成方法（如GANs、自编码器、扩散模型等）的优势与局限，并提供了分类、评估指标和数据集的详细概述。


<details>
  <summary>Details</summary>
Motivation: 运动生成在计算机视觉、图形学和机器人学中具有广泛应用，但多样化的生成方法需要系统化的综述以促进比较和挑战识别。

Method: 基于生成策略对运动生成方法进行分类，分析架构原理、条件机制和生成设置，并总结评估指标与数据集。

Result: 提供了对2023年以来顶级会议论文的详细分析，明确了当前方法的优缺点和开放挑战。

Conclusion: 该综述为研究人员和实践者提供了清晰的比较框架和未来研究方向，推动了运动生成领域的发展。

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [11] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合2D扩散编辑和逆渲染的3D场景局部编辑方法，解决了3D语义解析性能不足的问题，实现了高效且一致的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 3D语义解析在局部编辑中表现不佳，限制了编辑的精确性和一致性，需要一种更有效的方法来支持精确的区域修改。

Method: 利用2D扩散编辑识别修改区域，通过逆渲染进行3D定位，结合深度图预测初始化3D高斯分布，并通过迭代优化提升结构和纹理一致性。

Result: 实验表明，该方法在性能上达到最优，同时实现了4倍的速度提升。

Conclusion: 该方法为3D场景局部编辑提供了一种高效且一致的新解决方案。

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [12] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: OpenWorldSAM扩展了SAM2，通过集成轻量级视觉语言模型的多模态嵌入，实现了开放词汇场景下的对象分割。


<details>
  <summary>Details</summary>
Motivation: 解决基于开放语言提示的对象分割问题，将文本语义精确映射到空间掩码，并处理多样化和未见过的类别。

Method: 结合SAM2和视觉语言模型的多模态嵌入，支持多样提示、高效训练、实例感知和强泛化能力。

Result: 在多个基准测试中（如ADE20k、PASCAL等），实现了开放词汇语义、实例和全景分割的领先性能。

Conclusion: OpenWorldSAM在开放词汇场景下表现出色，具有高效性和强泛化能力。

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [13] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: 开发了一种基于AI的智能喷雾系统，用于实时检测杂草并动态调整喷雾量，以减少除草剂使用和环境问题。


<details>
  <summary>Details</summary>
Motivation: 解决现代农业中除草剂过度使用导致的成本增加、环境污染和杂草抗药性问题。

Method: 集成轻量级YOLO11n和YOLO11n-seg深度学习模型，部署在NVIDIA Jetson Orin Nano上，通过Arduino Uno控制喷嘴。

Result: YOLO11n模型mAP@50为0.98，喷雾覆盖率为24.22%，能根据冠层大小实时调整喷雾量。

Conclusion: 展示了实时深度学习与低成本硬件结合在选择性除草剂应用中的潜力，未来将扩展检测能力并进行更多验证。

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [14] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: 利用自然驾驶视频和大视觉模型分析老年驾驶员的认知状态，提出一种早期检测认知衰退的框架。


<details>
  <summary>Details</summary>
Motivation: 当前认知衰退（如阿尔茨海默病和轻度认知障碍）的诊断方法耗时且昂贵，导致漏诊率高。通过分析真实驾驶行为，提取与认知衰退相关的“数字指纹”，实现早期检测。

Method: 提出一个框架，结合大视觉模型和自然驾驶视频，分析驾驶员行为，分类认知状态并预测疾病进展。

Result: 该方法能够识别功能损害的早期预警信号，支持主动干预策略的开发。

Conclusion: 该研究为早期检测认知衰退提供了可扩展、非侵入性的监测系统，有助于减轻老龄化社会中认知衰退的社会和经济负担。

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [15] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: 论文提出了一种基于尺度不变噪声的扩散模型（Cloud Diffusion Model），以替代传统的白噪声扩散模型，旨在提升推理速度、高频细节和可控性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用白噪声生成图像，但自然图像的统计特性具有尺度不变性，因此提出使用尺度不变噪声以更好地匹配自然图像的特性。

Method: 通过将尺度不变噪声（而非白噪声）引入扩散模型，构建Cloud Diffusion Model。

Result: 预计该模型能实现更快的推理、更好的高频细节和更高的可控性。

Conclusion: Cloud Diffusion Model有望改进传统扩散模型的性能，未来将通过实验验证其效果。

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [16] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet提出了一种多视图扩散架构，通过并行应用扩散模型生成一致的多视图图像，显著提升了3D网格重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决单图像生成多视图图像时空间一致性不足的问题，以提高3D网格重建质量。

Method: 使用并行扩散模型构建共享潜在空间，通过投影和融合多视图编码生成一致的多视图图像。

Result: LoomNet在15秒内生成16个高质量且一致的多视图图像，在图像质量和重建指标上优于现有方法。

Conclusion: LoomNet通过共享潜在空间和多视图融合，显著提升了多视图生成的一致性和多样性。

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [17] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: 提出了一种跨模态检索模型llama-nemoretriever-colembed，通过改进NVIDIA Eagle2 VLM架构并集成ColBERT式交互机制，在多个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 满足跨模态检索系统日益增长的需求。

Method: 改进NVIDIA Eagle2 VLM架构，替换因果注意力为双向注意力，集成ColBERT式交互机制，采用两阶段训练策略。

Result: 3B模型在ViDoRe V1和V2上分别取得NDCG@5 91.0和63.5，性能领先。

Conclusion: 模型在检索精度上表现优异，但需权衡存储和效率。

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [18] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 论文提出了一种增强低成本单目行车记录仪数据的流程，模拟非洲驾驶场景中的光学失真和天气影响，并发布了工具包和基准结果。


<details>
  <summary>Details</summary>
Motivation: 解决非洲等发展中地区自动驾驶数据集稀缺的问题，提升低资源环境下的感知能力。

Method: 通过折射模块模拟低质量镜头和空气湍流的光学效果，天气模块添加雾和镜头光晕，并提供了三种图像恢复模型的基准性能。

Result: 发布了失真工具包、增强数据集和基准结果，支持非洲等低资源环境下的感知研究。

Conclusion: 该方法为低成本数据增强提供了有效工具，有助于填补发展中地区自动驾驶研究的空白。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [19] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: ReLayout是一种基于关系链式思维（relation-CoT）的新方法，通过引入明确的关系定义和布局原型重平衡采样器，解决了现有LLM方法在布局生成中的结构和多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的布局生成方法未能充分解释视觉主题和设计元素之间的空间关系，导致生成布局的结构和多样性不足。

Method: ReLayout通过引入区域、显著性和边距等明确关系定义，将布局分解为更小、结构化且递归的子布局，并设计布局原型重平衡采样器以量化不同布局风格。

Result: 实验结果表明，ReLayout在生成结构化和多样化的布局方面优于基线方法，且更符合人类审美和可解释性。

Conclusion: ReLayout通过关系链式思维和原型重平衡，显著提升了布局生成的质量和多样性。

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [20] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: 提出了一种跨模态转换引导网络（CTNet），用于解决多模态人脸防伪（FAS）中的分布差异和模态缺失问题。


<details>
  <summary>Details</summary>
Motivation: 在多模态FAS中，真实人脸的特征转换比伪造人脸更一致，且模态缺失问题显著。

Method: 通过学习真实样本的跨模态特征转换构建通用特征空间，并利用伪造样本的不一致性检测OOD攻击；同时从RGB模态学习互补的IR和深度特征。

Result: CTNet在大多数协议中优于现有的多模态FAS方法。

Conclusion: CTNet有效解决了多模态FAS中的分布差异和模态缺失问题，提升了性能。

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [21] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散的半监督缺陷检测框架（DSYM），通过两阶段协作训练和联合优化策略，显著提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统工业缺陷检测方法效率低、成本高且鲁棒性差，需要一种高精度且对标注数据依赖低的解决方案。

Method: 采用两阶段协作训练机制和联合优化策略，利用标注数据初始化训练，并通过生成伪标签引入未标注数据。条件扩散模型生成多尺度伪缺陷样本，CLIP跨模态特征过滤噪声。

Result: 在NEU-DET数据集上，与传统监督方法相比，使用相同标注数据时达到78.4% mAP@0.5，仅需40%标注数据时达到75.1% mAP@0.5。

Conclusion: DSYM框架为工业质量检测提供了一种高精度、低标注依赖的缺陷检测方案，已开源。

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [22] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: GSVR是一种基于2D高斯分布的视频表示方法，显著提升了视频解码速度和训练效率，同时保持了高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积网络的视频表示方法解码速度慢、训练时间长，GSVR旨在解决这些问题。

Method: 提出混合变形场建模视频动态，结合三平面运动和多项式运动；动态感知时间切片策略自适应分组视频；量化感知微调避免性能下降。

Result: 在Bunny和UVG数据集上，GSVR解码速度达800+FPS，训练时间仅2秒/帧，性能优于现有方法。

Conclusion: GSVR在视频插值和压缩任务中表现优异，解码速度和训练效率显著提升。

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [23] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0是一个开源的OCR和文档解析工具包，提供多语言文本识别、分层文档解析和关键信息提取功能，性能媲美主流视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 满足大语言模型时代对文档理解的日益增长需求。

Method: 提出PP-OCRv5、PP-StructureV3和PP-ChatOCRv4三种解决方案，支持多语言文本识别、分层文档解析和关键信息提取。

Result: 模型参数少于1亿，但性能与数十亿参数的视觉语言模型相当。

Conclusion: PaddleOCR 3.0为开发者提供了高效的工具和模型库，支持异构硬件加速，便于构建智能文档应用。

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [24] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: Accordion是一个图形设计生成框架，首次尝试将AI生成的设计转换为可编辑的分层设计，并通过用户提示优化无意义的AI生成文本。


<details>
  <summary>Details</summary>
Motivation: AI生成的设计虽然质量高但缺乏可编辑性，而分层设计对设计师至关重要。Accordion旨在解决这一问题，将AI设计转化为可编辑的分层设计。

Method: 基于视觉语言模型（VLM），分三个阶段设计提示引导VLM执行任务，采用自上而下的方式分解分层设计，并利用SAM等视觉专家辅助。

Result: 在DesignIntention基准测试中表现优异，支持文本到模板、背景添加文本和文本去渲染等任务，并能生成设计变体。

Conclusion: Accordion成功将AI生成设计转化为可编辑的分层设计，为设计师提供了高效工具。

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [25] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: Kernel Density Steering (KDS) 是一种新的推理时间框架，通过显式局部模式搜索提升扩散模型在图像修复中的鲁棒性和高保真输出。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像修复中常出现保真度不一致和不良伪影的问题，需要一种更鲁棒的方法。

Method: KDS 使用 N 粒子扩散样本集合，通过计算其输出的局部核密度估计梯度，引导样本向更高密度的区域移动，避免伪影。

Result: 实验表明，KDS 在超分辨率和图像修复任务中显著提升了定量和定性性能。

Conclusion: KDS 是一种即插即用的框架，无需重新训练或外部验证器，可无缝集成到各种扩散采样器中。

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [26] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: 提出了一种名为GenHMC的生成方法，利用未配对的HMC数据生成高质量合成图像，解决了VR/AR中真实感虚拟角色动画的挑战。


<details>
  <summary>Details</summary>
Motivation: 在VR/AR中，获取真实面部状态的困难导致现有方法依赖昂贵的配对数据收集，且无法复用。

Method: 通过生成模型GenHMC，利用未配对的HMC数据直接生成合成图像，解耦面部表情和外观。

Result: 方法能够生成更准确的地面真实数据，并泛化到未见过的身份，提升数据效率和准确性。

Conclusion: GenHMC为虚拟角色动画提供了更高效和准确的解决方案，减少了对配对数据的依赖。

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [27] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: AdaptaGen框架通过分层语义优化和跨模态适应机制，解决了领域特定图像生成中的语义偏差和幻觉问题，显著提升了图像质量、多样性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在领域特定图像生成中忽视了语义理解与视觉表示的依赖关系，且未能有效融入领域特定语义约束，导致生成结果出现幻觉和语义偏差。

Method: 提出AdaptaGen框架，结合矩阵提示优化和多视角理解，设计跨模态适应机制和两阶段标题语义转换，以保持语义一致性和视觉多样性。

Result: 实验表明，AdaptaGen在40个类别上仅需每类16张图像，即可显著提升图像质量、多样性和语义一致性。

Conclusion: AdaptaGen通过分层语义优化和跨模态适应，有效解决了领域特定图像生成的挑战，为高质量视觉内容生成提供了新思路。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [28] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 论文提出了一种结合RGB和事件传感器的融合方法，用于解决航天器姿态估计中极端光照条件下的挑战。


<details>
  <summary>Details</summary>
Motivation: 航天器姿态估计对自主在轨操作至关重要，但传统RGB传感器在极端光照条件下表现不佳，而事件传感器虽动态范围高，但空间分辨率低且低运动时信噪比差。

Method: 采用分束棱镜实现光学和时间对齐，开发了基于RANSAC的融合技术，结合RGB和事件传感器的优势，并引入dropout不确定性估计检测极端条件。

Result: 在实验室多种挑战性光照条件下收集的数据集上验证了方法的有效性，支持事件传感器在姿态估计中的应用。

Conclusion: RGB和事件传感器融合方法有效解决了极端光照问题，数据集将公开以支持社区研究。

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [29] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: 论文提出了一种基于焦点映射的特征提取器（OFFSET），用于解决组合图像检索中的视觉数据不均匀性和文本数据优先级问题。


<details>
  <summary>Details</summary>
Motivation: 组合图像检索（CIR）能灵活表达用户需求，但现有方法存在视觉数据不均匀性和文本优先级忽略的问题，导致查询特征退化和视觉焦点偏差。

Method: 提出OFFSET网络，包含主导部分分割和双焦点映射模块，以及文本引导的焦点修正模块，以减少噪声干扰并增强修改焦点的感知。

Result: 在四个基准数据集上的实验验证了方法的优越性。

Conclusion: OFFSET通过改进特征提取和焦点修正，显著提升了组合图像检索的性能。

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [30] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种基于Contourlet变换的复杂扩散模型，用于PolSAR图像分类，解决了传统扩散模型在捕获相位信息和保留细节上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统实值扩散模型在PolSAR数据中难以处理复杂相位信息且细节保留不足。

Method: 利用Contourlet变换分解数据，设计知识引导的复杂扩散网络，结合高低频特征提升分类精度。

Result: 在三个真实PolSAR数据集上表现优于现有方法，尤其在边缘细节和区域同质性上。

Conclusion: 该方法有效提升了PolSAR图像分类的精度和细节保留能力。

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [31] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出动态秩适应（DRA）方法，通过动态分配特征重要性来增强预训练视觉语言模型对新类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调视觉语言模型时对所有特征平等处理，导致对无关特征的过拟合，影响新类别的识别能力。

Method: DRA通过动态评估和分组特征重要性，分配不同的秩，并引入通道响应机制和L1正则化以稳定训练。

Result: 实验表明DRA在多个基准测试中优于现有方法，显著提升新类别的泛化性能。

Conclusion: DRA通过动态特征重要性分配有效提升了模型对新类别的泛化能力，具有广泛的应用潜力。

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [32] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的框架，用于分析和逆转脑损伤过程，通过分割异常区域、估计和逆转组织变形，最终估计损伤前的健康大脑。


<details>
  <summary>Details</summary>
Motivation: 现有脑损伤分割方法未能区分受损和变形组织，导致准确性不足。

Method: 采用扩散模型框架，包括分割异常区域、逆转组织变形、分离核心损伤区域，并通过修复估计健康大脑。

Result: 与传统方法相比，该方法在损伤分割、表征和大脑标记方面表现出更高的准确性。

Conclusion: 该框架为脑损伤分析提供了更精确的工具，适用于临床和研究应用。

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [33] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: R-VLM是一种新型GUI元素定位方法，通过放大区域提案和IoU感知目标函数，显著提升了GUI自动化的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉GUI代理在处理复杂屏幕截图时效率低且准确性不足，需改进元素定位方法。

Method: 提出R-VLM方法，利用放大区域提案和IoU感知目标函数优化元素定位。

Result: 在ScreenSpot和AgentStudio基准上提升13%的定位准确率，在AITW和Mind2Web任务中提升3.2-9.7%。

Conclusion: R-VLM通过结合VLM与传统目标检测技术，显著提升了GUI自动化的性能。

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [34] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 论文介绍了首个大规模医学视频生成数据集MedVideoCap-55K，并基于此开发了MedGen模型，在视觉质量和医学准确性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学视频生成在临床培训和教育中至关重要，但现有模型因缺乏高质量数据集而表现不佳。

Method: 构建了包含55,000个医学视频片段的MedVideoCap-55K数据集，并开发了MedGen模型。

Result: MedGen在开源模型中表现领先，与商业系统相当。

Conclusion: 数据集和模型为医学视频生成研究提供了重要资源。

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [35] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种集成结构提示（ISP）方法，用于增强视觉语言模型（VLM）中文本和图像分支的信息交互，通过自结构和跨结构提示模块建模模态内和模态间的结构关系，并引入样本探测模块动态调整损失系数。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了可学习提示与模态内及模态间令牌的结构关系，且难以平衡基类和新类的性能。

Method: 提出ISP方法，包含自结构和跨结构提示模块，以及动态调整损失系数的样本探测模块。

Result: 在基类到新类泛化、跨数据集评估和领域泛化三个广泛使用的设置中，ISP表现出色。

Conclusion: ISP通过建模结构关系和动态调整损失系数，显著提升了VLM的性能和泛化能力。

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [36] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: LiON-LoRA通过线性可扩展性、正交性和范数一致性改进LoRA融合，实现对视频扩散模型中相机轨迹和物体运动的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在控制相机轨迹和物体运动时存在不稳定融合和非线性可扩展性问题。

Method: 提出LiON-LoRA框架，分析LoRA特征的正交性，强制范数一致性，并集成可控令牌到扩散变换器中。

Result: 实验表明LiON-LoRA在轨迹控制精度和运动强度调整上优于现有方法，泛化能力强。

Conclusion: LiON-LoRA为视频扩散模型提供了更稳定和精确的控制方法。

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [37] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 论文对高光谱异常检测（HAD）技术进行了全面比较，涵盖统计模型、表示方法、经典机器学习和深度学习模型，评估了17个基准数据集，发现深度学习模型检测精度最高，统计模型速度最快。


<details>
  <summary>Details</summary>
Motivation: 高光谱异常检测技术虽发展迅速，但仍面临计算复杂度高、噪声敏感和泛化能力有限等问题，需系统比较现有方法以指导未来研究。

Method: 通过分类比较统计模型、表示方法、经典机器学习和深度学习模型，使用ROC、AUC等指标在17个数据集上评估性能。

Result: 深度学习模型检测精度最高，统计模型速度最快，但各有优缺点。

Conclusion: 研究为高光谱异常检测领域提供了有价值的见解，指出了未来研究方向。

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [38] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: SenseShift6D是一个新的RGB-D数据集，用于研究6D物体姿态估计在真实世界光照和传感器变化下的性能，并探索测试时传感器控制的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计数据集在固定光照和传感器设置下表现良好，但未考虑真实世界的变化。SenseShift6D填补了这一空白，研究传感器控制对性能的影响。

Method: 通过物理调整13种RGB曝光、9种RGB增益、自动曝光、4种深度模式和5种光照水平，生成101.9k RGB和10k深度图像，覆盖1,380种传感器-光照组合。

Result: 测试时传感器控制比数字数据增强更有效，性能接近或优于增加训练数据量和多样性。RGB和深度传感器单独调整有效，联合调整效果更佳。

Conclusion: SenseShift6D将6D姿态评估从数据为中心扩展到传感器感知的鲁棒性，为自适应感知系统奠定了基础。

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [39] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: 本文提出了一种全自动的白平衡校正机制，用于优化显微镜图像的色彩平衡，实验证明其效果优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 显微镜操作中获取色彩准确且平衡的图像具有挑战性，需要一种自动化的解决方案。

Method: 提出了一种全自动的白平衡校正算法，并在200张显微镜图像上进行了实验验证。

Result: 该算法在病理形态学常用的染色图像上表现优于传统的数字摄影白平衡算法。

Conclusion: 该自动白平衡算法在显微镜图像处理中具有更高的有效性，尤其适用于特定染色技术。

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [40] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt是一个从单视图图像生成高质量、可交互的铰接式物体的框架，通过三阶段流程实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注表面几何和纹理，忽略了部件分解和铰接建模，且依赖多视图或交互数据，限制了可扩展性。

Method: 三阶段流程：1) 重建部件分割的完整3D网格；2) 微调视频扩散模型以捕捉部件级铰接先验；3) 优化铰接运动并进行全局纹理细化。

Result: 实验表明，DreamArt能生成高质量铰接物体，具有准确的部件形状、高保真外观和合理的铰接。

Conclusion: DreamArt为铰接式物体生成提供了可扩展的解决方案。

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [41] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出TalkFashion，一种基于文本指令的多功能虚拟试穿系统，结合大语言模型和多模态模型，实现全自动局部编辑和整体换装。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟试穿方法功能单一、灵活性不足的问题，通过文本指令实现多功能试穿和编辑。

Method: 利用大语言模型分析用户指令并激活不同处理流程，结合基于指令的局部重绘模型，无需手动提供掩码。

Result: 实验结果显示，该方法在语义一致性和视觉质量上优于现有方法。

Conclusion: TalkFashion通过文本指令实现了多功能虚拟试穿，提升了编辑任务的灵活性和自动化程度。

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [42] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: SPADE框架通过空间感知去噪网络改进开放词汇PSG，结合逆扩散模型和空间感知关系图变换器，显著提升空间关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放词汇设置下依赖预训练视觉语言模型（VLMs），但忽略了其在空间关系推理中的固有局限性，导致关系预测不理想。

Method: SPADE框架包括两步：(1) 基于逆扩散的UNet校准，通过轻量级LoRA微调策略；(2) 空间感知关系图变换器，捕捉局部和长程上下文信息。

Result: 在PSG和Visual Genome数据集上，SPADE在封闭和开放场景中均优于现有方法，尤其在空间关系预测方面表现突出。

Conclusion: SPADE通过空间感知设计有效解决了VLMs在空间关系推理中的不足，为开放词汇PSG提供了新思路。

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [43] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: 提出了一种名为DREAM的自回归模型，用于端到端文档重建，解决了现有方法中的错误传播和布局信息丢失问题，并在多个子任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有文档重建方法存在错误传播和布局信息丢失的局限性，需要一种更全面的端到端解决方案。

Method: 提出DREAM模型，将文本图像转换为文档重建序列，并引入标准化任务定义、DSM评估指标和DocRec1K数据集。

Result: DREAM在文档重建任务中表现优异，同时在多个子任务（如布局分析、文本识别等）中具有竞争力。

Conclusion: DREAM是一种高效的端到端文档重建模型，能够全面捕捉文档元素信息，并在多个任务中表现良好。

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [44] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: 论文提出了一种基于太阳高度角的合成相机传感器数据生成方法，解决了白天光照变化研究的不足，并展示了其在扩散模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于真实数据采集成本高且受限，研究转向合成数据生成。现有研究在白天光照变化方面存在不足，主要因标签稀缺。

Method: 引入太阳高度角作为全局条件变量，结合定制归一化方法，以捕捉光照特性和图像噪声。

Result: 该方法能准确模拟光照特性和噪声，适用于扩散模型。

Conclusion: 太阳高度角为合成数据生成提供了高效且无需大量标注的解决方案。

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [45] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: 提出了一种生成3D桥梁数据的系统框架，用于解决真实数据不完整的问题，支持分割和补全网络的训练。


<details>
  <summary>Details</summary>
Motivation: 桥梁老化与退化问题日益严重，传统人工检测效率低，现有3D点云技术因数据不完整（如缺失标签和扫描遮挡）受限。

Method: 开发了一个自动生成完整点云（含组件级实例标注、高保真颜色和精确法向量）的框架，并可扩展为模拟多样且物理真实的缺失点云。

Result: 实验显示，使用合成数据训练的PointNet++模型在真实桥梁语义分割中达到84.2%的mIoU，KT-Net在组件补全任务中表现优异。

Conclusion: 该研究为桥梁结构的3D视觉分析提供了创新方法和基础数据集，对推动基础设施自动化管理维护具有重要意义。

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [46] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种新的“2D-3D-2D”框架，通过将2D对象提升为3D表示，在3D环境中进行编辑，再投影回2D图像，解决了现有2D编辑方法在一致性和对象身份保持上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在2D图像编辑中表现优异，但在一致性和对象身份保持上存在局限，因此需要一种更有效的方法。

Method: 采用“2D-3D-2D”框架，先将2D对象转换为3D表示，在3D环境中进行编辑，再通过重投影和无缝修复技术将编辑后的对象还原到2D图像中。

Result: 实验表明，该方法在一致性和对象身份保持上优于现有2D编辑方法（如DragGAN和DragDiffusion），性能显著提升。

Conclusion: 该框架通过引入3D编辑环境，显著提升了2D图像编辑的一致性和对象身份保持能力，为未来研究提供了新方向。

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [47] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: 提出了一种结合视觉基础模型（VFM）和大型语言模型（LLM）的新框架，通过融合模块提升视频理解中的高级认知任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在高级认知任务（如因果推理和未来预测）上表现不足，缺乏常识性世界知识。

Method: 设计了一个融合模块（受Q-Former启发），将视觉特征转化为语言对齐表示，并采用两阶段训练策略（预训练和微调）。

Result: 模型在多个基准测试中达到最优性能，并展现出零样本泛化能力。

Conclusion: 该工作将机器感知从简单识别推向认知理解，为更智能的AI系统铺平道路。

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [48] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种新的少样本分割方法I²R，通过全局语义聚合和方向性掩码策略解决支持-查询图像间的语义差距和视觉相似但语义不同区域的问题，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决少样本分割中因支持-查询图像间的语义差距和视觉相似区域导致的性能下降问题。

Method: 1) 使用类别特定的高级表示聚合全局语义线索；2) 采用方向性掩码策略抑制不一致的支持-查询像素对。

Result: 在PASCAL-5ⁱ和COCO-20ⁱ基准测试中，1-shot设置下mIoU分别提升了1.9%和2.1%。

Conclusion: I²R方法有效解决了少样本分割中的关键问题，性能优于现有方法。

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [49] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: 提出了一种名为USIGAN的新方法，用于解决IHC虚拟染色中弱配对条件下的空间异质性问题，通过提取全局形态语义并设计UOT-CTM和PC-SCM机制，显著提升了生成结果的病理语义一致性。


<details>
  <summary>Details</summary>
Motivation: 在弱配对条件下，IHC虚拟染色任务面临空间异质性导致的病理语义不一致问题，需要一种高效且成本低的解决方案。

Method: 提出USIGAN方法，通过不平衡自信息特征传输提取全局形态语义，设计UOT-CTM和PC-SCM机制优化联合分布和病理语义一致性。

Result: 在两个公开数据集上的实验表明，USIGAN在IoD和Pearson-R等临床指标上表现优异，具有更好的临床相关性。

Conclusion: USIGAN方法有效解决了弱配对条件下的病理语义不一致问题，为病理分析提供了高效且可靠的虚拟染色解决方案。

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [50] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: DFYP是一种动态融合框架，通过结合光谱通道注意力、边缘自适应空间建模和可学习融合机制，提高了作物产量预测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间建模能力和跨作物类型及年份的泛化能力上存在不足，DFYP旨在解决这些问题。

Method: DFYP包含三个关键组件：分辨率感知通道注意力模块（RCA）、自适应算子学习网络（AOL-Net）和双分支架构，结合可学习融合机制。

Result: 在MODIS和Sentinel-2数据集上的实验表明，DFYP在RMSE、MAE和R2指标上优于现有方法。

Conclusion: DFYP在多种农业场景下表现出高效性和鲁棒性，适用于实际农业监测。

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [51] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为D-FCGS的前馈压缩框架，用于动态高斯点云序列的高效压缩，解决了现有方法中场景重建与优化依赖编码耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 自由视点视频（FVV）需要高效的动态3D表示压缩方法，但现有方法通常依赖于场景优化，限制了泛化能力。

Method: 采用Group-of-Frames（GoF）结构和I-P帧编码，通过稀疏控制点提取帧间运动，并利用双先验感知熵模型压缩运动张量。重建时使用控制点引导的运动补偿和细化网络提升视觉一致性。

Result: D-FCGS在实验中实现了与优化方法相当的率失真性能，压缩比超过40倍，且在2秒内完成，同时保持多视角视觉质量。

Conclusion: D-FCGS为动态3D高斯点云的前馈压缩提供了新思路，推动了FVV在沉浸式应用中的可扩展传输与存储。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [52] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: GeoMag是一个用于遥感图像的多粒度解析框架，通过动态调整注意力和分辨率，提升小目标识别能力并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言模型（RS-VLMs）局限于图像和区域级任务，缺乏像素级处理能力，且计算资源消耗大。

Method: 提出GeoMag框架，结合任务驱动的多粒度分辨率调整（TMRA）和提示引导的语义感知裁剪（PSC），动态优化注意力范围。

Result: 在10个基准测试中，GeoMag在像素级任务中表现优异，同时在其他粒度任务中保持竞争力。

Conclusion: GeoMag通过动态注意力机制和分辨率优化，显著提升了遥感图像解析的效率和性能。

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [53] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: 论文提出了一种灵活的多模态跟踪框架，通过动态激活计算单元和新型融合机制，解决了现有跟踪器在数据缺失时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 多模态数据在视觉跟踪中能提升鲁棒性，但传感器同步问题导致数据缺失，现有跟踪器缺乏适应性。

Method: 提出基于缺失率的动态计算单元激活机制，结合异构专家混合融合和视频级掩码策略。

Result: 模型在9个基准测试中达到SOTA性能，适应不同缺失率和场景复杂度。

Conclusion: 该框架在多模态数据缺失情况下表现优异，代码和基准将公开。

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [54] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 本文研究了遥感（RS）图像场景分类中解释方法和评估指标的有效性，分析了五种特征归因方法和十种解释指标，发现其局限性，并提供了选择指南。


<details>
  <summary>Details</summary>
Motivation: 现有xAI方法和评估指标多针对自然图像设计，直接用于遥感图像可能不适用，因此需要研究其在RS场景分类中的有效性。

Method: 方法学和实验分析五种特征归因方法（Occlusion、LIME、GradCAM、LRP、DeepLIFT）和十种解释指标（涵盖忠实性、鲁棒性、定位性、复杂性、随机性）在三个RS数据集上的表现。

Result: 发现扰动方法依赖基线和场景空间特征，梯度方法在多标签图像中表现不佳，定位和复杂性指标对大面积类别不可靠，而鲁棒性和随机性指标更稳定。

Conclusion: 基于分析结果，提供了在RS图像场景分类中选择解释方法、指标和超参数的指南。

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [55] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: MGPO是一种基于强化学习的框架，通过多轮对话自动裁剪关键视觉区域，提升大模型在高分辨率图像上的表现，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 解决大模型处理高分辨率图像时视觉标记过多且无关的问题。

Method: 采用多轮对话框架和强化学习，通过自动裁剪子图像聚焦关键区域，利用二元奖励函数优化。

Result: 在无标注数据上表现优于GRPO，OOD任务提升5.2%，超越GPT-4o。

Conclusion: MGPO能有效提升模型的视觉定位能力，且无需额外标注。

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [56] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: 论文提出通过引入深度感知提升视频实例分割（VIS）的鲁棒性，探索了三种深度集成方法，其中EDC和SV显著提升了性能，EDC方法在OVIS基准上达到56.2 AP，创下新记录。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割面临遮挡、运动模糊和外观变化等挑战，需提升鲁棒性。

Method: 研究了三种深度集成方法：EDC（扩展深度通道）、SV（共享ViT）和DS（深度监督）。

Result: EDC和SV显著提升VIS鲁棒性，EDC在Swin-L骨干下达到56.2 AP，创OVIS基准新记录。

Conclusion: 深度信息是提升视频理解鲁棒性的关键因素。

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [57] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种稀疏表示方法，通过两阶段策略（预测体素占用率和稀疏特征计算）实现高效高分辨率神经表面重建，显著减少存储需求并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 密集3D特征体积虽有效但分辨率受限，难以扩展到高分辨率场景，限制了重建质量。

Method: 两阶段方法：先预测体素占用率，再在占用率高的体素中进行特征计算和体积渲染；开发了高效采样、特征聚合和查询算法。

Result: 存储需求减少50倍以上，支持512^3分辨率（通常为128^3），重建精度优于现有方法。

Conclusion: 稀疏表示方法显著提升了神经表面重建的效率和精度，适用于高分辨率场景。

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [58] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Tora2是Tora的增强版，通过解耦个性化提取器和门控自注意力机制，实现了多实体外观和运动的同步定制，显著提升了视频生成的质量和控制能力。


<details>
  <summary>Details</summary>
Motivation: 改进现有扩散变换模型在视频生成中的多实体外观和运动定制能力，解决多模态条件训练中的对齐问题。

Method: 引入解耦个性化提取器生成个性化嵌入，设计门控自注意力机制整合轨迹、文本和视觉信息，并使用对比损失优化运动与个性化嵌入的映射。

Result: Tora2在多实体定制视频生成中表现优异，提供了先进的运动控制能力，性能与现有定制方法相当。

Conclusion: Tora2在多条件视频生成领域取得了重要进展，首次实现了多实体外观和运动的同步定制。

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [59] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: T-LoRA是一种针对扩散模型个性化设计的时序依赖低秩适应框架，通过动态调整适应策略和权重参数化技术，解决了单图像定制中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在有限样本下容易过拟合，影响泛化能力和输出多样性。本文旨在通过单图像定制解决这一问题，因其具有最大实用潜力。

Method: 提出T-LoRA框架，包含动态适应策略（基于扩散时间步调整低秩更新）和权重参数化技术（正交初始化确保适配器独立性）。

Result: T-LoRA及其组件在概念保真度和文本对齐方面优于标准LoRA和其他方法，适用于数据有限和资源受限场景。

Conclusion: T-LoRA展示了在数据受限情况下扩散模型个性化的潜力，实现了概念保真度和多样性的平衡。

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [60] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 提出了一种自动生成三元组的可扩展流程和合成数据集CIRHS，并引入Hybrid Contextual Alignment框架，显著提升了零样本和监督性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有CIR方法依赖昂贵人工标注三元组的问题，提升可扩展性和零样本能力。

Method: 利用LLM生成多样化提示，控制文本到图像生成模型生成图像对，构建CIRHS数据集；提出CoAlign框架实现全局对齐和局部推理。

Result: 在三个基准测试中实现出色的零样本性能，并在监督训练中超越现有最优方法。

Conclusion: 首次证明在完全合成数据集上训练CIR模型的可行性，验证了检索框架的有效性。

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [61] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: SCINet提出了一种新的部分多标签学习框架，通过捕捉标签与实例的共现模式，结合多模态模型和跨模态融合模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 部分多标签学习需要从不完全标注的数据中提取知识，核心挑战在于准确识别标签与实例之间的模糊关系。

Method: 提出SCINet框架，包括双主导提示模块（利用多模态模型捕捉文本-图像相关性）、跨模态融合模块（建模标签间和实例间关系）和内在语义增强策略（通过图像变换增强语义理解）。

Result: 在四个基准数据集上的实验表明，SCINet优于现有方法。

Conclusion: SCINet通过共现模式和多模态建模，有效解决了部分多标签学习的挑战。

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [62] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: 通过集成多个先进模型提升Deepfake检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测模型在基准数据集上表现优异，但在分布外数据上性能显著下降，需提升泛化能力。

Method: 采用集成学习方法，结合多个先进非对称模型的预测概率，评估其在两个分布外数据集上的表现。

Result: 实验表明，单一模型在不同场景下表现不一致，而集成方法在所有场景中均提供更稳定可靠的性能。

Conclusion: 非对称集成方法为现实中的Deepfake检测提供了鲁棒且可扩展的解决方案。

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [63] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种基于点云与卫星图像对齐的结构化地理配准方法，解决了GNSS信号缺失区域的定位问题，显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号缺失的高楼和桥梁密集区域，现有依赖GNSS和IMU的方法因假设不稳定而失效，需新的解决方案。

Method: 使用预训练Point Transformer分割道路点，提取道路骨架和交叉点进行全局刚性对齐，再通过RBF插值局部优化，并结合SRTM地形数据进行高程校正。

Result: 在KITTI和Perth数据集上，平面配准精度分别提升55.3%和77.4%，高程相关性分别提升30.5%和50.4%。

Conclusion: 该方法无需先验定位信息，能有效恢复GNSS信息并重建城市级3D地图，适用于复杂城市环境。

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [64] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: 本文提出了一种名为GCDA的新框架，通过改进文本编码器和引入字符感知注意力机制，解决了扩散模型在生成图像中无法正确渲染文本的问题，显著提升了文本的可读性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像扩散模型在生成逼真和多样化图像方面表现出色，但无法生成可读且拼写正确的文本，限制了其在广告、学习和创意设计等实际应用中的潜力。

Method: GCDA框架通过双流文本编码器（编码语义和字形信息）、字符感知注意力机制（避免失真）和OCR辅助微调（优化文本可读性）三个模块改进扩散模型。

Result: 在MARIO-10M和T2I-CompBench等数据集上，GCDA在文本渲染（字符错误率0.08 vs 0.21）、人类感知和图像质量（FID:14.3）方面均达到新SOTA。

Conclusion: GCDA显著提升了扩散模型生成文本的能力，为实际应用提供了更高质量的文本图像生成解决方案。

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [65] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: VisualSpeaker提出了一种基于光真实感渲染和视觉语音识别监督的新方法，显著提升了3D面部动画的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖网格域，无法充分利用2D计算机视觉和图形学的快速视觉创新，限制了3D面部动画的表现力和真实感。

Method: 通过光真实感可微分渲染和视觉语音识别监督，结合预训练的视觉自动语音识别模型，提出了一种感知唇读损失函数。

Result: 在MEAD数据集上，Lip Vertex Error指标提升了56.1%，同时保持了网格驱动动画的可控性。

Conclusion: VisualSpeaker在提升动画质量的同时，支持准确的嘴部动作，对消除手语虚拟形象中的歧义至关重要。

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [66] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: MEDTalk框架通过解耦内容和情感嵌入空间，结合音频和文本输入，生成动态且细粒度的情感3D面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于静态和预定义情感标签，缺乏多样性和自然性。

Method: 通过跨重建过程解耦内容和情感嵌入空间，结合音频、文本和多模态输入动态调整情感特征。

Result: 生成具有同步唇动和生动表情的动态情感面部动画，适用于工业生产线。

Conclusion: MEDTalk在情感表达多样性和自然性上优于现有方法，具有实际应用潜力。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [67] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: 提出了一种多模态因果分析模型（MCAM），用于自动驾驶视频理解中的行为识别与推理，解决了现有方法的浅层因果挖掘、模态间伪相关性和忽略自我车辆层面因果建模的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自动驾驶视频理解中存在浅层因果挖掘、模态间伪相关性和忽略自我车辆层面因果建模的局限性，需要一种更有效的模型。

Method: 设计了多级特征提取器捕获长程依赖，动态建模驾驶场景的因果分析模块，以及视觉-语言转换器对齐关键视觉特征与语言表达。

Result: 在BDD-X和CoVLA数据集上，MCAM实现了视觉-语言因果关系学习的最优性能，并能有效捕捉视频序列中的因果特征。

Conclusion: MCAM在自动驾驶应用中表现出色，代码已开源。

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [68] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: 提出了一种新的法线积分方法，显式处理深度不连续性并适用于通用中心相机模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常隐式处理深度不连续性，且仅限于正交或理想针孔相机，限制了应用范围。

Method: 基于局部平面性假设，通过表面法线与射线方向的约束建模，显式处理不连续性。

Result: 在标准法线积分基准测试中达到最优性能，并首次支持通用中心相机模型。

Conclusion: 新方法显著提升了法线积分的准确性和适用性。

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [69] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [70] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: 论文提出了一个名为CAST-Phys的新型高质量数据集，用于多模态远程生理情感识别，解决了现有数据集的不足和接触式设备对情感体验的影响。


<details>
  <summary>Details</summary>
Motivation: 当前情感计算领域缺乏高质量的多模态数据集，且接触式设备可能干扰真实情感反应，因此需要开发非接触式多模态情感识别方法。

Method: 通过构建CAST-Phys数据集，包含PPG、EDA、RR等生理信号和高分辨率面部视频，支持远程信号恢复和多模态情感识别。

Result: 分析表明生理信号在情感识别中至关重要，尤其在面部表情不足的场景下，多模态融合显著提升了识别效果。

Conclusion: CAST-Phys数据集为远程情感识别技术提供了重要支持，展示了多模态融合在非接触式情感识别中的潜力。

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [71] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: DS@GT团队在PlantCLEF 2025挑战赛中提出了一种多物种植物识别方案，结合了视觉Transformer、分块策略和领域先验适应，取得了第二名的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决植被样方图像中多物种植物识别的挑战，提升识别精度。

Method: 使用ViTD2PC24All进行分块推理，4x4分块策略，结合PaCMAP和K-Means聚类及地理过滤，通过多数投票和贝叶斯先验加权聚合结果。

Result: 在私有排行榜上取得了0.348的宏平均F1分数，无需额外训练。

Conclusion: 该方法高效且可复现，代码已开源。

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [72] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock提出了一种基于3D高斯泼溅的几何感知反射建模框架，通过显式解耦传输和反射成分，提升复杂反射场景的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF和3DGS）在处理反射表面时，常将反射误认为物理几何，导致重建质量下降。

Method: 采用双分支表示和高阶球谐函数捕捉高频反射细节，结合反射去除模块和几何感知的双边平滑约束。

Result: Ref-Unlock在反射场景渲染中显著优于传统GS方法，与NeRF模型竞争，并支持灵活的反射编辑。

Conclusion: Ref-Unlock为反射场景的逼真渲染提供了一种高效且通用的解决方案。

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [73] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video是一个统一的视频理解和生成框架，通过多模态大语言模型（MLLMs）生成视觉线索，结合扩散解码器生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要集中于图像处理，缺乏统一的视频理解和生成模型，因此提出了Omni-Video框架。

Method: 采用轻量级架构设计，将视觉头附加到MLLMs上生成视觉标记，并通过适配器将其输入扩散解码器；采用高效的多阶段训练方案。

Result: 模型在视频生成、编辑和理解任务中表现出良好的泛化能力。

Conclusion: Omni-Video为统一视频建模提供了高效且有效的解决方案。

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [74] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种无提示条件扩散框架，用于多目标图像增强，通过局部-全局语义融合策略和LoRA注入知识，解决现有方法依赖文本或原始图像导致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多目标图像时，要么依赖文本条件导致生成对象偏离原始数据，要么依赖原始图像导致多样性不足。

Method: 采用局部-全局语义融合策略提取图像语义替代文本，通过LoRA注入知识，并设计基于奖励模型的计数损失辅助训练。

Result: 实验表明，该方法优于现有基线，在下游任务和域外泛化能力上表现优异。

Conclusion: 提出的框架有效解决了生成多目标图像时的偏差和多样性问题，提升了数据增强效果。

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [75] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: 提出一种基于相似性或邻接矩阵的平滑算子，通过Sinkhorn算法归一化为类似Laplacian的扩散算子，适用于不规则数据。


<details>
  <summary>Details</summary>
Motivation: 传统Laplacian平滑需要严格的结构化域，而简单卷积核和消息传递层对边界有偏倚，需一种更通用的平滑方法。

Method: 利用对称Sinkhorn算法将正平滑算子归一化为扩散算子，继承Laplacian的优良性质。

Result: 生成的算子不仅近似热扩散，还能保留Laplacian的谱信息，适用于点云等不规则数据。

Conclusion: 该方法为不规则数据提供了类似Laplacian的平滑和处理能力，扩展了形状分析和匹配的应用。

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [76] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: OmniPart是一个新型框架，用于生成具有可编辑部分结构的3D对象，通过两阶段协同方法实现高语义解耦和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数生成方法只能产生整体形状，限制了交互应用的实用性，因此需要一种能够生成具有明确、可编辑部分结构的3D资产的方法。

Method: OmniPart分为两阶段：(1) 自回归结构规划模块生成可控的3D部分边界框；(2) 空间条件整流流模型在规划布局内同时合成所有3D部分。

Result: 实验表明，OmniPart实现了最先进的性能，支持用户定义的部分粒度、精确定位和多样化下游应用。

Conclusion: OmniPart为更可解释、可编辑和多功能3D内容铺平了道路。

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [77] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: 论文提出了一种针对科学图表问答的方法，通过优化提示、链式推理和集成模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉问答方法在科学数据解释中的不足，如数值处理和多步推理。

Method: 使用5B至8B参数的模型（如InternVL3）和集成视觉语言模型进行实验。

Result: InternVL3在SciVQA测试集上ROUGE-1和ROUGE-L F1得分为0.740，BERTScore为0.983；集成模型进一步提升了性能。

Conclusion: 提示优化、链式推理和集成模型能有效提升科学视觉问答的能力。

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [78] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: 论文提出了一种通过合成文化数据集CulTwin和定制对比学习改进CLIP模型的方法，以提升其在文化相关任务中的细粒度识别能力。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型（如CLIP）在多模态理解方面表现优异，但在区分视觉相似但文化背景不同的概念时存在困难，主要由于缺乏高质量文化特定数据集和上下文知识。

Method: 设计了数据生成流程构建CulTwin数据集，并通过定制对比学习微调CLIP模型，生成CultureCLIP。

Result: CultureCLIP在文化相关基准测试中表现优于基础CLIP，某些任务中细粒度概念识别提升5.49%，同时保留了CLIP的泛化能力。

Conclusion: 通过数据合成和模型微调，CultureCLIP能有效捕捉细微文化差异，验证了方法的有效性。

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [79] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: SceneDINO提出了一种无监督的语义场景补全方法，利用自监督学习和多视角一致性，无需昂贵标注即可推断3D几何和语义。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵标注数据的依赖，探索无监督的语义场景补全方法。

Method: 结合自监督表示学习和2D无监督场景理解技术，利用多视角一致性自监督训练，提出3D特征蒸馏方法。

Result: 在3D和2D无监督场景理解中达到最佳分割精度，线性探测3D特征与监督方法相当，并展示了领域泛化和多视角一致性。

Conclusion: SceneDINO为单图像3D场景理解提供了强大的无监督基础。

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [80] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: RSRefSeg 2提出了一种解耦的双阶段框架，通过粗定位和精细分割改进遥感图像分割，结合CLIP和SAM的优势，显著提升了分割精度和语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理复杂语义关系和跨模态对齐时存在局限性，主要由于耦合的定位与分割机制导致误差传播和泛化性不足。

Method: 采用双阶段框架：CLIP用于粗定位和生成提示，SAM用于精细分割；引入级联二阶提示器优化语义提示。

Result: 在多个数据集上（RefSegRS、RRSIS-D、RISBench）表现优于现有方法，分割精度提升约3% gIoU。

Conclusion: RSRefSeg 2通过解耦设计和基础模型协作，显著提升了遥感图像分割的精度和语义理解能力。

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [81] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: AnthroTAP提出了一种自动化生成伪标签训练数据的流程，利用SMPL模型解决点跟踪任务中数据标注困难的问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 人类运动数据复杂且标注困难，但适合训练鲁棒的点跟踪器。

Method: 使用SMPL模型拟合视频中的人体，生成伪轨迹，处理遮挡并过滤不可靠轨迹。

Result: 在TAP-Vid基准测试中达到最优性能，数据量和计算资源需求大幅降低。

Conclusion: AnthroTAP为点跟踪任务提供了一种高效的数据生成和训练方法。

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [82] [Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics](https://arxiv.org/abs/2507.05534)
*Erik Hemberg,Eric Liu,Lucille Fuller,Stephen Moskal,Una-May O'Reilly*

Main category: cs.NE

TL;DR: 研究了两种控制器表示方法，结合不同进化算法（包括一种新型LLM支持的变异算子），通过网络安全场景评估代理学习效果，比较了单边进化与协同进化的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探索不同控制器表示与进化算法组合对团队性能的影响，特别是在网络安全场景中。

Method: 结合两种控制器表示与多种进化算法（包括LLM支持的变异算子），通过单边进化和协同进化实验评估性能。

Result: 基于代码逻辑的语法进化算法表现最佳；协同进化降低了性能波动，而单边优化能实现更高的性能峰值。

Conclusion: 协同进化减少了性能波动，但单边优化能实现更高的性能峰值；语法进化算法在团队性能上表现最优。

Abstract: We investigate two representation alternatives for the controllers of teams
of cyber agents. We combine these controller representations with different
evolutionary algorithms, one of which introduces a novel LLM-supported mutation
operator. Using a cyber security scenario, we evaluate agent learning when one
side is trained to compete against a side that does not evolve and when two
sides coevolve with each other. This allows us to quantify the relative merits
and tradeoffs of representation and algorithm combinations in terms of team
performance. Our versions of grammatical evolution algorithms using grammars
that allow a controller to be expressed in code-like logic can achieve the best
team performance. The scenario also allows us to compare the performance impact
and dynamics of coevolution versus evolution under different combinations.
Across the algorithms and representations, we observe that coevolution reduces
the performance highs and lows of both sides while it induces fluctuations on
both sides. In contrast, when only one-side is optimized, performance peaks are
higher and is more sustained than when both sides are optimized with
coevolution.

</details>


### [83] [A Universal Framework for Large-Scale Multi-Objective Optimization Based on Particle Drift and Diffusion](https://arxiv.org/abs/2507.05847)
*Jia-Cheng Li,Min-Rong Chen,Guo-Qiang Zeng,Jian Weng,Man Wang,Jia-Lin Mai*

Main category: cs.NE

TL;DR: 提出了一种基于粒子漂移和扩散的通用框架，用于解决大规模多目标优化问题，显著提升了算法的收敛性和多样性。


<details>
  <summary>Details</summary>
Motivation: 大规模多目标优化问题因高维决策变量导致现有进化算法在收敛性和多样性上表现不佳。

Method: 将优化过程分为三个子阶段（两个粗调和一个细调），根据不同阶段采用不同的漂移-扩散策略模拟粒子运动。

Result: 实验表明，该框架显著提升了MOEAs的收敛性和多样性，并提高了计算效率。

Conclusion: 该框架为大规模多目标优化问题提供了一种有效的解决方案，具有实际应用价值。

Abstract: Large-scale multi-objective optimization poses challenges to existing
evolutionary algorithms in maintaining the performances of convergence and
diversity because of high dimensional decision variables. Inspired by the
motion of particles in physics, we propose a universal framework for
large-scale multi-objective optimization based on particle drift and diffusion
to solve these challenges in this paper. This framework innovatively divides
the optimization process into three sub-stages: two coarse-tuning sub-stages
and one fine-tuning sub-stage. Different strategies of drift-diffusion
operations are performed on the guiding solutions according to the current
sub-stage, ingeniously simulating the movement of particles under diverse
environmental conditions. Finally, representative evolutionary algorithms are
embedded into the proposed framework, and their effectiveness are evaluated
through comparative experiments on various large-scale multi-objective problems
with 1000 to 5000 decision variables. Moreover, comparative algorithms are
conducted on neural network training problems to validate the effectiveness of
the proposed framework in the practical problems. The experimental results
demonstrate that the framework proposed in this paper significantly enhances
the performance of convergence and diversity of MOEAs, and improves the
computational efficiency of algorithms in solving large-scale multi-objective
optimization problems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [84] [A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation](https://arxiv.org/abs/2507.05331)
*TRI LBM Team,Jose Barreiros,Andrew Beaulieu,Aditya Bhat,Rick Cory,Eric Cousineau,Hongkai Dai,Ching-Hsin Fang,Kunimatsu Hashimoto,Muhammad Zubair Irshad,Masha Itkina,Naveen Kuppuswamy,Kuan-Hui Lee,Katherine Liu,Dale McConachie,Ian McMahon,Haruki Nishimura,Calder Phillips-Grafflin,Charles Richter,Paarth Shah,Krishnan Srinivasan,Blake Wulfe,Chen Xu,Mengchao Zhang,Alex Alspach,Maya Angeles,Kushal Arora,Vitor Campagnolo Guizilini,Alejandro Castro,Dian Chen,Ting-Sheng Chu,Sam Creasey,Sean Curtis,Richard Denitto,Emma Dixon,Eric Dusel,Matthew Ferreira,Aimee Goncalves,Grant Gould,Damrong Guoy,Swati Gupta,Xuchen Han,Kyle Hatch,Brendan Hathaway,Allison Henry,Hillel Hochsztein,Phoebe Horgan,Shun Iwase,Donovon Jackson,Siddharth Karamcheti,Sedrick Keh,Joseph Masterjohn,Jean Mercat,Patrick Miller,Paul Mitiguy,Tony Nguyen,Jeremy Nimmer,Yuki Noguchi,Reko Ong,Aykut Onol,Owen Pfannenstiehl,Richard Poyner,Leticia Priebe Mendes Rocha,Gordon Richardson,Christopher Rodriguez,Derick Seale,Michael Sherman,Mariah Smith-Jones,David Tago,Pavel Tokmakov,Matthew Tran,Basile Van Hoorick,Igor Vasiljevic,Sergey Zakharov,Mark Zolotas,Rares Ambrus,Kerri Fetzer-Borelli,Benjamin Burchfiel,Hadas Kress-Gazit,Siyuan Feng,Stacie Ford,Russ Tedrake*

Main category: cs.RO

TL;DR: 论文通过扩展Diffusion Policy范式，评估了多任务机器人操作策略（LBMs），发现多任务预训练提高了策略的成功率和鲁棒性，并减少了新任务的学习数据需求。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人操作和基础模型取得了进展，但对真实性能的评估仍具挑战性，阻碍了发展和理解。

Method: 通过模拟和真实世界数据，扩展Diffusion Policy范式，提出并验证了评估流程，与单任务基线进行盲随机对比。

Result: 多任务预训练使策略更成功、鲁棒，且能更快教授复杂任务，数据需求显著减少；性能随预训练规模和多样性提升。

Conclusion: 多任务预训练是提升机器人操作策略性能的有效途径，未来可进一步扩展规模和多样性。

Abstract: Robot manipulation has seen tremendous progress in recent years, with
imitation learning policies enabling successful performance of dexterous and
hard-to-model tasks. Concurrently, scaling data and model size has led to the
development of capable language and vision foundation models, motivating
large-scale efforts to create general-purpose robot foundation models. While
these models have garnered significant enthusiasm and investment, meaningful
evaluation of real-world performance remains a challenge, limiting both the
pace of development and inhibiting a nuanced understanding of current
capabilities. In this paper, we rigorously evaluate multitask robot
manipulation policies, referred to as Large Behavior Models (LBMs), by
extending the Diffusion Policy paradigm across a corpus of simulated and
real-world robot data. We propose and validate an evaluation pipeline to
rigorously analyze the capabilities of these models with statistical
confidence. We compare against single-task baselines through blind, randomized
trials in a controlled setting, using both simulation and real-world
experiments. We find that multi-task pretraining makes the policies more
successful and robust, and enables teaching complex new tasks more quickly,
using a fraction of the data when compared to single-task baselines. Moreover,
performance predictably increases as pretraining scale and diversity grows.
Project page: https://toyotaresearchinstitute.github.io/lbm1/

</details>


### [85] [Feature Geometry for Stereo Sidescan and Forward-looking Sonar](https://arxiv.org/abs/2507.05410)
*Kalin Norman,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种基于几何的方法，用于在由前视声纳和侧扫声纳组成的跨模态立体声纳系统中，将观测到的特征从一个声纳投影到另一个声纳。


<details>
  <summary>Details</summary>
Motivation: 解决海洋机器人中的立体声学数据融合问题，利用声学几何实现跨模态声纳的特征投影。

Method: 受立体相机极线几何启发，提出声纳几何模型，利用相对位姿信息实现特征投影，并分析特征位置和声纳相对位姿对投影的影响。

Result: 通过模拟结果确定了适用于野外机器人应用（如特征对应和3D信息恢复）的理想立体配置。

Conclusion: 该方法为跨模态立体声纳系统提供了一种有效的几何解决方案，适用于海洋机器人应用。

Abstract: In this paper, we address stereo acoustic data fusion for marine robotics and
propose a geometry-based method for projecting observed features from one sonar
to another for a cross-modal stereo sonar setup that consists of both a
forward-looking and a sidescan sonar. Our acoustic geometry for sidescan and
forward-looking sonar is inspired by the epipolar geometry for stereo cameras,
and we leverage relative pose information to project where an observed feature
in one sonar image will be found in the image of another sonar. Additionally,
we analyze how both the feature location relative to the sonar and the relative
pose between the two sonars impact the projection. From simulated results, we
identify desirable stereo configurations for applications in field robotics
like feature correspondence and recovery of the 3D information of the feature.

</details>


### [86] [CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning](https://arxiv.org/abs/2507.05458)
*Yi-Shiuan Tung,Bradley Hayes,Alessandro Roncone*

Main category: cs.RO

TL;DR: CRED是一种用于主动偏好学习（APL）的轨迹生成方法，通过联合优化环境设计和轨迹选择，提升奖励估计效果。


<details>
  <summary>Details</summary>
Motivation: 机器人需适应人类偏好（如距离、时间和安全的平衡），但现有方法难以探索完整轨迹空间或生成信息量大的查询。

Method: CRED通过环境设计“想象”新场景，并利用反事实推理生成多样且信息量大的轨迹用于排序。

Result: 在GridWorld和OpenStreetMap导航实验中，CRED提升了奖励学习效果并表现出良好的泛化能力。

Conclusion: CRED通过优化环境设计和轨迹选择，有效解决了长时任务中的奖励学习问题。

Abstract: For effective real-world deployment, robots should adapt to human
preferences, such as balancing distance, time, and safety in delivery routing.
Active preference learning (APL) learns human reward functions by presenting
trajectories for ranking. However, existing methods often struggle to explore
the full trajectory space and fail to identify informative queries,
particularly in long-horizon tasks. We propose CRED, a trajectory generation
method for APL that improves reward estimation by jointly optimizing
environment design and trajectory selection. CRED "imagines" new scenarios
through environment design and uses counterfactual reasoning -- by sampling
rewards from its current belief and asking "What if this reward were the true
preference?" -- to generate a diverse and informative set of trajectories for
ranking. Experiments in GridWorld and real-world navigation using OpenStreetMap
data show that CRED improves reward learning and generalizes effectively across
different environments.

</details>


### [87] [Gaussian Process-Based Active Exploration Strategies in Vision and Touch](https://arxiv.org/abs/2507.05522)
*Ho Jin Choi,Nadia Figueroa*

Main category: cs.RO

TL;DR: 提出了一种融合视觉和触觉的GPDF表示方法，用于机器人主动感知物体属性，无需大量预训练数据。


<details>
  <summary>Details</summary>
Motivation: 机器人因缺乏先验知识难以理解物体属性，而人类通过多感官交互学习。

Method: 通过视觉和触觉数据构建GPDF表示，利用点云和不确定性估计迭代优化几何形状。

Result: 实验表明，该方法能有效恢复精确的3D结构，并探索物体表面属性。

Conclusion: GPDF方法为机器人提供了主动探索复杂物体几何和属性的能力，具有扩展潜力。

Abstract: Robots struggle to understand object properties like shape, material, and
semantics due to limited prior knowledge, hindering manipulation in
unstructured environments. In contrast, humans learn these properties through
interactive multi-sensor exploration. This work proposes fusing visual and
tactile observations into a unified Gaussian Process Distance Field (GPDF)
representation for active perception of object properties. While primarily
focusing on geometry, this approach also demonstrates potential for modeling
surface properties beyond geometry. The GPDF encodes signed distance using
point cloud, analytic gradient and Hessian, and surface uncertainty estimates,
which are attributes that common neural network shape representation lack. By
utilizing a point cloud to construct a distance function, GPDF does not need
extensive pretraining on large datasets and can incorporate observations by
aggregation. Starting with an initial visual shape estimate, the framework
iteratively refines the geometry by integrating dense vision measurements using
differentiable rendering and tactile measurements at uncertain surface regions.
By quantifying multi-sensor uncertainties, it plans exploratory motions to
maximize information gain for recovering precise 3D structures. For the
real-world robot experiment, we utilize the Franka Research 3 robot
manipulator, which is fixed on a table and has a customized DIGIT tactile
sensor and an Intel Realsense D435 RGBD camera mounted on the end-effector. In
these experiments, the robot explores the shape and properties of objects
assumed to be static and placed on the table. To improve scalability, we
investigate approximation methods like inducing point method for Gaussian
Processes. This probabilistic multi-modal fusion enables active exploration and
mapping of complex object geometries, extending potentially beyond geometry.

</details>


### [88] [PAPRLE (Plug-And-Play Robotic Limb Environment): A Modular Ecosystem for Robotic Limbs](https://arxiv.org/abs/2507.05555)
*Obin Kwon,Sankalp Yamsani,Noboru Myers,Sean Taylor,Jooyoung Hong,Kyungseo Park,Alex Alspach,Joohyung Kim*

Main category: cs.RO

TL;DR: PAPRLE是一个模块化的机器人肢体生态系统，支持灵活配置和控制，适用于多种输入设备和任务场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人肢体配置和控制的灵活性问题，支持多样化的输入设备和任务需求。

Method: 通过模块化设计和可插拔的操纵设备，支持关节空间和任务空间控制，并提供实时力反馈。

Result: 在多种实际场景中验证了系统的多功能性，支持不同设备和机器人的组合。

Conclusion: PAPRLE的模块化设计和开源发布将推动机器人控制和AI研究的进一步发展。

Abstract: We introduce PAPRLE (Plug-And-Play Robotic Limb Environment), a modular
ecosystem that enables flexible placement and control of robotic limbs. With
PAPRLE, a user can change the arrangement of the robotic limbs, and control
them using a variety of input devices, including puppeteers, gaming
controllers, and VR-based interfaces. This versatility supports a wide range of
teleoperation scenarios and promotes adaptability to different task
requirements. To further enhance configurability, we introduce a pluggable
puppeteer device that can be easily mounted and adapted to match the target
robot configurations. PAPRLE supports bilateral teleoperation through these
puppeteer devices, agnostic to the type or configuration of the follower robot.
By supporting both joint-space and task-space control, the system provides
real-time force feedback, improving user fidelity and physical interaction
awareness. The modular design of PAPRLE facilitates novel spatial arrangements
of the limbs and enables scalable data collection, thereby advancing research
in embodied AI and learning-based control. We validate PAPRLE in various
real-world settings, demonstrating its versatility across diverse combinations
of leader devices and follower robots. The system will be released as open
source, including both hardware and software components, to support broader
adoption and community-driven extension. Additional resources and
demonstrations are available at the project website:
https://uiuckimlab.github.io/paprle-pages

</details>


### [89] [Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube](https://arxiv.org/abs/2507.05607)
*Chongshan Fan,Shenghai Yuan*

Main category: cs.RO

TL;DR: Auto-RubikAI是一个模块化自主规划框架，结合知识库、视觉语言模型和大语言模型，以最小数据需求解决结构化操作任务（如魔方还原）。


<details>
  <summary>Details</summary>
Motivation: 传统机器人系统依赖预定义脚本或大规模演示数据，Auto-RubikAI旨在实现可解释的多步任务执行，减少数据依赖。

Method: 系统采用知识库模块解决符号推理问题，视觉语言模型解析RGB-D输入构建3D场景，大语言模型生成控制代码。

Result: 实验显示79%的任务成功率，相比基线方法减少步骤并保持可解释性和安全性。

Conclusion: Auto-RubikAI为智能制造和机器人教育提供了高效、模块化的任务规划基础。

Abstract: This paper presents Auto-RubikAI, a modular autonomous planning framework
that integrates a symbolic Knowledge Base (KB), a vision-language model (VLM),
and a large language model (LLM) to solve structured manipulation tasks
exemplified by Rubik's Cube restoration. Unlike traditional robot systems based
on predefined scripts, or modern approaches relying on pretrained networks and
large-scale demonstration data, Auto-RubikAI enables interpretable, multi-step
task execution with minimal data requirements and no prior demonstrations. The
proposed system employs a KB module to solve group-theoretic restoration steps,
overcoming LLMs' limitations in symbolic reasoning. A VLM parses RGB-D input to
construct a semantic 3D scene representation, while the LLM generates
structured robotic control code via prompt chaining. This tri-module
architecture enables robust performance under spatial uncertainty. We deploy
Auto-RubikAI in both simulation and real-world settings using a 7-DOF robotic
arm, demonstrating effective Sim-to-Real adaptation without retraining.
Experiments show a 79% end-to-end task success rate across randomized
configurations. Compared to CFOP, DeepCubeA, and Two-Phase baselines, our
KB-enhanced method reduces average solution steps while maintaining
interpretability and safety. Auto-RubikAI provides a cost-efficient, modular
foundation for embodied task planning in smart manufacturing, robotics
education, and autonomous execution scenarios. Code, prompts, and hardware
modules will be released upon publication.

</details>


### [90] [DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation](https://arxiv.org/abs/2507.05627)
*Young Hun Kim,Seungyeon Kim,Yonghyeon Lee,Frank Chongwoo Park*

Main category: cs.RO

TL;DR: DreamGrasp利用预训练图像生成模型的想象力，通过粗3D重建、对比学习实例分割和文本引导细化，实现复杂场景下的鲁棒3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏RGB图像下部分视图3D识别的挑战，特别是在遮挡和杂乱的真实场景中，现有方法难以泛化。

Method: 结合粗3D重建、对比学习实例分割和文本引导的实例细化。

Result: 实验表明，DreamGrasp能准确恢复物体几何，并支持下游任务如顺序清理和目标检索。

Conclusion: DreamGrasp在复杂多物体环境中表现优异，克服了现有方法的局限性。

Abstract: Partial-view 3D recognition -- reconstructing 3D geometry and identifying
object instances from a few sparse RGB images -- is an exceptionally
challenging yet practically essential task, particularly in cluttered, occluded
real-world settings where full-view or reliable depth data are often
unavailable. Existing methods, whether based on strong symmetry priors or
supervised learning on curated datasets, fail to generalize to such scenarios.
In this work, we introduce DreamGrasp, a framework that leverages the
imagination capability of large-scale pre-trained image generative models to
infer the unobserved parts of a scene. By combining coarse 3D reconstruction,
instance segmentation via contrastive learning, and text-guided instance-wise
refinement, DreamGrasp circumvents limitations of prior methods and enables
robust 3D reconstruction in complex, multi-object environments. Our experiments
show that DreamGrasp not only recovers accurate object geometry but also
supports downstream tasks like sequential decluttering and target retrieval
with high success rates.

</details>


### [91] [A Physics-Based Continuum Model for Versatile, Scalable, and Fast Terramechanics Simulation](https://arxiv.org/abs/2507.05643)
*Huzaifa Unjhawala,Luning Bakke,Harry Zhang,Michael Taylor,Ganesh Arivoli,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: Chrono::CRM是一种基于物理的、可扩展的模拟解决方案，用于解决地形力学问题，支持复杂任务如挖掘和与可变形轮交互。


<details>
  <summary>Details</summary>
Motivation: 超越半经验地形力学方法（如Bekker-Wong/Janosi-Hanamoto），提供更通用的物理模型。

Method: 基于Chrono的SPH框架，支持刚性和柔性工具与地形交互，并通过GPU加速实现高效计算。

Result: 验证了与实验数据的一致性，计算效率接近半经验方法，支持大规模地形模拟（10公里，1亿SPH粒子）。

Conclusion: Chrono::CRM为大规模高保真地形模拟提供了开源解决方案，适用于进一步研究和应用。

Abstract: This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.

</details>


### [92] [Learning-Augmented Model-Based Multi-Robot Planning for Time-Critical Search and Inspection Under Uncertainty](https://arxiv.org/abs/2507.06129)
*Abhish Khanal,Joseph Prince Mathew,Cameron Nowzari,Gregory J. Stein*

Main category: cs.RO

TL;DR: 提出了一种多机器人规划框架，用于在不确定条件下协调时间关键的多机器人搜索任务，通过图神经网络和模型规划器提高效率。


<details>
  <summary>Details</summary>
Motivation: 在灾难响应或监视任务中，快速识别需要紧急关注的区域至关重要，但全面部署响应团队效率低下或不可行。

Method: 使用图神经网络从噪声传感器数据中估计需要关注的兴趣点（PoIs）的可能性，并指导多机器人模型规划器制定成本效益计划。

Result: 模拟实验显示，与未学习和学习基线相比，规划器性能分别提高了16.3%、26.7%和26.2%（1、3、5机器人）。

Conclusion: 该方法在模拟和真实四旋翼飞行器平台上均验证了有效性，显著提升了多机器人搜索任务的效率。

Abstract: In disaster response or surveillance operations, quickly identifying areas
needing urgent attention is critical, but deploying response teams to every
location is inefficient or often impossible. Effective performance in this
domain requires coordinating a multi-robot inspection team to prioritize
inspecting locations more likely to need immediate response, while also
minimizing travel time. This is particularly challenging because robots must
directly observe the locations to determine which ones require additional
attention. This work introduces a multi-robot planning framework for
coordinated time-critical multi-robot search under uncertainty. Our approach
uses a graph neural network to estimate the likelihood of PoIs needing
attention from noisy sensor data and then uses those predictions to guide a
multi-robot model-based planner to determine the cost-effective plan. Simulated
experiments demonstrate that our planner improves performance at least by
16.3\%, 26.7\%, and 26.2\% for 1, 3, and 5 robots, respectively, compared to
non-learned and learned baselines. We also validate our approach on real-world
platforms using quad-copters.

</details>


### [93] [3DGS_LSR:Large_Scale Relocation for Autonomous Driving Based on 3D Gaussian Splatting](https://arxiv.org/abs/2507.05661)
*Haitao Lu,Haijier Chen,Haoze Liu,Shoujian Zhang,Bo Xu,Ziao Liu*

Main category: cs.RO

TL;DR: 提出了一种基于3D高斯泼溅（3DGS）的大规模重定位框架3DGS-LSR，仅需单目RGB图像即可实现厘米级定位，适用于复杂城市环境。


<details>
  <summary>Details</summary>
Motivation: 解决复杂城市环境中GNSS定位不可靠及传统地图方法存储和计算效率低的问题。

Method: 结合多传感器数据构建高精度3DGS地图，使用SuperPoint和SuperGlue进行特征提取与匹配，通过迭代优化策略逐步优化定位结果。

Result: 在KITTI数据集上，3DGS-LSR在城镇道路、林荫大道和交通密集高速路上的平均定位精度分别为0.026m、0.029m和0.081m，显著优于其他方法。

Conclusion: 3DGS-LSR为自主机器人提供了在GNSS失效的复杂城市环境中可靠的定位能力。

Abstract: In autonomous robotic systems, precise localization is a prerequisite for
safe navigation. However, in complex urban environments, GNSS positioning often
suffers from signal occlusion and multipath effects, leading to unreliable
absolute positioning. Traditional mapping approaches are constrained by storage
requirements and computational inefficiency, limiting their applicability to
resource-constrained robotic platforms. To address these challenges, we propose
3DGS-LSR: a large-scale relocalization framework leveraging 3D Gaussian
Splatting (3DGS), enabling centimeter-level positioning using only a single
monocular RGB image on the client side. We combine multi-sensor data to
construct high-accuracy 3DGS maps in large outdoor scenes, while the robot-side
localization requires just a standard camera input. Using SuperPoint and
SuperGlue for feature extraction and matching, our core innovation is an
iterative optimization strategy that refines localization results through
step-by-step rendering, making it suitable for real-time autonomous navigation.
Experimental validation on the KITTI dataset demonstrates our 3DGS-LSR achieves
average positioning accuracies of 0.026m, 0.029m, and 0.081m in town roads,
boulevard roads, and traffic-dense highways respectively, significantly
outperforming other representative methods while requiring only monocular RGB
input. This approach provides autonomous robots with reliable localization
capabilities even in challenging urban environments where GNSS fails.

</details>


### [94] [Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains](https://arxiv.org/abs/2507.05663)
*Neelay Joglekar,Fei Liu,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 提出了一种稳定的跟踪闭环控制器，用于解决RCM机械臂运动链中不可见部分的误差问题，并验证了其性能。


<details>
  <summary>Details</summary>
Motivation: RCM机械臂在微创手术中至关重要，但关节读数误差会影响控制精度，尤其是不可见部分的误差无法通过视觉校正。

Method: 设计了一种稳定的跟踪闭环控制器，并将其集成到双层控制方案中，用于整个运动链。

Result: 在仿真和实际环境中验证了控制器的稳定性，填补了相关研究空白。

Conclusion: 为从远程操作到自主手术的过渡提供了关键见解。

Abstract: Remote Center of Motion (RCM) robotic manipulators have revolutionized
Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation
within the patient's body cavity without disturbing the insertion point on the
patient. Accurate RCM tool control is vital for incorporating autonomous
subtasks like suturing, blood suction, and tumor resection into robotic
surgical procedures, reducing surgeon fatigue and improving patient outcomes.
However, these cable-driven systems are subject to significant joint reading
errors, corrupting the kinematics computation necessary to perform control.
Although visual tracking with endoscopic cameras can correct errors on in-view
joints, errors in the kinematic chain prior to the insertion point are
irreparable because they remain out of view. No prior work has characterized
the stability of control under these conditions. We fill this gap by designing
a provably stable tracking-in-the-loop controller for the out-of-view portion
of the RCM manipulator kinematic chain. We additionally incorporate this
controller into a bilevel control scheme for the full kinematic chain. We
rigorously benchmark our method in simulated and real world settings to verify
our theoretical findings. Our work provides key insights into the next steps
required for the transition from teleoperated to autonomous surgery.

</details>


### [95] [Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control](https://arxiv.org/abs/2507.05674)
*Xinyao Qin,Xiaoteng Ma,Yang Qi,Qihan Liu,Chuanyi Xue,Ning Gui,Qinyu Dong,Jun Yang,Bin Liang*

Main category: cs.RO

TL;DR: DMLoco是一个基于扩散模型的框架，结合多任务预训练和在线PPO微调，用于四足机器人的语言条件控制和鲁棒任务转换。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人领域表现出强大能力，但在足式运动中的应用较少，主要由于稳定性问题和有限数据下的任务转换困难。

Method: 通过扩散模型预训练多任务策略，再通过在线PPO微调，结合DDIM高效采样和TensorRT优化部署。

Result: 策略在50Hz下运行，实现了语言引导的适应性运动，适用于资源受限平台。

Conclusion: DMLoco为足式机器人提供了一种可扩展且高效的解决方案，支持语言引导的鲁棒控制。

Abstract: Recent research has highlighted the powerful capabilities of imitation
learning in robotics. Leveraging generative models, particularly diffusion
models, these approaches offer notable advantages such as strong multi-task
generalization, effective language conditioning, and high sample efficiency.
While their application has been successful in manipulation tasks, their use in
legged locomotion remains relatively underexplored, mainly due to compounding
errors that affect stability and difficulties in task transition under limited
data. Online reinforcement learning (RL) has demonstrated promising results in
legged robot control in the past years, providing valuable insights to address
these challenges. In this work, we propose DMLoco, a diffusion-based framework
for quadruped robots that integrates multi-task pretraining with online PPO
finetuning to enable language-conditioned control and robust task transitions.
Our approach first pretrains the policy on a diverse multi-task dataset using
diffusion models, enabling language-guided execution of various skills. Then,
it finetunes the policy in simulation to ensure robustness and stable task
transition during real-world deployment. By utilizing Denoising Diffusion
Implicit Models (DDIM) for efficient sampling and TensorRT for optimized
deployment, our policy runs onboard at 50Hz, offering a scalable and efficient
solution for adaptive, language-guided locomotion on resource-constrained
robotic platforms.

</details>


### [96] [Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning](https://arxiv.org/abs/2507.05695)
*Xiatao Sun,Yuxuan Wang,Shuo Yang,Yinxing Chen,Daniel Rakita*

Main category: cs.RO

TL;DR: 论文提出hPGA-DP方法，通过结合Projective Geometric Algebra (PGA)提升扩散策略的几何归纳偏置，提高训练效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人学习中需要重复学习基础空间表示，引入几何归纳偏置可减少冗余并提升效率。

Method: 提出hPGA-DP方法，结合PGA和P-GATr，采用混合架构（U-Net或Transformer）进行去噪。

Result: 实验表明hPGA-DP提升了任务性能和训练效率，并实现更快的收敛。

Conclusion: hPGA-DP通过几何偏置和混合架构显著提升了扩散策略的性能和效率。

Abstract: Diffusion policies have become increasingly popular in robot learning due to
their reliable convergence in motion generation tasks. At a high level, these
policies learn to transform noisy action trajectories into effective ones,
conditioned on observations. However, each time such a model is trained in a
robotics context, the network must relearn fundamental spatial representations
and operations, such as translations and rotations, from scratch in order to
ground itself and operate effectively in a 3D environment. Incorporating
geometric inductive biases directly into the network can alleviate this
redundancy and substantially improve training efficiency. In this paper, we
introduce hPGA-DP, a diffusion policy approach that integrates a mathematical
framework called Projective Geometric Algebra (PGA) to embed strong geometric
inductive biases. PGA is particularly well-suited for this purpose as it
provides a unified algebraic framework that naturally encodes geometric
primitives, such as points, directions, and rotations, enabling neural networks
to reason about spatial structure through interpretable and composable
operations. Specifically, we propose a novel diffusion policy architecture that
incorporates the Projective Geometric Algebra Transformer (P-GATr), leveraging
its E(3)-equivariant properties established in prior work. Our approach adopts
a hybrid architecture strategy, using P-GATr as both a state encoder and action
decoder, while employing U-Net or Transformer-based modules for the denoising
process. Several experiments and ablation studies in both simulated and
real-world environments demonstrate that hPGA-DP not only improves task
performance and training efficiency through the geometric bias of P-GATr, but
also achieves substantially faster convergence through its hybrid model
compared to architectures that rely solely on P-GATr.

</details>


### [97] [DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving](https://arxiv.org/abs/2507.05710)
*Hyeongchan Ham,Heejin Ahn*

Main category: cs.RO

TL;DR: 提出了一种基于分布鲁棒优化（DRO）和证据深度学习（EDL）的框架，用于处理自动驾驶中的感知不确定性，并通过动态调整保守性提升安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖神经网络感知，但其不确定性可能导致控制决策的安全风险，需解决这一问题。

Method: 结合DRO和EDL，提出动态调整保守性的模糊集，并将其整合到模型预测控制（MPC）中，形成DRO-EDL-MPC算法。

Result: 在CARLA模拟器中验证，高感知置信度下保持效率，低置信度下实施保守约束。

Conclusion: 该框架有效平衡了自动驾驶中感知不确定性与安全性，具有实际应用潜力。

Abstract: Safety is a critical concern in motion planning for autonomous vehicles.
Modern autonomous vehicles rely on neural network-based perception, but making
control decisions based on these inference results poses significant safety
risks due to inherent uncertainties. To address this challenge, we present a
distributionally robust optimization (DRO) framework that accounts for both
aleatoric and epistemic perception uncertainties using evidential deep learning
(EDL). Our approach introduces a novel ambiguity set formulation based on
evidential distributions that dynamically adjusts the conservativeness
according to perception confidence levels. We integrate this uncertainty-aware
constraint into model predictive control (MPC), proposing the DRO-EDL-MPC
algorithm with computational tractability for autonomous driving applications.
Validation in the CARLA simulator demonstrates that our approach maintains
efficiency under high perception confidence while enforcing conservative
constraints under low confidence.

</details>


### [98] [Simultaneous Triggering and Synchronization of Sensors and Onboard Computers](https://arxiv.org/abs/2507.05717)
*Morten Nissov,Nikhil Khedekar,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出了一种低成本、实时的传感器时间戳同步系统，用于解决机器人高保真估计算法中时间戳不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 传感器数据的时间戳准确性对机器人实时估计至关重要，但常被忽视。现有方法虽可通过后处理或调整参数缓解问题，但会牺牲性能。

Method: 引入了一种利用现成组件和成熟同步方法的通用系统，支持高、低速传感器的同步和触发功能。

Result: 系统展示了其在传感器同步和触发方面的能力。

Conclusion: 该系统为实时、低成本的时间戳同步提供了一种有效解决方案。

Abstract: High fidelity estimation algorithms for robotics require accurate data.
However, timestamping of sensor data is a key issue that rarely receives the
attention it deserves. Inaccurate timestamping can be compensated for in
post-processing but is imperative for online estimation. Simultaneously, even
online mitigation of timing issues can be achieved through a relaxation of the
tuning parameters from their otherwise more performative optimal values, but at
a detriment to performance. To address the need for real-time, low-cost
timestamping, a versatile system which utilizes readily-available components
and established methods for synchronization is introduced. The synchronization
and triggering (of both high- and low-rate sensors) capabilities of the system
are demonstrated.

</details>


### [99] [Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model](https://arxiv.org/abs/2507.06174)
*Koki Yamane,Yunhan Li,Masashi Konosu,Koki Inami,Junji Oaki,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本文提出了一种基于4通道双边控制的低成本机械臂快速遥操作方法，解决了传统单边控制缺乏力反馈的问题，并通过实验验证了力信息对模仿学习性能的提升。


<details>
  <summary>Details</summary>
Motivation: 低成本机械臂在遥操作中缺乏力反馈，限制了其在快速或接触丰富任务中的应用。本文旨在解决这一问题。

Method: 利用4通道双边控制，结合非线性补偿、速度和外力估计以及可变增益，实现无力传感器的快速遥操作。

Result: 实验表明，该方法在快速遥操作中表现良好，且力信息的引入提升了模仿学习的性能。

Conclusion: 本文方法为低成本机械臂的高保真遥操作和数据收集提供了实用解决方案。

Abstract: In recent years, the advancement of imitation learning has led to increased
interest in teleoperating low-cost manipulators to collect demonstration data.
However, most existing systems rely on unilateral control, which only transmits
target position values. While this approach is easy to implement and suitable
for slow, non-contact tasks, it struggles with fast or contact-rich operations
due to the absence of force feedback. This work demonstrates that fast
teleoperation with force feedback is feasible even with force-sensorless,
low-cost manipulators by leveraging 4-channel bilateral control. Based on
accurately identified manipulator dynamics, our method integrates nonlinear
terms compensation, velocity and external force estimation, and variable gain
corresponding to inertial variation. Furthermore, using data collected by
4-channel bilateral control, we show that incorporating force information into
both the input and output of learned policies improves performance in imitation
learning. These results highlight the practical effectiveness of our system for
high-fidelity teleoperation and data collection on affordable hardware.

</details>


### [100] [A Learning-based Planning and Control Framework for Inertia Drift Vehicles](https://arxiv.org/abs/2507.05748)
*Bei Zhou,Zhouheng Li,Lei Xie,Hongye Su,Johannes Betz*

Main category: cs.RO

TL;DR: 论文提出了一种基于贝叶斯优化的学习框架，用于解决自动驾驶赛车在惯性漂移中的快速过渡和路径跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 惯性漂移是自动驾驶赛车在连续急转弯中的关键动作，但快速过渡和模型误差使其难以控制。

Method: 利用贝叶斯优化开发规划逻辑和控制策略，以减少速度损失并提升系统性能。

Result: 在8字形路径的仿真中，框架实现了平滑稳定的惯性漂移。

Conclusion: 该框架有效解决了惯性漂移中的控制挑战，提升了自动驾驶赛车的性能。

Abstract: Inertia drift is a transitional maneuver between two sustained drift stages
in opposite directions, which provides valuable insights for navigating
consecutive sharp corners for autonomous racing.However, this can be a
challenging scenario for the drift controller to handle rapid transitions
between opposing sideslip angles while maintaining accurate path tracking.
Moreover, accurate drift control depends on a high-fidelity vehicle model to
derive drift equilibrium points and predict vehicle states, but this is often
compromised by the strongly coupled longitudinal-lateral drift dynamics and
unpredictable environmental variations. To address these challenges, this paper
proposes a learning-based planning and control framework utilizing Bayesian
optimization (BO), which develops a planning logic to ensure a smooth
transition and minimal velocity loss between inertia and sustained drift
phases. BO is further employed to learn a performance-driven control policy
that mitigates modeling errors for enhanced system performance. Simulation
results on an 8-shape reference path demonstrate that the proposed framework
can achieve smooth and stable inertia drift through sharp corners.

</details>


### [101] [LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving](https://arxiv.org/abs/2507.05754)
*Yuhang Zhang,Jiaqi Liu,Chengkai Xu,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: LeAD是一种结合模仿学习和大型语言模型的双速率自动驾驶架构，通过多模态感知和链式推理提升复杂场景处理能力，在CARLA模拟器中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在复杂场景和边缘案例中表现不佳，无法有效理解交通语义信息和其他参与者意图，导致决策与熟练驾驶员不一致。

Method: LeAD整合了模仿学习的端到端框架和大型语言模型，高频率子系统负责实时感知-规划-控制循环，低频率模块通过多模态感知和链式推理优化决策。

Result: 在CARLA模拟器中，LeAD在非常规场景中表现优异，Leaderboard V1基准得分71分，路线完成率93%。

Conclusion: LeAD通过双速率架构和LLM增强，显著提升了自动驾驶系统在复杂场景中的表现。

Abstract: A principal barrier to large-scale deployment of urban autonomous driving
systems lies in the prevalence of complex scenarios and edge cases. Existing
systems fail to effectively interpret semantic information within traffic
contexts and discern intentions of other participants, consequently generating
decisions misaligned with skilled drivers' reasoning patterns. We present LeAD,
a dual-rate autonomous driving architecture integrating imitation
learning-based end-to-end (E2E) frameworks with large language model (LLM)
augmentation. The high-frequency E2E subsystem maintains real-time
perception-planning-control cycles, while the low-frequency LLM module enhances
scenario comprehension through multi-modal perception fusion with HD maps and
derives optimal decisions via chain-of-thought (CoT) reasoning when baseline
planners encounter capability limitations. Our experimental evaluation in the
CARLA Simulator demonstrates LeAD's superior handling of unconventional
scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route
completion of 93%.

</details>


### [102] [Communication-Efficient Module-Wise Federated Learning for Grasp Pose Detection in Cluttered Environments](https://arxiv.org/abs/2507.05861)
*Woonsang Kang,Joohyung Lee,Seungjun Kim,Jungchan Cho,Yoonseon Oh*

Main category: cs.RO

TL;DR: 提出了一种模块化联邦学习框架，用于高效训练抓取姿态检测模型，减少通信开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决抓取姿态检测（GPD）在联邦学习中因大模型通信开销高而难以应用的问题。

Method: 分析GPD模型各模块的学习动态，优先训练收敛慢的模块，分两阶段进行高效通信训练。

Result: 在GraspNet-1B数据集和真实机器人实验中，性能优于FedAvg等基线方法。

Conclusion: 该框架在通信成本和模型性能之间取得了更好的平衡，适用于去中心化训练。

Abstract: Grasp pose detection (GPD) is a fundamental capability for robotic autonomy,
but its reliance on large, diverse datasets creates significant data privacy
and centralization challenges. Federated Learning (FL) offers a
privacy-preserving solution, but its application to GPD is hindered by the
substantial communication overhead of large models, a key issue for
resource-constrained robots. To address this, we propose a novel module-wise FL
framework that begins by analyzing the learning dynamics of the GPD model's
functional components. This analysis identifies slower-converging modules, to
which our framework then allocates additional communication effort. This is
realized through a two-phase process: a standard full-model training phase is
followed by a communication-efficient phase where only the identified subset of
slower-converging modules is trained and their partial updates are aggregated.
Extensive experiments on the GraspNet-1B dataset demonstrate that our method
outperforms standard FedAvg and other baselines, achieving higher accuracy for
a given communication budget. Furthermore, real-world experiments on a physical
robot validate our approach, showing a superior grasp success rate compared to
baseline methods in cluttered scenes. Our work presents a
communication-efficient framework for training robust, generalized GPD models
in a decentralized manner, effectively improving the trade-off between
communication cost and model performance.

</details>


### [103] [Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data](https://arxiv.org/abs/2507.05884)
*Chang Liu,Zhexiong Xue,Tamas Sziranyi*

Main category: cs.RO

TL;DR: 论文比较了多种路径规划算法在2D和3D道路网络中的性能，发现Dijkstra算法在稳定性和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在非结构化环境中导航的挑战，如森林和山区，地形复杂且道路条件不规则。

Method: 使用高分辨率卫星图像和机载LiDAR数据生成加权像素级道路网络，测试A*、Dijkstra、RRT*和NIACO算法在2D和3D场景中的性能。

Result: Dijkstra算法在2D和3D场景中均表现最稳定和高效，尤其是在密集的像素级地理空间道路地图上。

Conclusion: Dijkstra算法适用于静态地形导航，为未来复杂环境约束下的动态路径规划研究奠定了基础。

Abstract: Autonomous vehicle navigation in unstructured environments, such as forests
and mountainous regions, presents significant challenges due to irregular
terrain and complex road conditions. This work provides a comparative
evaluation of mainstream and well-established path planning algorithms applied
to weighted pixel-level road networks derived from high-resolution satellite
imagery and airborne LiDAR data. For 2D road-map navigation, where the weights
reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel
Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe
satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra,
RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset,
which provides detailed elevation information. All algorithms are assessed
under identical start and end point conditions, focusing on path cost,
computation time, and memory consumption. Results demonstrate that Dijkstra
consistently offers the most stable and efficient performance in both 2D and 3D
scenarios, particularly when operating on dense, pixel-level geospatial
road-maps. These findings highlight the reliability of Dijkstra-based planning
for static terrain navigation and establish a foundation for future research on
dynamic path planning under complex environmental constraints.

</details>


### [104] [FineGrasp: Towards Robust Grasping for Delicate Objects](https://arxiv.org/abs/2507.05978)
*Yun Du,Mengao Zhao,Tianwei Lin,Yiwei Jin,Chaodong Huang,Zhizhong Su*

Main category: cs.RO

TL;DR: FineGrasp提出了一种改进的抓取方法，通过增强网络处理精细区域、优化标签平衡策略以及引入模拟数据集，显著提升了小物体抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在抓取小物体或精细部件时表现不佳，导致整个系统失败。

Method: 1. 改进网络以处理精细区域；2. 优化标签平衡策略；3. 引入模拟数据集并进行混合训练。

Result: 实验结果显示，尤其是在小物体抓取方面有显著提升。

Conclusion: FineGrasp在语义抓取中表现出高效性。

Abstract: Recent advancements in robotic grasping have led to its integration as a core
module in many manipulation systems. For instance, language-driven semantic
segmentation enables the grasping of any designated object or object part.
However, existing methods often struggle to generate feasible grasp poses for
small objects or delicate components, potentially causing the entire pipeline
to fail. To address this issue, we propose a novel grasping method, FineGrasp,
which introduces improvements in three key aspects. First, we introduce
multiple network modifications to enhance the ability of to handle delicate
regions. Second, we address the issue of label imbalance and propose a refined
graspness label normalization strategy. Third, we introduce a new simulated
grasp dataset and show that mixed sim-to-real training further improves grasp
performance. Experimental results show significant improvements, especially in
grasping small objects, and confirm the effectiveness of our system in semantic
grasping.

</details>


### [105] [AURA-CVC: Autonomous Ultrasound-guided Robotic Assistance for Central Venous Catheterization](https://arxiv.org/abs/2507.05979)
*Deepak Raina,Lidia Al-Zogbi,Brian Teixeira,Vivek Singh,Ankur Kapoor,Thorsten Fleiter,Muyinatu A. Lediju Bell,Vinciya Pandian,Axel Krieger*

Main category: cs.RO

TL;DR: 提出了一种端到端的机器人超声引导中心静脉导管插入（CVC）流程，结合深度学习与机器人运动规划，实现了高精度的血管定位与针插入。


<details>
  <summary>Details</summary>
Motivation: CVC手术的成功率受限于解剖变异和操作者依赖性，错误可能导致严重并发症，因此需要自动化解决方案。

Method: 使用深度学习模型从深度图像识别解剖标志，机器人规划扫描、分割、重建血管，并定位最佳插入区域，最终在超声引导下完成针插入。

Result: 在10次模拟临床场景中，首次尝试成功率100%，血管重建平均误差2.15毫米，针插入误差小于1毫米。

Conclusion: 这是首个在高仿真模型上验证的机器人CVC系统，展示了临床转化的潜力。

Abstract: Purpose: Central venous catheterization (CVC) is a critical medical procedure
for vascular access, hemodynamic monitoring, and life-saving interventions. Its
success remains challenging due to the need for continuous ultrasound-guided
visualization of a target vessel and approaching needle, which is further
complicated by anatomical variability and operator dependency. Errors in needle
placement can lead to life-threatening complications. While robotic systems
offer a potential solution, achieving full autonomy remains challenging. In
this work, we propose an end-to-end robotic-ultrasound-guided CVC pipeline,
from scan initialization to needle insertion. Methods: We introduce a
deep-learning model to identify clinically relevant anatomical landmarks from a
depth image of the patient's neck, obtained using RGB-D camera, to autonomously
define the scanning region and paths. Then, a robot motion planning framework
is proposed to scan, segment, reconstruct, and localize vessels (veins and
arteries), followed by the identification of the optimal insertion zone.
Finally, a needle guidance module plans the insertion under ultrasound guidance
with operator's feedback. This pipeline was validated on a high-fidelity
commercial phantom across 10 simulated clinical scenarios. Results: The
proposed pipeline achieved 10 out of 10 successful needle placements on the
first attempt. Vessels were reconstructed with a mean error of 2.15
\textit{mm}, and autonomous needle insertion was performed with an error less
than or close to 1 \textit{mm}. Conclusion: To our knowledge, this is the first
robotic CVC system demonstrated on a high-fidelity phantom with integrated
planning, scanning, and insertion. Experimental results show its potential for
clinical translation.

</details>


### [106] [Robust Speech-Workload Estimation for Intelligent Human-Robot Systems](https://arxiv.org/abs/2507.05985)
*Julian Fortune,Julie A. Adams,Jamison Heard*

Main category: cs.RO

TL;DR: 论文提出了一种实时估计语音工作负荷的算法，旨在通过调整系统需求避免不良工作负荷状态，提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 在任务环境中，高低工作负荷交替会影响任务表现，实时估计和调整工作负荷状态（尤其是语音负荷）是提升性能的关键。

Method: 提出了一种实时估计语音工作负荷的算法，并分析了其准确性和泛化能力。

Result: 算法能够准确估计语音负荷，并在不同个体和人机协作模式中表现出良好的泛化性。

Conclusion: 实时语音工作负荷估计是开发自适应人机系统的关键要素。

Abstract: Demanding task environments (e.g., supervising a remotely piloted aircraft)
require performing tasks quickly and accurately; however, periods of low and
high operator workload can decrease task performance. Intelligent modulation of
the system's demands and interaction modality in response to changes in
operator workload state may increase performance by avoiding undesirable
workload states. This system requires real-time estimation of each workload
component (i.e., cognitive, physical, visual, speech, and auditory) to adapt
the correct modality. Existing workload systems estimate multiple workload
components post-hoc, but few estimate speech workload, or function in
real-time. An algorithm to estimate speech workload and mitigate undesirable
workload states in real-time is presented. An analysis of the algorithm's
accuracy is presented, along with the results demonstrating the algorithm's
generalizability across individuals and human-machine teaming paradigms.
Real-time speech workload estimation is a crucial element towards developing
adaptive human-machine systems.

</details>


### [107] [SCCRUB: Surface Cleaning Compliant Robot Utilizing Bristles](https://arxiv.org/abs/2507.06053)
*Jakub F. Kowalewski,Keeyon Hajjafar,Alyssa Ugent,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 软机器人手臂通过神经网络学习逆运动学和弹性，实现了开环力和位置控制，成功清除表面顽固污渍。


<details>
  <summary>Details</summary>
Motivation: 传统刚性机器人虽能施加足够力但安全性差，软机器人虽安全但难以持续施力。研究旨在解决软机器人施力不足的问题。

Method: 训练神经网络学习软机器人手臂的逆运动学和弹性，实现开环控制。

Result: 机器人成功清除盘子上烧焦食物和马桶座上的果酱，平均清除率达99.7%。

Conclusion: 研究表明软机器人能安全有效地清除顽固污渍，扩展了其应用场景。

Abstract: Scrubbing surfaces is a physically demanding and time-intensive task.
Removing adhered contamination requires substantial friction generated through
pressure and torque or high lateral forces. Rigid robotic manipulators, while
capable of exerting these forces, are usually confined to structured
environments isolated from humans due to safety risks. In contrast, soft robot
arms can safely work around humans and adapt to environmental uncertainty, but
typically struggle to transmit the continuous torques or lateral forces
necessary for scrubbing. Here, we demonstrate a soft robotic arm scrubbing
adhered residues using torque and pressure, a task traditionally challenging
for soft robots. We train a neural network to learn the arm's inverse
kinematics and elasticity, which enables open-loop force and position control.
Using this learned model, the robot successfully scrubbed burnt food residue
from a plate and sticky fruit preserve from a toilet seat, removing an average
of 99.7% of contamination. This work demonstrates how soft robots, capable of
exerting continuous torque, can effectively and safely scrub challenging
contamination from surfaces.

</details>


### [108] [Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling](https://arxiv.org/abs/2507.06149)
*Charles Champagne Cossette,Taylor Scott Clawson,Andrew Feit*

Main category: cs.RO

TL;DR: 提出一种新颖算法，用于估计动态物体间碰撞概率，通过自适应sigma点采样方案实现快速计算，误差中位数为3.5%，运行时间中位数为0.21ms。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略碰撞概率的时间依赖性，导致高估碰撞概率，本文旨在解决这一问题。

Method: 采用自适应sigma点采样方案，处理具有高斯分布轨迹的动态物体碰撞概率估计。

Result: 在400个6秒自动驾驶日志片段中测试，算法误差中位数为3.5%，运行时间中位数为0.21ms。

Conclusion: 该算法高效且准确，特别适用于动态物体的碰撞概率估计。

Abstract: A novel algorithm is presented for the estimation of collision probabilities
between dynamic objects with uncertain trajectories, where the trajectories are
given as a sequence of poses with Gaussian distributions. We propose an
adaptive sigma-point sampling scheme, which ultimately produces a fast, simple
algorithm capable of estimating the collision probability with a median error
of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold
6226R Processor. Importantly, the algorithm explicitly accounts for the
collision probability's temporal dependence, which is often neglected in prior
work and otherwise leads to an overestimation of the collision probability.
Finally, the method is tested on a diverse set of relevant real-world
scenarios, consisting of 400 6-second snippets of autonomous vehicle logs,
where the accuracy and latency is rigorously evaluated.

</details>


### [109] [Evaluation of Habitat Robotics using Large Language Models](https://arxiv.org/abs/2507.06157)
*William Li,Lei Hamilton,Kaise Al-natour,Sanjeev Mohindra*

Main category: cs.RO

TL;DR: 本文评估了大型语言模型在Meta PARTNER基准测试中解决机器人任务的表现，发现推理模型o3-mini优于非推理模型GPT-4o和Llama 3。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在机器人协作任务中的有效性，为机器人开发提供新方向。

Method: 使用Meta PARTNER基准测试，评估多种前沿模型在随机厨房场景中的表现。

Result: o3-mini在集中式、分散式、全观测和部分观测配置中均表现优异。

Conclusion: 推理模型在机器人任务中表现更优，为机器人开发提供了有前景的研究方向。

Abstract: This paper focuses on evaluating the effectiveness of Large Language Models
at solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR
provides simplified environments and robotic interactions within randomized
indoor kitchen scenes. Each randomized kitchen scene is given a task where two
robotic agents cooperatively work together to solve the task. We evaluated
multiple frontier models on Meta PARTNER environments. Our results indicate
that reasoning models like OpenAI o3-mini outperform non-reasoning models like
OpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied
environments. o3-mini displayed outperform across centralized, decentralized,
full observability, and partial observability configurations. This provides a
promising avenue of research for embodied robotic development.

</details>


### [110] [Learning Agile Tensile Perching for Aerial Robots from Demonstrations](https://arxiv.org/abs/2507.06172)
*Kangle Yuan,Atar Babgei,Luca Romanello,Hai-Nguyen Nguyen,Ronald Clark,Mirko Kovac,Sophie F. Armanini,Basaran Bahadir Kocer*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的轨迹框架，用于解决无人机在张力系留栖息中的动态控制问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在栖息时能节省能量，但张力系留栖息引入复杂的动态控制挑战，需精确管理。

Method: 采用Soft Actor-Critic from Demonstrations (SACfD)算法，结合最优和次优演示，提升训练效率和响应性。

Result: 框架实现了对位置和速度的精确控制，成功完成系留栖息任务。

Conclusion: 通过仿真和实验验证了框架的有效性，为无人机栖息提供了可靠解决方案。

Abstract: Perching on structures such as trees, beams, and ledges is essential for
extending the endurance of aerial robots by enabling energy conservation in
standby or observation modes. A tethered tensile perching mechanism offers a
simple, adaptable solution that can be retrofitted to existing robots and
accommodates a variety of structure sizes and shapes. However, tethered tensile
perching introduces significant modelling challenges which require precise
management of aerial robot dynamics, including the cases of tether slack &
tension, and momentum transfer. Achieving smooth wrapping and secure anchoring
by targeting a specific tether segment adds further complexity. In this work,
we present a novel trajectory framework for tethered tensile perching,
utilizing reinforcement learning (RL) through the Soft Actor-Critic from
Demonstrations (SACfD) algorithm. By incorporating both optimal and suboptimal
demonstrations, our approach enhances training efficiency and responsiveness,
achieving precise control over position and velocity. This framework enables
the aerial robot to accurately target specific tether segments, facilitating
reliable wrapping and secure anchoring. We validate our framework through
extensive simulation and real-world experiments, and demonstrate effectiveness
in achieving agile and reliable trajectory generation for tensile perching.

</details>


### [111] [Is Diversity All You Need for Scalable Robotic Manipulation?](https://arxiv.org/abs/2507.06219)
*Modi Shi,Li Chen,Jin Chen,Yuxiang Lu,Chiming Liu,Guanghui Ren,Ping Luo,Di Huang,Maoqing Yao,Hongyang Li*

Main category: cs.RO

TL;DR: 本文研究了机器人操作中数据多样性的作用，挑战了“多样性越多越好”的传统观点，并提出了任务多样性、多体现预训练和专家多样性的新见解。


<details>
  <summary>Details</summary>
Motivation: 探索机器人操作中数据扩展的有效原则，以提升基础模型的性能。

Method: 通过实验分析任务、体现和专家三个维度的数据多样性，并提出分布去偏方法。

Result: 发现任务多样性比单任务数据量更重要，单体现数据预训练效果优于多体现数据，专家多样性可能干扰学习。提出的GO-1-Pro方法性能提升15%。

Conclusion: 为机器人操作数据集的有效扩展提供了新视角和实践指导。

Abstract: Data scaling has driven remarkable success in foundation models for Natural
Language Processing (NLP) and Computer Vision (CV), yet the principles of
effective data scaling in robotic manipulation remain insufficiently
understood. In this work, we investigate the nuanced role of data diversity in
robot learning by examining three critical dimensions-task (what to do),
embodiment (which robot to use), and expert (who demonstrates)-challenging the
conventional intuition of "more diverse is better". Throughout extensive
experiments on various robot platforms, we reveal that (1) task diversity
proves more critical than per-task demonstration quantity, benefiting transfer
from diverse pre-training tasks to novel downstream scenarios; (2)
multi-embodiment pre-training data is optional for cross-embodiment
transfer-models trained on high-quality single-embodiment data can efficiently
transfer to different platforms, showing more desirable scaling property during
fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,
arising from individual operational preferences and stochastic variations in
human demonstrations, can be confounding to policy learning, with velocity
multimodality emerging as a key contributing factor. Based on this insight, we
propose a distribution debiasing method to mitigate velocity ambiguity, the
yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to
using 2.5 times pre-training data. Collectively, these findings provide new
perspectives and offer practical guidance on how to scale robotic manipulation
datasets effectively.

</details>


### [112] [EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow](https://arxiv.org/abs/2507.06224)
*Yixiang Chen,Peiyan Li,Yan Huang,Jiabing Yang,Kehan Chen,Liang Wang*

Main category: cs.RO

TL;DR: EC-Flow是一种直接从无动作标签视频中学习机器人操作的框架，通过预测体现中心流，显著提升了对多样化操作场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖低级别动作标签数据集或局限于刚性物体场景，无法处理变形物体、遮挡和非位移任务。

Method: EC-Flow通过预测体现中心流并结合语言指令和目标对齐模块，无需动作标签即可学习操作。

Result: 在仿真和真实任务中，EC-Flow在遮挡物体处理（62%提升）、变形物体操作（45%提升）和非位移任务（80%提升）上表现优异。

Conclusion: EC-Flow提供了一种无需动作标签、易于部署的机器人操作学习框架，适用于复杂场景。

Abstract: Current language-guided robotic manipulation systems often require low-level
action-labeled datasets for imitation learning. While object-centric flow
prediction methods mitigate this issue, they remain limited to scenarios
involving rigid objects with clear displacement and minimal occlusion. In this
work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly
learns manipulation from action-unlabeled videos by predicting
embodiment-centric flow. Our key insight is that incorporating the embodiment's
inherent kinematics significantly enhances generalization to versatile
manipulation scenarios, including deformable object handling, occlusions, and
non-object-displacement tasks. To connect the EC-Flow with language
instructions and object interactions, we further introduce a goal-alignment
module by jointly optimizing movement consistency and goal-image prediction.
Moreover, translating EC-Flow to executable robot actions only requires a
standard robot URDF (Unified Robot Description Format) file to specify
kinematic constraints across joints, which makes it easy to use in practice. We
validate EC-Flow on both simulation (Meta-World) and real-world tasks,
demonstrating its state-of-the-art performance in occluded object handling (62%
improvement), deformable object manipulation (45% improvement), and
non-object-displacement tasks (80% improvement) than prior state-of-the-art
object-centric flow methods. For more information, see our project website at
https://ec-flow1.github.io .

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [113] [A Compositional Approach to Diagnosing Faults in Cyber-Physical Systems](https://arxiv.org/abs/2507.05438)
*Josefine B. Graebener,Inigo Incer,Richard M. Murray*

Main category: eess.SY

TL;DR: 该论文提出了一种基于合约的设计方法，用于高效识别导致系统级故障的组件及其规范中的具体谓词。


<details>
  <summary>Details</summary>
Motivation: 在信息物理系统中，系统级故障的原因往往难以追踪，因此需要一种高效的方法来定位故障源。

Method: 利用组合设计和假设-保证合约，通过计算系统级合约来识别故障组件及其规范中的具体谓词。

Result: 通过Pacti工具实现，并在DARPA城市挑战赛的自动驾驶车辆案例中验证了方法的有效性。

Conclusion: 该方法能够高效地定位系统级故障的根源，为信息物理系统的故障诊断提供了新思路。

Abstract: Identifying the cause of a system-level failure in a cyber-physical system
(CPS) can be like tracing a needle in a haystack. This paper approaches the
problem by assuming that the CPS has been designed compositionally and that
each component in the system is associated with an assume-guarantee contract.
We exploit recent advances in contract-based design that show how to compute
the contract for the entire system using the component-level contracts. When
presented with a system-level failure, our approach is able to efficiently
identify the components that are responsible for the system-level failure
together with the specific predicates in those components' specifications that
are involved in the fault. We implemented this approach using Pacti and
demonstrate it through illustrative examples inspired by an autonomous vehicle
in the DARPA urban challenge.

</details>


### [114] [Risk-Aware Aerocapture Guidance Through a Probabilistic Indicator Function](https://arxiv.org/abs/2507.05454)
*Grace E. Calkins,Jay W. McMahon,Alireza Doostan,David C. Woffinden*

Main category: eess.SY

TL;DR: 提出了一种基于生成模型的概率指示函数的风险感知气动捕获制导算法，用于提高低精度导航任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 气动捕获对轨迹误差敏感，特别是在低成本任务中导航精度不足时，考虑每种失败模式的概率可以提高性能。

Method: 使用生成模型构建概率指示函数，估计逃逸、撞击或捕获概率，并将这些概率整合到修正制导指令中。

Result: 在高不确定性场景中，该方法比现有数值预测校正算法表现更好，69%至100%的可恢复案例在逃逸或撞击边缘被挽救。

Conclusion: 风险感知气动捕获制导算法提高了捕获性能和鲁棒性，尤其适用于导航不确定性高的任务。

Abstract: Aerocapture is sensitive to trajectory errors, particularly for low-cost
missions with imprecise navigation. For such missions, considering the
probability of each failure mode when computing guidance commands can increase
performance. A risk-aware aerocapture guidance algorithm is proposed that uses
a generative-modeling-based probabilistic indicator function to estimate
escape, impact, or capture probabilities. The probability of each mode is
incorporated into corrective guidance commands to increase the likelihood of
successful capture. The proposed method is evaluated against state-of-the-art
numeric predictor-corrector guidance algorithms in high-uncertainty scenarios
where entry interface dispersions lead to nontrivial failure probabilities.
When using a probabilistic indicator function in guidance, 69% to 100% of
recoverable cases are saved in near-escape and near-impact scenarios. In
addition, the probabilistic indicator is compared to a first-order fading
memory filter for density estimation, showing improvements in apoapsis error
even when a fading filter is included. The probabilistic indicator function can
also accurately predict failure probability for dispersions outside its
training data, showing generalizability. The proposed risk-aware aerocapture
guidance algorithm improves capture performance and robustness to entry
interface state dispersions, especially for missions with high navigation
uncertainty.

</details>


### [115] [Constraint Hypergraphs as a Unifying Framework for Digital Twins](https://arxiv.org/abs/2507.05494)
*John Morris,Douglas L. Van Bossuyt,Edward Louis,Gregory Mocko,John Wagner*

Main category: eess.SY

TL;DR: 提出了一种基于约束超图的新数学形式，用于构建可互操作的数字孪生框架，解决了传统数字孪生难以连接和修改的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数字孪生因接口限制导致互操作性差，难以灵活部署和修改，亟需一种通用框架。

Method: 使用约束超图表示系统行为，将数字孪生建模为两个耦合系统，通过分解模型实现自主白盒仿真。

Result: 在微电网案例中验证了框架的有效性，实现了跨环境的数据交互和模拟。

Conclusion: 约束超图框架为数字孪生提供了通用解决方案，有望推动其在各领域的应用，增强科学合作。

Abstract: Digital twins, used to represent physical systems, have been lauded as tools
for understanding reality. Complex system behavior is typically captured in
domain-specific models crafted by subject experts. Contemporary methods for
employing models in a digital twin require prescriptive interfaces, resulting
in twins that are difficult to connect, redeploy, and modify. The limited
interoperability of these twins has prompted calls for a universal framework
enabling observability across model aggregations. Here we show how a new
mathematical formalism called a constraint hypergraph serves as such a
framework by representing system behavior as the composition of set-based
functions. A digital twin is shown to be the second of two coupled systems
where both adhere to the same constraint hypergraph, permitting the properties
of the first to be observable from the second. Interoperability is given by
deconstructing models into a structure enabling autonomous, white-box
simulation of system properties. The resulting digital twins can interact
immediately with both human and autonomous agents. This is demonstrated in a
case study of a microgrid, showing how both measured and simulated data from
the aggregated twins can be provided regardless of the operating environment.
By connecting models, constraint hypergraphs supply scientists and modelers
robust means to capture, communicate, and combine digital twins across all
fields of study. We expect this framework to expand the use of digital twins,
enriching scientific insights and collaborations by providing a structure for
characterizing complex systems.

</details>


### [116] [Basic Computations in Fault Tree Analysis](https://arxiv.org/abs/2507.05509)
*Hamid Jahanian*

Main category: eess.SY

TL;DR: 本文总结了故障树分析（FTA）的基本原理，旨在帮助工程师更好地理解和验证FTA工具的输出。


<details>
  <summary>Details</summary>
Motivation: FTA工具虽然强大，但可能掩盖计算过程，结果的准确性依赖用户对方法和工具的熟悉程度。本文旨在澄清FTA的基本概念和计算过程。

Method: 探讨了定性和定量FTA分析的基本原理，并涉及一致性和共识等概念。

Result: 通过深入理解FTA的基本概念，工程师可以更有效地解释和验证FTA工具的输出。

Conclusion: 本文为工程师提供了FTA基础计算的简明概述，有助于提升其应用FTA的能力。

Abstract: Fault Tree Analysis (FTA) is a well-established method in failure analysis
and is widely used in safety and reliability assessments. While FTA tools
enable users to manage complex analyses effectively, they can sometimes obscure
the underlying calculation processes. As a result, the soundness of FTA results
often hinges on the user's expertise and familiarity with the methodology and
the tool. This paper aims to explore the fundamental principles underlying both
qualitative and quantitative FTA analyses, while addressing broader conceptual
considerations such as coherence and consensus. By developing a deeper
understanding of these concepts, engineers can improve their ability to
interpret, verify, and make informed use of the outputs generated by FTA tools.
This paper does not propose a novel concept in FTA but aims to compile and
present a concise overview of the fundamental computations in FTA.

</details>


### [117] [Sparsity-Promoting Dynamic Mode Decomposition Applied to Sea Surface Temperature Fields](https://arxiv.org/abs/2507.05711)
*Zhicheng Zhang,Yoshihiko Susuki,Atsushi Okazaki*

Main category: eess.SY

TL;DR: 利用Koopman模态分解分析非线性高维气候系统，通过稀疏动态模态分解提取主导时空模态，实现高效、可解释的低维系统动力学表示。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过数据驱动方法理解气候系统的非线性动态，提取显著气候模态，为降阶建模和气候预测提供框架。

Method: 采用Koopman模态分解和稀疏动态模态分解技术，从历史时间序列数据（如月海表温度）中提取主导时空模态。

Result: 成功提取了气候变率中的显著相干结构，实现了系统动力学的低维表示。

Conclusion: 结合Koopman模态和稀疏技术，为气候系统的降阶建模和预测提供了潜在框架。

Abstract: In this paper, we leverage Koopman mode decomposition to analyze the
nonlinear and high-dimensional climate systems acting on the observed data
space. The dynamics of atmospheric systems are assumed to be equation-free,
with the linear evolution of observables derived from measured historical
long-term time-series data snapshots, such as monthly sea surface temperature
records, to construct a purely data-driven climate dynamics. In particular,
sparsity-promoting dynamic mode decomposition is exploited to extract the
dominant spatial and temporal modes, which are among the most significant
coherent structures underlying climate variability, enabling a more efficient,
interpretable, and low-dimensional representation of the system dynamics. We
hope that the combined use of Koopman modes and sparsity-promoting techniques
will provide insights into the significant climate modes, enabling
reduced-order modeling of the climate system and offering a potential framework
for predicting and controlling weather and climate variability.

</details>


### [118] [Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning](https://arxiv.org/abs/2507.05785)
*Jian Kai,Tianwei Zhang,Zihan Ling,Yang Cao,Can Shen*

Main category: eess.SY

TL;DR: RBWE是一个基于离线强化学习的带宽估计框架，通过Q集成和高斯混合策略解决OOD风险，提升性能。实验显示其显著减少误差并改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在动态网络中适应性有限，而在线强化学习成本高且可能中断服务。离线强化学习利用真实数据，但面临OOD动作、策略提取和部署稳定性等挑战。

Method: RBWE结合Q集成和高斯混合策略，减少OOD风险，并采用回退机制确保部署稳定性。

Result: 实验表明，RBWE减少18%的高估误差，提升18.6%的10%分位用户体验。

Conclusion: RBWE在实时通信系统中表现出实际有效性，解决了离线强化学习的挑战。

Abstract: Accurate bandwidth estimation (BWE) is critical for real-time communication
(RTC) systems. Traditional heuristic approaches offer limited adaptability
under dynamic networks, while online reinforcement learning (RL) suffers from
high exploration costs and potential service disruptions. Offline RL, which
leverages high-quality data collected from real-world environments, offers a
promising alternative. However, challenges such as out-of-distribution (OOD)
actions, policy extraction from behaviorally diverse datasets, and reliable
deployment in production systems remain unsolved. We propose RBWE, a robust
bandwidth estimation framework based on offline RL that integrates Q-ensemble
(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD
risks and enhance policy learning. A fallback mechanism ensures deployment
stability by switching to heuristic methods under high uncertainty.
Experimental results show that RBWE reduces overestimation errors by 18% and
improves the 10th percentile Quality of Experience (QoE) by 18.6%,
demonstrating its practical effectiveness in real-world RTC applications.

</details>


### [119] [Assessing Linear Control Strategies for Zero-Speed Fin Roll Damping](https://arxiv.org/abs/2507.05867)
*Nikita Savin,Elena Ambrosovskaya,Dmitry Romaev,Anton Proskurnikov*

Main category: eess.SY

TL;DR: 论文提出了一种基于零速鳍的船舶横摇稳定系统，采用线性控制架构并考虑非线性阻力和执行器限制，仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统水动力鳍在低速或零速条件下失效，因此需要开发一种新的横摇稳定系统。

Method: 采用Navis JSC开发的零速鳍系统，基于拖曳机制和主动振荡，提出线性控制架构并考虑非线性因素。

Result: 在高保真船舶模型上的仿真结果表明，该方法有效。

Conclusion: 提出的零速鳍系统在低速或零速条件下能有效稳定船舶横摇。

Abstract: Roll stabilization is a critical aspect of ship motion control, particularly
for vessels operating in low-speed or zero-speed conditions, where traditional
hydrodynamic fins lose their effectiveness. In this paper, we consider a roll
damping system, developed by Navis JSC, based on two actively controlled
zero-speed fins. Unlike conventional fin stabilizers, zero-speed fins employ a
drag-based mechanism and active oscillations to generate stabilizing forces
even when the vessel is stationary. We propose a simple linear control
architecture that, however, accounts for nonlinear drag forces and actuator
limitations. Simulation results on a high-fidelity vessel model used for HIL
testing demonstrate the effectiveness of the proposed approach.

</details>


### [120] [Low voltage user phase reconfiguration as a planning problem](https://arxiv.org/abs/2507.05910)
*Sari Kerckhove,Marta Vanin,Reinhilde D'hulst,Dirk Van Hertem*

Main category: eess.SY

TL;DR: 论文提出了三种静态相位重构方法（MINLP、MIQP和GA），用于解决低压配电网中的相位不平衡问题，其中MIQP方法在可扩展性和一致性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 低压配电网中的相位不平衡会导致资产利用率低下、额外损耗和设备故障。随着分布式能源的增加，这一问题可能加剧，需要有效的静态相位重构方法。

Method: 论文提出了三种静态相位重构方法：精确的混合整数非线性规划（MINLP）、混合整数二次近似（MIQP）和遗传算法（GA），支持不同的不平衡目标。

Result: MIQP方法尽管使用代理目标，但能有效缓解不同类型的不平衡，且在可扩展性和一致性上优于MINLP和GA。

Conclusion: 静态相位重构是一种有前景的解决方案，MIQP方法因其高效性和实用性成为最佳选择。

Abstract: Considerable levels of phase imbalance in low voltage (LV) distribution
networks imply that grid assets are suboptimally utilized and can cause
additional losses, equipment failure and degradation. With the ongoing energy
transition, the installation of additional single-phase distributed energy
resources may further increase the phase imbalance if no countermeasures are
taken.
  Phase reconfiguration is a cost-effective solution to reduce imbalance.
However, dynamic reconfiguration, through real-time phase swapping of loads
using remotely controlled switches, is often impractical because these switches
are too costly for widespread installation at LV users. Approaching phase
reconfiguration as a planning problem, i.e. static reconfiguration, is an
underaddressed but promising alternative. Effective static approaches that
allow appropriate imbalance objectives are currently lacking.
  This paper presents reliable and expressive static phase reconfiguration
methods that grid operators can easily integrate into routine maintenance for
effective phase balancing.
  We present and compare three static methods, an exact mixed-integer nonlinear
formulation (MINLP), a mixed-integer quadratic approximation (MIQP), and a
genetic algorithm (GA), each supporting different imbalance objectives. The
MIQP approach, despite using proxy objectives, efficiently mitigates the
different types of imbalance considered, and outperforms both MINLP and GA in
scalability and consistency.

</details>


### [121] [Optimal Placement of Smart Hybrid Transformers in Distribution Networks](https://arxiv.org/abs/2507.05967)
*Samuel Hayward,Martin Doff-Sotta,Michael Merlin,Matthew Williams,Thomas Morstyn*

Main category: eess.SY

TL;DR: 本文提出了一种新方法，通过顺序线性编程确定混合变压器在配电网中的最佳位置和利用率，以最大化其净现值。


<details>
  <summary>Details</summary>
Motivation: 混合变压器结合了传统变压器和电力电子技术，能够提供电压和无功功率控制能力，但其在配电网中的优化配置尚未充分研究。

Method: 采用顺序线性编程方法，考虑混合变压器的非线性特性和约束条件，优化其位置和利用率。

Result: 测试案例表明，混合变压器的安装和使用可以提高有功功率输出的收入，最高净现值达到656万英镑，年利润增长45.53%。

Conclusion: 混合变压器的安装成本可通过提高收益得到合理补偿，证明了其在配电网中的经济价值。

Abstract: Hybrid transformers are a relatively new technology that combine conventional
power transformers with power electronics to provide voltage and reactive power
control capabilities in distribution networks. This paper proposes a novel
method of determining the optimal location and utilisation of hybrid
transformers in 3-phase distribution networks to maximise the net present value
of hybrid transformers based on their ability to increase the export of power
produced by distributed generators over their operational lifespan. This has
been accomplished through sequential linear programming, a key feature of which
is the consideration of nonlinear characteristics and constraints relating to
hybrid transformer power electronics and control capabilities. Test cases were
carried out in a modified version of the Cigre European Low Voltage
Distribution Network Benchmark, which has been extended by connecting it with
two additional low voltage distribution test networks. All test case results
demonstrate that the installation and utilisation of hybrid transformers can
improve the income earned from exporting excess active power, justifying their
installation cost (with the highest net present value being {\pounds}6.56
million, resulting from a 45.53 percent increase in estimated annual profits
due to coordinated HT compensation).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [122] [Inaugural MOASEI Competition at AAMAS'2025: A Technical Report](https://arxiv.org/abs/2507.05469)
*Ceferino Patino,Tyler J. Billings,Alireza Saleh Abadi,Daniel Redder,Adam Eck,Prashant Doshi,Leen-Kiat Soh*

Main category: cs.MA

TL;DR: MOASEI竞赛是一个多智能体AI基准测试活动，旨在评估开放世界条件下的决策能力。竞赛包含三个赛道，吸引了11支团队参与，展示了多种技术解决方案，并提供了开放环境中的泛化和适应策略。


<details>
  <summary>Details</summary>
Motivation: 评估智能体在动态、部分可观测的开放环境中的决策能力，推动开放智能体系统的研究。

Method: 基于free-range-zoo环境套件，设计了动态、部分可观测的开放任务，包括三个赛道（Wildfire、Rideshare、Cybersecurity），并采用多种评估指标（期望效用、鲁棒性、响应性）。

Result: 11支团队参与，展示了包括图神经网络、卷积架构、预测建模和大语言模型驱动的元优化在内的多样化解决方案。结果显示了在开放环境中泛化和适应的潜力。

Conclusion: MOASEI竞赛为开放智能体系统研究提供了实证见解和基础设施，推动了该领域的发展。

Abstract: We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI)
Competition, a multi-agent AI benchmarking event designed to evaluate
decision-making under open-world conditions. Built on the free-range-zoo
environment suite, MOASEI introduced dynamic, partially observable domains with
agent and task openness--settings where entities may appear, disappear, or
change behavior over time. The 2025 competition featured three
tracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct
dimensions of openness and coordination complexity. Eleven teams from
international institutions participated, with four of those teams submitting
diverse solutions including graph neural networks, convolutional architectures,
predictive modeling, and large language model--driven meta--optimization.
Evaluation metrics centered on expected utility, robustness to perturbations,
and responsiveness to environmental change. The results reveal promising
strategies for generalization and adaptation in open environments, offering
both empirical insight and infrastructure for future research. This report
details the competition's design, findings, and contributions to the open-agent
systems research community.

</details>


### [123] [Large Language Models for Agent-Based Modelling: Current and possible uses across the modelling cycle](https://arxiv.org/abs/2507.05723)
*Loïs Vanhée,Melania Borit,Peer-Olaf Siebers,Roger Cremades,Christopher Frantz,Önder Gürcan,František Kalvas,Denisa Reshef Kera,Vivek Nallur,Kavin Narasimhan,Martin Neumann*

Main category: cs.MA

TL;DR: 本文探讨了大型语言模型（LLMs）在基于代理建模（ABM）中的潜在应用与挑战，从建模周期的各个阶段进行了批判性分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自然语言理解和生成能力上的进步，ABM社区对其在建模过程中的潜在贡献产生了兴趣。

Method: 通过回顾LLMs在ABM中的当前应用，并沿着建模周期（从问题表述到结果文档化与交流）进行批判性分析。

Result: LLMs在ABM中具有潜力，但也面临挑战。

Conclusion: LLMs为ABM提供了新的可能性，但需谨慎评估其应用。

Abstract: The emergence of Large Language Models (LLMs) with increasingly sophisticated
natural language understanding and generative capabilities has sparked interest
in the Agent-based Modelling (ABM) community. With their ability to summarize,
generate, analyze, categorize, transcribe and translate text, answer questions,
propose explanations, sustain dialogue, extract information from unstructured
text, and perform logical reasoning and problem-solving tasks, LLMs have a good
potential to contribute to the modelling process. After reviewing the current
use of LLMs in ABM, this study reflects on the opportunities and challenges of
the potential use of LLMs in ABM. It does so by following the modelling cycle,
from problem formulation to documentation and communication of model results,
and holding a critical stance.

</details>


### [124] [From General Relation Patterns to Task-Specific Decision-Making in Continual Multi-Agent Coordination](https://arxiv.org/abs/2507.06004)
*Chang Yao,Youfang Lin,Shoucheng Song,Hao Wu,Yuqing Ma,Shang Han,Kai Lv*

Main category: cs.MA

TL;DR: 论文提出了一种名为RPG的方法，通过提取任务无关的关系模式并映射到任务特定的决策器，解决了持续多智能体强化学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 研究持续多智能体强化学习（Co-MARL）中的灾难性遗忘问题，并提出一种能够学习新协作策略同时保持对旧任务记忆的方法。

Method: 提出RPG方法，包括关系捕捉器和任务特定决策器，通过条件超网络将任务无关的关系模式映射到不同动作空间，并引入正则化项防止遗忘。

Result: 在SMAC和LBF上的实验表明，RPG能有效防止灾难性遗忘，并实现对新任务的零样本泛化。

Conclusion: RPG方法通过结合任务无关的关系模式和任务特定的决策机制，成功解决了Co-MARL中的灾难性遗忘问题，并展示了良好的泛化能力。

Abstract: Continual Multi-Agent Reinforcement Learning (Co-MARL) requires agents to
address catastrophic forgetting issues while learning new coordination policies
with the dynamics team. In this paper, we delve into the core of Co-MARL,
namely Relation Patterns, which refer to agents' general understanding of
interactions. In addition to generality, relation patterns exhibit
task-specificity when mapped to different action spaces. To this end, we
propose a novel method called General Relation Patterns-Guided Task-Specific
Decision-Maker (RPG). In RPG, agents extract relation patterns from dynamic
observation spaces using a relation capturer. These task-agnostic relation
patterns are then mapped to different action spaces via a task-specific
decision-maker generated by a conditional hypernetwork. To combat forgetting,
we further introduce regularization items on both the relation capturer and the
conditional hypernetwork. Results on SMAC and LBF demonstrate that RPG
effectively prevents catastrophic forgetting when learning new tasks and
achieves zero-shot generalization to unseen tasks.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [125] [Fairness-Aware Static and Dynamic Assortment Optimization: Optimal Selection with Balanced Market Share](https://arxiv.org/abs/2507.05606)
*Omar El Housni,Qing Feng,Huseyin Topaloglu*

Main category: cs.GT

TL;DR: 论文提出了一种市场份额平衡约束，以解决在线零售商在优化产品组合时可能导致的销售不平衡问题，并在静态和动态设置下研究了该约束下的优化方法。


<details>
  <summary>Details</summary>
Motivation: 在线零售商在优化产品组合时，单纯追求收入最大化可能导致销售不平衡，影响供应商参与度和产品多样性。因此，需要引入公平性约束。

Method: 在多项Logit模型下，引入市场份额平衡约束，研究静态和动态优化问题。静态问题通过多项式时间算法解决，动态问题设计渐近最优策略。

Result: 静态问题可多项式时间求解，动态策略在库存充足时渐近最优。

Conclusion: 市场份额平衡约束能有效平衡销售公平性和收入最大化，适用于多种实际场景。

Abstract: Assortment optimization is a critical tool for online retailers aiming to
maximize revenue. However, optimizing purely for revenue can lead to imbalanced
sales across products, potentially causing supplier disengagement and reduced
product diversity. To address these fairness concerns, we introduce a market
share balancing constraint that limits the disparity in expected sales between
any two offered products to a factor of a given parameter $\alpha$. We study
both static and dynamic assortment optimization under the multinomial logit
(MNL) model with this fairness constraint. In the static setting, the seller
selects a distribution over assortments that satisfies the market share
balancing constraint while maximizing expected revenue. We show that this
problem can be solved in polynomial time, and we characterize the structure of
the optimal solution: a product is included if and only if its revenue and
preference weight exceed certain thresholds. We further extend our analysis to
settings with additional feasibility constraints on the assortment and
demonstrate that, given a $\beta$-approximation oracle for the constrained
problem, we can construct a $\beta$-approximation algorithm under the fairness
constraint. In the dynamic setting, each product has a finite initial
inventory, and the seller implements a dynamic policy to maximize total
expected revenue while respecting both inventory limits and the market share
balancing constraint in expectation. We design a policy that is asymptotically
optimal, with its approximation ratio converging to one as inventories grow
large.

</details>


### [126] [Minimal balanced collections and their applications to core stability and other topics of game theory](https://arxiv.org/abs/2507.05898)
*Dylan Laplace Mermoud,Michel Grabisch,Peter Sudhölter*

Main category: cs.GT

TL;DR: 论文研究了最小平衡集合的生成问题，实现了Peleg算法以生成n≤7的所有最小平衡集合，并提出了基于最小平衡集合的快速算法，用于验证合作博弈中的核心是否为稳定集。


<details>
  <summary>Details</summary>
Motivation: 最小平衡集合是有限集合划分的推广，在合作博弈论和离散数学中有重要应用，但其数量在n>4时未知。研究旨在解决生成和验证问题。

Method: 实现Peleg算法生成最小平衡集合；提出基于最小平衡集合的算法，验证合作博弈核心是否为稳定集。

Result: 成功生成n≤7的最小平衡集合；算法比线性规划方法更快验证博弈性质。

Conclusion: 最小平衡集合的生成和验证算法为合作博弈论提供了高效工具，扩展了平衡集合的应用范围。

Abstract: Minimal balanced collections are a generalization of partitions of a finite
set of n elements and have important applications in cooperative game theory
and discrete mathematics. However, their number is not known beyond n = 4. In
this paper we investigate the problem of generating minimal balanced
collections and implement the Peleg algorithm, permitting to generate all
minimal balanced collections till n = 7. Secondly, we provide practical
algorithms to check many properties of coalitions and games, based on minimal
balanced collections, in a way which is faster than linear programming-based
methods. In particular, we construct an algorithm to check if the core of a
cooperative game is a stable set in the sense of von Neumann and Morgenstern.
The algorithm implements a theorem according to which the core is a stable set
if and only if a certain nested balancedness condition is valid. The second
level of this condition requires generalizing the notion of balanced collection
to balanced sets.

</details>


### [127] [Rethinking Pricing in Energy Markets: Pay-as-Bid vs Pay-as-Clear](https://arxiv.org/abs/2507.06035)
*Ioannis Caragiannis,Zhile Jiang,Stratis Skoulakis*

Main category: cs.GT

TL;DR: 论文比较了能源市场中的Pay-as-Clear (PC) 和 Pay-as-Bid (PB) 定价机制，发现PB在均衡价格上表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨能源市场设计中的定价机制选择，以最小化成本并应对策略性行为。

Method: 理论分析和模拟实验，包括混合策略纳什均衡和后悔学习动态。

Result: PB在均衡价格上始终优于PC，且对策略操纵更具鲁棒性。

Conclusion: PB机制在能源市场中更具优势，尤其是在抑制高价均衡方面。

Abstract: The design of energy markets is a subject of ongoing debate, particularly
concerning the choice between the widely adopted Pay-as-Clear (PC) pricing
mechanism and the alternative Pay-as-Bid (PB). These mechanisms determine how
energy producers are compensated: under PC, all selected producers are paid the
market-clearing price (i.e., the highest accepted bid), while under PB, each
selected producer is paid their own submitted bid. The overarching objective is
to meet the total demand for energy at minimal cost in the presence of
strategic behavior. We present two key theoretical results. First, no mechanism
can uniformly dominate PC or PB. This means that for any mechanism
$\mathcal{M}$, there exists a market configuration and a mixed-strategy Nash
equilibrium of PC (respectively for PB) that yields strictly lower total energy
costs than under $\mathcal{M}$. Second, in terms of worst-case equilibrium
outcomes, PB consistently outperforms PC: across all market instances, the
highest possible equilibrium price under PB is strictly lower than that under
PC. This suggests a structural robustness of PB to strategic manipulation.
These theoretical insights are further supported by extensive simulations based
on no-regret learning dynamics, which consistently yield lower average market
prices in several energy market settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [128] [Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes](https://arxiv.org/abs/2507.05304)
*Saqib Nazir,Olivier Lézoray,Sébastien Bougleux*

Main category: cs.GR

TL;DR: 3DGeoMeshNet是一种基于GCN的新框架，通过各向异性卷积层直接在空间域中学习全局和局部特征，保留了原始多边形网格格式，提高了3D网格重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于3D网格的非欧几里得性质，传统CNN难以处理，而现有GCN方法多依赖各向同性滤波器或谱分解，无法同时捕捉局部和全局特征。

Method: 提出3DGeoMeshNet，采用多尺度编码器-解码器结构，通过全局和局部路径分别捕捉大尺度几何结构和细粒度局部细节。

Result: 在COMA人脸数据集上的实验表明，3DGeoMeshNet在重建准确性上表现优异。

Conclusion: 3DGeoMeshNet通过直接在空间域中学习特征，避免了中间表示转换，显著提升了3D网格重建的精度。

Abstract: 3D meshes are fundamental data representations for capturing complex
geometric shapes in computer vision and graphics applications. While
Convolutional Neural Networks (CNNs) have excelled in structured data like
images, extending them to irregular 3D meshes is challenging due to the
non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a
solution by applying convolutions to graph-structured data, but many existing
methods rely on isotropic filters or spectral decomposition, limiting their
ability to capture both local and global mesh features. In this paper, we
introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework
that uses anisotropic convolution layers to effectively learn both global and
local features directly in the spatial domain. Unlike previous approaches that
convert meshes into intermediate representations like voxel grids or point
clouds, our method preserves the original polygonal mesh format throughout the
reconstruction process, enabling more accurate shape reconstruction. Our
architecture features a multi-scale encoder-decoder structure, where separate
global and local pathways capture both large-scale geometric structures and
fine-grained local details. Extensive experiments on the COMA dataset
containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of
reconstruction accuracy.

</details>


### [129] [LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures](https://arxiv.org/abs/2507.06109)
*Seungoh Han,Jaehoon Jang,Hyunsu Kim,Jaeheung Surh,Junhyung Kwak,Hyowon Ha,Kyungdon Joo*

Main category: cs.GR

TL;DR: LighthouseGS提出了一种基于3D高斯泼溅的实时新视角合成框架，适用于手持设备拍摄的室内场景，解决了窄基线和纹理缺失的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法需要高精度图像覆盖整个场景，限制了普通用户的可用性。LighthouseGS旨在通过简单的全景式运动（如手机拍摄）实现高质量渲染。

Method: 利用粗略几何先验（如相机位姿和单目深度估计）和室内平面结构，提出平面支架组装初始化方法和稳定剪枝策略，并引入几何和光度校正。

Result: 在真实和合成室内场景测试中，LighthouseGS实现了逼真渲染，优于现有方法。

Conclusion: LighthouseGS展示了全景视图合成和物体放置的潜力，为普通用户提供了便捷的高质量3D渲染方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel
view synthesis (NVS) with impressive quality in indoor scenes. However,
achieving high-fidelity rendering requires meticulously captured images
covering the entire scene, limiting accessibility for general users. We aim to
develop a practical 3DGS-based NVS framework using simple panorama-style motion
with a handheld camera (e.g., mobile device). While convenient, this
rotation-dominant motion and narrow baseline make accurate camera pose and 3D
point estimation challenging, especially in textureless indoor scenes. To
address these challenges, we propose LighthouseGS, a novel framework inspired
by the lighthouse-like sweeping motion of panoramic views. LighthouseGS
leverages rough geometric priors, such as mobile device camera poses and
monocular depth estimation, and utilizes the planar structures often found in
indoor environments. We present a new initialization method called plane
scaffold assembly to generate consistent 3D points on these structures,
followed by a stable pruning strategy to enhance geometry and optimization
stability. Additionally, we introduce geometric and photometric corrections to
resolve inconsistencies from motion drift and auto-exposure in mobile devices.
Tested on collected real and synthetic indoor scenes, LighthouseGS delivers
photorealistic rendering, surpassing state-of-the-art methods and demonstrating
the potential for panoramic view synthesis and object placement.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [130] [Esports and expertise: what competitive gaming can teach us about mastery](https://arxiv.org/abs/2507.05446)
*Ben Boudaoud,Josef Spjut,Joohwan Kim,Arjun Madhusudan,Benjamin Watson*

Main category: cs.HC

TL;DR: 论文探讨了人机交互中从原子任务到整体技能掌握的转变，强调电竞等高层次实践的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统人机交互研究关注原子任务和通用性，但电竞等领域的成功表明，高层次的技能掌握同样重要。

Method: 通过对比传统任务完成时间与电竞选手的技能优化，提出新视角。

Result: 研究发现，成功更依赖于任务特定技能的优化，而非单纯的速度提升。

Conclusion: 人机交互研究应更多关注高层次实践和技能掌握，而非仅限原子任务。

Abstract: Historically, much research and development in human computer interaction has
focused on atomic and generalizable tasks, where task completion time indicates
productivity. However, the emergence of competitive games and esports reminds
us of an alternative perspective on human performance in HCI: mastery of
higher-level, holistic practices. Just as a world-renowned artist is rarely
evaluated for their individual brush strokes, so skilled competitive gamers
rarely succeed solely by completing individual mouse movements or keystrokes as
quickly as possible. Instead, they optimize more task-specific skills, adeptly
performing challenges deep in the learning curve for their game of choice.

</details>


### [131] [NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones](https://arxiv.org/abs/2507.05447)
*Aiur Nanzatov,Lourdes Peña-Castillo,Oscar Meruvia-Pastor*

Main category: cs.HC

TL;DR: NRXR-ID是一种在虚拟现实（VR）中实现双因素认证（2FA）的技术，利用智能手机完成认证挑战，无需摘下头戴显示器（HMD）。


<details>
  <summary>Details</summary>
Motivation: VR环境下2FA难以实现，因为用户佩戴HMD无法看到现实世界。

Method: 提出NRXR-ID技术，通过智能手机完成认证挑战，包括四种挑战类型（如棋盘式挑战），并在三种配置下测试。

Result: 棋盘式视觉匹配挑战表现最佳，其次是数字PIN挑战。

Conclusion: NRXR-ID在VR中有效实现了2FA，棋盘式挑战是最优选择。

Abstract: Two-factor authentication (2FA) has become widely adopted as an efficient and
secure way to validate someone's identity online. Two-factor authentication is
difficult in virtual reality (VR) because users are usually wearing a
head-mounted display (HMD) which does not allow them to see their real-world
surroundings. We present NRXR-ID, a technique to implement two-factor
authentication while using extended reality systems and smartphones. The
proposed method allows users to complete an authentication challenge using
their smartphones without removing their HMD. We performed a user study where
we explored four types of challenges for users, including a novel
checkers-style challenge. Users responded to these challenges under three
different configurations, including a technique that uses the smartphone to
support gaze-based selection without the use of VR controllers. A 4X3
within-subjects design allowed us to study all the variations proposed. We
collected performance metrics and performed user experience questionnaires to
collect subjective impressions from 30 participants. Results suggest that the
checkers-style visual matching challenge was the most appropriate option,
followed by entering a digital PIN challenge submitted via the smartphone and
answered within the VR environment.

</details>


### [132] [GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data for Health and Wellbeing](https://arxiv.org/abs/2507.05461)
*Akshat Choube,Ha Le,Jiachen Li,Kaixin Ji,Vedant Das Swain,Varun Mishra*

Main category: cs.HC

TL;DR: 论文提出了一种新型的开放式感知系统GLOSS，能够进行多模态数据三角测量，显著优于现有的RAG技术。


<details>
  <summary>Details</summary>
Motivation: 智能手机和可穿戴设备的普及为健康和行为预测提供了数据支持，但如何从被动感知数据中提取高层次、全面的理解仍具挑战性。

Method: 开发了GLOSS系统，支持开放式感知和复杂多模态三角测量。

Result: GLOSS在准确性和一致性上显著优于RAG技术（87.93% vs 29.31%）。

Conclusion: GLOSS展示了在UbiComp和HCI领域的潜力，但仍存在局限性。

Abstract: The ubiquitous presence of smartphones and wearables has enabled researchers
to build prediction and detection models for various health and behavior
outcomes using passive sensing data from these devices. Achieving a high-level,
holistic understanding of an individual's behavior and context, however,
remains a significant challenge. Due to the nature of passive sensing data,
sensemaking -- the process of interpreting and extracting insights -- requires
both domain knowledge and technical expertise, creating barriers for different
stakeholders. Existing systems designed to support sensemaking are either not
open-ended or cannot perform complex data triangulation. In this paper, we
present a novel sensemaking system, Group of LLMs for Open-ended Sensemaking
(GLOSS), capable of open-ended sensemaking and performing complex multimodal
triangulation to derive insights. We demonstrate that GLOSS significantly
outperforms the commonly used Retrieval-Augmented Generation (RAG) technique,
achieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31%
accuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS
through four use cases inspired by prior and ongoing work in the UbiComp and
HCI communities. Finally, we discuss the potential of GLOSS, its broader
implications, and the limitations of our work.

</details>


### [133] [AnatomyCarve: A VR occlusion management technique for medical images based on segment-aware clipping](https://arxiv.org/abs/2507.05572)
*Andrey Titov,Tina N. H. Nantenaina,Marta Kersten-Oertel,Simon Drouin*

Main category: cs.HC

TL;DR: AnatomyCarve是一种在VR环境中开发的新技术，通过选择性裁剪3D医学图像，生成类似解剖书籍的高质量插图，提升可视化效果。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学图像中自遮挡问题，现有方法无法充分展示内部解剖结构，而手绘医学插图能有效处理遮挡。

Method: 开发AnatomyCarve技术，结合高级渲染和自然交互，选择性裁剪3D医学图像，保留空间关系和上下文信息。

Result: 用户研究显示AnatomyCarve支持定制化解剖可视化，用户满意度高，适用于教育和临床。

Conclusion: AnatomyCarve在VR环境中有效解决了3D医学图像的可视化问题，具有教育和临床应用潜力。

Abstract: Visualizing 3D medical images is challenging due to self-occlusion, where
anatomical structures of interest can be obscured by surrounding tissues.
Existing methods, such as slicing and interactive clipping, are limited in
their ability to fully represent internal anatomy in context. In contrast,
hand-drawn medical illustrations in anatomy books manage occlusion effectively
by selectively removing portions based on tissue type, revealing 3D structures
while preserving context. This paper introduces AnatomyCarve, a novel technique
developed for a VR environment that creates high-quality illustrations similar
to those in anatomy books, while remaining fast and interactive. AnatomyCarve
allows users to clip selected segments from 3D medical volumes, preserving
spatial relations and contextual information. This approach enhances
visualization by combining advanced rendering techniques with natural user
interactions in VR. Usability of AnatomyCarve was assessed through a study with
non-experts, while surgical planning effectiveness was evaluated with
practicing neurosurgeons and residents. The results show that AnatomyCarve
enables customized anatomical visualizations, with high user satisfaction,
suggesting its potential for educational and clinical applications.

</details>


### [134] [W2W: A Simulated Exploration of IMU Placement Across the Human Body for Designing Smarter Wearable](https://arxiv.org/abs/2507.05532)
*Lala Shakti Swarup Ray,Bo Zhou,Paul Lukowicz*

Main category: cs.HC

TL;DR: W2W是一个基于仿真的框架，用于系统评估IMU在身体不同位置的效用，挑战传统传感器放置规范。


<details>
  <summary>Details</summary>
Motivation: 传统IMU放置依赖启发式和惯例，缺乏系统性评估，W2W旨在填补这一空白。

Method: 利用标记的运动捕捉数据生成512个解剖分布区域的合成IMU信号，并通过真实数据验证其可靠性。

Result: W2W显示与真实数据高度一致，并发现被忽视的高效区域，挑战传统放置规范。

Conclusion: W2W可作为优化传感器放置的强大工具，提供数据驱动的设计策略。

Abstract: Inertial measurement units (IMUs) are central to wearable systems for
activity recognition and pose estimation, but sensor placement remains largely
guided by heuristics and convention. In this work, we introduce Where to Wear
(W2W), a simulation-based framework for systematic exploration of IMU placement
utility across the body. Using labeled motion capture data, W2W generates
realistic synthetic IMU signals at 512 anatomically distributed surface
patches, enabling high-resolution, task-specific evaluation of sensor
performance. We validate reliability of W2W by comparing spatial performance
rankings from synthetic data with real IMU recordings in two multimodal
datasets, confirming strong agreement in activity-wise trends. Further analysis
reveals consistent spatial trends across activity types and uncovers overlooked
high-utility regions that are rarely used in commercial systems. These findings
challenge long-standing placement norms and highlight opportunities for more
efficient, task-adaptive sensor configurations. Overall, our results
demonstrate that simulation with W2W can serve as a powerful design tool for
optimizing sensor placement, enabling scalable, data-driven strategies that are
impractical to obtain through physical experimentation alone.

</details>


### [135] [Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents](https://arxiv.org/abs/2507.05820)
*Syemin Park,Soobin Park,Youn-kyung Lim*

Main category: cs.HC

TL;DR: Constella是一个基于LLM的多代理工具，帮助作家创建相互关联的角色，通过三个功能（FRIENDS DISCOVERY、JOURNALS、COMMENTS）支持角色关系的构建和深化。


<details>
  <summary>Details</summary>
Motivation: 研究发现作家在创作角色时面临困难，如难以想象新角色、平衡角色间的异同以及深化关系。

Method: 设计了Constella工具，利用LLM技术提供角色建议、展示角色内心世界和角色间互动。

Result: 部署研究表明，Constella帮助作家创建了更丰富的角色社区，加深了对角色关系的理解。

Conclusion: 多代理交互可以分散作家对角色群体的注意力与努力。

Abstract: Creating a cast of characters by attending to their relational dynamics is a
critical aspect of most long-form storywriting. However, our formative study
(N=14) reveals that writers struggle to envision new characters that could
influence existing ones, to balance similarities and differences among
characters, and to intricately flesh out their relationships. Based on these
observations, we designed Constella, an LLM-based multi-agent tool that
supports storywriters' interconnected character creation process. Constella
suggests related characters (FRIENDS DISCOVERY feature), reveals the inner
mindscapes of several characters simultaneously (JOURNALS feature), and
manifests relationships through inter-character responses (COMMENTS feature).
Our 7-8 day deployment study with storywriters (N=11) shows that Constella
enabled the creation of expansive communities composed of related characters,
facilitated the comparison of characters' thoughts and emotions, and deepened
writers' understanding of character relationships. We conclude by discussing
how multi-agent interactions can help distribute writers' attention and effort
across the character cast.

</details>


### [136] [Information Needs and Practices Supported by ChatGPT](https://arxiv.org/abs/2507.05537)
*Tim Gorichanaz*

Main category: cs.HC

TL;DR: 研究通过分析205个用户案例，探讨了ChatGPT作为信息来源的使用场景和信息实践，发现其广泛应用于生活多个领域，并支持六类信息实践。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT如何满足用户的信息需求及支持的信息实践，以理解AI时代信息需求的新内涵。

Method: 采用定性内容分析法分析205个用户案例。

Result: ChatGPT用于家庭、工作、休闲等多个领域，支持写作、决策、识别、构思、交流、批评六类信息实践。

Conclusion: AI时代的信息需求应被重新定义为‘技能性应对世界’，为生成式AI与信息需求研究提供新方向。

Abstract: This study considers ChatGPT as an information source, investigating the
information needs that people come to ChatGPT with and the information
practices that ChatGPT supports, through a qualitative content analysis of 205
user vignettes. The findings show that ChatGPT is used in a range of life
domains (home/family, work, leisure, etc.) and for a range of human needs
(writing/editing, learning, simple programming tasks, etc.), constituting the
information needs that people use ChatGPT to address. Related to these
information needs, the findings show six categories of information practices
that ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and
Critiquing. This work suggests that, in the AI age, information need should be
conceptualized not just as a matter of "getting questions answered" or even
"making sense," but as skillfully coping in the world, a notion that includes
both understanding and action. This study leads to numerous opportunities for
future work at the junction of generative AI and information needs, seeking,
use and experience.

</details>


### [137] [StoryGrid: A Tangible Interface for Student Expression](https://arxiv.org/abs/2507.05600)
*Tom Moher,Louis Gomez,Janet Kim,Claudia Hindo,Benjamin Watson,Stephen Fransen,Tim McEneany*

Main category: cs.HC

TL;DR: StorySpace是一个基于课堂的多媒体海报设计和展示系统，通过物理令牌操作投影内容，用于增强学生对文学的理解。


<details>
  <summary>Details</summary>
Motivation: 设计StorySpace旨在通过互动多媒体技术提升学生在文学课堂中的参与度和理解能力。

Method: 基于PITAboard技术，StorySpace允许学生通过物理令牌操作投影内容，并根据师生反馈调整界面和功能。

Result: StorySpace丰富了学生对文学作品的解读，特别是在关注受众和反映多元视角方面。

Conclusion: StorySpace展示了互动多媒体技术在教育中的潜力，能够有效提升学生的文学理解和表达能力。

Abstract: StorySpace is a classroom-based design and presentation system for
interactive multimedia posters. Employing the technology base first used in
Eden's PITAboard [2002], StorySpace allows groups of learners to manipulate
projected multimedia objects on a horizontal board using a small collection of
shared physical tokens. In this paper, we present the ongoing design history of
StorySpace in the context of its introduction within an urban high school
literature class. Interface modifications based on student and teacher feedback
led on changes in token semantics and media importing methods. We describe how
StorySpace features enriched students' interpretations of literature, with
particular emphasis in two areas: (1) attention to audience, and (2) reflection
of multiple perspectives.

</details>


### [138] [Hapster: Using Apple Watch Haptics to Enable Live Low-Friction Student Feedback in the Physical Classroom](https://arxiv.org/abs/2507.05605)
*Oleg Aleksandrovich Golev,Michelle Huang,Chanketya Nop,Kritin Vongthongsri,Andrés Monroy-Hernández,Parastoo Abtahi*

Main category: cs.HC

TL;DR: Hapster是一个通过Apple Watch以视觉和触觉方式实时传递学生反馈的原型系统，实验表明其有效但存在触觉感知挑战。


<details>
  <summary>Details</summary>
Motivation: 现有学生响应系统（SRSs）仅依赖视觉界面，Hapster旨在通过多模态反馈提升教学互动性。

Method: 设计并评估Hapster原型，通过Apple Watch以视觉和触觉方式传递学生反馈，实验涉及6名教师和155名学生。

Result: 系统有效提升了师生互动，但教师反映触觉序列的区分和感知存在困难。

Conclusion: 触觉可作为有效的实时反馈机制，但需平衡系统灵活性与潜在滥用，并进一步研究可访问性和交互方式。

Abstract: The benefits of student response systems (SRSs) for in-person lectures are
well-researched. However, all current SRSs only rely on a visual interface to
relay information to the instructor. We describe the design and evaluation of
Hapster, a prototype system that uses an Apple Watch to deliver live,
aggregated student feedback to the instructor via both visual and vibro-tactile
modalities. We evaluated this system with 6 instructors and 155 students at a
U.S. university. Participants reported that the system was effective at
delivering live student feedback and facilitating better engagement from both
the instructor and the students. However, instructors also noted several
challenges with differentiating and perceiving the haptic sequences while
lecturing. We conclude by discussing the tradeoff between system flexibility
and abuse potential while identifying opportunities for further research
regarding accessibility, content moderation, and additional interaction
modalities. Our results suggest that haptics can be used as an effective live
feedback mechanism for instructors in the physical classroom.

</details>


### [139] [Breaking the Plane: Exploring Real-Time Visualization of 3D Surfaces in Augmented Reality with Handwritten Input](https://arxiv.org/abs/2507.05616)
*Liam Franco Esparraguera,Kristoffer Selberg,Brian Lou,Jenny Sun,Beza Desta,Andrés Monroy-Hernández,Parastoo Abtahi*

Main category: cs.HC

TL;DR: 介绍了一款名为Breaking the Plane的AR应用，通过手写输入可视化3D数学函数，结合实时交互和动态可视化提升学习效果。


<details>
  <summary>Details</summary>
Motivation: AR技术已被证明能增强数学概念的学习动机和理解，但现有系统未结合手写方程解析与3D可视化，缺乏实时交互功能。

Method: 开发了一款AR系统，支持手写方程解析、图形操作和3D函数绘图，实现动态交互。

Result: 系统在用户参与度上显著优于其他系统，使用便捷性与流行可视化工具相当，被认为最有助于问题解决，并受到用户高度青睐。

Conclusion: 该系统成功结合了手写输入与3D可视化，为数学学习提供了更高效的交互工具。

Abstract: We introduce Breaking the Plane, an augmented reality (AR) application built
for AR headsets that enables users to visualize 3D mathematical functions using
handwritten input. Researchers have demonstrated overlaying 3D visualizations
of mathematical concepts through AR enhances learning motivation and
comprehension, and equation parsing makes the authoring of teaching materials
more time-efficient for instructors. Previous works have developed AR systems
that separately employ equation parsing and 3D mathematical visualizations, but
work has yet to be done to combine those features by enabling real-time
interactions and dynamic visualizations that help users learn in situ. We
explore this by developing an AR system featuring handwritten equation parsing,
graph manipulation, and a 3D function plotter. We found that our system
significantly surpassed other systems in engagement, achieved comparable ease
of use to a popular visualization tool, was considered the most effective in
aiding problem-solving, and was highly preferred by participants for future
use.

</details>


### [140] [Evaluation of Large Language Model-Driven AutoML in Data and Model Management from Human-Centered Perspective](https://arxiv.org/abs/2507.05962)
*Jiapeng Yao,Lantian Zhang,Jiping Huang*

Main category: cs.HC

TL;DR: 研究表明，基于大型语言模型（LLM）的AutoML框架显著提升ML实施成功率，减少开发时间，并缩小技术差距。


<details>
  <summary>Details</summary>
Motivation: 组织在利用机器学习（ML）能力时面临技术复杂性障碍，本研究探讨如何通过LLM提升ML技术的可访问性。

Method: 通过15名专业人士的用户研究，比较LLM-based AutoML与传统方法的组织影响。

Result: 93.34%用户表现更优，60%开发时间减少，错误解决时间缩短73%。

Conclusion: 自然语言界面是复杂技术系统的有效接口，可帮助组织民主化ML能力。

Abstract: As organizations increasingly seek to leverage machine learning (ML)
capabilities, the technical complexity of implementing ML solutions creates
significant barriers to adoption and impacts operational efficiency. This
research examines how Large Language Models (LLMs) can transform the
accessibility of ML technologies within organizations through a human-centered
Automated Machine Learning (AutoML) approach. Through a comprehensive user
study involving 15 professionals across various roles and technical
backgrounds, we evaluate the organizational impact of an LLM-based AutoML
framework compared to traditional implementation methods. Our research offers
four significant contributions to both management practice and technical
innovation: First, we present pioneering evidence that LLM-based interfaces can
dramatically improve ML implementation success rates, with 93.34% of users
achieved superior performance in the LLM condition, with 46.67% showing higher
accuracy (10-25% improvement over baseline) and 46.67% demonstrating
significantly higher accuracy (>25% improvement over baseline), while 6.67%
maintained comparable performance levels; and 60% reporting substantially
reduced development time. Second, we demonstrate how natural language
interfaces can effectively bridge the technical skills gap in organizations,
cutting implementation time by 50% while improving accuracy across all
expertise levels. Third, we provide valuable insights for organizations
designing human-AI collaborative systems, showing that our approach reduced
error resolution time by 73% and significantly accelerated employee learning
curves. Finally, we establish empirical support for natural language as an
effective interface for complex technical systems, offering organizations a
path to democratize ML capabilities without compromising quality or
performance.

</details>


### [141] [Exploring Collaboration Patterns and Strategies in Human-AI Co-creation through the Lens of Agency: A Scoping Review of the Top-tier HCI Literature](https://arxiv.org/abs/2507.06000)
*Shuning Zhang,Hui Wang,Xin Yi*

Main category: cs.HC

TL;DR: 论文综述了HCI/CSCW领域134篇文献，提出了一个关于AI协作中代理分配与控制的综合框架，包括代理模式、控制机制及互动情境的分类，并提供了未来研究的指导。


<details>
  <summary>Details</summary>
Motivation: 随着AI在共创中成为活跃的协作者，理解代理的分配与动态至关重要，但目前缺乏对HCI/CSCW文献中代理配置与控制机制的系统性综述。

Method: 回顾了过去20年134篇顶级HCI/CSCW文献，分析了代理模式、控制机制及互动情境。

Result: 提出了一个综合理论框架、控制机制的操作目录、跨情境代理配置图，以及未来研究的指导。

Conclusion: 填补了HCI/CSCW领域代理研究的空白，为共创系统的设计和未来研究提供了实用指导。

Abstract: As Artificial Intelligence (AI) increasingly becomes an active collaborator
in co-creation, understanding the distribution and dynamic of agency is
paramount. The Human-Computer Interaction (HCI) perspective is crucial for this
analysis, as it uniquely reveals the interaction dynamics and specific control
mechanisms that dictate how agency manifests in practice. Despite this
importance, a systematic synthesis mapping agency configurations and control
mechanisms within the HCI/CSCW literature is lacking. Addressing this gap, we
reviewed 134 papers from top-tier HCI/CSCW venues (e.g., CHI, UIST, CSCW) over
the past 20 years. This review yields four primary contributions: (1) an
integrated theoretical framework structuring agency patterns, control
mechanisms, and interaction contexts, (2) a comprehensive operational catalog
of control mechanisms detailing how agency is implemented; (3) an actionable
cross-context map linking agency configurations to diverse co-creative
practices; and (4) grounded implications and guidance for future CSCW research
and the design of co-creative systems, addressing aspects like trust and
ethics.

</details>


### [142] [Large Language Models Predict Human Well-being -- But Not Equally Everywhere](https://arxiv.org/abs/2507.06141)
*Pat Pataranutaporn,Nattavudh Powdthavee,Chayapatr Archiwaranguprok,Pattie Maes*

Main category: cs.HC

TL;DR: 研究评估了大型语言模型（LLMs）在预测全球多样化人口主观幸福感时的表现，发现模型在数据不足的国家中准确性下降，存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 主观幸福感是经济、医疗和政策决策的关键指标，研究旨在评估LLMs是否能准确预测全球多样化的幸福感。

Method: 使用来自64个国家64,000名个体的数据，评估了四种领先的LLMs，并通过预注册实验分析其预测机制。

Result: LLMs在数据不足的国家中预测准确性下降，依赖表面语言相似性而非概念理解，导致系统性误估。

Conclusion: LLMs在预测全球幸福感方面具有潜力但存在局限性，需加强验证以确保其适用性。

Abstract: Subjective well-being is a key metric in economic, medical, and policy
decision-making. As artificial intelligence provides scalable tools for
modelling human outcomes, it is crucial to evaluate whether large language
models (LLMs) can accurately predict well-being across diverse global
populations. We evaluate four leading LLMs using data from 64,000 individuals
in 64 countries. While LLMs capture broad correlates such as income and health,
their predictive accuracy decreases in countries underrepresented in the
training data, highlighting systematic biases rooted in global digital and
economic inequality. A pre-registered experiment demonstrates that LLMs rely on
surface-level linguistic similarity rather than conceptual understanding,
leading to systematic misestimations in unfamiliar or resource-limited
settings. Injecting findings from underrepresented contexts substantially
enhances performance, but a significant gap remains. These results highlight
both the promise and limitations of LLMs in predicting global well-being,
underscoring the importance of robust validation prior to their implementation
across these areas.

</details>


### [143] [V(is)owel: An Interactive Vowel Chart to Understand What Makes Visual Pronunciation Effective in Second Language Learning](https://arxiv.org/abs/2507.06202)
*Charlotte Kiesel,Dipayan Mukherjee,Mark Hasegawa-Johnson,Karrie Karahalios*

Main category: cs.HC

TL;DR: 视觉反馈结合音频能加速第二语言发音的学习，通过可视化工具V(is)owel提供直接映射舌头动作的反馈，研究发现视觉反馈更有效且能激发更多练习。


<details>
  <summary>Details</summary>
Motivation: 探索视觉反馈在第二语言发音学习中的作用，明确可视化方法的哪些方面对改进发音最有效。

Method: 开发交互式元音图表V(is)owel，并与纯音频方法对比，研究学习者如何解析视觉和听觉反馈。

Result: 视觉反馈提供直接映射物理动作的解剖学反馈，更有效且激发更多练习；V(is)owel在发音改进上表现优于纯音频方法。

Conclusion: 视觉反馈方法在第二语言学习中具有潜力，设计时应包含直接映射物理动作的明确反馈，尤其适用于未经语音训练的初学者。

Abstract: Visual feedback speeds up learners' improvement of pronunciation in a second
language. The visual combined with audio allows speakers to see sounds and
differences in pronunciation that they are unable to hear. Prior studies have
tested different visual methods for improving pronunciation, however, we do not
have conclusive understanding of what aspects of the visualizations contributed
to improvements. Based on previous work, we created V(is)owel, an interactive
vowel chart. Vowel charts provide actionable feedback by directly mapping
physical tongue movement onto a chart. We compared V(is)owel with an
auditory-only method to explore how learners parse visual and auditory feedback
to understand how and why visual feedback is effective for pronunciation
improvement. The findings suggest that designers should include explicit
anatomical feedback that directly maps onto physical movement for phonetically
untrained learners. Furthermore, visual feedback has the potential to motivate
more practice since all eight of the participants cited using the visuals as a
goal with V(is)owel versus relying on their own judgment with audio alone.
Their statements are backed up by all participants practicing words with
V(is)owel more than with audio-only. Our results indicate that V(is)owel is
effective at providing actionable feedback, demonstrating the potential of
visual feedback methods in second language learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization](https://arxiv.org/abs/2507.05263)
*Kaichen Ouyang*

Main category: cs.LG

TL;DR: 论文通过类比安德森局域化分析了GNN中的过平滑现象，提出参与度作为量化指标，并探讨了通过减少信息传播无序性缓解过平滑的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着GNN深度增加，过平滑问题导致节点表征失去独特性，研究旨在理解其机制并提出解决方案。

Method: 通过类比安德森局域化分析过平滑现象，引入参与度作为量化指标，并进行理论分析。

Result: 过平滑可理解为低频模式扩展和高频模式局域化，减少信息传播无序性可能缓解该问题。

Conclusion: 论文揭示了GNN过平滑与安德森局域化的潜在联系，为缓解过平滑提供了理论依据。

Abstract: Graph Neural Networks (GNNs) have shown great potential in graph data
analysis due to their powerful representation capabilities. However, as the
network depth increases, the issue of over-smoothing becomes more severe,
causing node representations to lose their distinctiveness. This paper analyzes
the mechanism of over-smoothing through the analogy to Anderson localization
and introduces participation degree as a metric to quantify this phenomenon.
Specifically, as the depth of the GNN increases, node features homogenize after
multiple layers of message passing, leading to a loss of distinctiveness,
similar to the behavior of vibration modes in disordered systems. In this
context, over-smoothing in GNNs can be understood as the expansion of
low-frequency modes (increased participation degree) and the localization of
high-frequency modes (decreased participation degree). Based on this, we
systematically reviewed the potential connection between the Anderson
localization behavior in disordered systems and the over-smoothing behavior in
Graph Neural Networks. A theoretical analysis was conducted, and we proposed
the potential of alleviating over-smoothing by reducing the disorder in
information propagation.

</details>


### [145] [Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction](https://arxiv.org/abs/2507.05284)
*Mustafa Kamal,Niyaz Bin Hashem,Robin Krambroeckers,Nabeel Mohammed,Shafin Rahman*

Main category: cs.LG

TL;DR: 提出了一种通过白化外生输入以减少冗余并增强长期依赖捕捉能力的方法，显著提升了时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理外生输入时存在冗余和长期依赖捕捉能力不足的问题。

Method: 通过全局统计白化外生输入，增强其对长期模式和趋势的感知能力，同时不增加回溯窗口长度。

Result: 在四个基准数据集上实现了最先进的性能，优于11个基线模型。

Conclusion: 该方法为时间序列预测中外生输入的使用提供了一种鲁棒且有效的替代方案。

Abstract: Although most transformer-based time series forecasting models primarily
depend on endogenous inputs, recent state-of-the-art approaches have
significantly improved performance by incorporating external information
through exogenous inputs. However, these methods face challenges, such as
redundancy when endogenous and exogenous inputs originate from the same source
and limited ability to capture long-term dependencies due to fixed look-back
windows. In this paper, we propose a method that whitens the exogenous input to
reduce redundancy that may persist within the data based on global statistics.
Additionally, our approach helps the exogenous input to be more aware of
patterns and trends over extended periods. By introducing this refined,
globally context-aware exogenous input to the endogenous input without
increasing the lookback window length, our approach guides the model towards
improved forecasting. Our approach achieves state-of-the-art performance in
four benchmark datasets, consistently outperforming 11 baseline models. These
results establish our method as a robust and effective alternative for using
exogenous inputs in time series forecasting.

</details>


### [146] [Dataless Neural Networks for Resource-Constrained Project Scheduling](https://arxiv.org/abs/2507.05322)
*Marc Bara*

Main category: cs.LG

TL;DR: 本文首次提出了一种无数据神经网络方法，用于解决资源受限项目调度问题（RCPSP），通过数学框架将离散约束转化为可微目标，支持梯度优化。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未将无数据神经网络方法扩展到RCPSP，本文填补了这一空白。

Method: 利用平滑松弛和自动微分技术，将调度约束转化为可微目标，并采用密集时间网格表示。

Result: 目前正在进行PSPLIB基准测试（J30、J60、J120），具体结果将在后续版本中报告。

Conclusion: 本文为RCPSP提供了一种新的无数据神经网络解决方案，支持GPU并行化，突破了传统顺序算法的限制。

Abstract: Dataless neural networks represent a paradigm shift in applying neural
architectures to combinatorial optimization problems, eliminating the need for
training datasets by encoding problem instances directly into network
parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating
the viability of dataless approaches for the Maximum Independent Set problem,
our comprehensive literature review reveals that no published work has extended
these methods to the Resource-Constrained Project Scheduling Problem (RCPSP).
This paper addresses this gap by presenting the first dataless neural network
approach for RCPSP, providing a complete mathematical framework that transforms
discrete scheduling constraints into differentiable objectives suitable for
gradient-based optimization. Our approach leverages smooth relaxations and
automatic differentiation to unlock GPU parallelization for project scheduling,
traditionally a domain of sequential algorithms. We detail the mathematical
formulation for both precedence and renewable resource constraints, including a
memory-efficient dense time-grid representation. Implementation and
comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120)
are currently underway, with empirical results to be reported in an updated
version of this paper.

</details>


### [147] [Compressing Deep Neural Networks Using Explainable AI](https://arxiv.org/abs/2507.05286)
*Kimia Soroush,Mohsen Raji,Behnam Ghavami*

Main category: cs.LG

TL;DR: 提出了一种基于可解释人工智能（XAI）的深度神经网络（DNN）压缩方法，通过梯度技术计算参数重要性，结合剪枝和量化，显著减小模型尺寸并提升精度。


<details>
  <summary>Details</summary>
Motivation: DNN的高计算和内存成本限制了其在资源受限设备上的应用，XAI技术为理解DNN内部机制提供了新思路，从而优化压缩方法。

Method: 使用梯度技术（如LRP）计算参数重要性分数，剪除零或负重要性参数，并对剩余参数进行混合精度量化。

Result: 模型尺寸减少64%，精度提升42%，优于现有XAI压缩方法。

Conclusion: 结合XAI的压缩方法能高效减小DNN模型尺寸，同时保持或提升性能，适用于边缘设备部署。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in many
tasks but it often comes at a high computational cost and memory usage.
Compression techniques, such as pruning and quantization, are applied to reduce
the memory footprint of DNNs and make it possible to accommodate them on
resource-constrained edge devices. Recently, explainable artificial
intelligence (XAI) methods have been introduced with the purpose of
understanding and explaining AI methods. XAI can be utilized to get to know the
inner functioning of DNNs, such as the importance of different neurons and
features in the overall performance of DNNs. In this paper, a novel DNN
compression approach using XAI is proposed to efficiently reduce the DNN model
size with negligible accuracy loss. In the proposed approach, the importance
score of DNN parameters (i.e. weights) are computed using a gradient-based XAI
technique called Layer-wise Relevance Propagation (LRP). Then, the scores are
used to compress the DNN as follows: 1) the parameters with the negative or
zero importance scores are pruned and removed from the model, 2)
mixed-precision quantization is applied to quantize the weights with
higher/lower score with higher/lower number of bits. The experimental results
show that, the proposed compression approach reduces the model size by 64%
while the accuracy is improved by 42% compared to the state-of-the-art
XAI-based compression method.

</details>


### [148] [Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization](https://arxiv.org/abs/2507.05583)
*Yuhang Li,Shiqi Chen,Tingyu Gong,Aydogan Ozcan*

Main category: cs.LG

TL;DR: 提出了一种基于PPO的无模型强化学习方法，用于原位训练衍射光学处理器，解决了现有方法收敛慢和不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 衍射光学网络是实现高效信息处理的灵活平台，但硬件缺陷和噪声导致优化困难。现有原位优化方法因数据利用效率低而性能受限。

Method: 采用PPO算法，高效复用原位测量数据并约束策略更新，实现更稳定和快速的收敛。

Result: 实验验证了该方法在多种任务中表现优异，包括能量聚焦、全息图像生成、像差校正和光学图像分类。

Conclusion: 该方法无需系统建模，能直接处理实际系统中的未知缺陷，为复杂光学系统提供可扩展的优化框架。

Abstract: Optical computing holds promise for high-speed, energy-efficient information
processing, with diffractive optical networks emerging as a flexible platform
for implementing task-specific transformations. A challenge, however, is the
effective optimization and alignment of the diffractive layers, which is
hindered by the difficulty of accurately modeling physical systems with their
inherent hardware imperfections, noise, and misalignments. While existing in
situ optimization methods offer the advantage of direct training on the
physical system without explicit system modeling, they are often limited by
slow convergence and unstable performance due to inefficient use of limited
measurement data. Here, we introduce a model-free reinforcement learning
approach utilizing Proximal Policy Optimization (PPO) for the in situ training
of diffractive optical processors. PPO efficiently reuses in situ measurement
data and constrains policy updates to ensure more stable and faster
convergence. We experimentally validated our method across a range of in situ
learning tasks, including targeted energy focusing through a random diffuser,
holographic image generation, aberration correction, and optical image
classification, demonstrating in each task better convergence and performance.
Our strategy operates directly on the physical system and naturally accounts
for unknown real-world imperfections, eliminating the need for prior system
knowledge or modeling. By enabling faster and more accurate training under
realistic experimental constraints, this in situ reinforcement learning
approach could offer a scalable framework for various optical and physical
systems governed by complex, feedback-driven dynamics.

</details>


### [149] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: P-DivGNN是一种基于物理信息的机器学习框架，用于在微观尺度上重建局部应力场，结合图神经网络和周期性边界条件。


<details>
  <summary>Details</summary>
Motivation: 在断裂分析或局部疲劳准则定义中，局部应力场的预测至关重要。传统方法计算成本高，需要高效替代方案。

Method: 将周期性微观结构表示为图，结合消息传递图神经网络，并在训练中引入物理约束以确保应力场平衡。

Result: 在非线性超弹性情况下，相比有限元模拟，P-DivGNN显著提高了计算速度。

Conclusion: P-DivGNN为大规模应用提供了一种高效且物理一致的局部应力场预测方法。

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [150] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 本文比较了基于特征和基于GAN的演示学习方法，分析了奖励函数结构及其对策略学习的影响，并指出两种方法的优缺点及适用场景。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过奖励函数设计优化策略学习，比较不同方法在运动模仿、泛化和适应性方面的表现。

Method: 对比分析基于特征的方法（提供密集、可解释的奖励）和基于GAN的方法（提供隐式、分布式的监督），并讨论其优缺点。

Result: 基于特征的方法擅长高保真运动模仿但泛化能力有限，基于GAN的方法灵活但训练不稳定。两者均受益于结构化运动表示。

Conclusion: 方法选择应基于任务需求（如保真度、多样性、可解释性、适应性），而非范式优劣。本文为演示学习提供了决策框架。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [151] [Neural Velocity for hyperparameter tuning](https://arxiv.org/abs/2507.05309)
*Gianluca Dalmasso,Andrea Bragagnolo,Enzo Tartaglione,Attilio Fiandrotti,Marco Grangetto*

Main category: cs.LG

TL;DR: 论文提出NeVe方法，通过“神经速度”动态调整学习率和停止训练，减少对验证集的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统超参数调优依赖验证损失，但需要额外数据集，NeVe通过神经速度指标优化训练过程。

Method: 利用神经速度（神经元传递函数变化率）动态调整学习率和停止训练，甚至可通过噪声输入计算。

Result: 实验表明神经速度能有效优化神经网络训练，减少对验证集的需求。

Conclusion: 神经速度是优化神经网络训练的高效指标，具有潜在应用价值。

Abstract: Hyperparameter tuning, such as learning rate decay and defining a stopping
criterion, often relies on monitoring the validation loss. This paper presents
NeVe, a dynamic training approach that adjusts the learning rate and defines
the stop criterion based on the novel notion of "neural velocity". The neural
velocity measures the rate of change of each neuron's transfer function and is
an indicator of model convergence: sampling neural velocity can be performed
even by forwarding noise in the network, reducing the need for a held-out
dataset. Our findings show the potential of neural velocity as a key metric for
optimizing neural network training efficiently

</details>


### [152] [Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces](https://arxiv.org/abs/2507.05315)
*Madina Kojanazarova,Florentin Bieder,Robin Sandkühler,Philippe C. Cattin*

Main category: cs.LG

TL;DR: 提出了一种基于条件图神经网络（cGNN）的数据驱动模型，用于预测软组织的变形和交互力，解决了传统方法依赖分割、网格化和刚度估计的复杂性。


<details>
  <summary>Details</summary>
Motivation: 软组织的模拟在虚拟环境中对医学应用至关重要，但其高变形性带来了显著挑战。

Method: 使用条件图神经网络（cGNN），结合表面点和施力位置，预测变形和力。通过迁移学习，先在质量-弹簧模拟上训练，再用实验数据微调。

Result: 模型预测变形的距离误差为0.35±0.03 mm（变形≤30 mm），力的绝对误差为0.37±0.05 N（力≤7.5 N）。

Conclusion: 该数据驱动方法为虚拟环境中的软组织模拟提供了有前景的解决方案，并可能适用于其他需要真实软组织模拟的领域。

Abstract: Soft tissue simulation in virtual environments is becoming increasingly
important for medical applications. However, the high deformability of soft
tissue poses significant challenges. Existing methods rely on segmentation,
meshing and estimation of stiffness properties of tissues. In addition, the
integration of haptic feedback requires precise force estimation to enable a
more immersive experience. We introduce a novel data-driven model, a
conditional graph neural network (cGNN) to tackle this complexity. Our model
takes surface points and the location of applied forces, and is specifically
designed to predict the deformation of the points and the forces exerted on
them. We trained our model on experimentally collected surface tracking data of
a soft tissue phantom and used transfer learning to overcome the data scarcity
by initially training it with mass-spring simulations and fine-tuning it with
the experimental data. This approach improves the generalisation capability of
the model and enables accurate predictions of tissue deformations and
corresponding interaction forces. The results demonstrate that the model can
predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations
up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces
up to 7.5 N. Our data-driven approach presents a promising solution to the
intricate challenge of simulating soft tissues within virtual environments.
Beyond its applicability in medical simulations, this approach holds the
potential to benefit various fields where realistic soft tissue simulations are
required.

</details>


### [153] [Going Beyond Heuristics by Imposing Policy Improvement as a Constraint](https://arxiv.org/abs/2507.05328)
*Chi-Chang Lee,Zhang-Wei Hong,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 论文提出了一种新方法HEPO，通过最大化策略改进而非策略不变性，有效利用启发式奖励，减少人工设计奖励的负担。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，启发式奖励的引入常需人工精心平衡，且现有基于策略不变性的方法效果不佳。

Method: 提出HEPO框架，通过最大化策略改进来利用启发式奖励，避免奖励黑客问题。

Result: HEPO在标准基准测试中表现优异，即使启发式奖励设计不佳也能取得良好效果。

Conclusion: HEPO显著减少了人工设计奖励的需求，提升了强化学习的实用性。

Abstract: In many reinforcement learning (RL) applications, augmenting the task rewards
with heuristic rewards that encode human priors about how a task should be
solved is crucial for achieving desirable performance. However, because such
heuristics are usually not optimal, much human effort and computational
resources are wasted in carefully balancing tasks and heuristic rewards.
Theoretically rigorous ways of incorporating heuristics rely on the idea of
\textit{policy invariance}, which guarantees that the performance of a policy
obtained by maximizing heuristic rewards is the same as the optimal policy with
respect to the task reward. However, in practice, policy invariance doesn't
result in policy improvement, and such methods are known to empirically perform
poorly. We propose a new paradigm to mitigate reward hacking and effectively
use heuristics based on the practical goal of maximizing policy improvement
instead of policy improvement. Our framework, Heuristic Enhanced Policy
Optimization (HEPO), effectively leverages heuristics while avoiding the
pitfall of prior methods for mitigating reward hacking. HEPO achieves superior
performance on standard benchmarks with well-engineered reward functions. More
surprisingly, HEPO allows policy optimization to achieve good performance even
when heuristics are not well-engineered and designed by non-expert humans,
showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a
plug-and-play optimization method for leveraging heuristics in reinforcement
learning. Code is available at https://github.com/Improbable-AI/hepo.

</details>


### [154] [Causal Foundation Models: Disentangling Physics from Instrument Properties](https://arxiv.org/abs/2507.05333)
*Jeroen Audenaert,Daniel Muthukrishna,Paul F. Gregory,David W. Hogg,V. Ashley Villar*

Main category: cs.LG

TL;DR: 提出了一种基于因果关系的双编码器模型，用于分离时间序列数据中的物理现象和仪器效应，显著提升了模型在低数据量下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 观测数据中物理现象与仪器效应的混杂限制了模型在异构或多仪器环境中的泛化能力。

Method: 采用双编码器架构和结构化对比学习，利用自然观测三元组分离物理信号和仪器效应的潜在表示。

Result: 在模拟的天文时间序列数据上，模型显著优于传统单潜在空间模型，尤其在低数据量任务中表现突出。

Conclusion: 结果表明，将因果结构嵌入表示学习对结构化数据至关重要，支持了基础模型的关键能力，如少样本泛化和高效适应。

Abstract: Foundation models for structured time series data must contend with a
fundamental challenge: observations often conflate the true underlying physical
phenomena with systematic distortions introduced by measurement instruments.
This entanglement limits model generalization, especially in heterogeneous or
multi-instrument settings. We present a causally-motivated foundation model
that explicitly disentangles physical and instrumental factors using a
dual-encoder architecture trained with structured contrastive learning.
Leveraging naturally occurring observational triplets (i.e., where the same
target is measured under varying conditions, and distinct targets are measured
under shared conditions) our model learns separate latent representations for
the underlying physical signal and instrument effects. Evaluated on simulated
astronomical time series designed to resemble the complexity of variable stars
observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS),
our method significantly outperforms traditional single-latent space foundation
models on downstream prediction tasks, particularly in low-data regimes. These
results demonstrate that our model supports key capabilities of foundation
models, including few-shot generalization and efficient adaptation, and
highlight the importance of encoding causal structure into representation
learning for structured data.

</details>


### [155] [Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training](https://arxiv.org/abs/2507.05386)
*Song Lai,Haohan Zhao,Rong Feng,Changyi Ma,Wenzhuo Liu,Hongbo Zhao,Xi Lin,Dong Yi,Min Xie,Qingfu Zhang,Hongbin Liu,Gaofeng Meng,Fei Zhu*

Main category: cs.LG

TL;DR: 本文比较了监督微调（SFT）和强化微调（RFT）在持续后训练（CPT）中的表现，发现RFT能更好地保留知识并提升模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 探索持续后训练中学习范式（SFT与RFT）对知识保留的影响。

Method: 在七个多模态任务上实验，使用Qwen2.5-VL-7B-Instruct模型，分析SFT和RFT的表现。

Result: RFT能有效保留先前任务知识并提升通用能力，而SFT导致灾难性遗忘。

Conclusion: RFT是持续后训练的稳健范式，并提出改进RFT稳定性的算法。

Abstract: Continual post-training (CPT) is a popular and effective technique for
adapting foundation models like multimodal large language models to specific
and ever-evolving downstream tasks. While existing research has primarily
concentrated on methods like data replay, model expansion, or parameter
regularization, the fundamental role of the learning paradigm within CPT
remains largely unexplored. This paper presents a comparative analysis of two
core post-training paradigms: supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT), investigating their respective impacts on knowledge
retention during CPT. Our experiments are conducted on a benchmark comprising
seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base
model for continual post-training. The investigation yields two significant
findings: (1) When continuously learning on downstream tasks, SFT leads to
catastrophic forgetting of previously learned tasks. In contrast, RFT
inherently preserves prior knowledge and achieve performance comparable to
multi-task training. (2) RFT successfully protects and even enhances the
model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).
Conversely, SFT degrades general model capabilities severely. Further analysis
shows that explicit mechanisms, such as KL penalty and chain-of-thought
reasoning, are not the primary factors. Instead, we find that the implicit
regularization inherent to RFT is a key factor in mitigating forgetting.
Finally, we propose a rollout-based instance filtering algorithm to improve the
stability and efficiency of RFT. Our comprehensive study demonstrates the
superiority of RFT as a robust paradigm for continual post-training.

</details>


### [156] [Robust Power System State Estimation using Physics-Informed Neural Networks](https://arxiv.org/abs/2507.05874)
*Solon Falas,Markos Asprou,Charalambos Konstantinou,Maria K. Michael*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经网络（PINN）的混合方法，用于提升电力系统状态估计的准确性和鲁棒性，尤其在故障或网络攻击情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统在状态估计和实时监控方面面临响应速度和准确性的挑战，尤其是在故障或网络攻击条件下。

Method: 通过将物理定律嵌入神经网络架构，PINN提高了传输电网应用中的估计准确性，并在数据篡改攻击等安全问题上表现出潜力。

Result: 实验结果表明，该方法在未见过的训练数据子集上准确率提高83%，在新数据集上性能提升65%；在数据篡改攻击下，PINN比传统神经网络准确率高93%。

Conclusion: PINN方法在电力系统状态估计中表现出显著优势，尤其在故障和网络攻击场景下具有更高的准确性和鲁棒性。

Abstract: Modern power systems face significant challenges in state estimation and
real-time monitoring, particularly regarding response speed and accuracy under
faulty conditions or cyber-attacks. This paper proposes a hybrid approach using
physics-informed neural networks (PINNs) to enhance the accuracy and
robustness, of power system state estimation. By embedding physical laws into
the neural network architecture, PINNs improve estimation accuracy for
transmission grid applications under both normal and faulty conditions, while
also showing potential in addressing security concerns such as data
manipulation attacks. Experimental results show that the proposed approach
outperforms traditional machine learning models, achieving up to 83% higher
accuracy on unseen subsets of the training dataset and 65% better performance
on entirely new, unrelated datasets. Experiments also show that during a data
manipulation attack against a critical bus in a system, the PINN can be up to
93% more accurate than an equivalent neural network.

</details>


### [157] [Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification](https://arxiv.org/abs/2507.05405)
*Luca Marzari,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: PT-LiRPA结合LiRPA的过近似技术与采样方法，显著收紧神经网络输出的线性边界，降低形式验证成本并提供概率保证。


<details>
  <summary>Details</summary>
Motivation: 解决现有形式验证方法在计算紧密度和效率上的不足，特别是在复杂神经网络验证中的挑战。

Method: 结合LiRPA的过近似技术和采样方法，估计可达集以收紧线性边界。

Result: 在标准验证基准上，PT-LiRPA将鲁棒性证书提升至3.31倍和2.26倍，并在99%置信度下解决复杂问题。

Conclusion: PT-LiRPA为形式验证提供了一种高效且概率可靠的新方法，适用于复杂神经网络验证。

Abstract: We present $\textbf{P}$robabilistically $\textbf{T}$ightened
$\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation
$\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines
over-approximation techniques from LiRPA-based approaches with a sampling-based
method to compute tight intermediate reachable sets. In detail, we show that
with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the
estimated reachable sets, significantly tightens the lower and upper linear
bounds of a neural network's output, reducing the computational cost of formal
verification tools while providing probabilistic guarantees on verification
soundness. Extensive experiments on standard formal verification benchmarks,
including the International Verification of Neural Networks Competition, show
that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by
up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic
approach results in a valuable solution for challenging competition entries
where state-of-the-art formal verification methods fail, allowing us to provide
answers with high confidence (i.e., at least 99%).

</details>


### [158] [AXLearn: Modular Large Model Training on Heterogeneous Infrastructure](https://arxiv.org/abs/2507.05411)
*Mark Lee,Tom Gunter,Chang Lan,John Peebles,Hanzhi Zhou,Kelvin Zou,Sneha Bangalore,Chung-Cheng Chiu,Nan Du,Xianzhi Du,Philipp Dufter,Ruixuan Hou,Haoshuo Huang,Dongseong Hwang,Xiang Kong,Jinhao Lei,Tao Lei,Meng Li,Li Li,Jiarui Lu,Zhiyun Lu,Yiping Ma,David Qiu,Vivek Rathod,Senyu Tong,Zhucheng Tu,Jianyu Wang,Yongqiang Wang,Zirui Wang,Floris Weers,Sam Wiseman,Guoli Yin,Bowen Zhang,Xiyou Zhou,Danyang Zhuo,Cheng Leong,Ruoming Pang*

Main category: cs.LG

TL;DR: AXLearn是一个模块化、高性能的深度学习系统，支持异构硬件基础设施，通过独特的封装设计保持低复杂度，同时实现与现有系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习系统在模块化和异构硬件支持上的不足，提高模型开发和实验的效率。

Method: 设计严格的组件封装接口，引入基于代码行数（LoC）的模块化度量方法，支持快速集成新功能。

Result: AXLearn在模块化方面表现出色，集成新功能仅需少量代码，同时保持高性能。

Conclusion: AXLearn通过模块化和低复杂度设计，为大规模深度学习模型的开发和训练提供了高效解决方案。

Abstract: We design and implement AXLearn, a production deep learning system that
facilitates scalable and high-performance training of large deep learning
models. Compared to other state-of-the-art deep learning systems, AXLearn has a
unique focus on modularity and support for heterogeneous hardware
infrastructure. AXLearn's internal interfaces between software components
follow strict encapsulation, allowing different components to be assembled to
facilitate rapid model development and experimentation on heterogeneous compute
infrastructure. We introduce a novel method of quantifying modularity via
Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains
constant complexity as we scale the components in the system, compared to
linear or quadratic complexity in other systems. This allows integrating
features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred
of modules with just 10 lines of code, compared to hundreds as required in
other systems. At the same time, AXLearn maintains equivalent performance
compared to state-of-the-art training systems. Finally, we share our experience
in the development and operation of AXLearn.

</details>


### [159] [Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift](https://arxiv.org/abs/2507.05412)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 论文提出了一种名为RepLIn的训练算法，用于学习因果相关潜在变量的鲁棒判别表示，通过显式强制执行干预期间的统计独立性，提高了干预数据的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理干预数据时忽略了因果模型的独立性关系，导致预测性能在观察数据和干预数据之间存在较大差异。本文旨在解决这一问题。

Method: 论文首先分析了性能差异与干预因果模型独立性条件的关系，推导了线性模型中干预数据比例的充分条件，并提出了RepLIn算法来显式强制执行统计独立性。

Result: 实验表明，RepLIn在合成数据集和真实图像、文本数据集上均能有效降低干预数据的误差，且适用于连续和离散潜在变量。

Conclusion: RepLIn能够显著提高干预数据的鲁棒性，适用于大规模因果图，并扩展了现有方法的局限性。

Abstract: We consider the problem of learning robust discriminative representations of
causally-related latent variables. In addition to observational data, the
training dataset also includes interventional data obtained through targeted
interventions on some of these latent variables to learn representations robust
against the resulting interventional distribution shifts. Existing approaches
treat interventional data like observational data, even when the underlying
causal model is known, and ignore the independence relations that arise from
these interventions. Since these approaches do not fully exploit the causal
relational information resulting from interventions, they learn representations
that produce large disparities in predictive performance on observational and
interventional data, which worsens when the number of interventional training
samples is limited. In this paper, (1) we first identify a strong correlation
between this performance disparity and adherence of the representations to the
independence conditions induced by the interventional causal model. (2) For
linear models, we derive sufficient conditions on the proportion of
interventional data in the training dataset, for which enforcing interventional
independence between representations corresponding to the intervened node and
its non-descendants lowers the error on interventional data. Combining these
insights, (3) we propose RepLIn, a training algorithm to explicitly enforce
this statistical independence during interventions. We demonstrate the utility
of RepLIn on a synthetic dataset and on real image and text datasets on facial
attribute classification and toxicity detection, respectively. Our experiments
show that RepLIn is scalable with the number of nodes in the causal graph and
is suitable to improve the robust representations against interventional
distribution shifts of both continuous and discrete latent variables.

</details>


### [160] [EmissionNet: Air Quality Pollution Forecasting for Agriculture](https://arxiv.org/abs/2507.05416)
*Prady Saligram,Tanvir Bhathal*

Main category: cs.LG

TL;DR: 论文探讨了农业排放对空气污染的贡献，提出了两种深度学习模型（EmissionNet和EmissionNet-Transformer）用于预测N$_2$O排放。


<details>
  <summary>Details</summary>
Motivation: 农业排放是空气污染的重要来源，但传统物理模型难以捕捉复杂的非线性污染物相互作用。

Method: 提出了两种深度学习架构（EmissionNet和EmissionNet-Transformer），结合卷积和Transformer技术处理高分辨率排放数据。

Result: 模型能够有效提取时空依赖性，预测N$_2$O排放。

Conclusion: 深度学习模型在农业排放预测中具有潜力，优于传统方法。

Abstract: Air pollution from agricultural emissions is a significant yet often
overlooked contributor to environmental and public health challenges.
Traditional air quality forecasting models rely on physics-based approaches,
which struggle to capture complex, nonlinear pollutant interactions. In this
work, we explore forecasting N$_2$O agricultural emissions through evaluating
popular architectures, and proposing two novel deep learning architectures,
EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage
convolutional and transformer-based architectures to extract spatial-temporal
dependencies from high-resolution emissions data

</details>


### [161] [Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack](https://arxiv.org/abs/2507.05441)
*Edward Raff,Karen Kukla,Michel Benaroch,Joseph Comprix*

Main category: cs.LG

TL;DR: 论文提出了一种名为MVMO的攻击方法，用于帮助财务困境公司操纵财务报告，同时满足反相关目标。


<details>
  <summary>Details</summary>
Motivation: 财务困境公司有动机操纵财务报告以隐藏困境并获取个人利益，现有攻击方法无法满足反相关目标。

Method: 引入MVMO攻击方法，调整攻击者的搜索方向，以找到更多满足条件的攻击方案。

Result: 实验结果显示，50%的情况下，公司可以虚增收益100-200%，同时降低欺诈评分15%。

Conclusion: MVMO攻击方法在实际中具有可行性，通过与律师和专业会计师合作验证了其现实威胁。

Abstract: Bad actors, primarily distressed firms, have the incentive and desire to
manipulate their financial reports to hide their distress and derive personal
gains. As attackers, these firms are motivated by potentially millions of
dollars and the availability of many publicly disclosed and used financial
modeling frameworks. Existing attack methods do not work on this data due to
anti-correlated objectives that must both be satisfied for the attacker to
succeed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that
adapt the attacker's search direction to find $20\times$ more satisfying
attacks compared to standard attacks. The result is that in $\approx50\%$ of
cases, a company could inflate their earnings by 100-200%, while simultaneously
reducing their fraud scores by 15%. By working with lawyers and professional
accountants, we ensure our threat model is realistic to how such frauds are
performed in practice.

</details>


### [162] [2048: Reinforcement Learning in a Delayed Reward Environment](https://arxiv.org/abs/2507.05465)
*Prady Saligram,Tanvir Bhathal,Robby Manihani*

Main category: cs.LG

TL;DR: 论文提出了一种分布式的多步强化学习框架，用于优化长期性能，并在2048游戏中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 延迟和稀疏的奖励是强化学习中的主要挑战，特别是在2048游戏中，即时反馈可能导致局部最优但全局次优的策略。

Method: 开发了四种代理变体：标准DQN、PPO、QR-DQN和新型H-DQN，结合了分布学习、决斗架构等技术。

Result: H-DQN表现最佳，最高得分达41.828K，并达到4096方块，显著优于其他方法。

Conclusion: 分布式多步目标在稀疏奖励领域显著提升性能，未来可通过模型规划和课程学习进一步优化。

Abstract: Delayed and sparse rewards present a fundamental obstacle for
reinforcement-learning (RL) agents, which struggle to assign credit for actions
whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes
this challenge: although frequent small score changes yield immediate feedback,
they often mislead agents into locally optimal but globally suboptimal
strategies. In this work, we introduce a unified, distributional multi-step RL
framework designed to directly optimize long-horizon performance. Using the
open source Gym-2048 environment we develop and compare four agent variants:
standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN
(H-DQN) that integrates distributional learning, dueling architectures, noisy
networks, prioritized replay, and more. Empirical evaluation reveals a clear
hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to
5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048
tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These
results demonstrate that distributional, multi-step targets substantially
enhance performance in sparse-reward domains, and they suggest promising
avenues for further gains through model-based planning and curriculum learning.

</details>


### [163] [Epistemically-guided forward-backward exploration](https://arxiv.org/abs/2507.05477)
*Núria Armengol Urpí,Marin Vlastelica,Georg Martius,Stelian Coros*

Main category: cs.LG

TL;DR: 论文提出了一种基于前向-后向表示（FB）的探索策略，用于零样本强化学习，以优化样本效率。


<details>
  <summary>Details</summary>
Motivation: 零样本强化学习需要在缺乏具体奖励的情况下快速适应未来问题，而FB表示虽能学习最优策略，但通常依赖其他探索算法。作者认为FB表示应直接用于探索以提高效率。

Method: 设计了一种基于FB表示的探索策略，通过最小化FB表示的后验方差来减少认知不确定性。

Result: 实验表明，这种探索策略显著提高了FB算法的样本效率。

Conclusion: FB表示应直接用于探索，以提升零样本强化学习的效率。

Abstract: Zero-shot reinforcement learning is necessary for extracting optimal policies
in absence of concrete rewards for fast adaptation to future problem settings.
Forward-backward representations (FB) have emerged as a promising method for
learning optimal policies in absence of rewards via a factorization of the
policy occupancy measure. However, up until now, FB and many similar zero-shot
reinforcement learning algorithms have been decoupled from the exploration
problem, generally relying on other exploration algorithms for data collection.
We argue that FB representations should fundamentally be used for exploration
in order to learn more efficiently. With this goal in mind, we design
exploration policies that arise naturally from the FB representation that
minimize the posterior variance of the FB representation, hence minimizing its
epistemic uncertainty. We empirically demonstrate that such principled
exploration strategies improve sample complexity of the FB algorithm
considerably in comparison to other exploration methods. Code is publicly
available at https://sites.google.com/view/fbee-url.

</details>


### [164] [Dynamic Regret Reduces to Kernelized Static Regret](https://arxiv.org/abs/2507.05478)
*Andrew Jacobsen,Alessandro Rudi,Francesco Orabona,Nicolo Cesa-Bianchi*

Main category: cs.LG

TL;DR: 该论文通过将动态遗憾问题转化为函数空间中的静态遗憾问题，利用RKHS框架，实现了对线性损失和其他类型损失的最优动态遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 研究在线凸优化中的动态遗憾问题，旨在通过函数空间的方法提升对不同类型损失的处理能力。

Method: 将动态遗憾问题转化为函数空间中的静态遗憾问题，并利用RKHS框架设计算法。

Result: 在线性损失下实现了最优动态遗憾保证，并在其他损失类型下获得了新的无尺度和方向自适应的动态遗憾保证。

Conclusion: 通过RKHS框架，论文不仅扩展了动态遗憾的应用范围，还提供了实际可计算的算法。

Abstract: We study dynamic regret in online convex optimization, where the objective is
to achieve low cumulative loss relative to an arbitrary benchmark sequence. By
observing that competing with an arbitrary sequence of comparators
$u_{1},\ldots,u_{T}$ in $\mathcal{W}\subseteq\mathbb{R}^{d}$ is equivalent to
competing with a fixed comparator function $u:[1,T]\to \mathcal{W}$, we frame
dynamic regret minimization as a static regret problem in a function space. By
carefully constructing a suitable function space in the form of a Reproducing
Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal
$R_{T}(u_{1},\ldots,u_{T}) = \mathcal{O}(\sqrt{\sum_{t}\|u_{t}-u_{t-1}\|T})$
dynamic regret guarantee in the setting of linear losses, and yields new
scale-free and directionally-adaptive dynamic regret guarantees. Moreover,
unlike prior dynamic-to-static reductions -- which are valid only for linear
losses -- our reduction holds for any sequence of losses, allowing us to
recover $\mathcal{O}\big(\|u\|^2+d_{\mathrm{eff}}(\lambda)\ln T\big)$ bounds in
exp-concave and improper linear regression settings, where
$d_{\mathrm{eff}}(\lambda)$ is a measure of complexity of the RKHS. Despite
working in an infinite-dimensional space, the resulting reduction leads to
algorithms that are computable in practice, due to the reproducing property of
RKHSs.

</details>


### [165] [Navigating Sparse Molecular Data with Stein Diffusion Guidance](https://arxiv.org/abs/2507.05482)
*Van Khoa Nguyen,Lionel Blondé,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 论文提出了一种基于代理随机最优控制目标的训练自由扩散引导框架（SDG），通过修正近似后验误差，显著提升了扩散模型的生成性能。


<details>
  <summary>Details</summary>
Motivation: 随机最优控制（SOC）在扩散模型微调中表现优异，但计算成本高；而现有的训练自由方法虽快速但近似误差大，导致引导不可靠。

Method: 结合SOC与训练自由方法，提出SDG框架，利用Stein变分推断修正后验误差，并引入新的运行成本函数。

Result: 在分子生成任务中，SDG显著优于标准训练自由方法。

Conclusion: SDG通过理论修正和实际优化，为扩散模型提供了一种高效可靠的引导方法，具有广泛应用潜力。

Abstract: Stochastic optimal control (SOC) has recently emerged as a principled
framework for fine-tuning diffusion models. However, its dependence on
computationally intensive simulations makes it impractical for fast sampling.
In parallel, a class of training-free approaches has been developed that guides
diffusion models using off-the-shelf classifiers on predicted clean samples,
bypassing the need to train classifiers on noisy data. These methods can be
interpreted as approximate SOC schemes, using Tweedie's formula to estimate
diffusion posteriors. In practice, however, such direct approximations can
introduce significant errors, leading to unreliable guidance. In this work, we
unify the strengths of both paradigms by proposing a novel training-free
diffusion guidance framework based on a surrogate stochastic optimal control
objective. We derive a new theoretical bound on the value function that reveals
the necessity of correcting the approximate posteriors to remain faithful to
the true diffusion posterior. To this end, we connect the problem with Stein
variational inference, which seeks the steepest descent direction that
minimizes the Kullback-Leibler discrepancy between the two posteriors. Our
method, which we refer to as Stein Diffusion Guidance (SDG), introduces a
principled correction mechanism and incorporates a novel running cost
functional to enable effective guidance in low-density regions. Experiments on
challenging molecular generation tasks demonstrate that SDG significantly
outperforms standard training-free guidance methods, highlighting its potential
for broader applications.

</details>


### [166] [Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN)](https://arxiv.org/abs/2507.05498)
*Reza T. Batley,Chanwook Park,Wing Kam Liu,Sourav Saha*

Main category: cs.LG

TL;DR: Ex-HiDeNN是一种新颖的神经网络架构，结合符号回归，从有限数据中发现可解释的闭式表达式，并在多个基准和工程应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法在构建复杂关系时高效，但从复杂数据中发现可解释且准确的闭式表达式仍具挑战性。

Method: 提出Ex-HiDeNN算法，包含两步流程和可分离性检查，结合符号回归从数据中提取闭式表达式。

Result: 在基准问题和工程应用中，Ex-HiDeNN表现优异，误差显著低于传统方法。

Conclusion: Ex-HiDeNN在闭式表达式发现方面具有潜力，但仍存在局限性，未来可进一步扩展。

Abstract: Data-driven science and computation have advanced immensely to construct
complex functional relationships using trainable parameters. However,
efficiently discovering interpretable and accurate closed-form expressions from
complex dataset remains a challenge. The article presents a novel approach
called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that
uses an accurate, frugal, fast, separable, and scalable neural architecture
with symbolic regression to discover closed-form expressions from limited
observation. The article presents the two-step Ex-HiDeNN algorithm with a
separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN
are tested on several benchmark problems, including discerning a dynamical
system from data, and the outcomes are reported. Ex-HiDeNN generally shows
outstanding approximation capability in these benchmarks, producing orders of
magnitude smaller errors compared to reference data and traditional symbolic
regression. Later, Ex-HiDeNN is applied to three engineering applications: a)
discovering a closed-form fatigue equation, b) identification of hardness from
micro-indentation test data, and c) discovering the expression for the yield
surface with data. In every case, Ex-HiDeNN outperformed the reference methods
used in the literature. The proposed method is built upon the foundation and
published works of the authors on Hierarchical Deep Learning Neural Network
(HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about
the current limitations and future extensions of Ex-HiDeNN.

</details>


### [167] [Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs](https://arxiv.org/abs/2507.05507)
*Godwin Badu-Marfo,Bilal Farooq*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的集成架构（GCLSTM），用于动态预测校园建筑占用率和建筑间人流，结合Wi-Fi日志和建筑使用计划，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统人流预测方法无法捕捉复杂动态模式的问题，同时保护个人隐私。

Method: 使用图卷积网络（GCN）与LSTM结合的GCLSTM模型，直接从Wi-Fi数据中学习流量模式，无需假设用户行为。

Result: GCLSTM在多伦多大都会大学真实数据上的表现显著优于MLP和线性回归等传统方法。

Conclusion: GCLSTM为动态人流预测提供了高效且隐私保护的解决方案。

Abstract: We present an integrated graph-based neural networks architecture for
predicting campus buildings occupancy and inter-buildings movement at dynamic
temporal resolution that learns traffic flow patterns from Wi-Fi logs combined
with the usage schedules within the buildings. The relative traffic flows are
directly estimated from the WiFi data without assuming the occupant behaviour
or preferences while maintaining individual privacy. We formulate the problem
as a data-driven graph structure represented by a set of nodes (representing
buildings), connected through a route of edges or links using a novel Graph
Convolution plus LSTM Neural Network (GCLSTM) which has shown remarkable
success in modelling complex patterns. We describe the formulation, model
estimation, interpretability and examine the relative performance of our
proposed model. We also present an illustrative architecture of the models and
apply on real-world WiFi logs collected at the Toronto Metropolitan University
campus. The results of the experiments show that the integrated GCLSTM models
significantly outperform traditional pedestrian flow estimators like the Multi
Layer Perceptron (MLP) and Linear Regression.

</details>


### [168] [Beyond Communication Overhead: A Multilevel Monte Carlo Approach for Mitigating Compression Bias in Distributed Learning](https://arxiv.org/abs/2507.05508)
*Ze'ev Zukerman,Bassel Hamoud,Kfir Y. Levy*

Main category: cs.LG

TL;DR: 提出了一种新颖的多级蒙特卡洛（MLMC）压缩方案，结合了有偏和无偏压缩器的优点，解决了分布式学习中通信开销的问题。


<details>
  <summary>Details</summary>
Motivation: 分布式学习中通信开销是关键瓶颈，现有梯度压缩技术在有偏和无偏压缩器之间存在权衡。

Method: 引入MLMC压缩方案，利用有偏压缩器构建统计无偏估计，并应用于Top-k和位级压缩器。

Result: 方法在分布式深度学习任务中表现优异，验证了其有效性。

Conclusion: MLMC方案成功弥合了有偏和无偏压缩器之间的差距，提升了分布式学习的效率。

Abstract: Distributed learning methods have gained substantial momentum in recent
years, with communication overhead often emerging as a critical bottleneck.
Gradient compression techniques alleviate communication costs but involve an
inherent trade-off between the empirical efficiency of biased compressors and
the theoretical guarantees of unbiased compressors. In this work, we introduce
a novel Multilevel Monte Carlo (MLMC) compression scheme that leverages biased
compressors to construct statistically unbiased estimates. This approach
effectively bridges the gap between biased and unbiased methods, combining the
strengths of both. To showcase the versatility of our method, we apply it to
popular compressors, like Top-$k$ and bit-wise compressors, resulting in
enhanced variants. Furthermore, we derive an adaptive version of our approach
to further improve its performance. We validate our method empirically on
distributed deep learning tasks.

</details>


### [169] [Heterogeneous Causal Learning for Optimizing Aggregated Functions in User Growth](https://arxiv.org/abs/2507.05510)
*Shuyang Du,Jennifer Zhang,Will Y. Zou*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的处理效果优化方法，用于提升用户增长营销的效果，直接建模关键业务指标的提升，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 优化昂贵的营销活动，最大化用户参与度，提升用户增长策略的效果。

Method: 利用深度学习从历史实验中学习，优化用户选择和奖励分配，通过软最大门控联合优化参数。

Result: 在综合评估中，提出的算法比现有方法（如R-learner和Causal Forest）表现优异，效果提升超过20%。

Conclusion: 该方法具有成本效益和实际应用价值，已在全球范围内成功部署。

Abstract: User growth is a major strategy for consumer internet companies. To optimize
costly marketing campaigns and maximize user engagement, we propose a novel
treatment effect optimization methodology to enhance user growth marketing. By
leveraging deep learning, our algorithm learns from past experiments to
optimize user selection and reward allocation, maximizing campaign impact while
minimizing costs. Unlike traditional prediction methods, our model directly
models uplifts in key business metrics. Further, our deep learning model can
jointly optimize parameters for an aggregated loss function using softmax
gating. Our approach surpasses traditional methods by directly targeting
desired business metrics and demonstrates superior algorithmic flexibility in
handling complex business constraints. Comprehensive evaluations, including
comparisons with state-of-the-art techniques such as R-learner and Causal
Forest, validate the effectiveness of our model. We experimentally demonstrate
that our proposed constrained and direct optimization algorithms significantly
outperform state-of-the-art methods by over $20\%$, proving their
cost-efficiency and real-world impact. The versatile methods can be applied to
various product scenarios, including optimal treatment allocation. Its
effectiveness has also been validated through successful worldwide production
deployments.

</details>


### [170] [Deep Learning of Continuous and Structured Policies for Aggregated Heterogeneous Treatment Effects](https://arxiv.org/abs/2507.05511)
*Jennifer Y. Zhang,Shuyang Du,Will Y. Zou*

Main category: cs.LG

TL;DR: 论文提出了一种深度学习框架，用于处理多政策变量的异质处理效应（HTE）估计，并直接对受试者进行排序。


<details>
  <summary>Details</summary>
Motivation: 随着HTE估计在科学和工业应用中的普及，政策空间从二元扩展到结构化，需要新的方法来处理多政策变量。

Method: 结合神经增强的朴素贝叶斯层和深度学习框架，处理连续和离散政策变量，并直接排序聚合处理效应函数。

Result: 在公共数据集上验证了方法的有效性，提升了性能。

Conclusion: 该方法为深度学习异质政策提供了一个通用框架，展示了其在多政策变量下的潜力。

Abstract: As estimation of Heterogeneous Treatment Effect (HTE) is increasingly adopted
across a wide range of scientific and industrial applications, the treatment
action space can naturally expand, from a binary treatment variable to a
structured treatment policy. This policy may include several policy factors
such as a continuous treatment intensity variable, or discrete treatment
assignments. From first principles, we derive the formulation for incorporating
multiple treatment policy variables into the functional forms of individual and
average treatment effects. Building on this, we develop a methodology to
directly rank subjects using aggregated HTE functions. In particular, we
construct a Neural-Augmented Naive Bayes layer within a deep learning framework
to incorporate an arbitrary number of factors that satisfies the Naive Bayes
assumption. The factored layer is then applied with continuous treatment
variables, treatment assignment, and direct ranking of aggregated treatment
effect functions. Together, these algorithms build towards a generic framework
for deep learning of heterogeneous treatment policies, and we show their power
to improve performance with public datasets.

</details>


### [171] [Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning](https://arxiv.org/abs/2507.05526)
*Anish Dhir,Cristiana Diaconu,Valentinian Mihai Lungu,James Requeima,Richard E. Turner,Mark van der Wilk*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的端到端模型MACE-TNP，用于预测贝叶斯模型平均干预后验分布，解决了因果结构不确定性带来的计算难题。


<details>
  <summary>Details</summary>
Motivation: 在缺乏领域知识的情况下，从观测数据中发现因果结构存在不确定性，传统方法容易过度自信。贝叶斯推断虽能管理这种不确定性，但计算复杂度高。

Method: 提出MACE-TNP模型，通过元学习直接预测贝叶斯模型平均干预后验分布，避免昂贵的计算。

Result: 实验表明MACE-TNP优于强贝叶斯基线。

Conclusion: 元学习为复杂贝叶斯因果推断提供了一种灵活且可扩展的范式，未来可应用于更具挑战性的场景。

Abstract: In scientific domains -- from biology to the social sciences -- many
questions boil down to \textit{What effect will we observe if we intervene on a
particular variable?} If the causal relationships (e.g.~a causal graph) are
known, it is possible to estimate the intervention distributions. In the
absence of this domain knowledge, the causal structure must be discovered from
the available observational data. However, observational data are often
compatible with multiple causal graphs, making methods that commit to a single
structure prone to overconfidence. A principled way to manage this structural
uncertainty is via Bayesian inference, which averages over a posterior
distribution on possible causal structures and functional mechanisms.
Unfortunately, the number of causal structures grows super-exponentially with
the number of nodes in the graph, making computations intractable. We propose
to circumvent these challenges by using meta-learning to create an end-to-end
model: the Model-Averaged Causal Estimation Transformer Neural Process
(MACE-TNP). The model is trained to predict the Bayesian model-averaged
interventional posterior distribution, and its end-to-end nature bypasses the
need for expensive calculations. Empirically, we demonstrate that MACE-TNP
outperforms strong Bayesian baselines. Our work establishes meta-learning as a
flexible and scalable paradigm for approximating complex Bayesian causal
inference, that can be scaled to increasingly challenging settings in the
future.

</details>


### [172] [Mitigating Shortcut Learning with InterpoLated Learning](https://arxiv.org/abs/2507.05527)
*Michalis Korakakis,Andreas Vlachos,Adrian Weller*

Main category: cs.LG

TL;DR: InterpoLL通过插值学习削弱多数类样本中的捷径影响，提升模型在少数类样本上的泛化能力，同时不影响多数类样本的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缓解捷径影响时存在模型特定性、调参困难、计算成本高且未能改善学习表示的问题，InterpoLL旨在解决这些问题。

Method: InterpoLL通过插值多数类样本的表示，引入少数类样本的特征，从而削弱捷径的影响。

Result: 实验表明，InterpoLL在多个自然语言理解任务中优于ERM和现有方法，提升了少数类样本的泛化能力。

Conclusion: InterpoLL具有广泛的适用性，适用于多种架构，且不损害多数类样本的性能。

Abstract: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts,
i.e., spurious correlations between input attributes and labels that are
prevalent in the majority of the training data but unrelated to the task at
hand. This reliance hinders generalization on minority examples, where such
correlations do not hold. Existing shortcut mitigation approaches are
model-specific, difficult to tune, computationally expensive, and fail to
improve learned representations. To address these issues, we propose
InterpoLated Learning (InterpoLL) which interpolates the representations of
majority examples to include features from intra-class minority examples with
shortcut-mitigating patterns. This weakens shortcut influence, enabling models
to acquire features predictive across both minority and majority examples.
Experimental results on multiple natural language understanding tasks
demonstrate that InterpoLL improves minority generalization over both ERM and
state-of-the-art shortcut mitigation methods, without compromising accuracy on
majority examples. Notably, these gains persist across encoder,
encoder-decoder, and decoder-only architectures, demonstrating the method's
broad applicability.

</details>


### [173] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 该论文提出了一种针对图神经网络（GNNs）的硬件故障攻击方法GBFA，通过逐层翻转权重参数中的特定位，显著降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GNNs在图形数据上表现出色，但硬件加速器的安全性问题被忽视。本文研究了GNNs对硬件故障攻击的脆弱性。

Method: GBFA采用两步法：1）基于马尔可夫模型预测层执行顺序；2）通过梯度排名识别脆弱位并翻转。

Result: 实验显示，GBFA仅翻转一位即可使GraphSAGE在Cora数据集上的预测准确率下降17%。

Conclusion: 层感知攻击策略对GNNs的安全性至关重要，GBFA展示了硬件攻击的实际威胁。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [174] [Theoretical Learning Performance of Graph Neural Networks: The Impact of Jumping Connections and Layer-wise Sparsification](https://arxiv.org/abs/2507.05533)
*Jiawei Sun,Hongkang Li,Meng Wang*

Main category: cs.LG

TL;DR: 论文分析了跳跃连接和图稀疏化在GCN中的理论作用，首次证明了稀疏化保持泛化性能的条件，并揭示了不同层对稀疏化的需求差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究对跳跃连接和图稀疏化在GCN中的理论分析不足，缺乏对泛化性能的深入理解。

Method: 通过理论分析跳跃连接与图稀疏化的结合，提出稀疏有效邻接矩阵$A^*$，并验证其在不同层中的作用。

Result: 稀疏化保持泛化性能的关键是保留支持消息传播的重要边；跳跃连接导致不同层对稀疏化需求不同。

Conclusion: 论文首次理论证明了跳跃连接在图稀疏化中的作用，为GCN的设计提供了理论依据。

Abstract: Jumping connections enable Graph Convolutional Networks (GCNs) to overcome
over-smoothing, while graph sparsification reduces computational demands by
selecting a sub-matrix of the graph adjacency matrix during neighborhood
aggregation. Learning GCNs with graph sparsification has shown empirical
success across various applications, but a theoretical understanding of the
generalization guarantees remains limited, with existing analyses ignoring
either graph sparsification or jumping connections. This paper presents the
first learning dynamics and generalization analysis of GCNs with jumping
connections using graph sparsification. Our analysis demonstrates that the
generalization accuracy of the learned model closely approximates the highest
achievable accuracy within a broad class of target functions dependent on the
proposed sparse effective adjacency matrix $A^*$. Thus, graph sparsification
maintains generalization performance when $A^*$ preserves the essential edges
that support meaningful message propagation. We reveal that jumping connections
lead to different sparsification requirements across layers. In a
two-hidden-layer GCN, the generalization is more affected by the sparsified
matrix deviations from $A^*$ of the first layer than the second layer. To the
best of our knowledge, this marks the first theoretical characterization of
jumping connections' role in sparsification requirements. We validate our
theoretical results on benchmark datasets in deep GCNs.

</details>


### [175] [Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge](https://arxiv.org/abs/2507.05540)
*Chunhui Gu,Mohammad Sadegh Nasr,James P. Long,Kim-Anh Do,Ehsan Irajizad*

Main category: cs.LG

TL;DR: 提出LSC-GNN方法，通过引入外部干净链接约束噪声图的嵌入，提升GNN在噪声环境下的性能。


<details>
  <summary>Details</summary>
Motivation: GNN在噪声边的情况下表现不佳，需要一种方法利用外部干净链接指导噪声图的嵌入。

Method: 训练两个编码器，一个在全图上，另一个在正则化图上，通过惩罚潜在表示差异避免过拟合噪声边。

Result: 在基准数据集上，LSC-GNN优于标准及抗噪声GNN，并在异构图和小规模蛋白质-代谢物网络上验证了其有效性。

Conclusion: LSC-GNN能提升噪声关系结构下的预测性能和可解释性。

Abstract: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose
Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate
external "clean" links and guide embeddings of a noisy target graph. We train
two encoders--one on the full graph (target plus external edges) and another on
a regularization graph excluding the target's potentially noisy links--then
penalize discrepancies between their latent representations. This constraint
steers the model away from overfitting spurious edges. Experiments on benchmark
datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs
subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and
validate it on a small protein-metabolite network, where metabolite-protein
interactions reduce noise in protein co-occurrence data. Our results highlight
LSC-GNN's potential to boost predictive performance and interpretability in
settings with noisy relational structures.

</details>


### [176] [Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation](https://arxiv.org/abs/2507.06111)
*Mohamad H. Danesh,Maxime Wabartha,Stanley Wu,Joelle Pineau,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: UARL是一种新型强化学习框架，通过不确定性感知和模拟环境训练，提升策略在真实世界中的安全性和鲁棒性，无需直接与目标域交互。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在真实世界部署中的分布偏移、安全性问题以及直接交互的不实用性。

Method: 使用评论家集合量化策略不确定性，结合渐进环境随机化，在模拟环境中迭代优化高不确定性区域。

Result: 在MuJoCo基准和四足机器人上验证了UARL的有效性，表现出可靠的OOD检测、性能提升和样本效率。

Conclusion: UARL为强化学习在真实世界中的安全部署提供了一种可行方案，避免了直接交互的风险。

Abstract: Deploying reinforcement learning (RL) policies in real-world involves
significant challenges, including distribution shifts, safety concerns, and the
impracticality of direct interactions during policy refinement. Existing
methods, such as domain randomization (DR) and off-dynamics RL, enhance policy
robustness by direct interaction with the target domain, an inherently unsafe
practice. We propose Uncertainty-Aware RL (UARL), a novel framework that
prioritizes safety during training by addressing Out-Of-Distribution (OOD)
detection and policy adaptation without requiring direct interactions in target
domain. UARL employs an ensemble of critics to quantify policy uncertainty and
incorporates progressive environmental randomization to prepare the policy for
diverse real-world conditions. By iteratively refining over high-uncertainty
regions of the state space in simulated environments, UARL enhances robust
generalization to the target domain without explicitly training on it. We
evaluate UARL on MuJoCo benchmarks and a quadrupedal robot, demonstrating its
effectiveness in reliable OOD detection, improved performance, and enhanced
sample efficiency compared to baselines.

</details>


### [177] [Gait-Based Hand Load Estimation via Deep Latent Variable Models with Auxiliary Information](https://arxiv.org/abs/2507.05544)
*Jingyi Gao,Sol Lim,Seokhyun Chung*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的负载估计框架，结合了辅助信息（如无负载行走时的基线步态模式和携带方式），通过深度潜在变量建模与时序卷积网络和双向交叉注意力机制融合负载和无负载步态模式，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常直接从负载步态映射到手部负载，限制了泛化能力和预测准确性。因此，需要一种能够结合辅助信息的方法来提升性能。

Method: 采用深度潜在变量建模与时序卷积网络和双向交叉注意力机制，融合负载和无负载步态模式，并在无需携带方式标签的情况下估计负载大小。

Result: 实验表明，结合辅助信息和显式融合机制显著提高了负载估计的准确性。

Conclusion: 该框架通过结合辅助信息和显式融合机制，有效提升了负载估计的准确性，同时避免了携带方式标签的需求。

Abstract: Machine learning methods are increasingly applied to ergonomic risk
assessment in manual material handling, particularly for estimating carried
load from gait motion data collected from wearable sensors. However, existing
approaches often rely on direct mappings from loaded gait to hand load,
limiting generalization and predictive accuracy. In this study, we propose an
enhanced load estimation framework that incorporates auxiliary information,
including baseline gait patterns during unloaded walking and carrying style.
While baseline gait can be automatically captured by wearable sensors and is
thus readily available at inference time, carrying style typically requires
manual labeling and is often unavailable during deployment. Our model
integrates deep latent variable modeling with temporal convolutional networks
and bi-directional cross-attention to capture gait dynamics and fuse loaded and
unloaded gait patterns. Guided by domain knowledge, the model is designed to
estimate load magnitude conditioned on carrying style, while eliminating the
need for carrying style labels at inference time. Experiments using real-world
data collected from inertial measurement units attached to participants
demonstrate substantial accuracy gains from incorporating auxiliary information
and highlight the importance of explicit fusion mechanisms over naive feature
concatenation.

</details>


### [178] [Preemptive Solving of Future Problems: Multitask Preplay in Humans and Machines](https://arxiv.org/abs/2507.05561)
*Wilka Carvalho,Sam Hall-McMaster,Honglak Lee,Samuel J. Gershman*

Main category: cs.LG

TL;DR: Multitask Preplay是一种新算法，通过重放一个任务的经验来预学习其他未执行任务，以支持快速适应任务表现。实验证明其在网格世界和Craftax环境中优于传统方法，并能提升AI代理在多任务环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 人类通常只能同时执行少量任务，但可以利用一个任务的经验预学习其他未执行任务的解决方案。研究旨在验证这一假设并开发相应算法。

Method: 提出Multitask Preplay算法，通过重放一个任务的经验进行“预播放”（模拟未执行任务），学习预测性表征以支持快速适应。

Result: 在网格世界和Craftax环境中，Multitask Preplay优于传统方法，能预测人类泛化行为并提升AI代理的跨任务表现。

Conclusion: Multitask Preplay是一种可扩展的理论，能解释人类如何通过反事实学习泛化多任务，同时显著提升AI代理在多任务环境中的性能。

Abstract: Humans can pursue a near-infinite variety of tasks, but typically can only
pursue a small number at the same time. We hypothesize that humans leverage
experience on one task to preemptively learn solutions to other tasks that were
accessible but not pursued. We formalize this idea as Multitask Preplay, a
novel algorithm that replays experience on one task as the starting point for
"preplay" -- counterfactual simulation of an accessible but unpursued task.
Preplay is used to learn a predictive representation that can support fast,
adaptive task performance later on. We first show that, compared to traditional
planning and predictive representation methods, multitask preplay better
predicts how humans generalize to tasks that were accessible but not pursued in
a small grid-world, even when people didn't know they would need to generalize
to these tasks. We then show these predictions generalize to Craftax, a
partially observable 2D Minecraft environment. Finally, we show that Multitask
Preplay enables artificial agents to learn behaviors that transfer to novel
Craftax worlds sharing task co-occurrence structure. These findings demonstrate
that Multitask Preplay is a scalable theory of how humans counterfactually
learn and generalize across multiple tasks; endowing artificial agents with the
same capacity can significantly improve their performance in challenging
multitask environments.

</details>


### [179] [The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://arxiv.org/abs/2507.05578)
*Alexander Xiong,Xuandong Zhao,Aneesh Pappu,Dawn Song*

Main category: cs.LG

TL;DR: 本文综述了大型语言模型（LLMs）的记忆现象，探讨了其影响因素、检测方法及缓解策略，并分析了技术、隐私和性能维度的研究现状。


<details>
  <summary>Details</summary>
Motivation: LLMs的记忆行为引发了对模型行为、隐私风险及学习与记忆边界的关注，需系统研究其影响与应对方法。

Method: 综合近期研究，分析记忆现象的关键驱动因素（如训练数据重复、训练动态等），并评估检测方法（如前缀提取、成员推断等）和缓解策略（如数据清理、差分隐私等）。

Result: 研究发现记忆现象受多种因素影响，现有检测方法有效但仍有局限，缓解策略需平衡隐私与模型性能。

Conclusion: 本文全面总结了LLM记忆的研究现状，指出了未来工作的关键方向，强调了在减少有害记忆与保持模型效用之间的平衡挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks, yet they also exhibit memorization of their training
data. This phenomenon raises critical questions about model behavior, privacy
risks, and the boundary between learning and memorization. Addressing these
concerns, this paper synthesizes recent studies and investigates the landscape
of memorization, the factors influencing it, and methods for its detection and
mitigation. We explore key drivers, including training data duplication,
training dynamics, and fine-tuning procedures that influence data memorization.
In addition, we examine methodologies such as prefix-based extraction,
membership inference, and adversarial prompting, assessing their effectiveness
in detecting and measuring memorized content. Beyond technical analysis, we
also explore the broader implications of memorization, including the legal and
ethical implications. Finally, we discuss mitigation strategies, including data
cleaning, differential privacy, and post-training unlearning, while
highlighting open challenges in balancing the minimization of harmful
memorization with utility. This paper provides a comprehensive overview of the
current state of research on LLM memorization across technical, privacy, and
performance dimensions, identifying critical directions for future work.

</details>


### [180] [The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction](https://arxiv.org/abs/2507.05584)
*Beibei Li*

Main category: cs.LG

TL;DR: 提出了一种结合经典谱方法和注意力神经网络的统一傅里叶谱Transformer网络，用于高精度预测复杂动力系统。


<details>
  <summary>Details</summary>
Motivation: 结合谱方法和Transformer的优势，解决传统数值方法和机器学习方法在长期预测中的局限性。

Method: 将原始PDE转化为谱常微分方程，用高精度数值求解器生成训练数据，并用Transformer建模谱系数的演化。

Result: 在二维不可压缩Navier-Stokes方程和一维Burgers方程上验证，谱Transformer在有限训练数据下仍能实现高精度长期预测。

Conclusion: 该框架泛化能力强，为复杂动力系统的实时预测和控制提供了新范式。

Abstract: In this work we propose a unified Fourier Spectral Transformer network that
integrates the strengths of classical spectral methods and attention based
neural architectures. By transforming the original PDEs into spectral ordinary
differential equations, we use high precision numerical solvers to generate
training data and use a Transformer network to model the evolution of the
spectral coefficients. We demonstrate the effectiveness of our approach on the
two dimensional incompressible Navier-Stokes equations and the one dimensional
Burgers' equation. The results show that our spectral Transformer can achieve
highly accurate long term predictions even with limited training data, better
than traditional numerical methods and machine learning methods in forecasting
future flow dynamics. The proposed framework generalizes well to unseen data,
bringing a promising paradigm for real time prediction and control of complex
dynamical systems.

</details>


### [181] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 论文通过大规模实证研究，分析了强化学习中的奖励黑客问题，提出自动化检测框架，并在多种环境和算法中验证其有效性，同时探讨了缓解技术及其实际挑战。


<details>
  <summary>Details</summary>
Motivation: 奖励黑客问题对自主代理的部署构成严重威胁，但目前缺乏系统的检测和缓解方法。

Method: 研究分析了15,247个训练片段，覆盖15种RL环境和5种算法，并开发了自动化检测框架，涵盖六类奖励黑客行为。

Result: 检测框架的精确度为78.4%，召回率为81.7%，计算开销低于5%。缓解技术在某些场景下将黑客频率降低54.6%。

Conclusion: 奖励密度和与真实目标的匹配度显著影响黑客频率，但实际应用中缓解技术面临概念漂移和对抗性适应等挑战。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


### [182] [Graph Learning](https://arxiv.org/abs/2507.05636)
*Feng Xia,Ciyuan Peng,Jing Ren,Falih Gozi Febrinanto,Renqiang Luo,Vidya Saikrishna,Shuo Yu,Xiangjie Kong*

Main category: cs.LG

TL;DR: 该论文综述了图学习的发展、关键技术及其应用，并探讨了未来挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 图学习因其能建模复杂非欧几里得关系而成为AI重要子领域，但需解决可扩展性、可解释性等问题以释放潜力。

Method: 综述了图学习的关键维度，包括可扩展、时序、多模态、生成式、可解释和负责任图学习，并回顾了相关技术。

Result: 总结了图学习在各领域的应用及技术进展，同时提出未来研究方向。

Conclusion: 该综述为研究者和从业者提供了图学习领域的全面指南，并展望了未来发展方向。

Abstract: Graph learning has rapidly evolved into a critical subfield of machine
learning and artificial intelligence (AI). Its development began with early
graph-theoretic methods, gaining significant momentum with the advent of graph
neural networks (GNNs). Over the past decade, progress in scalable
architectures, dynamic graph modeling, multimodal learning, generative AI,
explainable AI (XAI), and responsible AI has broadened the applicability of
graph learning to various challenging environments. Graph learning is
significant due to its ability to model complex, non-Euclidean relationships
that traditional machine learning struggles to capture, thus better supporting
real-world applications ranging from drug discovery and fraud detection to
recommender systems and scientific reasoning. However, challenges like
scalability, generalization, heterogeneity, interpretability, and
trustworthiness must be addressed to unlock its full potential. This survey
provides a comprehensive introduction to graph learning, focusing on key
dimensions including scalable, temporal, multimodal, generative, explainable,
and responsible graph learning. We review state-of-the-art techniques for
efficiently handling large-scale graphs, capturing dynamic temporal
dependencies, integrating heterogeneous data modalities, generating novel graph
samples, and enhancing interpretability to foster trust and transparency. We
also explore ethical considerations, such as privacy and fairness, to ensure
responsible deployment of graph learning models. Additionally, we identify and
discuss emerging topics, highlighting recent integration of graph learning and
other AI paradigms and offering insights into future directions. This survey
serves as a valuable resource for researchers and practitioners seeking to
navigate the rapidly evolving landscape of graph learning.

</details>


### [183] [FACT: the Features At Convergence Theorem for neural networks](https://arxiv.org/abs/2507.05644)
*Enric Boix-Adsera,Neil Mallinar,James B. Simon,Mikhail Belkin*

Main category: cs.LG

TL;DR: 论文提出了“收敛特征定理”（FACT），揭示了神经网络权重在非零权重衰减训练下的自洽方程，并通过实验验证了其有效性。基于此，提出了新算法FACT-RFM，在表格数据上表现优异，并能捕捉神经网络训练中的特征学习行为。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何学习和表示特征是深度学习理论的核心挑战。

Method: 提出FACT定理，描述神经网络权重收敛时的自洽方程，并基于此开发新算法FACT-RFM。

Result: 实验验证FACT的有效性，FACT-RFM在表格数据上表现优异，并能模拟神经网络训练中的特征学习行为。

Conclusion: FACT为理解神经网络特征学习提供了理论工具，FACT-RFM展示了其实际应用潜力。

Abstract: A central challenge in deep learning theory is to understand how neural
networks learn and represent features. To this end, we prove the Features at
Convergence Theorem (FACT), which gives a self-consistency equation that neural
network weights satisfy at convergence when trained with nonzero weight decay.
For each weight matrix $W$, this equation relates the "feature matrix" $W^\top
W$ to the set of input vectors passed into the matrix during forward
propagation and the loss gradients passed through it during backpropagation. We
validate this relation empirically, showing that neural features indeed satisfy
the FACT at convergence. Furthermore, by modifying the "Recursive Feature
Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at
a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on
tabular data and captures various feature learning behaviors that occur in
neural network training, including grokking in modular arithmetic and phase
transitions in learning sparse parities.

</details>


### [184] [Canine Clinical Gait Analysis for Orthopedic and Neurological Disorders: An Inertial Deep-Learning Approach](https://arxiv.org/abs/2507.05671)
*Netta Palez,Léonie Straß,Sebastian Meller,Holger Volk,Anna Zamansky,Itzik Klein*

Main category: cs.LG

TL;DR: 研究利用可穿戴惯性传感器和深度学习区分犬类神经性和骨科性步态异常，准确率达96%（多分类）和82%（二分类）。


<details>
  <summary>Details</summary>
Motivation: 兽医临床中难以区分神经性和骨科性步态异常，需客观诊断工具。

Method: 开发深度学习模型，优化传感器配置和模型架构，使用29只狗的数据集。

Result: 多分类准确率96%，二分类准确率82%，模型具泛化能力。

Conclusion: 惯性传感器结合深度学习可作为区分步态异常的实用工具。

Abstract: Canine gait analysis using wearable inertial sensors is gaining attention in
veterinary clinical settings, as it provides valuable insights into a range of
mobility impairments. Neurological and orthopedic conditions cannot always be
easily distinguished even by experienced clinicians. The current study explored
and developed a deep learning approach using inertial sensor readings to assess
whether neurological and orthopedic gait could facilitate gait analysis. Our
investigation focused on optimizing both performance and generalizability in
distinguishing between these gait abnormalities. Variations in sensor
configurations, assessment protocols, and enhancements to deep learning model
architectures were further suggested. Using a dataset of 29 dogs, our proposed
approach achieved 96% accuracy in the multiclass classification task
(healthy/orthopedic/neurological) and 82% accuracy in the binary classification
task (healthy/non-healthy) when generalizing to unseen dogs. Our results
demonstrate the potential of inertial-based deep learning models to serve as a
practical and objective diagnostic and clinical aid to differentiate gait
assessment in orthopedic and neurological conditions.

</details>


### [185] [Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach](https://arxiv.org/abs/2507.05685)
*Xiaobing Chen,Boyang Zhang,Xiangwei Zhou,Mingxuan Sun,Shuai Zhang,Songyang Zhang,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 该论文探讨了联邦学习（FL）与专家混合（MoE）结合时面临的系统级挑战，并提出了一种动态客户端-专家对齐的设计方案，以提高训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决在去中心化数据上训练大规模AI模型时的隐私保护和效率问题，特别是在异构客户端资源与复杂专家协调之间的挑战。

Method: 提出了一种概念性系统设计，包括动态适应性评分、全局专家负载监控和客户端能力分析。

Result: 通过解决系统性问题，实现了更高效、可扩展和鲁棒的训练机制，减少了收敛所需的通信轮次。

Conclusion: 该设计为在边缘计算中广泛部署大规模联邦MoE结构模型提供了可能，具有超高通信效率。

Abstract: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE)
presents a compelling pathway for training more powerful, large-scale
artificial intelligence models (LAMs) on decentralized data while preserving
privacy. However, efficient federated training of these complex MoE-structured
LAMs is hindered by significant system-level challenges, particularly in
managing the interplay between heterogeneous client resources and the
sophisticated coordination required for numerous specialized experts. This
article highlights a critical, yet underexplored concept: the absence of robust
quantitative strategies for dynamic client-expert alignment that holistically
considers varying client capacities and the imperative for system-wise load
balancing. Specifically, we propose a conceptual system design for intelligent
client-expert alignment that incorporates dynamic fitness scoring, global
expert load monitoring, and client capacity profiling. By tackling these
systemic issues, we can unlock more scalable, efficient, and robust training
mechanisms {with fewer communication rounds for convergence}, paving the way
for the widespread deployment of large-scale federated MoE-structured LAMs in
edge computing with ultra-high communication efficiency.

</details>


### [186] [AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs](https://arxiv.org/abs/2507.05687)
*Shangzhan Li,Zefan Wang,Ye He,Yuxuan Li,Qi Shi,Jianling Li,Yonggang Hu,Wanxiang Che,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: AutoTriton是一个基于强化学习的模型，用于自动优化Triton编程中的关键参数，如瓦片大小和内存访问模式，以减少手动调优的负担。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的内核开发需要跨硬件优化计算单元，手动调优关键参数（如瓦片大小和内存访问模式）耗时且复杂，阻碍了性能优化和广泛采用。

Method: AutoTriton通过监督微调（SFT）获取Triton编程知识，并使用强化学习（RL）结合基于规则和执行的奖励进一步优化编程能力。

Result: 在TritonBench和KernelBench的五个评估通道中，AutoTriton（8B模型）表现与主流大模型（如Claude-4-Sonnet和DeepSeek-R1-0528）相当。

Conclusion: AutoTriton展示了强化学习在自动生成高性能内核中的潜力，为构建更高效的AI系统奠定了基础。

Abstract: Kernel development in deep learning requires optimizing computational units
across hardware while balancing memory management, parallelism, and
hardware-specific optimizations through extensive empirical tuning. Although
domain-specific languages like Triton simplify GPU programming by abstracting
low-level details, developers must still manually tune critical parameters such
as tile sizes and memory access patterns through iterative experimentation,
creating substantial barriers to optimal performance and wider adoption. In
this work, we introduce AutoTriton, the first model dedicated to Triton
programming powered by reinforcement learning (RL). AutoTriton performs
supervised fine-tuning (SFT) to be equipped with essential Triton programming
expertise using a high-quality data gathering pipeline, and conducts RL with
Group Relative Policy Optimization (GRPO) algorithm, combining a rule-based
reward and an execution-based reward to further improve Triton programming
ability, sequentially. Experiments across five evaluation channels of
TritonBench and KernelBench illustrate that our 8B model AutoTriton achieves
performance comparable to mainstream large models, including Claude-4-Sonnet
and DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial
role of each module within AutoTriton, including the SFT stage, the RL stage,
and the reward design strategy. These findings underscore the promise of RL for
automatically generating high-performance kernels, and since high-performance
kernels are core components of AI systems, this breakthrough establishes an
important foundation for building more efficient AI systems. The model and code
will be available at https://github.com/AI9Stars/AutoTriton.

</details>


### [187] [MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://arxiv.org/abs/2507.05720)
*Yucheng Shi,Wenhao Yu,Zaitang Li,Yonglin Wang,Hongming Zhang,Ninghao Liu,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: MobileGUI-RL是一个在线训练GUI代理的框架，通过自探索和任务合成提高可扩展性，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理通常在离线环境中训练，导致可扩展性差、过拟合和策略脆弱。

Method: MobileGUI-RL通过自探索合成任务课程，并改进GRPO算法以适应GUI导航。

Result: 在三个在线移动代理基准测试中表现优于现有方法。

Conclusion: MobileGUI-RL框架有效解决了现有方法的局限性，提升了GUI代理的性能。

Abstract: Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

</details>


### [188] [Hierarchical Task Offloading for UAV-Assisted Vehicular Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2507.05722)
*Hongbao Li,Ziye Jia,Sijie He,Kun Guo,Qihui Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于部分卸载的双层无人机辅助边缘计算架构，通过整合高空无人机的中继能力和低空无人机的计算支持，优化了系统延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现有无人机辅助卸载策略在协调异构计算资源和适应动态网络条件方面不足，亟需改进。

Method: 采用双层架构，结合软演员-评论家算法，将全局决策（卸载比例和轨迹规划）与局部调度（优先级机制）分离。

Result: 仿真表明，该方法在任务完成率、系统效率和收敛速度上优于基线，具有强鲁棒性和动态适应性。

Conclusion: 提出的架构和算法有效提升了无人机辅助边缘计算的性能，适用于动态车载环境。

Abstract: With the emergence of compute-intensive and delay-sensitive applications in
vehicular networks, unmanned aerial vehicles (UAVs) have emerged as a promising
complement for vehicular edge computing due to the high mobility and flexible
deployment. However, the existing UAV-assisted offloading strategies are
insufficient in coordinating heterogeneous computing resources and adapting to
dynamic network conditions. Hence, this paper proposes a dual-layer
UAV-assisted edge computing architecture based on partial offloading, composed
of the relay capability of high-altitude UAVs and the computing support of
low-altitude UAVs. The proposed architecture enables efficient integration and
coordination of heterogeneous resources. A joint optimization problem is
formulated to minimize the system delay and energy consumption while ensuring
the task completion rate. To solve the high-dimensional decision problem, we
reformulate the problem as a Markov decision process and propose a hierarchical
offloading scheme based on the soft actor-critic algorithm. The method
decouples global and local decisions, where the global decisions integrate
offloading ratios and trajectory planning into continuous actions, while the
local scheduling is handled via designing a priority-based mechanism.
Simulations are conducted and demonstrate that the proposed approach
outperforms several baselines in task completion rate, system efficiency, and
convergence speed, showing strong robustness and applicability in dynamic
vehicular environments.

</details>


### [189] [Jigsaw: Training Multi-Billion-Parameter AI Weather Models with Optimized Model Parallelism](https://arxiv.org/abs/2507.05753)
*Deifilia Kieckhefen,Markus Götz,Lars H. Heyen,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: WeatherMixer是一种基于多层感知机的架构，通过线性扩展输入规模学习全球天气现象，结合Jigsaw并行化方案提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 高分辨率、长时效的复杂大气动力学建模需要大神经网络和海量数据，导致内存和I/O带宽成为瓶颈。

Method: 提出WeatherMixer架构和Jigsaw并行化方案，结合域并行和张量并行，消除内存冗余。

Result: 在256个GPU上训练，峰值性能达9和11 PFLOPs，强缩放和弱缩放表现优异。

Conclusion: WeatherMixer和Jigsaw显著提升了天气建模的计算效率和性能。

Abstract: AI-based methods have revolutionized atmospheric forecasting, with recent
successes in medium-range forecasting spurring the development of climate
foundation models. Accurate modeling of complex atmospheric dynamics at high
spatial resolutions and longer lead times requires large neural networks and
gigabyte-sized data samples, making accelerator memory and I/O-bandwidth the
bottlenecks for model training. We introduce WeatherMixer, a
multi-layer-perceptron-based architecture whose workload scales linearly with
input size, allowing the model to learn global weather phenomena at accuracies
similar to numerical weather prediction. To cope with the computational demand,
we propose Jigsaw, a novel model parallelization scheme that employs both
domain and tensor parallelism, eliminating memory redundancy. Jigsaw exceeds
state-of-the-art performance in strong scaling in compute-communication-limited
systems and achieves superscalar weak scaling in I/O-bandwidth-limited systems.
We scale training to 256 GPUs, reaching peak performances of 9 and 11 PFLOPs,
23% and 28% of theoretical peaks, achieving 68% and 72% scaling efficiency
versus 51% without model parallelism.

</details>


### [190] [From Motion to Meaning: Biomechanics-Informed Neural Network for Explainable Cardiovascular Disease Identification](https://arxiv.org/abs/2507.05783)
*Comte Valentin,Gemma Piella,Mario Ceresa,Miguel A. Gonzalez Ballester*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习图像配准和物理信息正则化的创新方法，用于预测心脏组织的生物力学特性并进行疾病分类。


<details>
  <summary>Details</summary>
Motivation: 心脏疾病是全球发病率和死亡率的主要原因之一，需要准确及时的诊断策略。

Method: 利用Neo-Hookean材料的能量应变公式建模心脏组织变形，结合深度学习和物理信息正则化优化变形场，提取特征用于分类。

Result: 在ACDC数据集上，左心室腔、右心室腔和心肌的Dice分数分别为0.945、0.908和0.905。分类算法在测试集上达到100%准确率。

Conclusion: 该方法通过可解释的人工智能提高了心脏疾病诊断的准确性和可靠性，为个性化治疗提供了支持。

Abstract: Cardiac diseases are among the leading causes of morbidity and mortality
worldwide, which requires accurate and timely diagnostic strategies. In this
study, we introduce an innovative approach that combines deep learning image
registration with physics-informed regularization to predict the biomechanical
properties of moving cardiac tissues and extract features for disease
classification. We utilize the energy strain formulation of Neo-Hookean
material to model cardiac tissue deformations, optimizing the deformation field
while ensuring its physical and biomechanical coherence. This explainable
approach not only improves image registration accuracy, but also provides
insights into the underlying biomechanical processes of the cardiac tissues.
Evaluation on the Automated Cardiac Diagnosis Challenge (ACDC) dataset achieved
Dice scores of 0.945 for the left ventricular cavity, 0.908 for the right
ventricular cavity, and 0.905 for the myocardium. Subsequently, we estimate the
local strains within the moving heart and extract a detailed set of features
used for cardiovascular disease classification. We evaluated five
classification algorithms, Logistic Regression, Multi-Layer Perceptron, Support
Vector Classifier, Random Forest, and Nearest Neighbour, and identified the
most relevant features using a feature selection algorithm. The best performing
classifier obtained a classification accuracy of 98% in the training set and
100% in the test set of the ACDC dataset. By integrating explainable artificial
intelligence, this method empowers clinicians with a transparent understanding
of the model's predictions based on cardiac mechanics, while also significantly
improving the accuracy and reliability of cardiac disease diagnosis, paving the
way for more personalized and effective patient care.

</details>


### [191] [Predicting Graph Structure via Adapted Flux Balance Analysis](https://arxiv.org/abs/2507.05806)
*Sevvandi Kandanaarachchi,Ziqi Xu,Stefan Westerlund,Conrad Sanderson*

Main category: cs.LG

TL;DR: 提出了一种结合时间序列预测和改良的FBA方法，用于动态图结构预测，解决了顶点不变的假设限制。


<details>
  <summary>Details</summary>
Motivation: 现有图预测方法假设顶点不变，限制了应用范围，需改进以适应动态图结构。

Method: 结合时间序列预测和改良的FBA方法，引入适用于图增长的约束条件。

Result: 在合成和真实数据集上验证了方法的有效性。

Conclusion: 该方法能有效预测动态图结构，适用于多种应用场景。

Abstract: Many dynamic processes such as telecommunication and transport networks can
be described through discrete time series of graphs. Modelling the dynamics of
such time series enables prediction of graph structure at future time steps,
which can be used in applications such as detection of anomalies. Existing
approaches for graph prediction have limitations such as assuming that the
vertices do not to change between consecutive graphs. To address this, we
propose to exploit time series prediction methods in combination with an
adapted form of flux balance analysis (FBA), a linear programming method
originating from biochemistry. FBA is adapted to incorporate various
constraints applicable to the scenario of growing graphs. Empirical evaluations
on synthetic datasets (constructed via Preferential Attachment model) and real
datasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the
proposed approach.

</details>


### [192] [Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters](https://arxiv.org/abs/2507.05807)
*Marco Roschkowski*

Main category: cs.LG

TL;DR: 本文提出了一种名为Soup-Adapter的方法，通过训练多个独立适配器并平均其输出，解决了少样本领域适应中的超参数调优和模型鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 解决少样本领域适应中因缺乏大型验证数据集而难以调优超参数的问题，以及模型在分布偏移下的鲁棒性问题。

Method: 训练多个独立适配器，平均其输出以提高性能和鲁棒性，即使适配器使用不同超参数训练。

Result: 新模型性能更高，对分布偏移更鲁棒，且对关键超参数（如残差比率）更不敏感。

Conclusion: Soup-Adapter方法有效解决了少样本领域适应中的两大问题，并首次探索了CLIP适配器风格技术在DINOv2中的应用。

Abstract: In this paper, we tackle two fundamental problems in few-shot domain
adaptation of foundation models. First, hyperparameter tuning is often
impractical due to the lack of large validation datasets. Second, model
robustness under distribution shifts where test time data deviates slightly
from training distributions, remains a concern. We show that by training
multiple independent adapters and averaging their outputs, the new model has a
higher performance and is more robust to distribution shifts compared to any
individual adapter. This improvement holds even when the adapters are trained
with diverse hyperparameters sampled from a wide range, resulting in varied
individual performance. Consequently, our method addresses both of the problems
described above. The ensemble is also significantly less sensitive to the
residual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble
can be reparameterized to a single adapter again using a principled
concatenation of the parameters, we refer to our method as Soup-Adapter. This
is also the first study to explore CLIP adapter-style techniques for DINOv2 and
to directly compare them with CLIP in this setting.

</details>


### [193] [Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs](https://arxiv.org/abs/2507.05810)
*Sofiia Chorna,Kateryna Tarelkina,Eloïse Berthier,Gianni Franchi*

Main category: cs.LG

TL;DR: 提出了一种新的框架和交互工具BAGEL，用于全局分析深度学习模型的行为，揭示语义概念在模型内部的表示和传播。


<details>
  <summary>Details</summary>
Motivation: 传统基于概念的可解释性方法局限于局部解释，无法全局分析模型行为。本文旨在扩展这些方法，以更全面地理解模型的决策机制。

Method: 通过分析高层语义属性（概念）在模型内部的涌现、交互和传播，系统量化概念在不同层的表示，并开发可视化平台BAGEL展示这些关系。

Result: 揭示了模型内部的潜在电路和信息流，帮助识别虚假相关性，增强模型的可信度。

Conclusion: 该框架是模型无关且可扩展的，有助于深入理解深度学习模型在数据集偏差下的泛化行为。

Abstract: While concept-based interpretability methods have traditionally focused on
local explanations of neural network predictions, we propose a novel framework
and interactive tool that extends these methods into the domain of mechanistic
interpretability. Our approach enables a global dissection of model behavior by
analyzing how high-level semantic attributes (referred to as concepts) emerge,
interact, and propagate through internal model components. Unlike prior work
that isolates individual neurons or predictions, our framework systematically
quantifies how semantic concepts are represented across layers, revealing
latent circuits and information flow that underlie model decision-making. A key
innovation is our visualization platform that we named BAGEL (for Bias Analysis
with a Graph for global Explanation Layers), which presents these insights in a
structured knowledge graph, allowing users to explore concept-class
relationships, identify spurious correlations, and enhance model
trustworthiness. Our framework is model-agnostic, scalable, and contributes to
a deeper understanding of how deep learning models generalize (or fail to) in
the presence of dataset biases. The demonstration is available at
https://knowledge-graph-ui-4a7cb5.gitlab.io/.

</details>


### [194] [Fair Domain Generalization: An Information-Theoretic View](https://arxiv.org/abs/2507.05823)
*Tangzheng Lian,Guanyu Hu,Dimitrios Kollias,Xinyu Yang,Oya Celiktutan*

Main category: cs.LG

TL;DR: 该论文提出了FairDG问题，旨在在未见目标域中同时最小化预期风险和公平性违规，并通过信息论视角设计了PAFDG框架。


<details>
  <summary>Details</summary>
Motivation: 解决领域泛化（DG）和算法公平性之间的脱节问题，确保在未见域中同时实现性能和公平性。

Method: 基于互信息的上界分析，设计PAFDG框架，通过帕累托优化权衡效用与公平性。

Result: 在真实视觉和语言数据集上，PAFDG实现了优于现有方法的效用-公平性权衡。

Conclusion: PAFDG为领域泛化和公平性提供了统一的解决方案，并通过实验验证了其有效性。

Abstract: Domain generalization (DG) and algorithmic fairness are two critical
challenges in machine learning. However, most DG methods focus only on
minimizing expected risk in the unseen target domain without considering
algorithmic fairness. Conversely, fairness methods typically do not account for
domain shifts, so the fairness achieved during training may not generalize to
unseen test domains. In this work, we bridge these gaps by studying the problem
of Fair Domain Generalization (FairDG), which aims to minimize both expected
risk and fairness violations in unseen target domains. We derive novel mutual
information-based upper bounds for expected risk and fairness violations in
multi-class classification tasks with multi-group sensitive attributes. These
bounds provide key insights for algorithm design from an information-theoretic
perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal
Fairness for Domain Generalization), a practical framework that solves the
FairDG problem and models the utility-fairness trade-off through Pareto
optimization. Experiments on real-world vision and language datasets show that
PAFDG achieves superior utility-fairness trade-offs compared to existing
methods.

</details>


### [195] [Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning](https://arxiv.org/abs/2507.05852)
*Samuel Ofosu Mensah,Kerol Djoumessi,Philipp Berens*

Main category: cs.LG

TL;DR: 提出了一种联邦学习框架，通过原型和轻量级适配器解决通信开销和统计异质性问题，同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的通信开销和统计异质性挑战，同时增强模型的可解释性。

Method: 使用原型表示和轻量级适配器模块，替代完整模型权重的传输，实现局部模型与全局结构的对齐。

Result: 在视网膜眼底图像数据集上实验，分类任务准确率优于基线算法，并展示了可解释性。

Conclusion: 该框架有效减少了通信负载，解决了统计异质性，同时提供了模型的可解释性。

Abstract: Federated learning (FL) provides a promising paradigm for collaboratively
training machine learning models across distributed data sources while
maintaining privacy. Nevertheless, real-world FL often faces major challenges
including communication overhead during the transfer of large model parameters
and statistical heterogeneity, arising from non-identical independent data
distributions across clients. In this work, we propose an FL framework that 1)
provides inherent interpretations using prototypes, and 2) tackles statistical
heterogeneity by utilising lightweight adapter modules to act as compressed
surrogates of local models and guide clients to achieve generalisation despite
varying client distribution. Each client locally refines its model by aligning
class embeddings toward prototype representations and simultaneously adjust the
lightweight adapter. Our approach replaces the need to communicate entire model
weights with prototypes and lightweight adapters. This design ensures that each
client's model aligns with a globally shared structure while minimising
communication load and providing inherent interpretations. Moreover, we
conducted our experiments on a real-world retinal fundus image dataset, which
provides clinical-site information. We demonstrate inherent interpretable
capabilities and perform a classification task, which shows improvements in
accuracy over baseline algorithms.

</details>


### [196] [Universal Embeddings of Tabular Data](https://arxiv.org/abs/2507.05904)
*Astrid Franz,Frederik Hoppe,Marianne Michaelis,Udo Göbel*

Main category: cs.LG

TL;DR: 提出了一种任务无关的表格数据嵌入框架，通过图结构和图自编码器生成嵌入，支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 工业数据库中表格数据广泛存在，但应用任务多样且未预先定义，需要通用嵌入方法。

Method: 将表格数据转为图结构，利用图自编码器生成实体嵌入，再聚合为行嵌入。

Result: 在真实数据集上表现优于现有通用表格嵌入方法。

Conclusion: 该方法能高效处理未见样本，适用于多种下游任务。

Abstract: Tabular data in relational databases represents a significant portion of
industrial data. Hence, analyzing and interpreting tabular data is of utmost
importance. Application tasks on tabular data are manifold and are often not
specified when setting up an industrial database. To address this, we present a
novel framework for generating universal, i.e., task-independent embeddings of
tabular data for performing downstream tasks without predefined targets. Our
method transforms tabular data into a graph structure, leverages Graph
Auto-Encoders to create entity embeddings, which are subsequently aggregated to
obtain embeddings for each table row, i.e., each data sample. This two-step
approach has the advantage that unseen samples, consisting of similar entities,
can be embedded without additional training. Downstream tasks such as
regression, classification or outlier detection, can then be performed by
applying a distance-based similarity measure in the embedding space.
Experiments on real-world datasets demonstrate that our method achieves
superior performance compared to existing universal tabular data embedding
techniques.

</details>


### [197] [Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data](https://arxiv.org/abs/2507.05914)
*Rui Huang,Shitong Shao,Zikai Zhou,Pukun Zhao,Hangyu Guo,Tian Ye,Lichen Bai,Shuo Yang,Zeke Xie*

Main category: cs.LG

TL;DR: 论文提出了一种名为D2C的新框架，用于扩散模型的数据集压缩，显著减少训练所需的数据量和时间，同时保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型训练资源密集，需要大量数据和计算时间，研究旨在通过数据集压缩降低训练成本。

Method: D2C框架分为两阶段：Select阶段通过扩散难度评分和间隔采样选择紧凑多样的子集；Attach阶段增强子集的语义和视觉表示。

Result: 实验显示D2C在多种条件下显著加速训练（如100倍速度提升），仅用0.8%数据达到FID 4.3。

Conclusion: D2C框架为扩散模型提供了一种高效的数据集压缩方法，大幅降低训练成本且不牺牲生成质量。

Abstract: Diffusion models have achieved remarkable success in various generative
tasks, but training them remains highly resource-intensive, often requiring
millions of images and many days of GPU computation. From a data-centric
perspective addressing this limitation, we study diffusion dataset condensation
as a new and challenging problem setting. The goal is to construct a
"synthetic" sub-dataset with significantly fewer samples than the original
dataset, enabling high-quality diffusion model training with greatly reduced
cost. To the best of our knowledge, we are the first to formally investigate
dataset condensation for diffusion models, whereas prior work focused on
training discriminative models. To tackle this new challenge, we propose a
novel Diffusion Dataset Condensation (D2C) framework, which consists of two
phases: Select and Attach. The Select phase identifies a compact and diverse
subset using a diffusion difficulty score and interval sampling. The Attach
phase enhances the selected subset by attaching rich semantic and visual
representations to strengthen the conditional signals. Extensive experiments
across various dataset sizes, model architectures, and resolutions show that
our D2C framework enables significantly faster diffusion model training with
dramatically fewer data, while preserving high visual quality. Notably, for the
SiT-XL/2 architecture, D2C achieves a 100x training speed-up, reaching a FID
score of 4.3 in just 40k steps using only 0.8% of the training data.

</details>


### [198] [Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling](https://arxiv.org/abs/2507.05950)
*Pinar Bisgin,Tom Strube,Niklas Tschorn,Michael Pantförder,Maximilian Fecke,Ingrid Ljungvall,Jens Häggström,Gerhard Wess,Christoph Schummer,Sven Meister,Falk M. Howar*

Main category: cs.LG

TL;DR: 研究探讨了兽医医学中标签噪声对AI模型训练的负面影响，并通过整合多位专家意见减少噪声，显著提升了犬类心脏杂音分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 兽医医学中标签噪声对AI模型性能的负面影响，尤其是犬类心脏听诊数据的分类问题。

Method: 通过多位专家标注140个心脏声音记录（HSR），筛选出70个高质量数据，并利用个体心周期扩展训练数据，评估了AdaBoost、XGBoost和Random Forest三种算法。

Result: XGBoost表现最佳，分类准确性显著提升，尤其是对轻度、中度和响亮心脏杂音的检测敏感性和特异性均有大幅提高。

Conclusion: 减少标签噪声对提升犬类心脏杂音分类算法性能至关重要，XGBoost在此任务中表现突出。

Abstract: Noisy labels pose significant challenges for AI model training in veterinary
medicine. This study examines expert assessment ambiguity in canine
auscultation data, highlights the negative impact of label noise on
classification performance, and introduces methods for label noise reduction.
To evaluate whether label noise can be minimized by incorporating multiple
expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated
regarding the intensity of holosystolic heart murmurs caused by Myxomatous
Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of
70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging
individual heart cycles, the training data was expanded and classification
robustness was enhanced. The investigation encompassed training and evaluating
three classification algorithms: AdaBoost, XGBoost, and Random Forest. While
AdaBoost and Random Forest exhibited reasonable performances, XGBoost
demonstrated notable improvements in classification accuracy. All algorithms
showed significant improvements in classification accuracy due to the applied
label noise reduction, most notably XGBoost. Specifically, for the detection of
mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity
from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23%
to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling
category, sensitivity and specificity increased from 58.28% to 95.09% and from
84.84% to 89.69%, respectively. These results highlight the importance of
minimizing label noise to improve classification algorithms for the detection
of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart
sound classification, label noise reduction, machine learning, XGBoost,
veterinary cardiology, MMVD.

</details>


### [199] [Simple Convergence Proof of Adam From a Sign-like Descent Perspective](https://arxiv.org/abs/2507.05966)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 论文提出了一种新的视角，将Adam优化器视为符号类优化器，简化了收敛性分析，并证明了其在更弱假设下的最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 尽管Adam在深度神经网络训练中表现出色，但其理论收敛性分析仍不完善，现有方法需要强假设和复杂技巧。

Method: 将Adam重新表述为符号类优化器，简化了收敛性分析，并在广义p-仿射方差和(L0, L1, q)-平滑性等弱假设下进行证明。

Result: 证明了Adam在更弱假设下达到最优收敛速率O(1/T^(1/4))，并揭示了动量在确保收敛中的关键作用。

Conclusion: 新视角简化了理论分析，为Adam的学习率调优提供了实用指导，缩小了理论与实践之间的差距。

Abstract: Adam is widely recognized as one of the most effective optimizers for
training deep neural networks (DNNs). Despite its remarkable empirical success,
its theoretical convergence analysis remains unsatisfactory. Existing works
predominantly interpret Adam as a preconditioned stochastic gradient descent
with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t -
\frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective
necessitates strong assumptions and intricate techniques, resulting in lengthy
and opaque convergence proofs that are difficult to verify and extend. In
contrast, we propose a novel interpretation by treating Adam as a sign-like
optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t
\frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This
reformulation significantly simplifies the convergence analysis. For the first
time, with some mild conditions, we prove that Adam achieves the optimal rate
of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O}
\left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the
generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without
dependence on the model dimensionality or the numerical stability parameter
$\epsilon$. Additionally, our theoretical analysis provides new insights into
the role of momentum as a key factor ensuring convergence and offers practical
guidelines for tuning learning rates in Adam, further bridging the gap between
theory and practice.

</details>


### [200] [KnowIt: Deep Time Series Modeling and Interpretation](https://arxiv.org/abs/2507.06009)
*M. W. Theunissen,R. Rabe,M. H. Davel*

Main category: cs.LG

TL;DR: KnowIt是一个灵活的框架，用于构建和解释深度时间序列模型，提供Python工具包，支持自定义数据集、架构和可解释性技术。


<details>
  <summary>Details</summary>
Motivation: 为复杂时间序列数据提供知识发现环境，支持用户构建强大的深度学习模型并解释其行为，推动这一未充分探索领域的发展。

Method: 通过定义良好的接口解耦数据集、神经网络架构和可解释性技术，支持动态建模和解释。

Result: KnowIt实现了灵活的时间序列建模和解释，支持用户自定义需求。

Conclusion: KnowIt旨在成为深度时间序列建模的可信工具，通过持续开发和协作推动领域进步。

Abstract: KnowIt (Knowledge discovery in time series data) is a flexible framework for
building deep time series models and interpreting them. It is implemented as a
Python toolkit, with source code and documentation available from
https://must-deep-learning.github.io/KnowIt. It imposes minimal assumptions
about task specifications and decouples the definition of dataset, deep neural
network architecture, and interpretability technique through well defined
interfaces. This ensures the ease of importing new datasets, custom
architectures, and the definition of different interpretability paradigms while
maintaining on-the-fly modeling and interpretation of different aspects of a
user's own time series data. KnowIt aims to provide an environment where users
can perform knowledge discovery on their own complex time series data through
building powerful deep learning models and explaining their behavior. With
ongoing development, collaboration and application our goal is to make this a
platform to progress this underexplored field and produce a trusted tool for
deep time series modeling.

</details>


### [201] [Kamae: Bridging Spark and Keras for Seamless ML Preprocessing](https://arxiv.org/abs/2507.06021)
*George Barrowclough,Marian Andrecki,James Shinner,Daniele Donghi*

Main category: cs.LG

TL;DR: Kamae是一个开源Python库，将PySpark预处理管道转换为等效的Keras模型，确保训练和推理环境中的特征预处理一致性。


<details>
  <summary>Details</summary>
Motivation: 解决生产推荐系统中特征预处理在训练和推理环境中的逻辑重复问题，减少工程负担和数据集偏移风险。

Method: 提供可配置的Spark转换器和估计器，每个映射到对应的Keras层，实现端到端预处理一致性。

Result: 在MovieLens数据集和Expedia的Learning-to-Rank管道等实际用例中验证了框架的实用性。

Conclusion: Kamae通过统一预处理逻辑，显著提升了推荐系统的开发效率和一致性。

Abstract: In production recommender systems, feature preprocessing must be faithfully
replicated across training and inference environments. This often requires
duplicating logic between offline and online environments, increasing
engineering effort and introducing risks of dataset shift. We present Kamae, an
open-source Python library that bridges this gap by translating PySpark
preprocessing pipelines into equivalent Keras models. Kamae provides a suite of
configurable Spark transformers and estimators, each mapped to a corresponding
Keras layer, enabling consistent, end-to-end preprocessing across the ML
lifecycle. Framework's utility is illustrated on real-world use cases,
including MovieLens dataset and Expedia's Learning-to-Rank pipelines. The code
is available at https://github.com/ExpediaGroup/kamae.

</details>


### [202] [Multi-view mid fusion: a universal approach for learning in an HDLSS setting](https://arxiv.org/abs/2507.06026)
*Lynn Houthuys*

Main category: cs.LG

TL;DR: 本文提出了一种通用的多视图中融合方法，用于高维低样本量（HDLSS）环境下的学习，通过将高维特征向量拆分为多个子集作为不同视图，验证了方法的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 高维低样本量（HDLSS）环境在许多应用中带来挑战，本文旨在解决这一问题。

Method: 提出三种视图构建方法，将高维特征向量拆分为子集作为不同视图，并采用多视图中融合技术。

Result: 实验验证了该方法在不同模型和学习任务中的有效性和泛化能力。

Conclusion: 本文为多视图中融合学习的通用优势研究奠定了基础。

Abstract: The high-dimensional low-sample-size (HDLSS) setting presents significant
challenges in various applications where the feature dimension far exceeds the
number of available samples. This paper introduces a universal approach for
learning in HDLSS setting using multi-view mid fusion techniques. It shows how
existing mid fusion multi-view methods perform well in an HDLSS setting even if
no inherent views are provided. Three view construction methods are proposed
that split the high-dimensional feature vectors into smaller subsets, each
representing a different view. Extensive experimental validation across
model-types and learning tasks confirm the effectiveness and generalization of
the approach. We believe the work in this paper lays the foundation for further
research into the universal benefits of multi-view mid fusion learning.

</details>


### [203] [EdgeCodec: Onboard Lightweight High Fidelity Neural Compressor with Residual Vector Quantization](https://arxiv.org/abs/2507.06040)
*Benjamin Hodo,Tommaso Polonelli,Amirhossein Moallemi,Luca Benini,Michele Magno*

Main category: cs.LG

TL;DR: EdgeCodec是一种用于风力涡轮机叶片气压数据的端到端神经压缩器，通过非对称自编码器和残差向量量化器实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 解决风力涡轮机叶片气压数据传输中的高能耗问题，延长传感器寿命。

Method: 采用非对称自编码器架构，结合判别器和残差向量量化器进行训练。

Result: 压缩比达2'560:1至10'240:1，重建误差低于3%，实时运行于GAP9微控制器，无线传输能耗降低2.9倍。

Conclusion: EdgeCodec在高效压缩和能耗优化方面表现优异，适用于实时传感器网络。

Abstract: We present EdgeCodec, an end-to-end neural compressor for barometric data
collected from wind turbine blades. EdgeCodec leverages a heavily asymmetric
autoencoder architecture, trained with a discriminator and enhanced by a
Residual Vector Quantizer to maximize compression efficiency. It achieves
compression rates between 2'560:1 and 10'240:1 while maintaining a
reconstruction error below 3%, and operates in real time on the GAP9
microcontroller with bitrates ranging from 11.25 to 45 bits per second.
Bitrates can be selected on a sample-by-sample basis, enabling on-the-fly
adaptation to varying network conditions. In its highest compression mode,
EdgeCodec reduces the energy consumption of wireless data transmission by up to
2.9x, significantly extending the operational lifetime of deployed sensor
units.

</details>


### [204] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 论文提出了一种结合局部和全局卷积神经网络（LGCNN）的方法，用于解决地下水流动和热传输问题，克服了传统数值模拟的高成本和数据驱动模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在科学和工程应用中常因训练数据有限或质量低而表现不佳，尤其是地下水流动和热传输这类复杂问题。

Method: 采用局部-全局卷积神经网络（LGCNN），结合轻量级数值替代模型（全局）和卷积神经网络（局部），分别处理传输过程、地下水速度和热扩散过程。

Result: LGCNN成功模拟了城市范围内的地下温度场，并在真实数据上展示了良好的泛化能力，无需重新训练即可扩展到更大区域。

Conclusion: LGCNN为解决复杂物理过程的建模提供了一种高效且可扩展的方法，代码和数据已公开以确保可重复性。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [205] [QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models](https://arxiv.org/abs/2507.06079)
*Sebastian Siegel,Ming-Jay Yang,Younes Bouhadjar,Maxime Fabre,Emre Neftci,John Paul Strachan*

Main category: cs.LG

TL;DR: 结构化状态空间模型（SSM）在长序列处理中表现优异，量化感知训练（QAT）可显著降低其复杂度，并增强对模拟噪声的鲁棒性，适合边缘计算设备部署。


<details>
  <summary>Details</summary>
Motivation: 探索量化感知训练（QAT）对SSM在边缘硬件（如模拟内存计算芯片）上的影响，以提升资源受限设备的性能。

Method: 采用QAT技术，分析模型大小与数值精度的关系，结合结构剪枝，并在模拟内存计算芯片上部署SSM。

Result: QAT将SSM复杂度降低两个数量级，增强了对模拟噪声的鲁棒性，并实现了结构剪枝。

Conclusion: QAT显著优化了SSM在边缘计算设备上的部署效果，提升了计算效率。

Abstract: Structured State Space models (SSM) have recently emerged as a new class of
deep learning models, particularly well-suited for processing long sequences.
Their constant memory footprint, in contrast to the linearly scaling memory
demands of Transformers, makes them attractive candidates for deployment on
resource-constrained edge-computing devices. While recent works have explored
the effect of quantization-aware training (QAT) on SSMs, they typically do not
address its implications for specialized edge hardware, for example, analog
in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can
significantly reduce the complexity of SSMs by up to two orders of magnitude
across various performance metrics. We analyze the relation between model size
and numerical precision, and show that QAT enhances robustness to analog noise
and enables structural pruning. Finally, we integrate these techniques to
deploy SSMs on a memristive analog in-memory computing substrate and highlight
the resulting benefits in terms of computational efficiency.

</details>


### [206] [CoRE: Enhancing Metacognition with Label-free Self-evaluation in LRMs](https://arxiv.org/abs/2507.06087)
*Haoxi Li,Sikai Bai,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 论文提出CoRE方法，通过分析推理轨迹的几何特性，实现无标签自评估，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）存在过度思考现象，导致推理效率低下，需要一种无外部标签的自评估方法。

Method: 提出Chain-of-Reasoning Embedding（CoRE）和CoRE-Eval框架，利用隐藏状态分析推理轨迹的几何特性，动态终止冗余推理。

Result: 在数学推理基准测试中，CoRE-Eval减少推理步骤13.7%-33.2%，准确率提升约10%，32B模型在AIME基准上达到70.0%准确率。

Conclusion: CoRE方法有效提升LRMs的推理效率和准确性，为无标签自评估提供了新思路。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
domains like mathematics and program synthesis. Despite their strong
performance, LRMs often exhibit overthinking -- excessive and redundant
reasoning steps that introduce inefficiencies during inference. This phenomenon
raises an important question for LRM self-evaluation: How can a model
autonomously assess the correctness of its own reasoning trajectory without
external labels? To address this, we propose Chain-of-Reasoning Embedding
(CoRE), a series of hidden states in latent space to enable label-free
self-evaluation on intermediate reasoning steps of LRMs, so as to enhance
metacognition abilities for improved reasoning efficiency. By analyzing the
geometric properties of the CoRE trajectories, we reveal that redundant
reasoning usually presents cyclical fluctuations, which correspond to
repetitive and unconscious reflection/exploration. Leveraging this insight, we
further introduce a training-free, label-free self-evaluation framework,
CoRE-Eval, to detect such patterns and dynamically determine whether to
terminate reasoning early. Extensive experiments on mathematical reasoning
benchmarks (GSM8K, MATH-500, and AIME) and across model sizes from 7B to 32B
demonstrate that CoRE-Eval reduces chain-of-thought length by 13.7% to 33.2%
while improving answer accuracy by around 10%, achieving 70.0% accuracy on the
challenging AIME benchmark with the 32B model.

</details>


### [207] [Subspace-based Approximate Hessian Method for Zeroth-Order Optimization](https://arxiv.org/abs/2507.06125)
*Dongyoon Kim,Sungjae Lee,Wonjin Lee,Kwang In Kim*

Main category: cs.LG

TL;DR: 提出了一种基于子空间的近似Hessian方法（ZO-SAH），通过随机选择二维子空间估计Hessian矩阵，显著降低了函数评估成本，并在多个基准数据集上表现优于现有零阶优化方法。


<details>
  <summary>Details</summary>
Motivation: 零阶优化中梯度信息难以获取或计算成本高，现有方法多依赖一阶近似，而引入二阶信息可加速收敛但成本过高。

Method: ZO-SAH通过随机选择二维子空间拟合二次多项式估计Hessian，并采用周期性子空间切换策略复用函数评估。

Result: 在八个基准数据集（包括逻辑回归和深度神经网络训练任务）上，ZO-SAH的收敛速度显著快于现有零阶方法。

Conclusion: ZO-SAH通过有效利用二阶信息和降低函数评估成本，为零阶优化提供了一种高效解决方案。

Abstract: Zeroth-order optimization addresses problems where gradient information is
inaccessible or impractical to compute. While most existing methods rely on
first-order approximations, incorporating second-order (curvature) information
can, in principle, significantly accelerate convergence. However, the high cost
of function evaluations required to estimate Hessian matrices often limits
practical applicability. We present the subspace-based approximate Hessian
(ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these
costs by focusing on randomly selected two-dimensional subspaces. Within each
subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the
objective function and extracting its second-order coefficients. To further
reduce function-query costs, ZO-SAH employs a periodic subspace-switching
strategy that reuses function evaluations across optimization steps.
Experiments on eight benchmark datasets, including logistic regression and deep
neural network training tasks, demonstrate that ZO-SAH achieves significantly
faster convergence than existing zeroth-order methods.

</details>


### [208] [Topic Modeling and Link-Prediction for Material Property Discovery](https://arxiv.org/abs/2507.06139)
*Ryan C. Barron,Maksim E. Eren,Valentin Stanev,Cynthia Matuszek,Boian S. Alexandrov*

Main category: cs.LG

TL;DR: 论文提出了一种基于层次非负矩阵分解和布尔矩阵分解的AI驱动框架，用于预测科学文献网络中的隐藏关联，并验证了其在材料科学领域的有效性。


<details>
  <summary>Details</summary>
Motivation: 科学文献网络和知识图谱通常庞大、稀疏且噪声多，存在缺失链接的问题，需要一种方法推断隐藏关联以推动跨学科探索。

Method: 结合层次非负矩阵分解（HNMFk）、布尔矩阵分解（BNMFk）和逻辑矩阵分解（LMF），构建三级主题树，并通过集成方法融合离散可解释性和概率评分。

Result: 模型成功预测了缺失链接，例如在超导体中移除相关文献后仍能关联超导TMD簇，展示了其发现隐藏连接的能力。

Conclusion: 该方法在复杂材料领域中有效发现隐藏关联，并通过交互式工具支持科学发现，适用于跨学科研究。

Abstract: Link prediction infers missing or future relations between graph nodes, based
on connection patterns. Scientific literature networks and knowledge graphs are
typically large, sparse, and noisy, and often contain missing links between
entities. We present an AI-driven hierarchical link prediction framework that
integrates matrix factorization to infer hidden associations and steer
discovery in complex material domains. Our method combines Hierarchical
Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization
(BNMFk) with automatic model selection, as well as Logistic matrix
factorization (LMF), we use to construct a three-level topic tree from a
46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs).
These materials are studied in a variety of physics fields with many current
and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with
probabilistic scoring. The resulting HNMFk clusters map each material onto
coherent topics like superconductivity, energy storage, and tribology. Also,
missing or weakly connected links are highlight between topics and materials,
suggesting novel hypotheses for cross-disciplinary exploration. We validate our
method by removing publications about superconductivity in well-known
superconductors, and show the model predicts associations with the
superconducting TMD clusters. This shows the method finds hidden connections in
a graph of material to latent topic associations built from scientific
literature, especially useful when examining a diverse corpus of scientific
documents covering the same class of phenomena or materials but originating
from distinct communities and perspectives. The inferred links generating new
hypotheses, produced by our method, are exposed through an interactive
Streamlit dashboard, designed for human-in-the-loop scientific discovery.

</details>


### [209] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: 论文分析了卷积层中步长引入的混叠现象，提出了基于框架理论的方法来评估稳定性，并推导了优化目标以减少混叠。


<details>
  <summary>Details</summary>
Motivation: 研究卷积层中步长导致的混叠现象及其对数值稳定性和统计泛化的影响。

Method: 采用框架理论分析1D卷积核的混叠现象，推导稳定性边界和Parseval稳定性特征，并提出优化目标抑制混叠。

Result: 提出了两种高效优化目标以减少混叠，并推导了随机核卷积层的混叠效应期望和方差的闭式表达式。

Conclusion: 通过框架理论分析混叠现象，为卷积层的稳定性提供了理论支持，并提出了实用的优化方法。

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


### [210] [A Method for Optimizing Connections in Differentiable Logic Gate Networks](https://arxiv.org/abs/2507.06173)
*Wout Mommen,Lars Keuninckx,Matthias Hartmann,Piet Wambacq*

Main category: cs.LG

TL;DR: 提出了一种新型方法，用于优化深度可微分逻辑门网络（LGNs）中的部分连接，通过概率分布选择最优连接，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何优化逻辑门网络的连接方式，以提高性能并减少所需逻辑门数量。

Method: 使用概率分布对每个门输入的连接子集进行优化，选择最优连接后确定门类型。

Result: 在Yin-Yang、MNIST和Fashion-MNIST基准测试中，优化后的LGNs表现优于固定连接的LGNs，且逻辑门数量大幅减少。

Conclusion: 该方法为实现完全可训练的布尔逻辑提供了一条有效途径。

Abstract: We introduce a novel method for partial optimization of the connections in
Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a
probability distribution over a subset of connections per gate input, selecting
the connection with highest merit, after which the gate-types are selected. We
show that the connection-optimized LGNs outperform standard fixed-connection
LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only
a fraction of the number of logic gates. When training all connections, we
demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on
the MNIST data set. Additionally, we show that our network has 24 times fewer
gates, while performing better on the MNIST data set compared to standard fully
connected LGNs. As such, our work shows a pathway towards fully trainable
Boolean logic.

</details>


### [211] [Differential Mamba](https://arxiv.org/abs/2507.06204)
*Nadav Schneider,Itamar Zimerman,Eliya Nachmani*

Main category: cs.LG

TL;DR: 论文探讨了将Transformer的差分设计技术应用于Mamba架构的可行性，提出了一种新的差分机制，提升了Mamba的性能和检索能力。


<details>
  <summary>Details</summary>
Motivation: 序列模型（如Transformer和RNN）常过度关注无关上下文，导致噪声中间表示，影响模型能力。研究旨在解决Mamba架构中的这一问题。

Method: 提出了一种新的差分机制，并进行了架构修改，通过语言建模基准验证其有效性。

Result: 新方法在Mamba上表现优于原始版本，提升了检索能力和性能。

Conclusion: 研究表明，差分设计可有效缓解Mamba模型的过度关注问题，代码已公开。

Abstract: Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

</details>


### [212] [Modern Methods in Associative Memory](https://arxiv.org/abs/2507.06211)
*Dmitry Krotov,Benjamin Hoover,Parikshit Ram,Bao Pham*

Main category: cs.LG

TL;DR: 本文介绍了关联记忆（如Hopfield网络）及其在现代AI架构中的应用，包括理论进展与实践教程。


<details>
  <summary>Details</summary>
Motivation: 关联记忆模型因其信息存储能力和与先进AI架构（如Transformers和扩散模型）的联系而受到关注，为理解传统AI网络提供了新视角。

Method: 通过拉格朗日公式设计分布式模型，并结合数学推导和编程实践进行教学。

Result: 关联记忆的理论与实践结合，推动了新型架构的设计和有用表示的学习。

Conclusion: 关联记忆为AI研究提供了新的理论工具和实践方法，未来潜力巨大。

Abstract: Associative Memories like the famous Hopfield Networks are elegant models for
describing fully recurrent neural networks whose fundamental job is to store
and retrieve information. In the past few years they experienced a surge of
interest due to novel theoretical results pertaining to their information
storage capabilities, and their relationship with SOTA AI architectures, such
as Transformers and Diffusion Models. These connections open up possibilities
for interpreting the computation of traditional AI networks through the
theoretical lens of Associative Memories. Additionally, novel Lagrangian
formulations of these networks make it possible to design powerful distributed
models that learn useful representations and inform the design of novel
architectures. This tutorial provides an approachable introduction to
Associative Memories, emphasizing the modern language and methods used in this
area of research, with practical hands-on mathematical derivations and coding
notebooks.

</details>


### [213] [Deep Learning Optimization of Two-State Pinching Antennas Systems](https://arxiv.org/abs/2507.06222)
*Odysseas G. Karagiannidis,Victoria E. Galanopoulou,Panagiotis D. Diamantoulakis,Zhiguo Ding,Octavia Dobre*

Main category: cs.LG

TL;DR: 论文研究了在波导中优化选择固定位置的可控天线子集以最大化用户终端通信速率的问题，提出了一种基于神经网络的解决方案，并考虑了用户位置不确定性。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统的发展需要灵活、高效且经济的天线技术，可控天线因其动态控制电磁波传播的能力成为潜在解决方案。

Method: 将问题建模为组合分数0-1二次规划，利用不同复杂度的神经网络架构从数据中学习激活策略，并整合用户位置不确定性。

Result: 仿真结果验证了所提模型的有效性和鲁棒性。

Conclusion: 神经网络方法能有效解决可控天线激活优化问题，适用于实际部署场景。

Abstract: The evolution of wireless communication systems requires flexible,
energy-efficient, and cost-effective antenna technologies. Pinching antennas
(PAs), which can dynamically control electromagnetic wave propagation through
binary activation states, have recently emerged as a promising candidate. In
this work, we investigate the problem of optimally selecting a subset of
fixed-position PAs to activate in a waveguide, when the aim is to maximize the
communication rate at a user terminal. Due to the complex interplay between
antenna activation, waveguide-induced phase shifts, and power division, this
problem is formulated as a combinatorial fractional 0-1 quadratic program. To
efficiently solve this challenging problem, we use neural network architectures
of varying complexity to learn activation policies directly from data,
leveraging spatial features and signal structure. Furthermore, we incorporate
user location uncertainty into our training and evaluation pipeline to simulate
realistic deployment conditions. Simulation results demonstrate the
effectiveness and robustness of the proposed models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [214] [Adaptive Linearly Constrained Minimum Variance Volumetric Active Noise Control](https://arxiv.org/abs/2507.05657)
*Manan Mittal,Ryan M. Corey,Andrew C. Singer*

Main category: cs.SD

TL;DR: 本文提出了一种基于时间域的线性约束最小方差主动噪声控制（LCMV ANC）方法，通过定义线性约束实现空间选择性噪声控制，优于传统的多点误差最小化方法。


<details>
  <summary>Details</summary>
Motivation: 传统体积噪声控制方法通过多点误差最小化抑制区域内的声能，但空间响应灵活性不足。本文旨在提供一种更灵活的空间噪声控制方法。

Method: 采用时间域线性约束最小方差优化框架，结合滤波-X最小均方（FxLMS）自适应算法在线更新滤波器系数。

Result: 仿真和实验结果验证了该方法在噪声降低和约束遵循方面的有效性，实现了空间选择性和宽带噪声控制。

Conclusion: LCMV ANC方法在空间噪声控制中表现出更高的灵活性和有效性，优于传统多点方法。

Abstract: Traditional volumetric noise control typically relies on multipoint error
minimization to suppress sound energy across a region, but offers limited
flexibility in shaping spatial responses. This paper introduces a time-domain
formulation for linearly constrained minimum variance active noise control
(LCMV ANC) for spatial control filter design. We demonstrate how the LCMV ANC
optimization framework allows system designers to prioritize noise reduction at
specific spatial locations through strategically defined linear constraints,
providing a more flexible alternative to uniformly weighted multipoint error
minimization. An adaptive algorithm based on filtered-X least mean squares
(FxLMS) is derived for online adaptation of filter coefficients. Simulation and
experimental results validate the proposed method's noise reduction and
constraint adherence, demonstrating effective, spatially selective, and
broadband noise control compared to multipoint volumetric noise control.

</details>


### [215] [Beamforming with Random Projections: Upper and Lower Bounds](https://arxiv.org/abs/2507.05662)
*Manan Mittal,Ryan M. Corey,Andrew C. Singer*

Main category: cs.SD

TL;DR: 论文提出了一种基于多随机投影的数据驱动降维和波束成形方法，通过混合波束成形器在SNR和SINR增益上优于传统MVDR波束成形器，同时引入计算复杂度作为设计权衡。


<details>
  <summary>Details</summary>
Motivation: 分布式麦克风阵列中，不同阵列对声源的幅度和相位差异显著，传统波束成形器在噪声增益和干扰抑制之间存在权衡。

Method: 采用多随机投影作为预处理方案，结合数据驱动降维和波束成形，设计混合波束成形器。

Result: 混合波束成形器在SNR和SINR增益上优于MVDR波束成形器，且计算复杂度更低。

Conclusion: 该方法通过引入计算复杂度作为设计权衡，更好地利用信号结构，提升实时性能，并推导了压缩波束成形器输出功率的上下界。

Abstract: Beamformers often trade off white noise gain against the ability to suppress
interferers. With distributed microphone arrays, this trade-off becomes crucial
as different arrays capture vastly different magnitude and phase differences
for each source. We propose the use of multiple random projections as a
first-stage preprocessing scheme in a data-driven approach to dimensionality
reduction and beamforming. We show that a mixture beamformer derived from the
use of multiple such random projections can effectively outperform the minimum
variance distortionless response (MVDR) beamformer in terms of signal-to-noise
ratio (SNR) and signal-to-interferer-and-noise ratio (SINR) gain. Moreover, our
method introduces computational complexity as a trade-off in the design of
adaptive beamformers, alongside noise gain and interferer suppression. This
added degree of freedom allows the algorithm to better exploit the inherent
structure of the received signal and achieve better real-time performance while
requiring fewer computations. Finally, we derive upper and lower bounds for the
output power of the compressed beamformer when compared to the full complexity
MVDR beamformer.

</details>


### [216] [Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners](https://arxiv.org/abs/2507.05729)
*Katsuhiko Yamamoto,Koichi Miyazaki*

Main category: cs.SD

TL;DR: 论文提出了一种基于Mamba的语音清晰度预测（SIP）模型，替代了传统的Transformer模型，以解决计算和内存成本高的问题，同时保持了竞争性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于Transformer的SIP模型在计算和内存成本上较高，可能影响低延迟和高效能设备的性能，且对双耳信号的时间处理能力有限。

Method: 采用基于Mamba的SIP模型，替代Transformer中的自注意力机制，以优化计算效率和内存使用。

Result: 实验结果表明，基于Mamba的SIP模型在保持较少参数的同时，性能与基线模型相当，并能有效捕捉双耳信号的上下文和空间信息。

Conclusion: Mamba-based SIP模型在计算效率和性能上具有优势，适合用于低延迟和高效能设备。

Abstract: Speech intelligibility prediction (SIP) models have been used as objective
metrics to assess intelligibility for hearing-impaired (HI) listeners. In the
Clarity Prediction Challenge 2 (CPC2), non-intrusive binaural SIP models based
on transformers showed high prediction accuracy. However, the self-attention
mechanism theoretically incurs high computational and memory costs, making it a
bottleneck for low-latency, power-efficient devices. This may also degrade the
temporal processing of binaural SIPs. Therefore, we propose Mamba-based SIP
models instead of transformers for the temporal processing blocks. Experimental
results show that our proposed SIP model achieves competitive performance
compared to the baseline while maintaining a relatively small number of
parameters. Our analysis suggests that the SIP model based on bidirectional
Mamba effectively captures contextual and spatial speech information from
binaural signals.

</details>


### [217] [Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning](https://arxiv.org/abs/2507.05900)
*Zengjing Chen,Lu Wang,Chengzhi Xing*

Main category: cs.SD

TL;DR: 本文研究了水下声学网络中稳定的声学中继分配问题，提出了两种稳定分配目标，并引入激光混沌多处理学习（LC-ML）方法以实现高效稳定。


<details>
  <summary>Details</summary>
Motivation: 解决水下声学网络中稳定中继分配问题，探索不同于现有文献的两种稳定分配目标。

Method: 采用激光混沌多处理学习（LC-ML）方法，利用激光混沌生成的随机数学习中继分配。

Result: 激光混沌随机数和多处理在交换过程中对高吞吐量和环境适应性有积极作用；模糊认知比精确认知更稳定。

Conclusion: 提供了一种实用方法，为复杂水下环境中的中继选择奠定了基础。

Abstract: This study addresses the problem of stable acoustic relay assignment in an
underwater acoustic network. Unlike the objectives of most existing literature,
two distinct objectives, namely classical stable arrangement and ambiguous
stable arrangement, are considered. To achieve these stable arrangements, a
laser chaos-based multi-processing learning (LC-ML) method is introduced to
efficiently obtain high throughput and rapidly attain stability. In order to
sufficiently explore the relay's decision-making, this method uses random
numbers generated by laser chaos to learn the assignment of relays to multiple
source nodes. This study finds that the laser chaos-based random number and
multi-processing in the exchange process have a positive effect on higher
throughput and strong adaptability with environmental changing over time.
Meanwhile, ambiguous cognitions result in the stable configuration with less
volatility compared to accurate ones. This provides a practical and useful
method and can be the basis for relay selection in complex underwater
environments.

</details>


### [218] [Differentiable Reward Optimization for LLM based TTS system](https://arxiv.org/abs/2507.05911)
*Changfeng Gao,Zhihao Du,Shiliang Zhang*

Main category: cs.SD

TL;DR: DiffRO方法通过直接计算神经编解码器令牌的奖励，优化TTS系统性能，结合Gumbel-Softmax技术和多任务奖励模型，显著提升发音准确性和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于人类反馈的强化学习（RLHF）在TTS中依赖合成音频计算奖励，DiffRO旨在通过直接利用神经编解码器令牌简化流程并提升性能。

Method: DiffRO直接计算神经编解码器令牌的奖励，使用Gumbel-Softmax技术使奖励函数可微分，并引入多任务奖励模型提供多角度反馈。

Result: DiffRO显著提升TTS系统的发音准确性，在seed-tts-eval基准上达到SOTA WER结果，并能零样本控制情感和质量属性。

Conclusion: DiffRO通过直接优化神经编解码器令牌奖励，结合多任务奖励模型，为TTS系统提供了高效且灵活的优化方案。

Abstract: This paper proposes a novel Differentiable Reward Optimization (DiffRO)
method aimed at enhancing the performance of neural codec language models based
text-to-speech (TTS) systems. In contrast to conventional reinforcement
learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly
compute the rewards based on neural codec tokens, rather than relying on
synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to
render the reward function differentiable, thereby streamlining the RLHF
training process. Additionally, we introduce a multi-task reward (MTR) model
which can provide feedback from different perspectives and find that it can
augment the system's capability to follow instructions effectively.Experimental
results indicate that DiffRO significantly improves the pronunciation accuracy
of the TTS system, achieving state-of-the-art (SOTA) WER results on the
seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we
demonstrate the ability to control emotional and quality attributes in a
zero-shot manner.

</details>


### [219] [Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol](https://arxiv.org/abs/2507.06070)
*Christos Nikou,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: 论文提出了一种新的评估协议，模拟真实环境下的音频识别，发现现有CNN模型性能下降，并通过改进增强管道和引入Transformer模型提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度神经网络的音频指纹方法在受控条件下表现良好，但在真实噪声环境中性能显著下降，需要更贴近实际的评估和改进方法。

Method: 提出新评估协议，生成不同噪声水平的录音；改进增强管道（低通和高通滤波器）；开发基于Transformer的模型，结合语义相关领域的知识迁移。

Result: Transformer模型在所有噪声水平和查询时长下优于CNN模型，低噪声下1秒查询准确率47.99%，10秒查询97%；高噪声下15秒查询检测率56.5%。

Conclusion: 新评估协议揭示了现有方法的局限性，改进的增强管道和Transformer模型显著提升了真实环境中的音频识别性能。

Abstract: Recent advances in song identification leverage deep neural networks to learn
compact audio fingerprints directly from raw waveforms. While these methods
perform well under controlled conditions, their accuracy drops significantly in
real-world scenarios where the audio is captured via mobile devices in noisy
environments. In this paper, we introduce a novel evaluation protocol designed
to better reflect such real-world conditions. We generate three recordings of
the same audio, each with increasing levels of noise, captured using a mobile
device's microphone. Our results reveal a substantial performance drop for two
state-of-the-art CNN-based models under this protocol, compared to previously
reported benchmarks. Additionally, we highlight the critical role of the
augmentation pipeline during training with contrastive loss. By introduction
low pass and high pass filters in the augmentation pipeline we significantly
increase the performance of both systems in our proposed evaluation.
Furthermore, we develop a transformer-based model with a tailored projection
module and demonstrate that transferring knowledge from a semantically relevant
domain yields a more robust solution. The transformer architecture outperforms
CNN-based models across all noise levels, and query durations. In low noise
conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in
finding the correct song, surpassing by 14%, and by 18.5% the second-best
performing model, respectively, Under heavy noise levels, we achieve a
detection rate 56.5% for 15-second query duration. All experiments are
conducted on public large-scale dataset of over 100K songs, with queries
matched against a database of 56 million vectors.

</details>


### [220] [Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis](https://arxiv.org/abs/2507.06116)
*Xintong Hu,Yixuan Chen,Rui Yang,Wenxiang Guo,Changhao Pan*

Main category: cs.SD

TL;DR: 本文提出了一种基于自监督学习语音模型的增强MOS预测系统，采用Mixture of Experts（MoE）分类头，并利用多商业生成模型的合成数据进行数据增强。尽管采用了MoE架构和扩展数据集，但模型在句子级预测任务中的性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 自动语音质量评估在语音合成系统开发中至关重要，但现有模型在不同粒度预测任务中表现差异显著。本文旨在改进这一问题。

Method: 基于自监督模型（如wav2vec2），设计专用MoE架构，并利用多商业生成模型的合成数据进行数据增强。

Result: 尽管采用了MoE架构和扩展数据集，模型在句子级预测任务中的性能提升有限。

Conclusion: 本文揭示了当前方法在句子级质量评估中的局限性，为自动语音质量评估领域提供了新的技术路径，并探讨了不同评估粒度性能差异的根本原因。

Abstract: Automatic speech quality assessment plays a crucial role in the development
of speech synthesis systems, but existing models exhibit significant
performance variations across different granularity levels of prediction tasks.
This paper proposes an enhanced MOS prediction system based on self-supervised
learning speech models, incorporating a Mixture of Experts (MoE) classification
head and utilizing synthetic data from multiple commercial generation models
for data augmentation. Our method builds upon existing self-supervised models
such as wav2vec2, designing a specialized MoE architecture to address different
types of speech quality assessment tasks. We also collected a large-scale
synthetic speech dataset encompassing the latest text-to-speech, speech
conversion, and speech enhancement systems. However, despite the adoption of
the MoE architecture and expanded dataset, the model's performance improvements
in sentence-level prediction tasks remain limited. Our work reveals the
limitations of current methods in handling sentence-level quality assessment,
provides new technical pathways for the field of automatic speech quality
assessment, and also delves into the fundamental causes of performance
differences across different assessment granularities.

</details>
