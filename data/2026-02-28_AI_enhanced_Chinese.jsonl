{"id": "2602.22862", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22862", "abs": "https://arxiv.org/abs/2602.22862", "authors": ["Enda Xiang", "Haoxiang Ma", "Xinzhu Ma", "Zicheng Liu", "Di Huang"], "title": "GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion", "comment": "Accepted to CVPR 2026", "summary": "This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u6269\u6563\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u7b56\u7565\u5f15\u5bfc\u52a8\u4f5c\u89e3\u7801\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u91cd\u5efa\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u6293\u53d6\u7b56\u7565\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u6293\u53d6\u7b56\u7565\u901a\u5e38\u5b58\u5728\u6293\u53d6\u6267\u884c\u4e0d\u7cbe\u786e\u3001\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u7269\u4f53\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u6269\u6563\u7b56\u7565\u65b9\u6cd5\u5df2\u6210\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u6293\u53d6\u4f5c\u4e3a\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u5b50\u4efb\u52a1\uff0c\u5176\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u9700\u8981\u7279\u522b\u5173\u6ce8\u3002", "method": "1. \u5c06\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u6269\u6563\u7b56\u7565\u6846\u67b6\uff1b2. \u4f7f\u7528\u6f5c\u5728\u6269\u6563\u7b56\u7565\u5f15\u5bfc\u52a8\u4f5c\u5757\u89e3\u7801\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8fd0\u52a8\u8f68\u8ff9\u7b26\u5408\u53ef\u884c\u7684\u6293\u53d6\u914d\u7f6e\uff1b3. \u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u81ea\u76d1\u7763\u91cd\u5efa\u76ee\u6807\uff0c\u4ece\u4e2d\u95f4\u8868\u793a\u4e2d\u91cd\u5efa\u8155\u90e8\u76f8\u673a\u56fe\u50cf\u4ee5\u5d4c\u5165\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u52a8\u6001\u6293\u53d6\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6293\u53d6\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u91cd\u5efa\u76ee\u6807\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u6293\u53d6\u7b56\u7565\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22922", "abs": "https://arxiv.org/abs/2602.22922", "authors": ["Sophia Taddei", "Wouter Koppen", "Eligia Alfio", "Stefano Nuzzo", "Louis Flynn", "Maria Alejandra Diaz", "Sebastian Rojas Gonzalez", "Tom Dhaene", "Kevin De Pauw", "Ivo Couckuyt", "Tom Verstraten"], "title": "Bayesian Preference Elicitation: Human-In-The-Loop Optimization of An Active Prosthesis", "comment": "8 pages, 5 figures", "summary": "Tuning active prostheses for people with amputation is time-consuming and relies on metrics that may not fully reflect user needs. We introduce a human-in-the-loop optimization (HILO) approach that leverages direct user preferences to personalize a standard four-parameter prosthesis controller efficiently. Our method employs preference-based Multiobjective Bayesian Optimization that uses a state-or-the-art acquisition function especially designed for preference learning, and includes two algorithmic variants: a discrete version (\\textit{EUBO-LineCoSpar}), and a continuous version (\\textit{BPE4Prost}). Simulation results on benchmark functions and real-application trials demonstrate efficient convergence, robust preference elicitation, and measurable biomechanical improvements, illustrating the potential of preference-driven tuning for user-centered prosthesis control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u8c03\u6574\u5047\u80a2\u63a7\u5236\u5668\u53c2\u6570\uff0c\u901a\u8fc7\u504f\u597d\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u8c03\u4f18\u3002", "motivation": "\u4f20\u7edf\u5047\u80a2\u8c03\u4f18\u8fc7\u7a0b\u8017\u65f6\u4e14\u4f9d\u8d56\u7684\u6307\u6807\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u53cd\u6620\u7528\u6237\u9700\u6c42\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4e2a\u6027\u5316\u8c03\u4f18\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4eba\u7c7b\u5728\u73af\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u7528\u6237\u76f4\u63a5\u504f\u597d\u6765\u4e2a\u6027\u5316\u56db\u53c2\u6570\u5047\u80a2\u63a7\u5236\u5668\u3002\u4f7f\u7528\u57fa\u4e8e\u504f\u597d\u7684\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u5305\u542b\u4e24\u79cd\u7b97\u6cd5\u53d8\u4f53\uff1a\u79bb\u6563\u7248\u672cEUBO-LineCoSpar\u548c\u8fde\u7eed\u7248\u672cBPE4Prost\u3002", "result": "\u5728\u57fa\u51c6\u51fd\u6570\u548c\u5b9e\u9645\u5e94\u7528\u8bd5\u9a8c\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u6536\u655b\u3001\u7a33\u5065\u7684\u504f\u597d\u83b7\u53d6\u4ee5\u53ca\u53ef\u6d4b\u91cf\u7684\u751f\u7269\u529b\u5b66\u6539\u8fdb\u3002", "conclusion": "\u504f\u597d\u9a71\u52a8\u7684\u8c03\u4f18\u65b9\u6cd5\u5728\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u5047\u80a2\u63a7\u5236\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u4e2a\u6027\u5316\u5047\u80a2\u63a7\u5236\u5668\u53c2\u6570\u3002"}}
{"id": "2602.22940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22940", "abs": "https://arxiv.org/abs/2602.22940", "authors": ["Leon Tolksdorf", "Arturo Tejada", "Christian Birkner", "Nathan van de Wouw"], "title": "Considering Perspectives for Automated Driving Ethics: Collective Risk in Vehicular Motion Planning", "comment": "17 pages, 6 figures, 2 tables", "summary": "Recent automated vehicle (AV) motion planning strategies evolve around minimizing risk in road traffic. However, they exclusively consider risk from the AV's perspective and, as such, do not address the ethicality of its decisions for other road users. We argue that this does not reduce the risk of each road user, as risk may be different from the perspective of each road user. Indeed, minimizing the risk from the AV's perspective may not imply that the risk from the perspective of other road users is also being minimized; in fact, it may even increase. To test this hypothesis, we propose an AV motion planning strategy that supports switching risk minimization strategies between all road user perspectives. We find that the risk from the perspective of other road users can generally be considered different to the risk from the AV's perspective. Taking a collective risk perspective, i.e., balancing the risks of all road users, we observe an AV that minimizes overall traffic risk the best, while putting itself at slightly higher risk for the benefit of others, which is consistent with human driving behavior. In addition, adopting a collective risk minimization strategy can also be beneficial to the AV's travel efficiency by acting assertively when other road users maintain a low risk estimate of the AV. Yet, the AV drives conservatively when its planned actions are less predictable to other road users, i.e., associated with high risk. We argue that such behavior is a form of self-reflection and a natural prerequisite for socially acceptable AV behavior. We conclude that to facilitate ethicality in road traffic that includes AVs, the risk-perspective of each road user must be considered in the decision-making of AVs.", "AI": {"tldr": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u5e94\u8003\u8651\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u89c6\u89d2\uff0c\u800c\u975e\u4ec5\u4ece\u81ea\u8eab\u89c6\u89d2\u6700\u5c0f\u5316\u98ce\u9669\uff0c\u91c7\u7528\u96c6\u4f53\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u80fd\u66f4\u597d\u5730\u5e73\u8861\u6574\u4f53\u4ea4\u901a\u98ce\u9669", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\u4ec5\u4eceAV\u81ea\u8eab\u89c6\u89d2\u6700\u5c0f\u5316\u98ce\u9669\uff0c\u672a\u8003\u8651\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u4f26\u7406\u95ee\u9898\u3002\u4eceAV\u89c6\u89d2\u6700\u5c0f\u5316\u98ce\u9669\u4e0d\u4e00\u5b9a\u80fd\u51cf\u5c11\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\uff0c\u751a\u81f3\u53ef\u80fd\u589e\u52a0\u4ed6\u4eec\u7684\u98ce\u9669", "method": "\u63d0\u51fa\u652f\u6301\u5728\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u89c6\u89d2\u4e4b\u95f4\u5207\u6362\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u7684AV\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u91c7\u7528\u96c6\u4f53\u98ce\u9669\u89c6\u89d2\u5e73\u8861\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669", "result": "\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u89c6\u89d2\u901a\u5e38\u4e0eAV\u89c6\u89d2\u4e0d\u540c\uff1b\u91c7\u7528\u96c6\u4f53\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u7684AV\u80fd\u6700\u597d\u5730\u964d\u4f4e\u6574\u4f53\u4ea4\u901a\u98ce\u9669\uff0c\u540c\u65f6\u4e3a\u4ed6\u4eba\u5229\u76ca\u627f\u62c5\u7565\u9ad8\u81ea\u8eab\u98ce\u9669\uff1b\u8be5\u7b56\u7565\u8fd8\u80fd\u901a\u8fc7\u81ea\u4fe1\u884c\u4e3a\u63d0\u9ad8AV\u7684\u51fa\u884c\u6548\u7387", "conclusion": "\u4e3a\u5b9e\u73b0\u5305\u542bAV\u7684\u9053\u8def\u4ea4\u901a\u4f26\u7406\uff0cAV\u51b3\u7b56\u5fc5\u987b\u8003\u8651\u6bcf\u4e2a\u9053\u8def\u4f7f\u7528\u8005\u7684\u98ce\u9669\u89c6\u89d2\uff0c\u96c6\u4f53\u98ce\u9669\u6700\u5c0f\u5316\u7b56\u7565\u662f\u5b9e\u73b0\u793e\u4f1a\u53ef\u63a5\u53d7AV\u884c\u4e3a\u7684\u81ea\u7136\u524d\u63d0"}}
{"id": "2602.22952", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22952", "abs": "https://arxiv.org/abs/2602.22952", "authors": ["Yuan Tang", "Bruno V. Adorno", "Brendan A. McGrath", "Andrew Weightman"], "title": "Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy", "comment": null, "summary": "Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient's body and tissues near the target. Simulations were performed to validate the controller's implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u7ecf\u76ae\u6269\u5f20\u6c14\u7ba1\u5207\u5f00\u672f\u7684\u673a\u5668\u4eba\u7a7f\u523a\u7cfb\u7edf\uff0c\u4f7f\u7528\u7535\u78c1\u4f20\u611f\u5668\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u7ea6\u675f\u63a7\u5236\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7a7f\u523a\u7cbe\u5ea6", "motivation": "\u5f53\u524dPDT\u7a7f\u523a\u5b8c\u5168\u624b\u52a8\u64cd\u4f5c\uff0c\u5b58\u5728\u8f83\u5927\u4f4d\u7f6e\u548c\u89d2\u5ea6\u8bef\u5dee\uff085\u6beb\u7c73\u548c30\u5ea6\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u5e76\u53d1\u75c7\u5982\u5927\u51fa\u8840\u548c\u6c14\u7ba1\u540e\u58c1\u7a7f\u5b54\uff0c\u9700\u8981\u63d0\u9ad8\u7cbe\u5ea6\u548c\u964d\u4f4e\u64cd\u4f5c\u96be\u5ea6", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u901f\u5ea6\u63a7\u5236\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5668\uff0c\u4f7f\u7528\u4e24\u4e2a\u7535\u78c1\u4f20\u611f\u5668\uff08\u4e00\u4e2a\u5728\u9488\u5c16\uff0c\u4e00\u4e2a\u5728\u6c14\u7ba1\u5185\uff09\u8fdb\u884c\u59ff\u6001\u5f15\u5bfc\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7ea6\u675f\u63a7\u5236\u5668\u5728\u7ebf\u9002\u5e94\u4e0d\u786e\u5b9a\u7684\u8fd0\u52a8\u5b66\u53c2\u6570\u5e76\u907f\u514d\u78b0\u649e", "result": "\u5728\u4eba\u4f53\u6a21\u578b\u4e0a\u8fdb\u884c400\u6b21PDT\u7a7f\u523a\u6d4b\u8bd5\uff0c\u7edd\u5bf9\u4e2d\u4f4d\u7a7f\u523a\u4f4d\u7f6e\u8bef\u5dee\u4e3a1.7\u6beb\u7c73\uff08IQR\uff1a1.9\u6beb\u7c73\uff09\uff0c\u4e2d\u7ebf\u504f\u5dee\u4e3a4.13\u5ea6\uff08IQR\uff1a4.55\u5ea6\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u624b\u52a8\u64cd\u4f5c", "conclusion": "\u673a\u5668\u4ebaPDT\u7a7f\u523a\u7cfb\u7edf\u5728\u6a21\u62df\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u663e\u793a\u51fa\u8f83\u5c0f\u7684\u504f\u5dee\uff0c\u5e76\u63d0\u4f9b\u4e86\u65e0\u78b0\u649e\u63d2\u5165\u7684\u5f62\u5f0f\u4fdd\u8bc1\uff0c\u8868\u660e\u673a\u5668\u4ebaPDT\u7a7f\u523a\u7684\u53ef\u884c\u6027"}}
{"id": "2602.23017", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23017", "abs": "https://arxiv.org/abs/2602.23017", "authors": ["Dean Zadok", "Tom Naamani", "Yuval Bar-Ratson", "Elisha Barash", "Oren Salzman", "Alon Wolf", "Alex M. Bronstein", "Nili Krausz"], "title": "DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks", "comment": null, "summary": "Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u8f7b\u91cf\u5316\u76843D\u6253\u5370\u673a\u5668\u4eba\u5047\u80a2\u624b\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u589e\u5f3a\u4e0e\u7535\u5b50\u8bbe\u5907\uff08\u5982\u952e\u76d8\u3001\u94a2\u7434\uff09\u4ea4\u4e92\u7684\u7075\u5de7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u7269\u4f53\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5047\u80a2\u624b\u65e0\u6cd5\u590d\u5236\u4eba\u624b\u7075\u5de7\u6027\u548c\u76f4\u89c2\u63a7\u5236\uff0c\u5546\u4e1a\u5047\u80a2\u624b\u7f3a\u4e4f\u7cbe\u7ec6\u624b\u6307\u8fd0\u52a8\u6240\u9700\u7684\u7cbe\u5ea6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u952e\u76d8\u6253\u5b57\u3001\u94a2\u7434\u6f14\u594f\u7b49\u9700\u8981\u7cbe\u786e\u624b\u6307\u6309\u538b\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4f4e\u6210\u672c\u3001\u8f7b\u91cf\u5316\u76843D\u6253\u5370\u673a\u5668\u4eba\u5047\u80a2\u624b\uff0c\u5305\u542b\u624b\u6307\u5916\u5c55/\u5185\u6536\u95f4\u8ddd\u8c03\u8282\u673a\u5236\u3001\u4f18\u5316\u6253\u5b57\u529f\u80fd\u76842-D\u8155\u5173\u8282\uff08\u5c3a\u9aa8/\u6861\u9aa8\u504f\u8f6c\u63a7\u5236\uff09\u4ee5\u53ca\u72ec\u7acb\u624b\u6307\u6309\u538b\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u7814\u7a76\u8bc1\u660e\u53c2\u4e0e\u8005\u80fd\u591f\u4f7f\u7528\u8be5\u673a\u5668\u4eba\u5047\u80a2\u624b\u5b9e\u65f6\u6267\u884c\u952e\u76d8\u6253\u5b57\u548c\u94a2\u7434\u6f14\u594f\uff0c\u652f\u6301\u4e0d\u540c\u7ea7\u522b\u7684\u624b\u6307\u548c\u624b\u8155\u8fd0\u52a8\uff0c\u6bd4\u4ee5\u5f80\u8bbe\u8ba1\u66f4\u6709\u6548\u5730\u6267\u884c\u6309\u952e\u6253\u5b57\u52a8\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bbe\u8ba1\u80fd\u591f\u589e\u5f3a\u5047\u80a2\u624b\u7684\u529f\u80fd\u6027\uff0c\u7279\u522b\u662f\u5728\u4e0e\u7535\u5b50\u8bbe\u5907\u4ea4\u4e92\u65b9\u9762\uff0c\u4e3a\u9700\u8981\u7cbe\u786e\u624b\u6307\u8fd0\u52a8\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23024", "abs": "https://arxiv.org/abs/2602.23024", "authors": ["Jiahao Liu", "Cui Wenbo", "Haoran Li", "Dongbin Zhao"], "title": "InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation", "comment": "16 pages, 9 figures", "summary": "Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.", "AI": {"tldr": "InCoM\u6846\u67b6\u901a\u8fc7\u610f\u56fe\u9a71\u52a8\u7684\u611f\u77e5\u548c\u7ed3\u6784\u5316\u534f\u8c03\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5168\u8eab\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u63a7\u5236\u8026\u5408\u548c\u611f\u77e5\u6ce8\u610f\u529b\u5206\u914d\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u5168\u8eab\u79fb\u52a8\u64cd\u4f5c\u9700\u8981\u534f\u8c03\u63a7\u5236\u79fb\u52a8\u5e95\u5ea7\u548c\u673a\u68b0\u81c2\uff0c\u5e76\u5728\u52a8\u6001\u53d8\u5316\u7684\u89c6\u89d2\u4e0b\u4fdd\u6301\u9c81\u68d2\u611f\u77e5\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5e95\u5ea7\u548c\u624b\u81c2\u52a8\u4f5c\u7684\u5f3a\u8026\u5408\u4f7f\u5168\u8eab\u63a7\u5236\u4f18\u5316\u590d\u6742\u5316\uff0c\u4ee5\u53ca\u79fb\u52a8\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u611f\u77e5\u6ce8\u610f\u529b\u5206\u914d\u4e0d\u4f73\u3002", "method": "\u63d0\u51faInCoM\u6846\u67b6\uff1a1\uff09\u63a8\u65ad\u6f5c\u5728\u8fd0\u52a8\u610f\u56fe\u4ee5\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u591a\u5c3a\u5ea6\u611f\u77e5\u7279\u5f81\uff0c\u5b9e\u73b0\u9636\u6bb5\u81ea\u9002\u5e94\u7684\u611f\u77e5\u6ce8\u610f\u529b\u5206\u914d\uff1b2\uff09\u5f15\u5165\u51e0\u4f55-\u8bed\u4e49\u7ed3\u6784\u5316\u5bf9\u9f50\u673a\u5236\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u5e94\u6027\uff1b3\uff09\u8bbe\u8ba1\u89e3\u8026\u534f\u8c03\u6d41\u5339\u914d\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u663e\u5f0f\u5efa\u6a21\u534f\u8c03\u7684\u5e95\u5ea7-\u624b\u81c2\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728\u4e09\u4e2aManiSkill-HAB\u573a\u666f\u4e2d\uff0cInCoM\u5728\u6ca1\u6709\u7279\u6743\u611f\u77e5\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u4e8628.2%\u300126.1%\u548c23.6%\u7684\u6210\u529f\u7387\u3002", "conclusion": "InCoM\u901a\u8fc7\u610f\u56fe\u9a71\u52a8\u7684\u611f\u77e5\u548c\u7ed3\u6784\u5316\u534f\u8c03\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5168\u8eab\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u63a7\u5236\u8026\u5408\u548c\u611f\u77e5\u6ce8\u610f\u529b\u5206\u914d\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.23051", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23051", "abs": "https://arxiv.org/abs/2602.23051", "authors": ["Aihong Wang", "Tenghui Xie", "Fuxi Wen", "Jun Li"], "title": "An Empirical Analysis of Cooperative Perception for Occlusion Risk Mitigation", "comment": "Accepted for publication in IEEE Internet of Things Journal (Regular Article), 2026. DOI: 10.1109/JIOT.2026.3668184", "summary": "Occlusions present a significant challenge for connected and automated vehicles, as they can obscure critical road users from perception systems. Traditional risk metrics often fail to capture the cumulative nature of these threats over time adequately. In this paper, we propose a novel and universal risk assessment metric, the Risk of Tracking Loss (RTL), which aggregates instantaneous risk intensity throughout occluded periods. This provides a holistic risk profile that encompasses both high-intensity, short-term threats and prolonged exposure. Utilizing diverse and high-fidelity real-world datasets, a large-scale statistical analysis is conducted to characterize occlusion risk and validate the effectiveness of the proposed metric. The metric is applied to evaluate different vehicle-to-everything (V2X) deployment strategies. Our study shows that full V2X penetration theoretically eliminates this risk, the reduction is highly nonlinear; a substantial statistical benefit requires a high penetration threshold of 75-90%. To overcome this limitation, we propose a novel asymmetric communication framework that allows even non-connected vehicles to receive warnings. Experimental results demonstrate that this paradigm achieves better risk mitigation performance. We found that our approach at 25% penetration outperforms the traditional symmetric model at 75%, and benefits saturate at only 50% penetration. This work provides a crucial risk assessment metric and a cost-effective, strategic roadmap for accelerating the safety benefits of V2X deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u8bc4\u4f30\u6307\u6807RTL\uff0c\u7528\u4e8e\u91cf\u5316\u906e\u6321\u98ce\u9669\uff0c\u5e76\u901a\u8fc7V2X\u90e8\u7f72\u7b56\u7565\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u53d1\u73b0\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\u80fd\u66f4\u9ad8\u6548\u964d\u4f4e\u98ce\u9669\u3002", "motivation": "\u906e\u6321\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u611f\u77e5\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u4f20\u7edf\u98ce\u9669\u6307\u6807\u96be\u4ee5\u6355\u6349\u968f\u65f6\u95f4\u7d2f\u79ef\u7684\u5a01\u80c1\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRisk of Tracking Loss (RTL)\u6307\u6807\uff0c\u805a\u5408\u906e\u6321\u671f\u95f4\u7684\u77ac\u65f6\u98ce\u9669\u5f3a\u5ea6\uff1b\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u7edf\u8ba1\u5206\u6790\uff1b\u5e94\u7528\u8be5\u6307\u6807\u8bc4\u4f30\u4e0d\u540cV2X\u90e8\u7f72\u7b56\u7565\uff1b\u63d0\u51fa\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\u3002", "result": "RTL\u80fd\u6709\u6548\u8868\u5f81\u906e\u6321\u98ce\u9669\uff1b\u5b8c\u5168V2X\u6e17\u900f\u7406\u8bba\u4e0a\u53ef\u6d88\u9664\u98ce\u9669\uff0c\u4f46\u9700\u898175-90%\u7684\u9ad8\u6e17\u900f\u9608\u503c\uff1b\u975e\u5bf9\u79f0\u901a\u4fe1\u6846\u67b6\u572825%\u6e17\u900f\u7387\u4e0b\u4f18\u4e8e\u4f20\u7edf\u5bf9\u79f0\u6a21\u578b\u572875%\u7684\u8868\u73b0\uff0c50%\u6e17\u900f\u7387\u5373\u53ef\u8fbe\u5230\u9971\u548c\u6548\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u906e\u6321\u98ce\u9669\u8bc4\u4f30\u6307\u6807RTL\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u975e\u5bf9\u79f0V2X\u90e8\u7f72\u7b56\u7565\uff0c\u80fd\u52a0\u901f\u5b9e\u73b0V2X\u7684\u5b89\u5168\u6548\u76ca\u3002"}}
{"id": "2602.23109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23109", "abs": "https://arxiv.org/abs/2602.23109", "authors": ["Kai Chen", "Yuyao Huang", "Guang Chen"], "title": "Towards Intelligible Human-Robot Interaction: An Active Inference Approach to Occluded Pedestrian Scenarios", "comment": "14 pages, 6 figures, Proceedings of the 2026 ACM/IEEE International Conference on Human-Robot Interaction (HRI'26)", "summary": "The sudden appearance of occluded pedestrians presents a critical safety challenge in autonomous driving. Conventional rule-based or purely data-driven approaches struggle with the inherent high uncertainty of these long-tail scenarios. To tackle this challenge, we propose a novel framework grounded in Active Inference, which endows the agent with a human-like, belief-driven mechanism. Our framework leverages a Rao-Blackwellized Particle Filter (RBPF) to efficiently estimate the pedestrian's hybrid state. To emulate human-like cognitive processes under uncertainty, we introduce a Conditional Belief Reset mechanism and a Hypothesis Injection technique to explicitly model beliefs about the pedestrian's multiple latent intentions. Planning is achieved via a Cross-Entropy Method (CEM) enhanced Model Predictive Path Integral (MPPI) controller, which synergizes the efficient, iterative search of CEM with the inherent robustness of MPPI. Simulation experiments demonstrate that our approach significantly reduces the collision rate compared to reactive, rule-based, and reinforcement learning (RL) baselines, while also exhibiting explainable and human-like driving behavior that reflects the agent's internal belief state.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7RBPF\u4f30\u8ba1\u884c\u4eba\u6df7\u5408\u72b6\u6001\uff0c\u5f15\u5165\u6761\u4ef6\u4fe1\u5ff5\u91cd\u7f6e\u548c\u5047\u8bbe\u6ce8\u5165\u6280\u672f\u5efa\u6a21\u884c\u4eba\u6f5c\u5728\u610f\u56fe\uff0c\u4f7f\u7528CEM\u589e\u5f3a\u7684MPPI\u63a7\u5236\u5668\u8fdb\u884c\u89c4\u5212\uff0c\u663e\u8457\u964d\u4f4e\u906e\u6321\u884c\u4eba\u573a\u666f\u7684\u78b0\u649e\u7387\u3002", "motivation": "\u906e\u6321\u884c\u4eba\u7684\u7a81\u7136\u51fa\u73b0\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5173\u952e\u5b89\u5168\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u6216\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u957f\u5c3e\u573a\u666f\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u3002", "method": "1. \u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u8d4b\u4e88\u667a\u80fd\u4f53\u7c7b\u4eba\u7684\u4fe1\u5ff5\u9a71\u52a8\u673a\u5236\uff1b2. \u4f7f\u7528Rao-Blackwellized\u7c92\u5b50\u6ee4\u6ce2\u5668\u9ad8\u6548\u4f30\u8ba1\u884c\u4eba\u6df7\u5408\u72b6\u6001\uff1b3. \u5f15\u5165\u6761\u4ef6\u4fe1\u5ff5\u91cd\u7f6e\u673a\u5236\u548c\u5047\u8bbe\u6ce8\u5165\u6280\u672f\uff0c\u663e\u5f0f\u5efa\u6a21\u884c\u4eba\u591a\u4e2a\u6f5c\u5728\u610f\u56fe\uff1b4. \u91c7\u7528\u4ea4\u53c9\u71b5\u65b9\u6cd5\u589e\u5f3a\u7684\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u5668\u8fdb\u884c\u89c4\u5212\uff0c\u7ed3\u5408CEM\u7684\u9ad8\u6548\u8fed\u4ee3\u641c\u7d22\u548cMPPI\u7684\u56fa\u6709\u9c81\u68d2\u6027\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u3001\u57fa\u4e8e\u89c4\u5219\u548c\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u78b0\u649e\u7387\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u53ef\u89e3\u91ca\u7684\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\uff0c\u53cd\u6620\u4e86\u667a\u80fd\u4f53\u7684\u5185\u90e8\u4fe1\u5ff5\u72b6\u6001\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u906e\u6321\u884c\u4eba\u573a\u666f\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u901a\u8fc7\u7c7b\u4eba\u7684\u4fe1\u5ff5\u9a71\u52a8\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u884c\u4e3a\u3002"}}
{"id": "2602.23206", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23206", "abs": "https://arxiv.org/abs/2602.23206", "authors": ["Chung Hee Kim", "Shivani Kamtikar", "Tye Brady", "Taskin Padir", "Joshua Migdal"], "title": "Grasp, Slide, Roll: Comparative Analysis of Contact Modes for Tactile-Based Shape Reconstruction", "comment": "8 pages, 11 figures, Accepted by ICRA 2026", "summary": "Tactile sensing allows robots to gather detailed geometric information about objects through physical interaction, complementing vision-based approaches. However, efficiently acquiring useful tactile data remains challenging due to the time-consuming nature of physical contact and the need to strategically choose contact locations that maximize information gain while minimizing physical interactions. This paper studies how different contact modes affect object shape reconstruction using a tactile-enabled dexterous gripper. We compare three contact interaction modes: grasp-releasing, sliding induced by finger-grazing, and palm-rolling. These contact modes are combined with an information-theoretic exploration framework that guides subsequent sampling locations using a shape completion model. Our results show that the improved tactile sensing efficiency of finger-grazing and palm-rolling translates into faster convergence in shape reconstruction, requiring 34% fewer physical interactions while improving reconstruction accuracy by 55%. We validate our approach using a UR5e robot arm equipped with an Inspire-Robots Dexterous Hand, showing robust performance across primitive object geometries.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u89e6\u89c9\u4ea4\u4e92\u6a21\u5f0f\uff08\u6293\u63e1\u91ca\u653e\u3001\u624b\u6307\u6ed1\u52a8\u3001\u624b\u638c\u6eda\u52a8\uff09\u5bf9\u7269\u4f53\u5f62\u72b6\u91cd\u5efa\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u624b\u6307\u6ed1\u52a8\u548c\u624b\u638c\u6eda\u52a8\u6a21\u5f0f\u80fd\u663e\u8457\u63d0\u9ad8\u89e6\u89c9\u611f\u77e5\u6548\u7387\uff0c\u51cf\u5c1134%\u7684\u7269\u7406\u4ea4\u4e92\u6b21\u6570\u540c\u65f6\u63d0\u9ad855%\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u89e6\u89c9\u611f\u77e5\u80fd\u8ba9\u673a\u5668\u4eba\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u83b7\u53d6\u7269\u4f53\u7684\u8be6\u7ec6\u51e0\u4f55\u4fe1\u606f\uff0c\u8865\u5145\u89c6\u89c9\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002\u7136\u800c\uff0c\u7269\u7406\u63a5\u89e6\u8017\u65f6\u4e14\u9700\u8981\u7b56\u7565\u6027\u5730\u9009\u62e9\u63a5\u89e6\u4f4d\u7f6e\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u540c\u65f6\u6700\u5c0f\u5316\u4ea4\u4e92\u6b21\u6570\uff0c\u8fd9\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u63a5\u89e6\u4ea4\u4e92\u6a21\u5f0f\uff1a\u6293\u63e1\u91ca\u653e\u3001\u624b\u6307\u6ed1\u52a8\uff08\u624b\u6307\u6ed1\u52a8\u5f15\u8d77\uff09\u548c\u624b\u638c\u6eda\u52a8\u3002\u8fd9\u4e9b\u6a21\u5f0f\u4e0e\u4fe1\u606f\u8bba\u63a2\u7d22\u6846\u67b6\u7ed3\u5408\uff0c\u4f7f\u7528\u5f62\u72b6\u8865\u5168\u6a21\u578b\u6307\u5bfc\u540e\u7eed\u91c7\u6837\u4f4d\u7f6e\u3002\u5b9e\u9a8c\u5728\u914d\u5907Inspire-Robots\u7075\u5de7\u624b\u7684UR5e\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u624b\u6307\u6ed1\u52a8\u548c\u624b\u638c\u6eda\u52a8\u6a21\u5f0f\u663e\u8457\u63d0\u9ad8\u4e86\u89e6\u89c9\u611f\u77e5\u6548\u7387\uff0c\u5728\u5f62\u72b6\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u51cf\u5c11\u4e8634%\u7684\u7269\u7406\u4ea4\u4e92\u6b21\u6570\uff0c\u540c\u65f6\u63d0\u9ad8\u4e8655%\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002\u65b9\u6cd5\u5728\u57fa\u672c\u51e0\u4f55\u4f53\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u4e0d\u540c\u7684\u89e6\u89c9\u4ea4\u4e92\u6a21\u5f0f\u5bf9\u7269\u4f53\u5f62\u72b6\u91cd\u5efa\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u624b\u6307\u6ed1\u52a8\u548c\u624b\u638c\u6eda\u52a8\u6a21\u5f0f\u76f8\u6bd4\u4f20\u7edf\u6293\u63e1\u91ca\u653e\u6a21\u5f0f\u80fd\u66f4\u9ad8\u6548\u5730\u83b7\u53d6\u89e6\u89c9\u4fe1\u606f\uff0c\u51cf\u5c11\u7269\u7406\u4ea4\u4e92\u9700\u6c42\u5e76\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u3002"}}
{"id": "2602.23253", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23253", "abs": "https://arxiv.org/abs/2602.23253", "authors": ["Yijie Guo", "Iretiayo Akinola", "Lars Johannsmeier", "Hugo Hadfield", "Abhishek Gupta", "Yashraj Narang"], "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly", "comment": null, "summary": "Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.", "AI": {"tldr": "SPARR\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eff\u771f\u8bad\u7ec3\u57fa\u7840\u7b56\u7565\u548c\u73b0\u5b9e\u4e16\u754c\u6b8b\u5dee\u7b56\u7565\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u4eba\u7c7b\u76d1\u7763\u7684\u4f9d\u8d56\u3002", "motivation": "\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u9700\u8981\u7cbe\u786e\u7684\u63a5\u89e6\u5f0f\u64cd\u4f5c\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4eff\u771f\u8bad\u7ec3\u7684\u65b9\u6cd5\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u907f\u514d\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u4e14\u7f3a\u4e4f\u5bf9\u73af\u5883\u53d8\u5316\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSPARR\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eff\u771f\u8bad\u7ec3\u7684\u57fa\u7840\u7b56\u7565\u548c\u73b0\u5b9e\u4e16\u754c\u5b66\u4e60\u7684\u6b8b\u5dee\u7b56\u7565\u3002\u57fa\u7840\u7b56\u7565\u5728\u4eff\u771f\u4e2d\u4f7f\u7528\u4f4e\u7ea7\u72b6\u6001\u89c2\u6d4b\u548c\u5bc6\u96c6\u5956\u52b1\u8bad\u7ec3\uff0c\u63d0\u4f9b\u521d\u59cb\u884c\u4e3a\u5148\u9a8c\uff1b\u6b8b\u5dee\u7b56\u7565\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u4f7f\u7528\u89c6\u89c9\u89c2\u6d4b\u548c\u7a00\u758f\u5956\u52b1\u5b66\u4e60\uff0c\u8865\u507f\u52a8\u529b\u5b66\u5dee\u5f02\u548c\u4f20\u611f\u5668\u566a\u58f0\u3002", "result": "\u5728\u591a\u79cd\u4e24\u90e8\u4ef6\u88c5\u914d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6210\u529f\u7387\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u9ad838.4%\uff0c\u5468\u671f\u65f6\u95f4\u51cf\u5c1129.7%\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u73b0\u5b9e\u4e16\u754cRL\u65b9\u6cd5\uff0c\u4e0d\u9700\u8981\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "SPARR\u901a\u8fc7\u7ed3\u5408\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u88c5\u914d\u4e2d\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7387\u4e14\u65e0\u9700\u4eba\u7c7b\u76d1\u7763\u7684\u88c5\u914d\u7b56\u7565\u3002"}}
{"id": "2602.23283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23283", "abs": "https://arxiv.org/abs/2602.23283", "authors": ["Mike Y. Michelis", "Nana Obayashi", "Josie Hughes", "Robert K. Katzschmann"], "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots", "comment": null, "summary": "Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment using a simplified, stateless hydrodynamics formulation implemented in the widespread robotics framework MuJoCo. With just two real-world swimming trajectories, we identify five fluid parameters that allow a matching to experimental behavior and generalize across a range of actuation frequencies. We show that this stateless fluid model can generalize to unseen actuation and outperform classical analytical models such as the elongated body theory. This simulation environment runs faster than real-time and can easily enable downstream learning algorithms such as reinforcement learning for target tracking, reaching a 93% success rate. Due to the simplicity and ease of use of the model and our open-source simulation environment, our results show that even simple, stateless models -- when carefully matched to physical data -- can serve as effective digital twins for soft underwater robots, opening up new directions for scalable learning and control in aquatic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b80\u5316\u65e0\u72b6\u6001\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u7684\u808c\u8171\u9a71\u52a8\u9c7c\u673a\u5668\u4eba\u4eff\u771f\u73af\u5883\uff0c\u4ec5\u9700\u4e24\u6761\u771f\u5b9e\u6e38\u6cf3\u8f68\u8ff9\u5373\u53ef\u5339\u914d\u5b9e\u9a8c\u884c\u4e3a\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb\u4e8e\u5b9e\u65f6\uff0c\u4fbf\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b49\u4e0b\u6e38\u63a7\u5236\u7b97\u6cd5\u5e94\u7528\u3002", "motivation": "\u6a21\u4eff\u6e38\u6cf3\u52a8\u7269\u7684\u4f18\u96c5\u8fd0\u52a8\u662f\u8f6f\u4f53\u673a\u5668\u4eba\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u5efa\u6a21\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u590d\u6742\u63a7\u5236\u6216\u5f3a\u5316\u5b66\u4e60\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u5b9e\u7528\u7684\u4eff\u771f\u73af\u5883\u6765\u4fc3\u8fdb\u6c34\u4e0b\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5b66\u4e60\u4e0e\u63a7\u5236\u3002", "method": "\u5728MuJoCo\u673a\u5668\u4eba\u6846\u67b6\u4e2d\u6784\u5efa\u808c\u8171\u9a71\u52a8\u9c7c\u673a\u5668\u4eba\u6a21\u578b\uff0c\u91c7\u7528\u7b80\u5316\u7684\u65e0\u72b6\u6001\u6d41\u4f53\u52a8\u529b\u5b66\u516c\u5f0f\u3002\u4ec5\u4f7f\u7528\u4e24\u6761\u771f\u5b9e\u6e38\u6cf3\u8f68\u8ff9\u8bc6\u522b\u4e94\u4e2a\u6d41\u4f53\u53c2\u6570\uff0c\u4f7f\u4eff\u771f\u884c\u4e3a\u4e0e\u5b9e\u9a8c\u5339\u914d\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u9a71\u52a8\u9891\u7387\u3002", "result": "\u8be5\u65e0\u72b6\u6001\u6d41\u4f53\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9a71\u52a8\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u5206\u6790\u6a21\u578b\u5982\u7ec6\u957f\u4f53\u7406\u8bba\u3002\u4eff\u771f\u73af\u5883\u8fd0\u884c\u901f\u5ea6\u5feb\u4e8e\u5b9e\u65f6\uff0c\u5728\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fbe\u523093%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u65e0\u72b6\u6001\u6a21\u578b\uff0c\u53ea\u8981\u4ed4\u7ec6\u5339\u914d\u7269\u7406\u6570\u636e\uff0c\u5c31\u53ef\u4ee5\u4f5c\u4e3a\u8f6f\u4f53\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u6709\u6548\u6570\u5b57\u5b6a\u751f\u4f53\uff0c\u4e3a\u6c34\u751f\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u548c\u63a7\u5236\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.23287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23287", "abs": "https://arxiv.org/abs/2602.23287", "authors": ["Demiana R. Barsoum", "Mahdieh Nejati Javaremi", "Larisa Y. C. Loke", "Brenna D. Argall"], "title": "Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning", "comment": "13 pages, 8 figures, to appear in the proceedings of the 2026 Human-Robot Interaction (HRI) Conference", "summary": "Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06\u4f4e\u7ef4\u63a5\u53e3\u63a7\u5236\u7684\u673a\u5668\u4eba\u6f14\u793a\u63d0\u5347\u5230\u5b8c\u6574\u63a7\u5236\u7a7a\u95f4\uff0c\u89e3\u51b3\u63a5\u53e3\u9650\u5236\u5bfc\u81f4\u7684\u6b21\u4f18\u8fd0\u52a8\u95ee\u9898\u3002", "motivation": "\u8f85\u52a9\u673a\u5668\u4eba\u7528\u6237\u5e38\u901a\u8fc7\u4f4e\u7ef4\u63a5\u53e3\uff08\u59821\u7ef4\u5438\u5439\u63a5\u53e3\uff09\u63a7\u5236\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\uff0c\u8fd9\u79cd\u7ef4\u5ea6\u4e0d\u5339\u914d\u5bfc\u81f4\u53ea\u80fd\u8bbf\u95ee\u90e8\u5206\u63a7\u5236\u7ef4\u5ea6\uff0c\u4f7f\u6f14\u793a\u5d4c\u5165\u53cd\u6620\u63a5\u53e3\u9650\u5236\u800c\u975e\u7528\u6237\u610f\u56fe\u7684\u6b21\u4f18\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\uff0c\u8003\u8651\u4efb\u52a1\u3001\u73af\u5883\u548c\u63a5\u53e3\u7ea6\u675f\uff0c\u5c06\u6f14\u793a\u63d0\u5347\u5230\u673a\u5668\u4eba\u7684\u5b8c\u6574\u63a7\u5236\u7a7a\u95f4\u3002\u4f7f\u75282\u7ef4\u64cd\u7eb5\u6746\u548c1\u7ef4\u5438\u5439\u63a7\u5236\u63a5\u53e3\u5728\u4e24\u79cd\u4e0d\u540c\u76847\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\u8fdb\u884c\u771f\u5b9e\u4e16\u754cADL\u4efb\u52a1\u6f14\u793a\u8bc4\u4f30\u3002", "result": "\u91cd\u5efa\u540e\u7684\u8f68\u8ff9\u6bd4\u63a5\u53e3\u7ea6\u675f\u7684\u5bf9\u5e94\u8f68\u8ff9\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\uff0c\u540c\u65f6\u5c0a\u91cd\u7528\u6237\u504f\u597d\u3002\u5206\u6790\u91cd\u5efa\u6f14\u793a\u548c\u6d3e\u751f\u63a7\u5236\u7b56\u7565\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8f68\u8ff9\u91cd\u5efa\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8f85\u52a9\u673a\u5668\u4eba\u4e2d\u63a5\u53e3\u7ef4\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5c06\u4f4e\u7ef4\u63a5\u53e3\u6f14\u793a\u63d0\u5347\u5230\u5b8c\u6574\u63a7\u5236\u7a7a\u95f4\uff0c\u4ea7\u751f\u66f4\u4f18\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002"}}
