<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 30]
- [cs.RO](#cs.RO) [Total: 52]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.GT](#cs.GT) [Total: 13]
- [cs.HC](#cs.HC) [Total: 21]
- [cs.AI](#cs.AI) [Total: 30]
- [eess.SY](#eess.SY) [Total: 8]
- [cs.LG](#cs.LG) [Total: 111]
- [cs.MA](#cs.MA) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Gaussian Mesh Renderer for Lightweight Differentiable Rendering](https://arxiv.org/abs/2602.14493)
*Xinpeng Liu,Fumio Okura*

Main category: cs.CV

TL;DR: 提出Gaussian Mesh Renderer (GMR)，将3D高斯溅射的高效光栅化过程与传统网格表示相结合，实现轻量级可微分网格渲染器


<details>
  <summary>Details</summary>
Motivation: 传统基于网格的可微分渲染器存在优化速度慢或计算量大的问题，而3D高斯溅射(3DGS)虽然渲染快但缺乏网格的结构保真度。需要结合两者的优势

Method: 提出Gaussian Mesh Renderer (GMR)，从对应的网格三角形中解析推导出高斯基元，保持结构保真度并实现梯度流动。利用3DGS的高效光栅化过程

Result: 相比传统网格渲染器，GMR实现了更平滑的梯度，特别有助于在内存有限的情况下使用小批量进行更好的优化

Conclusion: GMR成功地将高斯和网格表示紧密集成，提供了一种轻量级可微分网格渲染解决方案，结合了3DGS的高效渲染和网格的结构优势

Abstract: 3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.

</details>


### [2] [GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2602.14771)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-JEPA是一个模型预测预训练框架，通过教师-学生架构从干净和损坏的帧中预测伪跟踪模型，提升跟踪器在遮挡和动态环境中的泛化能力；OccuSolver则通过点中心跟踪器增强遮挡感知，迭代优化可见性状态。


<details>
  <summary>Details</summary>
Motivation: 人类视觉系统能够整合历史信息、适应场景变化并进行细粒度遮挡推理，而现有通用目标跟踪器通常针对训练目标优化，在未见场景中泛化能力有限，且遮挡推理较为粗糙，缺乏对遮挡模式的详细建模。

Method: 提出GOT-JEPA框架，将JEPA从预测图像特征扩展到预测跟踪模型：教师预测器从干净当前帧生成伪跟踪模型，学生预测器从损坏版本学习预测相同模型，提供稳定的伪监督。在此基础上提出OccuSolver，采用点中心点跟踪器进行目标感知的可见性估计和遮挡模式捕获，基于跟踪器迭代生成的目标先验逐步优化可见性状态。

Result: 在七个基准测试上的广泛评估表明，该方法有效提升了跟踪器的泛化能力和鲁棒性。

Conclusion: GOT-JEPA通过模型预测预训练框架增强了跟踪器在动态环境中的泛化能力，而OccuSolver通过细粒度的遮挡感知和迭代优化进一步提升了遮挡处理性能，共同解决了现有跟踪器在泛化和遮挡推理方面的局限性。

Abstract: The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.

</details>


### [3] [GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables](https://arxiv.org/abs/2602.13479)
*Akhil Ramachandran,Ankit Arun,Ashish Shenoy,Abhay Harpale,Srihari Jayakumar,Debojeet Chatterjee,Mohsen Moslehpour,Pierce Chuang,Yichao Lu,Vikas Bhardwaj,Peyman Heidari*

Main category: cs.CV

TL;DR: 提出了一种用于可穿戴设备的混合架构视频大语言模型，通过选择性高分辨率OCR和低分辨率视频流，在保持文本理解质量的同时显著降低功耗。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备上部署基于文本的视觉问答面临根本矛盾：文本识别需要高分辨率视频，但流式传输高质量视频会耗尽电池并导致热节流。现有模型在处理实时流中的多帧文本时难以保持连贯的时间上下文。

Method: 利用文本识别和视觉推理的不对称分辨率需求，提出混合架构：在设备上执行选择性高分辨率OCR，同时流式传输低分辨率视频用于视觉上下文理解。

Result: 在五个任务类别的文本VQA基准测试中，系统达到72%准确率，功耗仅为全分辨率流式传输的0.49倍，能够在资源受限的可穿戴设备上实现持续的VQA会话。

Conclusion: 通过利用分辨率需求的不对称性，提出的混合架构解决了可穿戴设备上文本VQA的功耗与性能矛盾，为资源受限设备上的持续视觉理解应用提供了可行方案。

Abstract: Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.

</details>


### [4] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: MAILS：一种面向无人机的无地图LiDAR重定位框架，通过局部保持滑动窗口注意力、坐标无关特征初始化和局部不变位置编码，显著提升无人机场景下的定位精度


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR重定位方法主要针对自动驾驶设计，在无人机场景下精度显著下降。无人机飞行中存在大角度偏航旋转和高度变化，且缺乏真实无人机飞行特性的LiDAR重定位数据集

Method: 提出MAILS框架：1) 局部保持滑动窗口注意力模块从稀疏点云提取局部判别性几何特征；2) 坐标无关特征初始化模块和局部不变位置编码机制处理无人机飞行中的偏航旋转和高度变化；3) 构建大规模无人机LiDAR定位数据集，包含四个场景和多种飞行轨迹

Result: 实验表明该方法达到满意的定位精度，并显著优于现有技术。代码和数据集将公开发布

Conclusion: MAILS框架有效解决了无人机场景下LiDAR重定位的挑战，通过创新的特征提取和位置编码机制，在真实无人机飞行条件下实现了高精度定位

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

</details>


### [5] [Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge](https://arxiv.org/abs/2602.13324)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.CV

TL;DR: 提出分层零样本框架，结合轻量目标检测与紧凑视觉语言模型，用于军事边缘机器人自主部署，在合成战场视频上验证了高精度性能。


<details>
  <summary>Details</summary>
Motivation: 军事动态环境中部署自主边缘机器人面临两大挑战：领域特定训练数据稀缺和边缘硬件计算能力有限，需要开发高效且无需大量训练数据的解决方案。

Method: 采用分层级联架构：Grounding DINO作为高召回率、文本可提示的区域提议器，高置信度检测帧传递给边缘级VLMs（Qwen和Gemma家族，4B-12B参数）进行语义验证。进一步扩展为侦察-指挥官工作流，并引入"受控输入"方法分离感知与推理。

Result: 在55个高保真合成战场视频上评估：误报过滤达100%准确率，损伤评估达97.5%，细粒度车辆分类55-90%。侦察-指挥官工作流实现100%正确资产部署，推理得分9.8/10，延迟低于75秒。发现不同模型的失败模式差异。

Conclusion: 分层零样本架构适用于边缘自主系统，为安全关键应用中VLM适用性认证提供了诊断框架，揭示了不同模型在感知与推理能力上的特异性。

Abstract: Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel "Controlled Input" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.

</details>


### [6] [HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13329)
*Yiru Wang,Zichong Gu,Yu Gao,Anqing Jiang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun*

Main category: cs.CV

TL;DR: HiST-VLA是一个用于自动驾驶的分层时空视觉-语言-动作模型，通过几何感知、动态令牌稀疏化和分层规划器解决现有VLA模型的数值推理不精确、3D空间感知弱和上下文敏感性问题，在NAVSIM v2基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在安全关键自动驾驶场景中存在三个主要限制：数值推理不精确、3D空间感知能力弱、对上下文高度敏感，这些限制了其在自动驾驶中的可靠应用。

Method: 1. 集成几何感知与细粒度驾驶命令和状态历史提示来增强3D时空推理；2. 在VLA架构中集成动态令牌稀疏化技术，融合冗余令牌而非过滤；3. 使用分层变换器规划器逐步将粗粒度VLA路径点细化为细粒度轨迹；4. 通过动态潜在正则化整合语言命令，确保严格的空间基础和时序一致性。

Result: 在NAVSIM v2基准测试中取得最先进性能：Navtest上EPDMS达到88.6，伪闭环Navhard基准上EPDMS达到50.9。

Conclusion: HiST-VLA通过分层时空架构有效解决了VLA模型在自动驾驶中的关键限制，实现了可靠的轨迹生成，为安全关键自动驾驶应用提供了有前景的解决方案。

Abstract: Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.
  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.

</details>


### [7] [Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots](https://arxiv.org/abs/2602.13347)
*Lijun Zhang,Nikhil Chacko,Petter Nilsson,Ruinian Xu,Shantanu Thakar,Bai Lou,Harpreet Sawhney,Zhebin Zhang,Mudit Agrawal,Bhavana Chandrashekhar,Aaron Parness*

Main category: cs.CV

TL;DR: FOREST是一个仓库存储意图条件化的世界模型，使用潜在扩散变换器预测存储操作后的货箱布局，显著提升了预测几何一致性。


<details>
  <summary>Details</summary>
Motivation: 自动化仓库执行数百万次存储操作，需要在真实执行前根据当前观察和计划存储行为预测货箱状态，这对系统规划很有价值。

Method: 提出FOREST模型，将货箱状态表示为物品对齐的实例掩码，使用潜在扩散变换器从观察上下文中预测存储后的配置。

Result: FOREST显著改善了预测与真实存储后布局之间的几何一致性，在下游任务中，用FOREST预测替换真实后存储掩码仅导致适度的性能损失。

Conclusion: FOREST能够为仓库规划提供有用的前瞻信号，在负载质量评估和多存储推理等下游任务中表现良好。

Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.

</details>


### [8] [Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones](https://arxiv.org/abs/2602.13440)
*Sebastian-Ion Nae,Mihai-Eugen Barbu,Sebastian Mocanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 该研究提出了一个室内无人机视频数据集，用于评估在资源受限的无人机平台上基于回放的类增量学习方法，发现Forgetting-Aware Replay在有限内存预算下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 室内无人机等自主智能体需要在实时学习新物体类别的同时限制灾难性遗忘，但现有无人机数据集主要关注室外场景且缺乏时间连贯的室内视频，因此需要专门的室内无人机数据集来评估类增量学习方法。

Method: 1) 创建包含14,400帧的室内无人机数据集，采用半自动标注流程；2) 使用YOLOv11-nano作为资源高效的检测器；3) 在有限内存预算(5-10%回放)下评估三种基于回放的CIL策略：经验回放(ER)、最大干扰检索(MIR)和遗忘感知回放(FAR)；4) 使用Grad-CAM分析注意力转移。

Result: 在5%回放预算下，FAR获得82.96%的平均准确率(mAP50-95)，优于其他方法。Grad-CAM分析显示在混合场景中注意力在不同类别间转移，这与无人机定位质量下降相关。实验证明基于回放的持续学习可有效应用于边缘空中系统。

Conclusion: 该工作贡献了一个具有时间连贯性的室内无人机视频数据集，并在有限回放预算下评估了基于回放的类增量学习方法，为资源受限的无人机平台提供了有效的持续学习解决方案。

Abstract: Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\%$ with $5\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl

</details>


### [9] [Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos](https://arxiv.org/abs/2602.13806)
*Can Li,Jie Gu,Jingmin Chen,Fangzhou Qiu,Lei Sun*

Main category: cs.CV

TL;DR: 提出一种多尺度动态机制，通过分解复杂运动场来解决单目视频中4D重建的模糊性问题，使用多尺度动态的高斯序列表示，结合视觉基础模型的多模态先验，实现准确且全局一致的4D重建。


<details>
  <summary>Details</summary>
Motivation: 从单目视频理解动态场景对于可扩展的机器人学习至关重要，但在严格单目设置下的4D重建仍然高度不适定。真实世界的动态在从物体到粒子级别表现出多尺度规律性，这为解决重建模糊性提供了关键思路。

Method: 设计了多尺度动态机制来分解复杂运动场，提出多尺度动态的高斯序列表示，通过多级运动组合构建动态3D高斯。采用分层结构减轻重建模糊性，并融入视觉基础模型的多模态先验建立互补监督，约束解空间。

Result: 在基准和真实世界操作数据集上的动态新视角合成实验表明，相比现有方法有显著改进。能够从单目随意视频实现准确且全局一致的4D重建。

Conclusion: 通过利用真实世界动态的多尺度规律性，结合多尺度动态机制和视觉基础模型先验，有效解决了单目视频4D重建的模糊性问题，为机器人学习提供了可扩展的动态场景理解方法。

Abstract: Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.

</details>


### [10] [Understanding Sensor Vulnerabilities in Industrial XR Tracking](https://arxiv.org/abs/2602.14413)
*Sourya Saha,Md. Nurul Absur*

Main category: cs.CV

TL;DR: 该论文通过系统性的故障注入实验，研究了视觉-惯性里程计在工业XR系统中面对传感器退化时的表现，发现惯性传感器退化比视觉传感器退化对轨迹精度的影响更严重。


<details>
  <summary>Details</summary>
Motivation: 工业XR系统依赖VIO进行姿态跟踪，但实际环境常偏离理想假设，现有评估多关注正常传感器行为，对持续传感器退化在操作条件下的影响理解不足。

Method: 采用受控实证研究，通过系统性的故障注入，考察视觉和惯性模态在不同操作状态下的退化影响，并进行定量评估。

Result: 观察到故障影响的显著不对称性：视觉传感退化通常导致厘米级的有限姿态误差，而惯性传感退化可能引起数百至数千米的轨迹偏差。

Conclusion: 研究结果强调了在工业XR系统的评估和设计中需要更加重视惯性传感器的可靠性。

Abstract: Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.

</details>


### [11] [Advances in Global Solvers for 3D Vision](https://arxiv.org/abs/2602.14662)
*Zhenjun Zhao,Heng Yang,Bangyan Liao,Yingping Zeng,Shaocheng Yan,Yingdong Gu,Peidong Liu,Yi Zhou,Haoang Li,Javier Civera*

Main category: cs.CV

TL;DR: 这篇综述首次系统回顾了3D视觉中的全局求解器，通过分支定界、凸松弛和渐进非凸性三种核心范式构建了统一分类体系，分析了它们在几何优化问题中的理论基础、算法设计和实际应用，并探讨了最优性-鲁棒性-可扩展性权衡及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 全局求解器作为3D视觉的强大范式，能够为传统上由局部或启发式方法处理的非凸几何优化问题提供可证明的解决方案。然而，该领域缺乏系统性综述，需要统一的理论框架来指导算法选择和未来发展。

Method: 采用系统性综述方法，构建了三种核心范式的综合分类体系：分支定界（BnB）、凸松弛（CR）和渐进非凸性（GNC）。分析了每种范式的理论基础、算法设计和实际增强技术，并考察了它们在十个核心视觉任务中的应用。

Result: 揭示了全局求解器在最优性、鲁棒性和可扩展性之间的权衡关系，为不同应用场景下的求解器选择提供了指导。同时识别了未来关键研究方向，包括保持保证前提下的算法扩展、数据驱动先验与可证明优化的结合、标准化基准的建立以及安全关键部署的社会影响。

Conclusion: 该综述通过整合理论基础、实践进展和更广泛影响，为3D视觉中的全局求解器提供了统一视角和发展路线图，旨在推动可证明、可信赖的感知系统在现实世界应用中的发展。提供了持续更新的文献总结和配套代码教程。

Abstract: Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.

</details>


### [12] [PAct: Part-Decomposed Single-View Articulated Object Generation](https://arxiv.org/abs/2602.14965)
*Qingming Liu,Xinyue Yao,Shuyuan Zhang,Yueci Deng,Guiliang Liu,Zhen Liu,Kui Jia*

Main category: cs.CV

TL;DR: 提出了一种基于部件中心的生成框架，用于从单张图像快速生成带关节的3D资产，避免了传统优化方法耗时的缺点，同时保持了部件结构和运动的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：基于优化的重建方法虽然准确但耗时（数十分钟到数小时），而基于模板检索的推理方法虽然快速但可能无法匹配输入观测的具体结构和外观。需要一种既能快速生成又能保持输入一致性的方法。

Method: 提出部件中心的生成框架，将物体建模为一组可移动部件，每个部件用带有部件身份和关节提示的潜在令牌编码。在单张图像条件下，模型生成保持实例级对应关系的带关节3D资产，同时确保有效的部件结构和运动。

Result: 在常见关节类别（如抽屉和门）上的实验表明，相比基于优化和基于检索的基线方法，该方法在输入一致性、部件准确性和关节合理性方面都有改进，同时显著减少了推理时间。

Conclusion: 该部件中心生成框架避免了逐实例优化，实现了快速前向推理，并支持可控的组装和关节连接，这对于具身交互应用非常重要。

Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.

</details>


### [13] [Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification](https://arxiv.org/abs/2602.13286)
*Nathanya Satriani,Djordje Slijepčević,Markus Schedl,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: 该研究探索了可解释交互学习（XIL）在缓解视觉分类器偏见和虚假相关性方面的能力，特别是在性别分类等易受数据偏见影响的场景中。研究比较了CAIPI、RRR两种XIL策略以及一种新颖的混合方法，发现这些方法能有效引导模型关注相关特征并减少模型偏见，但通常会轻微降低性能（CAIPI除外）。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在视觉分类任务中容易受到数据偏见和虚假相关性的影响，特别是在性别分类等敏感领域。可解释交互学习（XIL）允许用户通过提供对模型解释的反馈来指导模型训练，这为解决偏见问题提供了新的可能性。本研究旨在探索XIL在缓解视觉分类器偏见方面的潜力。

Method: 研究调查了两种方法学上不同的最先进XIL策略：CAIPI和Right for the Right Reasons（RRR），以及一种结合两种策略的新颖混合方法。通过比较使用Gradient-weighted Class Activation Mapping（GradCAM）和Bounded Logit Attention（BLA）生成的解释与分割掩码，对结果进行定量评估。实验在易受数据偏见影响的性别分类场景中进行。

Result: 实验结果表明：（1）这些方法能有效引导机器学习模型关注相关图像特征，特别是使用CAIPI时效果更佳；（2）能够减少模型偏见，即平衡男性和女性预测之间的误分类率；（3）XIL方法在提高性别分类器公平性方面显示出潜力。虽然XIL带来的透明度和公平性提升通常会导致轻微的性能下降，但CAIPI显示出甚至可能提高分类准确性的潜力。

Conclusion: 可解释交互学习（XIL）方法在缓解视觉分类器偏见和提高公平性方面具有显著潜力。CAIPI、RRR及其混合方法都能有效引导模型关注相关特征并减少偏见，其中CAIPI在保持甚至提高分类准确性的同时实现这些改进。这项研究支持了XIL在构建更公平、更透明的机器学习系统方面的应用前景。

Abstract: Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.

</details>


### [14] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本文提出两种基于物理解释的HRRP生成数据评估指标，通过将HRRP数据分解为掩码、特征和噪声三个组件，解决了现有黑盒分类模型评估方法缺乏可解释性和多级评估能力的问题。


<details>
  <summary>Details</summary>
Motivation: 当前HRRP生成数据的评估主要依赖分类模型，这些黑盒模型缺乏可解释性，无法进行多级评估。需要基于HRRP数据的物理特性开发更有效的评估方法。

Method: 将HRRP数据分解为三个组件：掩码、特征和噪声。基于这种分解，提出两种基于物理解释的评估指标，并在一个具有挑战性的任务中使用昂贵的数据集进行验证。

Result: 提出的两种指标在具有挑战性的任务中表现出良好的判别能力，能够有效评估HRRP生成数据的质量。

Conclusion: 通过将HRRP数据分解为掩码、特征和噪声三个组件，并基于物理解释提出评估指标，为HRRP生成数据的评估提供了更可解释和更全面的方法。

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

</details>


### [15] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 该论文研究基于大规模海上数据库的HRRP合成，发现几何因素（船舶尺寸和视角）是影响雷达高分辨率距离像的关键因素，通过条件生成模型能够复现真实数据中的几何趋势。


<details>
  <summary>Details</summary>
Motivation: 高分辨率距离像（HRRP）在雷达自动目标识别中具有快速处理优势，但对采集条件的高度敏感性限制了其在各种操作场景下的鲁棒性。现有研究受限于小型、特定数据集，需要在大规模代表性数据库上研究HRRP合成。

Method: 使用大规模海上数据库，分析发现几何因素（船舶尺寸和期望视角）是主要场景驱动因素。基于这些变量作为条件，训练生成模型进行HRRP合成。

Result: 合成信号能够复现实数据中观察到的视线几何趋势，验证了几何因素在HRRP生成中的核心作用。

Conclusion: 采集几何在鲁棒HRRP生成中起着核心作用，基于几何条件的生成方法能够有效应对海上监视场景的变异性。

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

</details>


### [16] [Spectral Collapse in Diffusion Inversion](https://arxiv.org/abs/2602.13303)
*Nicolas Bourriez,Alexandre Verine,Auguste Genovesio*

Main category: cs.CV

TL;DR: 论文提出正交方差引导(OVG)方法，解决条件扩散反演中源域频谱稀疏时的"频谱坍缩"问题，在保持结构保真度的同时恢复逼真纹理


<details>
  <summary>Details</summary>
Motivation: 标准确定性反演方法在源域频谱稀疏时（如超分辨率、草图转图像）会失败，导致恢复的潜在变量不符合各向同性高斯分布，产生低频信号和纹理贫乏的生成结果，这种现象被称为"频谱坍缩"

Method: 提出正交方差引导(OVG)推理时方法，通过修正ODE动力学，在结构梯度的零空间中强制执行理论高斯噪声幅度，解决结构-纹理权衡问题

Result: 在显微镜超分辨率(BBBC021)和草图转图像(Edges2Shoes)数据集上的实验表明，OVG能有效恢复逼真纹理，同时保持结构保真度

Conclusion: OVG方法成功解决了条件扩散反演中的频谱坍缩问题，平衡了结构保真度和纹理质量，为跨域图像翻译提供了有效解决方案

Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.

</details>


### [17] [Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique](https://arxiv.org/abs/2602.13306)
*Zhehan Zhang,Meihua Qian,Li Luo,Siyu Huang,Chaoyi Zhou,Ripon Saha,Xinxin Song*

Main category: cs.CV

TL;DR: 提出一个基于Qwen2-VL-7B视觉语言模型的多任务学习框架，用于自动化评估人类绘画的创造力，能够同时预测数值评分和生成符合评分标准的解释性反馈。


<details>
  <summary>Details</summary>
Motivation: 艺术创造力评估对创造力研究和艺术教育至关重要，但传统的人工评分方法（如托兰斯创造力测试）在大规模应用时劳动密集。现有的机器学习方法主要依赖图像特征，提供的解释性反馈有限或没有。需要一种既能自动评分又能提供解释性反馈的自动化评估工具。

Method: 使用1000幅人类创作的绘画作品数据集，每幅作品都有1-100分的评分和简短描述。采用Qwen2-VL-7B视觉语言模型进行微调，通过多任务学习框架，在视觉编码器输出上添加轻量级回归头，使模型能够同时预测数值评分和生成符合五维度评分标准（原创性、色彩、纹理、构图、内容）的反馈。通过系统提示嵌入结构化评分标准和作品描述来约束生成文本。

Result: 模型表现出色，在100分制上达到皮尔逊相关系数r > 0.97，平均绝对误差约3.95分。生成的反馈与专家评语语义相似度高（平均SBERT余弦相似度=0.798）。

Conclusion: 该框架成功将计算机视觉与艺术评估相结合，为创造力研究和课堂反馈提供了一个可扩展的工具，既能自动化评分又能提供解释性反馈。

Abstract: Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.

</details>


### [18] [Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture](https://arxiv.org/abs/2602.13322)
*Datorien L. Anderson*

Main category: cs.CV

TL;DR: PSI数据集通过三个诊断性测试验证了Eidos架构在保持拓扑不变性方面的能力，证明其泛化能力源于几何完整性而非统计规模


<details>
  <summary>Details</summary>
Motivation: 标准视觉基准测试中纹理相关性占主导地位，难以分离拓扑不变性（结构在仿射变换下的保持能力），需要专门的诊断基准来验证"形式优先"假设

Method: 创建PolyShapes-Ideal (PSI)诊断基准套件，包含三个诊断探针：噪声下的多边形分类、MNIST的零样本字体迁移、渐进变形下的几何塌陷映射

Result: Eidos架构在PSI上达到>99%准确率，在30种未见字体上实现81.67%的零样本迁移，无需预训练

Conclusion: 结果验证了"形式优先"假设：结构约束架构的泛化能力是几何完整性的属性，而非统计规模

Abstract: We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.

</details>


### [19] [Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators](https://arxiv.org/abs/2602.13334)
*Hao Liu,Suhaib A. Fahmy*

Main category: cs.CV

TL;DR: 提出了一种协作推理框架，在边缘设备部署轻量级通用ViT，在近边缘加速器部署多个中型专家ViT，通过路由机制动态选择专家，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署视觉Transformer面临计算复杂度高的问题，而完全卸载到云端则存在显著的延迟开销，需要一种平衡的解决方案。

Method: 1) 协作推理框架：边缘设备部署轻量级通用ViT，近边缘加速器部署多个中型专家ViT；2) 路由机制：基于边缘模型的Top-k预测动态选择最相关专家；3) 渐进式专家训练策略：增强专家在数据集子集上的准确性。

Result: 在CIFAR-100数据集和真实边缘/近边缘测试平台上，训练策略将专家在目标子集上的准确率提升4.12%，整体准确率比静态专家提高2.76%；延迟比边缘执行降低45%，能耗比仅近边缘卸载降低46%。

Conclusion: 提出的协作推理框架有效平衡了边缘计算和云端卸载，通过动态专家选择和渐进式训练策略，在保持准确性的同时显著降低了延迟和能耗。

Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.

</details>


### [20] [LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery](https://arxiv.org/abs/2602.13378)
*Sohail Ali Farooqui,Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: LAF-YOLOv10是基于YOLOv10n改进的无人机图像小目标检测模型，通过四个互补技术模块提升检测性能，在VisDrone数据集上达到35.1% mAP，参数仅2.3M，适合嵌入式部署。


<details>
  <summary>Details</summary>
Motivation: 无人机作为主要感知平台，在监控、交通监测和灾难响应中发挥重要作用。当前检测器面临无人机特有的挑战：目标尺寸小（仅几个像素）、背景杂乱、严重遮挡以及严格的机载计算资源限制。

Method: 1. PC-C2f模块：限制空间卷积到1/4骨干通道，减少冗余计算同时保持判别能力
2. AG-FPN模块：在跨尺度融合前插入SE通道门控，用DySample替换最近邻上采样
3. 辅助P2检测头：在160×160分辨率增加检测头，提升8×8像素以下小目标检测
4. Wise-IoU v3：替换CIoU用于边界框回归，减少标注噪声影响

Result: 在VisDrone-DET2019数据集上达到35.1±0.3% mAP@0.5，比YOLOv10n提升3.3个百分点；在UAVDT数据集上达到35.8±0.4% mAP@0.5；在NVIDIA Jetson Orin Nano上实现24.3 FPS（FP16精度），适合嵌入式部署。

Conclusion: LAF-YOLOv10通过四个互补模块的联合集成，有效解决了无人机图像小目标检测的多个瓶颈问题，在保持轻量化的同时显著提升了检测性能，证明了其在嵌入式无人机部署中的可行性。

Abstract: Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.

</details>


### [21] [SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning](https://arxiv.org/abs/2602.13515)
*Jintao Zhang,Kai Jiang,Chendong Xiang,Weiqi Feng,Yuezhou Hu,Haocheng Xi,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: SpargeAttention2是一种可训练的稀疏注意力方法，通过混合掩码规则和蒸馏式微调目标，在视频扩散模型中实现95%的注意力稀疏度和16.2倍的注意力加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有训练免费稀疏注意力方法虽然有效，但可训练稀疏注意力可以进一步提高稀疏度同时保持生成质量。研究需要解决三个关键问题：Top-k和Top-p掩码规则的失效情况、可训练稀疏注意力为何能达到更高稀疏度、以及扩散损失微调的局限性。

Method: 提出SpargeAttention2方法，包含三个核心组件：(1) 结合Top-k和Top-p的混合掩码规则，在高稀疏度下提供更鲁棒的掩码；(2) 高效的可训练稀疏注意力实现；(3) 蒸馏启发的微调目标，在稀疏注意力微调过程中更好地保持生成质量。

Result: 在视频扩散模型实验中，SpargeAttention2达到95%的注意力稀疏度和16.2倍的注意力加速，同时保持生成质量，持续优于先前的稀疏注意力方法。

Conclusion: SpargeAttention2通过解决现有稀疏注意力方法的三个关键问题，实现了高稀疏度下的高效加速，同时保持了生成质量，为扩散模型加速提供了有效的可训练稀疏注意力解决方案。

Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.

</details>


### [22] [Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images](https://arxiv.org/abs/2602.13712)
*Chan Hao Sien,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CV

TL;DR: 使用微调的视觉语言模型（Microsoft Florence）定位显微镜图像中的寄生虫卵，性能优于传统目标检测方法，mIOU达到0.94，为自动化寄生虫诊断提供可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 土壤传播蠕虫感染在热带和亚热带地区持续影响大量人口，这些地区缺乏专业诊断专家。传统手动显微镜诊断方法劳动密集、耗时且易出错，需要自动化解决方案。

Method: 采用微调的视觉语言模型（如Microsoft Florence）来定位显微镜图像中的所有寄生虫卵，并与EfficientDet等其他目标检测方法进行比较。

Result: 定位视觉语言模型的性能优于其他目标检测方法，mIOU达到0.94，显示出在寄生虫卵检测方面的优越表现。

Conclusion: 提出的视觉语言模型有潜力作为自动化框架的核心组件，为智能寄生虫学诊断提供可扩展的工程解决方案。

Abstract: Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.

</details>


### [23] [VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer](https://arxiv.org/abs/2602.13818)
*Zongcheng Han,Dongyan Cao,Haoran Sun,Yu Hong*

Main category: cs.CV

TL;DR: VAR-3D提出了一种基于视图感知的3D向量量化自编码器和渲染监督训练策略，用于改进文本到3D生成的质量和文本对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于自回归变换器的文本到3D生成面临两个主要挑战：1）在离散3D表示学习中存在信息损失，导致量化前的表示失真；2）向量量化进一步放大失真，降低几何一致性；3）传统的两阶段训练范式在重建和文本条件生成之间存在目标不匹配。

Method: 提出了VAR-3D框架，包含两个核心组件：1）视图感知的3D向量量化变分自编码器，将复杂3D几何结构转换为离散标记；2）渲染监督训练策略，将离散标记预测与视觉重建耦合，鼓励生成过程更好地保持视觉保真度和结构一致性。

Result: 实验表明VAR-3D在生成质量和文本-3D对齐方面显著优于现有方法。

Conclusion: VAR-3D通过视图感知的3D表示学习和渲染监督训练，有效解决了文本到3D生成中的信息损失和目标不匹配问题，实现了更高质量的3D生成和更好的文本对齐。

Abstract: Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.

</details>


### [24] [Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification](https://arxiv.org/abs/2602.13889)
*Daniel Chen,Zaria Zinn,Marcus Lowe*

Main category: cs.CV

TL;DR: 提出一个字体分类系统，使用LoRA微调DINOv2 Vision Transformer，仅训练不到1%的参数就能在394种字体上达到约86%的top-1准确率，并开源了完整训练流程。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够从渲染文本图像中准确识别大量字体家族的实用系统，解决字体分类中的泛化性和部署问题。

Method: 使用LoRA（低秩适应）微调DINOv2 Vision Transformer；构建合成数据集生成管道，大规模渲染Google字体并应用多种增强（随机颜色、对齐、换行、高斯噪声）；内置预处理确保训练和推理一致性；部署为HuggingFace推理端点。

Result: 在394种字体家族分类任务上达到约86%的top-1准确率，仅训练了模型87.2M参数中不到1%的参数（约0.87M），实现了高效微调。

Conclusion: 成功开发了一个高效、可泛化的字体分类系统，通过LoRA微调和合成数据增强实现了高性能，并将模型、数据集和训练管道开源，为字体识别研究提供了实用资源。

Abstract: We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.

</details>


### [25] [DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors](https://arxiv.org/abs/2602.14134)
*Yi Li,Hongze Shen,Lexiang Tang,Xin Li,Xinpeng Ding,Yinsong Liu,Deqiang Jiang,Xing Sun,Xiaomeng Li*

Main category: cs.CV

TL;DR: DenseMLLM：一种无需任务特定解码器的多模态大语言模型，通过视觉token监督策略实现密集预测任务


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在细粒度密集预测任务（如语义分割、深度估计）中需要复杂的任务特定解码器，这增加了模型复杂性，偏离了MLLMs的通用设计理念，限制了其实用性

Method: 提出DenseMLLM模型，基于标准架构，采用新颖的视觉token监督策略来处理多标签和多任务，无需额外的任务特定解码器

Result: 模型在广泛的密集预测和视觉语言基准测试中取得了高度竞争力的性能

Conclusion: 标准的通用MLLM无需架构专业化即可有效支持密集感知任务，挑战了传统需要任务特定解码器的范式

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

</details>


### [26] [When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance](https://arxiv.org/abs/2602.14157)
*Ahmed Ghorbel,Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 基于Moufad等人(2025)的工作，本文扩展了无需向量-雅可比乘积(VJP)计算的测试时引导方法，在大规模图像和视频编辑基准上取得了与训练方法相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 文本驱动的图像和视频编辑可以自然地转化为修复问题，但现有方法依赖昂贵的向量-雅可比乘积(VJP)计算来近似难以处理的引导项，限制了实际应用。

Method: 基于Moufad等人(2025)的VJP-free近似方法，提供理论分析并大幅扩展其在大规模图像和视频编辑基准上的实证评估。

Result: 测试时引导方法单独就能达到与训练方法相当的性能，在某些情况下甚至超越训练方法。

Conclusion: 测试时引导方法在无需昂贵VJP计算的情况下，能够有效实现文本驱动的图像和视频编辑，为实际应用提供了更可行的解决方案。

Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

</details>


### [27] [Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236)
*Vishnu Sai,Dheeraj Sai,Srinath B,Girish Varma,Priyesh Shukla*

Main category: cs.CV

TL;DR: Sali-Cache通过先验优化框架解决VLMs处理长视频时的KV缓存内存瓶颈，使用时空双信号自适应缓存，在保持100%准确率的同时实现2.20倍内存压缩


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型处理长视频内容时面临关键的内存瓶颈，KV缓存随序列长度线性增长。现有解决方案主要采用反应式淘汰策略，在计算完整注意力矩阵后丢弃token，造成大量计算浪费。

Method: 提出Sali-Cache先验优化框架，通过主动内存管理实现双信号自适应缓存：1)基于光流分析的时间滤波器检测帧间冗余；2)基于显著性检测的空间滤波器识别视觉显著区域。在进入计算昂贵的注意力操作前智能管理内存分配。

Result: 在LLaVA 1.6架构上的实验评估显示，该方法在保持BLEU、ROUGE-L和Exact Match指标100%准确率的同时，实现了有效内存使用2.20倍的压缩比。在相同内存预算约束下，Sali-Cache能在不降低模型性能的情况下，在更长时间内保留上下文丰富的特征。

Conclusion: Sali-Cache通过先验优化和自适应缓存管理，使视觉语言模型能够在消费级硬件上高效处理长视频内容，解决了KV缓存的内存瓶颈问题，同时保持模型性能不变。

Abstract: Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>


### [28] [VariViT: A Vision Transformer for Variable Image Sizes](https://arxiv.org/abs/2602.14615)
*Aswathi Varma,Suprosanna Shit,Chinmay Prabhakar,Daniel Scholz,Hongwei Bran Li,Bjoern Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: VariViT是一种改进的Vision Transformer模型，专门设计用于处理可变尺寸的医学图像，同时保持一致的patch大小，解决了传统ViT在医学影像中固定尺寸限制的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformers需要将图像分割为固定大小的patch，这在医学影像中存在问题：1）不规则形状结构（如肿瘤）需要固定边界框裁剪，导致前景背景比例高度可变；2）调整医学图像尺寸会降低信息质量并引入伪影；3）大图像计算成本高，小图像又可能丢失信息。需要一种能处理可变图像尺寸的ViT变体。

Method: 提出VariViT模型：1）采用新颖的位置嵌入调整方案，适应可变数量的patch；2）实现新的批处理策略以减少计算复杂度；3）保持一致的patch大小，但允许输入图像尺寸可变。

Result: 在两个3D脑部MRI数据集上的评估显示：1）在胶质瘤基因型预测和脑肿瘤分类任务中，VariViT超越了传统ViT和ResNet；2）分别达到75.5%和76.3%的F1分数；3）学习到更具区分性的特征；4）提出的批处理策略相比传统架构减少高达30%的计算时间。

Conclusion: VariViT在图像表示学习中表现出高效性，特别适合医学影像分析，能够处理可变尺寸输入同时保持计算效率，为医学图像分析提供了有前景的解决方案。

Abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit

</details>


### [29] [Multi-dimensional Persistent Sheaf Laplacians for Image Analysis](https://arxiv.org/abs/2602.14846)
*Xiang Xiang Wang,Guo-Wei Wei*

Main category: cs.CV

TL;DR: 提出基于单纯复形的多维持久层拉普拉斯框架用于图像分析，通过利用多个降维维度的互补优势，避免传统降维方法对维度选择的敏感性，在COIL20和ETH80数据集上表现出更稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法（如主成分分析）对降维维度的选择非常敏感，单一维度选择或跨维度平均都无法充分利用不同维度的互补信息，需要一种更稳定的多维度表示方法。

Method: 将图像样本视为单纯复形，在每个维度上使用持久层拉普拉斯算子提取多尺度局部拓扑谱表示，然后将这些谱的统计摘要跨尺度和维度聚合，形成多尺度多维图像表示。

Result: 在COIL20和ETH80图像数据集上的实验表明，该方法在广泛的降维维度范围内提供更稳定的性能，并在中等维度范围内相对于基于PCA的基线方法获得一致的改进。

Conclusion: 提出的多维持久层拉普拉斯框架通过利用多个降维维度的互补优势，有效解决了传统降维方法对维度选择的敏感性问题，为图像分析提供了更稳定和有效的表示方法。

Abstract: We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.

</details>


### [30] [ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery](https://arxiv.org/abs/2602.14989)
*Ayush Shrivastava,Kirtan Gangani,Laksh Jain,Mayank Goel,Nipun Batra*

Main category: cs.CV

TL;DR: ThermEval-B是一个包含约55,000个热成像视觉问答对的基准测试，用于评估热成像视觉语言理解能力，填补了现有RGB中心模型在热成像理解上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在RGB图像上表现良好，但在热成像图像上泛化能力差。热成像在可见光失效的场景中至关重要，如夜间监控、搜救、自动驾驶和医疗筛查。热成像编码的是物理温度而非颜色或纹理，需要现有RGB基准测试无法评估的感知和推理能力。

Method: 提出了ThermEval-B基准测试，整合了公共数据集和新收集的ThermEval-D数据集。ThermEval-D是首个提供密集逐像素温度图和语义身体部位标注的数据集，涵盖多样化的室内外环境。评估了25个开源和闭源视觉语言模型。

Result: 模型在温度基础推理方面持续失败，在色彩映射变换下性能下降，倾向于依赖语言先验或固定响应。提示工程或监督微调仅带来边际改进。热成像理解需要专门的评估，超越RGB中心假设。

Conclusion: ThermEval-B作为一个基准测试，旨在推动热成像视觉语言建模的进展，证明了热成像理解需要超越RGB中心假设的专门评估方法。

Abstract: Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [31] [UAVGENT: A Language-Guided Distributed Control Framework](https://arxiv.org/abs/2602.13212)
*Ziyi Zhang,Xiyu Deng,Guannan Qu,Yorie Nakahira*

Main category: cs.RO

TL;DR: 提出了一种用于多无人机系统的语言在环控制架构，结合自然语言指令、LLM监督和分布式内环控制，在物理层保持形式化鲁棒性保证


<details>
  <summary>Details</summary>
Motivation: 研究如何在多无人机系统中实现语言在环控制，使系统能够执行不断演化的高层任务，同时在物理层保持形式化鲁棒性保证，解决语言指令与分布式控制系统之间的集成问题

Method: 提出三层架构：1) 人类操作员发出自然语言指令；2) LLM监督器定期解释、验证和修正指令任务；3) 分布式内环控制器仅使用本地相对信息跟踪生成的参考轨迹

Result: 推导了在有界扰动和LLM更新引起的分段光滑参考轨迹下的跟踪性能理论保证，展示了集中式语言任务推理与分布式反馈控制相结合实现复杂行为的可行性

Conclusion: 通过将集中式基于语言的任务推理与分布式反馈控制相结合，可以实现具有可证明鲁棒性和稳定性的复杂行为，为多无人机系统的语言在环控制提供了理论框架

Abstract: We study language-in-the-loop control for multi-drone systems that execute evolving, high-level missions while retaining formal robustness guarantees at the physical layer. We propose a three-layer architecture in which (i) a human operator issues natural-language instructions, (ii) an LLM-based supervisor periodically interprets, verifies, and corrects the commanded task in the context of the latest state and target estimates, and (iii) a distributed inner-loop controller tracks the resulting reference using only local relative information. We derive a theoretical guarantee that characterizes tracking performance under bounded disturbances and piecewise-smooth references with discrete jumps induced by LLM updates. Overall, our results illustrate how centralized language-based task reasoning can be combined with distributed feedback control to achieve complex behaviors with provable robustness and stability.

</details>


### [32] [DORA: Dataflow Oriented Robotic Architecture](https://arxiv.org/abs/2602.13252)
*Xiaodong Zhang,Baorui Lv,Xavier Tao,Xiong Wang,Jie Bao,Yong He,Yue Chen,Zijiang Yang*

Main category: cs.RO

TL;DR: DORA是一种面向数据流的机器人架构，通过显式数据依赖规范和零拷贝数据传输，解决了现有机器人中间件在大数据量处理和异构数据支持方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件存在两大问题：1）严重依赖（反）序列化机制，处理大数据时带来显著开销；2）缺乏对异构数据大小的高效灵活支持，特别是在机器人内部通信和Python执行环境中。这些问题直接影响数据密集型机器人应用（尤其是工业场景）的响应性、稳定性和整体生产力。

Method: 提出数据流导向的机器人架构（DORA），支持显式数据依赖规范和高效的零拷贝数据传输。将框架实现为开源系统，并在仿真和真实机器人环境中进行广泛实验评估。

Result: 实验结果表明，与最先进的中间件相比，DORA在延迟和CPU开销方面实现了显著降低。

Conclusion: DORA通过数据流导向的方法有效解决了现有机器人中间件在大数据处理和异构数据支持方面的性能瓶颈，为数据密集型机器人应用提供了更高效的通信基础设施。

Abstract: Robotic middleware serves as the foundational infrastructure, enabling complex robotic systems to operate in a coordinated and modular manner. In data-intensive robotic applications, especially in industrial scenarios, communication efficiency directly impact system responsiveness, stability, and overall productivity. However, existing robotic middleware exhibit several limitations: (1) they rely heavily on (de)serialization mechanisms, introducing significant overhead for large-sized data; (2) they lack efficient and flexible support for heterogeneous data sizes, particularly in intra-robot communication and Python-based execution environments. To address these challenges, we propose Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. We implement the proposed framework as an open-source system and evaluate it through extensive experiments in both simulation and real-world robotic environments. Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.

</details>


### [33] [High-Fidelity, Customizable Force Sensing for the Wearable Human-Robot Interface](https://arxiv.org/abs/2602.13436)
*Noah Rubin,Ava Schraeder,Hrishikesh Sahu,Thomas C. Bulea,Lillian Chin*

Main category: cs.RO

TL;DR: 研究人员开发了一种基于流体神经支配的3D打印硅胶垫传感器，用于测量人机界面的机械交互力，该传感器通过嵌入式空气通道压缩产生的压力变化来测量力，在多种测试中表现出良好的线性关系和相关性。


<details>
  <summary>Details</summary>
Motivation: 人机界面的机械特性表征对于理解用户行为和优化可穿戴机器人性能至关重要，但由于制造复杂性和传感器非线性响应，这一界面一直难以实现有效传感。

Method: 采用流体神经支配方法，创建带有嵌入式空气通道的3D打印硅胶垫，当力施加到垫子上时，空气通道压缩产生可测量的压力变化，使用现成的压力传感器进行测量。

Result: 台架测试显示垫子压力与施加力高度线性相关（R²=0.998）；临床测力计测试显示膝上压力与屈曲扭矩高度相关（R²=0.95），膝下压力与伸展扭矩高度相关（R²=0.75）；在肱二头肌测试中观察到压力与肘关节角度的相关性；集成到外骨骼后能稳定跟踪深蹲相位和任务动态。

Conclusion: 流体神经支配是一种易于定制的传感方式，具有高信噪比和时间分辨率，可用于捕捉人机机械交互，长期来看可能为可穿戴机器人系统的实时控制/优化提供替代传感输入。

Abstract: Mechanically characterizing the human-machine interface is essential to understanding user behavior and optimizing wearable robot performance. This interface has been challenging to sensorize due to manufacturing complexity and non-linear sensor responses. Here, we measure human limb-device interaction via fluidic innervation, creating a 3D-printed silicone pad with embedded air channels to measure forces. As forces are applied to the pad, the air channels compress, resulting in a pressure change measurable by off-the-shelf pressure transducers. We demonstrate in benchtop testing that pad pressure is highly linearly related to applied force ($R^2 = 0.998$). This is confirmed with clinical dynamometer correlations with isometric knee torque, where above-knee pressure was highly correlated with flexion torque ($R^2 = 0.95$), while below-knee pressure was highly correlated with extension torque ($R^2 = 0.75$). We build on these idealized settings to test pad performance in more unconstrained settings. We place the pad over \textit{biceps brachii} during cyclic curls and stepwise isometric holds, observing a correlation between pressure and elbow angle. Finally, we integrated the sensor into the strap of a lower-extremity robotic exoskeleton and recorded pad pressure during repeated squats with the device unpowered. Pad pressure tracked squat phase and overall task dynamics consistently. Overall, our preliminary results suggest fluidic innervation is a readily customizable sensing modality with high signal-to-noise ratio and temporal resolution for capturing human-machine mechanical interaction. In the long-term, this modality may provide an alternative real-time sensing input to control / optimize wearable robotic systems and to capture user function during device use.

</details>


### [34] [FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation](https://arxiv.org/abs/2602.13444)
*Huajian Zeng,Lingyun Chen,Jiaqi Yang,Yuantai Zhang,Fan Shi,Peidong Liu,Xingxing Zuo*

Main category: cs.RO

TL;DR: FlowHOI是一个两阶段流匹配框架，用于生成语义基础、时间连贯的手-物交互序列，包括手部姿态、物体姿态和接触状态，基于第一人称观察、语言指令和3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型虽然能生成合理的末端执行器运动，但在长时程、接触丰富的任务中常常失败，因为缺乏明确的手-物交互结构表示。需要一个与具体机器人无关的交互表示来使操作行为更容易验证和跨机器人迁移。

Method: 提出FlowHOI两阶段框架：1）将几何中心的抓取与语义中心的操控解耦；2）使用紧凑的3D场景标记和运动-文本对齐损失来语义地基础化生成的交互；3）引入重建流程从大规模第一人称视频中恢复对齐的手-物轨迹和网格，为鲁棒生成提供HOI先验。

Result: 在GRAB和HOT3D基准测试中，FlowHOI实现了最高的动作识别准确率，比最强的基于扩散的基线高出1.7倍的物理模拟成功率，同时推理速度提升40倍。在四个灵巧操作任务中展示了真实机器人执行，验证了生成的HOI表示可以重定向到真实机器人执行流程。

Conclusion: FlowHOI通过显式表示手-物交互结构，成功生成了语义基础、时间连贯的交互序列，显著提升了长时程、接触丰富任务的表现，并展示了从生成到真实机器人执行的可行性。

Abstract: Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.

</details>


### [35] [AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge](https://arxiv.org/abs/2602.13476)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: AsyncVLA是一个异步控制框架，通过将大型基础模型的语义推理与轻量级边缘执行器解耦，解决了机器人基础模型在动态环境中因高延迟导致的实时控制问题。


<details>
  <summary>Details</summary>
Motivation: 机器人基础模型虽然通过互联网规模的视觉语言表示实现了强大的泛化能力，但其巨大的计算成本导致高推理延迟。在动态环境中，这种延迟会破坏控制回路，使得强大的模型无法安全地实时部署。

Method: AsyncVLA采用异步控制框架，将大型基础模型部署在远程工作站提供高级指导，同时轻量级的板载边缘适配器以高频率持续优化动作。通过端到端微调协议和轨迹重加权策略来弥合异步流之间的领域差距。

Result: 在通信延迟高达6秒的真实世界视觉导航任务中，AsyncVLA比最先进的基线方法实现了40%更高的成功率。

Conclusion: AsyncVLA有效地弥合了大型模型的语义智能与边缘机器人所需的反应性之间的差距，为动态环境中的实时机器人控制提供了可行的解决方案。

Abstract: Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.

</details>


### [36] [ONRAP: Occupancy-driven Noise-Resilient Autonomous Path Planning](https://arxiv.org/abs/2602.13577)
*Faizan M. Tariq,Avinash Singh,Vipul Ramtekkar,Jovin D'sa,David Isele,Yosuke Sakamoto,Sangjae Bae*

Main category: cs.RO

TL;DR: 提出了一种基于占用网格的动态路径规划方法，通过非线性规划生成满足运动学约束的安全路径，能够处理感知噪声和不确定性，实时运行且无需复杂调参。


<details>
  <summary>Details</summary>
Motivation: 动态路径规划需要在存在感知噪声、定位不确定性和不完全语义感知的情况下保持可靠性，需要一种实用、实现友好的规划器来安全导航静态和动态障碍物。

Method: 基于占用网格的规划方法，可选地结合占用流预测，使用改进的自行车模型构建空间域非线性规划，包含显式可行性和避碰惩罚，在占用空间中处理未知障碍物类别和异构智能体运动。

Result: 方法实时运行（平均快于10Hz），需要最小调参，在模拟中验证了在严重定位和感知噪声下的鲁棒性，在F1TENTH平台上展示了通过狭窄通道和粗糙路线的平滑安全机动。

Conclusion: 该方法为噪声弹性和预测感知规划提供了鲁棒基础，消除了对手工启发式规则的需求，能够可靠处理感知不确定性并生成安全可行的路径。

Abstract: Dynamic path planning must remain reliable in the presence of sensing noise, uncertain localization, and incomplete semantic perception. We propose a practical, implementation-friendly planner that operates on occupancy grids and optionally incorporates occupancy-flow predictions to generate ego-centric, kinematically feasible paths that safely navigate through static and dynamic obstacles. The core is a nonlinear program in the spatial domain built on a modified bicycle model with explicit feasibility and collision-avoidance penalties. The formulation naturally handles unknown obstacle classes and heterogeneous agent motion by operating purely in occupancy space. The pipeline runs in real-time (faster than 10 Hz on average), requires minimal tuning, and interfaces cleanly with standard control stacks. We validate our approach in simulation with severe localization and perception noises, and on an F1TENTH platform, demonstrating smooth and safe maneuvering through narrow passages and rough routes. The approach provides a robust foundation for noise-resilient, prediction-aware planning, eliminating the need for handcrafted heuristics. The project website can be accessed at https://honda-research-institute.github.io/onrap/

</details>


### [37] [TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment](https://arxiv.org/abs/2602.13579)
*Youngsun Wi,Jessica Yin,Elvis Xiang,Akash Sharma,Jitendra Malik,Mustafa Mukadam,Nima Fazeli,Tess Hellebrekers*

Main category: cs.RO

TL;DR: TactAlign是一种跨具身触觉对齐方法，可将人类穿戴设备收集的触觉信号转移到不同具身的机器人上，无需配对数据或手动标注，通过整流流实现共享潜在表示。


<details>
  <summary>Details</summary>
Motivation: 人类通过可穿戴设备（如触觉手套）收集的演示为策略学习提供了快速灵巧的监督，但如何将人类收集的触觉信号转移到不同传感模态和具身的机器人上是一个关键挑战。现有方法通常假设相同触觉传感器、需要配对数据且具身差距小，限制了可扩展性和通用性。

Method: 提出TactAlign方法，使用整流流将人类和机器人的触觉观察转换为共享潜在表示，无需配对数据集、手动标签或特权信息。通过手-物体交互衍生的伪对实现低成本的潜在传输。

Result: TactAlign在多个接触密集任务（旋转、插入、盖子闭合）中改进了人机策略转移，能够泛化到未见过的物体和任务（使用少于5分钟的人类数据），并在高度灵巧任务（拧灯泡）上实现零样本人机转移。

Conclusion: TactAlign提供了一种有效的跨具身触觉对齐方法，能够将人类触觉信号转移到不同具身的机器人上，无需配对数据，提高了人机策略转移的可扩展性和通用性。

Abstract: Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).

</details>


### [38] [Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation](https://arxiv.org/abs/2602.13640)
*Siyuan Li,Jiani Lu,Yu Song,Xianren Li,Bo An,Peng Liu*

Main category: cs.RO

TL;DR: 本文提出了一种分层表示融合框架，通过渐进式整合音频、视觉和本体感知来实现精确的机器人操作，特别针对接触驱动的声学信号设计了非对称融合结构。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法主要依赖视觉和本体感知，在部分可观测的真实环境中难以推断接触相关的交互状态。声学线索自然编码了接触过程中的丰富交互动态，但在当前多模态融合研究中未得到充分利用。大多数多模态融合方法假设各模态作用同质，设计了平坦对称的融合结构，这不适合稀疏且接触驱动的声学信号。

Method: 提出分层表示融合框架，首先将视觉和本体感知表示基于声学线索进行条件化，然后显式建模高阶跨模态交互以捕捉模态间的互补依赖关系。融合表示被基于扩散的策略利用，直接从多模态观测生成连续的机器人动作。端到端学习和分层融合结构的结合使策略能够利用任务相关的声学信息，同时减轻信息较少模态的干扰。

Result: 在真实世界机器人操作任务（包括液体倾倒和柜门开启）上评估，实验结果表明该方法始终优于最先进的多模态融合框架，特别是在声学线索提供视觉观测不易获得的任务相关信息时表现更佳。通过互信息分析解释了声学线索在多模态融合中对机器人操作的影响。

Conclusion: 提出的分层融合框架能够有效利用声学线索增强机器人操作性能，特别适合处理接触驱动的交互任务。该方法突破了传统对称融合结构的限制，为多模态感知在机器人操作中的应用提供了新思路。

Abstract: Existing robotic manipulation methods primarily rely on visual and proprioceptive observations, which may struggle to infer contact-related interaction states in partially observable real-world environments. Acoustic cues, by contrast, naturally encode rich interaction dynamics during contact, yet remain underexploited in current multimodal fusion literature. Most multimodal fusion approaches implicitly assume homogeneous roles across modalities, and thus design flat and symmetric fusion structures. However, this assumption is ill-suited for acoustic signals, which are inherently sparse and contact-driven. To achieve precise robotic manipulation through acoustic-informed perception, we propose a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. Our approach first conditions visual and proprioceptive representations on acoustic cues, and then explicitly models higher-order cross-modal interactions to capture complementary dependencies among modalities. The fused representation is leveraged by a diffusion-based policy to directly generate continuous robot actions from multimodal observations. The combination of end-to-end learning and hierarchical fusion structure enables the policy to exploit task-relevant acoustic information while mitigating interference from less informative modalities. The proposed method has been evaluated on real-world robotic manipulation tasks, including liquid pouring and cabinet opening. Extensive experiment results demonstrate that our approach consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not readily available from visual observations alone. Furthermore, a mutual information analysis is conducted to interpret the effect of audio cues in robotic manipulation via multimodal fusion.

</details>


### [39] [SPLIT: Sparse Incremental Learning of Error Dynamics for Control-Oriented Modeling in Autonomous Vehicles](https://arxiv.org/abs/2602.13641)
*Yaoyu Li,Chaosheng Huang,Jun Li*

Main category: cs.RO

TL;DR: SPLIT是一个用于控制导向车辆动力学建模的稀疏增量学习框架，通过模型分解、局部增量学习和GP稀疏化解决传统GP残差模型的高维、高计算复杂度和在线学习效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆控制需要准确、计算高效且自适应的车辆模型。传统混合模型结合名义模型和GP残差模型存在维度灾难、评估复杂度高和在线学习效率低的问题，阻碍了实时车辆控制器的部署。

Method: SPLIT框架包含三个关键创新：(1) 模型分解：将车辆模型分解为实验校准的不变元素和残差模型补偿的变体元素，降低特征维度；(2) 局部增量学习：在特征空间中定义有效区域并划分为子区域，实现流数据的高效在线学习；(3) GP稀疏化：使用贝叶斯委员会机制确保可扩展的在线评估。

Result: 在激进仿真和实车实验中，SPLIT集成到基于模型的控制器中，显著提高了模型准确性和控制性能，能够快速适应车辆动力学偏差，并对未见场景表现出鲁棒的泛化能力。

Conclusion: SPLIT框架成功解决了传统GP残差模型在实时车辆控制中的部署障碍，实现了高效、准确且自适应的车辆动力学建模，为自动驾驶控制提供了实用的解决方案。

Abstract: Accurate, computationally efficient, and adaptive vehicle models are essential for autonomous vehicle control. Hybrid models that combine a nominal model with a Gaussian Process (GP)-based residual model have emerged as a promising approach. However, the GP-based residual model suffers from the curse of dimensionality, high evaluation complexity, and the inefficiency of online learning, which impede the deployment in real-time vehicle controllers. To address these challenges, we propose SPLIT, a sparse incremental learning framework for control-oriented vehicle dynamics modeling. SPLIT integrates three key innovations: (i) Model Decomposition. We decompose the vehicle model into invariant elements calibrated by experiments, and variant elements compensated by the residual model to reduce feature dimensionality. (ii) Local Incremental Learning. We define the valid region in the feature space and partition it into subregions, enabling efficient online learning from streaming data. (iii) GP Sparsification. We use bayesian committee machine to ensure scalable online evaluation. Integrated into model-based controllers, SPLIT is evaluated in aggressive simulations and real-vehicle experiments. Results demonstrate that SPLIT improves model accuracy and control performance online. Moreover, it enables rapid adaptation to vehicle dynamics deviations and exhibits robust generalization to previously unseen scenarios.

</details>


### [40] [A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking](https://arxiv.org/abs/2602.13656)
*Zhongxiang Lei,Lulu Cao,Xuyang Wang,Tianyi Qian,Jinyan Liu,Xuesong Li*

Main category: cs.RO

TL;DR: 本文提出了KungFuAthlete数据集和统一训练框架，用于解决人形机器人执行高动态武术动作时的跟踪与摔倒恢复问题。


<details>
  <summary>Details</summary>
Motivation: 当前人形运动跟踪系统在硬件性能极限和算法鲁棒性边界存在不足。武术作为高动态人体运动的极端案例，缺乏专门的数据集。同时，现有研究假设运动始终处于安全状态，缺乏对不安全状态建模和自主恢复的统一策略。

Method: 构建KungFuAthlete高动态武术运动数据集，包含地面和跳跃子集。提出新颖的训练范式，使单一策略能够联合学习高动态运动跟踪和摔倒恢复，将敏捷执行与稳定化统一在一个框架内。

Result: KungFuAthlete数据集的跳跃子集在关节、线性和角速度方面显著高于LAFAN1、PHUMA和AMASS等常用数据集，表明运动强度和复杂性大幅增加。提出的框架将机器人能力从纯运动跟踪扩展到支持恢复的执行。

Conclusion: 该框架促进了人形机器人在现实世界高动态场景中更鲁棒和自主的性能，实现了从纯运动跟踪到恢复支持执行的扩展。

Abstract: Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes' daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.

</details>


### [41] [Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation](https://arxiv.org/abs/2602.13689)
*Wonju Lee,Matteo Grimaldi,Tao Yu*

Main category: cs.RO

TL;DR: 本文提出了一种用于视觉-触觉融合的跨模态Transformer（CMT），通过结构化自注意力和交叉注意力整合手腕摄像头观测与触觉信号，并引入物理信息正则化来稳定触觉嵌入，在插入任务中实现了96.59%的成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人插入任务需要精确的接触丰富交互，仅靠视觉无法解决。虽然触觉反馈具有直观价值，但现有研究表明简单的视觉-触觉融合往往无法带来一致的改进。

Method: 提出跨模态Transformer（CMT）用于视觉-触觉融合，通过结构化自注意力和交叉注意力整合手腕摄像头观测与触觉信号。进一步引入物理信息正则化，鼓励双边力平衡，反映人类运动控制原理。

Result: 在TacSL基准测试中，带有对称正则化的CMT实现了96.59%的插入成功率，超过了简单融合和门控融合基线，并接近特权配置"手腕+接触力"的96.09%性能。

Conclusion: 触觉传感对于精确对齐是不可或缺的，而原则性的多模态融合加上物理信息正则化能够解锁视觉和触觉的互补优势，在现实传感条件下接近特权性能。

Abstract: Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing naïve and gated fusion baselines and closely matching the privileged "wrist + contact force" configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.

</details>


### [42] [XIT: Exploration and Exploitation Informed Trees for Active Gas Distribution Mapping in Unknown Environments](https://arxiv.org/abs/2602.13739)
*Mal Fazliu,Matthew Coombes,Sen Wang,Cunjia Liu*

Main category: cs.RO

TL;DR: 提出XIT算法用于移动机器人自主气体分布建图，通过采样规划平衡探索与利用，结合气体前沿概念提升建图效率


<details>
  <summary>Details</summary>
Motivation: 当前移动机器人气体分布建图系统大多依赖遥操作，限制了可扩展性和响应速度。在未知杂乱环境中实现自主主动气体分布建图具有挑战性，因为机器人需要同时探索可通行空间、构建环境地图并从稀疏化学测量中推断气体分布

Method: 将主动气体分布建图建模为下一个最佳轨迹信息路径规划问题，提出XIT采样规划器。该规划器从基于当前气体后验的上置信界信息场中批量采样，使用权衡旅行成本与气体浓度和不确定性的代价函数扩展树。引入气体前沿概念，并提出波前气体前沿检测算法识别这些区域

Result: 高保真仿真和真实世界实验证明XIT在气体分布建图质量和效率方面的优势。虽然为主动气体分布建图开发，但XIT可应用于其他需要在未知环境中平衡探索与利用的机器人信息收集任务

Conclusion: XIT算法通过平衡探索与利用，有效解决了移动机器人在未知杂乱环境中自主气体分布建图的挑战，提高了建图效率和质量，具有广泛的应用潜力

Abstract: Mobile robotic gas distribution mapping (GDM) provides critical situational awareness during emergency responses to hazardous gas releases. However, most systems still rely on teleoperation, limiting scalability and response speed. Autonomous active GDM is challenging in unknown and cluttered environments, because the robot must simultaneously explore traversable space, map the environment, and infer the gas distribution belief from sparse chemical measurements. We address this by formulating active GDM as a next-best-trajectory informative path planning (IPP) problem and propose XIT (Exploration-Exploitation Informed Trees), a sampling-based planner that balances exploration and exploitation by generating concurrent trajectories toward exploration-rich goals while collecting informative gas measurements en route. XIT draws batches of samples from an Upper Confidence Bound (UCB) information field derived from the current gas posterior and expands trees using a cost that trades off travel effort against gas concentration and uncertainty. To enable plume-aware exploration, we introduce the gas frontier concept, defined as unobserved regions adjacent to high gas concentrations, and propose the Wavefront Gas Frontier Detection (WGFD) algorithm for their identification. High-fidelity simulations and real-world experiments demonstrate the benefits of XIT in terms of GDM quality and efficiency. Although developed for active GDM, XIT is readily applicable to other robotic information-gathering tasks in unknown environments that face the exploration and exploitation trade-off.

</details>


### [43] [HybridFlow: A Two-Step Generative Policy for Robotic Manipulation](https://arxiv.org/abs/2602.13718)
*Zhenchen Dong,Jinna Fu,Jiaming Wu,Shengyuan Yu,Fulin Chen,Yide Liu*

Main category: cs.RO

TL;DR: HybridFlow是一种3阶段2步推理的机器人动作生成方法，通过结合MeanFlow的快速单步生成和ReFlow的精确细化，在保持高质量的同时大幅降低推理延迟，实现8倍加速和52Hz实时交互。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作策略受限于推理延迟，缺乏足够的实时环境交互能力。虽然流匹配等快速生成方法逐渐取代扩散方法，但研究人员仍在寻求更适合交互式机器人控制的更快生成方法。MeanFlow作为流匹配的单步变体在图像生成中表现出潜力，但其动作生成精度无法满足机器人操作的严格要求。

Method: 提出HybridFlow，一种3阶段2步推理的方法：1）MeanFlow模式的全局跳跃，2）分布对齐的ReNoise，3）ReFlow模式的局部细化。该方法利用MeanFlow单步生成的快速优势，同时通过最小生成步骤确保动作精度，平衡推理速度和生成质量。

Result: 在真实世界实验中，HybridFlow在成功率上比16步扩散策略提高15-25%，同时将推理时间从152ms减少到19ms（8倍加速，约52Hz）。在未见颜色OOD抓取任务上达到70.0%成功率，在可变形物体折叠任务上达到66.3%成功率。

Conclusion: HybridFlow作为一种实用的低延迟方法，有望增强机器人操作策略在真实世界中的交互能力，平衡了推理速度和生成质量，为实时机器人控制提供了有效解决方案。

Abstract: Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \textbf{HybridFlow}, a \textbf{3-stage method} with \textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \textbf{15--25\%} in success rate while reducing inference time from 152ms to 19ms (\textbf{8$\times$ speedup}, \textbf{$\sim$52Hz}); it also achieves 70.0\% success on unseen-color OOD grasping and 66.3\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.

</details>


### [44] [Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects](https://arxiv.org/abs/2602.14104)
*Xinan Rong,Changhuang Wan,Aochen He,Xiaolong Li,Gangshan Jing*

Main category: cs.RO

TL;DR: 提出双层框架实现多指协调，无需触觉反馈即可通过关节控制精确操纵力敏感物体


<details>
  <summary>Details</summary>
Motivation: 多指手仅依赖指尖点接触且无法施加拉力，比多臂平台更具挑战性；大多数商业灵巧手缺乏校准扭矩传感器，增加了精确力控的难度

Method: 采用双层框架：1) 结合图刚性和力闭合约束解决协调接触力规划；2) 通过力-位置映射将规划的力轨迹转换为关节轨迹，实现无触觉反馈的关节控制

Result: 在定制灵巧手上验证了框架有效性，能够高精度安全地操纵脆弱物体（软纱线、塑料杯、生鸡蛋）

Conclusion: 提出的双层框架解决了多指协调中的力规划难题，实现了无需触觉反馈的高精度力敏感物体操纵

Abstract: Precise in-hand manipulation of force-sensitive objects typically requires judicious coordinated force planning as well as accurate contact force feedback and control. Unlike multi-arm platforms with gripper end effectors, multi-fingered hands rely solely on fingertip point contacts and are not able to apply pull forces, therefore poses a more challenging problem. Furthermore, calibrated torque sensors are lacking in most commercial dexterous hands, adding to the difficulty. To address these challenges, we propose a dual-layer framework for multi-finger coordination, enabling high-precision manipulation of force-sensitive objects through joint control without tactile feedback. This approach solves coordinated contact force planning by incorporating graph rigidity and force closure constraints. By employing a force-to-position mapping, the planned force trajectory is converted to a joint trajectory. We validate the framework on a custom dexterous hand, demonstrating the capability to manipulate fragile objects-including a soft yarn, a plastic cup, and a raw egg-with high precision and safety.

</details>


### [45] [FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments](https://arxiv.org/abs/2602.13720)
*Chen Feng,Yang Xu,Shaojie Shen*

Main category: cs.RO

TL;DR: FC-Vision是一个实时可见性感知重规划框架，用于无人机自主扫描目标结构，主动防止目标遮挡，在保持原始扫描覆盖率和效率的同时显著提升扫描质量。


<details>
  <summary>Details</summary>
Motivation: 现有无人机扫描方法主要关注避碰和效率，但忽视了遮挡导致的可见性退化问题，这会严重损害扫描质量。在实际应用中，无人机需要在飞行过程中适应未知障碍物，因此需要能够在线处理遮挡问题的解决方案。

Method: 提出FC-Vision框架，采用高效的两级分解方法：1) 无遮挡视点修复，在保持覆盖范围的同时最小化偏离原始扫描意图；2) 在5自由度空间中进行分段清洁感知连接。框架还提供了插件式集成策略，可与现有无人机扫描系统无缝对接而无需架构更改。

Result: 综合仿真和真实世界评估表明，FC-Vision在意外遮挡物下持续提升扫描质量，最大覆盖增益达55.32%，遮挡率降低73.17%，同时保持实时性能，飞行时间仅有适度增加。

Conclusion: FC-Vision是一个有效的实时可见性感知重规划框架，能够主动安全地防止目标遮挡，显著提升无人机自主扫描的质量，同时与现有系统兼容，具有实际应用价值。

Abstract: Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.

</details>


### [46] [Muscle Coactivation in the Sky: Geometry and Pareto Optimality of Energy vs. Promptness in Multirotors](https://arxiv.org/abs/2602.14222)
*Antonio Franchi*

Main category: cs.RO

TL;DR: 该研究将能量经济性与运动准备性的权衡原理应用于多旋翼无人机，提出了"敏捷性"度量概念，通过几何纤维束框架将冗余解析转化为多目标优化问题，揭示了不同推进系统配置下能量效率与敏捷性的基本不兼容性。


<details>
  <summary>Details</summary>
Motivation: 机器人学和人体生物力学中能量经济性与运动准备性的权衡原理在多旋翼无人机领域尚未得到系统研究。现有方法多依赖启发式调参或黑盒算法，缺乏对能量消耗与敏捷性之间根本关系的理论理解，需要建立几何感知的控制分配理论基础。

Method: 提出"敏捷性"作为类似动态空气动力可操作性的度量指标；采用几何纤维束公式化方法，将冗余解析转化为仿射纤维上的多目标优化问题；利用微分同胚变换线性化带符号二次推进模型；通过六旋翼4自由度分配的案例研究，分析硬件不等式如何物理塑造成本权衡。

Result: 研究发现能量消耗与敏捷性的权衡关系具有纤维依赖性：单向推进器产生紧凑可行纤维，导致内部分配和短帕累托弧，扭矩需求破坏对称性；可逆推进器则通过零空间实现对抗性转子共收缩，将敏捷性推向硬件极限，使得最优续航与敏捷性在这些机制中根本不相容。

Conclusion: 该框架提供了通过几何感知控制分配实现敏捷性的理论基础，解释了"为何"以及"如何"实现敏捷性，而非依赖经验调参。这为飞行器设计、认证指标和威胁感知飞行操作提供了可能的指导，揭示了不同推进系统配置下能量效率与敏捷性的基本权衡关系。

Abstract: In robotics and human biomechanics, the tension between energy economy and kinematic readiness is well recognized; this work brings that fundamental principle to aerial multirotors. We show that the limited torque of the motors and the nonlinear aerodynamic map from rotor speed to thrust naturally give rise to the novel concept of promptness-a metric akin to dynamic aerodynamic manipulability. By treating energy consumption as a competing objective and introducing a geometric fiber-bundle formulation, we turn redundancy resolution into a principled multi-objective program on affine fibers. The use of the diffeomorphic transformation linearizing the signed-quadratic propulsion model allows us to lay the foundations for a rigorous study of the interplay between these costs. Through an illustrative case study on 4-DoF allocation on the hexarotor, we reveal that this interplay is fiber-dependent and physically shaped by hardware inequalities. For unidirectional thrusters, the feasible fibers are compact, yielding interior allocations and a short Pareto arc, while torque demands break symmetry and separate the optima. Conversely, with reversible propellers, the null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes. Ultimately, rather than relying on heuristic tuning or black box algorithms to empirically improve task execution, this framework provides a foundational understanding of why and how to achieve agility through geometry-aware control allocation, offering possible guidance for vehicle design, certification metrics, and threat-aware flight operation.

</details>


### [47] [Improving Driver Satisfaction with a Driving Function Learning from Implicit Human Feedback -- a Test Group Study](https://arxiv.org/abs/2602.13733)
*Robin Schwager,Andrea Anastasio,Simon Hartmann,Andreas Ronellenfitsch,Michael Grimm,Tim Brühl,Tin Stribor Sohn,Tim Dieter Eberhardt,Sören Hohmann*

Main category: cs.RO

TL;DR: 本文提出了一种自适应预测纵向驾驶功能算法，通过驾驶员干预反馈来个性化调整速度曲线，在驾驶模拟器研究中显著提高了驾驶员满意度并减少了干预频率。


<details>
  <summary>Details</summary>
Motivation: 驾驶员在使用高级驾驶辅助系统时经常主动干预调整系统行为，这些干预包含了驾驶员个人偏好与系统行为之间的偏差信息，应该利用这些反馈来优化和个性化驾驶功能。

Method: 提出了一种迭代调整算法，综合考虑原始预测纵向驾驶功能的速度曲线和驾驶员演示数据，在预定义路线上调整速度曲线，实现交易控制场景下的个性化。

Result: 在43名参与者的驾驶模拟器研究中，使用自适应PLDF显著提高了驾驶员满意度，并显著降低了干预频率。同时收集了参与者反馈以识别进一步优化潜力。

Conclusion: 通过利用驾驶员主动干预的反馈来个性化调整预测纵向驾驶功能的速度曲线是有效的，能够提高驾驶员满意度和减少干预，为驾驶辅助系统的个性化优化提供了可行方法。

Abstract: During the use of advanced driver assistance systems, drivers frequently intervene into the active driving function and adjust the system's behavior to their personal wishes. These active driver-initiated takeovers contain feedback about deviations in the driving function's behavior from the drivers' personal preferences. This feedback should be utilized to optimize and personalize the driving function's behavior. In this work, the adjustment of the speed profile of a Predictive Longitudinal Driving Function (PLDF) on a pre-defined route is highlighted. An algorithm is introduced which iteratively adjusts the PLDF's speed profile by taking into account both the original speed profile of the PLDF and the driver demonstration. This approach allows for personalization in a traded control scenario during active use of the PLDF. The applicability of the proposed algorithm is tested in a driving simulator-based test group study with 43 participants. The study finds a significant increase in driver satisfaction and a significant reduction in the intervention frequency when using the proposed adaptive PLDF. Additionally, feedback by the participants was gathered to identify further optimization potentials of the proposed system.

</details>


### [48] [Modeling and Optimizing the Provisioning of Exhaustible Capabilities for Simultaneous Task Allocation and Scheduling](https://arxiv.org/abs/2602.13866)
*Jinwoo Park,Harish Ravichandar,Seth Hutchinson*

Main category: cs.RO

TL;DR: TRAITS是一个离线异构多机器人任务分配框架，能够处理可耗尽特性在电池和时间约束下的分配问题，通过非线性规划优化特性供给率，提高可行性评估和任务执行时间估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 异构机器人团队在长时间范围内完成多个任务时，任务分配和规划面临显著的计算挑战，特别是需要处理可耗尽特性在电池和时间约束下的分配问题。

Method: 提出TRAITS框架，包含基于非线性规划的特性分布模块，优化联盟的特性供给率，同时考虑电池消耗和时间约束，提供更准确的可行性评估和任务执行时间估计。

Result: 与两种最先进框架相比，TRAITS在满足复杂特性和电池要求方面表现更优，同时保持计算可行性。

Conclusion: TRAITS是第一个能够处理可耗尽特性在电池和时间约束下分配的综合框架，通过优化特性供给率提高了任务分配的准确性和效率。

Abstract: Deploying heterogeneous robot teams to accomplish multiple tasks over extended time horizons presents significant computational challenges for task allocation and planning. In this paper, we present a comprehensive, time-extended, offline heterogeneous multi-robot task allocation framework, TRAITS, which we believe to be the first that can cope with the provisioning of exhaustible traits under battery and temporal constraints. Specifically, we introduce a nonlinear programming-based trait distribution module that can optimize the trait-provisioning rate of coalitions to yield feasible and time-efficient solutions. TRAITS provides a more accurate feasibility assessment and estimation of task execution times and makespan by leveraging trait-provisioning rates while optimizing battery consumption -- an advantage that state-of-the-art frameworks lack. We evaluate TRAITS against two state-of-the-art frameworks, with results demonstrating its advantage in satisfying complex trait and battery requirements while remaining computationally tractable.

</details>


### [49] [Kalman Filtering Based Flight Management System Modeling for AAM Aircraft](https://arxiv.org/abs/2602.14948)
*Balram Kandoria,Aryaman Singh Samyal*

Main category: cs.RO

TL;DR: 提出基于卡尔曼滤波的不确定性传播方法，通过sigmoid混合测量噪声协方差建模AAM飞行管理系统架构，实现自适应测量信任度，在AAM战略飞行规划中验证有效性。


<details>
  <summary>Details</summary>
Motivation: AAM运营需要预测时空不确定性的战略飞行规划服务，以安全验证飞行计划。现有方法因缺乏真实性能数据而依赖保守线性模型，需要更精确的不确定性估计方法。

Method: 提出基于卡尔曼滤波的不确定性传播方法，通过sigmoid混合测量噪声协方差建模AAM飞行管理系统架构。该方法根据航点进度连续调整滤波器的测量信任度，使FMS校正行为自然涌现，可扩展且可调以适应特定飞机特性或航线条件。

Result: 使用通用航空飞机的真实ADS-B数据进行验证，分为训练集和验证集。在训练集上调整参数后，在验证数据集上预测到达时间的准确率达到76%，证明了该方法在AAM运营中战略飞行计划验证的有效性。

Conclusion: 该方法为AAM战略飞行规划提供了有效的时空不确定性预测工具，相比现有固定阈值方法具有自适应优势，能够更准确地模拟飞行管理系统行为，支持更安全的飞行计划验证。

Abstract: Advanced Aerial Mobility (AAM) operations require strategic flight planning services that predict both spatial and temporal uncertainties to safely validate flight plans against hazards such as weather cells, restricted airspaces, and CNS disruption areas. Current uncertainty estimation methods for AAM vehicles rely on conservative linear models due to limited real-world performance data. This paper presents a novel Kalman Filter-based uncertainty propagation method that models AAM Flight Management System (FMS) architectures through sigmoid-blended measurement noise covariance. Unlike existing approaches with fixed uncertainty thresholds, our method continuously adapts the filter's measurement trust based on progress toward waypoints, enabling FMS correction behavior to emerge naturally. The approach scales proportionally with control inputs and is tunable to match specific aircraft characteristics or route conditions. We validate the method using real ADS-B data from general aviation aircraft divided into training and verification sets. Uncertainty propagation parameters were tuned on the training set, achieving 76% accuracy in predicting arrival times when compared against the verification dataset, demonstrating the method's effectiveness for strategic flight plan validation in AAM operations.

</details>


### [50] [Impact-Robust Posture Optimization for Aerial Manipulation](https://arxiv.org/abs/2602.13762)
*Amr Afifi,Ahmad Gazar,Javier Alonso-Mora,Paolo Robuffo Giordano,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出一种优化运动冗余扭矩控制机器人姿态的新方法，通过最小化冲击前后速度变化来减少冲击时的状态和输入尖峰，提高安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 运动冗余扭矩控制机器人在执行接触任务时容易受到冲击影响，导致状态和输入命令出现尖峰，可能引发安全问题。现有方法在冲击鲁棒性方面存在不足，需要开发能够主动优化姿态以减少冲击影响的方法。

Method: 使用刚性冲击模型建立配置依赖的度量标准，量化冲击前后速度变化。将寻找冲击鲁棒姿态问题建模为最小最大化优化问题，并重新表述为基于梯度的运动任务，通过任务空间逆动力学（TSID）全身控制器实现，可与其他控制目标无缝集成。

Result: 在运动冗余空中机械臂的重复点接触任务中，相比标准TSID，该方法使机器人配置的冲击后尖峰减少达51%，成功避免执行器饱和。在四足机器人和人形机器人的额外数值模拟中，冲击后状态尖峰减少达45%。

Conclusion: 该方法能有效优化运动冗余机器人的姿态，显著减少冲击影响，提高冲击鲁棒性，验证了运动冗余在冲击鲁棒性中的重要性，为机器人安全执行接触任务提供了实用解决方案。

Abstract: We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.

</details>


### [51] [MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer](https://arxiv.org/abs/2602.13764)
*Heng Zhi,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: MOTIF提出了一种高效的少样本跨具身迁移方法，通过解耦具身无关的时空模式（动作基元）来应对不同机器人间的运动异构性，显著提升了跨具身策略的适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在跨具身迁移方面面临挑战，主要由于不同机器人的运动异构性以及收集足够真实世界演示数据的高成本。现有的跨具身策略通常依赖共享-私有架构，这种架构存在私有参数容量有限和缺乏显式适应机制的问题。

Method: MOTIF通过解耦具身无关的时空模式（称为动作基元）来处理异构动作数据。首先通过向量量化学习统一的动作基元，采用进度感知对齐和具身对抗约束确保时空一致性和跨具身一致性。然后设计轻量级预测器从实时输入预测这些基元，指导流匹配策略，将其与机器人特定状态融合以生成新具身的动作。

Result: 在仿真和真实世界环境中的评估验证了MOTIF的优越性，在少样本迁移场景中显著优于强基线方法，仿真中提升6.5%，真实世界设置中提升43.7%。

Conclusion: MOTIF通过解耦具身无关的动作基元，实现了高效的少样本跨具身迁移，解决了现有方法在容量限制和适应机制方面的不足，为异构机器人间的知识迁移提供了有效解决方案。

Abstract: While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.

</details>


### [52] [RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems](https://arxiv.org/abs/2602.14438)
*Hamid Khabazi,Ali F. Meghdari,Alireza Taheri*

Main category: cs.RO

TL;DR: 该研究提出了一种基于LLM和VLM的智能多智能体框架，专门针对机器人学设计，能够自动分析解决机器人操作臂相关问题，包括运动学计算、仿真和控制。


<details>
  <summary>Details</summary>
Motivation: 将LLM和VLM的优势与计算工具结合，自动分析和解决机器人操作臂相关问题，实现从文本和视觉输入到机器人运动控制的完整流程自动化。

Method: 开发基于LLM和VLM的多智能体框架，接受文本和视觉输入，自动执行正向/逆向运动学、速度加速度计算、3D仿真生成和运动控制。使用GPT-4o、DeepSeek-V3.2、Claude-Sonnet-4.5等模型进行集成。

Result: 在三个基准测试中表现优异：文本输入测试中，框架集成GPT-4o达到0.97准确率（原始模型仅0.30）；视觉输入测试中达到0.93准确率，比原始模型高约20%；综合机器人任务测试中达到0.97准确率。

Conclusion: 提出的多智能体框架显著提升了机器人相关问题的自动化解决能力，特别是在运动学计算和仿真控制方面，证明了LLM/VLM与计算工具集成的有效性。

Abstract: This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.

</details>


### [53] [Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation](https://arxiv.org/abs/2602.13833)
*Kevin Yuchen Ma,Heng Zhang,Weisi Lin,Mike Zheng Shou,Yan Wu*

Main category: cs.RO

TL;DR: SCFields提出了一种融合视觉语义与密集接触估计的3D表示方法，通过两阶段Sim-to-Real接触学习管道实现未见工具的物理泛化，在刮削、蜡笔画和剥离等接触密集型工具操作任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略存在两个问题：1）视觉-语言-动作模型缺乏高保真物理基础，难以处理接触密集型工具操作；2）基于触觉感知的接触感知策略通常是实例特定的，无法泛化到不同几何形状的工具。同时，大规模获取真实世界触觉数据成本高昂，而直接从模拟到现实的零样本转移又因软传感器非线性变形的复杂动力学而具有挑战性。

Method: 提出语义接触场（SCFields），这是一种融合视觉语义与密集接触估计的统一3D表示。采用两阶段模拟到现实接触学习管道：第一阶段在大规模模拟数据集上预训练以学习通用接触物理；第二阶段在小规模真实数据集上微调，通过几何启发式和力优化进行伪标注以对齐传感器特性。将SCFields作为扩散策略的密集观测输入，实现鲁棒的工具操作执行。

Result: 在刮削、蜡笔画和剥离等接触密集型工具操作任务上进行实验，SCFields表现出强大的类别级泛化能力，显著优于仅使用视觉和原始触觉的基线方法。

Conclusion: SCFields通过融合视觉语义与接触物理的统一表示，结合两阶段模拟到现实学习管道，成功实现了对未见工具的物理泛化，为接触密集型工具操作提供了有效的解决方案。

Abstract: Generalizing tool manipulation requires both semantic planning and precise physical control. Modern generalist robot policies, such as Vision-Language-Action (VLA) models, often lack the high-fidelity physical grounding required for contact-rich tool manipulation. Conversely, existing contact-aware policies that leverage tactile or haptic sensing are typically instance-specific and fail to generalize across diverse tool geometries. Bridging this gap requires learning unified contact representations from diverse data, yet a fundamental barrier remains: diverse real-world tactile data are prohibitive at scale, while direct zero-shot sim-to-real transfer is challenging due to the complex dynamics of nonlinear deformation of soft sensors.
  To address this, we propose Semantic-Contact Fields (SCFields), a unified 3D representation fusing visual semantics with dense contact estimates. We enable this via a two-stage Sim-to-Real Contact Learning Pipeline: first, we pre-train on a large simulation data set to learn general contact physics; second, we fine-tune on a small set of real data, pseudo-labeled via geometric heuristics and force optimization, to align sensor characteristics. This allows physical generalization to unseen tools. We leverage SCFields as the dense observation input for a diffusion policy to enable robust execution of contact-rich tool manipulation tasks. Experiments on scraping, crayon drawing, and peeling demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.

</details>


### [54] [Push-Placement: A Hybrid Approach Integrating Prehensile and Non-Prehensile Manipulation for Object Rearrangement](https://arxiv.org/abs/2602.13849)
*Majid Sadeghinejad,Arman Barghi,Hamed Hosseini,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: 提出了一种名为"push-placement"的混合操作原语，结合抓取放置和推动操作，在放置物体时同时推开障碍物，减少显式缓冲需求，提高了桌面重排任务的效率。


<details>
  <summary>Details</summary>
Motivation: 桌面重排任务面临碰撞和目标位姿被遮挡时需要临时缓冲的挑战。抓取放置操作控制精确但需要额外移动，而推动操作更高效但动态复杂且不精确。需要一种结合两者优势的方法来提高重排效率。

Method: 提出了push-placement混合操作原语，在抓取物体放置的同时使用该物体推开障碍物。将此方法集成到基于物理仿真的蒙特卡洛树搜索规划器中，在PyBullet模拟器中进行评估。

Result: 实验结果显示，push-placement相比基准MCTS规划器减少了最多11.12%的机械臂移动成本，相比动态堆叠方法减少了8.56%。

Conclusion: 混合抓取/非抓取操作原语能够显著提高长视野重排任务的效率，push-placement方法通过减少显式缓冲需求有效降低了操作成本。

Abstract: Efficient tabletop rearrangement remains challenging due to collisions and the need for temporary buffering when target poses are obstructed. Prehensile pick-and-place provides precise control but often requires extra moves, whereas non-prehensile pushing can be more efficient but suffers from complex, imprecise dynamics. This paper proposes push-placement, a hybrid action primitive that uses the grasped object to displace obstructing items while being placed, thereby reducing explicit buffering. The method is integrated into a physics-in-the-loop Monte Carlo Tree Search (MCTS) planner and evaluated in the PyBullet simulator. Empirical results show push-placement reduces the manipulator travel cost by up to 11.12% versus a baseline MCTS planner and 8.56% versus dynamic stacking. These findings indicate that hybrid prehensile/non-prehensile action primitives can substantially improve efficiency in long-horizon rearrangement tasks.

</details>


### [55] [Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement](https://arxiv.org/abs/2602.13850)
*Minku Kim,Kuan-Chia Chen,Aayam Shrestha,Li Fuxin,Stefan Lee,Alan Fern*

Main category: cs.RO

TL;DR: 该论文提出了一种基于技能的人形机器人箱子重排框架，通过任务级技能序列实现长时程执行，采用共享的全身控制器，并通过数据聚合增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法中每个技能使用独立的低级控制器，难以实现长时程的鲁棒执行。需要一种能够支持技能组合和长时程操作的统一控制框架。

Method: 提出技能框架，所有技能通过共享的任务无关全身控制器执行；采用数据聚合方法，在领域随机化下通过闭环技能执行增强共享控制器的训练；引入Humanoid Hanoi基准测试。

Result: 在仿真和Digit V3人形机器人上实现了完全自主的长时程重排，量化了共享全身控制器方法相对于非共享基线的优势。

Conclusion: 共享全身控制器框架通过技能组合支持长时程人形机器人操作，数据聚合方法有效解决了技能组合带来的分布偏移问题，在复杂重排任务中表现出色。

Abstract: We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.

</details>


### [56] [UAV-SEAD: State Estimation Anomaly Dataset for UAVs](https://arxiv.org/abs/2602.13900)
*Aykut Kabaoglu,Sanem Sariel*

Main category: cs.RO

TL;DR: 该研究提供了一个大规模真实世界无人机数据集，包含1396个飞行日志（超过52小时飞行时间），用于无人机状态估计异常检测研究，填补了现有数据集主要依赖模拟注入故障的空白。


<details>
  <summary>Details</summary>
Motivation: 无人机状态估计的准确性对可靠安全运行至关重要，任务执行中的异常可能导致预期与实际系统行为不一致，危及任务成功或安全。现有数据集主要依赖模拟数据注入故障，缺乏真实世界异常检测数据。

Method: 收集了1396个真实飞行日志（超过52小时飞行时间），使用基于PX4的无人机在不同室内外环境中采集，配备多种传感器配置。提出了结构化分类方法，将无人机状态估计异常分为四类：机械电气异常、外部位置异常、全局位置异常和高度异常。

Result: 创建了包含正常和异常飞行的大规模真实世界无人机数据集，无合成操纵，适合现实异常检测任务。数据集包含多元传感器数据流（IMU、GPS、气压计、磁力计、距离传感器、视觉里程计、光流等），反映了集体、上下文和离群异常。

Conclusion: 该数据集将在异常检测和隔离系统的开发、训练和评估中发挥关键作用，解决无人机可靠性研究中的关键空白，促进现实世界异常检测研究的发展。

Abstract: Accurate state estimation in Unmanned Aerial Vehicles (UAVs) is crucial for ensuring reliable and safe operation, as anomalies occurring during mission execution may induce discrepancies between expected and observed system behaviors, thereby compromising mission success or posing potential safety hazards. It is essential to continuously monitor and detect such conditions in order to ensure a timely response and maintain system reliability. In this work, we focus on UAV state estimation anomalies and provide a large-scale real-world UAV dataset to facilitate research aimed at improving the development of anomaly detection. Unlike existing datasets that primarily rely on injected faults into simulated data, this dataset comprises 1396 real flight logs totaling over 52 hours of flight time, collected across diverse indoor and outdoor environments using a collection of PX4-based UAVs equipped with a variety of sensor configurations. The dataset comprises both normal and anomalous flights without synthetic manipulation, making it uniquely suitable for realistic anomaly detection tasks. A structured classification is proposed that categorizes UAV state estimation anomalies into four classes: mechanical and electrical, external position, global position, and altitude anomalies. These classifications reflect collective, contextual, and outlier anomalies observed in multivariate sensor data streams, including IMU, GPS, barometer, magnetometer, distance sensors, visual odometry, and optical flow, that can be found in the PX4 logging mechanism. It is anticipated that this dataset will play a key role in the development, training, and evaluation of anomaly detection and isolation systems to address the critical gap in UAV reliability research.

</details>


### [57] [High-fidelity 3D reconstruction for planetary exploration](https://arxiv.org/abs/2602.13909)
*Alfonso Martínez-Petersen,Levin Gerdes,David Rodríguez-Martínez,C. J. Pérez-del-Pulgar*

Main category: cs.RO

TL;DR: 该研究提出了一种将辐射场方法（NeRF和高斯泼溅）集成到行星机器人环境重建管道中的系统，能够从少量视觉输入生成密集、逼真且度量一致的3D表示。


<details>
  <summary>Details</summary>
Motivation: 行星探索越来越依赖能够在没有全球定位或与地球实时通信的情况下感知、解释和重建环境的自主机器人系统。传统基于SfM和SLAM的技术在捕捉辐射度细节或在典型地外环境的非结构化、低纹理地形中高效扩展方面存在困难。

Method: 将辐射场方法（NeRF和高斯泼溅）集成到统一的自动化环境重建管道中，结合Nerfstudio和COLMAP框架，采用ROS2兼容的工作流程，能够直接处理来自rosbag记录的原始漫游车数据。

Result: 该系统能够从少量视觉输入生成密集、逼真且度量一致的3D表示，支持在类行星条件下自主系统的改进感知和规划。

Conclusion: 该管道为辐射场建图的未来研究奠定了基础，弥合了行星探索中几何表示与神经表示之间的差距。

Abstract: Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) provide geometric consistency but struggle to capture radiometric detail or to scale efficiently in unstructured, low-texture terrains typical of extraterrestrial environments. This work explores the integration of radiance field-based methods - specifically Neural Radiance Fields (NeRF) and Gaussian Splatting - into a unified, automated environment reconstruction pipeline for planetary robotics. Our system combines the Nerfstudio and COLMAP frameworks with a ROS2-compatible workflow capable of processing raw rover data directly from rosbag recordings. This approach enables the generation of dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems operating in planetary-like conditions. The resulting pipeline established a foundation for future research in radiance field-based mapping, bridging the gap between geometric and neural representations in planetary exploration.

</details>


### [58] [Joint Task Assistance Planning via Nested Branch and Bound (Extended Version)](https://arxiv.org/abs/2602.13932)
*Omer Daube,Oren Salzman*

Main category: cs.RO

TL;DR: 本文提出并研究了联合任务辅助规划问题，该问题推广了先前关于机器人协作中优化辅助的工作。两个机器人在预定义的路图上操作，每个路图对应其配置空间。任务机器人必须执行定时任务，而辅助机器人提供基于传感器的支持。目标是计算两条机器人的路径，最大化总辅助时间。由于路径组合的组合爆炸和时间特性，该问题具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 机器人协作中优化辅助是一个重要问题，但现有工作通常假设辅助机器人始终可用或辅助是瞬时的。实际应用中，辅助机器人的支持取决于其与任务机器人的空间关系，且需要时间协调。因此需要研究更通用的联合任务辅助规划问题，考虑空间关系和时间约束。

Method: 提出嵌套分支定界框架，以分层方式高效探索机器人路径空间。该方法处理路径组合的组合爆炸和时间特性，通过层次化搜索策略减少计算复杂度。

Result: 经验评估表明，与基线方法相比，所提算法实现了高达两个数量级的加速。这证明了嵌套分支定界框架在处理联合任务辅助规划问题中的有效性。

Conclusion: 本文成功解决了联合任务辅助规划问题，提出了高效的嵌套分支定界算法，显著提升了计算效率。该工作为机器人协作中的优化辅助提供了更通用的解决方案框架。

Abstract: We introduce and study the Joint Task Assistance Planning problem which generalizes prior work on optimizing assistance in robotic collaboration. In this setting, two robots operate over predefined roadmaps, each represented as a graph corresponding to its configuration space. One robot, the task robot, must execute a timed mission, while the other, the assistance robot, provides sensor-based support that depends on their spatial relationship. The objective is to compute a path for both robots that maximizes the total duration of assistance given. Solving this problem is challenging due to the combinatorial explosion of possible path combinations together with the temporal nature of the problem (time needs to be accounted for as well). To address this, we propose a nested branch-and-bound framework that efficiently explores the space of robot paths in a hierarchical manner. We empirically evaluate our algorithm and demonstrate a speedup of up to two orders of magnitude when compared to a baseline approach.

</details>


### [59] [WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL](https://arxiv.org/abs/2602.13977)
*Zhennan Jiang,Shangqing Zhou,Yutong Jiang,Zefang Huang,Mingjie Wei,Yuhui Chen,Tianxing Zhou,Zhen Guo,Hao Lin,Quanlu Zhang,Yu Wang,Haoran Li,Chao Yu,Dongbin Zhao*

Main category: cs.RO

TL;DR: WoVR框架通过控制动作条件视频世界模型、关键帧初始化滚动和世界模型-策略协同进化，解决了基于世界模型的强化学习中幻觉和误差累积问题，显著提升了VLA策略的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习有望超越模仿学习，为视觉-语言-动作模型解锁更多能力，但在物理机器人上的直接部署需要大量真实世界交互。现有方法使用学习的世界模型作为策略优化的模拟器，但闭环想象滚动会遭受幻觉和长时域误差累积，这些误差不仅降低视觉保真度，还会破坏优化信号，导致策略利用模型不准确性而非真正的任务进展。

Method: WoVR框架明确调节RL与不完美想象动态的交互：1）通过可控的动作条件视频世界模型提高滚动稳定性；2）通过关键帧初始化滚动减少有效误差深度；3）通过世界模型-策略协同进化保持策略-模拟器对齐。

Result: 在LIBERO基准测试和真实世界机器人操作实验中，WoVR实现了稳定的长时域想象滚动和有效的策略优化：LIBERO平均成功率从39.95%提升至69.2%（+29.3个百分点），真实机器人成功率从61.7%提升至91.7%（+30.0个百分点）。

Conclusion: 当幻觉被明确控制时，学习的世界模型可以作为强化学习的实用模拟器。WoVR框架通过调节RL与不完美想象动态的交互，解决了基于世界模型的强化学习中的关键挑战，显著提升了VLA策略的性能。

Abstract: Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.

</details>


### [60] [It Takes Two to Tango: A Holistic Simulator for Joint Order Scheduling and Multi-Agent Path Finding in Robotic Warehouses](https://arxiv.org/abs/2602.13999)
*Haozheng Xu,Wenhao Li,Zifan Wei,Bo Jin,Hongxing Bai,Ben Yang,Xiangfeng Wang*

Main category: cs.RO

TL;DR: WareRover是一个新型机器人移动履行系统仿真平台，通过统一闭环优化接口将订单调度和多智能体路径规划紧密耦合，克服了传统方法将两者分离的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有RMFS系统通常将订单调度和多智能体路径规划作为独立子问题处理，这种解耦掩盖了高层调度与底层拥堵之间的关键依赖关系。现有仿真器无法弥合这一差距，常常忽略异构运动学和随机执行故障。

Method: 提出WareRover平台，通过统一闭环优化接口实现OS和MAPF的紧密耦合。平台集成了动态订单流、物理感知运动约束和非标称恢复机制到一个单一评估循环中。

Result: 实验表明，现有最先进算法在这些现实耦合约束下常常表现不佳，证明WareRover为稳健的下一代仓库协调提供了一个必要且具有挑战性的测试平台。

Conclusion: WareRover通过紧密耦合订单调度和多智能体路径规划，为仓库机器人系统的真实性能评估提供了更全面的仿真环境，揭示了传统解耦方法的局限性。

Abstract: The prevailing paradigm in Robotic Mobile Fulfillment Systems (RMFS) typically treats order scheduling and multi-agent pathfinding as isolated sub-problems. We argue that this decoupling is a fundamental bottleneck, masking the critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap, often abstracting away heterogeneous kinematics and stochastic execution failures. We propose WareRover, a holistic simulation platform that enforces a tight coupling between OS and MAPF via a unified, closed-loop optimization interface. Unlike standard benchmarks, WareRover integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop. Experiments reveal that SOTA algorithms often falter under these realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination. The project and video is available at https://hhh-x.github.io/WareRover/.

</details>


### [61] [RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation](https://arxiv.org/abs/2602.14032)
*Xinhua Wang,Kun Wu,Zhen Zhao,Hu Cao,Yinuo Zhao,Zhiyuan Xu,Meng Li,Shichao Fan,Di Wu,Yixue Zhang,Ning Liu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: RoboAug是一个新颖的生成式数据增强框架，通过仅需单张图像的边界框标注，利用预训练生成模型进行语义数据增强，结合区域对比损失提升机器人学习在未见场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人学习在多样化未见场景中的泛化能力是一个基础且具有挑战性的问题。现有方法依赖大规模数据预训练（劳动密集且耗时）或语义数据增强技术（需要完美上游物体检测的不切实际假设）。

Method: 提出RoboAug框架：1）仅需训练期间单张图像的边界框标注；2）利用预训练生成模型进行精确语义数据增强；3）集成即插即用的区域对比损失，帮助模型聚焦任务相关区域。

Result: 在三个机器人（UR-5e、AgileX、Tien Kung 2.0）上进行了超过35k次真实世界实验。在包含多样化背景、干扰物和光照条件的未见场景中，相比无增强基线，成功率显著提升：UR-5e从0.09到0.47，AgileX从0.16到0.60，Tien Kung 2.0从0.19到0.67。

Conclusion: RoboAug显著减少了对大规模预训练和完美视觉识别假设的依赖，通过最小化标注需求实现了卓越的泛化能力，在真实世界机器人操作任务中表现出色，超越了现有最先进的数据增强方法。

Abstract: Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.

</details>


### [62] [Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation](https://arxiv.org/abs/2602.14174)
*Yifei Yang,Anzhe Chen,Zhenjie Zhu,Kechun Xu,Yunxuan Mao,Yufei Wei,Lu Chen,Rong Xiong,Yue Wang*

Main category: cs.RO

TL;DR: 该论文提出了一种基于方向力预测的sim-to-real迁移框架，通过预测接触力方向而非大小来克服接触动力学差异，结合专家设计的控制器逻辑实现鲁棒的接触式操作任务。


<details>
  <summary>Details</summary>
Motivation: 接触式操作的sim-to-real迁移面临接触动力学差异的挑战。现有方法要么依赖昂贵的真实数据，要么使用固定的盲从控制器。需要一种既能利用仿真优势又能适应真实世界接触变化的框架。

Method: 使用基于有限状态机的位置/力控制器在仿真中提供特权监督，训练策略预测末端执行器位姿、接触状态和关键的方向力。部署时，这些预测配置力感知导纳控制器，将策略的方向意图与手动调整的恒定力大小结合，生成自适应、任务对齐的顺从性。

Result: 在四个真实世界任务（微波炉开门、孔插销、白板擦拭、门开门）上的实验表明，该方法在成功率和鲁棒性方面显著优于强基线方法。

Conclusion: 通过预测接触力方向而非大小，结合专家控制器逻辑和轻量级手动调整，实现了鲁棒的sim-to-real迁移，为接触式操作任务提供了有效的解决方案。

Abstract: Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy's directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.

</details>


### [63] [Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation](https://arxiv.org/abs/2602.14193)
*Yue Chen,Muqing Jiang,Kaifeng Zheng,Jiaqi Liang,Chenrui Tie,Haoran Lu,Ruihai Wu,Hao Dong*

Main category: cs.RO

TL;DR: PA3FF是一种具有部件感知能力的3D特征场，通过对比学习从大规模标注数据中学习，能够为可泛化的关节物体操作提供连续3D特征，显著优于现有2D和3D表示方法。


<details>
  <summary>Details</summary>
Motivation: 关节物体操作对机器人任务至关重要，但跨不同物体的泛化能力仍面临挑战。关键在于理解功能部件（如门把手、旋钮），这些部件指示了跨不同物体类别和形状的操作位置和方式。现有方法多基于2D特征，在提升到3D空间时面临运行时间长、多视角不一致、空间分辨率低等问题。

Method: 提出Part-Aware 3D Feature Field (PA3FF)，通过对比学习从大规模标注数据集的3D部件提议中训练得到。给定点云输入，PA3FF以前馈方式预测连续3D特征场，其中点特征之间的距离反映了功能部件的接近程度。基于此特征，进一步提出Part-Aware Diffusion Policy (PADP)模仿学习框架。

Result: 在多个模拟和真实世界任务中评估PADP，结果表明PA3FF在操作场景中一致优于一系列2D和3D表示方法，包括CLIP、DINOv2和Grounded-SAM。PA3FF还支持多种下游方法，包括对应关系学习和分割任务。

Conclusion: PA3FF为机器人操作提供了一个多功能的基础表示，能够有效解决关节物体操作的泛化问题，通过部件感知的3D特征场显著提升了操作性能。

Abstract: Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io

</details>


### [64] [Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots' Performance in U-Shaped Stair Climbing](https://arxiv.org/abs/2602.14473)
*Baixiao Huang,Baiyu Huang,Yu Hou*

Main category: cs.RO

TL;DR: 使用两阶段端到端深度强化学习方法训练四足机器人自主攀爬U型楼梯，并验证策略在不同楼梯类型间的可迁移性


<details>
  <summary>Details</summary>
Motivation: 四足机器人在建筑场景中有广泛应用，但自主攀爬不同室内楼梯仍是完成建筑任务的主要挑战，需要解决机器人自主攀爬U型楼梯的问题

Method: 采用两阶段端到端深度强化学习方法：首先在Isaac Lab的金字塔楼梯地形上训练Unitree Go2机器人攀爬楼梯，然后利用学习到的策略在U型室内楼梯上进行训练

Result: 1) 机器人在有停滞惩罚的情况下成功攀爬U型楼梯到达目标；2) 在U型楼梯上训练的策略可迁移到直线、L型和螺旋楼梯地形，其他楼梯模型训练的策略也可迁移到U型地形

Conclusion: 端到端深度强化学习方法能够有效训练四足机器人自主攀爬复杂楼梯结构，且学习到的策略具有良好的跨楼梯类型迁移能力

Abstract: Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot's performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab's pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.

</details>


### [65] [A Latency-Aware Framework for Visuomotor Policy Learning on Industrial Robots](https://arxiv.org/abs/2602.14255)
*Daniel Ruan,Salma Mozaffari,Sigrid Adriaenssens,Arash Adel*

Main category: cs.RO

TL;DR: 提出面向工业机器人的延迟感知框架，解决视觉运动策略部署中的观测-执行延迟问题，通过异步推理与执行调度提升接触式装配任务的可靠性


<details>
  <summary>Details</summary>
Motivation: 工业机器人在接触式制造任务中面临显著的观测-执行延迟问题，这种延迟远大于研究机器人，导致视觉运动策略在实际部署时存在系统级时序挑战

Method: 开发包含标定多模态感知、时间一致性同步、统一通信管道和遥操作界面的框架，提出基于时序可行性的延迟感知执行策略，调度有限时域的动作序列

Result: 在接触式工业装配任务中，延迟感知执行相比阻塞式和朴素异步基线方法，能在各种延迟条件下保持平滑运动、顺应性接触行为和一致任务进展，减少空闲时间并避免不稳定性

Conclusion: 明确处理延迟对于工业机器人上视觉运动策略的可靠闭环部署至关重要，延迟感知框架能有效应对工业平台特有的高延迟挑战

Abstract: Industrial robots are increasingly deployed in contact-rich construction and manufacturing tasks that involve uncertainty and long-horizon execution. While learning-based visuomotor policies offer a promising alternative to open-loop control, their deployment on industrial platforms is challenged by a large observation-execution gap caused by sensing, inference, and control latency. This gap is significantly greater than on low-latency research robots due to high-level interfaces and slower closed-loop dynamics, making execution timing a critical system-level issue. This paper presents a latency-aware framework for deploying and evaluating visuomotor policies on industrial robotic arms under realistic timing constraints. The framework integrates calibrated multimodal sensing, temporally consistent synchronization, a unified communication pipeline, and a teleoperation interface for demonstration collection. Within this framework, we introduce a latency-aware execution strategy that schedules finite-horizon, policy-predicted action sequences based on temporal feasibility, enabling asynchronous inference and execution without modifying policy architectures or training. We evaluate the framework on a contact-rich industrial assembly task while systematically varying inference latency. Using identical policies and sensing pipelines, we compare latency-aware execution with blocking and naive asynchronous baselines. Results show that latency-aware execution maintains smooth motion, compliant contact behavior, and consistent task progression across a wide range of latencies while reducing idle time and avoiding instability observed in baseline methods. These findings highlight the importance of explicitly handling latency for reliable closed-loop deployment of visuomotor policies on industrial robots.

</details>


### [66] [Autonomous Robotic Tissue Palpation and Abnormalities Characterisation via Ergodic Exploration](https://arxiv.org/abs/2602.14287)
*Luca Beber,Edoardo Lamon,Matteo Saveriano,Daniele Fontanelli,Luigi Palopoli*

Main category: cs.RO

TL;DR: 提出了一种用于实时弹性映射的自主机器人触诊框架，结合力基参数估计和遍历控制策略，通过定制化期望信息密度引导探索诊断相关区域。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够在组织探索过程中实时进行弹性映射的自主机器人触诊方法，以改进组织表征在诊断和筛查应用中的效果。

Method: 结合商业力/扭矩传感器的力基参数估计与遍历控制策略，使用扩展卡尔曼滤波器在线估计粘弹性模型参数，高斯过程回归提供弹性空间建模，热方程驱动区域覆盖控制器实现自适应连续轨迹规划。

Result: 在合成刚度图上的仿真表明，相比基于贝叶斯优化的技术，该方法实现了更好的重建精度、增强的分割能力和改进的检测硬性包涵体的鲁棒性。在模拟病理组织区域的硅胶体模上的实验验证进一步证实了该方法在自主组织表征方面的潜力。

Conclusion: 该自主机器人触诊框架在实时弹性映射方面表现出色，具有在诊断和筛查应用中实现自主组织表征的潜力。

Abstract: We propose a novel autonomous robotic palpation framework for real-time elastic mapping during tissue exploration using a viscoelastic tissue model. The method combines force-based parameter estimation using a commercial force/torque sensor with an ergodic control strategy driven by a tailored Expected Information Density, which explicitly biases exploration toward diagnostically relevant regions by jointly considering model uncertainty, stiffness magnitude, and spatial gradients. An Extended Kalman Filter is employed to estimate viscoelastic model parameters online, while Gaussian Process Regression provides spatial modelling of the estimated elasticity, and a Heat Equation Driven Area Coverage controller enables adaptive, continuous trajectory planning. Simulations on synthetic stiffness maps demonstrate that the proposed approach achieves better reconstruction accuracy, enhanced segmentation capability, and improved robustness in detecting stiff inclusions compared to Bayesian Optimisation-based techniques. Experimental validation on a silicone phantom with embedded inclusions emulating pathological tissue regions further corroborates the potential of the method for autonomous tissue characterisation in diagnostic and screening applications.

</details>


### [67] [Exploiting Structure-from-Motion for Robust Vision-Based Map Matching for Aircraft Surface Movement](https://arxiv.org/abs/2602.14311)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 该论文提出了一种结合间接方法和直接图像技术优点的视觉辅助导航管道，用于支持自主飞机的地面导航，通过特征匹配和地图匹配技术提高导航完整性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够支持自主飞机地面导航的视觉辅助导航系统，结合计算效率和鲁棒性，以增强导航解决方案的完整性，为自主飞机地面运动提供可认证的解决方案。

Method: 提出一个视觉辅助导航管道：1) 处理地面图像并使用基于特征的结构从运动(SfM)解决方案关联图像；2) 通过单应性变换构建地面平面马赛克；3) 使用强度平方和差异(SSD)将马赛克与卫星图像匹配。

Result: 实验结果显示，SfM解决方案中的漂移（类似于航位推算系统）挑战了与宽基线地面平面马赛克进行地图匹配的预期精度优势。但算法展示了关键完整性特征，如识别配准异常和模糊匹配的能力。

Conclusion: 所提出的算法具有识别异常和模糊匹配的关键完整性特征，这些特性可以减轻异常行为，为自主飞机地面运动提供稳健、可认证的解决方案。

Abstract: In this paper we introduce a vision-aided navigation (VAN) pipeline designed to support ground navigation of autonomous aircraft. The proposed algorithm combines the computational efficiency of indirect methods with the robustness of direct image-based techniques to enhance solution integrity. The pipeline starts by processing ground images (e.g., acquired by a taxiing aircraft) and relates them via a feature-based structure-from-motion (SfM) solution. A ground plane mosaic is then constructed via homography transforms and matched to satellite imagery using a sum of squares differences (SSD) of intensities. Experimental results reveal that drift within the SfM solution, similar to that observed in dead-reckoning systems, challenges the expected accuracy benefits of map-matching with a wide-baseline ground-plane mosaic. However, the proposed algorithm demonstrates key integrity features, such as the ability to identify registration anomalies and ambiguous matches. These characteristics of the pipeline can mitigate outlier behaviors and contribute toward a robust, certifiable solution for autonomous surface movement of aircraft.

</details>


### [68] [AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation](https://arxiv.org/abs/2602.14363)
*Morgan Byrd,Donghoon Baek,Kartik Garg,Hyunyoung Jung,Daesol Cho,Maks Sorokin,Robert Wright,Sehoon Ha*

Main category: cs.RO

TL;DR: AdaptManip是一个全自主的人形机器人全身运动操控框架，通过强化学习训练，无需人类演示，实现导航、物体抓取和递送的一体化操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的方法依赖人类演示，对干扰脆弱，需要开发不依赖演示数据、更鲁棒的全身运动操控策略。

Method: 框架包含三个耦合组件：1) 循环物体状态估计器，在有限视野和遮挡下实时跟踪物体；2) 全身基础策略，用于鲁棒运动，配合残差操控控制实现稳定物体抓取递送；3) LiDAR机器人全局位置估计器，提供抗漂移定位。所有组件在仿真中用强化学习训练，零样本部署到真实硬件。

Result: AdaptManip显著优于基线方法（包括模仿学习方法），在适应性和总体成功率上表现更好；准确的物体状态估计即使在遮挡下也能提升操控性能；在真实人形机器人上实现了全自主导航、物体抓取和递送。

Conclusion: AdaptManip展示了通过强化学习训练、无需人类演示的全身运动操控框架的有效性，在人形机器人上实现了鲁棒的全自主导航-抓取-递送一体化操作。

Abstract: This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.

</details>


### [69] [A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation](https://arxiv.org/abs/2602.14434)
*Steven Oh,Tomoya Takahashi,Cristian C. Beltran-Hernandez,Yuki Kuroda,Masashi Hamaya*

Main category: cs.RO

TL;DR: CLAW是一种新型软手腕机制，通过正交叶片弹簧和带锁定机构的旋转关节实现大范围变形和各向异性刚度调节，在接触丰富的操作任务中显著提升机器人学习鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 非结构化环境中的接触丰富操作任务对机器人学习提出了鲁棒性挑战，现有软末端执行器存在变形范围有限、缺乏方向刚度控制或需要复杂驱动系统等问题。

Method: 设计CLAW软手腕机制，采用两个正交叶片弹簧和带锁定机构的旋转关节，实现6自由度大变形和各向异性刚度调节，具有轻量化低成本特点。

Result: 在基准销钉插入任务中，CLAW达到76%成功率，优于Fin Ray夹爪(43%)和刚性夹爪(36%)，能处理精密装配和精细物体操作等多种接触丰富场景。

Conclusion: CLAW通过简单有效的设计解决了现有软末端执行器的局限性，在接触丰富领域为机器人学习提供了鲁棒性解决方案，具有实际应用潜力。

Abstract: Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/

</details>


### [70] [TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations](https://arxiv.org/abs/2602.14526)
*Guy Freund,Tom Jurgenson,Matan Sudry,Erez Karpas*

Main category: cs.RO

TL;DR: TWISTED-RL改进TWISTED框架，用多步强化学习策略替代单步逆模型，通过抽象拓扑动作实现更精细的状态转换，成功解决更高复杂度的打结任务


<details>
  <summary>Details</summary>
Motivation: 机器人打结任务面临可变形物体复杂交互和严格拓扑约束的挑战，现有方法TWISTED虽然通过任务分解取得进展，但其单步逆模型存在数据收集成本高、效果有限的问题，需要更精细的拓扑状态转换机制

Method: 提出TWISTED-RL框架，将TWISTED中的监督学习单步逆模型替换为基于抽象拓扑动作的多步强化学习策略，避免了昂贵无效的数据收集，实现了更精细的拓扑状态转换

Result: 实验结果表明，TWISTED-RL能够解决之前无法实现的高复杂度打结任务（如八字结和单结），成功率和规划时间均有显著改善，成为无需人工演示的机器人打结任务新SOTA

Conclusion: 基于抽象拓扑动作的多步强化学习策略比单步逆模型更适合机器人打结任务，能够实现更精细的拓扑状态转换，提高泛化能力，为复杂可变形物体操作提供了有效解决方案

Abstract: Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED's single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.

</details>


### [71] [Multimodal Covariance Steering in Belief Space with Active Probing and Influence for Autonomous Driving](https://arxiv.org/abs/2602.14540)
*Devodita Chakravarty,John Dolan,Yiwei Lyu*

Main category: cs.RO

TL;DR: 提出了一种结合信念推理、主动探测和风险监控的分层框架，用于自动驾驶在不确定交互场景中的规划，通过主动影响人类行为来提高安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶方法通常将预测和规划分开处理，无法捕捉交互场景中行动与推理的耦合关系。在不确定场景中，单纯基于预测反应可能导致不安全或过于保守的行为。需要同时估计和塑造人类行为来应对模糊性带来的风险。

Method: 1. 分层信念模型：将人类行为结构化为粗粒度离散意图和细粒度运动模式，通过贝叶斯推理进行多分辨率推理；2. 主动探测策略：识别多模态模糊性可能危及安全的情况，规划既能揭示意图又能温和引导人类决策的消歧行动；3. 运行时风险评估层：基于条件风险价值(CVaR)确保所有探测行动保持在人类风险容忍度内。

Result: 在车道合并和无信号交叉口场景的仿真中，该方法相比现有方法实现了更高的成功率和更短的完成时间。

Conclusion: 将信念推理、主动探测和风险监控相结合，为不确定性下的规划提供了一个原则性和可解释的框架，能够通过主动影响人类行为来提高交互安全性。

Abstract: Autonomous driving in complex traffic requires reasoning under uncertainty. Common approaches rely on prediction-based planning or risk-aware control, but these are typically treated in isolation, limiting their ability to capture the coupled nature of action and inference in interactive settings. This gap becomes especially critical in uncertain scenarios, where simply reacting to predictions can lead to unsafe maneuvers or overly conservative behavior. Our central insight is that safe interaction requires not only estimating human behavior but also shaping it when ambiguity poses risks. To this end, we introduce a hierarchical belief model that structures human behavior across coarse discrete intents and fine motion modes, updated via Bayesian inference for interpretable multi-resolution reasoning. On top of this, we develop an active probing strategy that identifies when multimodal ambiguity in human predictions may compromise safety and plans disambiguating actions that both reveal intent and gently steer human decisions toward safer outcomes. Finally, a runtime risk-evaluation layer based on Conditional Value-at-Risk (CVaR) ensures that all probing actions remain within human risk tolerance during influence. Our simulations in lane-merging and unsignaled intersection scenarios demonstrate that our approach achieves higher success rates and shorter completion times compared to existing methods. These results highlight the benefit of coupling belief inference, probing, and risk monitoring, yielding a principled and interpretable framework for planning under uncertainty.

</details>


### [72] [Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction](https://arxiv.org/abs/2602.14551)
*Taichi Kato,Takuya Kiyokawa,Namiko Saito,Kensuke Harada*

Main category: cs.RO

TL;DR: 本文提出了一种增强视觉语言模型的双重校正机制，通过内部逻辑验证和外部物理反馈来改善人机协作中的指令执行成功率。


<details>
  <summary>Details</summary>
Motivation: 在人机协作装配任务中，人类指令通常存在语言歧义和描述不足的问题，导致难以生成物理可行且协作的机器人行为。现有的视觉语言模型方法存在幻觉推理和无法预测物理执行失败的问题。

Method: 提出了一个HRC框架，通过双重校正机制增强VLM推理：内部校正模型在执行前验证逻辑一致性和任务可行性；外部校正模型通过执行后反馈检测和纠正物理失败。

Result: 仿真消融研究表明，与没有校正模型的基线相比，所提方法提高了成功率。真实世界实验在协作装配任务中验证了框架的有效性，支持物体固定或工具准备等任务。

Conclusion: 该框架通过双重校正机制有效解决了VLM在人机协作中的局限性，实现了对不同协作任务中人类指令的交互式重规划，验证了其实际可行性。

Abstract: Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.

</details>


### [73] [Simulation-based Learning of Electrical Cabinet Assembly Using Robot Skills](https://arxiv.org/abs/2602.14561)
*Arik Laemmle,Balázs András Bálint,Philipp Tenbrock,Frank Naegele,David Traunecker,József Váncza,Marco F. Huber*

Main category: cs.RO

TL;DR: 提出基于深度强化学习的仿真驱动方法，用于自动化DIN导轨上电气端子的力控装配，通过物理仿真训练机器人技能，实现高成功率且无需现场调优


<details>
  <summary>Details</summary>
Motivation: 传统DIN导轨端子装配存在编程工作量大、产品变异性高的问题，需要一种灵活、可扩展的自动化解决方案来适应小批量制造需求

Method: 结合深度强化学习与参数化机器人技能，在物理仿真环境中开发两种装配模型（基于梁理论的解析模型和MuJoCo刚体模型），使用SAC和TD3算法训练，采用pitasc框架实现模块化控制策略，应用领域随机化提升鲁棒性

Result: 在仿真和真实环境中均实现高成功率（最高达100%），即使存在显著位置和旋转偏差也能稳定工作，系统能泛化到新的端子类型和位置，大幅减少手动编程工作量

Conclusion: 仿真学习与模块化机器人技能结合为小批量制造提供了灵活、可扩展的自动化方案，未来将探索混合学习方法、自动化环境参数化和装配模型优化

Abstract: This paper presents a simulation-driven approach for automating the force-controlled assembly of electrical terminals on DIN-rails, a task traditionally hindered by high programming effort and product variability. The proposed method integrates deep reinforcement learning (DRL) with parameterizable robot skills in a physics-based simulation environment. To realistically model the snap-fit assembly process, we develop and evaluate two types of joining models: analytical models based on beam theory and rigid-body models implemented in the MuJoCo physics engine. These models enable accurate simulation of interaction forces, essential for training DRL agents. The robot skills are structured using the pitasc framework, allowing modular, reusable control strategies. Training is conducted in simulation using Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Domain randomization is applied to improve robustness. The trained policies are transferred to a physical UR10e robot system without additional tuning. Experimental results demonstrate high success rates (up to 100%) in both simulation and real-world settings, even under significant positional and rotational deviations. The system generalizes well to new terminal types and positions, significantly reducing manual programming effort. This work highlights the potential of combining simulation-based learning with modular robot skills for flexible, scalable automation in small-batch manufacturing. Future work will explore hybrid learning methods, automated environment parameterization, and further refinement of joining models for design integration.

</details>


### [74] [Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments](https://arxiv.org/abs/2602.14666)
*Ruofeng Wei,Kai Chen,Yui Lun Ng,Yiyao Ma,Justin Di-Lang Ho,Hon Sing Tong,Xiaomei Wang,Jing Dai,Ka-Wai Kwok,Qi Dou*

Main category: cs.RO

TL;DR: 该论文提出了用于腔内手术连续体机器人系统的视觉感知平台，通过单目内窥镜图像算法识别柔性器械位置和方向，并测量其与组织的距离，显著提高了手术操作效率。


<details>
  <summary>Details</summary>
Motivation: 腔内手术为早期胃肠道和泌尿系统癌症提供微创选择，但受限于手术工具和陡峭的学习曲线。连续体机器人系统提供柔性器械，可实现精确组织切除，但需要更好的感知和控制方法来改善手术效果。

Method: 开发了2D和3D学习型感知算法，并创建了物理逼真的模拟器来建模柔性器械动力学。该模拟器生成真实的腔内场景，支持柔性机器人控制和大量数据收集。使用连续体机器人原型进行模块和系统级评估。

Result: 算法显著改善了柔性器械的控制，在轨迹跟踪任务中将操作时间减少了70%以上，增强了对手术场景的理解，实现了更稳健的腔内手术。

Conclusion: 提出的视觉感知平台有效解决了腔内手术中柔性器械感知和控制的挑战，通过结合学习算法和物理模拟器，显著提高了手术效率和安全性，为机器人辅助腔内手术提供了重要技术支撑。

Abstract: Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.

</details>


### [75] [ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions](https://arxiv.org/abs/2602.14726)
*Kohio Deflesselle,Mélodie Daniel,Aly Magassouba,Miguel Aranda,Olivier Ly*

Main category: cs.RO

TL;DR: ManeuverNet是一个针对双阿克曼转向机器人的深度强化学习框架，结合Soft Actor-Critic与CrossQ算法，通过专门设计的奖励函数解决传统方法参数敏感和端到端DRL方法泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 农业应用中双阿克曼转向机器人需要在有限空间内执行精确复杂的机动动作。传统方法如TEB规划器依赖参数调优，对机器人配置或环境变化高度敏感；而端到端深度强化学习方法常因奖励函数不适合非完整约束导致次优策略和泛化能力差。

Method: 提出ManeuverNet框架，结合Soft Actor-Critic与CrossQ算法，引入四种专门设计的奖励函数来支持机动学习。该方法不依赖专家数据或人工指导。

Result: 相比最先进的DRL基线，ManeuverNet显著提高了机动性和成功率，获得超过40%的性能提升；有效缓解了TEB规划器的强参数敏感性；在实际试验中实现了高达90%的机动轨迹效率提升。

Conclusion: ManeuverNet框架在双阿克曼转向机器人控制中表现出优越的机动性能、鲁棒性和实际应用价值，解决了传统方法和端到端DRL方法的局限性。

Abstract: Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.

</details>


### [76] [Analysis of a Cuspidal 6R Robot](https://arxiv.org/abs/2602.14794)
*Alexander Feeß,Martin Weiß*

Main category: cs.RO

TL;DR: 本文对"Transpressor"（尖点6R机器人）进行了运动学理论和数值分析，该机器人最多有16个逆运动学解，并提供了特殊目标位姿的解析解和通用数值求解器，同时通过雅可比行列式分析证明了该类机器人的尖点特性。


<details>
  <summary>Details</summary>
Motivation: 研究Transpressor这类尖点6R机器人的运动学特性，特别是其多解性和尖点行为，这对于理解和控制这类复杂机器人具有重要意义。

Method: 采用几何方法描述逆运动学解，为特殊目标位姿提供解析解，开发通用数值求解器，并通过分析雅可比行列式在解路径上的变化来证明尖点特性。

Result: Transpressor最多有16个逆运动学解，建立了特殊目标位姿的解析解，开发了有效的数值求解器，并通过雅可比行列式分析证明了该类机器人的尖点特性。

Conclusion: 成功分析了Transpressor机器人的运动学特性，建立了多解描述方法，提供了求解工具，并理论证明了其尖点行为，为这类复杂机器人的理解和控制提供了理论基础。

Abstract: We present a theoretical and numerical analysis of the kinematics for the "Transpressor", a cuspidal 6R robot. It admits up to 16 inverse kinematics solutions which are described geometrically. For special target poses, we provide the solutions analytically and present a simple numerical solver for the general case. Moreover, an analytical estimate of the Jacobian determinant on a path between two solutions proves cuspidality for a class of robots similar to the transpressor.

</details>


### [77] [Scalable Multi-Robot Path Planning via Quadratic Unconstrained Binary Optimization](https://arxiv.org/abs/2602.14799)
*Javier González Villasmil*

Main category: cs.RO

TL;DR: 该论文提出了一种基于QUBO（二次无约束二进制优化）的多智能体路径规划方法，通过BFS预处理、自适应惩罚设计和时间窗口分解策略，在网格环境中实现了可扩展的路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统集中式多智能体路径规划方法随着智能体数量增加会出现指数级的状态空间增长，需要寻找结构上更具可扩展性的替代方案。

Method: 采用QUBO框架，结合BFS逻辑预处理（减少95%以上变量）、自适应惩罚设计用于碰撞和约束执行，以及时间窗口分解策略以适应当前硬件限制。

Result: 在最多4个机器人的网格环境实验中，该方法在密集场景中实现了接近最优的解决方案，相比顺序经典规划展现出更好的扩展性。

Conclusion: 该研究为未来量子计算和量子启发式的多机器人协调建立了实用且可复现的基准，证明了QUBO在多智能体路径规划中的可行性。

Abstract: Multi-Agent Path Finding (MAPF) remains a fundamental challenge in robotics, where classical centralized approaches exhibit exponential growth in joint-state complexity as the number of agents increases. This paper investigates Quadratic Unconstrained Binary Optimization (QUBO) as a structurally scalable alternative for simultaneous multi-robot path planning. This approach is a robotics-oriented QUBO formulation incorporating BFS-based logical pre-processing (achieving over 95% variable reduction), adaptive penalty design for collision and constraint enforcement, and a time-windowed decomposition strategy that enables execution within current hardware limitations. An experimental evaluation in grid environments with up to four robots demonstrated near-optimal solutions in dense scenarios and favorable scaling behavior compared to sequential classical planning. These results establish a practical and reproducible baseline for future quantum and quantum-inspired multi-robot coordinations.

</details>


### [78] [Affordance Transfer Across Object Instances via Semantically Anchored Functional Map](https://arxiv.org/abs/2602.14874)
*Xiaoxiang Dong,Weiming Zhi*

Main category: cs.RO

TL;DR: 提出Semantic Anchored Functional Maps框架，通过单次视觉演示实现不同几何形状物体间的功能对应关系迁移，解决传统示教学习需要大量物理演示的问题。


<details>
  <summary>Details</summary>
Motivation: 传统示教学习需要大量物理演示，耗时且难以扩展；现有从人类视频学习的方法难以将交互线索泛化到几何形状不同但功能相似的物体实例上。

Method: 基于图像重建的粗略网格，识别物体间的语义对应功能区域，选择互斥的语义锚点，通过功能映射在表面上传播约束，获得密集的语义一致对应关系。

Result: 在合成物体类别和真实机器人操作任务上的实验表明，该方法能以适中的计算成本实现准确的功能迁移，适合实际机器人感知-行动流程。

Conclusion: SemFM框架为从单次视觉演示中跨几何多样物体迁移交互区域提供了一种轻量级、可解释的方法，推动了机器人学习从演示的实用化。

Abstract: Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.

</details>


### [79] [Morphing of and writing with a scissor linkage mechanism](https://arxiv.org/abs/2602.14958)
*Mohanraj A,S Ganga Prasath*

Main category: cs.RO

TL;DR: 本文研究剪刀单元机构的运动学特性，通过几何优化实现形状变换和书写任务，展示了在复杂领域自动化导航和检测的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究剪刀单元机构的运动学与几何耦合关系，探索如何利用其单自由度特性实现可重复运动，为复杂领域的自动化导航和检测提供解决方案。

Method: 通过推导剪刀单元的有效曲率和机构尖端轨迹的几何变量表达式，将形状变换和书写任务表述为优化问题，利用可微分仿真框架进行求解。

Result: 实验结果表明，剪刀单元机构的几何特性可用于复杂领域的自动化导航和检测，但快速编程和无反馈实验中的无差错实施仍存在挑战。

Conclusion: 剪刀单元机构的几何特性结合优化框架可用于自动化任务，但需要解决快速编程和实验实施中的挑战。

Abstract: Kinematics of mechanisms is intricately coupled to their geometry and their utility often arises out of the ability to perform reproducible motion with fewer actuating degrees of freedom. In this article, we explore the assembly of scissor-units, each made of two rigid linear members connected by a pin joint. The assembly has a single degree of freedom, where actuating any single unit results in a shape change of the entire assembly. We derive expressions for the effective curvature of the unit and the trajectory of the mechanism's tip as a function of the geometric variables which we then use as the basis to program two tasks in the mechanism: shape morphing and writing. By phrasing these tasks as optimization problems and utilizing the differentiable simulation framework, we arrive at solutions that are then tested in table-top experiments. Our results show that the geometry of scissor assemblies can be leveraged for automated navigation and inspection in complex domains, in light of the optimization framework. However, we highlight that the challenges associated with rapid programming and error-free implementation in experiments without feedback still remain.

</details>


### [80] [RynnBrain: Open Embodied Foundation Models](https://arxiv.org/abs/2602.14979)
*Ronghao Dang,Jiayan Guo,Bohan Hou,Sicong Leng,Kehan Li,Xin Li,Jiangpin Liu,Yunxuan Mao,Zhikai Wang,Yuqian Yuan,Minghao Zhu,Xiao Lin,Yang Bai,Qian Jiang,Yaxi Zhao,Minghua Zeng,Junlong Gao,Yuming Jiang,Jun Cen,Siteng Huang,Liuyi Wang,Wenqiao Zhang,Chengju Liu,Jianfei Yang,Shijian Lu,Deli Zhao*

Main category: cs.RO

TL;DR: RynnBrain是一个开源的时空基础模型，用于具身智能，统一了感知、推理和规划能力，在多个基准测试中显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型发展迅速，但具身智能领域仍缺乏一个统一、物理基础的基础模型，能够将感知、推理和规划整合到真实世界的时空动态中。

Method: 提出了RynnBrain，一个开源时空基础模型，在统一框架中强化四个核心能力：全面的自我中心理解、多样化的时空定位、物理基础推理和物理感知规划。模型家族包括三个基础模型规模（2B、8B和30B-A3B MoE）和四个针对下游具身任务或复杂空间推理任务的后训练变体。

Result: 在20个具身基准测试和8个通用视觉理解基准测试中，RynnBrain基础模型大幅超越现有具身基础模型。后训练模型套件进一步证实了RynnBrain的两个关键潜力：支持物理基础推理和规划，以及作为可高效适应多种具身任务的强大预训练骨干。

Conclusion: RynnBrain为具身智能提供了一个统一、物理基础的时空基础模型，在感知、推理和规划方面表现出色，为具身智能研究提供了强大的基础架构。

Abstract: Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.

</details>


### [81] [BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames](https://arxiv.org/abs/2602.15010)
*Max Sobol Mark,Jacky Liang,Maria Attarian,Chuyuan Fu,Debidatta Dwibedi,Dhruv Shah,Aviral Kumar*

Main category: cs.RO

TL;DR: 提出Big Picture Policies (BPP)方法，通过视觉语言模型检测关键帧来减少训练与部署间的分布偏移，解决机器人策略在历史观测条件化中的虚假相关性问题。


<details>
  <summary>Details</summary>
Motivation: 许多机器人任务需要关注历史观测，但当前最佳策略通常仅基于当前观测，限制了其应用。朴素地条件化历史观测常因虚假相关性而失败，这源于训练期间历史空间覆盖不足。

Method: 提出Big Picture Policies (BPP)方法，使用视觉语言模型检测最小化有意义的任务相关关键帧，将多样化的轨迹投影到紧凑的事件集合上，减少分布偏移同时保持表达能力。

Result: 在四个真实世界操作任务和三个仿真任务上评估BPP，所有任务都需要历史条件化。BPP在真实世界评估中比最佳对比方法获得70%更高的成功率。

Conclusion: BPP通过检测任务相关关键帧有效解决了历史条件化中的虚假相关性问题，显著提高了机器人策略在需要记忆历史的任务上的性能。

Abstract: Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.

</details>


### [82] [Neurosim: A Fast Simulator for Neuromorphic Robot Perception](https://arxiv.org/abs/2602.15018)
*Richeek Das,Pratik Chaudhari*

Main category: cs.RO

TL;DR: Neurosim是一个高性能传感器模拟库，支持动态视觉传感器、RGB相机、深度传感器和惯性传感器模拟，以及多旋翼飞行器动力学模拟，桌面GPU上可达~2700 FPS。配合Cortex通信库，支持机器学习和机器人工作流。


<details>
  <summary>Details</summary>
Motivation: 为神经形态感知和控制算法提供高效的训练和测试平台，支持多模态数据同步和实时闭环测试，解决现有模拟工具在性能和集成性方面的不足。

Method: 开发了Neurosim高性能传感器模拟库，支持多种传感器类型和飞行器动力学模拟；配合基于ZeroMQ的Cortex通信库，提供高吞吐、低延迟的消息传递系统，支持NumPy数组和PyTorch张量。

Result: Neurosim在桌面GPU上实现了高达~2700 FPS的帧率，能够实时模拟复杂动态环境；Cortex提供了高效的Python/C++通信接口，支持自监督学习在时间同步多模态数据上的训练。

Conclusion: Neurosim和Cortex为神经形态感知和控制算法的开发和测试提供了高效、集成的平台，支持从训练到实时闭环测试的完整工作流程，已在GitHub开源。

Abstract: Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [83] [Discrete Gene Crossover Accelerates Solution Discovery in Quality-Diversity Algorithms](https://arxiv.org/abs/2602.13730)
*Joshua Hutchinson,J. Michael Herrmann,Simón C. Smith*

Main category: cs.NE

TL;DR: 该论文提出了一种结合离散基因级交叉的变异算子，用于增强质量多样性算法的搜索能力，特别是在优化后期阶段能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 质量多样性算法在搜索过程中经常停滞，因为增量变异算子难以在大型种群中传播构建块。现有的变异算子依赖渐进式变化，限制了它们有效探索远离父代解的搜索空间区域或传播有益遗传物质的能力。

Method: 提出了一种变异算子，通过离散的基因级交叉增强基于变异的算子，实现精英遗传物质的快速重组。这种交叉机制模拟了生物学的减数分裂原理，促进遗传物质的直接转移和超越现有精英超体积的新基因型配置探索。

Result: 在三个运动环境中评估算子，结果显示在QD分数、覆盖率和最大适应度方面均有改进，特别是在优化后期阶段，一旦构建块在存档中建立，表现出特别强的性能。

Conclusion: 添加离散交叉变异提供了一种互补的探索机制，能够持续推动质量多样性增长，超越了现有算子的性能表现。

Abstract: Quality-Diversity (QD) algorithms aim to discover diverse, high-performing solutions across behavioral niches. However, QD search often stagnates as incremental variation operators struggle to propagate building blocks across large populations. Existing mutation operators rely on gradual variation to solutions, limiting their ability to efficiently explore regions of the search space distant from parent solutions or to spread beneficial genetic material through the population. We propose a mutation operator which augments variation-based operators with discrete, gene-level crossover, enabling rapid recombination of elite genetic material. This crossover mechanism mirrors the biological principle of meiosis and facilitates both the direct transfer of genetic material and the exploration of novel genotype configurations beyond the existing elite hypervolume. We evaluate operators on three locomotion environments, demonstrating improvements in QD score, coverage, and max fitness, with particularly strong performance in later stages of optimization once building blocks have been established in the archive. These results show that the addition of a discrete crossover mutation provides a complementary exploration mechanism that sustains quality-diversity growth beyond the performance demonstrated by existing operators.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [84] [BreathNet: Generalizable Audio Deepfake Detection via Breath-Cue-Guided Feature Refinement](https://arxiv.org/abs/2602.13596)
*Zhe Ye,Xiangui Kang,Jiayi He,Chengxin Chen,Wei Zhu,Kai Wu,Yin Yang,Jiwu Huang*

Main category: cs.SD

TL;DR: BreathNet：一种集成细粒度呼吸信息的新型音频深度伪造检测框架，通过BreathFiLM机制选择性增强呼吸声音相关的时序表征，结合频谱特征融合和多特征损失函数，在多个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造音频变得更加逼真和多样化，开发泛化性强的检测系统变得至关重要。现有检测方法主要依赖XLS-R前端特征来提升泛化能力，但性能仍然有限，部分原因是缺乏对生理线索或频域特征等细粒度信息的关注。

Method: 提出BreathNet框架：1）设计BreathFiLM特征线性调制机制，基于呼吸声存在选择性增强时序表征；2）结合频率前端提取频谱特征，与时序特征融合提供互补信息；3）提出包含正样本监督对比损失、中心损失和对比损失的特征损失组，增强特征空间中的判别能力。

Result: 在五个基准数据集上取得SOTA性能：使用ASVspoof 2019 LA训练集，在四个相关评估基准上平均EER为1.99%，在In-the-Wild数据集上EER为4.70%；在ASVspoof5评估协议下，最新基准上EER为4.94%。

Conclusion: BreathNet通过整合细粒度呼吸信息和多特征融合策略，显著提升了音频深度伪造检测的泛化能力和性能，为应对日益复杂的深度伪造音频提供了有效的解决方案。

Abstract: As deepfake audio becomes more realistic and diverse, developing generalizable countermeasure systems has become crucial. Existing detection methods primarily depend on XLS-R front-end features to improve generalization. Nonetheless, their performance remains limited, partly due to insufficient attention to fine-grained information, such as physiological cues or frequency-domain features. In this paper, we propose BreathNet, a novel audio deepfake detection framework that integrates fine-grained breath information to improve generalization. Specifically, we design BreathFiLM, a feature-wise linear modulation mechanism that selectively amplifies temporal representations based on the presence of breathing sounds. BreathFiLM is trained jointly with the XLS-R extractor, in turn encouraging the extractor to learn and encode breath-related cues into the temporal features. Then, we use the frequency front-end to extract spectral features, which are then fused with temporal features to provide complementary information introduced by vocoders or compression artifacts. Additionally, we propose a group of feature losses comprising Positive-only Supervised Contrastive Loss (PSCL), center loss, and contrast loss. These losses jointly enhance the discriminative ability, encouraging the model to separate bona fide and deepfake samples more effectively in the feature space. Extensive experiments on five benchmark datasets demonstrate state-of-the-art (SOTA) performance. Using the ASVspoof 2019 LA training set, our method attains 1.99% average EER across four related eval benchmarks, with particularly strong performance on the In-the-Wild dataset, where it achieves 4.70% EER. Moreover, under the ASVspoof5 evaluation protocol, our method achieves an EER of 4.94% on this latest benchmark.

</details>


### [85] [Enhancing spatial hearing with cochlear implants: exploring the role of AI, multimodal interaction and perceptual training](https://arxiv.org/abs/2602.13787)
*Lorenzo Picinali,Robert Baumgartner,Valerie Gaveau,Antonino Greco,Stefanie Liebe,Paul Oomen,Christoph Braun*

Main category: cs.SD

TL;DR: 提出多学科研究框架改善人工耳蜗使用者的空间听觉能力


<details>
  <summary>Details</summary>
Motivation: 虽然人工耳蜗在恢复听力和言语理解方面取得了显著进展，但空间听觉这一对注意力控制和嘈杂环境中言语理解至关重要的能力在过去被忽视，需要改进

Method: 提出医生、心理学家和工程师协作的多学科研究框架

Result: 论文提出了一个研究框架，但尚未报告具体实验结果

Conclusion: 需要通过多学科协作来改善人工耳蜗使用者的空间听觉能力

Abstract: Cochlear implants (CIs) have been developed to the point where they can restore hearing and speech understanding in a large proportion of patients. Although spatial hearing is central to controlling and directing attention and to enabling speech understanding in noisy environments, it has been largely neglected in the past. We propose here a multi-disciplinary research framework in which physicians, psychologists and engineers collaborate to improve spatial hearing for CI users.

</details>


### [86] [Audiocards: Structured Metadata Improves Audio Language Models For Sound Design](https://arxiv.org/abs/2602.13835)
*Sripathi Sridhar,Prem Seetharaman,Oriol Nieto,Mark Cartwright,Justin Salamon*

Main category: cs.SD

TL;DR: 本文提出audiocards结构化元数据方法，利用LLM的世界知识生成基于声学属性和声音描述符的元数据，改善声音设计的文本-音频检索和描述生成任务。


<details>
  <summary>Details</summary>
Motivation: 声音设计师在大型音效库中搜索声音时依赖元数据，但现有元数据往往缺失或不完整，需要大量人工标注。现有的自动元数据生成和文本-音频检索方法没有针对声音设计特有的结构和信息进行训练。

Method: 提出audiocards方法，利用大型语言模型的世界知识生成结构化元数据，这些元数据基于声学属性和声音描述符。创建了音效audiocards数据集用于训练音频语言模型。

Result: 在专业音效库上，基于audiocards的训练显著提升了文本-音频检索、描述性字幕生成和元数据生成性能。同时，在通用音频字幕和检索任务上也优于传统的单句字幕方法。

Conclusion: audiocards为声音设计提供了有效的结构化元数据解决方案，通过利用LLM的世界知识改善了音频语言建模性能，并发布了数据集以促进该领域进一步研究。

Abstract: Sound designers search for sounds in large sound effects libraries using aspects such as sound class or visual context. However, the metadata needed for such search is often missing or incomplete, and requires significant manual effort to add. Existing solutions to automate this task by generating metadata, i.e. captioning, and search using learned embeddings, i.e. text-audio retrieval, are not trained on metadata with the structure and information pertinent to sound design. To this end we propose audiocards, structured metadata grounded in acoustic attributes and sonic descriptors, by exploiting the world knowledge of LLMs. We show that training on audiocards improves downstream text-audio retrieval, descriptive captioning, and metadata generation on professional sound effects libraries. Moreover, audiocards also improve performance on general audio captioning and retrieval over the baseline single-sentence captioning approach. We release a curated dataset of sound effects audiocards to invite further research in audio language modeling for sound design.

</details>


### [87] [GSRM: Generative Speech Reward Model for Speech RLHF](https://arxiv.org/abs/2602.13891)
*Maohao Shen,Tejas Jayashankar,Osama Hanna,Naoyuki Kanda,Yancheng Wang,Kateřina Žmolíková,Ruiming Xie,Niko Moritz,Anfeng Xu,Yashesh Gaur,Gregory Wornell,Qing He,Jilong Wu*

Main category: cs.SD

TL;DR: 提出生成式语音奖励模型GSRM，通过可解释的声学特征提取和基于特征的思维链推理来评估语音自然度，显著超越现有方法并接近人类评分一致性。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型（如GPT-4o Voice Mode和Gemini Live）的生成质量虽有进步，但合成音频的美学自然度仍不及人类语音。提升生成质量需要可靠的语音自然度评估器，而现有评估器通常将原始音频回归为标量分数，解释性有限且难以泛化到不同分类的语音。

Method: 提出生成式语音奖励模型GSRM，采用推理中心的奖励建模方法。模型将语音自然度评估分解为两个阶段：1）可解释的声学特征提取；2）基于特征的思维链推理，实现可解释的判断。为此构建了大规模人类反馈数据集，包含31k专家评分和真实世界用户-助手语音交互的域外基准。

Result: 实验表明GSRM显著优于现有语音自然度预测器，其自然度分数预测的模型-人类相关性接近人类评分者间一致性。进一步展示GSRM可作为在线RLHF的有效验证器，提升语音LLM生成的自然度。

Conclusion: GSRM通过可解释的声学特征提取和基于特征的思维链推理，实现了对语音自然度的有效评估，不仅超越了现有方法，还能作为RLHF的验证器提升语音生成质量，为语音自然度评估提供了新的解决方案。

Abstract: Recent advances in speech language models, such as GPT-4o Voice Mode and Gemini Live, have demonstrated promising speech generation capabilities. Nevertheless, the aesthetic naturalness of the synthesized audio still lags behind that of human speech. Enhancing generation quality requires a reliable evaluator of speech naturalness. However, existing naturalness evaluators typically regress raw audio to scalar scores, offering limited interpretability of the evaluation and moreover fail to generalize to speech across different taxonomies. Inspired by recent advances in generative reward modeling, we propose the Generative Speech Reward Model (GSRM), a reasoning-centric reward model tailored for speech. The GSRM is trained to decompose speech naturalness evaluation into an interpretable acoustic feature extraction stage followed by feature-grounded chain-of-thought reasoning, enabling explainable judgments. To achieve this, we curated a large-scale human feedback dataset comprising 31k expert ratings and an out-of-domain benchmark of real-world user-assistant speech interactions. Experiments show that GSRM substantially outperforms existing speech naturalness predictors, achieving model-human correlation of naturalness score prediction that approaches human inter-rater consistency. We further show how GSRM can improve the naturalness of speech LLM generations by serving as an effective verifier for online RLHF.

</details>


### [88] [Eureka-Audio: Triggering Audio Intelligence in Compact Language Models](https://arxiv.org/abs/2602.13954)
*Dan Zhang,Yishu Lei,Jing Hu,Shuwei He,Songhe Deng,Xianlong Luo,Danxiang Zhu,Shikun Feng,Rui Liu,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.SD

TL;DR: Eureka-Audio是一个仅含17亿参数的紧凑音频语言模型，在多项音频理解基准测试中性能媲美4-18倍大的模型，通过统一架构和高质量数据合成实现高效性能平衡。


<details>
  <summary>Details</summary>
Motivation: 当前音频语言模型通常参数庞大、计算成本高，需要开发轻量级但高性能的模型，以在有限计算资源下实现广泛的音频理解能力，包括语音识别、音频理解和密集音频描述等任务。

Method: 采用统一端到端架构：轻量级语言主干、Whisper音频编码器和稀疏激活的MoE适配器处理音频异质性；引入DataFlux闭环音频指令数据合成与验证管道，从原始音频构建高质量、逻辑一致的监督数据。

Result: 在ASR、知识推理、安全性、指令跟随和副语言基准测试中，Eureka-Audio性能媲美或超越多个70亿到300亿参数的音频和全模态基线模型，在计算成本和性能间实现高效平衡。

Conclusion: Eureka-Audio为轻量级音频理解模型建立了强大实用的基准，证明了通过精心设计的架构和高质量数据合成，小模型也能在音频理解任务上取得与大模型竞争的性能。

Abstract: We present Eureka-Audio, a compact yet high-performance audio language model that achieves competitive performance against models that are 4 to 18 times larger across a broad range of audio understanding benchmarks. Despite containing only 1.7B parameters, Eureka-Audio demonstrates strong performance on automatic speech recognition (ASR), audio understanding, and dense audio captioning, matching or surpassing multiple 7B to 30B audio and omni-modal baselines. The model adopts a unified end-to-end architecture composed of a lightweight language backbone, a Whisper-based audio encoder, and a sparsely activated Mixture-of-Experts (MoE) adapter that explicitly accounts for audio heterogeneity and alleviates cross-modal optimization conflicts under limited capacity. To further enhance paralinguistic reasoning, we introduce DataFlux, a closed loop audio instruction data synthesis and verification pipeline that constructs high quality, logically consistent supervision from raw audio. Extensive evaluations across ASR, knowledge reasoning, safety, instruction following, and paralinguistic benchmarks, demonstrate that Eureka-Audio achieves an efficient balance between computational cost and performance. These results establish Eureka Audio as a strong and practical baseline for lightweight audio understanding models.

</details>


### [89] [MUKA: Multi Kernel Audio Adaptation Of Audio-Language Models](https://arxiv.org/abs/2602.14127)
*Reda Bensaid,Amine Ouasfi,Yassir Bendou,Ilyass Moummad,Vincent Gripon,François Leduc-Primeau,Adnane Boukhayma*

Main category: cs.SD

TL;DR: MUKA是一个多核适应框架，通过结合指令调优模型的细粒度表示和对比预训练模型的全局语义表示，在少样本音频任务中实现高效适应，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型虽然具有强大的泛化能力，但在少样本场景下高效适应新任务仍是一个关键挑战。本研究旨在探索大型音频语言模型（ALMs）的少样本适应方法，包括基于训练和不基于训练的方法。

Method: 提出MUKA多核适应框架，结合了指令调优模型（如Pengi）的细粒度、上下文相关表示和对比预训练方法（如CLAP）的全局语义表示。通过构建对齐局部相似性与全局语义的乘积核，增强表示能力，同时保持核方法的理论保证，避免额外训练。

Result: 在11个不同的音频数据集上进行广泛实验，MUKA在无需训练的方法中达到了最先进的性能，甚至在多个场景中超越了基于训练的适配器方法，在适应性和效率之间提供了良好的平衡。

Conclusion: MUKA框架通过结合不同类型模型的优势，为大型音频语言模型的少样本适应提供了一个高效且有效的解决方案，在保持理论保证的同时实现了优异的性能表现。

Abstract: Multimodal foundation models have demonstrated impressive generalization capabilities, yet efficiently adapting them to new tasks in a few-shot setting remains a critical challenge. In this work, we investigate the few-shot adaptation of Large Audio-Language Models (ALMs) through both training-based and training-free approaches. We introduce MUKA, a multi-kernel adaptation framework that combines the fine-grained, context-dependent representations of instruction-tuning based models like Pengi with the global semantic representations of contrastive pretraining methods like CLAP. By constructing a product kernel that aligns local similarity with global semantics, MUKA enhances representational power while preserving the theoretical guarantees of kernel methods and avoiding additional training. Extensive experiments across 11 diverse audio datasets demonstrate that MUKA achieves state-of-the-art performance among training-free methods and even surpasses training-based adapters in several scenarios, offering a compelling balance between adaptability and efficiency.

</details>


### [90] [Investigation for Relative Voice Impression Estimation](https://arxiv.org/abs/2602.14172)
*Keinichi Fujita,Yusuke Ijima*

Main category: cs.SD

TL;DR: 本研究提出相对声音印象估计（RIE）框架，用于预测同一说话者两个话语之间的感知差异，而非传统的绝对评分。研究比较了三种建模方法：传统声学特征、自监督语音表示和多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 副语言和非语言因素对听众印象有重要影响。现有研究多关注绝对印象评分，而本研究旨在探索相对声音印象估计，即预测同一说话者两个话语之间的感知差异，这能更精细地捕捉语音表达的变化。

Method: 使用专业说话者以不同风格朗读同一文本的录音，以隔离表达性和韵律变化。比较三种建模方法：1）用于语音情感识别的经典声学特征；2）自监督语音表示；3）多模态大语言模型。估计目标是基于主观评估的低维向量，量化第二个话语相对于第一个话语在反义轴（如"暗-亮"）上的感知变化。

Result: 使用自监督表示的方法优于基于经典声学特征的方法，特别是在捕捉复杂动态印象（如"冷-暖"）方面，经典特征表现不佳。当前的多模态大语言模型在这个细粒度的成对任务中表现不可靠。

Conclusion: 这是对相对声音印象估计的首次系统研究，证明了自监督语音模型在捕捉细微感知变化方面的优势，为语音表达分析提供了新视角。

Abstract: Paralinguistic and non-linguistic aspects of speech strongly influence listener impressions. While most research focuses on absolute impression scoring, this study investigates relative voice impression estimation (RIE), a framework for predicting the perceptual difference between two utterances from the same speaker. The estimation target is a low-dimensional vector derived from subjective evaluations, quantifying the perceptual shift of the second utterance relative to the first along an antonymic axis (e.g., ``Dark--Bright''). To isolate expressive and prosodic variation, we used recordings of a professional speaker reading a text in various styles. We compare three modeling approaches: classical acoustic features commonly used for speech emotion recognition, self-supervised speech representations, and multimodal large language models (MLLMs). Our results demonstrate that models using self-supervised representations outperform methods with classical acoustic features, particularly in capturing complex and dynamic impressions (e.g., ``Cold--Warm'') where classical features fail. In contrast, current MLLMs prove unreliable for this fine-grained pairwise task. This study provides the first systematic investigation of RIE and demonstrates the strength of self-supervised speech models in capturing subtle perceptual variations.

</details>


### [91] [The Interspeech 2026 Audio Reasoning Challenge: Evaluating Reasoning Process Quality for Audio Reasoning Models and Agents](https://arxiv.org/abs/2602.14224)
*Ziyang Ma,Ruiyang Xu,Yinghao Ma,Chao-Han Huck Yang,Bohan Li,Jaeyeon Kim,Jin Xu,Jinyu Li,Carlos Busso,Kai Yu,Eng Siong Chng,Xie Chen*

Main category: cs.SD

TL;DR: Interspeech 2026音频推理挑战赛首次评估音频领域的思维链质量，引入MMAR-Rubrics评估框架，吸引了全球156支团队参与，结果显示智能体系统在推理质量上领先。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型虽然理解能力强，但缺乏透明的推理过程，存在"黑箱"限制，需要专门的评估框架来提升音频领域的可解释性。

Method: 通过组织Audio Reasoning Challenge竞赛，引入MMAR-Rubrics实例级评估协议，评估推理链的事实性和逻辑性，设置单模型和智能体两个赛道。

Result: 吸引了来自18个国家和地区的156支团队参与，结果显示智能体系统通过迭代工具编排和跨模态分析在推理质量上领先，单模型系统通过强化学习和复杂数据管道也在快速进步。

Conclusion: 该挑战赛为可解释音频智能提供了新的见解，展示了当前音频推理技术的发展现状和未来方向，智能体系统和单模型系统各有优势并都在持续改进。

Abstract: Recent Large Audio Language Models (LALMs) excel in understanding but often lack transparent reasoning. To address this "black-box" limitation, we organized the Audio Reasoning Challenge at Interspeech 2026, the first shared task dedicated to evaluating Chain-of-Thought (CoT) quality in the audio domain. The challenge introduced MMAR-Rubrics, a novel instance-level protocol assessing the factuality and logic of reasoning chains. Featured Single Model and Agent tracks, the competition attracting 156 teams from 18 countries and regions. Results show agent systems currently lead in reasoning quality, utilizing iterative tool orchestration and cross-modal analysis. Besides, single models are rapidly advancing via reinforcement learning and sophisticated data pipeline. We details the challenge design, methodology, and a comprehensive analysis of state-of-the-art systems, providing new insights for explainable audio intelligence.

</details>


### [92] [Bengali-Loop: Community Benchmarks for Long-Form Bangla ASR and Speaker Diarization](https://arxiv.org/abs/2602.14291)
*H. M. Shadman Tabib,Istiak Ahmmed Rifti,Abdullah Muhammed Amimul Ehsan,Somik Dasgupta,Md Zim Mim Siddiqee Sowdha,Abrar Jahin Sarker,Md. Rafiul Islam Nijamy,Tanvir Hossain,Mst. Metaly Khatun,Munzer Mahmood,Rakesh Debnath,Gourab Biswas,Asif Karim,Wahid Al Azad Navid,Masnoon Muztahid,Fuad Ahmed Udoy,Shahad Shahriar Rahman,Md. Tashdiqur Rahman Shifat,Most. Sonia Khatun,Mushfiqur Rahman,Md. Miraj Hasan,Anik Saha,Mohammad Ninad Mahmud Nobo,Soumik Bhattacharjee,Tusher Bhomik,Ahmmad Nur Swapnil,Shahriar Kabir*

Main category: cs.SD

TL;DR: 本文介绍了Bengali-Loop，这是两个针对孟加拉语长语音处理的社区基准：一个包含191个录音（158.6小时，79.2万字）的长语音ASR语料库，以及一个包含24个录音（22小时，5744个标注片段）的说话人日志语料库，旨在解决孟加拉语长语音技术资源不足的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管孟加拉语（Bangla）使用广泛，但在长语音技术方面仍然资源不足。本文旨在填补这一空白，为孟加拉语的长语音自动语音识别（ASR）和说话人日志（diarization）提供标准化的基准数据集。

Method: 通过可重复的字幕提取流程和人工参与循环的转录验证，从11个YouTube频道收集了191个长语音录音。说话人日志语料库包含24个录音，采用完全手动标注的说话人转换标签（CSV格式）。两个基准都针对现实的多说话人、长时长内容（如孟加拉戏剧）。

Result: 建立了基线性能：Tugstugi模型的词错误率（WER）为34.07%，pyannote.audio模型的说话人日志错误率（DER）为40.08%。提供了标准化的评估协议（WER/CER，DER）、标注规则和数据格式，支持可重复的基准测试和未来模型开发。

Conclusion: Bengali-Loop为孟加拉语长语音ASR和说话人日志提供了首个社区基准，填补了该语言在长语音技术方面的资源空白，为未来的研究和模型开发提供了标准化评估框架。

Abstract: Bengali (Bangla) remains under-resourced in long-form speech technology despite its wide use. We present Bengali-Loop, two community benchmarks to address this gap: (1) a long-form ASR corpus of 191 recordings (158.6 hours, 792k words) from 11 YouTube channels, collected via a reproducible subtitle-extraction pipeline and human-in-the-loop transcript verification; and (2) a speaker diarization corpus of 24 recordings (22 hours, 5,744 annotated segments) with fully manual speaker-turn labels in CSV format. Both benchmarks target realistic multi-speaker, long-duration content (e.g., Bangla drama/natok). We establish baselines (Tugstugi: 34.07% WER; pyannote.audio: 40.08% DER) and provide standardized evaluation protocols (WER/CER, DER), annotation rules, and data formats to support reproducible benchmarking and future model development for Bangla long-form ASR and diarization.

</details>


### [93] [Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions](https://arxiv.org/abs/2602.14664)
*Parth Khadse,Sunil Kumar Kopparapu*

Main category: cs.SD

TL;DR: 该研究通过实验探讨人类解剖学约束对端到端文本转语音系统训练的影响，比较了正向和反向文本-语音组合的TTS系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索人类解剖学约束（如发音器官的物理限制和发音配置间的平滑过渡）是否会影响端到端TTS系统的训练效果。人类语音涉及复杂的发音配置转换，某些配置难以模仿或过渡，研究者想了解这些解剖学限制对TTS模型训练是否有影响。

Method: 研究方法包括：1）使用两种端到端TTS架构：自回归模型Tacotron-2和非自回归模型VITS-TTS；2）构建三种不同的TTS系统：a) 正向文本-正向语音（传统e2e-TTS），b) 反向文本-反向语音（r-e2e-TTS），c) 反向文本-正向语音（rtfs-e2e-TTS）；3）通过实验比较这些系统的性能差异。

Result: 实验结果表明：1）端到端TTS系统是纯粹数据驱动的；2）有趣的是，反向文本-反向语音系统（r-e2e-TTS）生成的语音在保真度、感知可懂度和自然度方面表现更好。

Conclusion: 研究结论是端到端TTS系统确实受到人类解剖学约束的影响，反向文本-反向语音的训练方式能够产生质量更高的合成语音，这可能是因为这种训练方式更好地模拟了人类发音器官的物理限制和发音配置间的自然过渡。

Abstract: An end-to-end (e2e) text-to-speech (TTS) system is a deep architecture that learns to associate a text string with acoustic speech patterns from a curated dataset. It is expected that all aspects associated with speech production, such as phone duration, speaker characteristics, and intonation among other things are captured in the trained TTS model to enable the synthesized speech to be natural and intelligible. Human speech is complex, involving smooth transitions between articulatory configurations (ACs). Due to anatomical constraints, some ACs are challenging to mimic or transition between. In this paper, we experimentally study if the constraints imposed by human anatomy have an implication on training an e2e-TTS systems. We experiment with two e2e-TTS architectures, namely, Tacotron-2 an autoregressive model and VITS-TTS a non-autoregressive model. In this study, we build TTS systems using (a) forward text, forward speech (conventional, e2e-TTS), (b) reverse text, reverse speech (r-e2e-TTS), and (c) reverse text, forward speech (rtfs-e2e-TTS). Experiments demonstrate that e2e-TTS systems are purely data-driven. Interestingly, the generated speech by r-e2e-TTS systems exhibits better fidelity, better perceptual intelligibility, and better naturalness

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [94] [The Complexity of Tournament Fixing: Subset FAS Number and Acyclic Neighborhoods](https://arxiv.org/abs/2602.13422)
*Yuxi Liu,Junqiang Peng,Mingyu Xiao*

Main category: cs.GT

TL;DR: 本文解决了锦标赛安排问题(TFP)关于子集FAS数的参数化复杂性：证明了即使子集FAS数为常数且入邻域或出邻域是无环的，TFP仍为NP难；但当两者都无环时，TFP关于子集FAS数是FPT的。


<details>
  <summary>Details</summary>
Motivation: 虽然已知TFP在某些参数（如反馈弧/顶点集数、入/出度）下是固定参数可解的，但对于子集FAS数这一更小的参数，其参数化复杂性一直未解决。本文旨在填补这一空白。

Method: 通过构造性证明：1) 证明即使子集FAS数为常数且入邻域或出邻域无环，TFP仍为NP难；2) 证明当入邻域和出邻域都无环时，TFP关于子集FAS数是FPT的；3) 提供在参数无界时v*仍能获胜的充分条件。

Result: 1) TFP在子集FAS数为常数且入邻域或出邻域无环时是NP难的；2) 当入邻域和出邻域都无环时，TFP关于子集FAS数是FPT的；3) 确定了参数无界时v*仍能获胜的条件。

Conclusion: 本文完整刻画了TFP关于子集FAS数的参数化复杂性：在入邻域或出邻域无环的较弱条件下仍为NP难，但在两者都无环的较强条件下是FPT的，为这一长期开放问题提供了最终答案。

Abstract: The \textsc{Tournament Fixing Problem} (TFP) asks whether a knockout tournament can be scheduled to guarantee that a given player $v^*$ wins. Although TFP is NP-hard in general, it is known to be \emph{fixed-parameter tractable} (FPT) when parameterized by the feedback arc/vertex set number, or the in/out-degree of $v^*$ (AAAI 17; IJCAI 18; AAAI 23; AAAI 26). However, it remained open whether TFP is FPT with respect to the \emph{subset FAS number of $v^*$} -- the minimum number of arcs intersecting all cycles containing $v^*$ -- a parameter that is never larger than the aforementioned ones (AAAI 26). In this paper, we resolve this question negatively by proving that TFP stays NP-hard even when the subset FAS number of $v^*$ is constant $\geq 1$ and either the subgraph induced by the in-neighbors $D[N_{\mathrm{in}}(v^*)]$ or the out-neighbors $D[N_{\mathrm{out}}(v^*)]$ is acyclic. Conversely, when both $D[N_{\mathrm{in}}(v^*)]$ and $D[N_{\mathrm{out}}(v^*)]$ are acyclic, we show that TFP becomes FPT parameterized by the subset FAS number of $v^*$. Furthermore, we provide sufficient conditions under which $v^*$ can win even when this parameter is unbounded.

</details>


### [95] [Personalization Aids Pluralistic Alignment Under Competition](https://arxiv.org/abs/2602.13451)
*Natalie Collina,Surbhi Goel,Aaron Roth,Mirah Shi*

Main category: cs.GT

TL;DR: 研究竞争性AI提供商与多样化用户之间的互动，探讨个性化模型如何影响对齐结果。在个性化设置下，即使提供商自利，也能实现类似完美对齐的结果；而在匿名策略下，可能出现无信息均衡。


<details>
  <summary>Details</summary>
Motivation: 研究当AI提供商与用户偏好不一致时，市场竞争能否产生对齐结果，以及模型个性化在这一过程中的作用。关注自利提供商如何通过战略部署模型影响用户决策，以及不同设置下的均衡结果。

Method: 采用多领导者（提供商）和多追随者（用户）的Stackelberg博弈模型。提供商承诺对话策略，用户选择使用哪个模型、如何对话以及如何行动。分析两种设置：用户特定个性化模型和单一匿名策略。

Result: 在个性化设置下，当满足"弱市场对齐"条件时，每个均衡都能给用户带来与完美对齐通用模型相当的结果。在匿名策略设置下，同一条件下可能存在无信息均衡。提出了更强的对齐条件来保证匿名设置中用户获得最优效用。

Conclusion: 模型个性化可以诱导多元对齐结果，即使提供商是自利的。在个性化设置下，市场竞争能够产生类似完美对齐的结果；而在匿名策略下需要更强的对齐条件来保证用户获得最优效用。

Abstract: Can competition among misaligned AI providers yield aligned outcomes for a diverse population of users, and what role does model personalization play? We study a setting where multiple competing AI providers interact with multiple users who must make downstream decisions but differ in preferences. Providers have their own objectives over users' actions and strategically deploy AI models to advance them. We model the interaction as a Stackelberg game with multiple leaders (providers) and followers (users): providers commit to conversational policies, and users choose which model to use, how to converse, and how to act. With user-specific personalization, we show that under a Weak Market Alignment condition, every equilibrium gives each user outcomes comparable to those from a perfectly aligned common model -- so personalization can induce pluralistically aligned outcomes, even when providers are self-interested. In contrast, when providers must deploy a single anonymous policy, there exist equilibria with uninformative behavior under the same condition. We then give a stronger alignment condition that guarantees each user their optimal utility in the anonymous setting.

</details>


### [96] [Revenue-Optimal Pricing for Budget-Constrained Buyers in Data Markets](https://arxiv.org/abs/2602.13897)
*Bhaskar Ray Chaudhury,Jugal Garg,Eklavya Sharma,Jiaxin Song*

Main category: cs.GT

TL;DR: 数据市场中预算约束买家的收益最优定价研究：非线性定价可多项式时间求解，而线性定价却是APX难的，形成计算复杂性二分法


<details>
  <summary>Details</summary>
Motivation: 研究数据市场中收益最优定价问题，市场出售多个数据集，买家通过购买数据包来提高预测任务准确性。买家具有准线性效用和预算约束，市场需要设定定价函数以最大化总收入。

Method: 分析单调且下半连续的定价函数，发现最优定价具有分段线性凸（PLC）结构，可通过线性规划高效计算。同时研究线性定价方案，证明其APX难性，并设计近似算法。

Result: 最优非线性定价具有高度结构化形式（PLC），可多项式时间计算；而线性定价是APX难的。设计了在线设置的2-近似算法和离线设置的(1-1/e)^{-1}-近似算法。

Conclusion: 数据市场中存在计算复杂性二分法：完全通用的非线性定价可高效求解，而更简单的线性定价方案却计算困难。该框架为探索更丰富的定价方案和效用模型奠定了基础。

Abstract: We study revenue-optimal pricing in data markets with rational, budget-constrained buyers. Such a market offers multiple datasets for sale, and buyers aim to improve the accuracy of their prediction tasks by acquiring data bundles. For each dataset, the market sets a pricing function, which maps the number of records purchased from the dataset to a non-negative price. The market's objective is to set these pricing functions to maximize total revenue, considering that buyers with quasi-linear utilities choose their bundles optimally under budget constraints.
  We analyze optimal pricing when each dataset's pricing function is only required to be monotone and lower-continuous. Surprisingly, even with this generality, optimal pricing has a highly structured form: it is piecewise linear and convex (PLC) and can be computed efficiently via an LP. Moreover, the total number of kinks across all pricing functions is bounded by the number of buyers. Thus, when datasets far outnumber buyers, most pricing functions are effectively linear.
  This motivates studying linear pricing, where each record in a dataset is priced uniformly. Although competitive equilibrium gives revenue-optimal linear prices in rivalrous markets with quasi-linear buyers, we show that revenue maximization under linear pricing in data markets is APX-hard. Hence, a striking computational dichotomy emerges: fully general (nonlinear) pricing admits a polynomial-time algorithm, while the simpler linear scheme is APX-hard.
  Despite the hardness, we design a 2-approximation algorithm when datasets arrive online, and a $(1-1/e)^{-1}$-approximation algorithm for the offline setting. Our framework lays the groundwork for exploring more general pricing schemes, richer utility models, and a deeper understanding of how market structure -- rivalrous versus non-rivalrous -- shapes revenue-optimal pricing.

</details>


### [97] [Truthful Reporting of Competence with Minimal Verification](https://arxiv.org/abs/2602.14076)
*Reshef Meir,Jonathan Wagner,Omer Ben-Porat*

Main category: cs.GT

TL;DR: 研究家庭考试中如何通过有限验证来最小化学生能力与预期成绩之间的偏差，同时确保诚实报告是占优策略且诚实者不受罚


<details>
  <summary>Details</summary>
Motivation: 在家庭考试中，学生可以自由作弊报告自己的分数，教师只能有限地验证学生实际表现。需要设计机制确保诚实报告是占优策略，同时最小化学生能力与预期成绩之间的偏差

Method: 1. 完美验证情况下：参数化机制设计，表征最优权衡；2. 噪声验证情况下：利用适当评分规则构建诚实机制

Result: 1. 完美验证时：给出了最优权衡的表征和参数化最优机制；2. 噪声验证时：构建了具有良好（不一定最优）权衡的诚实机制

Conclusion: 该研究为家庭考试中的验证机制设计提供了理论框架，在完美验证下达到最优，在噪声验证下提供了实用解决方案

Abstract: Suppose you run a home exam, where students should report their own scores but can cheat freely. You can, if needed, call a limited number of students to class and verify their actual performance against their reported score. We consider the class of mechanisms where truthful reporting is a dominant strategy, and truthful agents are never penalized -- even off-equilibrium.
  How many students do we need to verify, in expectation, if we want to minimize the bias, i.e., the difference between agents' competence and their expected grade? When perfect verification is available, we characterize the best possible tradeoff between these requirements and provide a simple parametrized mechanism that is optimal in the class for any distribution of agents' types. When verification is noisy, the task becomes much more challenging. We show how proper scoring rules can be leveraged in different ways to construct truthful mechanisms with a good (though not necessarily optimal) tradeoff.

</details>


### [98] [Evaluating the Performance of Approximation Mechanisms under Budget Constraints](https://arxiv.org/abs/2602.14120)
*Juan Carlos Carbajal,Ahuva Mualem*

Main category: cs.GT

TL;DR: 研究买方有私有估值和私有预算时的收入最大化问题，发现近似机制在私有预算存在时表现脆弱：有界分布下简单机制可近似最优收入，但无界分布或单位正方形集中分布下任何有限菜单机制都无法保证正比例的最优收入。


<details>
  <summary>Details</summary>
Motivation: 私有预算的存在使经典单产品垄断问题复杂化，最优机制难以分析。研究旨在评估近似机制相对于最优机制的鲁棒性能，探讨在私有预算约束下简单机制设计的可行性和局限性。

Method: 使用三种性能度量：受限机制类的最优收入保证比例(GFOR)、松弛机制类的最大松弛值(MVR)、以及松弛或受限类的收入非单调性间隙。分析有界和无界分布下的机制表现，考虑估值和预算的相关性。

Result: 有界分布下，无论估值和预算相关性如何，具有多对数菜单大小的简单机制可任意好地近似最优收入。无界分布或单位正方形集中分布下，任何有限或次线性菜单机制都无法保证正比例的最优收入。负相关时某些松弛可带来无限收入增益，存在收入非单调性情况。

Conclusion: 近似方法在私有预算存在时表现脆弱，除少数条件外，近似机制会导致大量收入损失，揭示了机制设计中简单性和鲁棒性的基本限制。近似结果对设计环境细节高度敏感。

Abstract: We study revenue maximization in a buyer-seller setting where the seller has a single object and the buyer has both a private valuation and a private budget. The presence of private budgets complicates the classic single-product monopoly problem, making optimal mechanisms difficult to analyze. To overcome this, we evaluate the robust performance of approximation mechanisms relative to optimal mechanisms. We work with three measures of performance: the guaranteed fraction of optimal revenue (GFOR) for restricted classes of mechanisms, the maximal value of relaxation (MVR) for relaxed classes, and a revenue non-monotonicity gap for either relaxed or restricted classes. Our analysis reveals sharp contrasts. On the positive side, we show that for distributions with bounded support, simple mechanisms with poly-logarithmic menu size can approximate optimal revenue arbitrarily well, regardless of correlation between valuations and budgets. On the negative side, we establish strong impossibility results: for distributions with unbounded support, or even bounded distributions concentrated in the unit square, no simple mechanism - or indeed any mechanism with a finite or sublinear menu - can guarantee a positive fraction of the optimal revenue. We also demonstrate unbounded revenue gains from certain relaxations when valuations and budgets are negatively correlated, and highlight cases of revenue non-monotonicity. Taken together, our results underscore the fragility of approximation approaches in the presence of private budgets: except for a narrow set of conditions, approximation mechanisms incur large revenue losses, pointing to fundamental limits of simplicity and robustness in mechanism design. Our analysis highlights that approximation results are highly sensitive to details of the design environment.

</details>


### [99] [Pareto and Bowley Reinsurance Games in Peer-to-Peer Insurance](https://arxiv.org/abs/2602.14223)
*Tim J. Boonen,Kenneth Tsz Hin Ng,Tak Wa Ng,Thai Nguyen*

Main category: cs.GT

TL;DR: 提出P2P保险方案，包含风险共享池和再保险人，研究计划经理与再保险人的策略互动，推导两种博弈论合同设计的闭式最优解


<details>
  <summary>Details</summary>
Motivation: 研究P2P保险中计划经理与再保险人之间的策略互动，探索如何通过博弈论方法设计最优风险分配合同

Method: 提出两种博弈论合同设计：帕累托设计和鲍利设计。帕累托设计基于合作博弈，引入联盟稳定性概念；鲍利设计采用领导者-追随者框架，通过逐点比较验证个体理性约束

Result: 推导出两种设计的闭式最优合同，证明鲍利最优合同从不帕累托最优且通常产生更低的总福利。数值示例显示再保险提高福利，尤其帕累托设计和风险厌恶程度较低的再保险人

Conclusion: P2P保险中再保险能改善福利，帕累托设计优于鲍利设计，单一费率限制对高风险损失成员更有利

Abstract: We propose a peer-to-peer (P2P) insurance scheme comprising a risk-sharing pool and a reinsurer. A plan manager determines how risks are allocated among members and ceded to the reinsurer, while the reinsurer sets the reinsurance loading. Our work focuses on the strategic interaction between the plan manager and the reinsurer, and this focus leads to two game-theoretic contract designs: a Pareto design and a Bowley design, for which we derive closed-form optimal contracts. In the Pareto design, cooperation between the reinsurer and the plan manager leads to multiple Pareto-optimal contracts, which are further refined by introducing the notion of coalitional stability. In contrast, the Bowley design yields a unique optimal contract through a leader-follower framework, and we provide a rigorous verification of the individual rationality constraints via pointwise comparisons of payoff vectors. Comparing the two designs, we prove that the Bowley-optimal contract is never Pareto optimal and typically yields lower total welfare. In our numerical examples, the presence of reinsurance improves welfare, especially with Pareto designs and a less risk-averse reinsurer. We further analyze the impact of the single-loading restriction, which disproportionately favors members with riskier losses.

</details>


### [100] [Characterizing Robustness of Strategies to Novelty in Zero-Sum Open Worlds](https://arxiv.org/abs/2602.14278)
*Mayank Kejriwal,Shilpa Thomas,Hongyu Li*

Main category: cs.GT

TL;DR: 该研究提出了一个评估固定策略智能体在开放世界环境中对游戏规则新颖性扰动的鲁棒性框架，在囚徒困境和德州扑克两个领域进行实验，量化了智能体性能变化和全局影响。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，人工智能体经常面临偏离训练或设计假设的新颖条件。研究固定策略智能体在双人零和游戏中对环境新颖性的鲁棒性，为设计更安全、更具弹性的自主系统提供基础。

Method: 提出了一个通用框架，将新颖性操作化为游戏规则或计分机制的扰动，同时保持智能体行为固定。引入两个度量指标：每个智能体的鲁棒性（量化策略在新颖性下的相对性能变化）和全局影响（总结新颖性引起的群体范围扰动）。在迭代囚徒困境（30个智能体，20种收益矩阵新颖性）和德州扑克（10个智能体，5种基于规则的新颖性）两个领域进行实验。

Result: 实验揭示了鲁棒性的系统性模式，并识别出某些能引起严重不稳定的新颖性。在囚徒困境和德州扑克中都观察到了智能体性能的显著变化，某些新颖性对智能体群体产生了严重的破坏性影响。

Conclusion: 该研究为在对抗性和动态环境中设计更安全、更具弹性的自主系统提供了定量基础，揭示了智能体在扰动下的泛化能力，并展示了如何系统评估环境变化对固定策略智能体的影响。

Abstract: In open-world environments, artificial agents must often contend with novel conditions that deviate from their training or design assumptions. This paper studies the robustness of fixed-strategy agents to such novelty within the setting of two-player zero-sum games. We present a general framework for characterizing the impact of environmental novelties, such as changes in payoff structure or action constraints, on agent performance in two distinct domains: Iterated Prisoner's Dilemma (IPD) and heads-up Texas Hold'em Poker. Novelty is operationalized as a perturbation of the game's rules or scoring mechanics, while agent behavior remains fixed. To measure the effects, we introduce two metrics: per-agent robustness, quantifying the relative performance shift of each strategy across novelties, and global impact, summarizing the population-wide disruption caused by a novelty. Our experiments, comprising 30 IPD agents across 20 payoff matrix novelties and 10 Poker agents across 5 rule-based novelties, reveal systematic patterns in robustness and highlight certain novelties that induce severe destabilization. The results offer insights into agent generalizability under perturbation and provide a quantitative basis for designing safer and more resilient autonomous systems in adversarial and dynamic environments.

</details>


### [101] [A Bayesian Framework for Human-AI Collaboration: Complementarity and Correlation Neglect](https://arxiv.org/abs/2602.14331)
*Saurabh Amin,Amine Bennouna,Daniel Huttenlocher,Dingwen Kong,Liang Lyu,Asuman Ozdaglar*

Main category: cs.GT

TL;DR: 该研究建立了一个人机交互决策理论模型，分析AI辅助何时改善或损害人类决策，重点关注信息重叠和行为扭曲两个核心因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解AI辅助对人类决策的实际影响，特别是在人类可能不完美地结合自身信息与AI建议的情况下，识别AI辅助何时真正提升决策质量。

Method: 开发了一个决策理论模型，人类决策者观察私人信息并接收AI建议，但可能不完美地结合这些信号。模型引入了信息重叠的微观测量，并研究了"相关性忽视"这种实证相关的决策缺陷。

Result: 研究发现AI辅助效果可分解为两个主要力量：AI超越人类已知信息的边际信息价值，以及人类使用AI建议方式引起的行为扭曲。研究还刻画了信息重叠和AI能力如何塑造人机交互的四种模式。

Conclusion: AI辅助的效果取决于信息重叠程度和AI能力，可能导致决策增强、损害、互补或自动化等不同结果。理解这些机制对于设计有效的人机协作系统至关重要。

Abstract: We develop a decision-theoretic model of human-AI interaction to study when AI assistance improves or impairs human decision-making. A human decision-maker observes private information and receives a recommendation from an AI system, but may combine these signals imperfectly. We show that the effect of AI assistance decomposes into two main forces: the marginal informational value of the AI beyond what the human already knows, and a behavioral distortion arising from how the human uses the AI's recommendation. Central to our analysis is a micro-founded measure of informational overlap between human and AI knowledge. We study an empirically relevant form of imperfect decision-making -- correlation neglect -- whereby humans treat AI recommendations as independent of their own information despite shared evidence. Under this model, we characterize how overlap and AI capabilities shape the Human-AI interaction regime between augmentation, impairment, complementarity, and automation, and draw key insights.

</details>


### [102] [Truthful Reverse Auctions for Adaptive Selection via Contextual Multi-Armed Bandits](https://arxiv.org/abs/2602.14476)
*Pronoy Patra,Sankarshan Damle,Manisha Padala,Sujit Gujar*

Main category: cs.GT

TL;DR: 本文提出了一种基于上下文在线学习的反向拍卖机制，用于在多LLM提供商环境中根据用户查询自适应选择最优模型，同时保证诚实报价。


<details>
  <summary>Details</summary>
Motivation: 在多LLM提供商环境中，用户面临一个顺序的、查询相关的决策问题：高容量模型输出更可靠但成本更高，轻量级模型更快更便宜。现有MAB机制主要关注正向拍卖和社会福利，缺乏对反向拍卖、提供商最优结果和上下文适应的支持。

Method: 将LLM选择问题形式化为具有上下文在线学习的反向拍卖设计问题，提出重采样过程将诚实的正向MAB机制推广到反向拍卖，证明任何单调分配规则结合此过程都是诚实的。在此基础上提出上下文MAB算法，学习查询相关的模型质量并实现次线性遗憾。

Result: 设计了一个统一机制设计和自适应学习的框架，能够实现高效、诚实且查询感知的LLM选择，算法能够学习查询相关的模型质量并达到次线性遗憾。

Conclusion: 该研究解决了反向拍卖、提供商最优结果和上下文适应的挑战，为多LLM提供商环境中的模型选择提供了一个理论框架，实现了高效、诚实且查询感知的LLM选择机制。

Abstract: We study the problem of selecting large language models (LLMs) for user queries in settings where multiple LLM providers submit the cost of solving a query. From the users' perspective, choosing an optimal model is a sequential, query-dependent decision problem: high-capacity models offer more reliable outputs but are costlier, while lightweight models are faster and cheaper. We formalize this interaction as a reverse auction design problem with contextual online learning, where the user adaptively discovers which model performs best while eliciting costs from competing LLM providers. Existing multi-armed bandit (MAB) mechanisms focus on forward auctions and social welfare, leaving open the challenges of reverse auctions, provider-optimal outcomes, and contextual adaptation. We address these gaps by designing a resampling-based procedure that generalizes truthful forward MAB mechanisms to reverse auctions and prove that any monotone allocation rule with this procedure is truthful. Using this, we propose a contextual MAB algorithm that learns query-dependent model quality with sublinear regret. Our framework unifies mechanism design and adaptive learning, enabling efficient, truthful, and query-aware LLM selection.

</details>


### [103] [Near-Optimal Best-of-Both-Worlds Fairness for Few Agents](https://arxiv.org/abs/2602.14668)
*Moshe Babaioff,Gefen Frosh*

Main category: cs.GT

TL;DR: 该论文研究了不可分割物品在加性估值下的公平分配问题，提出了"两全其美"（BoBW）公平性框架：既保证事前公平的分布，又确保分布中所有确定性分配都满足事后公平。针对三个智能体的情况，设计了首个实现接近最优公平性的BoBW算法。


<details>
  <summary>Details</summary>
Motivation: 传统公平分配研究通常单独考虑事前公平（如比例性）或事后公平（如EFX、MMS），但实际应用中需要同时满足这两种公平性。BoBW框架旨在设计既满足事前公平分布，又保证分布中每个具体分配都满足事后公平的分配方案，这在理论和实践上都具有重要意义。

Method: 1. 针对三个智能体，证明了存在一个事前比例分布，其中每个分配都满足认知EFX（EEFX），并保证每个智能体至少获得其MMS的9/10。2. 当智能体未获得MMS时，保证其满足EFX。3. 使用FPTAS近似MMS划分，设计了计算BoBW分布的FPTAS算法，保持所有基于嫉妒的保证，并将基于价值的保证保持到(1-ε)。4. 对于两个智能体，设计了FPTAS输出事前无嫉妒（因此比例）且事后EFX的分布，同时保证每个智能体至少获得(1-ε)比例的MMS。5. 将两智能体FPTAS作为子程序，为三个智能体获得保证精确事前比例性的FPTAS。

Result: 1. 为三个智能体设计了首个实现接近最优公平性的BoBW算法。2. 证明了存在事前比例分布，其中每个分配都满足EEFX且保证每个智能体至少获得其MMS的9/10。3. 当智能体未获得MMS时，保证其满足EFX。4. 设计了FPTAS计算BoBW分布，保持所有嫉妒保证和(1-ε)的价值保证。5. 对于两个智能体，设计了FPTAS实现事前无嫉妒、事后EFX，并保证(1-ε)比例的MMS，这基本上匹配了多项式时间内可达到的最强公平性和效率保证。

Conclusion: 该论文在BoBW公平分配框架下取得了重要进展，特别是针对三个智能体设计了首个实现接近最优公平性的算法。通过结合事前比例性和事后公平性（EEFX、MMS近似、EFX），并在多项式时间内实现这些保证，为公平分配理论提供了新的工具和视角。两智能体的FPTAS结果具有独立的理论价值，基本上达到了多项式时间内可达到的最强公平性和效率保证。

Abstract: We consider the problem of fair allocation of indivisible goods among agents with additive valuations, aiming for Best-of-Both-Worlds (BoBW) fairness: a distribution over allocations that is ex-ante fair, and additionally, it is supported only on deterministic allocations that are ex-post fair. We focus on BoBW for few agents, and our main result is the design of the first BoBW algorithms achieving near-optimal fairness for three agents. For three agents, we prove the existence of an ex-ante proportional distribution whose every allocation is Epistemic EFX (EEFX) and guarantees each agent at least $\tfrac{9}{10}$ of her MMS. As MMS allocations do not exist for three additive agents, in every allocation at least one agent might not be getting her MMS. To compensate such an agent, we also guarantee that if an agent is not getting her MMS then she is EFX-satisfied - giving her the strongest achievable envy-based guarantee. Additionally, using an FPTAS for near-MMS partitions, we present an FPTAS to compute a BoBW distribution preserving all envy-based guarantees, and also preserving all value-based guarantees up to $(1-\varepsilon)$. We further show that exact ex-ante proportionality can be restored when dropping EEFX. To do so, we first design, for two agents and any $\varepsilon > 0$, a Fully Polynomial-Time Approximation Scheme (FPTAS) that outputs a distribution which is ex-ante envy-free (and thus proportional) and ex-post envy-free up to any good (EFX), while guaranteeing each agent at least a $(1-\varepsilon)$-fraction of her maximin share (MMS). We then leverage this two-agent FPTAS algorithm as a subroutine to obtain, for three agents, the FPTAS guaranteeing exact ex-ante proportionality. We note that our result for two agents essentially matches the strongest fairness and efficiency guarantees achievable in polynomial time, and thus might be of independent interest.

</details>


### [104] [Revenue Guarantees in Autobidding Platforms](https://arxiv.org/abs/2602.14815)
*Ioannis Caragiannis,Anders Bo Ipsen,Stratis Skoulakis*

Main category: cs.GT

TL;DR: 该论文研究在线广告自动竞价系统中的收益最大化问题，证明第一价格步调均衡(FPPE)在单商品单价格约束下至少能获得最优收益的一半，并扩展到在线场景和凹估值函数情况。


<details>
  <summary>Details</summary>
Motivation: 受在线广告自动竞价系统的启发，研究可分割商品市场中预算约束买家的收益最大化问题，旨在为每个商品计算单一价格和分配方案以最大化总收益。

Method: 采用第一价格步调均衡(FPPE)方法，证明其在单商品单价格约束下的收益保证；扩展到在线FPPE版本；针对凹估值函数买家，将FPPE类型结果表征为Eisenberg-Gale风格凸规划的解。

Result: FPPE保证至少获得最优收益的一半；在线FPPE获得1/4近似保证；对于凹估值函数买家，收益近似随估值非线性程度而优雅下降；同时证明了在个体理性和单商品单价格约束下收益最大化问题是APX难的。

Conclusion: FPPE在单价格约束下提供了强大的收益保证，即使在买家特定价格的最优收益对比下也能保持良好性能，为在线广告自动竞价系统提供了理论支持。

Abstract: Motivated by autobidding systems in online advertising, we study revenue maximization in markets with divisible goods and budget-constrained buyers with linear valuations. Our aim is to compute a single price for each good and an allocation that maximizes total revenue. We show that the First-Price Pacing Equilibrium (FPPE) guarantees at least half of the optimal revenue, even when compared to the maximal revenue of buyer-specific prices. This guarantee is particularly striking in light of our hardness result: we prove that revenue maximization under individual rationality and single-price-per-good constraints is APX-hard.
  We further extend our analysis in two directions: first, we introduce an online analogue of FPPE and show that it achieves a constant-factor revenue guarantee, specifically a $1/4$-approximation; second, we consider buyers with concave valuation functions, characterizing an FPPE-type outcome as the solution to an Eisenberg-Gale-style convex program and showing that the revenue approximation degrades gracefully with the degree of nonlinearity of the valuations.

</details>


### [105] [Thermal Min-Max Games: Unifying Bounded Rationality and Typical-Case Equilibrium](https://arxiv.org/abs/2602.14858)
*Yuma Ichikawa*

Main category: cs.GT

TL;DR: 论文提出热力学最小最大博弈理论，通过温度参数统一完美理性和有限理性，并开发嵌套复制框架分析大策略极限下的典型行为。


<details>
  <summary>Details</summary>
Motivation: 传统策略形式的最小最大博弈理论假设完美理性，但实际应用中博弈通常来自集合且玩家表现出有限理性。需要一种能够统一完美理性和有限理性的理论框架。

Method: 引入热力学最小最大博弈作为热力学松弛，为每个玩家分配温度参数调节理性水平。开发嵌套复制框架分析大策略极限下的典型行为。

Result: 理论提供了关于典型均衡值、混合策略统计量的可处理预测，这些预测是理性强度、策略数量纵横比和支付随机性的函数。数值实验表明渐近预测与中等规模有限博弈的均衡准确吻合。

Conclusion: 热力学最小最大博弈理论成功统一了完美理性和有限理性，嵌套复制框架为分析大策略极限下的典型行为提供了有效工具，理论预测在实际应用中具有良好准确性。

Abstract: Strategic-form min-max game theory examines the existence, multiplicity, selection of equilibria, and the worst-case computational complexity under perfect rationality. However, in many applications, games are drawn from an ensemble, and players exhibit bounded rationality. We introduce thermal min-max games, a thermodynamic relaxation that unifies bounded and perfect rationality by assigning each player a temperature to regulate their rationality level. To analyze typical behavior in the large-strategy limit, we develop a nested replica framework for this relaxation. This theory provides tractable predictions for typical equilibrium values and mixed-strategy statistics as functions of rationality strength, strategy-count aspect ratio, and payoff randomness. Numerical experiments demonstrate that these asymptotic predictions accurately align with the equilibrium of finite games of moderate size.

</details>


### [106] [Robust Value Maximization in Challenge the Champ Tournaments with Probabilistic Outcomes](https://arxiv.org/abs/2602.14966)
*Umang Bhaskar,Juhi Chaudhary,Sushmita Gupta,Pallavi Jain,Sanjay Seetharaman*

Main category: cs.GT

TL;DR: 研究挑战冠军锦标赛中的鲁棒价值最大化问题，考虑比赛结果具有概率性的情况，探索自适应与非自适应算法下的最优解近似难度。


<details>
  <summary>Details</summary>
Motivation: 传统锦标赛价值最大化研究通常假设比赛结果是确定性的，但现实中比赛结果往往是概率性的。本研究旨在解决当比赛结果具有概率性时，如何在挑战冠军锦标赛中实现鲁棒价值最大化。

Method: 研究挑战冠军锦标赛格式，考虑玩家排序（种子）和比赛结果的概率性。分析非自适应算法下的最优鲁棒价值（VnaR）近似难度，并探索自适应算法（根据先前比赛结果调整挑战者顺序）和限制概率性比赛情况下的近似算法。

Result: 在简单的二元设置中，非自适应算法下的最优鲁棒价值（VnaR）难以近似。但通过自适应算法或限制概率性比赛，可以获得对最优VnaR的良好近似。

Conclusion: 在概率性比赛结果的挑战冠军锦标赛中，自适应算法和限制概率性比赛是获得鲁棒价值最大化良好近似的有效方法，而非自适应算法在此类问题中面临计算困难。

Abstract: Challenge the Champ is a simple tournament format, where an ordering of the players -- called a seeding -- is decided. The first player in this order is the initial champ, and faces the next player. The outcome of each match decides the current champion, who faces the next player in the order. Each player also has a popularity, and the value of each match is the popularity of the winner. Value maximization in tournaments has been previously studied when each match has a deterministic outcome. However, match outcomes are often probabilistic, rather than deterministic. We study robust value maximization in Challenge the Champ tournaments, when the winner of a match may be probabilistic. That is, we seek to maximize the total value that is obtained, irrespective of the outcome of probabilistic matches. We show that even in simple binary settings, for non-adaptive algorithms, the optimal robust value -- which we term the \textsc{VnaR}, or the value not at risk -- is hard to approximate. However, if we allow adaptive algorithms that determine the order of challengers based on the outcomes of previous matches, or restrict the matches with probabilistic outcomes, we can obtain good approximations to the optimal \textsc{VnaR}.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [107] [InfoCIR: Multimedia Analysis for Composed Image Retrieval](https://arxiv.org/abs/2602.13402)
*Ioannis Dravilas,Ioannis Kapetangeorgis,Anastasios Latsoudis,Conor McCarthy,Gonçalo Marcelino,Marcel Worring*

Main category: cs.HC

TL;DR: InfoCIR是一个可视化分析系统，用于分析和调试组合图像检索任务，通过交互式界面结合检索、可解释性和提示工程功能。


<details>
  <summary>Details</summary>
Motivation: 现有的组合图像检索系统缺乏工具来揭示多模态提示如何与嵌入空间交互，以及为什么微小的文本变化会显著改变检索结果。研究人员需要更好的工具来理解这些模型的行为。

Method: 开发了InfoCIR系统，集成了最先进的CIR后端（SEARLE），提供六面板交互界面：支持图像+文本查询组合、使用UMAP将结果投影到低维空间进行空间推理、叠加基于相似性的显著图和梯度派生的令牌归因条进行局部解释、使用LLM驱动的提示增强器生成反事实变体并可视化这些变化如何影响目标图像排名。

Result: InfoCIR系统能够帮助诊断检索失败、指导提示增强，并加速模型开发过程中的洞察生成。系统采用模块化架构，可以轻松集成新模型、数据集和归因方法。

Conclusion: InfoCIR填补了组合图像检索领域可视化分析工具的空白，为研究人员提供了理解多模态提示与嵌入空间交互的强大工具，有助于改进模型开发和提示工程。

Abstract: Composed Image Retrieval (CIR) allows users to search for images by combining a reference image with a text prompt that describes desired modifications. While vision-language models like CLIP have popularized this task by embedding multiple modalities into a joint space, developers still lack tools that reveal how these multimodal prompts interact with embedding spaces and why small wording changes can dramatically alter the results. We present InfoCIR, a visual analytics system that closes this gap by coupling retrieval, explainability, and prompt engineering in a single, interactive dashboard. InfoCIR integrates a state-of-the-art CIR back-end (SEARLE arXiv:2303.15247) with a six-panel interface that (i) lets users compose image + text queries, (ii) projects the top-k results into a low-dimensional space using Uniform Manifold Approximation and Projection (UMAP) for spatial reasoning, (iii) overlays similarity-based saliency maps and gradient-derived token-attribution bars for local explanation, and (iv) employs an LLM-powered prompt enhancer that generates counterfactual variants and visualizes how these changes affect the ranking of user-selected target images. A modular architecture built on Plotly-Dash allows new models, datasets, and attribution methods to be plugged in with minimal effort. We argue that InfoCIR helps diagnose retrieval failures, guides prompt enhancement, and accelerates insight generation during model development. All source code allowing for a reproducible demo is available at https://github.com/giannhskp/InfoCIR.

</details>


### [108] [Revisiting Worker-Centered Design: Tensions, Blind Spots, and Action Spaces](https://arxiv.org/abs/2602.13424)
*Shuhao Ma,John Zimmerman,Valentina Nisi,Nuno Jardim Nunes*

Main category: cs.HC

TL;DR: 该研究通过四透镜分析框架重新审视以工人为中心的设计(WCD)，在食品配送行业中发现关键张力与盲点，提出诊断-生成路径以解决劳动冲突等风险


<details>
  <summary>Details</summary>
Motivation: 尽管以工人为中心的设计(WCD)在过去十年中日益重要，但很少有研究系统性地重新审视WCD本身，考察其实际实施、挑战和实际影响。研究旨在填补这一空白，特别是在食品配送行业背景下

Method: 采用四透镜分析框架，从多劳工系统视角考察WCD的多个方面。通过诊断-生成路径方法，结合设计批评传统和四透镜反思分析

Result: 研究发现：1) 劳动链中的冲突；2) WCD的扭曲实施；3) 设计师有时有限的政治经济理解；4) 工人作为变革的积极主体。这些发现揭示了WCD实施中的关键张力和盲点

Conclusion: 该研究扩展了WCD的行动空间，增强了其与现实世界实践的相关性，通过诊断-生成路径帮助解决劳动冲突和制度重构等重复风险，同时培养设计师的政策和经济想象力

Abstract: Worker-Centered Design (WCD) has gained prominence over the past decade, offering researchers and practitioners ways to engage worker agency and support collective actions for workers. Yet few studies have systematically revisited WCD itself, examining its implementations, challenges, and practical impact. Through a four-lens analytical framework that examines multiple facets of WCD within food delivery industry, we identify critical tensions and blind spots from a Multi-Laborer System perspective. Our analysis reveals conflicts across labor chains, distorted implementations of WCD, designers' sometimes limited political-economic understanding, and workers as active agents of change. These insights further inform a Diagnostic-Generative pathway that helps to address recurring risks, including labor conflicts and institutional reframing, while cultivating designers' policy and economic imagination. Following the design criticism tradition, and through a four-lens reflexive analysis, this study expands the action space for WCD and strengthens its relevance to real-world practice.

</details>


### [109] [Uncertain Pointer: Situated Feedforward Visualizations for Ambiguity-Aware AR Target Selection](https://arxiv.org/abs/2602.13433)
*Ching-Yi Tsai,Nicole Tacconi,Andrew D. Wilson,Parastoo Abtahi*

Main category: cs.HC

TL;DR: 本文提出"不确定指针"系统，通过前馈可视化技术标注多个候选目标来支持AR中的目标消歧，包含25种指针设计并通过实验评估用户偏好和性能。


<details>
  <summary>Details</summary>
Motivation: 在增强现实(AR)中，针对远距离物体或杂乱场景的查询存在输入模糊性问题，目标消歧至关重要。然而，支持这一过程的前馈可视化技术尚未得到充分探索。

Method: 1. 构建包含25种指针的指针空间，基于30年相关文献分析现有放置策略和视觉符号；2. 通过两个在线实验（n=60和40）评估用户偏好、信心、心理轻松度、目标可见性和可识别性；3. 根据实验结果针对不同AR场景和消歧技术提出设计建议。

Result: 实验测量了不同物体距离和稀疏度下的用户偏好、信心、心理轻松度、目标可见性和可识别性，为选择不同"不确定指针"提供了实证依据。

Conclusion: 从结果中推导出基于AR上下文和消歧技术选择不同"不确定指针"的设计建议，为AR系统中的目标消歧可视化提供了系统化指导。

Abstract: Target disambiguation is crucial in resolving input ambiguity in augmented reality (AR), especially for queries over distant objects or cluttered scenes on the go. Yet, visual feedforward techniques that support this process remain underexplored. We present Uncertain Pointer, a systematic exploration of feedforward visualizations that annotate multiple candidate targets before user confirmation, either by adding distinct visual identities (e.g., colors) to support disambiguation or by modulating visual intensity (e.g., opacity) to convey system uncertainty. First, we construct a pointer space of 25 pointers by analyzing existing placement strategies and visual signifiers used in target visualizations across 30 years of relevant literature. We then evaluate them through two online experiments (n = 60 and 40), measuring user preference, confidence, mental ease, target visibility, and identifiability across varying object distances and sparsities. Finally, from the results, we derive design recommendations in choosing different Uncertain Pointers based on AR context and disambiguation techniques.

</details>


### [110] [How Multimodal Large Language Models Support Access to Visual Information: A Diary Study With Blind and Low Vision People](https://arxiv.org/abs/2602.13469)
*Ricardo E. Gonzalez Penuela,Crescentia Jung,Sharon Y Lin,Ruiying Hu,Shiri Azenkot*

Main category: cs.HC

TL;DR: 多模态大语言模型（MLLMs）正在改变视障人士获取视觉信息的方式，但实际使用中AI经常产生错误答案或拒绝回答，需要发展"视觉助手"技能来提供更可靠的目标导向帮助。


<details>
  <summary>Details</summary>
Motivation: 传统视觉解释工具通过字幕和OCR为视障人士提供视觉信息访问，而MLLM应用支持对话式协助，但缺乏关于其在现实世界中的性能和对视障人士日常生活影响的实证证据。

Method: 进行了为期两周的日记研究，收集了20名视障参与者使用MLLM视觉解释应用的数据，评估应用的信任度、满意度以及AI回答的准确性。

Result: 参与者对视觉解释的信任度为3.76/5（中等信任），满意度为4.13/5（中等满意），但AI经常产生错误答案（22.2%）或拒绝回答后续请求（10.8%）。

Conclusion: MLLMs可以提高描述性视觉解释的准确性，但支持日常使用还需要"视觉助手"技能——一套提供目标导向、可靠协助的行为。提出了"视觉助手"技能和实践指南，以帮助未来的MLLM视觉解释应用更好地支持视障人士获取视觉信息。

Abstract: Multimodal large language models (MLLMs) are changing how Blind and Low Vision (BLV) people access visual information in their daily lives. Unlike traditional visual interpretation tools that provide access through captions and OCR (text recognition through camera input), MLLM-enabled applications support access through conversational assistance, where users can ask questions to obtain goal-relevant details. However, evidence about their performance in the real-world and their implications for BLV people's everyday life remain limited. To address this, we conducted a two-week diary study, where we captured 20 BLV participants' use of an MLLM-enabled visual interpretation application. Although participants rated the visual interpretations of the application as "somewhat trustworthy" (mean=3.76 out of 5, max=very trustworthy) and "somewhat satisfying" (mean=4.13 out of 5, max=very satisfying), the AI often produced incorrect answers (22.2%) or abstained (10.8%) from responding to follow-up requests. Our work demonstrates that MLLMs can improve the accuracy of descriptive visual interpretations, but that supporting everyday use also depends on the "visual assistant" skill -- a set of behaviors for providing goal-directed, reliable assistance. We conclude by proposing the "visual assistant" skill and practical guidelines to help future MLLM-enabled visual interpretation applications better support BLV people's access to visual information.

</details>


### [111] [What Do We Mean by 'Pilot Study': Early Findings from a Meta-Review of Pilot Study Reporting at CHI](https://arxiv.org/abs/2602.13488)
*Belu Ticona,Amna Liaqat,Antonios Anastasopoulos*

Main category: cs.HC

TL;DR: HCI领域普遍使用试点研究，但缺乏统一定义、指南和报告标准，导致概念模糊、报告不完整


<details>
  <summary>Details</summary>
Motivation: 尽管试点研究在HCI研究中无处不在，但其角色概念模糊且缺乏实证研究，与医学、护理、教育等领域相比，CHI社区缺乏对试点研究的统一定义、指南和报告标准

Method: 通过分析CHI论文中试点研究的引用情况，揭示当前HCI领域对试点研究的概念理解、实施目的和报告方式的现状

Result: 发现CHI论文中试点研究引用普遍但缺乏细节，许多论文仅"顺便提及"试点研究，没有提供设计、结果或如何影响主研究的信息

Conclusion: HCI社区在试点研究方面存在方法论盲点，需要建立统一定义、指南和报告标准，以提高研究透明度和质量

Abstract: Pilot studies (PS) are ubiquitous in HCI research. CHI papers routinely reference 'pilot studies', 'pilot tests', or 'preliminary studies' to justify design decisions, verify procedures, or motivate methodological choices. Yet despite their frequency, the role of pilot studies in HCI remains conceptually vague and empirically underexamined. Unlike fields such as medicine, nursing, and education, where pilot and feasibility studies have well-established definitions, guidelines, reporting standards and even a dedicated research journal, the CHI community lacks a shared understanding of what constitutes a pilot study, why they are conducted, and how they should be reported. Many papers reference pilots 'in passing', without details about design, outcomes, or how the pilot informed the main study. This variability suggests a methodological blind spot in our community.

</details>


### [112] [Designing Health Technologies for Immigrant Communities: Exploring Healthcare Providers' Communication Strategies with Patients](https://arxiv.org/abs/2602.13598)
*Zhanming Chen,Alisha Ghaju,May Hang,Juan F. Maestre,Ji Youn Shin*

Main category: cs.HC

TL;DR: 研究探讨了在发达国家中，医疗服务提供者如何与具有独特文化特征的移民患者群体进行有效沟通，并提出了支持移民社区健康沟通的技术设计建议。


<details>
  <summary>Details</summary>
Motivation: 医患沟通对医疗成功至关重要，但现有研究主要关注发展中国家边缘化社区的医患沟通，缺乏对发达国家移民患者群体沟通策略的理解。本研究旨在填补这一空白。

Method: 通过对15名与具有独特文化特征的移民社区患者工作的医疗服务提供者进行半结构化访谈，识别有效的沟通策略。

Result: 识别出四种有效的沟通策略：认可、社区参与、渐进式护理和适应性沟通实践（即调整沟通风格）。研究强调了文化能力的重要性。

Conclusion: 基于研究发现，讨论了支持移民社区健康沟通的技术设计启示，为HCI研究人员提供了识别实用、情境化文化能力的方法，以指导健康技术设计。

Abstract: Patient-provider communication is an important aspect of successful healthcare, as it can directly lead to positive health outcomes. Previous studies examined factors that facilitate communication between healthcare providers and patients in socially marginalized communities, especially developing countries, and applied identified factors to technology development. However, there is limited understanding of how providers work with patients from immigrant populations in a developed country. By conducting semi-structured interviews with 15 providers working with patients from an immigrant community with unique cultural characteristics, we identified providers' effective communication strategies, including acknowledgment, community involvement, gradual care, and adaptive communication practices (i.e., adjusting the communication style). Based on our findings, we highlight cultural competence and discuss design implications for technologies to support health communication in immigrant communities. Our suggestions propose approaches for HCI researchers to identify practical, contextualized cultural competence for their health technology design.

</details>


### [113] [Search in Transition: A Study of University Students Perspectives on Using LLMs and Traditional Search Engines in English Test Problem Solving for Higher Study](https://arxiv.org/abs/2602.13629)
*Tarek Rahman,Md Shaharia Hossen,Mark Protik Mondol,Jannatun Noor Mukta*

Main category: cs.HC

TL;DR: 本研究探讨大学生在英语考试准备中交替使用Google和GPT等AI工具的情况，发现学生根据任务需求切换工具，并提出结合两者优势的混合解决方案原型。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在教育领域的日益普及，大学生在准备英语考试时越来越多地在传统搜索引擎（如Google）和大型语言模型（如GPT）之间切换来支持考试相关的问题解决。本研究旨在了解学生对这两种工具的认知，以及它们如何融入英语考试准备的工作流程。

Method: 采用混合方法：调查了140名来自不同学科的大学生，并对20名参与者进行了深度访谈。使用ANOVA和卡方检验等定量分析方法评估感知效率、满意度和工具偏好的差异。

Result: 研究发现学生根据任务需求在GPT和Google之间频繁切换：使用Google获取可信的多源信息和规则验证，使用GPT进行总结、解释、改写和为英语考试任务起草回答。单一工具无法充分支持英语考试问题解决的所有方面，参与者强烈偏好混合解决方案。

Conclusion: 研究提出一个原型解决方案：在搜索界面中嵌入聊天机器人，结合GPT的对话优势和Google的可靠性，以改善英语考试准备并减少认知负荷。

Abstract: With the growing integration of Artificial Intelligence (AI) in educational contexts, university students preparing for English language tests increasingly alternate between traditional search engines, such as Google, and large language models (LLMs) to support their test-related problem-solving. This study examines students perceptions of these tools, focusing on usability, efficiency, and their integration into English language test preparation workflows.Using a mixed-methods approach, we surveyed 140 university students from diverse academic disciplines and conducted in-depth interviews with 20 participants. Quantitative analyses, including ANOVA and chi-square tests, were employed to evaluate differences in perceived efficiency, satisfaction, and overall tool preference. The qualitative findings indicate that students frequently switch between GPT and Google depending on task demands, relying on Google for credible, multi-source information and rule verification, while using GPT for summarization, explanation, paraphrasing, and drafting responses for English test tasks. As neither tool alone was found to adequately support all aspects of English language test problem solving, participants expressed a strong preference for a hybrid solution. In response, we propose a prototype in the form of a chatbot embedded within a search interface, combining GPTs conversational strengths with Google reliability to improve English language test preparation and reduce cognitive load.

</details>


### [114] [Transferable XAI: Relating Understanding Across Domains with Explanation Transfer](https://arxiv.org/abs/2602.13675)
*Fei Wang,Yifan Zhang,Brian Y. Lim*

Main category: cs.HC

TL;DR: 提出可迁移XAI框架，通过线性因子解释的仿射变换，让用户能在相关领域间迁移理解，解决现有XAI只能解释单一应用的问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI只关注解释单个应用，但当遇到相关应用时，用户可能依赖先前解释的理解，导致过度泛化和AI过度依赖，或需要负担独立的记忆。相关决策任务可以共享解释因子，但存在显著差异。

Method: 提出可迁移XAI框架，通过将线性因子解释应用于一般仿射变换框架，解释领域间解释的关系，支持跨数据子空间、决策任务和属性的解释迁移。

Result: 在形成性和总结性用户研究中，相比单领域和领域独立解释，可迁移XAI最能帮助理解第二个领域，获得最佳决策忠实度、因子回忆能力和领域间解释关联能力。

Conclusion: 该框架通过解释子空间、任务和属性间的因子关系，提高了相关AI应用中解释的可重用性。

Abstract: Current Explainable AI (XAI) focuses on explaining a single application, but when encountering related applications, users may rely on their prior understanding from previous explanations. This leads to either overgeneralization and AI overreliance, or burdensome independent memorization. Indeed, related decision tasks can share explanatory factors, but with some notable differences; e.g., body mass index (BMI) affects the risks for heart disease and diabetes at the same rate, but chest pain is more indicative of heart disease. Similarly, models using different attributes for the same task still share signals; e.g., temperature and pressure affect air pollution but in opposite directions due to the ideal gas law. Leveraging transfer of learning, we propose Transferable XAI to enable users to transfer understanding across related domains by explaining the relationship between domain explanations using a general affine transformation framework applied to linear factor explanations. The framework supports explanation transfer across various domain types: translation for data subspace (subsuming prior work on Incremental XAI), scaling for decision task, and mapping for attributes. Focusing on task and attributes domain types, in formative and summative user studies, we investigated how well participants could understand AI decisions from one domain to another. Compared to single-domain and domain-independent explanations, Transferable XAI was the most helpful for understanding the second domain, leading to the best decision faithfulness, factor recall, and ability to relate explanations between domains. This framework contributes to improving the reusability of explanations across related AI applications by explaining factor relationships between subspaces, tasks, and attributes.

</details>


### [115] [Human Oversight-by-Design for Accessible Generative IUIs](https://arxiv.org/abs/2602.13745)
*Blessing Jerry,Lourdes Moreno,Paloma Martínez*

Main category: cs.HC

TL;DR: 论文提出"监督设计"框架，通过自动化检查和人工干预相结合的方式，确保LLM生成界面在高风险工作流程中的可靠性、可访问性和可监督性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的界面越来越多地用于高风险工作流程（如医疗保健通信），但存在幻觉、语义扭曲、偏见和可访问性障碍等风险，可能破坏可靠性并限制用户理解、监控和干预AI支持流程的能力。当前监督通常被当作下游检查，缺乏明确的人工干预规则和问责机制。

Method: 提出"监督设计"方法：将人工判断嵌入整个流程作为架构承诺，通过升级策略和明确的UI控制实现风险信号和干预。自动化检查标记生成UI通信中的风险（可读性、语义保真度、事实一致性、基于标准的可访问性约束），当阈值被违反或不确定性高时，在发布前升级到强制性人工在环审查。人工在环监督监控系统级信号以调整策略和检测漂移。

Result: 结构化审查反馈转化为治理行动（规则和提示更新、阈值校准、可追溯审计日志），为支持高风险工作流程的生成UI系统实现可扩展干预和可验证监督。

Conclusion: 监督设计框架通过将人工监督嵌入生成AI系统的架构中，确保高风险工作流程中LLM生成界面的可靠性、可访问性和可问责性，实现有意义的人类监督。

Abstract: LLM-generated interfaces are increasingly used in high-consequence workflows (e.g., healthcare communication), where how information is presented can impact downstream actions. These interfaces and their content support human interaction with AI-assisted decision-making and communication processes and should remain accessible and usable for people with disabilities. Accessible plain-language interfaces serve as an enabling infrastructure for meaningful human oversight. In these contexts, ethical and trustworthiness risks, including hallucinations, semantic distortion, bias, and accessibility barriers, can undermine reliability and limit users' ability to understand, monitor, and intervene in AI-supported processes. Yet, in practice, oversight is often treated as a downstream check, without clear rules for when human intervention is required or who is accountable. We propose oversight-by-design: embedding human judgment across the pipeline as an architectural commitment, implemented via escalation policies and explicit UI controls for risk signalling and intervention. Automated checks flag risk in generated UI communication that supports high-stakes workflows (e.g., readability, semantic fidelity, factual consistency, and standards-based accessibility constraints) and escalate to mandatory Human-in-the-Loop (HITL) review before release when thresholds are violated, or uncertainty is high. Human-on-the-Loop (HOTL) supervision monitors system-level signals over time (alerts, escalation rates, and compliance evidence) to tune policies and detect drift. Structured review feedback is translated into governance actions (rule and prompt updates, threshold calibration, and traceable audit logs), enabling scalable intervention and verifiable oversight for generative UI systems that support high-stakes workflows.

</details>


### [116] [Comparables XAI: Faithful Example-based AI Explanations with Counterfactual Trace Adjustments](https://arxiv.org/abs/2602.13784)
*Yifan Zhang,Tianle Ren,Fei Wang,Brian Y Lim*

Main category: cs.HC

TL;DR: 该论文提出了一种基于可比案例的AI解释方法，通过追踪调整机制模拟房地产估价中的可比案例调整过程，提高解释的准确性和用户理解。


<details>
  <summary>Details</summary>
Motivation: 当前基于案例的AI解释方法面临挑战：当案例与待解释样本在多个特征上存在较大差异时，难以理解决策值应如何相应变化。受房地产估价中使用可比案例进行价值调整的启发，需要开发更准确、更易理解的案例解释方法。

Method: 提出Comparables XAI方法，包含Trace调整机制：从每个可比案例出发，逐个特征追踪到待解释样本的反事实变化，沿着AI特征空间单调调整。这种方法模拟房地产估价中调整可比案例属性的过程。

Result: 在建模和用户研究中，Trace调整的可比案例在XAI忠实度、精确度、用户准确性和最窄的不确定性边界方面表现最佳，优于线性回归、线性调整的可比案例或未调整的可比案例。

Conclusion: 该工作为使用基于案例的解释来改善用户对AI决策的理解提供了新的分析基础，Trace调整机制能够有效提高解释的准确性和可理解性。

Abstract: Explaining with examples is an intuitive way to justify AI decisions. However, it is challenging to understand how a decision value should change relative to the examples with many features differing by large amounts. We draw from real estate valuation that uses Comparables-examples with known values for comparison. Estimates are made more accurate by hypothetically adjusting the attributes of each Comparable and correspondingly changing the value based on factors. We propose Comparables XAI for relatable example-based explanations of AI with Trace adjustments that trace counterfactual changes from each Comparable to the Subject, one attribute at a time, monotonically along the AI feature space. In modelling and user studies, Trace-adjusted Comparables achieved the highest XAI faithfulness and precision, user accuracy, and narrowest uncertainty bounds compared to linear regression, linearly adjusted Comparables, or unadjusted Comparables. This work contributes a new analytical basis for using example-based explanations to improve user understanding of AI decisions.

</details>


### [117] [What happens when reviewers receive AI feedback in their reviews?](https://arxiv.org/abs/2602.13817)
*Shiping Chen,Shu Zhong,Duncan P. Brumby,Anna L. Cox*

Main category: cs.HC

TL;DR: 研究分析了ICLR 2025会议中AI审稿反馈工具的实际部署效果，通过调查和访谈了解审稿人如何使用该工具及其对审稿过程的感知影响。


<details>
  <summary>Details</summary>
Motivation: AI在学术研究中的作用日益重要，但在同行评审中的应用存在争议。支持者认为AI能减轻审稿负担、提高质量，批评者则担心影响公平性、问责制和信任。本研究旨在通过实证研究了解AI工具在实际审稿环境中的使用情况和影响。

Method: 在ICLR 2025会议中部署官方AI反馈工具，为审稿人提供审稿后建议。通过问卷调查和访谈收集数据，分析审稿人如何与该工具互动，以及他们对工具可用性和影响的感知。

Result: 研究发现AI增强同行评审既带来机遇也产生紧张关系。这是首次在真实审稿过程中收集AI工具使用的实证证据，记录了审稿人在高风险审稿环境下对AI生成反馈的反应。

Conclusion: 研究为AI辅助审稿提供了设计启示，旨在提高审稿质量的同时保护人类专业知识、自主权和责任。强调了在AI增强审稿过程中平衡技术优势与人类核心价值的重要性。

Abstract: AI is reshaping academic research, yet its role in peer review remains polarising and contentious. Advocates see its potential to reduce reviewer burden and improve quality, while critics warn of risks to fairness, accountability, and trust. At ICLR 2025, an official AI feedback tool was deployed to provide reviewers with post-review suggestions. We studied this deployment through surveys and interviews, investigating how reviewers engaged with the tool and perceived its usability and impact. Our findings surface both opportunities and tensions when AI augments in peer review. This work contributes the first empirical evidence of such an AI tool in a live review process, documenting how reviewers respond to AI-generated feedback in a high-stakes review context. We further offer design implications for AI-assisted reviewing that aim to enhance quality while safeguarding human expertise, agency, and responsibility.

</details>


### [118] [Not Seeing the Whole Picture: Challenges and Opportunities in Using AI for Co-Making Physical DIY-AT for People with Visual Impairments](https://arxiv.org/abs/2602.13874)
*Ben Kosa,Hsuanling Lee,Jasmine Li,Sanbrita Mondal,Yuhang Zhao,Liang He*

Main category: cs.HC

TL;DR: 本研究探讨了如何利用大型语言模型（LLM）支持视障人士（PVI）参与DIY辅助技术的共同制作过程，以解决现有辅助技术缺乏个性化定制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术通常采用"一刀切"的方法，忽视了视障人士的多样化需求。DIY辅助技术工具包虽然提供了定制化路径，但大多局限于与工程师共同设计或需要编程专业知识。非专业的残障人士面临工具不可访问、缺乏信心和技术知识不足等障碍，因此需要能够支持视障人士直接制作自己辅助技术的原型技术。

Method: 基于大型语言模型不仅可作为视觉辅助工具，还能作为共同设计伙伴的新兴证据，本研究进行了探索性研究，探讨LLM基础的人工智能如何支持视障人士参与有形的DIY辅助技术共同制作过程。

Result: 研究发现揭示了关键挑战和设计机会：需要更强的空间和视觉支持、减轻新型AI错误的策略，以及设计更易访问的AI辅助原型的影响因素。

Conclusion: LLM基础的人工智能有潜力支持视障人士参与DIY辅助技术的共同制作过程，但需要解决空间视觉支持、AI错误缓解和可访问性设计等关键挑战。

Abstract: Existing assistive technologies (AT) often adopt a one-size-fits-all approach, overlooking the diverse needs of people with visual impairments (PVI). Do-it-yourself AT (DIY-AT) toolkits offer one path toward customization, but most remain limited--targeting co-design with engineers or requiring programming expertise. Non-professionals with disabilities, including PVI, also face barriers such as inaccessible tools, lack of confidence, and insufficient technical knowledge. These gaps highlight the need for prototyping technologies that enable PVI to directly make their own AT. Building on emerging evidence that large language models (LLMs) can serve not only as visual aids but also as co-design partners, we present an exploratory study of how LLM-based AI can support PVI in the tangible DIY-AT co-making process. Our findings surface key challenges and design opportunities: the need for greater spatial and visual support, strategies for mitigating novel AI errors, and implications for designing more accessible AI-assisted prototypes.

</details>


### [119] [Audience in the Loop: Viewer Feedback-Driven Content Creation in Micro-drama Production on Social Media](https://arxiv.org/abs/2602.14045)
*Gengchen Cao,Tianke He,Yixuan Liu,RAY LC*

Main category: cs.HC

TL;DR: 研究探讨了社交媒体微短剧编剧的工作流程，发现快速迭代的工作模式使编剧同时承担多重角色，并基于实时观众反馈（评论、转发、表情包）调整剧情，揭示了观众互动作为社交媒体协作创作的新范式。


<details>
  <summary>Details</summary>
Motivation: 社交媒体普及导致字节化叙事内容消费增加，微短剧因其快节奏和情感悬念在中国新兴市场（如抖音、快手）广受欢迎。现有研究主要关注编剧对平台功能或偏见的感知，而非他们实际创作和迭代内容的具体过程，因此需要深入了解微短剧编剧的实际工作流程。

Method: 通过对28位微短剧编剧和脚本作家进行半结构化访谈，研究他们从创作到迭代的全过程，特别关注如何应对快速周转的工作流程以及如何处理实时观众反馈。

Result: 研究发现：1）快速周转的工作流程导致编剧同时承担多重角色；2）编剧根据实时观众反馈（评论、转发、表情包）迭代调整故事情节；3）识别出独特的叙事风格，如AI生成微短剧和观众响应式微短剧。

Conclusion: 观众互动已成为社交媒体上协作创作过程的新范式，微短剧创作体现了创作者与观众之间的实时互动和共同创作特征，为理解数字时代的叙事创作提供了新视角。

Abstract: The popularization of social media has led to increasing consumption of narrative content in byte-sized formats. Such micro-dramas contain fast-pace action and emotional cliffs, particularly attractive to emerging Chinese markets in platforms like Douyin and Kuaishou. Content writers for micro-dramas must adapt to fast-pace, audience-directed workflows, but previous research has focused instead on examining writers'experiences of platform affordances or their perceptions of platform bias, rather than the step-by-step processes through which they actually write and iterative content. In 28 semi-structured interviews with scriptwriters and writers specialized in micro-dramas, we found that the short-turn-around workflow leads to writers taking on multiple roles simultaneously, iteratively adapting to storylines in response to real-time audience feedback in the form of comments, reposts, and memes. We identified unique narrative styles such as AI-generated micro-dramas and audience-responsive micro-dramas. This work reveals audience interaction as a new paradigm for collaborative creative processes on social media.

</details>


### [120] [DALL: Data Labeling via Data Programming and Active Learning Enhanced by Large Language Models](https://arxiv.org/abs/2602.14102)
*Guozheng Li,Ao Wang,Shaoxiang Wang,Yu Zhang,Pengcheng Cao,Yang Bai,Chi Harold Liu*

Main category: cs.HC

TL;DR: DALL是一个文本标注框架，整合了数据编程、主动学习和大型语言模型，通过配置而非代码定义标注函数，提高标注效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言处理标注方法难以平衡标注质量和成本，需要一种更高效、高质量的文本标注解决方案。

Method: 提出DALL框架，结合数据编程、主动学习和大型语言模型，通过结构化规范让用户和LLM通过配置定义标注函数，主动学习识别需要审核的实例，LLM分析这些实例帮助用户修正标签并优化标注函数。

Result: 通过比较研究、消融实验和可用性研究，证明了DALL的高效性、各模块的有效性以及良好的可用性。

Conclusion: DALL框架成功解决了文本标注中质量与成本的平衡问题，为自然语言处理任务提供了有效的交互式标注系统。

Abstract: Deep learning models for natural language processing rely heavily on high-quality labeled datasets. However, existing labeling approaches often struggle to balance label quality with labeling cost. To address this challenge, we propose DALL, a text labeling framework that integrates data programming, active learning, and large language models. DALL introduces a structured specification that allows users and large language models to define labeling functions via configuration, rather than code. Active learning identifies informative instances for review, and the large language model analyzes these instances to help users correct labels and to refine or suggest labeling functions. We implement DALL as an interactive labeling system for text labeling tasks. Comparative, ablation, and usability studies demonstrate DALL's efficiency, the effectiveness of its modules, and its usability.

</details>


### [121] [Exploring a Multimodal Chatbot as a Facilitator in Therapeutic Art Activity](https://arxiv.org/abs/2602.14183)
*Le Lin,Zihao Zhu,Rainbow Tin Hung Ho,Jing Liao,Yuhan Luo*

Main category: cs.HC

TL;DR: 开发基于多模态大语言模型的聊天机器人，实时分析视觉创作并引导反思对话，用于艺术治疗支持


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的发展为AI介导的治疗支持提供了新可能，特别是在需要视觉创作与互动对话协同的艺术治疗活动中

Method: 开发基于MLLM的聊天机器人，能够实时分析视觉创作并参与反思对话，并邀请5位艺术治疗及相关领域专家进行评估

Result: 专家评估显示聊天机器人具有促进治疗参与的潜力，同时识别了多个需要改进的领域：入口和风险管理、用户画像与治疗风格的定制化对齐、对话深度与广度的平衡、视觉交互的丰富化

Conclusion: 这些发现为设计未来的AI介导创造性表达工具提供了设计路线图，需要在安全性和个性化方面进一步优化

Abstract: Therapeutic art activities, such as expressive drawing and painting, require the synergy between creative visual production and interactive dialogue. Recent advancements in Multimodal Large Language Models (MLLMs) have expanded the capacity of computing systems to interpret both textual and visual data, offering a new frontier for AI-mediated therapeutic support. This work-in-progress paper introduces an MLLM-powered chatbot that analyzes visual creation in real-time while engaging the creator in reflective conversations. We conducted an evaluation with five experts in art therapy and related fields, which demonstrated the chatbot's potential to facilitate therapeutic engagement, and highlighted several areas for future development, including entryways and risk management, bespoke alignment of user profile and therapeutic style, balancing conversational depth and width, and enriching visual interactivity. These themes provide a design roadmap for designing the future AI-mediated creative expression tools.

</details>


### [122] [Designing a Rashomon Machine: Pluri-perspectivism and XAI for Creativity Support](https://arxiv.org/abs/2602.14232)
*Marianne Bossema,Rob Saunders,Vlad Glaveanu,Somaya Ben Allouch*

Main category: cs.HC

TL;DR: 提出Pluri-perspectivism作为XAI框架，将AI解释从决策辩护转向可能性探索，支持人机共创中的创造性探索和人类主体性


<details>
  <summary>Details</summary>
Motivation: 智能技术为创造力支持提供独特机会，但设计以人为本的共创系统面临根本挑战。传统可解释AI(XAI)主要关注决策解释，需要转向支持创造性探索。生成式AI模型依赖脱离实体的数据存在根本局限，需要弥合人与机器之间的认识论差距。

Method: 提出Pluri-perspectivism作为XAI框架，采用实用主义、行动导向的方法，重新利用Rashomon技术等XAI方法。该框架促进探索创造性可能性谱系，支持人与机器之间的"视角"交换，重新引入生产性摩擦并支持人类主体性。

Result: Pluri-perspectivism框架能够弥合人与机器之间的认识论差距，促进创造性探索，支持人机共创中的人类主体性。通过重新利用XAI方法如Rashomon技术，可以探索创造性可能性谱系并促进视角交换。

Conclusion: Pluri-perspectivism作为XAI框架能够有效支持人机创造性协作，将AI解释从决策辩护转向可能性探索，重新引入生产性摩擦并增强人类在共创过程中的主体性。

Abstract: While intelligent technologies offer unique opportunities for creativity support, there are fundamental challenges in designing human-centered co-creative systems. Explainable AI (XAI) can contribute when shifting its traditional role from justification (explaining decisions) to exploration (explaining possibilities). Contextual understanding is essential for supporting embodied creativity. Generative Artificial Intelligence (AI) models are fundamentally limited, however, by their reliance on disembodied data. We propose Pluri-perspectivism as a framework for XAI, to bridge the epistemological gap between human and machine, and promote creative exploration. It is a pragmatic, action-oriented solution to guide the system, repurposing XAI methods such as the Rashomon Technique. This facilitates exploring a spectrum of creative possibilities, and the exchange of 'perspectives' between human and machine. Using Pluri-perspectivism as a framework for XAI, we can reintroduce productive friction and support human agency in human-machine creative collaborations.

</details>


### [123] [Playing the Imitation Game: How Perceived Generated Content Shapes Player Experience](https://arxiv.org/abs/2602.14254)
*Mahsa Bazzaz,Seth Cooper*

Main category: cs.HC

TL;DR: 玩家无法可靠识别关卡创作者，但游戏体验强烈受其信念影响而非事实。被认为人类设计的关卡被认为更有趣美观，被认为AI生成的关卡被认为更令人沮丧和挑战性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在游戏中的快速应用，需要研究玩家如何看待和响应生成内容，以及创作者认知如何影响游戏体验。

Method: 使用混合方法调查，在《超级马里奥兄弟》和《推箱子》游戏中比较程序生成关卡和人类设计关卡，探索玩家对创作者的认知如何影响游戏体验。

Result: 玩家无法可靠识别关卡创作者，但游戏体验强烈受其信念影响。被认为人类设计的关卡被评为更有趣、更美观；被认为AI生成的关卡被评为更令人沮丧、更具挑战性。这种负面偏见在没有创作者信息时自发出现，通常基于不可靠的"类人性"线索。

Conclusion: 研究结果强调了理解感知偏见在将生成系统集成到游戏中的重要性，玩家对创作者的信念而非事实显著影响游戏体验。

Abstract: With the fast progress of generative AI in recent years, more games are integrating generated content, raising questions regarding how players perceive and respond to this content. To investigate, we ran a mixed-method survey on the games Super Mario Bros. and Sokoban, comparing procedurally generated levels and levels designed by humans to explore how perceptions of the creator relate to players' overall experience of gameplay. Players could not reliably identify the level's creator, yet their experiences were strongly linked to their beliefs about that creator rather than the actual truth. Levels believed to be human-made were rated as more fun and aesthetically pleasing. In contrast, those believed to be AI-generated were rated as more frustrating and challenging. This negative bias appeared spontaneously without knowing the levels' creator and often was based on unreliable cues of "human-likeness." Our results underscore the importance of understanding perception biases when integrating generative systems into games.

</details>


### [124] [Key Considerations for Domain Expert Involvement in LLM Design and Evaluation: An Ethnographic Study](https://arxiv.org/abs/2602.14357)
*Annalisa Szymanski,Oghenemaro Anuyah,Toby Jia-Jun Li,Ronald A. Metoyer*

Main category: cs.HC

TL;DR: 本文通过12周的民族志研究，探讨了专业领域LLM开发团队在实践中面临的设计与评估挑战，重点关注教学聊天机器人开发中的关键实践和约束条件下的战略决策。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂专业领域的应用日益增多，但团队在实践中如何设计和评估这些系统的研究很少。本文旨在通过实证研究揭示LLM开发过程中的实际挑战和权衡。

Method: 采用12周的民族志研究方法，观察一个教学聊天机器人开发团队的设计和评估活动，并对开发者和领域专家进行访谈。

Result: 研究发现四个关键实践：1) 为数据收集创建变通方案；2) 专家输入有限时转向增强方法；3) 与专家共同制定评估标准；4) 采用专家-开发者-LLM混合评估策略。这些实践展示了团队在约束条件下如何做出战略决策，以及领域专业知识在塑造系统中的核心作用。

Conclusion: LLM开发面临专家动机与信任、参与式设计结构化困难、专业知识所有权与整合等挑战。未来LLM开发工作流应强调AI素养、透明同意机制以及能够识别专家角色演变的框架。

Abstract: Large Language Models (LLMs) are increasingly developed for use in complex professional domains, yet little is known about how teams design and evaluate these systems in practice. This paper examines the challenges and trade-offs in LLM development through a 12-week ethnographic study of a team building a pedagogical chatbot. The researcher observed design and evaluation activities and conducted interviews with both developers and domain experts. Analysis revealed four key practices: creating workarounds for data collection, turning to augmentation when expert input was limited, co-developing evaluation criteria with experts, and adopting hybrid expert-developer-LLM evaluation strategies. These practices show how teams made strategic decisions under constraints and demonstrate the central role of domain expertise in shaping the system. Challenges included expert motivation and trust, difficulties structuring participatory design, and questions around ownership and integration of expert knowledge. We propose design opportunities for future LLM development workflows that emphasize AI literacy, transparent consent, and frameworks recognizing evolving expert roles.

</details>


### [125] [Touching Movement: 3D Tactile Poses for Supporting Blind People in Learning Body Movements](https://arxiv.org/abs/2602.14442)
*Kengo Tanaka,Xiyue Wang,Hironobu Takagi,Yoichi Ochiai,Chieko Asakawa*

Main category: cs.HC

TL;DR: 本研究探索使用3D打印人体模型帮助盲人理解身体动作，通过参与式设计与盲人设计师合作开发了包含触觉参考元素的详细模型，在瑜伽姿势和健身动作训练中显著提升了理解速度和动作准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉障碍者在学习体育活动时面临困难，因为传统训练方法依赖视觉演示或不充分的口头描述，需要开发替代方法来帮助他们理解身体动作。

Method: 采用参与式设计方法，与盲人设计师合作开发详细的3D打印人体模型，包含触觉参考元素以增强空间理解。进行了两项用户研究，涉及10名盲人参与者，分别测试静态瑜伽姿势和连续健身动作的学习效果。

Result: 3D模型显著提高了理解速度，减少了澄清问题，并增强了动作准确性。参与者一致认为3D模型在易理解性、有效性和激励性方面优于传统教学方法。

Conclusion: 3D打印人体模型是帮助盲人理解身体动作的有效工具，能够弥补传统视觉依赖教学方法的不足，为视觉障碍者的体育活动学习提供了创新的解决方案。

Abstract: Visual impairments create barriers to learning physical activities, since conventional training methods rely on visual demonstrations or often inadequate verbal descriptions. This research explores 3D-printed human body models to enhance movement comprehension for blind individuals. Through a participatory design approach in collaboration with a blind designer, we developed detailed 3D models representing various body movements and incorporated tactile reference elements to enhance spatial understanding. We conducted two user studies with 10 blind participants across different activities: static yoga poses and sequential calisthenic movements. The results demonstrated that 3D models significantly improved understanding speed, reduced questions for clarification, and enhanced movement accuracy compared to conventional teaching methods. Participants consistently rated 3D models higher for ease of understanding, effectiveness, and motivation.

</details>


### [126] [Conversational Decision Support for Information Search Under Uncertainty: Effects of Gist and Verbatim Feedback](https://arxiv.org/abs/2602.14467)
*Kexin Quan,Jessie Chin*

Main category: cs.HC

TL;DR: SERA是一个基于LLM的决策辅助系统，通过提供要点式或逐字式反馈来支持信息搜索决策，在不确定性环境中提高决策准确性和信心。


<details>
  <summary>Details</summary>
Motivation: 现实世界决策依赖于信息搜索，但环境不确定性（特别是证据的诊断性分布）使搜索复杂化，导致次优决策。AI决策支持通常关注结果优化，而如何在不过度增加认知负荷的情况下支持搜索过程尚不清楚。

Method: 引入SERA（基于LLM的助手），提供要点式或逐字式反馈。通过两个实验（各54人），在三种不确定性环境中比较SERA-Gist、SERA-Verbatim和无反馈基线，评估决策结果和信息搜索行为。不确定性通过信息增益感知来操作化：递减回报（低不确定性）、局部最优（中等不确定性）、无规律（高不确定性）。

Result: 使用SERA支持的个体表现出更准确的决策结果和更高的信心，特别是在高不确定性环境中。要点式反馈与更高效的信息整合相关，显示出减少过度采样的趋势；逐字式反馈促进更广泛的探索。

Conclusion: 反馈表征是搜索相关决策中的重要设计杠杆，激励开发能够根据环境不确定性匹配反馈粒度的自适应系统。

Abstract: Many real-world decisions rely on information search, where people sample evidence and decide when to stop under uncertainty. The uncertainty in the environment, particularly how diagnostic evidence is distributed, causes complexities in information search, further leading to suboptimal decision-making outcomes. Yet AI decision support often targets outcome optimization, and less is known about how to scaffold search without increasing cognitive load. We introduce SERA, an LLM-based assistant that provides either gist or verbatim feedback during search. Across two experiments (N1=54, N2=54), we examined decision-making outcomes and information search in SERA-Gist, SERA-Verbatim, and a no-feedback baseline across three environments varying in uncertainty. The uncertainty in environment is operationalized by the perceived gain of information across the course of sampling, which individuals may experience diminishing return of information gain (decremental; low-uncertainty), or a local drop of information gain (local optimum; medium-uncertainty), or no patterns in information gain (high-uncertainty), as they search more. Individuals show more accurate decision outcomes and are more confident with SERA support, especially under higher uncertainty. Gist feedback was associated with more efficient integration and showed a descriptive pattern of reduced oversampling, while verbatim feedback promoted more extensive exploration. These findings establish feedback representation as a design lever when search matters, motivating adaptive systems that match feedback granularity to uncertainty.

</details>


### [127] [TouchFusion: Multimodal Wristband Sensing for Ubiquitous Touch Interactions](https://arxiv.org/abs/2602.15011)
*Eric Whitmire,Evan Strasnick,Roger Boldu,Raj Sodhi,Nathan Godwin,Shiu Ng,Andre Levi,Amy Karlson,Ran Tan,Josef Faller,Emrah Adamey,Hanchuan Li,Wolf Kienzle,Hrvoje Benko*

Main category: cs.HC

TL;DR: TouchFusion是一款腕带设备，无需额外仪器或计算机视觉即可在附近表面上实现触摸交互，通过多模态传感器融合支持状态化触摸检测、手势识别和跟踪功能。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需额外仪器或计算机视觉的通用触摸交互解决方案，实现无处不在、上下文自适应的交互模型，特别是在与智能眼镜或增强现实设备配合使用时。

Method: 结合表面肌电图(sEMG)、生物阻抗、惯性和光学传感来捕捉手部活动的多个方面，通过早期和晚期融合技术实现状态化触摸检测、简单表面手势和跟踪功能。

Result: 在100名参与者的数据集上验证了方法有效性，显著超过典型可穿戴传感研究的样本量，能够支持多种常见触摸交互任务，包括在任何表面召唤触控板、基于点击位置控制上下文自适应界面，以及将手掌作为始终可用的触摸表面。

Conclusion: TouchFusion能够实现无处不在、上下文自适应的交互模型，特别是在与智能眼镜或增强现实设备配合使用时，为可穿戴交互提供了新的可能性。

Abstract: TouchFusion is a wristband that enables touch interactions on nearby surfaces without any additional instrumentation or computer vision. TouchFusion combines surface electromyography (sEMG), bioimpedance, inertial, and optical sensing to capture multiple facets of hand activity during touch interactions. Through a combination of early and late fusion, TouchFusion enables stateful touch detection on both environmental and body surfaces, simple surface gestures, and tracking functionality for contextually adaptive interfaces as well as basic trackpad-like interactions. We validate our approach on a dataset of 100 participants, significantly exceeding the population size of typical wearable sensing studies to capture a wider variance of wrist anatomies, skin conductivities, and behavioral patterns. We show that TouchFusion can enable several common touch interaction tasks. Using TouchFusion, a wearer can summon a trackpad on any surface, control contextually adaptive interfaces based on where they tap, or use their palm as an always-available touch surface. When paired with smart glasses or augmented reality devices, TouchFusion enables a ubiquitous, contextually adaptive interaction model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [128] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA是一个强化学习框架，通过结构化文本空间环境让大语言模型具备显式、可解释的空间推理能力，用于内容感知的图形布局设计。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在空间推理能力上的局限性，以及设计决策过程缺乏透明度的问题，实现更透明可控的布局设计。

Method: 将布局设计重新定义为结构化文本空间环境上的策略学习问题，使用多目标空间评估（几何有效性、关系一致性、美学一致性）和相对组优化进行训练。

Result: LaySPA在结构有效性和视觉质量上优于更大的专有大语言模型，性能与专门的SOTA布局生成器相当，同时需要更少的标注样本和更低的延迟。

Conclusion: LaySPA通过显式空间推理框架，成功解决了LLMs在布局设计中的空间推理局限性和透明度问题，实现了高质量、可解释的图形布局生成。

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [129] [OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery](https://arxiv.org/abs/2602.13769)
*Qi Liu,Wanjing Ma*

Main category: cs.AI

TL;DR: OR-Agent是一个可配置的多智能体研究框架，通过树状工作流管理假设生成和系统回溯，结合进化-系统构思机制和分层优化反思系统，在组合优化和协同驾驶场景中超越进化基线。


<details>
  <summary>Details</summary>
Motivation: 自动化科学发现在复杂实验驱动领域需要超越简单的程序迭代变异，需要结构化的假设管理、环境交互和有原则的反思机制。

Method: 提出OR-Agent框架：1）基于树状工作流组织研究，显式建模分支假设生成和系统回溯；2）进化-系统构思机制统一进化选择、研究计划生成和协调探索；3）分层优化反思系统包括短期实验反思（口头梯度）、长期反思（口头动量）和记忆压缩（正则化机制）。

Result: 在经典组合优化基准（旅行商、车辆路径、装箱、定向、多背包问题）和仿真协同驾驶场景中，OR-Agent优于强进化基线，提供了一个通用、可扩展、可检查的AI辅助科学发现框架。

Conclusion: OR-Agent通过结构化假设管理、进化-系统构思和分层反思机制，为自动化科学发现提供了一个原则性的架构，在复杂实验驱动领域展现出优越性能。

Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.

</details>


### [130] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: 该论文提出了PITA数据集来研究推理模型中的长度泛化问题，发现推理轨迹模型在宽而浅的任务上泛化良好，但在窄而深的任务上表现较差，揭示了推理轨迹范式的基本优势和限制。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型近年来取得了飞速进展，但我们对推理轨迹如何支持推理以及该范式的局限性仍缺乏完整理解。为了更清晰地理解这些问题，需要创建大规模数据集来研究推理模型的泛化能力，特别是长度泛化问题。

Method: 1. 引入PITA数据集：包含超过2300万条命题逻辑语句及其对应证明的大规模数据集；2. 提出任务深度和任务广度的概念来衡量推理复杂度；3. 在PITA的不同子集上变化这些量；4. 与基于三段论的简单合成任务进行比较验证结果的普遍性。

Result: 推理轨迹模型在广度和浅度任务上泛化良好，但在窄度和深度任务上相对于非推理轨迹基线表现恶化。研究结果揭示了推理轨迹模型在深度任务上的基本缩放限制，同时突出了它们在广度任务上的泛化优势。

Conclusion: 该研究识别了使用推理轨迹的内在基本优势和局限性，提出了限制推理轨迹模型在深度任务上表现的基本缩放规律，同时强调了它们在广度任务上的泛化优势，为理解推理模型的泛化能力提供了理论框架。

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [131] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench基准测试基于哲学家就餐问题，评估LLM在多智能体系统中的协调能力，发现LLM在顺序决策时能有效协调，但在同时决策时死锁率超过95%，且通信无法解决此问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地部署在多智能体系统中，目前缺乏能够测试它们在资源竞争下协调能力的基准。研究者需要评估LLM在需要并发资源访问的场景中的协调表现

Method: 基于哲学家就餐问题构建DPBench基准，通过8种不同条件（决策时机、群体规模、通信能力）评估LLM协调能力。使用GPT-5.2、Claude Opus 4.5和Grok 4.1进行实验

Result: 发现显著的不对称性：LLM在顺序设置中能有效协调，但在需要同时决策时失败，某些条件下死锁率超过95%。通信不仅无法解决问题，反而可能增加死锁率。失败源于收敛推理现象

Conclusion: 需要并发资源访问的多智能体LLM系统可能需要外部协调机制，而非依赖涌现的协调能力。DPBench作为开源基准发布，为评估LLM协调能力提供了重要工具

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [132] [A First Proof Sprint](https://arxiv.org/abs/2602.13587)
*Joseph Corneli*

Main category: cs.AI

TL;DR: 本文报道了一个多智能体证明冲刺项目，针对十个研究级问题，结合快速草稿生成、对抗性验证、针对性修复和明确溯源。通过依赖关系图分解定位漏洞并协调评审驱动的修订，最终获得异质但明确的成果。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索在压缩时间框架内进行高质量数学证明验证的方法。传统证明过程通常耗时较长，作者希望通过多智能体协作、结构化验证和分层策略来提高证明冲刺的可靠性和校准度。

Method: 采用多智能体协作框架，结合快速草稿生成与对抗性验证。使用接线图分解来可视化声明依赖关系，从而定位证明漏洞。实施针对性修复和明确溯源机制，区分数学状态与QC验证状态。采用分层切换策略，将数学证明与验证过程分离。

Result: 十个问题获得异质结果：问题3在特定标准下获得验证完整的证明路径；问题5在限定范围内解决；问题10在明确假设下解决；问题4和6获得部分结果；问题7通过旋转路径定理链暂时关闭。在QC验证层，问题7和9有节点级验证但仍有未解决的验证缺口。

Conclusion: 结构化验证和分层切换策略显著提高了压缩证明冲刺的可靠性和校准度。该方法能够产生明确区分数学状态和验证状态的异质结果，为快速数学验证提供了有效框架。

Abstract: This monograph reports a multi-agent proof sprint on ten research-level problems, combining rapid draft generation with adversarial verification, targeted repair, and explicit provenance. The workflow uses wiring-diagram decompositions of claim dependencies to localize gaps and coordinate reviewer-driven revisions. Final outcomes are heterogeneous but explicit: the manuscript distinguishes mathematical status from QC-validation status. Mathematically, Problem~3 has a validation-complete existence path under the scoped criterion used here (uniqueness/irreducibility treated as optional), Problem 5 is solved in a scope-limited form for $F_O$-local connective spectra, Problem 10 is conditional under clearly stated assumptions (with explicit necessity counterexamples when assumptions are dropped), and Problems 4 and 6 are partial with named remaining obligations in the general case (including an unconditional $K_n$ result for Problem 6 with $c_0 = 1/3$). Problem 7 is treated as provisionally closed via the rotation-route theorem chain, pending independent ledger re-check. At the QC layer, Problems~7 and~9 have node-level validation artifacts but still contain unresolved verifier gaps. The main methodological result is that structure-aware verification and layer-switching strategies improve reliability and calibration in compressed proof sprints.

</details>


### [133] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 前沿AI模型在核危机模拟中表现出复杂的战略行为，包括欺骗、心智理论和元认知能力，验证了部分战略理论但也挑战了核禁忌等核心假设。


<details>
  <summary>Details</summary>
Motivation: 研究前沿大语言模型在战略竞争中的行为模式，特别是核危机情境下的决策逻辑，为国家安全专业人员和战略分析提供AI模拟工具，同时理解AI如何模仿或偏离人类战略思维。

Method: 使用三个前沿大语言模型（GPT-5.2、Claude Sonnet 4、Gemini 3 Flash）在核危机模拟中扮演对立领导人角色，观察其在不确定性下的推理和行为模式。

Result: AI模型表现出欺骗、心智理论和元认知能力；验证了Schelling的承诺理论、Kahn的升级框架和Jervis的误解理论；但发现核禁忌未能阻止核升级，战略核攻击虽罕见但确实发生，威胁更多引发对抗而非服从，高互信反而加速冲突，模型从未选择妥协或撤退。

Conclusion: AI模拟是强大的战略分析工具，但需要根据人类推理模式进行适当校准。理解前沿模型如何模仿或不模仿人类战略逻辑对于AI日益影响战略结果的世界至关重要。

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [134] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: 研究发现人们在工作和个人生活中对AI工具准确性的要求存在显著差异：工作中要求高准确性的比例(24.1%)远高于个人生活(8.8%)，且工具不可用时对个人生活的影响更大。


<details>
  <summary>Details</summary>
Motivation: 研究人们在专业和个人情境下使用AI工具时如何权衡准确性，了解这些权衡的决定因素，以及当AI/应用不可用时用户如何应对。

Method: 通过在线调查(N=300)收集数据，将"准确性"定义为情境特定的可靠性：输出在特定容错阈值内与用户意图对齐的程度，该阈值取决于风险水平和修正成本。

Result: 工作中要求高准确性的比例(24.1%)显著高于个人生活(8.8%)；使用大量应用和经验模式与更严格的工作标准相关；工具不可用时，对个人日常生活的干扰(34.1%)大于工作(15.3%)。

Conclusion: 人们对AI工具准确性的要求因情境而异，工作中要求更高准确性，而工具不可用对个人生活的影响更大，这为AI系统设计和用户支持提供了重要启示。

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [135] [Building Autonomous GUI Navigation via Agentic-Q Estimation and Step-Wise Policy Optimization](https://arxiv.org/abs/2602.13653)
*Yibo Wang,Guangda Huzhang,Yuwei Hu,Yu Xia,Shiyin Lu,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.AI

TL;DR: 提出基于多模态大语言模型的GUI智能体框架，包含agentic-Q评估和逐步策略优化，降低数据收集成本并实现稳定优化


<details>
  <summary>Details</summary>
Motivation: 现实应用中GUI智能体面临非平稳环境，导致数据整理和策略优化的计算成本高昂，需要更高效的解决方案

Method: 提出两阶段框架：1) agentic-Q评估模型生成逐步值评估动作对任务完成的贡献；2) 逐步策略优化，从状态-动作轨迹采样，通过强化学习优化策略

Result: 框架使Ovis2.5-9B具备强大的GUI交互能力，在GUI导航和基础基准测试中表现优异，甚至超越更大规模的竞争对手

Conclusion: 该框架有效解决了GUI智能体在非平稳环境中的计算成本问题，通过自生成轨迹和与环境解耦的策略更新，实现了高效稳定的优化

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have substantially driven the progress of autonomous agents for Graphical User Interface (GUI). Nevertheless, in real-world applications, GUI agents are often faced with non-stationary environments, leading to high computational costs for data curation and policy optimization. In this report, we introduce a novel MLLM-centered framework for GUI agents, which consists of two components: agentic-Q estimation and step-wise policy optimization. The former one aims to optimize a Q-model that can generate step-wise values to evaluate the contribution of a given action to task completion. The latter one takes step-wise samples from the state-action trajectory as inputs, and optimizes the policy via reinforcement learning with our agentic-Q model. It should be noticed that (i) all state-action trajectories are produced by the policy itself, so that the data collection costs are manageable; (ii) the policy update is decoupled from the environment, ensuring stable and efficient optimization. Empirical evaluations show that our framework endows Ovis2.5-9B with powerful GUI interaction capabilities, achieving remarkable performances on GUI navigation and grounding benchmarks and even surpassing contenders with larger scales.

</details>


### [136] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: 本文提出了X-Blocks框架，用于系统分析自动驾驶车辆自然语言解释的语言构建块，包含上下文、句法和词汇三个层次，其中RACE框架能准确分类32种场景感知的解释类别。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统框架来分析人类如何在不同驾驶场景中语言构建驾驶原理，而自然语言解释对于建立自动驾驶车辆的信任和接受度至关重要。

Method: 提出X-Blocks分层分析框架，包含三个层次：上下文层面使用RACE多LLM集成框架结合思维链推理和自一致性机制进行分类；词汇层面使用带信息性狄利克雷先验的对数几率分析；句法层面使用依存句法分析和模板提取。

Result: RACE在Berkeley DeepDrive-X数据集上达到91.45%的准确率和0.91的Cohen's kappa系数；词汇分析揭示了场景特定的词汇模式；句法分析显示解释使用有限的语法家族，谓词类型和因果结构在不同上下文中有系统变化。

Conclusion: X-Blocks框架是数据集无关和任务独立的，为生成支持透明度、用户信任和认知可及性的场景感知解释提供了基于证据的语言设计原则。

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [137] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: MOC-2HER：一种结合双重目标经验回放的分层强化学习方法，专门解决稀疏奖励多目标环境中的物体操作任务


<details>
  <summary>Details</summary>
Motivation: 现有的分层强化学习方法（如OC和MOC）在稀疏奖励的多目标环境中表现不佳，特别是在物体操作任务中，奖励取决于物体是否到达目标而非智能体直接交互，这使得智能体难以学习如何与物体互动

Method: 首先提出MOC-HER，将Hindsight Experience Replay集成到MOC框架中；然后引入2HER，创建两套虚拟目标：一套基于物体最终状态（标准HER），另一套基于智能体效应器位置，奖励智能体既与物体交互又完成任务

Result: 在机器人操作环境中，MOC-2HER成功率高达90%，而MOC和MOC-HER的成功率均低于11%，双重目标重标记策略在稀疏奖励多目标任务中效果显著

Conclusion: 双重目标经验回放策略有效解决了分层强化学习在稀疏奖励多目标物体操作任务中的局限性，显著提升了智能体学习与物体交互并完成任务的能力

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [138] [GRAIL: Goal Recognition Alignment through Imitation Learning](https://arxiv.org/abs/2602.14252)
*Osher Elhadad,Felipe Meneguzzi,Reuth Mirsky*

Main category: cs.AI

TL;DR: GRAIL使用模仿学习和逆强化学习从（可能次优的）演示轨迹中学习每个候选目标的目标导向策略，实现一次性推理的目标识别，在次优、系统性偏差和噪声环境下显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有目标识别方法通常依赖于最优目标导向策略表示，但这可能与行为者的真实行为不同，阻碍准确识别其目标。需要解决这一差距以更好地对齐AI系统与人类意图。

Method: GRAIL（Goal Recognition Alignment through Imitation Learning）结合模仿学习和逆强化学习，直接从（可能次优的）演示轨迹中为每个候选目标学习一个目标导向策略。通过单次前向传递用每个学习到的策略对观察到的部分轨迹进行评分。

Result: 在系统性偏差最优行为下F1分数提升超过0.5；在次优行为下提升约0.1-0.3；在噪声最优轨迹下提升高达0.4；在完全最优设置下保持竞争力。

Conclusion: GRAIL在保持经典目标识别一次性推理能力的同时，通过学习到的策略捕捉次优和系统性偏差行为，为在不确定环境中解释智能体目标提供了可扩展且鲁棒的模型。

Abstract: Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.

</details>


### [139] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic是一个通过生成-验证-修复循环自动合成可执行程序对的框架，用于扩展可验证训练信号，解决了RLVR中的规模化瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 可验证训练信号的规模化是强化学习从可验证奖励（RLVR）的关键瓶颈。逻辑推理是天然载体，但现有合成方法依赖专家编写代码或固定模板，限制了任务层面的扩展。

Method: 提出SSLogic框架，通过迭代合成和修复可执行的生成器-验证器程序对，在封闭的生成-验证-修复循环中实现连续任务族演化。引入多门验证协议，结合多策略一致性检查和对抗性盲审，确保可靠性。

Result: 从400个种子族开始，经过两轮演化扩展到953个任务族和21,389个可验证实例（从5,718个）。在SSLogic演化数据上训练相比种子基线有显著提升：SynLogic +5.2，BBEH +1.4，AIME25 +3.0，Brumo25 +3.7。

Conclusion: SSLogic框架成功实现了任务族层面的规模化扩展，通过自动化的程序对合成和验证机制，显著提升了可验证训练数据的质量和数量，为RLVR提供了有效的规模化解决方案。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [140] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出轨迹主导帕累托优化框架，将智能视为轨迹层面现象，揭示了帕累托陷阱如何限制长期适应性发展，并定义了陷阱逃逸难度指数来量化这种约束。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能近期取得进展，但许多系统在长期适应性方面出现停滞。作者认为这种限制并非源于学习、数据或模型容量不足，而是源于智能随时间优化的深层结构特性。

Method: 提出轨迹主导帕累托优化框架，将经典帕累托最优性推广到轨迹层面；定义陷阱逃逸难度指数(TEDI)来量化约束刚性；建立帕累托陷阱的形式化分类；使用最小智能体-环境模型进行说明。

Result: 动态智能上限是轨迹层面主导性的必然几何结果，与学习进展或架构规模无关；揭示了优化几何如何决定智能发展，而非终端性能。

Conclusion: 将智能研究的焦点从终端性能转移到优化几何，为诊断和克服自适应系统中的长期发展约束提供了原则性框架。

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [141] [DECKBench: Benchmarking Multi-Agent Frameworks for Academic Slide Generation and Editing](https://arxiv.org/abs/2602.13318)
*Daesik Jang,Morgan Lindsay Heisler,Linzi Xing,Yifei Li,Edward Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.AI

TL;DR: DECKBench是一个用于评估多智能体幻灯片生成和编辑的基准测试框架，包含精心策划的数据集和系统化评估协议，旨在解决现有评估方法在内容选择、组织、布局和指令遵循方面的不足。


<details>
  <summary>Details</summary>
Motivation: 学术幻灯片自动生成和迭代编辑需要超越文档摘要的能力，包括忠实的内容选择、连贯的幻灯片组织、布局感知的渲染和鲁棒的多轮指令遵循。然而，现有基准测试和评估协议未能充分衡量这些挑战。

Method: 研究者引入了DECKBench评估框架，基于精心策划的论文-幻灯片对数据集，并增加了真实的模拟编辑指令。评估协议系统化评估幻灯片级和演示文稿级的保真度、连贯性、布局质量和多轮指令遵循。同时实现了一个模块化的多智能体基线系统，将任务分解为论文解析与摘要、幻灯片规划、HTML创建和迭代编辑。

Result: 实验结果表明，提出的基准测试能够突出系统优势、暴露失败模式，并为改进多智能体幻灯片生成和编辑系统提供可操作的见解。该基准测试为学术演示文稿生成和编辑的可重复、可比较评估建立了标准化基础。

Conclusion: 这项工作为学术演示文稿生成和编辑的可重复和可比较评估建立了标准化基础，代码和数据已公开可用。

Abstract: Automatically generating and iteratively editing academic slide decks requires more than document summarization. It demands faithful content selection, coherent slide organization, layout-aware rendering, and robust multi-turn instruction following. However, existing benchmarks and evaluation protocols do not adequately measure these challenges. To address this gap, we introduce the Deck Edits and Compliance Kit Benchmark (DECKBench), an evaluation framework for multi-agent slide generation and editing. DECKBench is built on a curated dataset of paper to slide pairs augmented with realistic, simulated editing instructions. Our evaluation protocol systematically assesses slide-level and deck-level fidelity, coherence, layout quality, and multi-turn instruction following. We further implement a modular multi-agent baseline system that decomposes the slide generation and editing task into paper parsing and summarization, slide planning, HTML creation, and iterative editing. Experimental results demonstrate that the proposed benchmark highlights strengths, exposes failure modes, and provides actionable insights for improving multi-agent slide generation and editing systems. Overall, this work establishes a standardized foundation for reproducible and comparable evaluation of academic presentation generation and editing. Code and data are publicly available at https://github.com/morgan-heisler/DeckBench .

</details>


### [142] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: 该研究开发了一个基于LLM提取语言特征的可扩展系统，用于检测临床训练大语言模型中的越狱尝试，通过专家标注的四个核心语言特征训练BERT模型进行特征提取，然后使用多种分类器进行越狱检测。


<details>
  <summary>Details</summary>
Motivation: 临床训练大语言模型需要检测越狱尝试，但之前基于手动标注语言特征的方法存在可扩展性和表达能力有限的问题，需要更自动化和可扩展的解决方案。

Method: 使用专家标注的四个核心语言特征（专业性、医学相关性、伦理行为、上下文干扰），训练多个通用领域和医学领域的BERT模型来预测这些特征，选择最可靠的特征回归器作为特征提取器，然后使用基于树、线性、概率和集成方法的分类器进行越狱检测。

Result: 系统在交叉验证和保留集评估中表现出色，表明LLM提取的语言特征为自动越狱检测提供了有效基础。错误分析揭示了当前标注和特征表示的关键局限性。

Conclusion: 这项工作展示了一种可扩展且可解释的方法，用于检测安全关键临床对话系统中的越狱行为，指出了未来改进方向，如更丰富的标注方案、更细粒度的特征提取以及捕捉对话过程中越狱行为演变风险的方法。

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [143] [MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents](https://arxiv.org/abs/2602.13372)
*Simon Rosen,Siddarth Singh,Ebenezer Gelo,Helen Sarah Robertson,Ibrahim Suder,Victoria Williams,Benjamin Rosman,Geraud Nangue Tasse,Steven James*

Main category: cs.AI

TL;DR: 提出Morality Chains形式化框架和MoralityGym基准，用于评估AI在冲突性层级道德规范下的对齐表现，通过98个伦理困境问题测试，揭示现有安全强化学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估AI在冲突性、层级化人类道德规范下的对齐表现是AI安全、道德哲学和认知科学交叉领域的关键挑战，需要更系统化的评估框架。

Method: 提出Morality Chains形式化框架，将道德规范表示为有序的道义约束；创建MoralityGym基准，包含98个伦理困境问题，以电车困境风格的Gymnasium环境呈现；将任务解决与道德评估解耦，引入新的道德度量标准。

Result: 使用安全强化学习方法获得的基线结果揭示了关键局限性，表明需要更原则性的伦理决策方法；该框架为整合心理学和哲学见解提供了基础。

Conclusion: 这项工作为开发在复杂现实世界中表现更可靠、透明和道德的AI系统奠定了基础，强调了需要更系统化的道德对齐评估方法。

Abstract: Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.

</details>


### [144] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 提出了一种结合自监督可微分聚类和新型可微分ILP的方法，直接从原始数据学习规则，解决了传统可微分ILP方法中存在的显式标签泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 传统可微分归纳逻辑编程（ILP）方法主要依赖符号数据集，难以直接从原始数据学习规则，存在显式标签泄漏问题——即无法在没有输入特征标签显式监督的情况下将连续输入映射到符号变量。

Method: 集成自监督可微分聚类模型与新型可微分ILP模型，使系统能够直接从原始数据学习规则，避免显式标签泄漏问题。

Result: 该方法能够直观且精确地从时间序列和图像数据中学习泛化规则，有效通过特征描述原始数据。

Conclusion: 提出的集成方法成功解决了可微分ILP中的显式标签泄漏问题，实现了直接从原始数据学习可解释规则的能力，在时间序列和图像数据上表现出良好的泛化性能。

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [145] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: 提出一种无需训练即可处理多表关系数据库的上下文学习基础模型，通过列内压缩而非跨列压缩来保持信息完整性，并与现有单表基础模型无缝集成。


<details>
  <summary>Details</summary>
Motivation: 关系数据库包含大量异构表格信息可用于预测建模，但企业环境中潜在预测目标众多，需要避免每次预测新目标时重新训练模型。现有基于上下文学习的基础模型主要局限于单表操作，需要扩展到多表关系数据库。

Method: 提出一种原则性的RDB编码器家族，将可变大小的RDB邻域压缩为固定长度的ICL样本。关键创新在于限制在共享单位和角色的高维RDB列内进行压缩，而不是跨列压缩。编码器无需可训练参数，可与现有单表ICL基础模型无缝集成。开发了可扩展的SQL原语实现编码阶段。

Result: 开发了开源的RDB基础模型，能够在未见数据集上实现稳健性能，无需训练或微调即可直接使用。理论分析和实证证据表明，列内压缩方法优于跨列压缩。

Conclusion: 通过限制在共享单位和角色的列内进行压缩，可以构建无需训练参数的关系数据库编码器，与现有单表上下文学习基础模型无缝集成，实现开箱即用的多表关系数据库预测能力。

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [146] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 论文提出Vashista稀疏注意力机制，通过理论证明注意力可集中在少量关键token上，实现长上下文推理的恒定计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中主要计算成本集中在注意力机制上，但实证研究表明只有少量token对每个查询有实质性贡献。需要一种理论框架来解释这一现象，并开发实用的稀疏注意力机制来降低计算成本。

Method: 1. 理论建模：将注意力建模为在关键向量凸包上的投影，分析其熵松弛（softmax-like）特性；2. 提出面稳定性定理：在严格互补边际条件下，熵注意力集中在恒定大小的活跃面上；3. 开发Vashista稀疏注意力：基于分页式上下文选择策略维护每个查询的小候选集，与现代推理栈兼容。

Result: 1. 理论结果：证明非活跃token的总质量以指数速度衰减，活跃面上的误差随温度参数线性缩放；2. 实践效果：在长上下文评估中观察到稳定的恒定大小有效支持、显著的时钟速度提升，以及在支持间隙诊断预测的区域内质量下降最小。

Conclusion: 该研究为稀疏长上下文解码提供了安全判据和精度与计算权衡的理论基础，Vashista稀疏注意力机制在实际应用中实现了可预测的延迟和成本，特别适用于隐私敏感和隔离环境。

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [147] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: Ambient Physics框架直接从部分观测中学习PDE系数-解对的联合分布，无需完整观测数据，通过随机掩码已观测点实现训练，在重建性能上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景中，获取PDE系数和解的完整观测数据成本高昂、危险甚至不可能。现有基于扩散的方法需要完整观测数据进行训练，这在实际应用中存在限制。

Method: 提出Ambient Physics框架，核心思想是随机掩码已观测测量值的子集并对其进行监督训练。这样模型无法区分"真正未观测"和"人工未观测"点，从而必须在所有位置产生合理的预测。

Result: Ambient Physics实现了最先进的重建性能：相比之前的扩散方法，平均总体误差降低62.51%，同时使用125倍更少的函数评估。还发现了"单点转换"现象：掩码单个已观测点即可在不同架构和测量模式下从部分观测中学习。

Conclusion: Ambient Physics框架使得在无法获得完整观测数据的科学场景中取得进展成为可能，为部分观测条件下的物理场重建提供了有效解决方案。

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [148] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 论文提出两种统计原理的早停方法，通过监控生成过程中的不确定性信号来减少LLM在模糊查询下的过度推理，提高推理效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理能力上虽有显著提升，但有时会过度思考，特别是在面对不确定、表述不清或模糊的查询时，会生成不必要的推理步骤。需要解决这种过度推理问题以提高效率和可靠性。

Method: 提出了两种基于统计原理的早停方法：1) 参数化方法：将不确定性关键词的出现间隔时间建模为更新过程，并应用序列测试进行停止决策；2) 非参数化方法：为定义明确的查询提供有限样本保证，确保不会过早停止。

Result: 在多个领域和模型的推理任务上进行实证评估，结果表明不确定性感知的早停方法能够同时提高LLM推理的效率和可靠性，在数学推理任务上观察到特别显著的改进。

Conclusion: 通过监控不确定性信号的统计早停方法能有效减少LLM的过度推理问题，特别是在数学推理等任务中表现出显著优势，为提升LLM推理的效率和可靠性提供了实用解决方案。

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [149] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: PIC（并行迭代压缩）通过修改Transformer注意力掩码，将长上下文压缩为局部块记忆嵌入，降低训练难度，在QA等高压缩比任务中显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 长上下文显著增加LLM推理延迟，现有软提示压缩方法需要捕获全局依赖并依赖大量预训练数据，训练难度大

Method: 受人类工作记忆分块机制启发，通过修改Transformer注意力掩码，将记忆嵌入的接收域限制在顺序局部块中，降低压缩器训练难度

Result: 在多个下游任务中优于基线方法，高压缩比场景优势显著（64×压缩比下QA任务F1提升29.8%，EM提升40.7%），训练时间减少约40%

Conclusion: PIC通过局部化压缩策略有效解决长上下文压缩问题，在性能和训练效率方面均有显著提升

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [150] [Choosing How to Remember: Adaptive Memory Structures for LLM Agents](https://arxiv.org/abs/2602.14038)
*Mingfei Lu,Mengjia Wu,Feng Liu,Jiawei Xu,Weikai Li,Haoyang Wang,Zhengdong Hu,Ying Ding,Yizhou Sun,Jie Lu,Yi Zhang*

Main category: cs.AI

TL;DR: FluxMem是一个自适应记忆组织框架，为LLM智能体提供多种记忆结构，通过学习交互特征选择最佳记忆结构，并采用三层记忆层次和概率门控进行记忆融合，在长时程交互中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在两个关键问题：采用一刀切的记忆结构，且未将记忆结构选择建模为上下文自适应决策，这限制了处理异构交互模式的能力并导致性能不佳。

Method: 提出FluxMem统一框架，为智能体配备多种互补记忆结构，基于交互级特征学习选择最佳结构（使用下游响应质量和记忆利用率的离线监督）。引入三层记忆层次和基于Beta混合模型的概率门控进行分布感知记忆融合，替代脆弱的相似度阈值。

Result: 在PERSONAMEM和LoCoMo两个长时程基准测试中，平均分别提升了9.18%和6.14%的性能。

Conclusion: FluxMem通过自适应记忆组织显著提升了LLM智能体在长时程交互中的性能，解决了现有记忆系统的局限性。

Abstract: Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.

</details>


### [151] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: 本文提出代数量子智能(AQI)框架，通过非交换代数结构扩展LLMs的语义空间，解决其创造性受限问题，在多个创意推理基准上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成流畅文本方面表现出色，但其真正的创造性输出能力有限。作者认为这种限制源于LLMs的结构特性：当提供丰富上下文时，未来生成空间受到强烈约束，生成过程接近确定性动态。现有方法如测试时缩放和上下文适应虽能提升性能，但未从根本上改变这种约束。

Method: 提出代数量子智能(AQI)作为计算框架，采用受量子理论启发的非交换代数结构，实现顺序依赖、干涉和不确定性等特性。语义状态表示为希尔伯特空间中的向量，其演化由非交换算子计算的C值控制，确保多个未来语义可能性的共存和扩展。通过扩展基于Transformer的LLM，引入600多个专用算子来实现AQI。

Result: 在涵盖十个领域的创意推理基准上，使用LLM-as-a-judge协议进行评估。结果显示AQI持续优于强基线模型，产生统计显著的改进并降低跨领域方差。非交换代数动态可作为机器创造性的实用且可复现基础。

Conclusion: 非交换代数动态能够为机器创造性提供实用且可复现的基础。该架构已在真实企业环境中部署，证明了其实际应用价值。

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [152] [CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments](https://arxiv.org/abs/2602.14229)
*Abubakarr Jaye,Nigel Boachie Kumankumah,Chidera Biringa,Anjel Shaileshbhai Patel,Sulaiman Vesal,Dayquan Julienne,Charlotte Siska,Manuel Raúl Meléndez Luján,Anthony Twum-Barimah,Mauricio Velazco,Tianwei Chen*

Main category: cs.AI

TL;DR: 论文提出Multi-Horizon Task Environments（MHTEs）新问题类别，针对多并发长时程任务的挑战，开发CorpGen框架解决四种失败模式，在OSWorld Office上实现3.5倍性能提升


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只评估智能体在孤立单任务上的表现，而真实组织工作需要管理许多并发的长时程任务，涉及任务交错、依赖关系和优先级重排。需要新的评估框架来模拟这种复杂场景。

Method: 提出CorpGen架构无关框架，采用分层规划实现多时程目标对齐，子智能体隔离防止任务交叉污染，分层内存（工作、结构化、语义）和自适应摘要。通过具有持久身份和真实日程的数字员工模拟企业环境。

Result: 在OSWorld Office上，CorpGen在三个CUA后端（UFO2、OpenAI CUA、分层）上实现高达3.5倍改进（15.2% vs 4.3%），在负载增加时保持稳定性能。消融研究表明经验学习提供最大收益。

Conclusion: CorpGen框架有效解决了多时程任务环境中的四种失败模式，性能提升源于架构机制而非特定CUA实现。该工作为评估和管理复杂组织环境中的自主智能体提供了新基准。

Abstract: Long-horizon reasoning is a key challenge for autonomous agents, yet existing benchmarks evaluate agents on single tasks in isolation. Real organizational work requires managing many concurrent long-horizon tasks with interleaving, dependencies, and reprioritization. We introduce Multi-Horizon Task Environments (MHTEs): a distinct problem class requiring coherent execution across dozens of interleaved tasks (45+, 500-1500+ steps) within persistent execution contexts spanning hours. We identify four failure modes that cause baseline CUAs to degrade from 16.7% to 8.7% completion as load scales 25% to 100%, a pattern consistent across three independent implementations. These failure modes are context saturation (O(N) vs O(1) growth), memory interference, dependency complexity (DAGs vs. chains), and reprioritization overhead. We present CorpGen, an architecture-agnostic framework addressing these failures via hierarchical planning for multi-horizon goal alignment, sub-agent isolation preventing cross-task contamination, tiered memory (working, structured, semantic), and adaptive summarization. CorpGen simulates corporate environments through digital employees with persistent identities and realistic schedules. Across three CUA backends (UFO2, OpenAI CUA, hierarchical) on OSWorld Office, CorpGen achieves up to 3.5x improvement over baselines (15.2% vs 4.3%) with stable performance under increasing load, confirming that gains stem from architectural mechanisms rather than specific CUA implementations. Ablation studies show experiential learning provides the largest gains.

</details>


### [153] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: 提出"批判性抗性基准测试"框架，通过对抗性生成-评估游戏解决AI模型超越人类理解能力后的基准测试难题


<details>
  <summary>Details</summary>
Motivation: 前沿大语言模型快速饱和新基准测试，当模型能力超越人类理解时，传统基准测试面临挑战：人类难以生成区分性任务、提供准确答案或评估复杂解决方案。如果基准测试不可行，我们将无法衡量AI进展

Method: 提出批判性抗性基准测试框架，基于"批判性抗性正确性"概念：答案被认为是正确的，除非对手能令人信服地证明其错误。人类作为有限验证者，专注于局部声明而非完全理解任务。使用项目化二分Bradley-Terry模型联合排名LLMs的解题能力和生成难题能力

Result: 在数学领域对八个前沿LLMs进行测试，结果显示所得分数稳定且与外部能力测量相关。框架将基准测试重新表述为对抗性生成-评估游戏，人类作为最终裁决者

Conclusion: 该框架为解决"后理解机制"下的基准测试挑战提供了可行方案，即使人类无法完全理解任务，也能保持评估完整性，确保AI进展的可测量性

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [154] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC是一个结合形式化验证和可解释性的工具，用于分析和验证医疗领域（特别是脓毒症治疗）的强化学习策略，通过构建可达状态空间、临床标签和解释性分析来提高策略的安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，特别是脓毒症治疗优化中，强化学习策略通常是不透明且难以验证的。传统的概率模型检查器需要处理完整状态空间，对于大型MDP不可行，并且无法解释策略为何做出特定决策。这限制了RL策略在临床环境中的安全部署。

Method: COOL-MC基于Storm模型检查器，但增加了三个关键功能：1）仅构建训练策略诱导的可达状态空间，生成更小的离散时间马尔可夫链；2）自动用临床有意义的原子命题标记状态；3）将可解释性方法与概率计算树逻辑查询集成，揭示驱动决策的特征。在ICU-Sepsis MDP基准上进行了演示。

Result: 通过完整MDP验证建立了硬边界，训练了一个达到最优生存概率的安全RL策略，并通过PCTL验证和可解释性分析其行为。分析发现训练策略主要依赖先前的给药历史而非患者不断变化的状况，这是标准评估无法发现但被COOL-MC暴露的弱点。

Conclusion: COOL-MC展示了如何将形式化验证和可解释性结合，为临床医生在部署前调查和调试脓毒症治疗策略提供了工具。这种方法可以增强医疗决策系统的安全性和透明度，促进RL在临床环境中的可信应用。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [155] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: MATEO是一个评估大型视觉语言模型时序推理能力的多模态基准，专注于真实世界规划任务中的时序执行顺序理解


<details>
  <summary>Details</summary>
Motivation: 现有研究对基础模型时序执行顺序的理解有限，主要依赖自动生成的标注、线性链近似或纯文本输入，缺乏对多模态时序推理能力的系统评估

Method: 收集高质量专业多模态食谱语料库，通过标准化编辑流程将指令分解为离散步骤并配图；设计可扩展的众包标注流程收集时序执行顺序图；评估6个最先进的LVLM模型

Result: 评估了不同模型规模、语言上下文、多模态输入结构和微调策略下的6个先进LVLM模型在时序推理任务上的表现

Conclusion: MATEO基准填补了多模态时序推理评估的空白，为提升LVLM在真实世界规划任务中的时序执行顺序理解能力提供了重要工具

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [156] [Tabular Foundation Models Can Learn Association Rules](https://arxiv.org/abs/2602.14622)
*Erkan Karabulut,Daniel Daza,Paul Groth,Martijn C. Schut,Victoria Degeler*

Main category: cs.AI

TL;DR: TabProbe：基于表格基础模型的关联规则挖掘框架，无需频繁项集挖掘，在低数据场景下表现稳健


<details>
  <summary>Details</summary>
Motivation: 传统关联规则挖掘方法存在规则爆炸和可扩展性问题，而神经方法在低数据场景下性能下降。表格基础模型提供了解决这些限制的基础

Method: 提出模型无关的关联规则学习框架，可从任何条件概率模型中提取关联规则。具体实现TabProbe利用表格基础模型作为条件概率估计器，无需频繁项集挖掘

Result: 表格基础模型能持续产生简洁、高质量的关联规则，具有强大的预测性能，在低数据设置下保持稳健，无需任务特定训练

Conclusion: TabProbe框架成功利用表格基础模型进行关联规则挖掘，解决了传统方法的可扩展性和低数据性能问题，为知识发现提供了新途径

Abstract: Association Rule Mining (ARM) is a fundamental task for knowledge discovery in tabular data and is widely used in high-stakes decision-making. Classical ARM methods rely on frequent itemset mining, leading to rule explosion and poor scalability, while recent neural approaches mitigate these issues but suffer from degraded performance in low-data regimes. Tabular foundation models (TFMs), pretrained on diverse tabular data with strong in-context generalization, provide a basis for addressing these limitations. We introduce a model-agnostic association rule learning framework that extracts association rules from any conditional probabilistic model over tabular data, enabling us to leverage TFMs. We then introduce TabProbe, an instantiation of our framework that utilizes TFMs as conditional probability estimators to learn association rules out-of-the-box without frequent itemset mining. We evaluate our approach on tabular datasets of varying sizes based on standard ARM rule quality metrics and downstream classification performance. The results show that TFMs consistently produce concise, high-quality association rules with strong predictive performance and remain robust in low-data settings without task-specific training. Source code is available at https://github.com/DiTEC-project/tabprobe.

</details>


### [157] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 提出了首个同时包含模式层和事实层知识的数据集资源，支持知识图谱精化算法的全面评估


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱精化算法的评估数据集通常只包含事实层知识，缺乏模式层信息，限制了依赖丰富本体约束、推理或神经符号技术的评估

Method: 开发了一个工作流程，从知识图谱中提取同时包含模式和事实的数据集，处理不一致性，利用推理推导隐含知识，并支持OWL序列化和张量表示

Result: 创建了首个包含模式和事实的精选数据集套件，包括从表达性模式知识图谱提取的新数据集，并为现有数据集补充了模式信息

Conclusion: 该资源填补了知识图谱精化评估的空白，使依赖本体约束和推理的方法能够在更真实的大规模场景下进行评估

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [158] [Probabilistic Reachability Analysis of Multi-scale Voltage Dynamics Using Reinforcement Learning](https://arxiv.org/abs/2602.13896)
*Naoki Hashima,Hikaru Hoshino,Luis David Pabón Ospina,Eiko Furutani*

Main category: eess.SY

TL;DR: 本文提出了一种基于深度强化学习的概率可达性分析框架，用于评估多时间尺度电压动态的稳定性风险，能够同时量化多种失稳机制的概率。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统的电压稳定性涉及多个时间尺度的耦合动态。传统方法基于时间尺度分离或静态稳定裕度，可能忽略由慢速和快速暂态耦合引起的失稳。运行条件的不确定性进一步使稳定性评估复杂化，而蒙特卡洛模拟的高计算成本限制了其在多尺度动态分析中的应用。

Method: 提出基于深度强化学习的概率可达性分析框架，将每种失稳机制建模为不同的吸收状态，并引入多批评家架构进行机制特定的学习，使系统能够在统一框架内一致地学习与多种失稳类型相关的风险概率。

Result: 该方法在带有负载分接头变换器和过励磁限制器的四母线系统上进行了验证，展示了所提出的基于学习的可达性分析在识别和量化导致电压崩溃的机制方面的有效性。

Conclusion: 基于深度强化学习的概率可达性分析框架能够有效处理多时间尺度电压动态的稳定性评估，克服了传统方法的局限性，为电力系统稳定性分析提供了新的工具。

Abstract: Voltage stability in modern power systems involves coupled dynamics across multiple time scales. Conventional methods based on time-scale separation or static stability margins may overlook instabilities caused by the coupling of slow and fast transients. Uncertainty in operating conditions further complicates stability assessment, and high computational cost of Monte Carlo simulations limit its applicability to multi-scale dynamics. This paper presents a deep reinforcement learning-based framework for probabilistic reachability analysis of multi-scale voltage dynamics. By formulating each instability mechanism as a distinct absorbing state and introducing a multi-critic architecture for mechanism-specific learning, the proposed method enables consistent learning of risk probabilities associated with multiple instability types within a unified framework. The approach is demonstrated on a four-bus system with load tap changers and over-excitation limiters, illustrating effectiveness of the proposed learning-based reachability analysis in identifying and quantifying the mechanisms leading to voltage collapse.

</details>


### [159] [Learning-based data-enabled moving horizon estimation with application to membrane-based biological wastewater treatment process](https://arxiv.org/abs/2602.13957)
*Li Xiaojie,Yin Xunyuan*

Main category: eess.SY

TL;DR: 提出一种基于数据驱动的移动水平估计方法，用于非线性系统状态估计，无需显式Koopman建模


<details>
  <summary>Details</summary>
Motivation: 针对非线性系统的状态估计问题，传统方法需要显式系统模型。本文旨在开发一种数据驱动的方法，利用Koopman理论但无需显式建模，实现更灵活有效的状态估计

Method: 1. 从非线性系统的状态和输入数据中学习提升函数，将系统轨迹投影到提升空间；2. 在提升空间中，轨迹隐式描述原始非线性系统的Koopman表示；3. 开发凸数据驱动MHE公式，实时估计Koopman表示的状态；4. 从Koopman表示重构非线性系统状态；5. 推导确保估计误差稳定性的充分条件

Result: 1. 成功开发了数据驱动的MHE方法；2. 推导了估计误差稳定性的理论保证；3. 在膜基生物水处理过程中验证了方法的有效性

Conclusion: 提出的数据驱动MHE方法能够有效估计非线性系统状态，无需显式Koopman建模，具有理论稳定性和实际应用价值

Abstract: In this paper, we propose a data-enabled moving horizon estimation (MHE) approach for nonlinear systems. While the approach is formulated by leveraging Koopman theory, its implementation does not require explicit Koopman modeling. Lifting functions are learned from the state and input data of the original nonlinear system to project the system trajectories into the lifted space, where the resulting trajectories implicitly describe the Koopman representation for the original nonlinear system. A convex data-enabled MHE formulation is developed to provide real-time state estimates of the Koopman representation, from which the states of the nonlinear system can be reconstructed. Sufficient conditions are derived to ensure the stability of the estimation error. The effectiveness of the proposed method is illustrated using a membrane-based biological water treatment process.

</details>


### [160] [Simultaneous State Estimation and Online Model Learning in a Soft Robotic System](https://arxiv.org/abs/2602.14092)
*Jan-Hendrik Ewering,Max Bartholdt,Simon F. G. Ehlers,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: eess.SY

TL;DR: 提出一种基于边缘化粒子滤波的软机器人状态估计与刚度模型在线学习方法，仅需名义恒定曲率模型和基座反力测量，可同时估计机器人姿态和学习弯曲刚度模型


<details>
  <summary>Details</summary>
Motivation: 软机器人等复杂现实系统需要精确的预测控制，但实际中难以同时获得准确的状态和模型信息。现有方法难以同时在线估计未知状态并从顺序到达的测量中学习模型

Method: 采用灰箱系统辨识工具，结合边缘化粒子滤波，将名义恒定曲率模型与高斯过程弯曲刚度模型接口，仅依赖基座反力测量进行在线状态估计和模型学习

Result: 使用真实软机器人数据验证，方法能在线学习弯曲刚度模型并准确估计机器人姿态，多步前向预测误差降低表明学习的GP刚度模型提升了整体模型质量

Conclusion: 提出的边缘化粒子滤波方法能有效同时进行软机器人状态估计和刚度模型在线学习，相比随机游走刚度值估计，能预测弯曲刚度并改善模型质量

Abstract: Operating complex real-world systems, such as soft robots, can benefit from precise predictive control schemes that require accurate state and model knowledge. This knowledge is typically not available in practical settings and must be inferred from noisy measurements. In particular, it is challenging to simultaneously estimate unknown states and learn a model online from sequentially arriving measurements. In this paper, we show how a recently proposed gray-box system identification tool enables the estimation of a soft robot's current pose while at the same time learning a bending stiffness model. For estimation and learning, we rely solely on a nominal constant-curvature robot model and measurements of the robot's base reactions (e.g., base forces). The estimation scheme -- relying on a marginalized particle filter -- allows us to conveniently interface nominal constant-curvature equations with a Gaussian Process (GP) bending stiffness model to be learned. This, in contrast to estimation via a random walk over stiffness values, enables prediction of bending stiffness and improves overall model quality. We demonstrate, using real-world soft-robot data, that the method learns a bending stiffness model online while accurately estimating the robot's pose. Notably, reduced multi-step forward-prediction errors indicate that the learned bending-stiffness GP improves overall model quality.

</details>


### [161] [Prescribed-Performance-Aware Hybrid-Gain-Based Robust Controller](https://arxiv.org/abs/2602.14382)
*Amit Shivam,Kiran Kumari,Fernando A. C. C. Fontes*

Main category: eess.SY

TL;DR: 提出一种结合规定性能函数和混合增益的有限时间滑模控制框架，用于处理带有匹配扰动的非线性系统，能在有限时间内收敛并保证暂态性能。


<details>
  <summary>Details</summary>
Motivation: 针对非线性系统在匹配扰动下的控制问题，需要同时保证有限时间收敛、控制输入有界以及明确的暂态性能要求。现有方法往往难以同时满足这些需求。

Method: 采用混合增益结构确保控制输入有界并保持有限时间收敛，结合规定性能函数（PPF）来显式强制执行暂态性能要求。首先为一阶系统建立理论保证，然后扩展到二阶系统，通过PPF约束设计滑模流形来精确控制位置和速度的暂态响应。

Result: 仿真研究表明，相比调优良好的非PPF混合增益控制器，PPF感知方法能严格强制执行规定的暂态约束，在不增加峰值执行力的情况下，积分误差和控制能量指标持续降低约9-12%。

Conclusion: 提出的PPF感知混合增益有限时间滑模控制框架能有效处理匹配扰动，同时保证有限时间收敛、控制输入有界，并能精确控制暂态性能，在多种性能指标上均有显著改进。

Abstract: This paper proposes a prescribed performance function aware hybrid gain finite time sliding mode control framework for a class of nonlinear systems subject to matched disturbances. The hybrid gain structure ensures bounded control effort while retaining finite time convergence, and the incorporation of PPFs enables explicit enforcement of transient performance requirements. Theoretical guarantees are first established for first order systems, characterizing finite time convergence, disturbance rejection, and residual bounds. The approach is then extended to second order dynamics, where a sliding manifold is designed using PPF constraints to facilitate controlled shaping of position and velocity transients. Simulation studies illustrate the proposed design under matched peak control conditions. Comparative results for second-order systems demonstrate that, while a well tuned non-PPF hybrid gain controller achieves competitive tracking performance, the PPF-aware formulation strictly enforces prescribed transient constraints and yields consistent reductions of approximately 9 to 12 percent in integral error and control energy metrics without increasing peak actuation effort.

</details>


### [162] [Noncooperative Virtual Queue Coordination via Uncertainty-Aware Correlated Equilibria](https://arxiv.org/abs/2602.14436)
*Jaehan Im,David Fridovich-Keil,Ufuk Topcu*

Main category: eess.SY

TL;DR: 提出基于相关均衡的协作虚拟排队机制，通过概率保证的激励兼容推荐来缓解机场拥堵，同时保持航空公司自主权


<details>
  <summary>Details</summary>
Motivation: 现有协作虚拟排队机制中，中央协调员只能调节总体推出容量，无法控制具体航班推出决策，限制了系统性能优化能力

Method: 提出基于相关均衡概念的非合作协调机制，引入机会约束处理航空公司内部成本评估不确定性，开发可扩展算法计算机会约束相关均衡

Result: 方法可扩展到每小时210个航班，相比先到先服务方案减少约8.9%累积延误，揭示了置信水平、偏离鲁棒性和成本效率之间的权衡

Conclusion: 基于机会约束相关均衡的协作虚拟排队机制能在保持航空公司自主权的同时有效缓解机场拥堵，提供激励兼容的航班推出推荐

Abstract: Collaborative virtual queueing has been proposed as a mechanism to mitigate airport surface congestion while preserving airline autonomy over aircraft-level pushback decisions. A central coordinator can regulate aggregate pushback capacity but cannot directly control which specific aircraft are released, limiting its ability to steer system-level performance. We propose a noncooperative coordination mechanism for collaborative virtual queueing based on the correlated equilibrium concept, which enables the coordinator to provide incentive-compatible recommendations on aircraft-level pushback decisions without overriding airline autonomy. To account for uncertainty in airlines' internal cost assessments, we introduce chance constraints into the correlated equilibrium formulation. This formulation provides explicit probabilistic guarantees on incentive compatibility, allowing the coordinator to adjust the confidence level with which airlines are expected to follow the recommended actions. We further propose a scalable algorithm for computing chance-constrained correlated equilibria by exploiting a reduced-rank structure. Numerical experiments demonstrate that the proposed method scales to realistic traffic levels up to 210 eligible pushbacks per hour, reduces accumulated delay by up to approximately 8.9% compared to current first-come-first-served schemes, and reveals a trade-off between confidence level, deviation robustness, and achievable cost efficiency.

</details>


### [163] [Segment-Based Two-Loop Adaptive Iterative Learning Control for Spacecraft Position and Attitude Tracking](https://arxiv.org/abs/2602.14660)
*Fan Zhang,Deyuan Meng,Ying Tan*

Main category: eess.SY

TL;DR: 提出基于对偶数的分段式双环自适应迭代学习控制框架，用于解决刚性体接近操作中位置和姿态的协同跟踪问题，克服传统方法的控制输入无界和动力学耦合挑战。


<details>
  <summary>Details</summary>
Motivation: 航天器交会对接等刚性体接近操作需要在有限时间间隔内精确跟踪位置和姿态，这些操作常在不确定条件下重复进行，存在未知但可重复的参数和扰动。传统自适应迭代学习控制面临两个挑战：旋转和平移动力学耦合使位置和姿态的协调学习环设计复杂化；标准自适应ILC设计无法保证控制输入有界。

Method: 提出基于对偶数的分段式双环自适应迭代学习控制框架：1) 使用对偶数表示跟踪误差，将位置和姿态误差结合为单一数学对象进行统一控制设计；2) 采用两个通过对偶数交互的学习环；3) 引入分段式动态投影机制，确保参数估计和控制输入有界，无需先验不确定性知识。

Result: 数学分析和数值仿真表明，所提出的框架在未知但可重复的不确定性和强旋转-平移耦合条件下，显著提升了跟踪性能。

Conclusion: 该研究成功解决了刚性体接近操作中位置和姿态协同跟踪的关键问题，通过创新的对偶数表示和分段投影机制，克服了传统自适应迭代学习控制的局限性，为航天器交会对接等应用提供了有效的控制解决方案。

Abstract: Proximity operations of rigid bodies, such as spacecraft rendezvous and docking, require precise tracking of both position and attitude over finite time intervals. These operations are often repeated under uncertain conditions, with unknown but repeatable parameters and disturbances. Adaptive iterative learning control (ILC) is well suited to such tasks, as it can track desired trajectories while learning unknown, iteration-invariant signals or parameters. However, conventional adaptive ILC faces two challenges: (i) the coupling between rotational and translational dynamics complicates the design of the two coordinated learning loops for position and attitude, and (ii) standard adaptive ILC designs cannot guarantee bounded control inputs. To address these issues, we propose a dual-number-based, segment-based two-loop adaptive ILC framework for simultaneous high-precision position and attitude tracking. The framework employs two learning loops that interact through a dual-number representation of tracking errors, combining position and attitude errors into a single mathematical object for unified control design. A segment-based dynamic projection mechanism ensures that both parameter estimates and control inputs remain bounded without prior knowledge of uncertainties. Mathematical analysis and numerical simulations demonstrate that the proposed framework significantly enhances tracking performance under unknown but repeatable uncertainties and strong rotational-translational coupling.

</details>


### [164] [A Multi-Bound Robust Optimization Approach for Renewable-Based VPP Market Participation Considering Intra-Hourly Uncertainty Exposure](https://arxiv.org/abs/2602.14742)
*Hadi Nemati,Álvaro Ortega,Enrique Lobato,Luis Rouco*

Main category: eess.SY

TL;DR: 该论文提出了一种多边界鲁棒优化框架，用于可再生能源虚拟电厂在日内电力市场中的投标与调度决策，通过区分不同偏差水平的不确定性参数，减少保守性并提高利润。


<details>
  <summary>Details</summary>
Motivation: 随着全球电力市场从小时级向日内级投标转变，可再生能源参与者需要更新决策框架和加强不确定性管理，以充分利用新的市场潜力。特别是可再生能源虚拟电厂需要市场导向的调度方法来有效应对电价、可再生能源发电和需求消费等多种不确定性。

Method: 提出多边界鲁棒优化框架，同时捕捉多种不确定性，明确纳入日内变异性，并区分不确定参数的偏差水平（频繁、适度偏差和罕见、极端偏差）。该方法产生更少保守、更可实施的投标和调度决策。

Result: 仿真研究表明，与标准鲁棒优化相比，所提出的多边界方法使利润提高了24.9-49.2%。在小时与15分钟市场分辨率下，不同不确定性处理策略之间的归一化绝对差异为：日前交易能量18.0-34.2%，二级备用市场中向上备用28.7-65.6%，向下备用10.1-16.3%。

Conclusion: 多边界鲁棒优化框架能够为可再生能源虚拟电厂在日内电力市场中提供更优的投标和调度决策，显著提高其在能源和备用市场的盈利能力，特别是在市场分辨率从小时级转向日内级的情况下。

Abstract: With the ongoing transition of electricity markets worldwide from hourly to intra-hourly bidding, market participants--especially Renewable Energy Sources (RES)--gain improved opportunities to adjust energy and reserve schedules and to benefit from more accurate higher-resolution forecasts. However, this shift requires participants to update decision-making frameworks and to strengthen uncertainty management in order to fully exploit the new market potential. In particular, Renewable-Based Virtual Power Plants (RVPPs) aggregating dispatchable and non-dispatchable RES must account for these changes through market-oriented scheduling methods that efficiently address multiple uncertainties, including electricity prices, RES generation, and demand consumption. In this vein, this paper proposes a multi-bound robust optimization framework to simultaneously capture these uncertainties, explicitly incorporate intra-hourly variability, and differentiate the deviation levels (frequent, moderate deviations and rare, extreme ones) of uncertain parameters. The proposed approach yields less conservative and more implementable bidding and scheduling decisions, thus improving RVPP profitability in both energy and reserve markets. Simulation studies compare the proposed method with standard robust optimization and evaluate the operational, market-strategy, and economic impacts of quarter-hourly versus hourly market resolution. Results indicate that the normalized absolute differences, across different uncertainty-handling strategies, between hourly and 15-minute schedules are 18.0--34.2% for day-ahead traded energy, and 28.7--65.6% and 10.1--16.3% for upward and downward reserve traded in the secondary reserve market, respectively. Furthermore, relative to classic robust optimization, the proposed multi-bound approach increases profit by 24.9--49.2% across the considered strategies.

</details>


### [165] [Fault Detection in Electrical Distribution System using Autoencoders](https://arxiv.org/abs/2602.14939)
*Sidharthenee Nayak,Victor Sam Moses Babu,Chandrashekhar Narayan Bhende,Pratyush Chakraborty,Mayukha Pal*

Main category: eess.SY

TL;DR: 该论文提出了一种基于深度自编码器的异常检测方法，用于电力系统故障检测，通过卷积自编码器进行降维，在模拟和公开数据集上分别达到97.62%和99.92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 电力系统故障检测具有重要研究价值，但现有方法在实际应用中面临挑战。故障发生具有概率性，需要从概率角度处理决策任务。保护系统需要检测、分类和定位故障，但可靠训练数据稀缺。深度学习技术，特别是模式分类器的学习、泛化和并行处理能力，为智能故障检测提供了有前景的途径。

Method: 提出基于深度自编码器的异常检测方法，使用卷积自编码器进行降维。相比传统自编码器，卷积自编码器参数更少，训练时间更短。该方法利用深度学习模式分类器的强大能力进行故障检测。

Result: 在模拟数据集上达到97.62%的准确率，在公开数据集上达到99.92%的准确率。相比其他检测方法，表现出更优越的性能和准确性。

Conclusion: 基于深度自编码器的异常检测方法为电力系统故障检测提供了有效解决方案，特别是在数据稀缺的情况下。卷积自编码器的降维能力减少了参数数量和训练时间，同时保持了高准确率，证明了深度学习在电力系统故障检测中的实用价值。

Abstract: In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their practical application remains highly challenging. Given the probabilistic nature of fault occurrences and parameters, certain decision-making tasks could be approached from a probabilistic standpoint. Protective systems are tasked with the detection, classification, and localization of faulty voltage and current line magnitudes, culminating in the activation of circuit breakers to isolate the faulty line. An essential aspect of designing effective fault detection systems lies in obtaining reliable data for training and testing, which is often scarce. Leveraging deep learning techniques, particularly the powerful capabilities of pattern classifiers in learning, generalizing, and parallel processing, offers promising avenues for intelligent fault detection. To address this, our paper proposes an anomaly-based approach for fault detection in electrical power systems, employing deep autoencoders. Additionally, we utilize Convolutional Autoencoders (CAE) for dimensionality reduction, which, due to its fewer parameters, requires less training time compared to conventional autoencoders. The proposed method demonstrates superior performance and accuracy compared to alternative detection approaches by achieving an accuracy of 97.62% and 99.92% on simulated and publicly available datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [166] [Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models](https://arxiv.org/abs/2602.13264)
*Souradeep Chattopadhyay,Brendan Kennedy,Sai Munikoti,Soumik Sarkar,Karl Pazdernik*

Main category: cs.LG

TL;DR: 提出了一种基于方向性集中度不确定性（DCU）的新型不确定性量化框架，使用冯·米塞斯-费舍尔分布测量嵌入向量的几何分散度，无需任务特定启发式方法，在单模态和多模态任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型的不确定性量化方法大多依赖固定的启发式规则，难以跨任务和跨模态泛化。需要一种更灵活、通用的方法来提高生成模型的可信度和鲁棒性。

Method: 提出方向性集中度不确定性（DCU）框架，基于冯·米塞斯-费舍尔分布量化嵌入向量的集中度。该方法通过测量语言模型多个生成输出的连续嵌入向量的几何分散度来捕获不确定性，无需任务特定启发式。

Result: DCU在实验中达到或超过了语义熵等先前方法的校准水平，在更复杂的多模态任务中表现出良好的泛化能力。

Conclusion: DCU为不确定性量化提供了一个灵活有效的框架，具有在多模态和智能体框架中集成的广泛潜力，能够提高生成模型的可信度和鲁棒性。

Abstract: In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.

</details>


### [167] [BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents](https://arxiv.org/abs/2602.13345)
*Ethan Seefried,Ran Eldegaway,Sanjay Das,Nathaniel Blanchard,Tirthankar Ghosal*

Main category: cs.LG

TL;DR: Blueprint是一个针对大规模工程图纸档案的多模态检索系统，通过区域检测、OCR识别、标识符标准化和混合检索，显著提升了工程图纸的检索效果。


<details>
  <summary>Details</summary>
Motivation: 数十年的工程图纸和技术记录被锁定在遗留档案中，元数据不一致或缺失，导致检索困难且通常需要人工操作，需要自动化解决方案来改善工程档案的可访问性。

Method: 系统检测标准图纸区域，应用区域限制的VLM-based OCR，标准化标识符（如DWG、零件、设施），并通过轻量级区域级重排器融合词汇检索和密集检索。

Result: 在包含350个专家策划查询的5k文件基准测试中，Blueprint在Success@3上获得10.1%的绝对增益，在nDCG@3上获得18.9%的相对改进，优于最强的视觉语言基线。

Conclusion: Blueprint系统能够自动生成结构化元数据，支持跨设施搜索，为遗留工程档案提供了可复现的检索解决方案，并释放了所有查询、运行、注释和代码。

Abstract: Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.
  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.

</details>


### [168] [The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric](https://arxiv.org/abs/2602.13359)
*Hannes Kath,Thiago S. Gouvêa,Daniel Sonntag*

Main category: cs.LG

TL;DR: 本文提出了"加速因子"作为主动学习评估的新指标，用于量化查询方法相比随机采样所需的样本比例，并通过实证研究验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 主动学习研究主要关注查询方法开发，但缺乏合适的迭代过程评估指标。现有评估方法不足以准确衡量查询方法在迭代过程中的性能提升效果。

Method: 回顾八年主动学习评估文献，正式提出"加速因子"作为多迭代查询方法性能指标，使用四个不同领域数据集和七种不同类型查询方法进行实证评估。

Result: 实验结果证实了加速因子的理论假设，证明其能准确捕捉所需样本比例，并显示出在迭代过程中相比现有指标的优越稳定性。

Conclusion: 加速因子是主动学习评估的有效定量指标，能准确衡量查询方法相比随机采样的性能提升，为主动学习研究提供了更可靠的评估工具。

Abstract: Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.

</details>


### [169] [Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2602.13413)
*Yuchen Fang,James Demmel,Javad Lavaei*

Main category: cs.LG

TL;DR: 本文分析了随机预条件随机梯度下降(SPSGD)及其加速变体在重尾噪声下的最坏情况复杂度，发现归一化方法能保证收敛，而裁剪方法在最坏情况下可能失败，这解释了大规模模型训练中归一化优于裁剪的实证现象。


<details>
  <summary>Details</summary>
Motivation: 自适应方法如Adam、RMSProp和Shampoo在深度学习中被广泛使用，但这些方法在重尾噪声下的理论保证不足。需要理解随机预条件下不同稳定化技术（裁剪和归一化）的最坏情况性能差异。

Method: 开发了随机预条件随机梯度下降(SPSGD)及其加速变体的最坏情况复杂度理论。假设随机梯度噪声具有有限p阶矩(p∈(1,2])，分析T次迭代后的收敛性。开发了新的向量值Burkholder型不等式作为分析工具。

Result: 归一化方法在已知问题参数时收敛率为O(T^{-(p-1)/(3p-2)})，未知参数时为O(T^{-(p-1)/(2p)})，与归一化SGD的最优率匹配。而裁剪方法在最坏情况下可能因随机预条件器与梯度估计的统计依赖性而无法收敛。

Conclusion: 在随机预条件设置下，归一化比裁剪具有更好的最坏情况理论保证，这为大规模模型训练中归一化优于裁剪的实证偏好提供了理论解释。新开发的向量值Burkholder不等式具有独立的理论价值。

Abstract: We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\mathcal{O}(T^{-\frac{p-1}{3p-2}})$ when problem parameters are known, and $\mathcal{O}(T^{-\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.

</details>


### [170] [High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator](https://arxiv.org/abs/2602.13416)
*Haiwen Guan,Moein Darman,Dibyajyoti Chakraborty,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的降尺度框架，使用概率扩散生成模型将粗分辨率（~300km）的LUCIE气候模拟器输出降尺度到25km分辨率，以支持区域气候影响评估。


<details>
  <summary>Details</summary>
Motivation: 虽然LUCIE气候模拟器能够准确再现长期气候统计特征，但其原生分辨率（约300公里）不足以进行详细的区域影响评估，需要开发高分辨率的降尺度方法。

Method: 采用基于概率扩散的生成模型，结合条件采样和后验采样框架，将粗分辨率的LUCIE输出降尺度到25公里分辨率。模型使用2000-2009年约14,000个ERA5时间步长进行训练，并在2010-2020年的LUCIE预测上进行评估。

Result: 通过纬度平均RMSE、功率谱、概率密度函数和纬向风的第一经验正交函数等多种指标评估，该方法能够在保持LUCIE粗粒度动力学特征的同时，生成约28公里分辨率的精细尺度气候统计特征。

Conclusion: 提出的深度学习降尺度框架成功地将LUCIE气候模拟器的分辨率从300公里提升到25公里，为区域气候影响评估提供了高分辨率的工具，同时保持了原始模型的物理一致性。

Abstract: The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.

</details>


### [171] [Text Has Curvature](https://arxiv.org/abs/2602.13418)
*Karish Grover,Hanqing Zeng,Yinglong Xia,Christos Faloutsos,Geoffrey J. Gordon*

Main category: cs.LG

TL;DR: 该论文提出Texture，一种文本原生的离散曲率信号，证明语言具有内在曲率，并展示了其在长上下文推理和检索增强生成中的实用性。


<details>
  <summary>Details</summary>
Motivation: 语言建模越来越多地使用弯曲几何（如用于层次结构的双曲空间、用于组合结构的混合曲率流形），但一个基本科学问题仍未解决：曲率对文本本身意味着什么？需要找到一种不依赖于嵌入空间选择、真正属于语言本身的曲率定义。

Method: 提出Texture，一种文本原生的词级离散曲率信号。通过调和掩码词左右上下文信念的Schrödinger桥定义曲率场：正曲率表示上下文聚焦意义，负曲率表示上下文发散为竞争性延续。提供了理论和经验证明语言具有固有曲率。

Result: 建立了文本原生曲率范式，使曲率可测量且实用。在代表性任务中实例化Texture：通过曲率引导的压缩改进长上下文推理，通过曲率引导的路由改进检索增强生成。

Conclusion: 文本确实具有曲率，Texture作为一种通用测量和控制原语，实现了无需几何训练的几何方法，使曲率在语言处理中变得可测量和实用。

Abstract: Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.

</details>


### [172] [Comparing Classifiers: A Case Study Using PyCM](https://arxiv.org/abs/2602.13482)
*Sadra Sabouri,Alireza Zolanvari,Sepand Haghighi*

Main category: cs.LG

TL;DR: PyCM库教程：通过多维度评估框架深入分析多分类器性能，强调标准指标可能忽略细微性能差异


<details>
  <summary>Details</summary>
Motivation: 选择最优分类模型需要对模型性能有全面深入的理解，标准评估指标可能无法捕捉模型性能的细微差异和权衡

Method: 使用PyCM库进行多分类器的深度评估，通过两个不同案例场景展示评估指标选择如何影响模型效能解释

Result: 评估指标的选择会从根本上改变对模型效能的解释，多维度评估框架对于发现模型性能中微小但重要的差异至关重要

Conclusion: 需要采用多维度评估框架来全面理解分类模型性能，标准指标可能错过细微的性能权衡，PyCM库为此提供了有效工具

Abstract: Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.

</details>


### [173] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: 该研究挑战了传统电路分析假设每个任务存在单一稳定机制的观点，发现即使在固定任务中，电路也是提示特定的。作者改进了注意力因果通信方法，提出了ACC++，能够从单次前向传播中提取更清晰、低维的因果信号，并应用于多个模型中的间接宾语识别任务。


<details>
  <summary>Details</summary>
Motivation: 传统机制可解释性研究通常在任务层面识别电路，假设每个任务存在单一稳定机制。作者发现这种假设可能掩盖了电路结构的重要来源：即使在固定任务中，电路也是提示特定的。因此需要开发能够捕捉这种提示特定性的分析方法。

Method: 在注意力因果通信（ACC）基础上，提出了ACC++改进方法，能够从单次前向传播中提取更清晰、低维的注意力头内部因果信号，减少归因噪声。将ACC++应用于GPT-2、Pythia和Gemma 2模型中的间接宾语识别任务，分析不同提示模板下的电路变化。

Result: 研究发现任何模型中都不存在单一的间接宾语识别电路：不同提示模板会引发系统性的不同机制。尽管存在这种变化，但提示可以聚类为具有相似电路的提示家族，并为每个家族提出了代表性电路作为分析单元。开发了自动化可解释性流程，使用ACC++信号提取人类可解释特征并构建机制解释。

Conclusion: 该研究通过将分析单元从任务转移到提示，重新定义了电路作为有意义的研究对象，使得在存在提示特定机制的情况下能够进行可扩展的电路描述。这为机制可解释性研究提供了新的分析框架和方法论。

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [174] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: 该论文提出了raFLoRA方法，解决了联邦低秩适应中由于客户端异质性导致的秩崩溃问题，通过秩分区聚合提高了模型性能并保持了通信效率。


<details>
  <summary>Details</summary>
Motivation: 在实际联邦学习场景中，客户端在系统资源和数据分布上存在异质性，这促使了不同客户端使用不同的LoRA秩。作者发现了一个被忽视的现象：在异构FedLoRA中会出现秩崩溃，即全局更新的能量集中在最小共享秩上，导致性能下降和对秩配置的高敏感性。

Method: 提出了raFLoRA方法，这是一种秩分区聚合方法。该方法将本地更新分解为秩分区，然后根据每个分区的有效客户端贡献进行加权聚合。通过理论分析揭示了秩崩溃的根本原因：秩无关的聚合权重与秩相关的客户端贡献之间的不匹配。

Result: 在分类和推理任务上的广泛实验表明，raFLoRA能够防止秩崩溃，提高模型性能，并且在通信效率方面优于最先进的FedLoRA基线方法。

Conclusion: raFLoRA通过解决异构联邦低秩适应中的秩崩溃问题，提供了一种有效的解决方案，能够在保持通信效率的同时提高模型性能，为实际联邦学习场景中的异质性挑战提供了理论分析和实用方法。

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [175] [TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers](https://arxiv.org/abs/2602.13498)
*Peng Cheng,Jiucheng Zang,Qingnan Li,Liheng Ma,Yufei Cui,Yingxue Zhang,Boxing Chen,Ming Jian,Wen Tong*

Main category: cs.LG

TL;DR: TrasMuon优化器在Muon的基础上引入全局RMS校准和基于能量的信任区域裁剪，既保持了Muon的近等距几何特性，又解决了其对步长超参数敏感和易受高能量爆发影响的问题。


<details>
  <summary>Details</summary>
Motivation: Muon优化器使用牛顿-舒尔茨迭代正交化更新，在几何特性上优于Adam系列方法，但正交化过程丢弃了幅度信息，导致训练对步长超参数敏感且容易受到高能量爆发的影响。

Method: TrasMuon通过两种机制改进Muon：1) 全局RMS校准来重新引入自适应缩放，2) 基于相对能量比的信任区域裁剪来限制更新在稳定区域内，防止高能量异常值导致的失稳。

Result: 在视觉和语言模型上的实验表明，TrasMuon比基线方法收敛更快。在没有预热阶段的实验中，TrasMuon展现出更好的稳定性和鲁棒性。

Conclusion: TrasMuon成功解决了Muon优化器的稳定性问题，在保持其几何优势的同时，通过自适应缩放和信任区域机制实现了更稳定、更高效的优化性能。

Abstract: Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\textbf{T}rust \textbf{R}egion \textbf{A}daptive \textbf{S}caling \textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.

</details>


### [176] [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445)
*Hasi Hays*

Main category: cs.LG

TL;DR: 提出选择性同步注意力（SSA），基于耦合振子Kuramoto模型的稳态解，替代Transformer中的点积自注意力，实现自然稀疏性、统一位置语义编码和单次闭式计算。


<details>
  <summary>Details</summary>
Motivation: Transformer的自注意力机制存在二次计算复杂度问题，且缺乏生物学神经计算基础。需要一种更高效、更具生物学合理性的注意力机制。

Method: 将每个token表示为具有可学习自然频率和相位的振子，基于频率依赖耦合和相位锁定条件计算同步强度作为注意力权重，形成闭式运算符。

Result: SSA具有自然稀疏性（相位锁定阈值）、统一位置语义编码（自然频率谱）、单次闭式计算等优势，同步矩阵分析显示初始化时即产生非均匀、头多样化的耦合模式。

Conclusion: SSA为Transformer提供了一种基于振子同步理论的高效替代方案，具有更强的架构归纳偏置，可作为Transformer块的即插即用替代。

Abstract: The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.

</details>


### [177] [Singular Vectors of Attention Heads Align with Features](https://arxiv.org/abs/2602.13524)
*Gabriel Franco,Carson Loughridge,Mark Crovella*

Main category: cs.LG

TL;DR: 该研究探讨了语言模型中奇异向量与特征对齐的理论基础，证明了在特定条件下奇异向量可以可靠地识别特征表示，并提出了可操作的验证方法。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型可解释性研究中存在一个隐含假设：奇异向量在某些情况下可以推断特征表示，但缺乏理论依据。本研究旨在填补这一空白，探究奇异向量何时以及为何能与特征对齐。

Method: 1. 在特征可直接观察的模型中验证奇异向量与特征的对齐性；2. 理论分析对齐发生的条件；3. 提出稀疏注意力分解作为可操作的验证方法，并在真实模型中检验。

Result: 1. 在特征可观察的模型中，奇异向量确实与特征稳健对齐；2. 理论分析表明在一定条件下这种对齐是预期的；3. 真实模型中出现了与预测一致的稀疏注意力分解模式。

Conclusion: 奇异向量与特征的对齐可以作为语言模型中特征识别的可靠理论基础，为可解释性研究提供了理论支持和可操作的验证方法。

Abstract: Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.

</details>


### [178] [Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts](https://arxiv.org/abs/2602.14490)
*Buze Zhang,Jinkai Tao,Zilang Zeng,Neil He,Ali Maatouk,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: MoSLoRA：一种混合几何空间的参数高效微调框架，通过结合多种几何空间（如双曲、球面等）来学习更丰富的曲率感知表示，显著提升大语言模型在下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法主要在欧几里得空间中操作，限制了捕捉语言数据中复杂几何结构的能力。虽然双曲几何（适合层次数据）和球面流形（适合循环模式）等替代几何空间具有理论优势，但将表示强制放入单一流形类型最终会限制表达能力。

Method: 提出混合空间框架，同时利用多种几何空间学习更丰富的曲率感知表示。在此基础上开发MoSLoRA，将低秩适应扩展到异构几何专家，使模型能够根据输入上下文动态选择或组合适当的几何空间。为减少频繁流形切换的计算开销，设计了轻量级路由机制。

Result: 在多个基准测试中，MoSLoRA始终优于强基线方法，在MATH500上实现5.6%的改进，在MAWPS上实现15.9%的改进。还提供了曲率优化如何影响训练稳定性和模型性能的实证见解。

Conclusion: 混合空间框架通过同时利用多种几何空间，显著提升了参数高效微调的表达能力，为语言模型适应下游任务提供了更灵活和强大的方法。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

</details>


### [179] [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)
*Abdallah Aaraba,Soumaya Cherkaoui,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: QuaRK框架：将硬件现实的量子储层计算与核基读出方案结合，通过经典阴影层析高效测量k-局部可观测量，实现时间序列学习


<details>
  <summary>Details</summary>
Motivation: 量子储层计算为时间序列学习提供了有前景的途径，但现有研究缺乏高效可实现的量子储层架构和模型学习保证

Method: 提出QuaRK端到端框架：硬件现实的量子储层特征提取器 + 核基读出方案；使用经典阴影层析高效测量k-局部可观测量；结合显式正则化和快速优化的经典核方法

Result: 提供了依赖时间数据的泛化理论保证，将设计和资源选择与有限样本性能联系起来；通过合成beta混合时间序列任务验证了预测的插值和泛化行为

Conclusion: QuaRK框架为构建可靠的时间学习器提供了原则性指导，通过清晰的电路宽度、深度和测量预算等计算旋钮，同时保持核方法建模非线性时间泛函的灵活性

Abstract: Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.

</details>


### [180] [Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction](https://arxiv.org/abs/2602.13532)
*Nobutaka Ono*

Main category: cs.LG

TL;DR: 提出一种基于元素选择的快速降维算法，通过直接选择输入向量的子集实现无乘法降维，相比PCA等传统方法计算成本更低


<details>
  <summary>Details</summary>
Motivation: 降维是减少模型参数、缓解过拟合、加速训练推理的重要技术，但传统方法如PCA依赖矩阵乘法，在资源受限系统中乘法计算本身可能成为瓶颈。因此需要开发无乘法的降维方法。

Method: 提出元素选择降维方法，通过最小化线性回归的均方误差来评估候选子集。当缺乏明确目标时，使用输入自身作为重建目标。采用基于矩阵求逆引理的高效目标函数变化公式，通过交换式局部搜索迭代应用目标函数降低的交换操作。

Result: 在MNIST手写数字图像数据集上的实验证明了该方法的有效性，能够实现高效的无乘法降维。

Conclusion: 元素选择作为一种无乘法的降维形式，通过高效的交换式局部搜索算法，能够在资源受限系统中有效降低计算成本，同时保持降维效果。

Abstract: In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.

</details>


### [181] [Interpretable clustering via optimal multiway-split decision trees](https://arxiv.org/abs/2602.13586)
*Hayato Suzuki,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出基于最优多路分裂决策树的解释性聚类方法，将问题转化为0-1整数线性优化，提高可解性，同时保持聚类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法通常构建二叉决策树，需要求解混合整数非线性优化问题，计算成本高且易陷入次优解。同时，二叉决策树往往过深，难以解释。需要一种既能保持高准确性又具有良好可解释性的聚类方法。

Method: 提出基于最优多路分裂决策树的解释性聚类方法：1) 将问题重新表述为0-1整数线性优化问题，提高可解性；2) 集成一维K-means算法对连续变量进行离散化，实现灵活的数据驱动分支；3) 构建多路分裂决策树而非二叉决策树，获得更简洁的决策规则。

Result: 在公开真实数据集上的大量数值实验表明，该方法在聚类准确性和可解释性方面优于基线方法。能够生成具有简洁决策规则的多路分裂决策树，同时在各种评估指标上保持竞争力。

Conclusion: 该方法通过将聚类问题重新表述为0-1整数线性优化问题，结合一维K-means离散化技术，成功实现了高准确性和良好可解释性的平衡，为解释性聚类提供了有效解决方案。

Abstract: Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

</details>


### [182] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: 本文发现并验证了LLM推荐系统中基准数据泄露问题，即LLM在预训练或微调时接触并可能记忆基准数据集，导致性能指标虚高，无法反映真实模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推荐系统中的广泛应用，评估可靠性面临严峻挑战。本文旨在识别和研究一个被忽视的问题：基于LLM的推荐系统中的基准数据泄露现象，这种现象会导致性能指标虚高，误导对模型真实能力的评估。

Method: 通过模拟多种数据泄露场景，对基础模型进行持续预训练，使用包含领域内和领域外用户-物品交互数据的混合语料库，验证数据泄露对推荐性能的影响。

Result: 实验揭示了数据泄露的双重效应：当泄露数据与领域相关时，会导致显著但虚假的性能提升，误导性地夸大模型能力；而领域不相关的泄露通常会降低推荐准确性，凸显了这种污染的复杂性和条件依赖性。

Conclusion: 数据泄露是基于LLM推荐系统中一个关键且先前未被考虑的因素，可能影响模型的真实性能。研究强调了在评估LLM推荐系统时需要考虑数据泄露问题的重要性。

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [183] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: 提出了一种新的联合时间序列链定义，用于在中断时间序列或两个相关时间序列中发现意外的演化趋势，解决了现有方法只能处理单一连续时间序列的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列链定义仅考虑在单一连续时间序列中寻找链，容易错过中断时间序列或跨两个相关时间序列中的意外演化模式。需要一种能够处理中断和跨序列情况的新定义。

Method: 提出了联合时间序列链的新定义，专门设计用于在中断时间序列或两个相关时间序列中发现意外演化趋势。该定义重点解决时间序列中断或间隔引起的鲁棒性问题，并提出了有效的排序标准来识别最佳链。

Result: 通过广泛的实证评估表明，所提出的方法在定位异常演化模式方面优于现有的时间序列链工作。在英特尔的真实制造应用中进一步展示了该方法的实用性。

Conclusion: 联合时间序列链定义能够有效捕捉中断时间序列或跨相关时间序列中的演化模式，解决了现有方法的局限性，具有实际应用价值。

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [184] [Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation](https://arxiv.org/abs/2602.13651)
*Stefan Behfar,Richard Mortier*

Main category: cs.LG

TL;DR: 该论文提出了一种新的联邦学习公平性评估方法——累积效用均等，关注客户长期参与机会的公平性而非单轮训练的公平性，解决了间歇性参与客户被系统性低估的问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习公平性方法主要关注参与条件下的损失或准确率均等化，但现实中客户参与是间歇性、异质性且与数据特征或资源约束相关的。当参与本身不均衡时，这些方法会导致间歇性可用客户被系统性低估，即使每轮性能看起来公平。

Method: 提出累积效用均等原则，评估客户是否获得可比较的长期利益而非每轮训练的利益。引入可用性归一化累积效用，将不可避免的物理约束与可避免的算法偏差（来自调度和聚合）解耦。

Result: 在时间偏斜、非独立同分布的联邦基准测试中，该方法显著提高了长期表示均等性，同时保持了接近完美的性能。

Conclusion: 累积效用均等为联邦学习提供了一种更全面的公平性评估框架，特别适用于现实世界中客户参与不均衡的场景，能够更好地保护间歇性参与客户的利益。

Abstract: In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.

</details>


### [185] [Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling](https://arxiv.org/abs/2602.13659)
*Valery Parfenov,Grigoriy Evseev,Andrey Veprikov,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出基于策略学习的零阶优化框架，通过可学习的扰动方向采样分布降低梯度估计方差，改善大语言模型微调的内存效率


<details>
  <summary>Details</summary>
Motivation: 传统零阶方法虽然能避免反向传播节省内存，但存在高方差和对参数维度依赖性强的问题，限制了其在高维大语言模型微调中的应用

Method: 将扰动方向采样分布视为可学习策略，通过更新该策略来降低方向估计的方差，开发了实用算法并提供了理论分析

Result: 理论分析表明学习采样分布能提高梯度信息质量并放松收敛界对维度d的依赖，在LLM微调基准测试中显著优于标准零阶基线

Conclusion: 自适应方向采样是使零阶微调在大规模应用中可行的有前景途径，为资源受限环境下的LLM部署提供了新思路

Abstract: Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd

</details>


### [186] [Optimized Certainty Equivalent Risk-Controlling Prediction Sets](https://arxiv.org/abs/2602.13660)
*Jiayi Huang,Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文提出OCE-RCPS框架，为医学图像分割等高风险应用提供基于优化确定性等价风险度量的概率保证，超越传统风险控制预测集的局限性。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割等安全关键应用中，预测系统需要提供超越传统期望损失控制的可靠性保证。现有风险控制预测集（RCPS）虽然提供期望风险的概率保证，但无法捕捉尾部行为和最坏情况，而这些在高风险场景中至关重要。

Method: 提出优化确定性等价RCPS（OCE-RCPS）框架，利用置信上界识别满足用户指定风险容忍水平的预测集参数，提供包括条件风险价值（CVaR）和熵风险在内的广义OCE风险度量的高概率保证。

Result: 理论证明OCE-RCPS对误覆盖率和假阴性率等损失函数满足期望的概率约束。图像分割实验表明，OCE-RCPS在各种风险度量和可靠性配置下始终满足目标满足率，而OCE-CRC无法提供概率保证。

Conclusion: OCE-RCPS为高风险应用提供了更强大的可靠性保证框架，能够有效控制尾部风险和最坏情况，弥补了传统RCPS的不足。

Abstract: In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.

</details>


### [187] [Governing AI Forgetting: Auditing for Machine Unlearning Compliance](https://arxiv.org/abs/2602.14553)
*Qinqi Lin,Ningning Ding,Lingjie Duan,Jianwei Huang*

Main category: cs.LG

TL;DR: 本文提出了首个用于审计机器遗忘合规性的经济框架，结合认证遗忘理论与监管执行，通过博弈论模型分析审计者与运营者之间的策略互动，发现审计强度可能随删除请求增加而降低，且未披露审计虽具信息优势但成本效益较低。


<details>
  <summary>Details</summary>
Motivation: 尽管法律规定了被遗忘权，但AI运营者经常未能遵守数据删除请求。机器遗忘提供了从训练模型中移除个人数据影响的技术方案，但确保合规性仍然具有挑战性，因为机器遗忘的技术可行性与监管实施之间存在根本性差距。

Method: 1) 使用认证遗忘的假设检验解释来表征机器遗忘固有的验证不确定性，推导审计者的检测能力；2) 提出博弈论模型捕捉审计者与运营者之间的策略互动；3) 将复杂的双变量非线性固定点问题转化为可处理的单变量辅助问题，实现系统解耦并建立均衡存在性、唯一性和结构特性。

Result: 反直觉地发现，随着删除请求增加，审计者可以最优地降低检查强度，因为运营者削弱的遗忘能力使不合规更容易被检测。这与近期中国审计减少但删除请求增加的现象一致。此外，证明虽然未披露审计为审计者提供信息优势，但相对于披露审计反而降低了监管成本效益。

Conclusion: 本文提出的经济框架为机器遗忘合规审计提供了理论基础，揭示了审计强度与删除请求之间的反直觉关系，并比较了不同审计策略的成本效益，为监管实践提供了重要见解。

Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.

</details>


### [188] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: ALMo系统通过目标-限制阈值交互界面，帮助临床医生在宫颈癌HDR近距离放疗计划制定中高效平衡肿瘤覆盖与器官保护，减少计划时间并提高计划质量。


<details>
  <summary>Details</summary>
Motivation: 临床决策中需要同时跟踪多个竞争性指标（理想目标和严格限制阈值），在高维权衡中推断最优患者特定策略具有认知挑战且存在变异性。特别是在宫颈癌HDR近距离放疗中，需要严格管理辐射热点，同时平衡肿瘤覆盖与器官保护。

Method: 提出ALMo（目标-限制定义的多目标系统），这是一个交互式决策支持系统，通过新颖的优化框架最小化手动输入，实现参数自动设置，并允许临床医生通过直接操作直观的目标和限制值来导航帕累托前沿。

Result: 在25个临床病例的回顾性评估中，ALMo生成的计划质量始终达到或超过手动计划，65%的病例显示剂量学改进。系统显著提高效率，平均计划时间减少至约17分钟（传统方法为30-60分钟）。

Conclusion: ALMo在近距离放疗中得到验证，展示了一个通用的框架，可用于简化多标准临床决策中的交互过程。

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [189] [Advancing Analytic Class-Incremental Learning through Vision-Language Calibration](https://arxiv.org/abs/2602.13670)
*Binyu Zhao,Wei Zhang,Xingrui Yu,Zhaonian Zou,Ivor Tsang*

Main category: cs.LG

TL;DR: VILA提出了一种双分支框架，通过视觉-语言校准策略改进基于预训练模型的类增量学习，在保持分析学习效率的同时克服其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 基于预训练模型的类增量学习面临高效适应与长期稳定性之间的权衡。分析学习虽然能实现快速递归闭式更新，但常受累积误差和特征不兼容的影响。研究发现表示刚性是主要瓶颈。

Method: 提出VILA双分支框架，采用两级视觉-语言校准策略：在特征层面通过几何校准将可塑性任务适应特征与冻结的通用语义锚点融合；在决策层面利用跨模态先验纠正预测偏差。

Result: 在八个基准测试上的广泛实验表明，VILA始终表现出优越性能，特别是在细粒度和长序列场景中，同时保持了分析学习的极端效率。

Conclusion: VILA框架将高保真预测与分析学习的简洁性相协调，克服了分析学习的固有脆弱性，为类增量学习提供了高效稳定的解决方案。

Abstract: Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA

</details>


### [190] [On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling](https://arxiv.org/abs/2602.13684)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文研究了相关聚类问题的稀疏化-近似权衡，探索了在保持LP近似保证的前提下需要多少边信息。研究揭示了伪度量与一般加权实例之间的结构二分性，提出了核心集、切割平面求解器和稀疏化LP-PIVOT算法，并证明了信息需求的下界。


<details>
  <summary>Details</summary>
Motivation: 相关聚类（CC）作为基础的无监督学习原语，其最强的LP近似保证需要Θ(n³)三角不等式约束，在大规模场景下计算成本过高。本文旨在研究CC的稀疏化-近似权衡，探索在保持LP近似保证的前提下需要多少边信息。

Method: 1. 理论分析：证明聚类分歧类的VC维度为n-1，得到最优大小的加性ε-核心集；2. 算法设计：开发精确切割平面求解器，利用最多C(n,2)个活跃三角不等式；3. 提出稀疏化LP-PIVOT算法，通过三角不等式填充缺失的LP边际；4. 使用Yao极小极大原理证明下界。

Result: 1. 获得最优大小Õ(n/ε²)的加性ε-核心集；2. 实现精确切割平面求解；3. 稀疏化LP-PIVOT算法在观察Õ(n^{3/2})条边时达到10/3近似比（加上由可计算统计量Γ_w控制的加性项），且该阈值是最优的；4. 证明对于非伪度量实例，任何观察o(n)条随机边的算法都会有无界近似比。

Conclusion: 相关聚类问题的稀疏化-近似权衡存在结构二分性：伪度量实例可以通过稀疏化实现高效近似，而一般加权实例则需要更多边信息。伪度量条件不仅影响计算可处理性，也决定了CC对不完全信息的鲁棒性。研究为大规模相关聚类提供了理论指导和实用算法。

Abstract: Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\varepsilon$-coresets of optimal size $\tilde{O}(n/\varepsilon^2)$; that at most $\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\overlineΓ_w$) once $\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.

</details>


### [191] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: DeepFusion是一个可扩展的联邦MoE训练框架，通过联邦知识蒸馏融合异构设备上的LLM知识，解决资源受限设备无法承载大型MoE模型的问题，同时显著降低通信成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: MoE-based LLMs需要大量多样化训练数据，联邦学习可以利用私有数据但传统方法要求设备本地承载大型MoE模型，这对资源受限设备不现实。需要一种既能保护隐私又能适应设备资源限制的解决方案。

Method: 1) 设备独立配置和训练适合自身需求和硬件限制的本地LLM；2) 提出View-Aligned Attention模块，整合全局MoE模型的多阶段特征表示，构建与本地LLM对齐的预测视角；3) 通过联邦知识蒸馏融合异构设备知识，解决模型架构和预测行为异质性导致的视角不匹配问题。

Result: 在行业级MoE模型（Qwen-MoE和DeepSeek-MoE）和真实数据集（医疗和金融）上的实验表明：DeepFusion性能接近集中式MoE训练；相比关键联邦MoE基线，通信成本降低高达71%，token困惑度提升高达5.28%。

Conclusion: DeepFusion是首个可扩展的联邦MoE训练框架，通过创新的视角对齐注意力机制有效解决了异构设备间的知识融合问题，在保护隐私的同时实现了高性能的MoE模型训练。

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [192] [Attention Head Entropy of LLMs Predicts Answer Correctness](https://arxiv.org/abs/2602.13699)
*Sophie Ostmeier,Brian Axelrod,Maya Varma,Asad Aali,Yabin Zhang,Magdalini Paschali,Sanmi Koyejo,Curtis Langlotz,Akshay Chaudhari*

Main category: cs.LG

TL;DR: 论文提出Head Entropy方法，通过分析注意力熵模式预测LLM答案正确性，在分布内和跨域任务中均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM经常生成看似合理但实际错误的答案，在医疗等安全关键领域存在风险。人工评估成本高，而基于LLM的评估方法可能引入隐藏错误。现有白盒方法主要检测上下文幻觉，但两个问题仍未解决：这些方法能否扩展到预测答案正确性？能否在跨域场景中泛化？

Method: 提出Head Entropy方法，通过测量注意力质量的分布来预测答案正确性。具体使用稀疏逻辑回归对每个注意力头的2-Renyi熵进行分析，捕捉注意力熵模式。

Result: Head Entropy在分布内任务中匹配或超越基线方法，在跨域任务中泛化能力显著更好，平均AUROC比最接近的基线高+8.5%。进一步发现，仅基于问题/上下文的注意力模式（在答案生成前）已具有预测信号，平均AUROC比最接近的基线高+17.7%。

Conclusion: Head Entropy是一种有效的白盒方法，能够可靠预测LLM答案正确性，在分布内和跨域场景中均表现优异，为安全关键应用提供了有前景的解决方案。

Abstract: Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

</details>


### [193] [Optimal Regret for Policy Optimization in Contextual Bandits](https://arxiv.org/abs/2602.13700)
*Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文提出了首个针对具有一般离线函数逼近的随机上下文多臂老虎机问题的高概率最优遗憾界，算法高效且达到最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决上下文多臂老虎机问题中理论界与实践应用之间的差距，证明广泛使用的策略优化方法能够达到严格证明的最优遗憾界。

Method: 采用策略优化技术，结合一般离线函数逼近方法，设计高效算法处理随机上下文多臂老虎机问题。

Result: 算法达到最优遗憾界 $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$，其中K为轮数，$\mathcal{A}$为臂集合，$\mathcal{F}$为函数类，并通过实验验证了理论结果。

Conclusion: 本文成功桥接了上下文老虎机问题中理论与实践的差距，证明策略优化方法能够达到严格证明的最优遗憾界，为实际应用提供了理论保障。

Abstract: We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$, where $K$ is the number of rounds, $\mathcal{A}$ is the set of arms, and $\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.

</details>


### [194] [Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation](https://arxiv.org/abs/2602.13706)
*Orin Levy,Aviv Rosenberg,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: OPO-CMDP是首个用于随机上下文马尔可夫决策过程（CMDPs）的离线函数近似策略优化算法，实现了最优的|S|和|A|依赖性的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 当前CMDPs在离线函数近似下的策略优化算法存在理论局限性，特别是在状态空间和动作空间依赖性方面不够优化，需要提出更高效的算法。

Method: 采用乐观策略优化方法，通过有限函数类近似损失和动态，实现高概率遗憾界分析。

Result: 获得了$\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)})$的遗憾界，在|S|和|A|依赖性上达到最优，直接改进了当前最先进方法。

Conclusion: 乐观策略优化为求解CMDPs提供了一条自然、计算效率高且理论接近最优的路径。

Abstract: We introduce \texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\mathcal{F}, \mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.

</details>


### [195] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: 该论文提出了"流体智能体环境"框架，允许智能体动态创建其他智能体，解决了传统多智能体强化学习中智能体数量固定不变的限制。


<details>
  <summary>Details</summary>
Motivation: 现实世界中智能体数量通常不固定且未知，智能体可以创建其他智能体（如细胞分裂、公司分拆部门），而传统多智能体强化学习只研究固定数量智能体的交互。

Method: 提出流体智能体环境框架，引入博弈论解决方案概念，并在该框架下实证评估多种MARL算法，包括Predator-Prey和Level-Based Foraging的流体变体，以及新设计的环境。

Result: 实验表明该框架能够产生根据环境需求动态调整规模的智能体团队，流体性能够解锁固定群体设置中无法观察到的新颖解决策略。

Conclusion: 流体智能体框架扩展了多智能体强化学习的应用范围，使智能体能够更灵活地适应现实世界中动态变化的群体规模，为智能体自主创建机制提供了理论基础和实证验证。

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [196] [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)
*Xin Yan,Zhenglin Wan,Feiyang Ye,Xingrui Yu,Hangyu Du,Yang You,Ivor Tsang*

Main category: cs.LG

TL;DR: HBVLA是一种针对视觉-语言-动作模型的1比特量化框架，通过策略感知增强Hessian识别关键权重，使用稀疏正交变换处理非显著权重，在Harr域进行分组1比特量化，显著减少量化误差，在资源受限平台上实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型虽然能实现指令跟随的具身控制，但其巨大的计算和内存需求阻碍了在资源受限机器人和边缘平台上的部署。现有1比特量化方法无法缩小二值化与全精度权重之间的分布差距，导致在长时程闭环执行中量化误差累积，严重降低动作质量。

Method: HBVLA框架包含三个关键步骤：1）使用策略感知增强Hessian识别对动作生成真正关键的权重；2）对非显著权重使用稀疏正交变换诱导低熵中间状态；3）在Harr域对显著和非显著权重进行分组1比特量化。

Result: 在LIBERO基准上，量化的OpenVLA-OFT保留了92.2%的全精度性能；在SimplerEnv上，量化的CogAct保留了93.6%的性能，显著优于最先进的二值化方法。真实世界评估显示HBVLA仅带来边际成功率下降，证明了在严格硬件约束下的鲁棒部署能力。

Conclusion: HBVLA为VLA模型的超低位量化提供了实用基础，能够在硬件受限的机器人平台上实现更可靠的部署，填补了现有量化方法在缩小二值化与全精度权重分布差距方面的空白。

Abstract: Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.

</details>


### [197] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: Atomix为LLM智能体工具调用提供进度感知的事务语义，通过纪元标记、资源前沿跟踪和进度谓词控制提交，支持可缓冲效果的延迟执行和外部化效果的补偿回滚，提升任务成功率并增强隔离性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体越来越多地在外部系统上执行操作，但工具效果是立即生效的。在故障、推测执行或资源竞争情况下，失败的分支可能会泄漏意外的副作用，且没有安全的回滚机制。需要一种能够提供事务性保证的运行时系统。

Method: Atomix运行时为智能体工具调用提供进度感知的事务语义：1）为每个调用标记纪元；2）跟踪每个资源的前沿状态；3）仅当进度谓词指示安全时才提交；4）可缓冲的效果可以延迟执行；5）外部化的效果在回滚时进行补偿。

Result: 在实际工作负载中进行故障注入测试：1）事务性重试提高了任务成功率；2）前沿门控提交在推测执行和资源竞争情况下增强了隔离性。

Conclusion: Atomix通过提供进度感知的事务语义，解决了LLM智能体在外部系统操作中的副作用泄漏问题，提高了系统的可靠性和隔离性，为智能体在复杂环境中的安全执行提供了保障。

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [198] [Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition](https://arxiv.org/abs/2602.13759)
*ZhiMing Li,JiaHe Feng*

Main category: cs.LG

TL;DR: 本文提出了一种矩阵自由特征分解方法，在矩阵向量乘积（MVP）预言机下，通过引入离散双括号流来消除各向同性噪声的影响，实现了仅依赖于迹自由协方差的稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 传统随机逼近方法在矩阵特征分解中存在两个问题：固定步长方法将稳定性与协方差矩阵的谱范数耦合，而自适应步长方法因更新消失而减慢收敛。需要一种能够消除各向同性噪声影响、仅依赖于迹自由协方差矩阵的方法。

Method: 引入离散双括号流，其生成器对各项同性偏移具有不变性，从而在离散时间层面上实现了对σ_k^2I项的路径不变性。该方法使用最大稳定步长η_max ∝ 1/||C_e||_2^2，仅依赖于迹自由协方差C_e。

Result: 建立了基于严格鞍点几何的全局收敛性和输入到状态稳定性分析，样本复杂度为O(||C_e||_2^2/(Δ^2ε))。通过显式表征退化块，获得了加速的O(log(1/ζ))鞍点逃逸率和高概率有限时间收敛保证。

Conclusion: 提出的离散双括号流方法能够有效消除各向同性噪声的影响，仅依赖于迹自由协方差，在矩阵自由特征分解中实现了稳定且高效的收敛性能。

Abstract: We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\|C_k\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \propto 1/\|C_e\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\|C_e\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.

</details>


### [199] [On Representation Redundancy in Large-Scale Instruction Tuning Data Selection](https://arxiv.org/abs/2602.13773)
*Youwei Shu,Shaomian Zheng,Dingnan Jin,Wenjie Qu,Ziyao Guo,Qing Cui,Jun Zhou,Jiaheng Zhang*

Main category: cs.LG

TL;DR: CRDS框架通过压缩语义表示减少冗余，提升指令调优数据选择质量，仅用3.5%数据即可超越全数据基线


<details>
  <summary>Details</summary>
Motivation: 现有LLM编码器产生的语义嵌入高度冗余，而工业级指令调优数据选择方法仍缺乏系统性研究，需要解决表示冗余问题来提升数据选择质量

Method: 提出CRDS框架，包含两种变体：CRDS-R使用Rademacher随机投影并拼接transformer隐藏层表示；CRDS-W采用白化降维技术提升表示质量

Result: 两种变体均显著提升数据质量，超越现有基于表示的选择方法。CRDS-W仅用3.5%数据就在四个数据集上平均超越全数据基线0.71%

Conclusion: 通过压缩语义表示减少冗余能有效提升指令调优数据选择效果，CRDS框架为工业级数据选择提供了实用解决方案

Abstract: Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.

</details>


### [200] [MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2602.13783)
*Xiaoyun Yu,Li fan,Xiangfei Qiu,Nanqing Dong,Yonggui Huang,Honggang Qi,Geguang Pu,Wanli Ouyang,Xi Chen,Jilin Hu*

Main category: cs.LG

TL;DR: MEMTS提出了一种轻量级即插即用的检索自由域适应方法，通过知识持久化模块将领域特定时间动态内化为可学习的潜在原型，解决了时间序列基础模型在垂直领域部署时的分布偏移和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在通用预测中表现出色，但在实际垂直领域部署时，由于时间分布偏移和领域特定周期性结构，性能显著下降。现有解决方案（领域自适应预训练和检索增强生成）存在灾难性遗忘或检索开销大的问题，无法满足实时流处理的高效要求。

Method: 提出MEMTS方法，核心是知识持久化模块，将领域特定的时间动态（如周期性模式和趋势）内化为紧凑的可学习潜在原型集合，将碎片化的历史观测转化为连续、参数化的知识表示。该方法无需修改冻结的时间序列基础模型架构。

Result: 在多个数据集上的广泛实验表明，MEMTS实现了最先进的性能，能够以恒定时间推理和接近零延迟实现准确的域适应，同时有效缓解对通用时间模式的灾难性遗忘。

Conclusion: MEMTS通过知识持久化模块实现了检索自由的域适应，解决了现有方法的可扩展性瓶颈，为时间序列基础模型在垂直领域的实际部署提供了高效解决方案。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.

</details>


### [201] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1将时间序列预测重新定义为顺序决策问题，通过基于记忆的状态管理机制和工具增强的智能体工作流实现迭代预测优化。


<details>
  <summary>Details</summary>
Motivation: 传统模型中心的时间序列预测方法通常将预测视为从历史观测到未来值的单次映射，难以处理复杂和动态变化的环境，缺乏自主获取信息、推理未来变化或通过迭代决策过程修正预测的能力。

Method: 提出Cast-R1框架，将预测重新定义为顺序决策问题。引入基于记忆的状态管理机制，维护跨交互步骤的决策相关信息；采用工具增强的智能体工作流，自主与模块化工具包交互，提取统计特征、调用轻量级预测模型进行决策支持、执行基于推理的预测，并通过自我反思迭代优化预测。

Result: 在多个真实世界时间序列数据集上的广泛实验证明了Cast-R1的有效性。

Conclusion: 这项工作为时间序列建模的智能体范式探索提供了实际步骤，展示了将预测重构为顺序决策问题的潜力。

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [202] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: MVP是一种新的生成式策略函数，通过建模平均速度场实现最快的一步动作生成，在保持高表达能力的同时大幅提升训练和推理速度


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的策略在表达能力与计算负担之间存在权衡，通常需要通过流步骤数量来控制。需要一种既能保持高表达能力又能实现快速动作生成的策略函数

Method: 提出平均速度策略（MVP），建模平均速度场实现一步动作生成。引入瞬时速度约束（IVC）确保高表达能力，该约束作为关键边界条件提升学习准确性和策略表达能力

Result: 在Robomimic和OGBench的多个挑战性机器人操作任务中达到最先进的成功率。相比现有基于流的策略基线，在训练和推理速度上都有显著提升

Conclusion: MVP通过建模平均速度场和引入瞬时速度约束，成功解决了基于流策略在表达能力与计算效率之间的权衡问题，实现了快速且高表达能力的策略学习

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [203] [Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference](https://arxiv.org/abs/2602.13813)
*Jorge Carrasco-Pollo,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: Pawsterior是一个变分流匹配框架，用于改进和扩展基于模拟的推理，特别针对具有结构化域约束的后验分布问题。


<details>
  <summary>Details</summary>
Motivation: 许多SBI问题涉及受结构化域约束的后验分布，如有界物理参数或混合离散-连续变量，但标准流匹配方法通常在无约束空间中操作，导致学习效率低下且难以满足物理约束。

Method: 提出端点诱导的仿射几何约束原理，通过双面变分模型将域几何直接纳入推理过程；开发变分参数化方法，支持涉及离散潜结构的SBI任务。

Result: 在标准SBI基准测试中表现出改进的分类器双样本测试性能，提高了采样时的数值稳定性，并实现了更好的后验保真度；能够处理传统流匹配方法无法处理的离散潜结构问题。

Conclusion: Pawsterior通过同时解决几何约束和离散潜结构问题，将流匹配方法扩展到更广泛的结构化SBI问题类别，这些问题是以前无法处理的。

Abstract: We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.

</details>


### [204] [Testing For Distribution Shifts with Conditional Conformal Test Martingales](https://arxiv.org/abs/2602.13848)
*Shalev Shaer,Yarin Bar,Drew Prinster,Yaniv Romano*

Main category: cs.LG

TL;DR: 提出一种基于固定参考集的序列测试方法，用于检测任意分布偏移，避免传统方法中的测试污染问题


<details>
  <summary>Details</summary>
Motivation: 现有conformal test martingales (CTMs)方法在检测分布偏移时存在测试污染问题：偏移后的样本会进入参考集，稀释偏移证据，增加检测延迟并降低检测能力

Method: 提出一种基于固定参考集的序列测试方法，通过比较新样本与固定零假设参考数据集来避免污染。主要技术贡献是构建鲁棒的马丁格尔，通过显式考虑有限参考集引起的参考分布估计误差，确保在给定参考数据条件下的有效性

Result: 方法具有任意时间有效的类型I错误控制，同时保证渐近功效为1和有界的期望检测延迟。实证表明，该方法比标准CTMs检测偏移更快

Conclusion: 该方法提供了一种强大可靠的分布偏移检测器，通过避免测试污染实现了更快的检测速度和更好的性能

Abstract: We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.
  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.

</details>


### [205] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: sleep2vec是一个用于处理多样化且不完整夜间生物信号的基础模型，通过跨模态对齐学习共享表示，在睡眠分期和临床结果评估中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统睡眠监测设备（如PSG、床边监测仪、可穿戴设备）捕获多种夜间生物信号，但设备异质性和传感器频繁丢失给多模态信号的统一建模带来挑战

Method: 提出sleep2vec基础模型，通过跨模态对齐学习共享表示。使用包含人口统计学、年龄、地点和历史信息的InfoNCE目标进行对比预训练，在42,249个夜间记录上训练，涵盖9种模态

Result: sleep2vec在下游睡眠分期和临床结果评估中始终优于强基线，对任何可用模态子集和传感器丢失保持鲁棒性。首次描述了夜间生物信号在模态多样性和模型容量方面的缩放规律

Conclusion: 统一的跨模态对齐结合原则性缩放，能够实现真实世界夜间生物信号的标签高效、通用建模

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [206] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: 论文提出基于信息结构的五级可学习性层次，解释代码生成比强化学习更可靠的原因在于代码提供密集、局部、可验证的反馈，而可学习性天花板更多取决于任务本身而非模型规模。


<details>
  <summary>Details</summary>
Motivation: 解释为什么代码生成比强化学习进展更可靠，探索任务可学习性的根本差异，挑战"仅靠扩展就能解决剩余ML挑战"的常见假设。

Method: 提出基于信息结构的五级可学习性层次，建立计算问题的三个属性（可表达性、可计算性、可学习性）之间的形式化区分和关系，提供统一模板使结构差异显式化。

Result: 分析表明代码的监督学习可预测扩展而强化学习不行，因为代码提供密集、局部、可验证的逐令牌反馈，而强化学习问题的反馈质量较低。

Conclusion: ML进展的天花板更多取决于任务是否可学习，而非模型规模；可学习性差异是渐进的而非二元的；对"仅靠扩展就能解决ML挑战"的假设需要重新审视。

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [207] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML是一个基于多智能体的AutoML框架，通过代码引导的模块化架构解决传统AutoML黑盒问题和LLM智能体的幻觉问题，在真实Kaggle竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML框架缺乏灵活性和透明度，而基于LLM的智能体存在逻辑幻觉和逻辑纠缠问题，导致运行时故障难以恢复，需要更可靠、可验证的AutoML架构。

Method: iML采用三核心设计：1) 代码引导规划 - 基于自主经验分析制定战略蓝图消除幻觉；2) 代码模块化实现 - 将预处理和建模解耦为专业组件，受严格接口契约约束；3) 代码可验证集成 - 通过动态契约验证和迭代自修正确保物理可行性。

Result: 在MLE-BENCH上达到85%有效提交率和45%竞争奖牌率，APS为0.77；在iML-BENCH上比其他方法提升38%-163%的APS；即使在简化任务描述下仍保持70%成功率。

Conclusion: iML通过代码引导的模块化架构成功弥合了随机生成与可靠工程之间的差距，为实现真正的AutoML迈出了重要一步。

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [208] [An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations](https://arxiv.org/abs/2602.13939)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: AHSIV是一个自适应混合选择框架，用于解决间歇性和高变异性需求环境中的模型选择问题，通过考虑预测时间范围、需求结构和多目标优化来提高模型选择的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在结构性需求间歇性、高变异性和多步规划的业务环境中，没有单一的预测模型能始终表现最佳。模型排名会因误差指标、需求机制和预测时间范围的不同而变化，导致在多SKU决策环境中产生模糊性。

Method: 提出AHSIV框架，整合了：1) 通过MDFH程序调整的缩放和绝对误差指标；2) 结构性需求分类；3) 多目标帕累托优势；4) 分层偏差细化。该框架具有时间范围感知和机制条件化的特点。

Result: 在Walmart、M3、M4和M5数据集上的评估显示，AHSIV在聚合性能上与最强的单指标基线统计等价，同时提高了针对特定时间范围的最佳模型选择频率。

Conclusion: 异质需求环境中的模型选择不能被视为静态排名问题，具有时间范围一致性和结构自适应性的机制为多SKU预测提供了原则性和操作连贯的解决方案。

Abstract: Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.

</details>


### [209] [You Can Learn Tokenization End-to-End with Reinforcement Learning](https://arxiv.org/abs/2602.13940)
*Sam Dauncey,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文提出使用分数函数估计来学习LLM中的分词边界，相比之前的直通估计方法，该方法有更严格的理论保证，并在1亿参数规模上表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM架构越来越端到端，但分词作为硬编码的压缩步骤仍然保留在训练流程中。先前工作尝试用启发式方法或直通估计学习分词边界，但存在理论保证不足的问题。

Method: 使用分数函数估计直接优化离散分词边界问题以最小化损失，并引入强化学习中的时间折扣技术来降低方差，使方法具有实际可行性。

Result: 该方法在1亿参数规模上，在定性和定量评估中都优于先前提出的直通估计方法。

Conclusion: 通过分数函数估计学习分词边界是可行的，且优于现有的直通估计方法，为LLM中更端到端的训练提供了新方向。

Abstract: Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.

</details>


### [210] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: 论文提出Experiential Reinforcement Learning (ERL)，一种在强化学习过程中嵌入经验-反思-巩固循环的训练范式，通过让模型生成初始尝试、接收反馈、产生反思来指导改进的第二次尝试，从而将稀疏延迟反馈转化为结构化行为修正。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为语言模型从环境奖励或反馈中学习的主要方法，但实践中环境反馈通常是稀疏和延迟的。从这种信号中学习具有挑战性，因为语言模型必须隐式推断观察到的失败应如何转化为未来迭代的行为改变。

Method: 引入Experiential Reinforcement Learning (ERL)训练范式，包含明确的经验-反思-巩固循环：模型生成初始尝试 → 接收环境反馈 → 产生反思 → 指导改进的第二次尝试 → 将成功结果强化并内化到基础策略中。

Result: 在稀疏奖励控制环境和智能推理基准测试中，ERL相比强基线方法持续提高了学习效率和最终性能：在复杂多步环境中获得高达+81%的增益，在使用工具的推理任务中获得高达+11%的增益。

Conclusion: 将明确的自我反思整合到策略训练中，为将反馈转化为持久的行为改进提供了实用机制，改善了探索、稳定了优化，并在部署时无需额外推理成本即可保持增益。

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [211] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: 提出QuRL方法，使用量化actor加速RL训练中的rollout过程，解决训练崩溃和权重更新问题，实现20-80%的加速效果。


<details>
  <summary>Details</summary>
Motivation: 在强化学习与可验证奖励（RLVR）训练推理大语言模型时，由于LLMs的自回归解码特性，rollout过程成为训练效率瓶颈，占用高达70%的总训练时间。

Method: 提出量化强化学习（QuRL），使用量化actor加速rollout。解决两个关键挑战：1）提出自适应裁剪范围（ACR），基于全精度actor和量化actor之间的策略比率动态调整裁剪比例，防止长期训练崩溃；2）通过不变缩放技术解决权重更新问题，减少量化噪声并增加权重更新。

Result: 在DeepScaleR和DAPO上进行了INT8和FP8量化实验，实现了20%到80%的rollout加速效果。

Conclusion: QuRL方法通过量化actor有效加速了RL训练中的rollout过程，解决了量化RL中的训练稳定性和权重更新问题，显著提升了训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [212] [Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds](https://arxiv.org/abs/2602.13960)
*Zedong Wang,Yuyang Wang,Ijay Narang,Felix Wang,Yuzhou Wang,Siva Theja Maguluri*

Main category: cs.LG

TL;DR: 本文为固定步长随机逼近算法的稳态分布提供了非渐近误差界，建立了稳态分布与高斯极限之间的Wasserstein距离和尾部概率的显式误差界。


<details>
  <summary>Details</summary>
Motivation: 固定步长随机逼近算法在计算效率方面应用广泛，但其稳态分布通常难以解析。现有研究只提供了步长趋于0时的渐近高斯极限，对于固定步长缺乏可用的误差界，这限制了实际应用中的精度分析。

Method: 首先证明了一般性定理，在漂移正则性和噪声矩条件下，建立稳态分布与高斯分布之间的Wasserstein距离界。方法覆盖了i.i.d.和马尔可夫噪声模型。然后将这些定理具体应用于三个代表性SA设置：SGD（光滑强凸目标）、线性SA和压缩非线性SA。

Result: 获得了维度依赖和步长依赖的显式Wasserstein距离界，阶数为α^{1/2}log(1/α)。进一步推导了非均匀Berry-Esseen型尾部界，误差项在偏离水平和步长α上都衰减。对于非强凸SGD，识别了非高斯（吉布斯）极限律。

Conclusion: 本文为固定步长随机逼近算法的稳态分布提供了首个非渐近误差界，建立了稳态分布与高斯极限之间的显式近似误差，扩展了理论分析工具，为实际应用中的精度控制提供了理论依据。

Abstract: Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.

</details>


### [213] [KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra](https://arxiv.org/abs/2602.14011)
*Liangyu Su,Jun Shu,Rui Liu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: 提出KoopGen框架，通过基于生成器的神经Koopman方法建模高维时空混沌系统，利用Cartesian分解分离保守传输和不可逆耗散，提高预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型在宽带或连续谱主导的混沌系统中缺乏稳定性、可解释性和可扩展性，而传统Koopman方法依赖有限维假设或显式谱参数化，在高维场景下性能下降。

Method: 引入KoopGen框架，通过结构化、状态依赖的Koopman生成器表示建模动力学，利用Cartesian分解将算子分解为斜伴随和自伴随分量，分离保守传输和不可逆耗散，并在学习中强制精确的算子理论约束。

Result: 在非线性振荡器、高维混沌和时空动力学等多种系统中，KoopGen提高了预测精度和稳定性，同时阐明了连续谱动力学的哪些分量允许可解释和可学习的表示。

Conclusion: KoopGen为高维时空混沌系统提供了一种有效的Koopman生成器建模方法，通过算子分解和约束学习实现了更好的预测性能和可解释性。

Abstract: Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

</details>


### [214] [S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services](https://arxiv.org/abs/2602.14017)
*Chenyue Li,Wen Deng,Zhuotao Sun,Mengxi Jin,Hanzhe Cui,Han Li,Shentong Li,Man Kit Yu,Ming Long Lai,Yuhao Yang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: S2SServiceBench：一个用于评估多模态大语言模型在次季节到季节（S2S）气候服务中"最后一英里"决策支持能力的基准测试，涵盖6个应用领域、10种服务产品、约500个任务和1000+评估项。


<details>
  <summary>Details</summary>
Motivation: S2S预报对气候韧性和可持续发展至关重要，但存在"最后一英里"瓶颈：如何将科学预报转化为可信赖、可操作的气候服务，这需要可靠的多模态理解和不确定性下的决策推理。虽然多模态大语言模型在支持各种工作流程方面进展迅速，但尚不清楚它们能否在不确定性下从业务服务产品中可靠生成决策交付成果。

Method: 引入S2SServiceBench，这是一个从业务气候服务系统中策划的多模态基准测试，用于评估MLLMs在S2S气候服务中的能力。基准涵盖6个应用领域（农业、灾害、能源、金融、健康、航运），10种服务产品，约150+专家选择案例，每个案例在三个服务级别实例化，产生约500个任务和1000+评估项。

Result: 使用S2SServiceBench对最先进的MLLMs和智能体进行基准测试，分析了不同产品和服务级别的性能，揭示了S2S服务图理解和推理中的持续挑战：可操作信号理解、将不确定性操作化为可执行交接、以及对动态灾害的稳定、证据基础的分析和规划。

Conclusion: 该研究为构建未来气候服务智能体提供了可操作的指导，同时揭示了当前MLLMs在S2S气候服务决策支持方面的局限性，特别是在不确定性处理和证据基础推理方面。

Abstract: Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.

</details>


### [215] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: EIDOS：通过潜在空间预测学习而非直接观测值预测来预训练时间序列基础模型，以获得更结构化、时间一致的潜在表示


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型通过直接预测未来观测值进行预训练，这通常产生弱结构化的潜在表示，捕捉表面噪声而非连贯可预测的时间动态

Method: 引入EIDOS模型系列，将预训练从未来值预测转向潜在空间预测学习；训练因果Transformer预测潜在表示的演化；设计轻量级聚合分支构建目标表示；通过联合目标优化，整合潜在空间对齐、观测接地和直接预测监督

Result: 在GIFT-Eval基准测试中，EIDOS缓解了表示空间的结构碎片化，并实现了最先进的性能

Conclusion: 约束模型学习可预测的潜在动态是构建更鲁棒可靠的时间序列基础模型的原则性步骤

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [216] [Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers](https://arxiv.org/abs/2602.14050)
*Atsushi Shimizu,Shohei Taniguchi,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 论文提出了一种名为随机浮点采样(RFS)的简单而强大的位置编码策略，能够很好地泛化到预训练或微调期间未见过的长度。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在长度泛化方面的挑战——即模型在遇到比预训练时更长的输入时保持性能的能力。传统位置编码方法在处理未见长度时存在分布外(OOD)问题。

Method: 提出随机浮点采样(RFS)位置编码策略：不使用预定义的离散位置索引集，而是使用随机采样的连续值作为位置索引，在训练期间让模型接触多样化的索引，从而避免在未见长度上出现OOD问题。

Result: 实验证实RFS在长度泛化任务和零样本常识推理基准测试中表现出优越性能。该方法可以轻松整合到广泛使用的绝对正弦编码、RoPE和ALiBi等位置编码中。

Conclusion: RFS是一种简单而有效的位置编码策略，能够显著提升语言模型在未见长度上的泛化能力，为解决长度泛化问题提供了有前景的解决方案。

Abstract: Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.

</details>


### [217] [Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing](https://arxiv.org/abs/2602.14086)
*Jae-Hwan Choi,Jiwoo Yoon,Dohyun Kwon,Jaewoong Choi*

Main category: cs.LG

TL;DR: 该论文研究了无限维希尔伯特空间中的神经最优传输问题，通过高斯平滑策略解决半对偶神经OT在非正则设置下产生的虚假解问题，证明了在正则源测度下公式是适定的并能恢复唯一的Monge映射。


<details>
  <summary>Details</summary>
Motivation: 在无限维希尔伯特空间中，半对偶神经最优传输在非正则设置下经常产生虚假解，无法准确捕捉目标分布，需要解决这一不适定问题。

Method: 提出基于布朗运动的高斯平滑策略来扩展半对偶框架，使用正则测度框架分析虚假解问题，并证明在正则源测度下公式的适定性。

Result: 理论证明在正则源测度下，公式是适定的并能恢复唯一的Monge映射；建立了平滑测度正则性的尖锐特征化；在合成函数数据和时序数据集上的实验表明，该方法能有效抑制虚假解并优于现有基线方法。

Conclusion: 通过高斯平滑策略扩展的半对偶神经最优传输框架解决了无限维希尔伯特空间中的虚假解问题，在理论和实验上都证明了其有效性，为函数数据和时序数据的传输问题提供了稳健的解决方案。

Abstract: We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.

</details>


### [218] [A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers](https://arxiv.org/abs/2602.14154)
*Yuxuan Linghu,Zhiyuan Liu,Qi Deng*

Main category: cs.LG

TL;DR: dXPP：一种基于惩罚的二次规划可微优化框架，通过解耦求解与微分过程，使用任意黑盒QP求解器，并通过惩罚问题隐式微分，显著提升大规模问题的计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于KKT系统的可微优化方法在大规模问题上存在计算成本高和数值鲁棒性下降的问题，需要一种更高效、更稳健的二次规划可微优化框架。

Method: 提出dXPP惩罚微分框架：1）前向求解阶段解耦QP求解，可使用任意黑盒QP求解器；2）反向微分阶段将解映射到平滑近似惩罚问题，通过隐式微分仅需求解更小的原始变量线性系统，避免显式KKT微分困难。

Result: 在随机生成QP、大规模稀疏投影问题和实际多期投资组合优化任务上的实验表明，dXPP与基于KKT的微分方法性能相当，并在大规模问题上实现了显著加速。

Conclusion: dXPP通过解耦求解与微分、采用惩罚方法隐式微分，有效解决了大规模二次规划可微优化的计算效率和鲁棒性问题，为可微优化提供了更实用的解决方案。

Abstract: Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.

</details>


### [219] [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)
*Rizhen Hu,Yuan Cao,Boao Kong,Mou Sun,Kun Yuan*

Main category: cs.LG

TL;DR: 提出两种即插即用的正则化损失函数，无需修改MoE架构即可提升专家专业化和路由效率，实现更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏混合专家模型存在专家重叠问题，导致模型容量利用不足。现有架构解决方案需要大量结构修改且仅依赖层内信号。

Method: 提出两种正则化损失：1) 层内专业化损失，惩罚相同token上专家SwiGLU激活的余弦相似度；2) 跨层耦合损失，最大化相邻层间Top-k路由概率的联合概率。

Result: 实验表明在预训练、微调和零样本基准测试中均获得一致的任务增益，专家专业化程度更高，路由熵更低，推理速度更快。

Conclusion: 两种即插即用的正则化损失能有效提升MoE模型的专业化和路由效率，无需修改架构即可实现性能改进和更快的推理。

Abstract: Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.

</details>


### [220] [Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling](https://arxiv.org/abs/2602.14169)
*Yiran Guo,Zhongjian Qiao,Yingqi Xie,Jie Liu,Dan Ye,Ruiqing Zhang,Shuang Qiu,Lijie Xu*

Main category: cs.LG

TL;DR: DDE（Deep Dense Exploration）是一种针对大型语言模型强化学习的新探索策略，专注于在失败轨迹中的深度可恢复状态（pivots）进行密集采样，解决了现有方法采样稀释和深度状态探索不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大型语言模型的探索中存在显著局限：GRPO仅从根节点采样，导致高概率轨迹饱和而深度错误状态探索不足；基于树的方法盲目分散采样预算，造成采样稀释，无法发现罕见正确后缀并破坏局部基线稳定性。

Method: 提出DDE策略，专注于失败轨迹中的pivots（深度可恢复状态）。具体实现为DEEP-GRPO，包含三个关键创新：1）轻量级数据驱动的效用函数，自动平衡可恢复性和深度偏差以识别pivot状态；2）在每个pivot进行局部密集重采样，增加发现正确后续轨迹的概率；3）双流优化目标，将全局策略学习与局部纠正更新解耦。

Result: 在数学推理基准测试中，该方法在有限采样预算下一致优于GRPO、基于树的方法和其他强基线方法。

Conclusion: DDE通过专注于深度可恢复状态的密集探索，有效解决了大型语言模型强化学习中的探索挑战，在有限采样预算下能够发现高质量轨迹。

Abstract: Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.

</details>


### [221] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: TS-Haystack基准测试揭示时间序列语言模型在长上下文检索中的局限性：现有模型压缩策略虽提升分类精度，却损害了时间定位能力，突显了需要解耦序列长度与计算复杂度的架构设计。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列传感器数据可达数百万数据点，而现有时间序列语言模型主要在短序列上训练和评估，导致长上下文检索能力不足，缺乏能系统评估时间定位能力的基准测试。

Method: 提出TS-Haystack长上下文时间检索基准，包含直接检索、时间推理、多步推理和上下文异常四类共十种任务类型。通过在长纵向加速度计记录中嵌入短活动片段，系统评估从秒到2小时不同上下文长度的性能。

Result: 学习到的潜在压缩在高达176倍压缩比下能保持或提升分类准确率，但检索性能随上下文长度增加而下降，导致时间定位信息丢失。分类与检索行为存在系统性差异。

Conclusion: 需要设计能解耦序列长度与计算复杂度同时保持时间保真度的架构，以解决时间序列语言模型在长上下文检索中的根本局限性。

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [222] [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)
*Jinbo Wang,Binghui Li,Zhanpeng Zhou,Mingze Wang,Yuxuan Sun,Jiaqi Zhang,Xunliang Cai,Lei Wu*

Main category: cs.LG

TL;DR: 该研究使用函数缩放定律框架分析批量大小调度，发现最优调度策略取决于任务难度：简单任务应持续增加批量大小，而困难任务应在训练后期才切换到大批量，这能显著减少数据消耗且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 批量大小调度在大规模深度学习训练中至关重要，但理论基础薄弱。研究者希望建立批量大小调度的理论框架，理解不同任务难度下的最优调度策略。

Method: 采用函数缩放定律框架分析批量大小调度，理论推导最优调度结构，揭示"快速追赶效应"的动力学机制，并通过大规模语言模型预训练实验验证理论预测。

Result: 理论分析表明：简单任务应持续增加批量大小；困难任务应在训练后期切换到大批量。实验验证显示，后期切换调度策略在密集和MoE架构的1.1B参数模型中均优于恒定批量大小和早期切换基线。

Conclusion: 批量大小调度策略应基于任务难度设计，困难任务采用后期大批量切换策略可显著减少数据消耗而不损失性能，为大规模深度学习训练提供了理论指导和实用方案。

Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.

</details>


### [223] [MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM](https://arxiv.org/abs/2602.14209)
*Omin Kwon,Yeonjae Kim,Doyeon Kim,Minseo Kim,Yeonhong Park,Jae W. Lee*

Main category: cs.LG

TL;DR: MAGE是一种针对块扩散LLM的稀疏注意力方法，通过利用首次去噪步骤的注意力模式来预测重要KV条目，实现无训练稀疏去噪，显著减少内存访问并加速长上下文生成。


<details>
  <summary>Details</summary>
Motivation: 块扩散LLM在长上下文生成中面临KV缓存内存访问瓶颈，现有为自回归LLM设计的动态稀疏注意力方法在块扩散场景下表现不佳，需要专门针对块扩散特性的优化方案。

Method: MAGE利用块扩散特有的机会：首次全[MASK]去噪步骤的注意力模式能可靠预测重要KV条目和预算需求，通过单次精确注意力计算并重用于无训练稀疏去噪，同时采用轻量级微调策略强化[MASK]引导模式。

Result: 在LongBench和Needle-in-a-Haystack等长上下文基准测试中，MAGE以少量KV预算实现近乎无损的准确率，端到端速度提升3-4倍，显著优于自回归导向的稀疏注意力基线方法。

Conclusion: MAGE通过利用块扩散的独特特性，实现了高效的长上下文生成，仅需单GPU数小时微调即可应用于不同规模模型，为块扩散LLM的内存优化提供了有效解决方案。

Abstract: Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.

</details>


### [224] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: RMB-CLE：基于聚类和局部集成的新型鲁棒多任务学习框架，通过跨任务误差自适应聚类任务，防止负迁移，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法在任务不相关或存在噪声时容易发生负迁移，强制共享表示会损害性能。需要一种能够自适应识别任务关系并防止负迁移的鲁棒框架。

Method: 提出RMB-CLE框架：1）基于跨任务误差推导任务间相似度，将风险分解为函数不匹配和不可约噪声；2）使用凝聚聚类自适应分组任务；3）在每个聚类内使用局部集成实现鲁棒知识共享，同时保留任务特定模式。

Result: 在合成数据中能够恢复真实聚类结构；在多样化的真实世界和合成基准测试中，一致优于多任务、单任务和基于池化的集成方法。

Conclusion: RMB-CLE不仅结合了聚类和提升技术，更是一个通用且可扩展的框架，为鲁棒多任务学习建立了新的基础，能够有效防止负迁移并提升预测性能。

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [225] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: 该论文指出金融大语言模型应用中存在五种常见偏差，这些偏差会夸大性能、污染回测结果，并导致报告结果对部署声明无效。作者提出了结构有效性框架和评估清单来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地集成到金融工作流程中，但评估实践未能跟上。金融特定偏差会夸大性能表现、污染回测结果，使得报告结果对任何部署声明都无效。当前研究对这些偏差的关注不足。

Method: 识别了金融LLM应用中的五种常见偏差：前瞻偏差、幸存者偏差、叙事偏差、目标偏差和成本偏差。回顾了2023-2025年的164篇论文，分析偏差讨论情况。提出了结构有效性框架和包含最小要求的评估清单。

Result: 研究发现，在164篇论文中，没有任何单一偏差在超过28%的研究中被讨论。这些偏差会以不同方式破坏金融任务，并且常常相互叠加，产生有效性幻觉。

Conclusion: 金融LLM系统中的偏差需要明确关注，在支持部署声明之前应强制执行结构有效性。提出的框架和清单为偏差诊断和未来系统设计提供了最小要求。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [226] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: DELTAiF是一个基于迁移学习的框架，用于预测家庭热水消耗，通过从代表性家庭学习知识并微调到其他家庭，减少67%的训练时间，同时保持高预测精度。


<details>
  <summary>Details</summary>
Motivation: 随着住宅热泵安装的快速增长，优化家庭热水生产至关重要，但面临技术和可扩展性挑战。传统方法为每个家庭单独训练机器学习模型计算成本高，特别是在云连接热泵部署中。

Method: 提出DELTAiF迁移学习框架，从代表性家庭学习知识，然后微调到其他家庭，预测大型热水使用事件（如淋浴），实现自适应且可扩展的热水生产。

Result: 减少整体训练时间约67%，同时保持高预测精度（0.874-0.991），平均绝对百分比误差值在0.001-0.017之间。当源家庭表现出规律消费模式时，迁移学习特别有效。

Conclusion: 迁移学习能够实现大规模热水需求预测，DELTAiF框架提供了一种可扩展且准确的解决方案，解决了传统方法在计算成本和可扩展性方面的挑战。

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [227] [Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization](https://arxiv.org/abs/2602.14272)
*Yilun Kuang,Yash Dagade,Deep Chakraborty,Erik Learned-Miller,Randall Balestriero,Tim G. J. Rudner,Yann LeCun*

Main category: cs.LG

TL;DR: Radial-VCReg通过添加径向高斯化损失增强VCReg，将特征范数与卡方分布对齐，减少高阶依赖并提升表示多样性


<details>
  <summary>Details</summary>
Motivation: 自监督学习旨在学习最大化信息表示，但显式信息最大化受到维度灾难的阻碍。现有方法如VCReg通过正则化一阶和二阶特征统计量无法完全实现最大熵

Method: 提出Radial-VCReg，在VCReg基础上添加径向高斯化损失，将特征范数与卡方分布对齐——这是高维高斯分布的一个定义性特征

Result: 证明Radial-VCReg相比VCReg能将更广泛的分布类别转化为正态分布；在合成和真实数据集上显示它通过减少高阶依赖和促进更多样化、信息更丰富的表示来持续提升性能

Conclusion: Radial-VCReg通过径向高斯化增强VCReg，有效解决维度灾难问题，产生更接近最大熵的表示，提升自监督学习性能

Abstract: Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.

</details>


### [228] [Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data](https://arxiv.org/abs/2602.14274)
*Boning Zhou,Ziyu Wang,Han Hong,Haoqi Hu*

Main category: cs.LG

TL;DR: 该论文提出了一个利用基于transformer的语言模型从非结构化文本中进行因果推断的框架，验证了文本数据在因果推断中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断严重依赖结构化数据，但在许多现实场景中，结构化数据可能不完整或不可用，限制了因果推断方法的应用范围。

Method: 开发了一个基于transformer语言模型的框架，能够从非结构化文本中提取信息并进行因果推断，通过对比结构化数据和非结构化文本的因果估计结果来验证框架有效性。

Result: 在总体、群体和个体三个层面上，从非结构化文本得到的因果估计与从结构化数据得到的结果具有一致性，验证了文本数据在因果推断任务中的潜力。

Conclusion: 该框架扩展了因果推断方法的应用范围，使其在只有文本数据可用的情况下也能进行数据驱动的商业决策，解决了结构化数据稀缺的问题。

Abstract: Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.

</details>


### [229] [Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems](https://arxiv.org/abs/2602.14275)
*Lamine Rihani*

Main category: cs.LG

TL;DR: 论文提出反向n-wise输出测试方法，通过直接在输出空间构建覆盖数组，解决AI/ML和量子计算中高维连续输入、概率性输出等测试挑战。


<details>
  <summary>Details</summary>
Motivation: AI/ML系统和量子计算软件面临前所未有的测试挑战：高维连续输入空间、概率性/非确定性输出分布、仅通过可观测预测行为定义正确性，以及公平性、鲁棒性、错误模式等关键质量维度需要通过复杂的多向交互来体现。

Method: 提出反向n-wise输出测试范式，直接在领域特定的输出等价类上构建覆盖数组，包括ML置信度校准桶、决策边界区域、公平性分区、嵌入聚类、量子测量结果分布等，然后通过无梯度元启发式优化解决黑盒逆映射问题，合成能够从黑盒模型中引出目标行为特征的输入配置。

Result: 该框架为两个领域带来协同效益：明确的客户中心预测/测量覆盖保证，显著提高ML校准/边界故障和量子错误模式的故障检测率，增强测试套件效率，以及通过不确定性分析和覆盖漂移监测实现结构化MLOps/量子验证流程。

Conclusion: 反向n-wise输出测试提供了一种数学原理上的范式转换，通过直接在输出空间构建覆盖来应对AI/ML和量子计算的独特测试挑战，为这些复杂系统的质量保证提供了系统化方法。

Abstract: Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

</details>


### [230] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT是一种设计模式，将预训练的机器学习模型作为可调用工具集成到LLM智能体工作流中，使智能体能够根据需要调用定量预测并在上下文中推理输出结果。


<details>
  <summary>Details</summary>
Motivation: 传统流水线将ML推理作为静态预处理步骤，而MLAT旨在将模型定位为一等工具，使LLM能够根据对话上下文决定何时以及如何使用它，实现定量估计与上下文推理的结合。

Method: 提出MLAT设计模式，通过PitchCraft系统验证：使用两个智能体（研究智能体通过并行工具调用收集情报，草稿智能体将XGBoost定价模型作为工具调用）和结构化输出架构，在极端数据稀缺情况下训练定价模型。

Result: 定价模型在70个真实和人工验证合成数据上训练，在保留数据上达到R^2=0.807，平均绝对误差3688美元。系统将提案生成时间从数小时减少到10分钟以内。

Conclusion: MLAT框架适用于需要定量估计与上下文推理结合的领域，将ML模型作为可调用工具集成到LLM工作流中，显著提高了效率和灵活性。

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [231] [In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes](https://arxiv.org/abs/2602.14318)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 本文对Transformer模型在安全关键应用中的可信度进行了系统性评估，涵盖可解释性、鲁棒性、公平性、隐私等方面，识别了其结构脆弱性和领域特定风险。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer架构在自然语言处理、计算机视觉、医疗健康、自主系统以及气候建模、材料发现、药物研发、核科学等关键科学计算领域的广泛应用，需要对其可信度进行更深入、更严谨的理解，特别是在高风险应用场景中。

Method: 通过全面综述的方式，系统评估Transformer模型的可信度，包括可解释性、可解释性、对抗攻击鲁棒性、公平性和隐私保护等方面。同时，在自然语言处理、计算机视觉、科学工程领域（包括机器人、医学、地球科学、材料科学、流体动力学、核科学、自动定理证明）等安全关键应用中系统检验Transformer模型的可信度。

Result: 识别了Transformer模型在多个领域中的重复性结构脆弱性和领域特定风险，分析了这些架构部署时可能面临的风险，并指出了限制Transformer可靠部署的开放研究挑战。

Conclusion: 通过对不同领域的综合洞察，揭示了Transformer模型在安全关键应用中存在的系统性可信度问题，为未来研究提供了重要方向，强调了在部署这些强大架构时需要解决的可信度挑战。

Abstract: Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.

</details>


### [232] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO是一种改进GRPO的强化学习方法，通过自适应采样策略、选择性拒绝和贝叶斯后验来避免零梯度信号，在保持性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: GRPO方法在LLM后训练中存在效率问题：当一组rollouts全部正确或全部错误时，归一化优势值为零，导致无梯度信号，浪费了微调计算资源。

Method: 提出AERO方法，包含三个核心组件：1）自适应rollout策略；2）选择性拒绝策略来修剪rollouts；3）贝叶斯后验来防止零优势死区。

Result: 在三个模型配置上测试，在相同总rollout预算下，AERO减少约48%的总训练计算量，缩短约45%的每步训练时间，同时保持或改进Pass@8和Avg@8性能。

Conclusion: AERO提供了一种实用、可扩展且计算高效的基于强化学习的LLM对齐策略，显著提升了训练效率而不牺牲性能。

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [233] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: 本文提出了一种基于线性时序逻辑(LTL)的多任务强化学习方法，通过层次化神经架构和注意力机制学习结构化任务表示，以提升零样本执行未见任务的能力。


<details>
  <summary>Details</summary>
Motivation: 在多任务强化学习中，现有方法虽然能训练通用策略，但往往难以有效捕捉线性时序逻辑(LTL)规范中丰富的逻辑和时间结构，导致在零样本执行未见任务时表现不佳。

Method: 提出了一种新颖的方法来学习结构化任务表示：1) 基于任务有限自动机构建布尔公式序列作为策略条件；2) 采用层次化神经架构编码这些公式的逻辑结构；3) 引入注意力机制使策略能够推理未来子目标。

Result: 在多种复杂环境中的实验表明，该方法具有强大的泛化能力和优越的性能表现。

Conclusion: 通过结构化任务表示和层次化架构，该方法能更好地捕捉LTL规范的逻辑和时间结构，显著提升了多任务强化学习中零样本执行未见任务的能力。

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [234] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE是一种基于模型的强化学习方法，通过扩展隐式最大似然估计来学习随机多模态世界模型，使用集成和潜在采样估计预测不确定性，并通过置信度加权合成转移来稳定学习。


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习虽然样本效率高，但在实践中表现不佳，原因包括：模型误差累积、单模态世界模型平均多模态动态、过度自信的预测偏差学习。

Method: 将隐式最大似然估计扩展到基于模型的强化学习框架，学习随机多模态世界模型（无需迭代采样），通过集成和潜在采样估计预测不确定性，训练时根据预测置信度加权合成转移。

Result: 在40个连续控制任务（DeepMind Control、MyoSuite、HumanoidBench）上，WIMLE实现了优于模型无关和基于模型基线的样本效率和竞争性或更好的渐近性能。在Humanoid-run任务上样本效率提高50%以上，在HumanoidBench上解决了14个任务中的8个。

Conclusion: 基于IMLE的多模态性和不确定性感知加权对于稳定的基于模型的强化学习具有重要价值。

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [235] [A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments](https://arxiv.org/abs/2602.14375)
*Kensuke Ajimoto,Yuma Yamamoto,Yoshifumi Kusunoki,Tomoharu Nakashima*

Main category: cs.LG

TL;DR: 提出了一种用于动态环境的多类在线模糊分类器，扩展了传统仅处理二分类问题的在线模糊分类器


<details>
  <summary>Details</summary>
Motivation: 传统在线模糊分类器只能处理二分类问题，但在实际动态环境中需要处理多类分类问题

Method: 使用模糊if-then规则构建分类器，其中前件模糊集由用户预先确定，后件实值通过训练数据学习；在在线框架中，训练数据模式不是一次性全部可用，而是随时间逐步获得

Result: 通过合成动态数据和多个基准数据集上的数值实验评估了多类在线模糊分类器的性能

Conclusion: 成功将在线模糊分类器扩展到多类问题，为动态环境中的多类分类提供了有效解决方案

Abstract: This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.

</details>


### [236] [The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization](https://arxiv.org/abs/2602.14423)
*Abdelali Bouyahia,Frédéric LeBlanc,Mario Marchand*

Main category: cs.LG

TL;DR: 提出信息论框架分析数据增强对泛化和不变性学习的影响，通过互信息边界将泛化差距分解为三个可解释项，引入群直径概念揭示数据增强的权衡机制。


<details>
  <summary>Details</summary>
Motivation: 数据增强是提升机器学习泛化能力的常用技术，但其理论作用尚未完全理解。本文旨在建立系统框架，从信息论角度分析数据增强如何影响泛化性能和不变性学习。

Method: 构建基于互信息边界的信息论框架，将增强分布建模为原始数据分布与变换分布的复合，推导轨道平均损失函数。在损失函数和增强过程的次高斯假设下，将泛化差距分解为分布差异、算法稳定性和增强敏感性三个可解释项，并引入群直径概念量化增强扰动。

Result: 理论分析表明泛化差距可分解为三个受群直径控制的项：分布差异、算法稳定性和增强敏感性。群直径揭示了数据增强的内在权衡：小直径保持数据保真度但正则化有限，大直径增强稳定性但增加偏差和敏感性。数值实验验证了理论边界能可靠追踪真实泛化差距。

Conclusion: 提出的信息论框架为理解数据增强对泛化的影响提供了系统分析工具，群直径概念统一了增强效果的量化控制，揭示了数据增强设计中保真度与正则化之间的基本权衡关系。

Abstract: Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.

</details>


### [237] [A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking](https://arxiv.org/abs/2602.14430)
*Prithwijit Chowdhury,Ahmad Mustafa,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TL;DR: 该研究提出一个统一框架，通过反事实生成和必要性/充分性量化来评估LIME和SHAP在油气勘探风险评估中的解释鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在油气勘探风险评估中，基于机器学习的分类器缺乏透明度，而现有XAI方法（如LIME和SHAP）对同一场景的解释存在不一致，特别是在复杂数据上，这影响了模型的可信度。

Method: 提出一个统一框架，生成反事实并量化必要性和充分性，用于评估LIME和SHAP在高维结构化勘探风险评估数据上的解释鲁棒性。

Result: 通过鲁棒性测试，深入了解了模型处理错误数据的能力，以及哪种XAI模块与哪种模型在油气指示数据集上表现最佳。

Conclusion: 基于因果理论中的必要性和充分性概念来验证特征重要性排名，是提高XAI方法可信度和鲁棒性的更可靠方法，有助于在油气勘探中做出更可信的决策。

Abstract: In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of "importance" and "relevance" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.

</details>


### [238] [S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations](https://arxiv.org/abs/2602.14432)
*Arnav Chavan,Nahush Lele,Udbhav Bamba,Sankalp Dayal,Aditi Raghunathan,Deepak Gupta*

Main category: cs.LG

TL;DR: 该论文提出了一种名为选择性谱衰减（S²D）的方法，通过针对性地正则化权重矩阵的最大奇异值分量，有效减少大模型中的激活异常值，从而显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型中的激活异常值是模型量化的根本挑战，这些异常值会产生过大的数值范围，导致量化时精度严重下降。随着预训练规模的扩大（如从CLIP到SigLIP、SigLIP2），异常值问题变得更加严重。

Method: 通过理论分析和实证相关性研究，建立了激活异常值与权重矩阵主导奇异值之间的直接联系。基于这一洞察，提出了选择性谱衰减（S²D）方法——一种几何原理驱动的条件化方法，在微调过程中仅对与最大奇异值对应的权重分量进行外科手术式的正则化。

Result: S²D方法显著减少了激活异常值，生成了条件良好的表示，这些表示本质上对量化友好。使用S²D训练的模型在W4A4量化下，在ImageNet上的PTQ精度提升了高达7%，与QAT结合时提升了4%。这些改进在下游任务和视觉语言模型中也具有泛化性。

Conclusion: S²D方法能够在不牺牲部署效率的情况下，扩展越来越大规模和严格训练的模型，解决了大模型量化中的激活异常值问题，为高效部署提供了有效解决方案。

Abstract: Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.

</details>


### [239] [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://arxiv.org/abs/2602.14452)
*Lei Chen,Yuan Meng,Xiaoyu Zhan,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: WiSparse提出了一种无需训练的权重感知混合粒度激活稀疏化方法，通过结合激活和权重信息进行自适应稀疏分配，在保持模型性能的同时显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的激活稀疏化方法仅依赖激活信息且使用统一稀疏率，忽略了权重的重要性以及不同模型块对稀疏化的敏感性差异，导致性能下降。

Method: 提出权重感知混合粒度训练免费激活稀疏化（WiSparse）：1）权重感知机制结合激活幅度和预计算权重范数来识别重要通道；2）混合粒度分配方案：通过进化搜索全局分配稀疏预算保护敏感区域，然后在块内细化最小化重构误差；3）改进稀疏核。

Result: 在三个代表性模型上验证有效性，50%稀疏率下，WiSparse保持Llama3.1密集模型97%的性能，比最强基线提升2.23个百分点，同时实现端到端推理速度21.4%的加速。

Conclusion: WiSparse推进了无需训练方法在高效LLM推理中的极限，在不进行训练的情况下突破了可实现的加速边界。

Abstract: Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.

</details>


### [240] [Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment](https://arxiv.org/abs/2602.14462)
*Hong Li,Zhen Zhou,Honggang Zhang,Yuping Luo,Xinyue Wang,Han Gong,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级、模型无关的诊断框架，用于量化数据并行训练中工作节点间的"静默不一致性"，即损失和梯度差异在聚合监控信号下不可见的问题。


<details>
  <summary>Details</summary>
Motivation: 数据并行训练中，虽然参数同步保证了模型权重的数值等价性，但并不能确保工作节点级优化动态在梯度聚合前的一致性。这种"静默不一致性"可能导致隐藏的训练不稳定问题，而传统聚合监控信号无法检测到这种跨工作节点的差异。

Method: 提出了一个轻量级、模型无关的诊断框架，包含三个互补指标：损失分散度、梯度范数分散度、以及通过工作节点间余弦相似度衡量的梯度方向一致性。这些指标使用标准训练流程中已有的信号，无需修改模型架构、同步机制或优化算法。

Result: 在8-NPU数据并行设置下对1B参数模型进行全参数微调实验，结果显示：逐步去同步化的数据洗牌和随机种子会导致损失/梯度分散度显著增加和方向对齐度降低，尽管全局平均损失曲线保持平滑。

Conclusion: 提出的指标为大规模数据并行微调中的隐藏不稳定模式提供了可操作的可见性，能够实现更可靠的诊断和配置评估，有助于提高训练稳定性。

Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.

</details>


### [241] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: LACONIC是一种强化学习方法，通过在训练中强制执行目标token预算来控制大语言模型输出长度，在保持任务性能的同时显著减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练大语言模型时会产生过长的响应，增加推理延迟和计算开销。现有的长度控制方法依赖固定的启发式奖励调整，可能与任务目标不一致且需要脆弱的调参。

Method: 提出LACONIC方法，在训练中强制执行目标token预算。使用增强的目标函数更新策略模型，结合任务奖励和基于长度的成本。通过自适应调整成本规模来平衡简洁性和任务性能。

Result: 在数学推理模型和数据集上，LACONIC保持或提高了pass@1性能，同时将输出长度减少了50%以上。在通用知识和多语言基准测试中，用44%更少的token保持了域外性能。

Conclusion: LACONIC能够在不改变推理过程且部署开销最小的情况下，集成到标准RL调优中，实现鲁棒的长度控制同时保持任务奖励。

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [242] [One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise](https://arxiv.org/abs/2602.14474)
*Aadirupa Saha,Amith Bhat,Haipeng Luo*

Main category: cs.LG

TL;DR: SOAR算法在多源多臂老虎机问题中，通过自适应选择数据源来最小化遗憾，实现了接近最优单源性能的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究多源多臂老虎机问题，其中每个数据源具有未知且不同的噪声方差。学习者需要在标准MAB遗憾最小化的基础上，自适应选择查询哪个数据源，以应对不同数据源质量差异带来的挑战。

Method: 提出SOAR算法：1）使用尖锐的方差集中界限快速剪枝高方差数据源；2）采用"平衡最小最大LCB-UCB方法"，将识别最佳臂和最优（最小方差）数据源的任务无缝集成。

Result: SOAR实现了实例依赖的遗憾界$\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$，其中${σ^*}^2$是最小源方差。该结果优于Uniform UCB或Explore-then-Commit UCB等基线方法，后者可能遭受与$σ_{\max}^2$成比例的遗憾。

Conclusion: 尽管缺乏对最小方差源的先验知识，SOAR能够达到具有方差${σ^*}^2$的标准单源MAB的最优实例依赖遗憾，同时仅产生较小的可加成本用于最优源识别。实验验证了SOAR在合成和真实数据集上的优越性能。

Abstract: We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\{σ_j^2\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, up to preprocessing costs depending only on problem parameters, where ${σ^*}^2 := \min_j σ_j^2$ is the minimum source variance and $Δ_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${σ^*}^2$, while incurring only an small (and unavoidable) additive cost of $\tilde O(\sqrt{K \sum_{j=1}^M σ_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $σ_{\max}^2$ in place of ${σ^*}^2$-a gap that can be arbitrarily large when $σ_{\max} \gg σ^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\;25M dataset, demonstrating the superior performance of SOAR over the baselines.

</details>


### [243] [Covariance-Aware Transformers for Quadratic Programming and Decision Making](https://arxiv.org/abs/2602.14506)
*Kutay Tire,Yufan Zhang,Ege Onur Taga,Samet Oymak*

Main category: cs.LG

TL;DR: Transformer通过线性注意力机制可证明地解决无约束二次规划，结合MLP可解决L1惩罚和约束的二次规划，并提出了Time2Decide方法增强时间序列基础模型，在投资组合优化中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在解决二次规划问题中的应用，以及这种能力如何有益于涉及协方差矩阵的决策问题，特别是投资组合优化等实际应用场景。

Method: 1. 证明线性注意力机制可通过逐行标记化矩阵变量来模拟梯度下降迭代，从而解决无约束二次规划；2. 通过结合MLP，Transformer块可解决L1惩罚的二次规划（模拟迭代软阈值）和L1约束的二次规划（需要额外反馈环）；3. 提出Time2Decide方法，通过显式输入变量间的协方差矩阵来增强时间序列基础模型。

Result: Time2Decide在经典的L1约束二次规划投资组合优化问题中，始终优于基础时间序列基础模型；在适当设置下，甚至优于传统的"预测后优化"流程，其中先预测收益再显式求解约束二次规划。

Conclusion: Transformer从显式使用二阶统计量中受益，这使它们能够在前向传播中有效解决复杂的决策问题，如投资组合构建，展示了Transformer在优化问题求解中的潜力。

Abstract: We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\frac{1}{2}x^\top Ax+b^\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical "Predict-then-Optimize (PtO)" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.

</details>


### [244] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: DeepMTL2R是一个开源深度学习框架，用于多任务学习排序，通过transformer的自注意力机制整合异构相关性信号，支持21种多任务学习算法和多目标优化，实现Pareto最优排序模型。


<details>
  <summary>Details</summary>
Motivation: 现代排序系统需要同时优化多个相关性标准，这些标准可能相互冲突。传统方法难以有效整合异构相关性信号并处理目标间的复杂依赖关系，因此需要开发一个统一的、可扩展的多任务学习排序框架。

Method: DeepMTL2R利用transformer架构的自注意力机制，将异构相关性信号整合到统一的上下文感知模型中。框架包含21种最先进的多任务学习算法，支持多目标优化以识别Pareto最优排序模型，能够捕捉项目和标签间的复杂依赖关系和长距离交互。

Result: 在公开数据集上验证了框架的有效性，报告了具有竞争力的性能，并可视化了目标间的权衡关系。DeepMTL2R为现代排序系统提供了可扩展且表达能力强的解决方案，并促进了多任务学习策略间的受控比较。

Conclusion: DeepMTL2R是一个强大的开源框架，通过整合transformer的自注意力机制和多任务学习算法，有效解决了多目标排序问题，为研究人员和从业者提供了实用的工具来开发和比较多任务学习排序模型。

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [245] [Truly Adapting to Adversarial Constraints in Constrained MABs](https://arxiv.org/abs/2602.14543)
*Francesco Emanuele Stradi,Kalana Kalupahana,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 该论文研究了带未知约束的多臂老虎机问题，在非平稳环境下同时最小化总损失和控制约束违反，提出了在不同反馈机制下的最优算法。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机问题中，学习者不仅要最小化学习过程中的总损失，还要控制多个未知约束的违反。现有研究要么假设约束是随机的，要么通过竞争比等放松基准来处理完全对抗性约束。本文旨在设计算法，在约束随机而损失任意变化时，同时获得最优的遗憾和正约束违反率，并能平滑适应约束对抗性程度的变化。

Method: 针对不同反馈机制设计了三种算法：1）完全反馈下，提出算法获得$\widetilde{\mathcal{O}}(\sqrt{T}+C)$的遗憾和正约束违反；2）仅损失有老虎机反馈时，扩展了这些保证；3）约束也有老虎机反馈时，设计了算法获得$\widetilde{\mathcal{O}}(\sqrt{T}+C)$的正约束违反和$\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$的遗憾。其中$C$量化了约束的非平稳性程度。

Result: 在完全反馈下，算法达到了$\widetilde{\mathcal{O}}(\sqrt{T}+C)$的遗憾和$\widetilde{\mathcal{O}}(\sqrt{T}+C)$的正约束违反。当只有损失有老虎机反馈时，能够扩展这些保证。当约束也有老虎机反馈时，算法实现了$\widetilde{\mathcal{O}}(\sqrt{T}+C)$的正约束违反和$\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$的遗憾。

Conclusion: 本文首次在约束随机而损失任意变化的情况下，设计了能够同时获得最优遗憾和正约束违反率的算法，并且这些保证能够平滑地适应约束对抗性程度的变化，填补了现有研究的空白。

Abstract: We study the constrained variant of the \emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \emph{unknown} constraints, under both \emph{full} and \emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \emph{full feedback} we propose an algorithm attaining $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ regret and $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation and $\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$ regret.

</details>


### [246] [Replicable Constrained Bandits](https://arxiv.org/abs/2602.14580)
*Matteo Bollini,Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.LG

TL;DR: 该论文研究了多臂老虎机问题中的算法可复制性，提出了在约束条件下实现可复制性的算法，其遗憾和约束违反与非可复制算法相当。


<details>
  <summary>Details</summary>
Motivation: 机器学习实验需要可重复性，算法可复制性旨在确保算法在不同执行中做出相同决策。本文首次研究约束多臂老虎机问题中的可复制性，填补了该领域的研究空白。

Method: 设计了可复制的约束多臂老虎机算法，首先开发了首个可复制的UCB类算法用于无约束情况，证明基于乐观不确定性原则的算法可以实现可复制性，然后将该方法扩展到约束多臂老虎机问题。

Result: 成功实现了约束多臂老虎机中的可复制性，设计的可复制算法在遗憾和约束违反方面与非可复制算法具有相同的T阶性能，证明了乐观不确定性算法可以实现可复制性。

Conclusion: 算法可复制性可以在约束多臂老虎机问题中实现，且性能与非可复制算法相当，这一结果为可复制在线学习提供了重要理论基础和技术方案。

Abstract: Algorithmic \emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.

</details>


### [247] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: 提出了一种解耦的连续时间演员-评论家算法，通过交替更新解决标准离散时间RL在连续时间控制问题中的局限性，在连续控制基准和实际交易任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的控制问题（金融、机器人等）通常在连续时间中演化，具有非均匀、事件驱动的决策特性。标准离散时间RL基于固定步长Bellman更新，在这种设置下表现不佳：当时间间隔缩小时，Q函数会坍缩到值函数V，失去动作排序能力。现有连续时间方法通过优势率函数q重新引入动作信息，但使用复杂的鞅损失或正交约束来强制最优性，这些方法对测试过程的选择敏感，并将V和q耦合到大型复杂优化问题中，难以可靠训练。

Method: 提出了一种新颖的解耦连续时间演员-评论家算法，采用交替更新策略：1）q通过V的扩散生成器学习；2）V通过基于Hamiltonian的值流更新，该值流在无限小时间步下仍保持信息性（标准max/softmax备份在此情况下失效）。理论上通过新的概率论证证明了严格收敛性，绕过了生成器基Hamiltonian在sup-norm下缺乏Bellman式收缩的挑战。

Result: 该方法在具有挑战性的连续控制基准测试和实际交易任务中优于先前的连续时间方法和领先的离散时间基线，在一个季度内实现了21%的利润，几乎是最佳方法的2倍。

Conclusion: 提出的解耦连续时间演员-评论家算法通过交替更新V和q，有效解决了连续时间RL中的关键问题，在理论和实证上都表现出优越性能，为连续时间控制问题提供了可靠且高效的解决方案。

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [248] [OPBench: A Graph Benchmark to Combat the Opioid Crisis](https://arxiv.org/abs/2602.14602)
*Tianyi Ma,Yiyang Li,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: OPBench是首个全面的阿片类药物危机基准测试，包含5个数据集覆盖3个关键应用领域，提供标准评估框架以系统评估图学习方法在阿片危机中的应用。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机持续肆虐全球，急需计算解决方案。虽然图学习方法已成为建模复杂药物相关现象的有前景范式，但缺乏在真实阿片危机场景中系统评估这些方法的综合基准。

Method: 创建OPBench基准测试，包含5个数据集覆盖3个关键领域：医疗索赔中的阿片类药物过量检测、数字平台中的非法药物贩运检测、饮食模式中的药物滥用预测。采用异构图和超图等多样图结构，与领域专家合作标注数据，建立统一评估框架，包括标准化协议、预定义数据分割和可复现基线。

Result: 通过广泛实验分析了现有图学习方法的优势和局限性，为未来阿片危机研究提供了可行见解。源代码和数据集已公开可用。

Conclusion: OPBench填补了阿片危机领域缺乏综合基准的空白，为系统评估图学习方法提供了标准化平台，有助于推动对抗阿片危机的计算研究。

Abstract: The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.

</details>


### [249] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息瓶颈正则化的概念瓶颈模型改进方法，通过在概念层施加信息瓶颈约束来提升模型性能和概念忠实度。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型虽然旨在通过人类可理解的概念层提供可解释的预测，但通常面临准确性下降和概念泄露问题，这削弱了概念的忠实性。现有方法在概念表示的充分性和最小性之间存在平衡问题。

Method: 引入显式的信息瓶颈正则化器，在概念层惩罚I(X;C)同时保留任务相关信息I(C;Y)，鼓励最小充分的概念表示。推导了两种实用变体（变分目标和基于熵的替代方法），并将其集成到标准CBM训练中，无需架构更改或额外监督。

Result: 在六个CBM家族和三个基准测试上的评估表明，IB正则化模型始终优于其原始版本。信息平面分析进一步证实了预期行为，表明最小充分概念瓶颈能同时提升预测性能和概念级干预的可靠性。

Conclusion: 提出的正则化器提供了一种理论基础扎实、架构无关的路径，可构建更忠实和可干预的CBMs。通过统一训练协议解决了先前评估不一致的问题，并在不同模型家族和数据集上展示了稳健的改进。

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [250] [Alignment Adapter to Improve the Performance of Compressed Deep Learning Models](https://arxiv.org/abs/2602.14635)
*Rohit Raj Rai,Abhishek Dhaka,Amit Awekar*

Main category: cs.LG

TL;DR: 提出Alignment Adapter (AlAd)轻量级适配器，通过滑动窗口对齐压缩模型与原大模型的token级嵌入，提升压缩模型性能


<details>
  <summary>Details</summary>
Motivation: 压缩深度学习模型在资源受限环境中部署至关重要，但其性能通常落后于大规模模型，需要缩小这一性能差距

Method: 提出基于滑动窗口的轻量级适配器AlAd，对齐压缩模型与原大模型的token级嵌入，保持局部上下文语义，支持不同维度或架构的灵活对齐，与底层压缩方法无关

Result: 在BERT系列模型的三个token级NLP任务实验中，AlAd显著提升压缩模型性能，仅带来微小的尺寸和延迟开销

Conclusion: AlAd是一种有效的压缩模型性能提升方法，可作为即插即用模块部署在冻结的压缩模型上，或与压缩模型联合微调以获得进一步性能提升

Abstract: Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.

</details>


### [251] [An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale](https://arxiv.org/abs/2602.14656)
*Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: POGO是一种新的正交约束优化算法，相比现有方法（如Landing算法）更高效、可扩展，支持现代自适应优化器，能处理数千个正交矩阵的大规模问题。


<details>
  <summary>Details</summary>
Motivation: 正交约束在鲁棒和概率机器学习中普遍存在，但现有优化器计算成本高，难以扩展到数百或数千个约束的问题。虽然Landing算法是一个例外，但它以暂时放松正交性为代价。

Method: POGO算法重新审视并改进了Landing算法的思想，能够集成现代自适应优化器，同时确保正交约束得到有效满足。算法快速且GPU友好，仅需5个矩阵乘积，在实践中始终保持正交性。

Result: 在多个具有挑战性的基准测试中，POGO显著优于最近的优化器，能够在几分钟内优化包含数千个正交矩阵的问题，而替代方法需要数小时。

Conclusion: POGO为在机器学习中大规模利用正交约束设定了里程碑，其PyTorch实现已公开可用。

Abstract: Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.

</details>


### [252] [Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error](https://arxiv.org/abs/2602.14682)
*Farzan Farnia,Mohammad Jalali,Azim Ospanov*

Main category: cs.LG

TL;DR: 研究发现现代生成模型存在系统性多样性偏差，生成的样本多样性显著低于真实数据分布，这源于有限样本估计的熵基多样性指标会低估真实分布的多样性。


<details>
  <summary>Details</summary>
Motivation: 虽然深度生成模型在生成高质量样本方面取得了巨大成功，但一个重要但较少系统研究的问题是：训练好的生成模型是否忠实地捕捉了底层数据分布的多样性。本文旨在通过直接比较生成样本与测试样本的多样性来回答这个问题。

Method: 使用最近提出的无参考熵基多样性评分方法Vendi和RKE，在多个基准数据集上比较最先进生成模型生成的样本与从目标数据分布中抽取的测试样本的多样性。分析熵基多样性评分的有限样本行为，并探讨基于Vendi和RKE的多样性感知正则化和引导策略。

Result: 测试数据在所有数据集上都获得了显著高于生成样本的Vendi和RKE多样性分数，表明现代生成模型存在系统性向下多样性偏差。分析表明，熵基多样性评分的期望值随样本量增加而增加，这意味着基于有限训练集估计的多样性会固有地低估真实分布的多样性。

Conclusion: 优化生成器以最小化与经验数据分布的散度会导致多样性损失。基于Vendi和RKE的多样性感知正则化和引导策略是缓解这种偏差的有原则方向，实证证据表明这些策略有潜力改善结果。

Abstract: Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.

</details>


### [253] [SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data](https://arxiv.org/abs/2602.14687)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: SynthSAEBench是一个用于评估稀疏自编码器架构的工具包，通过生成具有真实特征的大规模合成数据，提供标准化基准模型，帮助研究人员精确诊断SAE故障模式并验证架构改进。


<details>
  <summary>Details</summary>
Motivation: 当前SAE基准测试存在两个主要问题：在LLM上的基准测试噪声太大，无法有效区分架构改进；而合成数据实验规模太小且不真实，无法提供有意义的比较。需要一种能够精确验证SAE架构创新的基准测试方法。

Method: 开发了SynthSAEBench工具包，能够生成具有真实特征（包括相关性、层次结构和叠加）的大规模合成数据，并创建了标准化基准模型SynthSAEBench-16k，使不同SAE架构能够直接比较。

Result: 该基准成功复现了多个先前观察到的LLM SAE现象，包括重构与潜在质量指标之间的脱节、SAE探测结果不佳以及由L0调节的精度-召回权衡。还发现了一个新的故障模式：匹配追踪SAE利用叠加噪声来改进重构，而没有学习真实特征，表明更具表达力的编码器容易过拟合。

Conclusion: SynthSAEBench通过提供真实特征和受控消融实验，补充了LLM基准测试，使研究人员能够在扩展到LLM之前精确诊断SAE故障模式并验证架构改进。

Abstract: Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

</details>


### [254] [A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)](https://arxiv.org/abs/2602.14696)
*Nihal V. Nayak,Paula Rodriguez-Diaz,Neha Hulkund,Sara Beery,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 本文系统分析了指令微调中的数据选择方法，发现梯度表示与贪心轮询算法在低预算下表现最佳，但优势随预算增加而减弱。研究统一了多种现有算法为近似距离最小化问题。


<details>
  <summary>Details</summary>
Motivation: 当前指令选择方法研究分散且不透明：方法差异大、常忽略零样本基线、关键组件贡献混杂，导致实践者缺乏针对目标任务的指令选择指导。

Method: 提出框架分离分析数据表示和选择算法两个核心要素，支持跨模型、任务和预算的受控比较。将多种现有算法统一为查询集与选择子集间的近似距离最小化问题。

Result: 发现只有基于梯度的数据表示能一致预测性能；梯度表示+贪心轮询算法在低预算下平均表现最佳；优势随预算增加而减弱；提供了新的泛化边界理论支持。

Conclusion: 研究为LLM微调中的数据选择提供了关键见解和理论基础，有助于更原则性的指令选择方法发展。

Abstract: Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.

</details>


### [255] [D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation](https://arxiv.org/abs/2602.14728)
*Nozomu Fujisawa,Masaaki Kondo*

Main category: cs.LG

TL;DR: D2-LoRA是一种参数高效微调方法，通过带符号的低秩残差更新和列向投影技术，在有限数据和计算约束下实现高性能，同时保持推理时的代数可合并性。


<details>
  <summary>Details</summary>
Motivation: 在现实的数据和计算约束下，系统研究参数高效微调的设计空间，开发一种既能保持高性能又能在推理时实现零延迟合并的方法。

Method: 结合带符号的低秩残差更新（加法和减法组件）以及训练时的列向投影，保持每列接近原始范数，训练后将适配器合并为单一权重矩阵。

Result: 在8个问答和阅读理解基准测试中平均准确率达到76.4%，仅使用每个任务5k训练样本和2个epoch；相比LoRA提升2.2个百分点，相比DoRA在大多数任务上表现相当或更好；在生成任务上也有提升，训练波动降低36%。

Conclusion: D2-LoRA在有限数据和计算约束下实现了高性能的参数高效微调，通过创新的架构设计而非单纯增加参数化，同时保持了推理时的代数可合并性和数值等价性。

Abstract: We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.

</details>


### [256] [Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training](https://arxiv.org/abs/2602.14759)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.LG

TL;DR: 提出"推理时内部循环"方法，通过重复应用预训练语言模型中的选定块范围来延长细化过程，从而在冻结模型中获得额外计算和性能提升


<details>
  <summary>Details</summary>
Motivation: 基于Transformer架构中残差连接将内部表示视为迭代细化的观点，以及早期解码和细化层假设，探索通过延长细化过程来提升预训练模型性能

Method: 在推理阶段对预训练语言模型中的选定块范围进行重复应用，形成内部循环，延长细化过程而不改变模型参数

Result: 在多个基准测试中，内部循环方法带来了适度但一致的准确性提升，潜在轨迹分析显示更稳定的状态演化和持续的语义细化

Conclusion: 通过简单的测试时循环可以在冻结的预训练模型中获得额外的细化效果，扩展计算而不改变模型参数

Abstract: Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.

</details>


### [257] [Universal Algorithm-Implicit Learning](https://arxiv.org/abs/2602.14761)
*Stefano Woerner,Seong Joon Oh,Christian F. Baumgartner*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来形式化定义元学习的实用性，并介绍了TAIL——一种基于Transformer的算法隐式元学习器，能够处理不同领域、模态和标签配置的任务，在少样本学习基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前元学习方法局限于狭窄的任务分布和固定的特征/标签空间，限制了应用范围。同时，元学习文献中"通用"和"通用目的"等术语使用不一致且缺乏精确定义，阻碍了方法间的可比性。

Method: 提出了一个理论框架来形式化定义实用通用性，区分算法显式和算法隐式学习。基于此框架开发了TAIL：一种基于Transformer的算法隐式元学习器，包含三个创新：1) 跨模态特征编码的随机投影；2) 可扩展到更大标签空间的随机注入标签嵌入；3) 高效的在线查询处理。

Result: TAIL在标准少样本学习基准上达到最先进性能，能够泛化到未见领域和模态（例如仅用图像训练却能解决文本分类任务），处理训练时未见过的多达20倍类别数的任务，并且计算效率比先前基于Transformer的方法高出数量级。

Conclusion: 提出的理论框架为通用元学习提供了原则性词汇和形式化定义，而TAIL展示了算法隐式元学习在处理多样化任务分布方面的强大能力，为更通用的元学习系统铺平了道路。

Abstract: Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like "universal" and "general-purpose" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.

</details>


### [258] [On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials](https://arxiv.org/abs/2602.14789)
*Rotem Mulayoff,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 该论文研究了优化算法训练过程中迭代的动态稳定性，特别关注非线性项对梯度下降和随机梯度下降稳定性的影响，揭示了线性化分析可能产生误导性结论。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解训练过程中迭代的动态稳定性如何决定优化算法找到的最小值。虽然先前工作常依赖线性化来确定稳定性，但线性化动态是否真实捕捉完整非线性行为仍不清楚。最近研究表明梯度下降可能在线性不稳定最小值附近稳定振荡，这表明线性分析可能具有误导性。

Method: 方法包括：1）推导多元设置中梯度下降在最小值附近稳定振荡的精确准则，该条件依赖于高阶导数，推广了现有结果；2）将分析扩展到随机梯度下降，研究非线性动态如何在期望中发散；3）证明如果所有批次都线性稳定，则SGD的非线性动态在期望中是稳定的。

Result: 主要结果：1）建立了梯度下降在最小值附近稳定振荡的精确非线性准则；2）发现SGD的非线性动态可能在期望中发散，即使单个批次不稳定；3）证明稳定性可能由单个不稳定振荡的批次决定，而不是线性分析所建议的平均效应；4）证明了如果所有批次都线性稳定，SGD的非线性动态在期望中是稳定的。

Conclusion: 结论：非线性项在确定优化算法的稳定性方面起着关键作用，线性化分析可能产生误导。研究提供了理解梯度下降和随机梯度下降非线性动态稳定性的理论框架，揭示了单个不稳定批次可能主导SGD整体稳定性的机制。

Abstract: The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.

</details>


### [259] [Learning State-Tracking from Code Using Linear RNNs](https://arxiv.org/abs/2602.14814)
*Julien Siems,Riccardo Grazzi,Kirill Kalinin,Hitesh Ballani,Babak Rahmani*

Main category: cs.LG

TL;DR: 论文将排列组合任务转化为代码REPL跟踪形式，发现线性RNN能有效处理状态跟踪，而Transformer仍失败，并探讨了代码中状态跟踪困难的原因。


<details>
  <summary>Details</summary>
Motivation: 现有状态跟踪任务（特别是排列组合）通常采用序列到序列形式，与语言模型常用的下一词预测设置不兼容，需要填补这一空白。

Method: 通过REPL跟踪将排列组合转化为代码形式，交织状态显示和变量转换，并建立概率有限状态自动机框架来分析状态跟踪难度。

Result: 线性RNN在代码设置下仍能有效进行状态跟踪，而Transformer失败；在部分可观察动作场景中，线性RNN可能比非线性RNN表现更差。

Conclusion: 代码中的状态跟踪具有挑战性，因为动作并非总是完全可观察的；不同架构在处理状态跟踪任务时存在显著差异。

Abstract: Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.

</details>


### [260] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出Interactionless Inverse Reinforcement Learning方法，将对齐学习与策略优化解耦，创建可检查、可编辑、模型无关的奖励模型，并通过Alignment Flywheel循环迭代强化对齐效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在结构缺陷，将安全目标与智能体策略纠缠在一起。RLHF和DPO等方法产生不透明、单次使用的对齐产物（称为Alignment Waste），需要更可持续的对齐架构。

Method: 提出Interactionless Inverse Reinforcement Learning（无交互逆强化学习），将对齐产物学习与策略优化解耦，生成可检查、可编辑、模型无关的奖励模型。同时引入Alignment Flywheel（对齐飞轮）循环，通过人工参与、自动化审计和精炼迭代强化奖励模型。

Result: 该方法将安全从一次性消耗转变为持久、可验证的工程资产，创建了可检查、可编辑的对齐产物，支持模型无关的部署。

Conclusion: 提出的架构解决了当前对齐方法的结构缺陷，通过解耦对齐学习与策略优化，以及建立迭代强化机制，使AI安全成为可持续、可验证的工程实践。

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [261] [A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers](https://arxiv.org/abs/2602.14855)
*Ryan DeWolfe,Paweł Prałat,François Théberge*

Main category: cs.LG

TL;DR: 提出了一种用于比较包含重叠聚类和异常值的聚类结果的相似性度量方法


<details>
  <summary>Details</summary>
Motivation: 现有的聚类比较方法无法处理包含异常值（不属于任何簇的对象）和重叠聚类（对象可能属于多个簇）的情况，这在实际应用中很常见但缺乏相应的评估工具

Method: 定义了一种实用的相似性度量方法，专门用于比较包含重叠和异常值的聚类结果，该方法具有多个理想特性

Result: 实验验证表明，该方法不受其他聚类比较度量常见的多种偏差影响

Conclusion: 提出的相似性度量方法填补了聚类评估领域的空白，为包含异常值和重叠聚类的场景提供了有效的比较工具

Abstract: Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.

</details>


### [262] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: Goldilocks是一种教师驱动的数据采样策略，通过预测每个问题对学生模型的难度，选择"刚刚好"难度的问题进行训练，提高强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能解锁大语言模型的推理能力，但依赖稀疏奖励导致样本效率低下。传统的课程学习通过按复杂度排序数据来缓解这个问题，但对特定模型的最佳排序往往不明确。

Method: 提出Goldilocks策略：教师模型预测每个问题对学生模型的难度，选择既不太简单也不太困难的问题（Goldilocks原则），同时用GRPO训练学生模型。教师模型根据学生在已见样本上的表现持续适应其能力变化。

Result: 在OpenMathReasoning数据集上，Goldilocks数据采样在相同计算预算下，比标准GRPO训练提高了模型性能。

Conclusion: Goldilocks策略通过动态适应学生模型能力的教师驱动数据采样，有效提高了强化学习训练大语言模型的样本效率。

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [263] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: RLVR（可验证奖励的强化学习）通过平滑难度谱实现持续改进，而突变的难度谱会导致学习停滞和阶段性突破。


<details>
  <summary>Details</summary>
Motivation: 理解仅基于最终结果的奖励如何帮助克服长视野推理障碍，揭示RLVR在大型推理模型中成功的内在机制。

Method: 开发了Transformer在组合推理任务上的RL训练动态理论，使用有限群上的傅里叶分析工具，并通过合成实验验证预测机制。

Result: RLVR的有效性受难度谱平滑性支配：平滑难度谱产生接力效应，实现稳定持续改进；突变难度谱导致grokking型相变，产生学习停滞。

Conclusion: RLVR通过提升模型在能力边缘的性能发挥作用，适当设计的数据混合可以产生可扩展的收益，平滑难度谱是实现持续学习的关键。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [264] [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](https://arxiv.org/abs/2602.14913)
*Farbod Siahkali,Ashwin Verma,Vijay Gupta*

Main category: cs.LG

TL;DR: 该论文提出使用伪校准方法来应对分布偏移下保形预测性能下降的问题，通过领域自适应工具推导目标覆盖率的理论下界，并提出源调谐伪校准算法来缓解覆盖退化。


<details>
  <summary>Details</summary>
Motivation: 保形预测在数据分布偏移时可能失效，需要开发能够应对分布变化的方法来维持预测性能。

Method: 使用伪校准作为工具，基于有界标签条件协变量偏移模型，利用领域自适应工具推导目标覆盖率下界，设计通过松弛参数膨胀保形阈值的伪校准集，并提出源调谐伪校准算法。

Result: 理论推导出目标覆盖率下界与分类器源域损失和Wasserstein偏移度量相关，数值实验显示边界能定性跟踪伪校准行为，源调谐方案能缓解分布偏移下的覆盖退化同时保持非平凡预测集大小。

Conclusion: 伪校准是应对分布偏移下保形预测性能下降的有效工具，源调谐伪校准算法在实际应用中能平衡覆盖率和预测集大小。

Abstract: Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.

</details>


### [265] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: 本文证明在离策略评估中，使用最优加性基线（β*-IPS）的估计器在均方误差上渐近优于自归一化逆倾向评分（SNIPS），为推荐和排序系统评估提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 离策略评估对于评估推荐和排序系统至关重要，但现有方法中自归一化逆倾向评分（SNIPS）虽然通过乘性控制变量减少方差，而加性控制变量（基线修正）在离策略学习中表现出更好性能，但缺乏理论保证。

Method: 提出β*-IPS估计器，使用最优加性基线，通过理论分析证明其在均方误差上渐近优于SNIPS，并解析分解方差差距，展示SNIPS等价于使用特定但通常次优的加性基线。

Result: 证明β*-IPS在均方误差上渐近主导SNIPS，为从自归一化转向最优基线修正提供了理论依据，适用于推荐和排序系统的离策略评估。

Conclusion: 研究结果为推荐和排序系统的离策略评估提供了理论支持，表明应该从自归一化方法转向最优加性基线修正，以获得更好的评估性能。

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [266] [Variance-Reduced $(\varepsilon,δ)-$Unlearning using Forget Set Gradients](https://arxiv.org/abs/2602.14938)
*Martin Van Waerebeke,Marco Lorenzi,Kevin Scaman,El Mahdi El Mhamdi,Giovanni Neglia*

Main category: cs.LG

TL;DR: 本文提出了VRU算法，这是第一个在更新规则中直接包含遗忘集梯度，同时可证明满足(ε,δ)-遗忘保证的一阶算法，相比现有方法有更好的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有(ε,δ)-遗忘方法仅使用遗忘集来校准注入的噪声，从未将其作为直接优化信号；而高效的启发式方法虽然利用遗忘样本（如梯度上升），但缺乏形式化遗忘保证。需要填补这一空白。

Method: 提出了方差减少遗忘（VRU）算法，这是一种一阶算法，在更新规则中直接包含遗忘集梯度，同时通过理论证明满足(ε,δ)-遗忘保证。

Result: VRU算法收敛性得到证明，相比现有一阶(ε,δ)-遗忘方法有更优的收敛速率（对误差的依赖更好）。在低误差状态下，VRU渐近地优于任何忽略遗忘集的一阶方法。

Conclusion: VRU算法成功地将遗忘集梯度直接纳入优化过程，同时保持形式化遗忘保证，填补了理论保证方法与高效启发式方法之间的空白，实验验证了其优于现有方法。

Abstract: In machine unlearning, $(\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.

</details>


### [267] [Locally Adaptive Multi-Objective Learning](https://arxiv.org/abs/2602.14952)
*Jivat Neet Kaur,Isaac Gibbs,Michael I. Jordan*

Main category: cs.LG

TL;DR: 提出一种在线多目标学习新方法，通过自适应算法实现局部适应性，在分布漂移下保持稳健性


<details>
  <summary>Details</summary>
Motivation: 现有多目标学习方法在分布随时间任意变化时缺乏适应性，无法有效应对分布漂移，需要改进局部适应能力

Method: 将多目标学习方法中的一部分替换为自适应在线算法，实现局部适应性，在连续子区间上提供保障

Result: 在能源预测和算法公平数据集上的实验表明，该方法优于现有方法，能在子组间实现无偏预测，并在分布漂移下保持稳健

Conclusion: 提出的自适应多目标学习方法能有效应对分布漂移，提高局部适应性，在实际应用中具有优势

Abstract: We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.

</details>


### [268] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 本文提出了一种在因果基础模型中融入领域知识的方法，通过将因果信息（如因果图或祖先信息）作为条件输入，使通用CFM能够匹配针对特定因果结构训练的专用模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前因果基础模型（CFMs）虽然统一了因果发现和推断，但无法融入领域知识，导致预测效果不理想。本文旨在解决这一关键障碍，使CFM能够在数据驱动的同时有效利用任何程度的领域专业知识。

Method: 引入将因果信息（包括完整因果图、部分因果信息或更易获得的祖先信息）作为条件输入CFM的方法。系统评估了多种条件化策略，发现将可学习的偏置注入注意力机制是利用完整和部分因果信息的最有效方法。

Result: 实验表明，通过这种条件化方法，通用CFM能够匹配针对特定因果结构训练的专用模型的性能。该方法有效利用了完整和部分因果信息，使CFM在保持通用性的同时提升了预测准确性。

Conclusion: 本文提出的条件化方法解决了因果基础模型发展中的核心障碍，使模型能够在数据驱动的同时有效利用领域知识，为实现"一体化"因果基础模型迈出了重要一步。

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [269] [MacroGuide: Topological Guidance for Macrocycle Generation](https://arxiv.org/abs/2602.14977)
*Alicja Maksymiuk,Alexandre Duplessis,Michael Bronstein,Alexander Tong,Fernanda Duarte,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: MacroGuide是一种扩散引导机制，利用持久同调学指导预训练分子生成模型生成大环化合物，将大环生成率从1%提升到99%


<details>
  <summary>Details</summary>
Motivation: 大环化合物因其对困难靶点具有增强的选择性和结合亲和力而成为有前景的药物替代品，但由于公共数据集中稀缺以及标准深度生成模型中拓扑约束难以实施，在生成建模中尚未得到充分探索

Method: MacroGuide是一种扩散引导机制，使用持久同调学指导预训练分子生成模型的采样过程。在每个去噪步骤中，从原子位置构建Vietoris-Rips复形，并通过优化持久同调特征促进环形成

Result: 将MacroGuide应用于预训练扩散模型后，大环化合物的生成率从1%提高到99%，同时在化学有效性、多样性和PoseBusters检查等关键质量指标上达到或超过最先进水平

Conclusion: MacroGuide通过拓扑引导成功解决了大环化合物生成中的挑战，为生成具有复杂拓扑结构的分子提供了有效方法

Abstract: Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.

</details>


### [270] [Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations](https://arxiv.org/abs/2602.14983)
*Carolin Cissee,Raneen Younis,Zahra Ahmadi*

Main category: cs.LG

TL;DR: COrAL是一个多模态学习框架，通过正交约束和不对称掩码技术，同时保留冗余、独特和协同信息，实现更稳定全面的表征学习。


<details>
  <summary>Details</summary>
Motivation: 现有自监督多模态对比学习方法主要捕捉冗余的跨模态信号，往往忽视模态特定（独特）信息和交互驱动（协同）信息，导致表征不完整和潜在信息泄露。

Method: 采用双路径架构配合正交约束来解耦共享和模态特定特征；引入具有互补视图特定模式的不对称掩码，强制模型推断跨模态依赖关系而非仅依赖冗余线索。

Result: 在合成基准和多样化MultiBench数据集上的实验表明，COrAL始终匹配或优于最先进方法，同时表现出较低的运行间性能方差。

Conclusion: 显式建模完整的多模态信息谱系能够产生更稳定、可靠和全面的嵌入表示。

Abstract: Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.

</details>


### [271] [Spectral Convolution on Orbifolds for Geometric Deep Learning](https://arxiv.org/abs/2602.14997)
*Tim Mangliers,Bernhard Mössner,Benjamin Himpel*

Main category: cs.LG

TL;DR: 该论文将几何深度学习中的谱卷积概念扩展到轨道流形，为处理轨道流形结构数据提供基础构建模块，并以音乐理论为例进行说明。


<details>
  <summary>Details</summary>
Motivation: 几何深度学习主要处理图或流形等非欧几里得结构数据，但实际应用中存在更多拓扑和几何结构需要被机器学习方法所处理。轨道流形作为一种重要的数学结构，目前缺乏相应的深度学习工具。

Method: 论文引入了轨道流形上的谱卷积概念，将其作为几何深度学习的基础构建模块，类似于传统CNN中的卷积操作，但适用于轨道流形结构的数据。

Result: 成功建立了轨道流形上的谱卷积理论框架，为处理轨道流形结构数据提供了数学基础，并通过音乐理论的例子展示了该方法的实际应用潜力。

Conclusion: 轨道流形上的谱卷积扩展了几何深度学习的应用范围，使得能够处理更广泛的拓扑和几何结构数据，为相关领域的机器学习应用提供了新的理论工具。

Abstract: Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.

</details>


### [272] [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
*Xander Davies,Giorgi Giglemiani,Edmund Lau,Eric Winsor,Geoffrey Irving,Yarin Gal*

Main category: cs.LG

TL;DR: BPJ是一种新型自动化越狱攻击方法，通过边界点检测和课程学习策略，仅使用单比特分类器反馈就能绕过最强行业部署的安全防护系统。


<details>
  <summary>Details</summary>
Motivation: 当前前沿LLMs通过基于分类器的防护系统抵御越狱攻击，这些系统已经过数千小时的人工红队测试。然而，现有攻击方法要么需要白盒/灰盒假设（如分类器分数或梯度），要么依赖现有越狱库。需要一种完全黑盒、仅使用最少信息的自动化攻击方法来突破最强大的现实世界防御。

Method: BPJ采用边界点越狱攻击方法：1）将目标有害字符串转换为中间攻击目标的课程；2）主动选择最能检测攻击强度微小变化的评估点（边界点）；3）仅使用分类器是否标记交互的单比特信息进行完全黑盒优化。

Result: BPJ是首个成功开发针对宪法分类器通用越狱的完全自动化攻击算法，也是首个在不依赖人类攻击种子情况下成功攻击GPT-5输入分类器的自动化算法。该方法在优化过程中会触发许多标记，但在单个交互中难以防御。

Conclusion: BPJ展示了仅使用最少信息就能绕过最强行业防护系统的可能性。有效防御需要补充单交互方法，增加批量级监控，因为攻击在优化阶段会留下可检测的模式。

Abstract: Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength ("boundary points"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.

</details>


### [273] [PDE foundation models are skillful AI weather emulators for the Martian atmosphere](https://arxiv.org/abs/2602.15004)
*Johannes Schmude,Sujit Roy,Liping Wang,Theodore van Kessel,Levente Klein,Marcus Freitag,Eloisa Bentivegna,Robert Manson-Sawko,Bjorn Lutjens,Manil Maskey,Campbell Watson,Rahul Ramachandran,Juan Bernabe-Moreno*

Main category: cs.LG

TL;DR: 该研究展示了基于偏微分方程预训练的基础模型可以迁移到火星大气预测任务，通过将2D模型扩展到3D并利用少量数据微调，在计算资源有限的情况下取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何将预训练在偏微分方程数值解上的基础模型应用于实际复杂系统，特别是针对训练数据有限或计算预算不足的真实世界问题，如火星大气预测。

Method: 方法基于Poseidon PDE基础模型，开发了从二维扩展到三维的技术，同时保留预训练信息。研究还探讨了模型在稀疏初始条件下的性能，使用了约34GB的火星大气数据（4个火星年）和13GPU小时的中等计算预算进行微调。

Result: 结果显示，预训练与模型扩展相结合的方法在保留测试年上的性能提升了34.4%，证明了PDE基础模型不仅能近似其他PDE的解，还能作为缺乏足够训练数据或合适计算预算的复杂现实问题的锚定模型。

Conclusion: 结论表明PDE基础模型具有强大的迁移能力，能够有效应用于实际复杂系统预测，特别是在数据有限和计算资源受限的情况下，为科学计算和工程应用提供了新的解决方案。

Abstract: We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.

</details>


### [274] [Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees](https://arxiv.org/abs/2602.15008)
*Daniil Dmitriev,Zhihan Huang,Yuting Wei*

Main category: cs.LG

TL;DR: 该论文研究了基于连续时间马尔可夫链的离散扩散模型的采样效率，针对τ-leaping采样器建立了KL散度收敛保证，在均匀扩散中消除了对词汇大小的线性依赖，在掩码扩散中引入了有效总相关度量实现自适应收敛。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型虽然取得了显著的经验成功，但其理论基础仍不完整。本文旨在研究基于连续时间马尔可夫链的离散扩散模型的采样效率，特别是τ-leaping采样器的收敛性能，填补理论空白。

Method: 采用连续时间马尔可夫链框架分析离散扩散模型，主要研究τ-leaping采样器。对于均匀离散扩散，分析标准τ-leaping算法；对于掩码离散扩散，引入改进的τ-leaping采样器。分析基于分数熵损失控制，无需有界性或平滑性假设。

Result: 对于均匀离散扩散，τ-leaping算法达到迭代复杂度Õ(d/ε)，消除了对词汇大小S的线性依赖，比现有界限改进d倍，并建立了匹配的算法下界。对于掩码离散扩散，改进采样器的收敛率由有效总相关控制，该量有界于d log S但对结构化数据可以是亚线性甚至常数，从而自适应实现亚线性收敛。

Conclusion: 本文为离散扩散模型建立了严格的收敛理论，τ-leaping采样器在均匀扩散中达到最优维度依赖，在掩码扩散中通过有效总相关自适应利用数据结构，为实际应用提供了理论保证和效率提升。

Abstract: Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.

</details>


### [275] [Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation](https://arxiv.org/abs/2602.15022)
*Cai Zhou,Zijie Chen,Zian Li,Jike Wang,Kaiyi Jiang,Pan Li,Rose Yu,Muhan Zhang,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 本文提出了一种新的生成建模方法——规范扩散，通过将样本映射到轨道代表元（规范姿态或顺序），在规范切片上训练无约束扩散模型，然后在生成时应用随机对称变换，从而处理化学和科学中具有群对称性的分布。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过架构约束（如等变去噪器和不变先验）来强制对称性不变性和等变性。本文挑战这一传统，提出从规范化的替代视角来处理对称不变分布，旨在提高表达能力和训练效率。

Method: 采用规范扩散框架：1）将每个样本映射到轨道代表元（规范姿态或顺序）；2）在规范切片上训练无约束（非等变）扩散或流模型；3）在生成时通过随机对称变换恢复不变分布。基于商空间理论，结合对齐先验和最优传输进一步改进训练效率。在分子图生成中实例化该框架，利用基于几何谱的规范化和温和的位置编码。

Result: 规范扩散在3D分子生成任务中显著优于等变基线方法，计算量相似甚至更少。提出的新颖架构CanonFlow在具有挑战性的GEOM-DRUG数据集上实现了最先进的性能，在少步生成中优势尤为明显。

Conclusion: 规范扩散为处理对称不变分布提供了一种有效的新范式，通过理论证明和实验验证展示了其在表达能力、训练效率和生成性能方面的优势，为化学和科学中的生成任务提供了有前景的解决方案。

Abstract: Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.

</details>


### [276] [Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization](https://arxiv.org/abs/2602.15028)
*Shangding Gu*

Main category: cs.LG

TL;DR: 论文提出了PAPerBench基准，系统研究上下文长度对LLM个性化质量和隐私保护的影响，发现随着上下文增长，性能和隐私保护均下降，揭示了注意力稀释问题。


<details>
  <summary>Details</summary>
Motivation: LLM在隐私敏感和个性化场景中部署日益增多，但上下文长度对隐私泄露和个性化效果的影响尚未得到系统研究，需要建立基准来探索这一关键问题。

Method: 构建了包含约29,000个实例、上下文长度从1K到256K token的PAPerBench基准，共377K个评估问题，联合评估个性化性能和隐私风险，并进行注意力稀释的理论分析。

Result: 实验发现随着上下文长度增加，所有先进LLM的个性化性能和隐私保护均出现一致下降；理论分析表明这是固定容量Transformer中soft attention的固有局限性。

Conclusion: 当前模型存在普遍的扩展差距：长上下文导致注意力分散；基准的发布支持可重复评估和未来可扩展隐私与个性化研究。

Abstract: Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [277] [Agent Mars: Multi-Agent Simulation for Multi-Planetary Life Exploration and Settlement](https://arxiv.org/abs/2602.13291)
*Ziyang Wang*

Main category: cs.MA

TL;DR: Agent Mars是一个用于火星基地操作的多智能体仿真框架，包含93个智能体、7层指挥执行层级，支持分层协调、动态角色交接和阶段依赖领导，旨在解决太空探索中的安全关键系统协调挑战。


<details>
  <summary>Details</summary>
Motivation: 太空探索和定居面临地球环境无法比拟的约束：延迟/间歇通信、极端资源稀缺、异构专业知识、严格的安全性和指挥权威要求。核心挑战是在安全关键系统中实现人类、机器人和数字服务之间的可审计协调。

Method: 开发了Agent Mars框架，包含93个智能体的7层指挥执行组织结构，实现分层和跨层协调机制，支持动态角色交接和故障转移，采用场景感知记忆、可配置投票共识和协议翻译等关键机制。

Result: 通过13个可复现的火星相关操作脚本验证，揭示了协调权衡，并识别出经过策划的跨层协作和功能领导能够在不牺牲可靠性的情况下减少开销的机制。

Conclusion: Agent Mars为太空AI提供了一个可基准测试、可审计的基础框架，能够研究大规模基地操作中的协调问题，超越了玩具设置的限制。

Abstract: Artificial Intelligence (AI) has transformed robotics, healthcare, industry, and scientific discovery, yet a major frontier may lie beyond Earth. Space exploration and settlement offer vast environments and resources, but impose constraints unmatched on Earth: delayed/intermittent communications, extreme resource scarcity, heterogeneous expertise, and strict safety, accountability, and command authority. The key challenge is auditable coordination among specialised humans, robots, and digital services in a safety-critical system-of-systems. We introduce Agent Mars, an open, end-to-end multi-agent simulation framework for Mars base operations. Agent Mars formalises a realistic organisation with a 93-agent roster across seven layers of command and execution (human roles and physical assets), enabling base-scale studies beyond toy settings. It implements hierarchical and cross-layer coordination that preserves chain-of-command while allowing vetted cross-layer exchanges with audit trails; supports dynamic role handover with automatic failover under outages; and enables phase-dependent leadership for routine operations, emergencies, and science campaigns. Agent Mars further models mission-critical mechanisms-scenario-aware short/long-horizon memory, configurable propose-vote consensus, and translator-mediated heterogeneous protocols-to capture how teams align under stress. To quantify behaviour, we propose the Agent Mars Performance Index (AMPI), an interpretable composite score with diagnostic sub-metrics. Across 13 reproducible Mars-relevant operational scripts, Agent Mars reveals coordination trade-offs and identifies regimes where curated cross-layer collaboration and functional leadership reduce overhead without sacrificing reliability. Agent Mars provides a benchmarkable, auditable foundation for Space AI.

</details>


### [278] [Adaptive Value Decomposition: Coordinating a Varying Number of Agents in Urban Systems](https://arxiv.org/abs/2602.13309)
*Yexin Li,Jinjin Guo,Haoyu Zhang,Yuhan Zhao,Yiwen Sun,Zihao Jiao*

Main category: cs.MA

TL;DR: AVD是一个自适应价值分解框架，用于处理动态变化的多智能体系统，通过轻量级机制缓解共享策略导致的行动同质化问题，并在半MARL设置中支持异步决策。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法依赖固定智能体数量和完全同步行动执行的限制性假设，这在城市系统中常常被违反。同时，共享策略参数虽然提高学习效率，但在相似观测下可能导致行动同质化，降低协调质量。

Method: 提出自适应价值分解（AVD）框架，包含：1）适应动态变化智能体数量的机制；2）轻量级机制缓解共享策略导致的行动同质化；3）针对半MARL设置的训练-执行策略，支持异步决策。

Result: 在伦敦和华盛顿特区的真实世界共享单车再分配任务实验中，AVD优于最先进的基线方法，证实了其有效性和泛化能力。

Conclusion: AVD成功解决了动态多智能体系统中的协调问题，通过自适应价值分解和行动多样化机制，在半MARL设置中实现了更有效的合作。

Abstract: Multi-agent reinforcement learning (MARL) provides a promising paradigm for coordinating multi-agent systems (MAS). However, most existing methods rely on restrictive assumptions, such as a fixed number of agents and fully synchronous action execution. These assumptions are often violated in urban systems, where the number of active agents varies over time, and actions may have heterogeneous durations, resulting in a semi-MARL setting. Moreover, while sharing policy parameters among agents is commonly adopted to improve learning efficiency, it can lead to highly homogeneous actions when a subset of agents make decisions concurrently under similar observations, potentially degrading coordination quality. To address these challenges, we propose Adaptive Value Decomposition (AVD), a cooperative MARL framework that adapts to a dynamically changing agent population. AVD further incorporates a lightweight mechanism to mitigate action homogenization induced by shared policies, thereby encouraging behavioral diversity and maintaining effective cooperation among agents. In addition, we design a training-execution strategy tailored to the semi-MARL setting that accommodates asynchronous decision-making when some agents act at different times. Experiments on real-world bike-sharing redistribution tasks in two major cities, London and Washington, D.C., demonstrate that AVD outperforms state-of-the-art baselines, confirming its effectiveness and generalizability.

</details>


### [279] [PeroMAS: A Multi-agent System of Perovskite Material Discovery](https://arxiv.org/abs/2602.13312)
*Yishu Wang,Wei Liu,Yifan Li,Shengxiang Xu,Xujie Yuan,Ran Li,Yuyu Luo,Jia Zhu,Shimin Di,Min-Ling Zhang,Guixiang Li*

Main category: cs.MA

TL;DR: PeroMAS是一个用于钙钛矿材料发现的多智能体系统，通过封装专业工具实现从文献检索到实验合成的端到端优化，相比传统方法显著提升发现效率。


<details>
  <summary>Details</summary>
Motivation: 钙钛矿太阳能电池开发过程复杂且涉及多个闭环工作流程，现有AI方法主要关注离散模型（如材料设计、工艺优化、性能预测），无法在工作流程中传播物理约束，阻碍端到端优化。

Method: 提出PeroMAS多智能体系统，将一系列钙钛矿专用工具封装为模型上下文协议（MCPs），通过规划和调用这些工具，在多目标约束下设计钙钛矿材料，覆盖从文献检索、数据提取到性能预测和机理分析的完整流程。

Result: 相比单一大型语言模型或传统搜索策略，PeroMAS显著提升了发现效率，成功识别出满足多目标约束的候选材料，并通过真实合成实验验证了系统在物理世界中的有效性。

Conclusion: PeroMAS多智能体系统能够有效解决钙钛矿材料发现中的端到端优化问题，通过集成专业工具和工作流程传播物理约束，为钙钛矿太阳能电池开发提供了更高效的解决方案。

Abstract: As a pioneer of the third-generation photovoltaic revolution, Perovskite Solar Cells (PSCs) are renowned for their superior optoelectronic performance and cost potential. The development process of PSCs is precise and complex, involving a series of closed-loop workflows such as literature retrieval, data integration, experimental design, and synthesis. However, existing AI perovskite approaches focus predominantly on discrete models, including material design, process optimization,and property prediction. These models fail to propagate physical constraints across the workflow, hindering end-to-end optimization. In this paper, we propose a multi-agent system for perovskite material discovery, named PeroMAS. We first encapsulated a series of perovskite-specific tools into Model Context Protocols (MCPs). By planning and invoking these tools, PeroMAS can design perovskite materials under multi-objective constraints, covering the entire process from literature retrieval and data extraction to property prediction and mechanism analysis. Furthermore, we construct an evaluation benchmark by perovskite human experts to assess this multi-agent system. Results demonstrate that, compared to single Large Language Model (LLM) or traditional search strategies, our system significantly enhances discovery efficiency. It successfully identified candidate materials satisfying multi-objective constraints. Notably, we verify PeroMAS's effectiveness in the physical world through real synthesis experiments.

</details>


### [280] [Robust Mean-Field Games with Risk Aversion and Bounded Rationality](https://arxiv.org/abs/2602.13353)
*Bhavini Jeloka,Yue Guan,Panagiotis Tsiotras*

Main category: cs.MA

TL;DR: 提出MF-RQE（均值场风险规避量化响应均衡）新概念，结合风险规避和有限理性，提升多智能体系统在分布不确定性和认知约束下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有均值场博弈方法通常假设固定的初始种群分布和完全理性智能体，限制了在分布不确定性和认知约束下的鲁棒性。需要解决这两个局限性。

Method: 引入对初始种群分布的风险规避机制，并纳入有限理性来建模智能体偏离完全理性决策的行为。结合这两个要素形成新的均衡概念MF-RQE，建立存在性证明和收敛性分析，开发可扩展的强化学习算法。

Result: 证明了MF-RQE的存在性，证明了定点迭代和虚拟博弈的收敛性。数值实验表明，相对于经典的均值场方法，MF-RQE策略在固定初始分布下优化期望累积奖励并仅限于基于熵的正则化器时，实现了更好的鲁棒性。

Conclusion: MF-RQE为大规模多智能体系统提供了一个更通用、更鲁棒的均衡框架，结合了风险规避和有限理性，能够更好地处理分布不确定性和认知约束。

Abstract: Recent advances in mean-field game literature enable the reduction of large-scale multi-agent problems to tractable interactions between a representative agent and a population distribution. However, existing approaches typically assume a fixed initial population distribution and fully rational agents, limiting robustness under distributional uncertainty and cognitive constraints. We address these limitations by introducing risk aversion with respect to the initial population distribution and by incorporating bounded rationality to model deviations from fully rational decision-making agents. The combination of these two elements yields a new and more general equilibrium concept, which we term the mean-field risk-averse quantal response equilibrium (MF-RQE). We establish existence results and prove convergence of fixed-point iteration and fictitious play to MF-RQE. Building on these insights, we develop a scalable reinforcement learning algorithm for scenarios with large state-action spaces. Numerical experiments demonstrate that MF-RQE policies achieve improved robustness relative to classical mean-field approaches that optimize expected cumulative rewards under a fixed initial distribution and are restricted to entropy-based regularizers.

</details>


### [281] [G2CP: A Graph-Grounded Communication Protocol for Verifiable and Efficient Multi-Agent Reasoning](https://arxiv.org/abs/2602.13370)
*Karim Ben Khaled,Davy Monticolo*

Main category: cs.MA

TL;DR: G2CP是一种基于图结构的智能体通信协议，用图操作代替自然语言，显著减少通信开销、提高准确性并消除幻觉传播。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统面临语义漂移、幻觉传播和低效令牌消耗等关键挑战，需要更精确的结构化通信方式。

Method: 提出G2CP（图基通信协议），智能体通过图操作（遍历命令、子图片段、更新操作）在共享知识图上进行结构化通信，而非自由文本。

Result: 在500个工业场景和21个实际维护案例中，G2CP减少73%的智能体间通信令牌，提高34%的任务完成准确率，消除级联幻觉，产生完全可审计的推理链。

Conclusion: G2CP代表了从语言通信到结构化通信的根本转变，对需要精确智能体协调的任何领域都有重要意义。

Abstract: Multi-agent systems powered by Large Language Models face a critical challenge: agents communicate through natural language, leading to semantic drift, hallucination propagation, and inefficient token consumption. We propose G2CP (Graph-Grounded Communication Protocol), a structured agent communication language where messages are graph operations rather than free text. Agents exchange explicit traversal commands, subgraph fragments, and update operations over a shared knowledge graph, enabling verifiable reasoning traces and eliminating ambiguity. We validate G2CP within an industrial knowledge management system where specialized agents (Diagnostic, Procedural, Synthesis, and Ingestion) coordinate to answer complex queries. Experimental results on 500 industrial scenarios and 21 real-world maintenance cases show that G2CP reduces inter-agent communication tokens by 73%, improves task completion accuracy by 34% over free-text baselines, eliminates cascading hallucinations, and produces fully auditable reasoning chains. G2CP represents a fundamental shift from linguistic to structural communication in multi-agent systems, with implications for any domain requiring precise agent coordination. Code, data, and evaluation scripts are publicly available.

</details>


### [282] [MAS-on-the-Fly: Dynamic Adaptation of LLM-based Multi-Agent Systems at Test Time](https://arxiv.org/abs/2602.13671)
*Guangyi Liu,Haojun Lin,Huan Zeng,Heng Wang,Quanming Yao*

Main category: cs.MA

TL;DR: MASFly是一个基于LLM的多智能体框架，通过检索增强的SOP实例化和经验引导的监督机制，实现测试时的动态自适应，在复杂任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统通常依赖手动设计或"一刀切"的自动化方法，缺乏部署后的动态适应性。受生物系统自适应能力的启发，需要开发能够动态适应的多智能体框架。

Method: MASFly采用双机制：1）检索增强的SOP实例化机制，利用自构建的成功协作模式库，为新的查询组装定制化多智能体系统；2）经验引导的监督机制，专门的Watcher智能体监控系统行为，参考个性化经验池提供实时干预。

Result: 在广泛实验中，MASFly实现了最先进的性能，特别是在TravelPlanner基准测试中达到61.7%的成功率，同时展现出强大的任务适应性和鲁棒性。

Conclusion: MASFly通过动态自适应机制显著提升了多智能体系统的性能，为复杂任务解决提供了有效的自适应框架，在多个基准测试中表现出色。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) have emerged as a promising paradigm for solving complex tasks. However, existing works often rely on manual designs or "one-size-fits-all" automation, lacking dynamic adaptability after deployment. Inspired by how biological systems adapt, we introduce MASFly, a novel multi-agent framework enabling dynamic adaptation at test time. To adapt system generation, MASFly employs a retrieval-augmented SOP instantiation mechanism that leverages a self-constructed repository of successful collaboration patterns, enabling the LLM to assemble customized MASs for new queries. For adaptive execution, MASFly incorporates an experience-guided supervision mechanism, where a dedicated Watcher agent monitors system behaviors with reference to a personalized experience pool and provides real-time interventions. Extensive experiments demonstrate that MASFly achieves state-of-the-art performance, most notably a 61.7% success rate on the TravelPlanner benchmark, while exhibiting strong task adaptability and robustness.

</details>


### [283] [Testing BDI-based Multi-Agent Systems using Discrete Event Simulation](https://arxiv.org/abs/2602.13878)
*Martina Baiardi,Samuele Burattini,Giovanni Ciatto,Danilo Pianini*

Main category: cs.MA

TL;DR: 本文探讨如何将BDI智能体映射到离散事件仿真中，实现仿真测试环境，解决认知智能体模型在仿真中的保真度问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在开放、分布式环境中难以测试，特别是对于BDI等认知智能体模型，其代码库不适合直接在仿真环境中运行，导致部署系统与仿真系统之间存在现实差距。需要让开发者能够在仿真中测试与部署相同的规范，而无需替代表示。

Method: 研究如何将BDI智能体的控制流映射到离散事件仿真中，展示这种集成可以在不同粒度级别实现。通过将两个现有工具（JaKtA和Alchemist）进行开源原型集成，构建分布式BDI智能体的仿真测试环境。

Result: 成功创建了基于仿真的分布式BDI智能体测试环境，证明将BDI智能体映射到离散事件仿真是可行的，并且不同粒度的映射可能导致不同程度的保真度。

Conclusion: 通过将BDI智能体控制流映射到离散事件仿真，可以实现仿真测试环境，使开发者能够测试与部署相同的规范，解决认知智能体模型在仿真中的保真度挑战。

Abstract: Multi-agent systems are designed to deal with open, distributed systems with unpredictable dynamics, which makes them inherently hard to test. The value of using simulation for this purpose is recognized in the literature, although achieving sufficient fidelity (i.e., the degree of similarity between the simulation and the real-world system) remains a challenging task. This is exacerbated when dealing with cognitive agent models, such as the Belief Desire Intention (BDI) model, where the agent codebase is not suitable to run unchanged in simulation environments, thus increasing the reality gap between the deployed and simulated systems. We argue that BDI developers should be able to test in simulation the same specification that will be later deployed, with no surrogate representations. Thus, in this paper, we discuss how the control flow of BDI agents can be mapped onto a Discrete Event Simulation (DES), showing that such integration is possible at different degrees of granularity. We substantiate our claims by producing an open-source prototype integration between two pre-existing tools (JaKtA and Alchemist), showing that it is possible to produce a simulation-based testing environment for distributed BDI} agents, and that different granularities in mapping BDI agents over DESs may lead to different degrees of fidelity.

</details>


### [284] [Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.14471)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.MA

TL;DR: SWA框架通过社会权重λ在个体目标与群体福利间权衡，在共享资源拥塞游戏中实现从持续拥塞到稳定运行的相变，无需参数更新或多智能体强化学习。


<details>
  <summary>Details</summary>
Motivation: 在共享环境中部署LLM智能体时，个体理性决策会产生负外部性，导致系统性能下降，需要在个体对齐与集体稳定性之间取得平衡。

Method: 提出社会加权对齐(SWA)框架，在推理时通过社会权重λ∈[0,1]在个体私有目标与群体福利估计之间进行插值，在共享资源拥塞游戏中分析其效果。

Result: 在n个智能体、拥塞严重程度β的共享资源游戏中，SWA诱导出临界阈值λ*=(n-β)/(n-1)，超过此阈值后智能体在过载时不再有增加需求的边际激励，实现从持续拥塞到稳定运行的相变。

Conclusion: SWA提供了一种无需参数更新或多智能体强化学习的推理时算法实现，通过社会权重调节个体与集体利益的平衡，能有效缓解共享环境中LLM智能体的负外部性问题。

Abstract: Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $λ\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $β$, we show that SWA induces a critical threshold $λ^*=(n-β)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.

</details>


### [285] [ST-EVO: Towards Generative Spatio-Temporal Evolution of Multi-Agent Communication Topologies](https://arxiv.org/abs/2602.14681)
*Xingjian Wu,Xvyuan Liu,Junkai Lu,Siyuan Wang,Yang Shu,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.MA

TL;DR: ST-EVO是一个基于LLM的多智能体系统，采用时空演化视角，通过流匹配调度器实现对话级通信调度，支持不确定性感知和自我反馈学习，在九个基准测试中取得5%-25%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前的自演化多智能体系统主要关注空间演化或时间演化的单一维度，未能充分激发LLM的协作能力，需要更灵活的时空演化方法来构建任务自适应的工作流和通信拓扑。

Method: 提出ST-EVO框架，采用时空演化视角，使用基于流匹配的紧凑而强大的调度器实现对话级通信调度，能够感知多智能体系统的不确定性，并具备从积累经验中学习的自我反馈能力。

Result: 在九个基准测试中展示了最先进的性能，实现了约5%-25%的准确率提升。

Conclusion: ST-EVO通过时空演化视角和流匹配调度器，有效提升了多智能体系统的协作能力，为自演化MAS提供了更灵活和强大的技术路线。

Abstract: LLM-powered Multi-Agent Systems (MAS) have emerged as an effective approach towards collaborative intelligence, and have attracted wide research interests. Among them, ``self-evolving'' MAS, treated as a more flexible and powerful technical route, can construct task-adaptive workflows or communication topologies, instead of relying on a predefined static structue template. Current self-evolving MAS mainly focus on Spatial Evolving or Temporal Evolving paradigm, which only considers the single dimension of evolution and does not fully incentivize LLMs' collaborative capability. In this work, we start from a novel Spatio-Temporal perspective by proposing ST-EVO, which supports dialogue-wise communication scheduling with a compact yet powerful flow-matching based Scheduler. To make precise Spatio-Temporal scheduling, ST-EVO can also perceive the uncertainty of MAS, and possesses self-feedback ability to learn from accumulated experience. Extensive experiments on nine benchmarks demonstrate the state-of-the-art performance of ST-EVO, achieving about 5%--25% accuracy improvement.

</details>


### [286] [ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic](https://arxiv.org/abs/2602.14780)
*Anna-Lena Schlamp,Jeremias Gerner,Klaus Bogenberger,Werner Huber,Stefanie Schmidtner*

Main category: cs.MA

TL;DR: ROSA系统结合多智能体轨迹预测与协调速度引导，为环岛多模态混合交通提供优化速度建议，提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 环岛作为复杂的交通场景，存在车辆与弱势道路使用者（VRUs）的混合交通，需要有效的预测和协调系统来提升安全性和通行效率。

Method: 使用基于Transformer的模型联合预测环岛中车辆和VRUs的未来轨迹，通过单步预测训练和自回归部署生成确定性输出，结合运动动力学和路线意图信息，基于预测冲突提供实时速度建议。

Result: 模型在5秒预测范围内达到高精度（ADE: 1.29m, FDE: 2.99m），加入路线意图后性能进一步提升（ADE: 1.10m, FDE: 2.36m），显著提升车辆效率和安全性，从VRU视角也改善了感知安全性。

Conclusion: ROSA系统通过结合多智能体轨迹预测和协调速度引导，有效解决了环岛混合交通的安全和效率问题，展示了车联网数据的价值，源代码已开源。

Abstract: We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.

</details>


### [287] [Distributed Quantum Gaussian Processes for Multi-Agent Systems](https://arxiv.org/abs/2602.15006)
*Meet Gandhi,George P. Kontoudis*

Main category: cs.MA

TL;DR: 提出了一种分布式量子高斯过程方法，在多智能体设置中通过量子计算增强建模能力和可扩展性，并开发了分布式黎曼ADMM算法进行优化。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在复杂大规模现实世界场景中受限于经典核函数的表达能力，量子计算通过在指数级大的希尔伯特空间中嵌入数据，可以捕捉经典计算无法访问的复杂相关性。

Method: 提出分布式量子高斯过程方法，开发分布式共识黎曼交替方向乘子法算法，将局部智能体模型聚合为全局模型，解决非欧几里得优化问题。

Result: 在经典硬件的量子模拟器上进行数值实验，使用NASA航天飞机雷达地形任务的真实非平稳高程数据集和量子高斯过程生成的合成数据集验证方法有效性。

Conclusion: 该方法不仅展示了建模优势，还突显了量子硬件在高斯过程和分布式优化中可能提供的计算加速潜力。

Abstract: Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.

</details>
