{"id": "2602.21259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21259", "abs": "https://arxiv.org/abs/2602.21259", "authors": ["Ricardo B. Grando", "Victor A. Kich", "Alisson H. Kolling", "Junior C. D. Jesus", "Rodrigo S. Guerra", "Paulo L. J. Drews-Jr"], "title": "Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles", "comment": "Accepted to the Brazilian Conference on Robotics 2026", "summary": "Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u6df7\u5408\u65e0\u4eba\u7a7a\u4e2d\u6c34\u4e0b\u98de\u884c\u5668\u6301\u7eed\u76d1\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u7edf\u4e00\u7684DRL\u67b6\u6784\u5904\u7406\u7a7a\u4e2d\u548c\u6c34\u4e0b\u4f20\u611f\u5668\u6570\u636e\uff0c\u5b9e\u73b0\u8de8\u57df\u9002\u5e94\u6027\u3002", "motivation": "\u6df7\u5408\u65e0\u4eba\u7a7a\u4e2d\u6c34\u4e0b\u98de\u884c\u5668\uff08HUAUVs\uff09\u80fd\u591f\u5728\u7a7a\u4e2d\u548c\u6c34\u4e0b\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u53ef\u7528\u4e8e\u68c0\u67e5\u3001\u6d4b\u7ed8\u3001\u641c\u7d22\u548c\u6551\u63f4\u7b49\u5e94\u7528\uff0c\u4f46\u7531\u4e8e\u7a7a\u6c14\u548c\u6c34\u57df\u7684\u4e0d\u540c\u52a8\u529b\u5b66\u7279\u6027\u548c\u7ea6\u675f\uff0c\u5f00\u53d1\u65b0\u65b9\u6cd5\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u91c7\u7528\u5171\u4eab\u7684DRL\u67b6\u6784\uff0c\u4f7f\u7528\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u6570\u636e\uff08\u7a7a\u4e2d\uff09\u548c\u58f0\u7eb3\u6570\u636e\uff08\u6c34\u4e0b\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e3a\u4e24\u79cd\u73af\u5883\u521b\u5efa\u7edf\u4e00\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8003\u8651\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u4e2a\u79fb\u52a8\u76ee\u6807\u52a8\u6001\u7684\u60c5\u51b5\u4e0b\uff0c\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8de8\u57df\u9002\u5e94\u6027\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u7a7a\u4e2d\u6c34\u4e0b\u98de\u884c\u5668\u7684\u53ef\u6269\u5c55\u81ea\u4e3b\u6301\u7eed\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.21266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21266", "abs": "https://arxiv.org/abs/2602.21266", "authors": ["Mor Levenhar", "Itzik Klein"], "title": "Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints", "comment": "12 pages, 5 figuers", "summary": "Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cc\u5206\u652f\u4fe1\u606f\u8f85\u52a9\u6846\u67b6\uff0c\u878d\u5408\u7b49\u5f0f\u548c\u4e0d\u7b49\u5f0f\u8fd0\u52a8\u7ea6\u675f\uff0c\u901a\u8fc7\u65b9\u5dee\u52a0\u6743\u65b9\u6848\u63d0\u9ad8\u57ce\u5e02\u8f66\u8f86\u5bfc\u822a\u7cbe\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u786c\u4ef6\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u536b\u661f\u4fe1\u53f7\u9891\u7e41\u88ab\u906e\u6321\uff0c\u4f4e\u6210\u672c\u60ef\u6027\u4f20\u611f\u5668\u5728\u957f\u65f6\u95f4\u4fe1\u53f7\u4e2d\u65ad\u65f6\u8bef\u5dee\u5feb\u901f\u7d2f\u79ef\u3002\u73b0\u6709\u975e\u5b8c\u6574\u7ea6\u675f\u7b49\u65b9\u6cd5\u5bf9\u8f66\u8f86\u8fd0\u52a8\u65bd\u52a0\u521a\u6027\u7b49\u5f0f\u5047\u8bbe\uff0c\u5728\u52a8\u6001\u57ce\u5e02\u9a7e\u9a76\u6761\u4ef6\u4e0b\u53ef\u80fd\u88ab\u8fdd\u53cd\uff0c\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u4fe1\u606f\u8f85\u52a9\u6846\u67b6\uff0c\u878d\u5408\u7b49\u5f0f\u548c\u4e0d\u7b49\u5f0f\u8fd0\u52a8\u7ea6\u675f\uff0c\u901a\u8fc7\u65b9\u5dee\u52a0\u6743\u65b9\u6848\u96c6\u6210\u5230\u73b0\u6709\u5bfc\u822a\u6ee4\u6ce2\u5668\u4e2d\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u8f6f\u4ef6\u4fee\u6539\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u786c\u4ef6\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u603b\u65f6\u957f4.3\u5c0f\u65f6\u3002\u5728GNSS\u5b8c\u5168\u53ef\u7528\u65f6\uff0c\u5782\u76f4\u4f4d\u7f6e\u8bef\u5dee\u51cf\u5c1116.7%\uff0c\u9ad8\u5ea6\u7cbe\u5ea6\u63d0\u9ad850.1%\uff1b\u5728GNSS\u4e0d\u53ef\u7528\u65f6\uff0c\u5782\u76f4\u6f02\u79fb\u51cf\u5c1124.2%\uff0c\u9ad8\u5ea6\u7cbe\u5ea6\u63d0\u9ad820.2%\u3002", "conclusion": "\u7528\u7269\u7406\u9a71\u52a8\u7684\u4e0d\u7b49\u5f0f\u8fb9\u754c\u66ff\u4ee3\u786c\u6027\u8fd0\u52a8\u7b49\u5f0f\u5047\u8bbe\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u514d\u8d39\u7684\u7b56\u7565\uff0c\u53ef\u63d0\u9ad8\u5bfc\u822a\u7684\u5f39\u6027\u3001\u8fde\u7eed\u6027\u548c\u6f02\u79fb\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u3001\u5730\u56fe\u6570\u636e\u6216\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2602.21302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21302", "abs": "https://arxiv.org/abs/2602.21302", "authors": ["Krishna Suresh", "Chris Atkeson"], "title": "Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control", "comment": "Project website: https://flying-knots.github.io", "summary": "Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u7ea7\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u64cd\u7eb5\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u5982\u7ef3\u7d22\uff09\uff0c\u5728\u786c\u4ef6\u4e0a\u76f4\u63a5\u5b66\u4e60\uff0c\u65e0\u9700\u5927\u91cf\u6f14\u793a\u6570\u636e\u6216\u4eff\u771f\uff0c\u6210\u529f\u5b9e\u73b0\"\u98de\u884c\u6253\u7ed3\"\u4efb\u52a1\u3002", "motivation": "\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u5982\u7ef3\u7d22\uff09\u5177\u6709\u65e0\u9650\u81ea\u7531\u5ea6\u4e14\u5448\u73b0\u6b20\u9a71\u52a8\u52a8\u529b\u5b66\u7279\u6027\uff0c\u8fd9\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u7684\u52a8\u6001\u64cd\u7eb5\u90fd\u6784\u6210\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6f14\u793a\u6570\u636e\u6216\u4eff\u771f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u7ea7\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5355\u4e2a\u4eba\u7c7b\u6f14\u793a\u548c\u7b80\u5316\u7ef3\u7d22\u6a21\u578b\uff1b2\uff09\u5728\u786c\u4ef6\u4e0a\u76f4\u63a5\u5b66\u4e60\uff1b3\uff09\u6bcf\u6b21\u8fed\u4ee3\u901a\u8fc7\u6c42\u89e3\u4e8c\u6b21\u89c4\u5212\u6784\u5efa\u673a\u5668\u4eba\u548c\u7ef3\u7d22\u7684\u5c40\u90e8\u9006\u6a21\u578b\uff0c\u5c06\u4efb\u52a1\u7a7a\u95f4\u8bef\u5dee\u4f20\u64ad\u5230\u52a8\u4f5c\u66f4\u65b0\u4e2d\u3002", "result": "\u57287\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u7ef3\u7d22\u4e0a\u8bc4\u4f30\uff08\u5305\u62ec\u94fe\u6761\u3001\u4e73\u80f6\u624b\u672f\u7ba1\u3001\u7f16\u7ec7\u7ef3\u548c\u7ede\u5408\u7ef3\uff0c\u539a\u5ea67-25mm\uff0c\u5bc6\u5ea60.013-0.5 kg/m\uff09\uff1a1\uff09\u6240\u6709\u7ef3\u7d22\u572810\u6b21\u8bd5\u9a8c\u5185\u8fbe\u5230100%\u6210\u529f\u7387\uff1b2\uff09\u65b9\u6cd5\u80fd\u5728\u7ea62-5\u6b21\u8bd5\u9a8c\u4e2d\u6210\u529f\u5728\u4e0d\u540c\u7ef3\u7d22\u7c7b\u578b\u95f4\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4efb\u52a1\u7ea7\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u52a8\u6001\u64cd\u7eb5\uff0c\u51cf\u5c11\u4e86\u5bf9\u5916\u90e8\u6570\u636e\u6216\u4eff\u771f\u7684\u4f9d\u8d56\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u5b66\u4e60\u548c\u8de8\u4e0d\u540c\u7c7b\u578b\u7ef3\u7d22\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2602.21316", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21316", "abs": "https://arxiv.org/abs/2602.21316", "authors": ["Milad Azizkhani", "Yue Chen"], "title": "Unified Complementarity-Based Contact Modeling and Planning for Soft Robots", "comment": "9 pages, 4 figures", "summary": "Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCUSP\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6761\u4ef6\u5316\u7ba1\u9053\u548c\u8fd0\u52a8\u5b66\u5f15\u5bfc\u7684\u9884\u70ed\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u8f6f\u673a\u5668\u4eba\u63a5\u89e6\u5efa\u6a21\u548c\u89c4\u5212\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u63a5\u89e6\u5efa\u6a21\u3001\u4eff\u771f\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u7840\u3002", "motivation": "\u8f6f\u673a\u5668\u4eba\u9700\u8981\u5b89\u5168\u3001\u81ea\u9002\u5e94\u7684\u73af\u5883\u4ea4\u4e92\uff0c\u8fd9\u4f9d\u8d56\u4e8e\u63a5\u89e6\u3002\u7136\u800c\uff0c\u8f6f\u673a\u5668\u4eba\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u5efa\u6a21\u548c\u89c4\u5212\u9762\u4e34\u6311\u6218\uff1a\u8eab\u4f53\u4e0a\u7684\u5bc6\u96c6\u63a5\u89e6\u5019\u9009\u5bfc\u81f4\u5197\u4f59\u7ea6\u675f\u548c\u79e9\u4e0d\u8db3\u7684LCP\uff0c\u800c\u9ad8\u521a\u5ea6\u548c\u4f4e\u6469\u64e6\u4e4b\u95f4\u7684\u5dee\u5f02\u5f15\u5165\u4e25\u91cd\u75c5\u6001\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u95ee\u9898\u7684\u8fd1\u4f3c\u6216\u57fa\u4e8e\u60e9\u7f5a\u7684\u5904\u7406\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e92\u8865\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u62ec\uff1a\u4e3a\u79bb\u6563\u5316\u8f6f\u673a\u5668\u4eba\u5f00\u53d1\u9c81\u68d2\u7684LCP\u6a21\u578b\uff1b\u4e09\u9636\u6bb5\u6761\u4ef6\u5316\u7ba1\u9053\uff08\u60ef\u6027\u79e9\u9009\u62e9\u53bb\u9664\u5197\u4f59\u63a5\u89e6\u3001Ruiz\u5747\u8861\u7ea0\u6b63\u5c3a\u5ea6\u5dee\u5f02\u548c\u75c5\u6001\u3001\u6cd5\u5411\u5757\u7684\u8f7b\u91cfTikhonov\u6b63\u5219\u5316\uff09\uff1b\u57fa\u4e8e\u76f8\u540c\u516c\u5f0f\u7684\u8fd0\u52a8\u5b66\u5f15\u5bfc\u9884\u70ed\u7b56\u7565\uff0c\u4f7f\u7528\u5e26\u4e92\u8865\u7ea6\u675f\u7684\u6570\u5b66\u89c4\u5212\u8fdb\u884c\u52a8\u6001\u8f68\u8ff9\u4f18\u5316\u3002", "result": "\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u7403\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002CUSP\u6846\u67b6\u4e3a\u8f6f\u673a\u5668\u4eba\u63a5\u89e6\u5efa\u6a21\u3001\u4eff\u771f\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u7269\u7406\u4e00\u81f4\u7684\u516c\u5f0f\u3002", "conclusion": "CUSP\u4e3a\u7edf\u4e00\u8f6f\u673a\u5668\u4eba\u63a5\u89e6\u5efa\u6a21\u3001\u4eff\u771f\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a5\u89e6\u4e30\u5bcc\u4ea4\u4e92\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.21389", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21389", "abs": "https://arxiv.org/abs/2602.21389", "authors": ["Zach J. Patterson", "Emily Sologuren", "Levi Cai", "Daniel Kim", "Alaa Maalouf", "Pascal Spino", "Daniela Rus"], "title": "Autonomous Sea Turtle Robot for Marine Fieldwork", "comment": "22 pages, 3 figures, 1 table, 5 supplementary figures, 1 supplementary table. Submitted for review", "summary": "Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u6d77\u9f9f\u4eff\u751f\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u4eff\u751f\u8fd0\u52a8\u4e0e\u73b0\u573a\u81ea\u4e3b\u80fd\u529b\uff0c\u80fd\u591f\u5728\u73ca\u745a\u7901\u7b49\u590d\u6742\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u5e76\u8ffd\u8e2a\u79fb\u52a8\u76ee\u6807\u3002", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u89c2\u6d4b\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u73ca\u745a\u7901\u7b49\u590d\u6742\u6816\u606f\u5730\u4e2d\u8fd1\u8ddd\u79bb\u64cd\u4f5c\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u5b89\u5168\u5bfc\u822a\u3001\u5e94\u5bf9\u6c34\u6d41\u3001\u53d8\u5149\u7167\u548c\u6709\u9650\u4f20\u611f\u7b49\u95ee\u9898\u3002\u73b0\u6709\u4eff\u751f\u5e73\u53f0\u5728\u53ef\u90e8\u7f72\u81ea\u4e3b\u6027\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u6d77\u9f9f\u4eff\u751f\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u96c6\u6210\u4e86\u89c6\u89c9\u9a71\u52a8\u7684\u63a7\u5236\u5806\u6808\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6-\u822a\u5411\u7a33\u5b9a\u63a7\u5236\u3001\u969c\u788d\u7269\u907f\u8ba9\u548c\u76ee\u6807\u4e2d\u5fc3\u63a7\u5236\uff0c\u80fd\u591f\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8ffd\u8e2a\u548c\u4ea4\u4e92\u79fb\u52a8\u7269\u4f53\u3002", "result": "\u5728\u63a7\u5236\u6c60\u5b9e\u9a8c\u548c\u65b0\u82f1\u683c\u5170\u6c34\u65cf\u9986\u6d3b\u73ca\u745a\u7901\u5c55\u533a\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u7a33\u5b9a\u64cd\u4f5c\u548c\u53ef\u9760\u8ffd\u8e2a\u5feb\u901f\u79fb\u52a8\u6d77\u6d0b\u52a8\u7269\u53ca\u6f5c\u6c34\u5458\u7684\u80fd\u529b\u3002\u5728\u8131\u7f06\u5b9e\u9a8c\u4e2d\uff0c\u969c\u788d\u7269\u907f\u8ba9\u6210\u529f\u738791%\uff0c\u5e76\u5f15\u5165\u4e86\u4f4e\u8ba1\u7b97\u91cf\u7684\u673a\u8f7d\u8ffd\u8e2a\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8f6f\u786c\u6df7\u5408\u4eff\u751f\u6c34\u4e0b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u654f\u611f\u751f\u6001\u7cfb\u7edf\u4e2d\u8fdb\u884c\u6700\u5c0f\u5e72\u6270\u7684\u63a2\u7d22\u548c\u8fd1\u8ddd\u79bb\u76d1\u6d4b\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u96c6\u65b0\u578b\u786c\u4ef6\u3001\u63a7\u5236\u548c\u73b0\u573a\u5b9e\u9a8c\u4e8e\u4e00\u4f53\u7684\u5b8c\u6574\u4eff\u751f\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2602.21531", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.21531", "abs": "https://arxiv.org/abs/2602.21531", "authors": ["Yue Yang", "Shuo Cheng", "Yu Fang", "Homanga Bharadhwaj", "Mingyu Ding", "Gedas Bertasius", "Daniel Szafir"], "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies", "comment": null, "summary": "General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.", "AI": {"tldr": "LiLo-VLA\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fd0\u8f93\u4e0e\u4ea4\u4e92\u89e3\u8026\u6765\u89e3\u51b3\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u9700\u8981\u638c\u63e1\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6d89\u53ca\u591a\u4e2a\u8fd0\u52a8\u7ed3\u6784\u53d8\u5316\uff08\u5982\u8fde\u63a5\u6216\u5206\u79bb\u7269\u4f53\uff09\u4e14\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u3002\u867d\u7136\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u80fd\u591f\u638c\u63e1\u591a\u6837\u7684\u539f\u5b50\u6280\u80fd\uff0c\u4f46\u5728\u7ec4\u5408\u8fd9\u4e9b\u6280\u80fd\u65f6\u9762\u4e34\u7ec4\u5408\u590d\u6742\u6027\uff0c\u5e76\u4e14\u7531\u4e8e\u73af\u5883\u654f\u611f\u6027\u5bb9\u6613\u4ea7\u751f\u7ea7\u8054\u5931\u8d25\u3002", "method": "\u63d0\u51faLiLo-VLA\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u8fd0\u8f93\u4e0e\u4ea4\u4e92\u89e3\u8026\uff1a\u4e00\u4e2a\u5230\u8fbe\u6a21\u5757\u5904\u7406\u5168\u5c40\u8fd0\u52a8\uff0c\u800c\u4e00\u4e2a\u4ea4\u4e92\u6a21\u5757\u91c7\u7528\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684VLA\u5904\u7406\u5b64\u7acb\u7684\u76ee\u6807\u7269\u4f53\uff0c\u786e\u4fdd\u5bf9\u65e0\u5173\u89c6\u89c9\u7279\u5f81\u7684\u9c81\u68d2\u6027\u548c\u7a7a\u95f4\u914d\u7f6e\u7684\u4e0d\u53d8\u6027\u3002\u8fd9\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u901a\u8fc7\u52a8\u6001\u91cd\u89c4\u5212\u548c\u6280\u80fd\u91cd\u7528\u6765\u5b9e\u73b0\u9c81\u68d2\u7684\u5931\u8d25\u6062\u590d\u3002", "result": "\u5728\u5305\u542b\u4e24\u4e2a\u6311\u6218\u6027\u5957\u4ef6\uff08LIBERO-Long++\u548cUltra-Long\uff09\u768421\u4efb\u52a1\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLiLo-VLA\u5b9e\u73b0\u4e8669%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4Pi0.5\u9ad8\u51fa41%\uff0c\u6bd4OpenVLA-OFT\u9ad8\u51fa67%\u3002\u57288\u4e2a\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0c\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523085%\u3002", "conclusion": "LiLo-VLA\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7ec4\u5408\u590d\u6742\u6027\u548c\u7ea7\u8054\u5931\u8d25\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21633", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21633", "abs": "https://arxiv.org/abs/2602.21633", "authors": ["Chenyv Liu", "Wentao Tan", "Lei Zhu", "Fengling Li", "Jingjing Li", "Guoli Yang", "Heng Tao Shen"], "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination", "comment": null, "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.", "AI": {"tldr": "SC-VLA\u6a21\u578b\u901a\u8fc7\u7a00\u758f\u4e16\u754c\u60f3\u8c61\u548c\u5728\u7ebf\u52a8\u4f5c\u4f18\u5316\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u7edf\u8ba1\u5148\u9a8c\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u52a8\u6001\u7684\u7a33\u5065\u7406\u89e3\uff1b\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\uff0c\u4e0e\u4e16\u754c\u52a8\u4f5c\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u81ea\u6211\u6539\u8fdb\u673a\u5236", "method": "\u8bbe\u8ba1\u7a00\u758f\u4e16\u754c\u60f3\u8c61\u6a21\u5757\uff0c\u96c6\u6210\u8f85\u52a9\u9884\u6d4b\u5934\u6765\u9884\u6d4b\u5f53\u524d\u4efb\u52a1\u8fdb\u5ea6\u548c\u672a\u6765\u8f68\u8ff9\u8d8b\u52bf\uff1b\u5f15\u5165\u5728\u7ebf\u52a8\u4f5c\u4f18\u5316\u6a21\u5757\uff0c\u57fa\u4e8e\u9884\u6d4b\u7684\u7a00\u758f\u672a\u6765\u72b6\u6001\u8c03\u6574\u8f68\u8ff9\u65b9\u5411", "result": "\u5728\u4eff\u771f\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u51cf\u5c1116%\u6b65\u9aa4\uff0c\u6210\u529f\u7387\u63d0\u9ad89%\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u63d0\u534714%", "conclusion": "SC-VLA\u901a\u8fc7\u7a00\u758f\u60f3\u8c61\u548c\u5185\u5728\u5f15\u5bfc\u7684\u52a8\u4f5c\u4f18\u5316\u5b9e\u73b0\u4e86\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd"}}
{"id": "2602.22010", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22010", "abs": "https://arxiv.org/abs/2602.22010", "authors": ["Yue Su", "Sijin Chen", "Haixin Shi", "Mingyu Liu", "Zhengshen Zhang", "Ningyuan Huang", "Weiheng Zhong", "Zhengbang Zhu", "Yuxiao Liu", "Xihui Liu"], "title": "World Guidance: World Modeling in Condition Space for Action Generation", "comment": "Project Page: https://selen-suyue.github.io/WoGNet/", "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/", "AI": {"tldr": "WoG\u6846\u67b6\u901a\u8fc7\u5c06\u672a\u6765\u89c2\u6d4b\u6620\u5c04\u4e3a\u7d27\u51d1\u6761\u4ef6\u5e76\u6ce8\u5165\u52a8\u4f5c\u63a8\u7406\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u53ef\u9884\u6d4b\u672a\u6765\u8868\u793a\u7684\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u52a8\u4f5c\u751f\u6210\u80fd\u529b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u9ad8\u6548\u53ef\u9884\u6d4b\u7684\u672a\u6765\u8868\u793a\u4e0e\u6307\u5bfc\u7cbe\u786e\u52a8\u4f5c\u751f\u6210\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u6539\u5584VLA\u6a21\u578b\u7684\u52a8\u4f5c\u751f\u6210\u80fd\u529b", "method": "\u63d0\u51faWoG\u6846\u67b6\uff0c\u5c06\u672a\u6765\u89c2\u6d4b\u6620\u5c04\u4e3a\u7d27\u51d1\u6761\u4ef6\u5e76\u6ce8\u5165\u52a8\u4f5c\u63a8\u7406\u6d41\u7a0b\uff0c\u8bad\u7ec3VLA\u6a21\u578b\u540c\u65f6\u9884\u6d4b\u8fd9\u4e9b\u538b\u7f29\u6761\u4ef6\u548c\u672a\u6765\u52a8\u4f5c\uff0c\u5728\u6761\u4ef6\u7a7a\u95f4\u5185\u5b9e\u73b0\u6709\u6548\u7684\u4e16\u754c\u5efa\u6a21", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4fc3\u8fdb\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u751f\u6210\uff0c\u8fd8\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u4ece\u5927\u91cf\u4eba\u7c7b\u64cd\u4f5c\u89c6\u9891\u4e2d\u6709\u6548\u5b66\u4e60\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u672a\u6765\u9884\u6d4b\u7684\u73b0\u6709\u65b9\u6cd5", "conclusion": "WoG\u6846\u67b6\u901a\u8fc7\u6761\u4ef6\u7a7a\u95f4\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u672a\u6765\u89c2\u6d4b\u8868\u793a\u4e2d\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3aVLA\u6a21\u578b\u7684\u52a8\u4f5c\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848"}}
