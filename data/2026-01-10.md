<div id=toc></div>

# Table of Contents

- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [1] [Listen to Rhythm, Choose Movements: Autoregressive Multimodal Dance Generation via Diffusion and Mamba with Decoupled Dance Dataset](https://arxiv.org/abs/2601.03323)
*Oran Duan,Yinghua Shen,Yingzhu Lv,Luyang Jie,Yaxin Liu,Qiong Wu*

Main category: cs.GR

TL;DR: LRCM是一个多模态引导的扩散框架，支持多样化输入模态和自回归舞蹈动作生成，通过特征解耦和Motion Temporal Mamba模块实现长序列的平滑生成。


<details>
  <summary>Details</summary>
Motivation: 当前舞蹈动作生成方法存在语义控制粗糙和长序列连贯性差的问题，需要开发能够支持多模态输入并实现高质量长序列生成的新框架。

Method: 提出LRCM多模态引导扩散框架，采用特征解耦范式分离运动捕捉数据、音频节奏和专业标注的全局/局部文本描述，集成音频潜在Conformer和文本潜在Cross-Conformer，并引入Motion Temporal Mamba模块实现自回归合成。

Result: 实验结果表明LRCM在功能能力和量化指标上都表现出色，在多模态输入场景和扩展序列生成方面显示出显著潜力。

Conclusion: LRCM框架有效解决了舞蹈动作生成中的语义控制和长序列连贯性问题，为多模态引导的舞蹈生成提供了有前景的解决方案。

Abstract: Advances in generative models and sequence learning have greatly promoted research in dance motion generation, yet current methods still suffer from coarse semantic control and poor coherence in long sequences. In this work, we present Listen to Rhythm, Choose Movements (LRCM), a multimodal-guided diffusion framework supporting both diverse input modalities and autoregressive dance motion generation. We explore a feature decoupling paradigm for dance datasets and generalize it to the Motorica Dance dataset, separating motion capture data, audio rhythm, and professionally annotated global and local text descriptions. Our diffusion architecture integrates an audio-latent Conformer and a text-latent Cross-Conformer, and incorporates a Motion Temporal Mamba Module (MTMM) to enable smooth, long-duration autoregressive synthesis. Experimental results indicate that LRCM delivers strong performance in both functional capability and quantitative metrics, demonstrating notable potential in multimodal input scenarios and extended sequence generation. We will release the full codebase, dataset, and pretrained models publicly upon acceptance.

</details>
