{"id": "2506.15887", "categories": ["cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.15887", "abs": "https://arxiv.org/abs/2506.15887", "authors": ["Jakub Tłuczek", "Victor Villin", "Christos Dimitrakakis"], "title": "Fair Contracts in Principal-Agent Games with Heterogeneous Types", "comment": null, "summary": "Fairness is desirable yet challenging to achieve within multi-agent systems,\nespecially when agents differ in latent traits that affect their abilities.\nThis hidden heterogeneity often leads to unequal distributions of wealth, even\nwhen agents operate under the same rules. Motivated by real-world examples, we\npropose a framework based on repeated principal-agent games, where a principal,\nwho also can be seen as a player of the game, learns to offer adaptive\ncontracts to agents. By leveraging a simple yet powerful contract structure, we\nshow that a fairness-aware principal can learn homogeneous linear contracts\nthat equalize outcomes across agents in a sequential social dilemma.\nImportantly, this fairness does not come at the cost of efficiency: our results\ndemonstrate that it is possible to promote equity and stability in the system\nwhile preserving overall performance.", "AI": {"tldr": "论文提出了一种基于重复委托-代理博弈的框架，通过自适应合同实现多智能体系统中的公平性，同时保持效率。", "motivation": "现实中的隐藏异质性导致财富分配不均，即使规则相同。研究旨在解决这一问题。", "method": "采用重复委托-代理博弈框架，委托人学习提供自适应合同，利用简单的合同结构。", "result": "公平性委托人能学习到同质线性合同，实现结果均等化，且不牺牲效率。", "conclusion": "该框架能在促进公平和稳定性的同时，保持系统整体性能。"}}
{"id": "2506.16120", "categories": ["cs.GT", "cs.LG", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16120", "abs": "https://arxiv.org/abs/2506.16120", "authors": ["Fivos Kalogiannis", "Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Ian Gemp", "Georgios Piliouras"], "title": "Solving Zero-Sum Convex Markov Games", "comment": "To appear in the Proceedings of the 2025 International Conference on\n  Machine Learning (ICML 2025)", "summary": "We contribute the first provable guarantees of global convergence to Nash\nequilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using\nindependent policy gradient methods. Convex Markov games, recently defined by\nGemp et al. (2024), extend Markov decision processes to multi-agent settings\nwith preferences that are convex over occupancy measures, offering a broad\nframework for modeling generic strategic interactions. However, even the\nfundamental min-max case of cMGs presents significant challenges, including\ninherent nonconvexity, the absence of Bellman consistency, and the complexity\nof the infinite horizon.\n  We follow a two-step approach. First, leveraging properties of\nhidden-convex--hidden-concave functions, we show that a simple nonconvex\nregularization transforms the min-max optimization problem into a\nnonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this\nregularization can stabilize the iterates of independent policy gradient\nmethods and ultimately lead them to converge to equilibria. Second, building on\nthis reduction, we address the general constrained min-max problems under\nNC-pPL and two-sided pPL conditions, providing the first global convergence\nguarantees for stochastic nested and alternating gradient descent-ascent\nmethods, which we believe may be of independent interest.", "AI": {"tldr": "论文首次证明了在两人零和凸马尔可夫博弈中，独立策略梯度方法能全局收敛到纳什均衡。通过非凸正则化将问题转化为NC-pPL目标，并提供了全局收敛保证。", "motivation": "凸马尔可夫博弈扩展了多智能体交互的建模框架，但其非凸性和无限时域复杂性带来了挑战。", "method": "采用两步法：1）通过非凸正则化将问题转化为NC-pPL目标；2）在NC-pPL条件下分析梯度下降-上升方法的收敛性。", "result": "证明了独立策略梯度方法在凸马尔可夫博弈中的全局收敛性。", "conclusion": "该方法为复杂博弈问题提供了理论保证，并可能对其他优化问题有独立意义。"}}
{"id": "2506.17058", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2506.17058", "abs": "https://arxiv.org/abs/2506.17058", "authors": ["Sébastien Lahaie", "Benjamin Schaeffer", "Yuanjun Zhou"], "title": "Bidder Feedback in First-Price Auctions for Video Advertising", "comment": null, "summary": "In first-price auctions for display advertising, exchanges typically\ncommunicate the \"minimum-bid-to-win\" to bidders after the auction as feedback\nfor their bidding algorithms. For a winner, this is the second-highest bid,\nwhile for losing bidders it is the highest bid. In this paper we investigate\nthe generalization of this concept to general combinatorial auctions, motivated\nby the domain of video advertising. In a video pod auction, ad slots during an\nadvertising break in a video stream are auctioned all at once, under several\nkinds of allocation constraints such as a constraint on total ad duration. We\ncast the problem in terms of computing bid updates (discounts and raises) that\nmaintain the optimality of the current allocation. Our main result\ncharacterizes the set of joint bid updates with this property as the core of an\nassociated bicooperative game. In the case of the assignment problem--a special\ncase of video pod auctions--we provide a linear programming characterization of\nthis bicooperative core. Our characterization leads to several candidates for a\ngeneralized minimum-bid-to-win. Drawing on video pod auction data from a real\nad exchange, we perform an empirical analysis to understand the bidding\ndynamics they induce and their convergence properties.", "AI": {"tldr": "本文研究了组合拍卖中“最低中标价”概念的推广，特别针对视频广告中的视频插播拍卖，提出了保持分配最优性的投标更新方法，并通过实证分析验证了其动态和收敛性。", "motivation": "研究动机源于视频广告中的组合拍卖问题，尤其是如何将“最低中标价”反馈机制推广到更复杂的拍卖场景中。", "method": "方法包括将问题建模为保持分配最优性的投标更新（折扣和加价），并通过双合作博弈的核心理论进行表征。对于分配问题的特殊情况，提供了线性规划的表征。", "result": "主要结果是通过双合作博弈核心理论表征了保持最优性的投标更新集合，并提出了几种广义最低中标价的候选方案。", "conclusion": "结论是通过实证分析验证了这些候选方案在视频插播拍卖中的动态和收敛性，为实际应用提供了理论支持。"}}
{"id": "2506.17115", "categories": ["cs.GT", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17115", "abs": "https://arxiv.org/abs/2506.17115", "authors": ["Ezzat Elokda", "Andrea Censi", "Emilio Frazzoli", "Florian Dörfler", "Saverio Bolognani"], "title": "A Vision for Trustworthy, Fair, and Efficient Socio-Technical Control using Karma Economies", "comment": null, "summary": "Control systems will play a pivotal role in addressing societal-scale\nchallenges as they drive the development of sustainable future smart cities. At\nthe heart of these challenges is the trustworthy, fair, and efficient\nallocation of scarce public resources, including renewable energy,\ntransportation, data, computation, etc.. Historical evidence suggests that\nmonetary control -- the prototypical mechanism for managing resource scarcity\n-- is not always well-accepted in socio-technical resource contexts. In this\nvision article, we advocate for karma economies as an emerging non-monetary\nmechanism for socio-technical control. Karma leverages the repetitive nature of\nmany socio-technical resources to jointly attain trustworthy, fair, and\nefficient allocations; by budgeting resource consumption over time and letting\nresource users ``play against their future selves.'' To motivate karma, we\nreview related concepts in economics through a control systems lens, and make a\ncase for a) shifting the viewpoint of resource allocations from single-shot and\nstatic to repeated and dynamic games; and b) adopting long-run Nash welfare as\nthe formalization of ``fairness and efficiency'' in socio-technical contexts.\nWe show that in many dynamic resource settings, karma Nash equilibria maximize\nlong-run Nash welfare. Moreover, we discuss implications for a future smart\ncity built on multi-karma economies: by choosing whether to combine different\nsocio-technical resources, e.g., electricity and transportation, in a single\nkarma economy, or separate into resource-specific economies, karma provides new\nflexibility to design the scope of fairness and efficiency.", "AI": {"tldr": "本文提出了一种基于“因果经济”的非货币机制，用于解决未来智能城市中稀缺公共资源的公平、高效分配问题。", "motivation": "传统货币机制在社会技术资源分配中效果不佳，需要一种更可信、公平且高效的替代方案。", "method": "通过动态重复博弈视角，引入因果经济机制，以长期纳什福利作为公平与效率的衡量标准。", "result": "研究表明，因果纳什均衡在动态资源分配中能最大化长期纳什福利。", "conclusion": "因果经济为未来智能城市的多资源分配提供了灵活的设计空间，可根据需求选择资源整合或分离。"}}
{"id": "2506.15742", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.15742", "abs": "https://arxiv.org/abs/2506.15742", "authors": ["Black Forest Labs", "Stephen Batifol", "Andreas Blattmann", "Frederic Boesel", "Saksham Consul", "Cyril Diagne", "Tim Dockhorn", "Jack English", "Zion English", "Patrick Esser", "Sumith Kulal", "Kyle Lacey", "Yam Levi", "Cheng Li", "Dominik Lorenz", "Jonas Müller", "Dustin Podell", "Robin Rombach", "Harry Saini", "Axel Sauer", "Luke Smith"], "title": "FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space", "comment": null, "summary": "We present evaluation results for FLUX.1 Kontext, a generative flow matching\nmodel that unifies image generation and editing. The model generates novel\noutput views by incorporating semantic context from text and image inputs.\nUsing a simple sequence concatenation approach, FLUX.1 Kontext handles both\nlocal editing and generative in-context tasks within a single unified\narchitecture. Compared to current editing models that exhibit degradation in\ncharacter consistency and stability across multiple turns, we observe that\nFLUX.1 Kontext improved preservation of objects and characters, leading to\ngreater robustness in iterative workflows.The model achieves competitive\nperformance with current state-of-the-art systems while delivering\nsignificantly faster generation times, enabling interactive applications and\nrapid prototyping workflows. To validate these improvements, we introduce\nKontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering\nfive task categories: local editing, global editing, character reference, style\nreference and text editing. Detailed evaluations show the superior performance\nof FLUX.1 Kontext in terms of both single-turn quality and multi-turn\nconsistency, setting new standards for unified image processing models.", "AI": {"tldr": "FLUX.1 Kontext是一个统一的生成流匹配模型，结合文本和图像输入生成新视图，支持局部编辑和生成任务，性能优于现有模型。", "motivation": "解决现有编辑模型在多轮操作中字符一致性和稳定性下降的问题，提升迭代工作流的鲁棒性。", "method": "采用简单的序列拼接方法，统一处理局部编辑和生成任务，结合语义上下文。", "result": "在单轮质量和多轮一致性上表现优异，生成速度更快，支持交互应用。", "conclusion": "FLUX.1 Kontext为统一图像处理模型设定了新标准，性能优于当前先进系统。"}}
{"id": "2506.15737", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15737", "abs": "https://arxiv.org/abs/2506.15737", "authors": ["Gautam Siddharth Kashyap", "Md Tabrez Nafis", "Samar Wazir"], "title": "A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture", "comment": null, "summary": "Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent\n(SGD) frequently encounters difficulties, including substantial computing\nexpense and the risk of converging to local optima, attributable to its\ndependence on partial weight gradients. Therefore, this work investigates\nParticle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two\npopulation-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to\nmitigate these constraints. A hybrid PSO-SGD strategy is developed to improve\nlocal search efficiency. The findings indicate that the hybrid PSO-SGD\ntechnique decreases the median training MSE by 90 to 95 percent relative to\nconventional GA and PSO across various network sizes (e.g., from around 0.02 to\napproximately 0.001 in the Sphere function). RMHC attains substantial\nenhancements, reducing MSE by roughly 85 to 90 percent compared to GA.\nSimultaneously, RS consistently exhibits errors exceeding 0.3, signifying\nsubpar performance. These findings underscore that hybrid and evolutionary\nprocedures significantly improve training efficiency and accuracy compared to\nconventional optimization methods and imply that the Building Block Hypothesis\n(BBH) may still be valid, indicating that advantageous weight structures are\nretained during evolutionary search.", "AI": {"tldr": "论文探讨了使用粒子群优化（PSO）和遗传算法（GA）替代随机梯度下降（SGD）以解决其计算成本高和易陷入局部最优的问题，并提出了一种混合PSO-SGD策略。实验表明，该策略显著降低了训练误差。", "motivation": "SGD在训练人工神经网络时存在计算成本高和易陷入局部最优的问题，因此研究PSO和GA作为替代优化方法。", "method": "开发了一种混合PSO-SGD策略，结合了PSO的全局搜索能力和SGD的局部搜索效率。", "result": "混合PSO-SGD策略将训练MSE中值降低了90%至95%，优于传统GA和PSO。RMHC也显著降低了MSE，而RS表现较差。", "conclusion": "混合和进化方法显著提升了训练效率和准确性，支持了Building Block Hypothesis（BBH）的有效性。"}}
{"id": "2506.15756", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15756", "abs": "https://arxiv.org/abs/2506.15756", "authors": ["João G. Ribeiro", "Yaniv Oren", "Alberto Sardinha", "Matthijs Spaan", "Francisco S. Melo"], "title": "RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains", "comment": null, "summary": "This paper proposes RecBayes, a novel approach for ad hoc teamwork under\npartial observability, a setting where agents are deployed on-the-fly to\nenvironments where pre-existing teams operate, that never requires, at any\nstage, access to the states of the environment or the actions of its teammates.\nWe show that by relying on a recurrent Bayesian classifier trained using past\nexperiences, an ad hoc agent is effectively able to identify known teams and\ntasks being performed from observations alone. Unlike recent approaches such as\nPO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some\nstage fully observable states of the environment, actions of teammates, or\nboth, or approaches such as ATPO (Ribeiro et al., 2023) that require the\nenvironments to be small enough to be tabularly modelled (Ribeiro et al.,\n2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes\nis both able to handle arbitrarily large spaces while never relying on either\nstates and teammates' actions. Our results in benchmark domains from the\nmulti-agent systems literature, adapted for partial observability and scaled up\nto 1M states and 2^125 observations, show that RecBayes is effective at\nidentifying known teams and tasks being performed from partial observations\nalone, and as a result, is able to assist the teams in solving the tasks\neffectively.", "AI": {"tldr": "RecBayes是一种新的方法，用于部分可观测环境下的临时团队协作，无需访问环境状态或队友动作。", "motivation": "解决现有方法需要完全可观测状态或队友动作的问题，适用于大规模环境。", "method": "使用基于过去经验的循环贝叶斯分类器，仅通过观测识别已知团队和任务。", "result": "在扩展到1M状态和2^125观测的基准测试中，RecBayes能有效识别团队和任务，并协助完成任务。", "conclusion": "RecBayes在无需状态或动作信息的情况下，适用于大规模部分可观测环境。"}}
{"id": "2506.15754", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15754", "abs": "https://arxiv.org/abs/2506.15754", "authors": ["Tahitoa Leygue", "Astrid Sabourin", "Christian Bolzmacher", "Sylvain Bouchigny", "Margarita Anastassova", "Quoc-Cuong Pham"], "title": "Explainable speech emotion recognition through attentive pooling: insights from attention-based temporal localization", "comment": null, "summary": "State-of-the-art transformer models for Speech Emotion Recognition (SER) rely\non temporal feature aggregation, yet advanced pooling methods remain\nunderexplored. We systematically benchmark pooling strategies, including\nMulti-Query Multi-Head Attentive Statistics Pooling, which achieves a 3.5\npercentage point macro F1 gain over average pooling. Attention analysis shows\n15 percent of frames capture 80 percent of emotion cues, revealing a localized\npattern of emotional information. Analysis of high-attention frames reveals\nthat non-linguistic vocalizations and hyperarticulated phonemes are\ndisproportionately prioritized during pooling, mirroring human perceptual\nstrategies. Our findings position attentive pooling as both a performant SER\nmechanism and a biologically plausible tool for explainable emotion\nlocalization. On Interspeech 2025 Speech Emotion Recognition in Naturalistic\nConditions Challenge, our approach obtained a macro F1 score of 0.3649.", "AI": {"tldr": "论文系统评估了语音情感识别中的池化策略，提出了一种多查询多头注意力统计池化方法，性能优于平均池化，并揭示了情感信息的局部化特征。", "motivation": "探索语音情感识别中先进的池化方法，提升模型性能并解释情感信息的分布模式。", "method": "系统比较多种池化策略，提出多查询多头注意力统计池化方法，并通过注意力分析定位情感信息。", "result": "新方法在宏F1分数上比平均池化提升3.5个百分点，分析显示15%的帧捕获了80%的情感线索。", "conclusion": "注意力池化不仅是一种高性能的语音情感识别机制，还能解释情感信息的局部化分布，具有生物学合理性。"}}
{"id": "2506.15788", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15788", "abs": "https://arxiv.org/abs/2506.15788", "authors": ["Baxi Chong", "Juntao He", "Daniel Irvine", "Tianyu Wang", "Esteban Flores", "Daniel Soto", "Jianfeng Lin", "Zhaochen Xu", "Vincent R Nienhusser", "Grigoriy Blekherman", "Daniel I. Goldman"], "title": "Robust control for multi-legged elongate robots in noisy environments", "comment": null, "summary": "Modern two and four legged robots exhibit impressive mobility on complex\nterrain, largely attributed to advancement in learning algorithms. However,\nthese systems often rely on high-bandwidth sensing and onboard computation to\nperceive/respond to terrain uncertainties. Further, current locomotion\nstrategies typically require extensive robot-specific training, limiting their\ngeneralizability across platforms. Building on our prior research connecting\nrobot-environment interaction and communication theory, we develop a new\nparadigm to construct robust and simply controlled multi-legged elongate robots\n(MERs) capable of operating effectively in cluttered, unstructured\nenvironments. In this framework, each leg-ground contact is thought of as a\nbasic active contact (bac), akin to bits in signal transmission. Reliable\nlocomotion can be achieved in open-loop on \"noisy\" landscapes via sufficient\nredundancy in bacs. In such situations, robustness is achieved through passive\nmechanical responses. We term such processes as those displaying mechanical\nintelligence (MI) and analogize these processes to forward error correction\n(FEC) in signal transmission. To augment MI, we develop feedback control\nschemes, which we refer to as computational intelligence (CI) and such\nprocesses analogize automatic repeat request (ARQ) in signal transmission.\nIntegration of these analogies between locomotion and communication theory\nallow analysis, design, and prediction of embodied intelligence control schemes\n(integrating MI and CI) in MERs, showing effective and reliable performance\n(approximately half body lengths per cycle) on complex landscapes with terrain\n\"noise\" over twice the robot's height. Our work provides a foundation for\nsystematic development of MER control, paving the way for terrain-agnostic,\nagile, and resilient robotic systems capable of operating in extreme\nenvironments.", "AI": {"tldr": "论文提出了一种基于通信理论的新型多足机器人控制框架，结合机械智能（MI）和计算智能（CI），实现了在复杂地形中的鲁棒运动。", "motivation": "现有机器人依赖于高带宽传感和计算，且训练过程缺乏通用性。本文旨在开发一种简单、通用的控制方法。", "method": "将足地接触视为基本主动接触（bac），类比通信中的比特传输，通过冗余和反馈控制（MI和CI）实现鲁棒运动。", "result": "机器人能在噪声地形中稳定运动（每周期约半个身长），地形噪声高度超过机器人高度的两倍。", "conclusion": "该框架为多足机器人控制提供了系统化基础，支持极端环境下的敏捷运动。"}}
{"id": "2506.15834", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15834", "abs": "https://arxiv.org/abs/2506.15834", "authors": ["Zachary D King", "Maryam Khalid", "Han Yu", "Kei Shibuya", "Khadija Zanna", "Marzieh Majd", "Ryan L Brown", "Yufei Shen", "Thomas Vaessen", "George Kypriotakis", "Christopher P Fagundes", "Akane Sano"], "title": "Machine Learning-based Context-Aware EMAs: An Offline Feasibility Study", "comment": null, "summary": "Mobile health (mHealth) systems help researchers monitor and care for\npatients in real-world settings. Studies utilizing mHealth applications use\nEcological Momentary Assessment (EMAs), passive sensing, and contextual\nfeatures to develop emotion recognition models, which rely on EMA responses as\nground truth. Due to this, it is crucial to consider EMA compliance when\nconducting a successful mHealth study. Utilizing machine learning is one\napproach that can solve this problem by sending EMAs based on the predicted\nlikelihood of a response. However, literature suggests that this approach may\nlead to prompting participants more frequently during emotions associated with\nresponsiveness, thereby narrowing the range of emotions collected. We propose a\nmulti-objective function that utilizes machine learning to identify optimal\ntimes for sending EMAs. The function identifies optimal moments by combining\npredicted response likelihood with model uncertainty in emotion predictions.\nUncertainty would lead the function to prioritize time points when the model is\nless confident, which often corresponds to underrepresented emotions. We\ndemonstrate that this objective function would result in EMAs being sent when\nparticipants are responsive and experiencing less commonly observed emotions.\nThe evaluation is conducted offline using two datasets: (1) 91 spousal\ncaregivers of individuals with Alzheimer's Disease and Related dementias\n(ADRD), (2) 45 healthy participants. Results show that the multi-objective\nfunction tends to be higher when participants respond to EMAs and report less\ncommonly observed emotions. This suggests that using the proposed objective\nfunction to guide EMA delivery could improve receptivity rates and capture a\nbroader range of emotions.", "AI": {"tldr": "论文提出了一种多目标函数，结合响应可能性和情绪预测不确定性，优化EMA发送时机，以提高响应率并捕捉更广泛的情绪范围。", "motivation": "mHealth研究中，EMA依从性对情绪识别模型至关重要，但现有方法可能导致情绪样本偏差。", "method": "提出多目标函数，结合响应可能性和模型不确定性，优化EMA发送时间。", "result": "在两个数据集上验证，多目标函数在响应EMA和捕捉少见情绪时表现更好。", "conclusion": "该方法可提高EMA响应率并丰富情绪数据多样性。"}}
{"id": "2506.15747", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.15747", "abs": "https://arxiv.org/abs/2506.15747", "authors": ["Fangzhou Lin", "Zilin Dai", "Rigved Sanku", "Songlin Hou", "Kazunori D Yamada", "Haichong K. Zhang", "Ziming Zhang"], "title": "A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion", "comment": "6 pages, 2 figures", "summary": "The single-view image guided point cloud completion (SVIPC) task aims to\nreconstruct a complete point cloud from a partial input with the help of a\nsingle-view image. While previous works have demonstrated the effectiveness of\nthis multimodal approach, the fundamental necessity of image guidance remains\nlargely unexamined. To explore this, we propose a strong baseline approach for\nSVIPC based on an attention-based multi-branch encoder-decoder network that\nonly takes partial point clouds as input, view-free. Our hierarchical\nself-fusion mechanism, driven by cross-attention and self-attention layers,\neffectively integrates information across multiple streams, enriching feature\nrepresentations and strengthening the networks ability to capture geometric\nstructures. Extensive experiments and ablation studies on the ShapeNet-ViPC\ndataset demonstrate that our view-free framework performs superiorly to\nstate-of-the-art SVIPC methods. We hope our findings provide new insights into\nthe development of multimodal learning in SVIPC. Our demo code will be\navailable at https://github.com/Zhang-VISLab.", "AI": {"tldr": "论文提出了一种基于注意力机制的多分支编码器-解码器网络，用于单视角图像引导点云补全任务，仅使用部分点云输入，无需视角依赖。实验表明其性能优于现有方法。", "motivation": "探索单视角图像引导在点云补全任务中的必要性，并提出一种无需图像输入的强基线方法。", "method": "采用注意力机制的多分支编码器-解码器网络，结合跨注意力和自注意力层，通过层次化自融合机制整合多流信息。", "result": "在ShapeNet-ViPC数据集上，该方法性能优于现有单视角图像引导方法。", "conclusion": "研究为多模态学习在点云补全任务中的发展提供了新见解。"}}
{"id": "2506.15685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15685", "abs": "https://arxiv.org/abs/2506.15685", "authors": ["Wang Yu-Hang", "Liu ying", "Fang liang", "Wang Xuelin", "Junkang Guo", "Shiwei Li", "Lei Gao", "Jian Liu", "Wenfei Yin"], "title": "Ignition Phase : Standard Training for Fast Adversarial Robustness", "comment": null, "summary": "Adversarial Training (AT) is a cornerstone defense, but many variants\noverlook foundational feature representations by primarily focusing on stronger\nattack generation. We introduce Adversarial Evolution Training (AET), a simple\nyet powerful framework that strategically prepends an Empirical Risk\nMinimization (ERM) phase to conventional AT. We hypothesize this initial ERM\nphase cultivates a favorable feature manifold, enabling more efficient and\neffective robustness acquisition. Empirically, AET achieves comparable or\nsuperior robustness more rapidly, improves clean accuracy, and cuts training\ncosts by 8-25\\%. Its effectiveness is shown across multiple datasets,\narchitectures, and when augmenting established AT methods. Our findings\nunderscore the impact of feature pre-conditioning via standard training for\ndeveloping more efficient, principled robust defenses. Code is available in the\nsupplementary material.", "AI": {"tldr": "AET通过在传统对抗训练前加入ERM阶段，优化特征表示，提升鲁棒性和训练效率。", "motivation": "现有对抗训练方法过于关注攻击生成，忽略了基础特征表示的重要性。", "method": "提出AET框架，在对抗训练前加入ERM阶段，优化特征流形。", "result": "AET在多个数据集和架构上表现优异，鲁棒性提升，训练成本降低8-25%。", "conclusion": "特征预条件化对高效鲁棒防御至关重要，AET为对抗训练提供了新思路。"}}
{"id": "2506.15910", "categories": ["eess.SY", "cs.DC", "cs.NI", "cs.SY", "C.2; I.6"], "pdf": "https://arxiv.org/pdf/2506.15910", "abs": "https://arxiv.org/abs/2506.15910", "authors": ["Zakria Qadir", "Muhammad Bilal", "Guoqiang Liu", "Xiaolong Xu"], "title": "Autonomous Trajectory Optimization for UAVs in Disaster Zone Using Henry Gas Optimization Scheme", "comment": "12 pages, 9 figuers", "summary": "The unmanned aerial vehicles (UAVs) in a disaster-prone environment plays\nimportant role in assisting the rescue services and providing the internet\nconnectivity with the outside world. However, in such a complex environment the\nselection of optimum trajectory of UAVs is of utmost importance. UAV trajectory\noptimization deals with finding the shortest path in the minimal possible time.\nIn this paper, a cluster optimization scheme (COS) is proposed using the Henry\ngas optimization (HGO) metaheuristic algorithm to identify the shortest path\nhaving minimal transportation cost and algorithm complexity. The mathematical\nmodel is designed for COS using the HGO algorithm and compared with the\nstate-of-the-art metaheuristic algorithms such as particle swarm optimization\n(PSO), grey wolf optimization (GWO), cuckoo search algorithm (CSA) and\nbarnacles mating optimizer (BMO). In order to prove the robustness of the\nproposed model, four different scenarios are evaluated that includes ambient\nenvironment, constrict environment, tangled environment, and complex\nenvironment. In all the aforementioned scenarios, the HGO algorithm outperforms\nthe existing algorithms. Particularly, in the ambient environment, the HGO\nalgorithm achieves a 39.3% reduction in transportation cost and a 16.8%\nreduction in computational time as compared to the PSO algorithm. Hence, the\nHGO algorithm can be used for autonomous trajectory optimization of UAVs in\nsmart cities.", "AI": {"tldr": "论文提出了一种基于亨利气体优化（HGO）算法的集群优化方案（COS），用于无人机（UAV）在复杂环境中的最优轨迹规划，显著降低了运输成本和计算时间。", "motivation": "在灾害多发环境中，无人机需要快速规划最优轨迹以支持救援服务和网络连接，现有算法在复杂环境中表现不足。", "method": "使用HGO算法设计数学模型，并与PSO、GWO、CSA和BMO等现有算法进行对比。", "result": "HGO算法在四种不同环境中均优于现有算法，特别是在普通环境中，运输成本降低39.3%，计算时间减少16.8%。", "conclusion": "HGO算法适用于智能城市中无人机的自主轨迹优化。"}}
{"id": "2506.15716", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2506.15716", "abs": "https://arxiv.org/abs/2506.15716", "authors": ["Angelos Assos", "Carmel Baharav", "Bailey Flanigan", "Ariel Procaccia"], "title": "Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies", "comment": null, "summary": "An increasingly influential form of deliberative democracy centers on\ncitizens' assemblies, where randomly selected people discuss policy questions.\nThe legitimacy of these panels hinges on their representation of the broader\npopulation, but panelists often drop out, leading to an unbalanced composition.\nAlthough participant attrition is mitigated in practice by alternates, their\nselection is not taken into account by existing methods. To address this gap,\nwe introduce an optimization framework for alternate selection. Our algorithmic\napproach, which leverages learning-theoretic machinery, estimates dropout\nprobabilities using historical data and selects alternates to minimize expected\nmisrepresentation. We establish theoretical guarantees for our approach,\nincluding worst-case bounds on sample complexity (with implications for\ncomputational efficiency) and on loss when panelists' probabilities of dropping\nout are mis-estimated. Empirical evaluation using real-world data demonstrates\nthat, compared to the status quo, our method significantly improves\nrepresentation while requiring fewer alternates.", "AI": {"tldr": "本文提出了一种优化框架，用于在公民集会中替补成员的选择，以解决因成员退出导致的不平衡问题。", "motivation": "公民集会的合法性依赖于其对广泛人群的代表性，但成员退出会导致不平衡，现有方法未考虑替补选择。", "method": "利用学习理论工具，基于历史数据估计退出概率，并选择替补以最小化预期的不代表性。", "result": "理论分析提供了样本复杂性和计算效率的保证，实证表明该方法显著提高了代表性且减少了替补需求。", "conclusion": "提出的算法框架有效解决了公民集会中的代表性问题，具有理论和实践优势。"}}
{"id": "2506.15786", "categories": ["cs.GR", "cs.AI", "cs.LG", "physics.comp-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.15786", "abs": "https://arxiv.org/abs/2506.15786", "authors": ["Peter Yichen Chen", "Minghao Guo", "Hanspeter Pfister", "Ming Lin", "William Freeman", "Qixing Huang", "Han-Wei Shen", "Wojciech Matusik"], "title": "Graphics4Science: Computer Graphics for Scientific Impacts", "comment": null, "summary": "Computer graphics, often associated with films, games, and visual effects,\nhas long been a powerful tool for addressing scientific challenges--from its\norigins in 3D visualization for medical imaging to its role in modern\ncomputational modeling and simulation. This course explores the deep and\nevolving relationship between computer graphics and science, highlighting past\nachievements, ongoing contributions, and open questions that remain. We show\nhow core methods, such as geometric reasoning and physical modeling, provide\ninductive biases that help address challenges in both fields, especially in\ndata-scarce settings. To that end, we aim to reframe graphics as a modeling\nlanguage for science by bridging vocabulary gaps between the two communities.\nDesigned for both newcomers and experts, Graphics4Science invites the graphics\ncommunity to engage with science, tackle high-impact problems where graphics\nexpertise can make a difference, and contribute to the future of scientific\ndiscovery. Additional details are available on the course website:\nhttps://graphics4science.github.io", "AI": {"tldr": "本文探讨了计算机图形学与科学之间的深远关系，展示了图形学作为科学建模语言的潜力。", "motivation": "旨在弥合计算机图形学与科学之间的词汇差距，推动图形学在科学发现中的应用。", "method": "通过几何推理和物理建模等核心方法，为数据稀缺环境下的科学问题提供解决方案。", "result": "课程展示了图形学在科学中的历史成就、当前贡献及未来潜力。", "conclusion": "呼吁图形学社区参与科学问题，推动科学发现的未来。"}}
{"id": "2506.15746", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2506.15746", "abs": "https://arxiv.org/abs/2506.15746", "authors": ["Kevin Xu", "Risto Miikkulainen"], "title": "Neural Cellular Automata for ARC-AGI", "comment": "8 pages, 5 figures", "summary": "Cellular automata and their differentiable counterparts, Neural Cellular\nAutomata (NCA), are highly expressive and capable of surprisingly complex\nbehaviors. This paper explores how NCAs perform when applied to tasks requiring\nprecise transformations and few-shot generalization, using the Abstraction and\nReasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that\nchallenges their capabilities in ways not previously explored. Specifically,\nthis paper uses gradient-based training to learn iterative update rules that\ntransform input grids into their outputs from the training examples and apply\nthem to the test inputs. Results suggest that gradient-trained NCA models are a\npromising and efficient approach to a range of abstract grid-based tasks from\nARC. Along with discussing the impacts of various design modifications and\ntraining constraints, this work examines the behavior and properties of NCAs\napplied to ARC to give insights for broader applications of self-organizing\nsystems.", "AI": {"tldr": "本文研究了神经细胞自动机（NCA）在需要精确变换和少样本泛化的任务中的表现，特别是在ARC-AGI领域中的应用。", "motivation": "探索NCA在复杂任务中的潜力，尤其是在抽象推理任务中的表现。", "method": "使用基于梯度的训练方法学习迭代更新规则，将输入网格转换为训练示例的输出。", "result": "结果表明，梯度训练的NCA模型在ARC的抽象网格任务中表现高效且具有潜力。", "conclusion": "NCA在自组织系统中的应用具有广泛潜力，本文为相关研究提供了见解。"}}
{"id": "2506.15856", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.15856", "abs": "https://arxiv.org/abs/2506.15856", "authors": ["Michael Ledford", "William Regli"], "title": "Learning to Coordinate Under Threshold Rewards: A Cooperative Multi-Agent Bandit Framework", "comment": null, "summary": "Cooperative multi-agent systems often face tasks that require coordinated\nactions under uncertainty. While multi-armed bandit (MAB) problems provide a\npowerful framework for decentralized learning, most prior work assumes\nindividually attainable rewards. We address the challenging setting where\nrewards are threshold-activated: an arm yields a payoff only when a minimum\nnumber of agents pull it simultaneously, with this threshold unknown in\nadvance. Complicating matters further, some arms are decoys - requiring\ncoordination to activate but yielding no reward - introducing a new challenge\nof wasted joint exploration. We introduce Threshold-Coop-UCB (T-Coop-UCB), a\ndecentralized algorithm that enables agents to jointly learn activation\nthresholds and reward distributions, forming effective coalitions without\ncentralized control. Empirical results show that T-Coop-UCB consistently\noutperforms baseline methods in cumulative reward, regret, and coordination\nmetrics, achieving near-Oracle performance. Our findings underscore the\nimportance of joint threshold learning and decoy avoidance for scalable,\ndecentralized cooperation in complex multi-agent", "AI": {"tldr": "论文提出了一种去中心化算法T-Coop-UCB，用于解决多智能体系统中阈值激活奖励和诱饵臂的协调学习问题。", "motivation": "多智能体系统在不确定性下需要协调行动，而现有MAB框架假设奖励可独立获取，无法处理阈值激活奖励和诱饵臂的挑战。", "method": "提出T-Coop-UCB算法，通过去中心化方式联合学习激活阈值和奖励分布，避免诱饵臂的无效探索。", "result": "实验表明T-Coop-UCB在累积奖励、遗憾和协调指标上优于基线方法，接近Oracle性能。", "conclusion": "联合阈值学习和诱饵避免对复杂多智能体系统的可扩展去中心化合作至关重要。"}}
{"id": "2506.15759", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15759", "abs": "https://arxiv.org/abs/2506.15759", "authors": ["Siyi Xie", "Hanxin Zhu", "Tianyu He", "Xin Li", "Zhibo Chen"], "title": "Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration", "comment": "17 pages, 7 figures. Project page:\n  https://x-drunker.github.io/Sonic4D-project-page/", "summary": "Recent advancements in 4D generation have demonstrated its remarkable\ncapability in synthesizing photorealistic renderings of dynamic 3D scenes.\nHowever, despite achieving impressive visual performance, almost all existing\nmethods overlook the generation of spatial audio aligned with the corresponding\n4D scenes, posing a significant limitation to truly immersive audiovisual\nexperiences. To mitigate this issue, we propose Sonic4D, a novel framework that\nenables spatial audio generation for immersive exploration of 4D scenes.\nSpecifically, our method is composed of three stages: 1) To capture both the\ndynamic visual content and raw auditory information from a monocular video, we\nfirst employ pre-trained expert models to generate the 4D scene and its\ncorresponding monaural audio. 2) Subsequently, to transform the monaural audio\ninto spatial audio, we localize and track the sound sources within the 4D\nscene, where their 3D spatial coordinates at different timestamps are estimated\nvia a pixel-level visual grounding strategy. 3) Based on the estimated sound\nsource locations, we further synthesize plausible spatial audio that varies\nacross different viewpoints and timestamps using physics-based simulation.\nExtensive experiments have demonstrated that our proposed method generates\nrealistic spatial audio consistent with the synthesized 4D scene in a\ntraining-free manner, significantly enhancing the immersive experience for\nusers. Generated audio and video examples are available at\nhttps://x-drunker.github.io/Sonic4D-project-page.", "AI": {"tldr": "Sonic4D提出了一种新框架，为4D场景生成空间音频，提升沉浸式体验。", "motivation": "现有4D生成方法忽略了空间音频的生成，限制了沉浸式体验。", "method": "分三阶段：1) 生成4D场景和单声道音频；2) 定位和跟踪声源；3) 基于物理模拟合成空间音频。", "result": "实验证明，该方法能生成与4D场景一致的空间音频，无需训练。", "conclusion": "Sonic4D显著提升了4D场景的沉浸式体验。"}}
{"id": "2506.15799", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15799", "abs": "https://arxiv.org/abs/2506.15799", "authors": ["Andrew Wagenmaker", "Mitsuhiko Nakamoto", "Yunchu Zhang", "Seohong Park", "Waleed Yagoub", "Anusha Nagabandi", "Abhishek Gupta", "Sergey Levine"], "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning", "comment": null, "summary": "Robotic control policies learned from human demonstrations have achieved\nimpressive results in many real-world applications. However, in scenarios where\ninitial performance is not satisfactory, as is often the case in novel\nopen-world settings, such behavioral cloning (BC)-learned policies typically\nrequire collecting additional human demonstrations to further improve their\nbehavior -- an expensive and time-consuming process. In contrast, reinforcement\nlearning (RL) holds the promise of enabling autonomous online policy\nimprovement, but often falls short of achieving this due to the large number of\nsamples it typically requires. In this work we take steps towards enabling fast\nautonomous adaptation of BC-trained policies via efficient real-world RL.\nFocusing in particular on diffusion policies -- a state-of-the-art BC\nmethodology -- we propose diffusion steering via reinforcement learning (DSRL):\nadapting the BC policy by running RL over its latent-noise space. We show that\nDSRL is highly sample efficient, requires only black-box access to the BC\npolicy, and enables effective real-world autonomous policy improvement.\nFurthermore, DSRL avoids many of the challenges associated with finetuning\ndiffusion policies, obviating the need to modify the weights of the base policy\nat all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,\nand for adapting pretrained generalist policies, illustrating its sample\nefficiency and effective performance at real-world policy improvement.", "AI": {"tldr": "论文提出了一种名为DSRL的方法，通过强化学习在扩散策略的潜在噪声空间中进行调整，实现了高效的自主策略改进，避免了传统方法的不足。", "motivation": "在开放世界场景中，行为克隆策略初始表现不佳时，传统方法需要额外的人类演示，成本高且耗时。强化学习虽能自主改进，但样本效率低。", "method": "提出DSRL方法，在扩散策略的潜在噪声空间上运行强化学习，无需修改基础策略权重，仅需黑盒访问。", "result": "DSRL在模拟和真实机器人任务中表现出高样本效率和有效性能，适用于预训练通用策略的调整。", "conclusion": "DSRL是一种高效且实用的方法，能够实现快速自主策略改进，适用于真实世界的机器人任务。"}}
{"id": "2506.15873", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15873", "abs": "https://arxiv.org/abs/2506.15873", "authors": ["Gregory Croisdale", "Emily Huang", "John Joon Young Chung", "Anhong Guo", "Xu Wang", "Austin Z. Henley", "Cyrus Omar"], "title": "DeckFlow: Iterative Specification on a Multimodal Generative Canvas", "comment": null, "summary": "Generative AI promises to allow people to create high-quality personalized\nmedia. Although powerful, we identify three fundamental design problems with\nexisting tooling through a literature review. We introduce a multimodal\ngenerative AI tool, DeckFlow, to address these problems. First, DeckFlow\nsupports task decomposition by allowing users to maintain multiple\ninterconnected subtasks on an infinite canvas populated by cards connected\nthrough visual dataflow affordances. Second, DeckFlow supports a specification\ndecomposition workflow where an initial goal is iteratively decomposed into\nsmaller parts and combined using feature labels and clusters. Finally, DeckFlow\nsupports generative space exploration by generating multiple prompt and output\nvariations, presented in a grid, that can feed back recursively into the next\ndesign iteration. We evaluate DeckFlow for text-to-image generation against a\nstate-of-practice conversational AI baseline for image generation tasks. We\nthen add audio generation and investigate user behaviors in a more open-ended\ncreative setting with text, image, and audio outputs.", "AI": {"tldr": "DeckFlow是一个多模态生成AI工具，通过任务分解、规范分解和生成空间探索解决现有工具的三大设计问题，并在文本到图像和音频生成任务中进行了评估。", "motivation": "现有生成AI工具存在设计问题，无法高效支持个性化媒体创作，DeckFlow旨在解决这些问题。", "method": "DeckFlow通过无限画布上的卡片连接、规范分解工作流和生成空间探索（网格展示多提示和输出变体）实现功能。", "result": "DeckFlow在文本到图像生成任务中优于现有对话式AI基线，并在多模态（文本、图像、音频）开放创意场景中验证了用户行为。", "conclusion": "DeckFlow通过创新设计解决了生成AI工具的局限性，支持更高效的个性化创作。"}}
{"id": "2506.15755", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15755", "abs": "https://arxiv.org/abs/2506.15755", "authors": ["Xiasi Wang", "Tianliang Yao", "Simin Chen", "Runqi Wang", "Lei YE", "Kuofeng Gao", "Yi Huang", "Yuan Yao"], "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service", "comment": "Accepted by ACL 2025", "summary": "Vision-Language Models (VLMs) have demonstrated great potential in real-world\napplications. While existing research primarily focuses on improving their\naccuracy, the efficiency remains underexplored. Given the real-time demands of\nmany applications and the high inference overhead of VLMs, efficiency\nrobustness is a critical issue. However, previous studies evaluate efficiency\nrobustness under unrealistic assumptions, requiring access to the model\narchitecture and parameters -- an impractical scenario in ML-as-a-service\nsettings, where VLMs are deployed via inference APIs. To address this gap, we\npropose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness\nin a realistic black-box setting. VLMInferSlow incorporates fine-grained\nefficiency modeling tailored to VLM inference and leverages zero-order\noptimization to search for adversarial examples. Experimental results show that\nVLMInferSlow generates adversarial images with imperceptible perturbations,\nincreasing the computational cost by up to 128.47%. We hope this research\nraises the community's awareness about the efficiency robustness of VLMs.", "AI": {"tldr": "论文提出VLMInferSlow方法，用于在真实黑盒场景下评估视觉语言模型（VLM）的效率鲁棒性。", "motivation": "现有研究主要关注VLM的准确性，而效率鲁棒性未充分探索，且现有评估方法假设不切实际。", "method": "提出VLMInferSlow，结合细粒度效率建模和零阶优化搜索对抗样本。", "result": "实验表明，VLMInferSlow生成的对抗图像可增加计算成本高达128.47%。", "conclusion": "研究呼吁关注VLM的效率鲁棒性问题。"}}
{"id": "2506.15686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15686", "abs": "https://arxiv.org/abs/2506.15686", "authors": ["Jiahe Qin", "Junpeng Li", "Changchun Hua", "Yana Yang"], "title": "Learning from M-Tuple Dominant Positive and Unlabeled Data", "comment": null, "summary": "Label Proportion Learning (LLP) addresses the classification problem where\nmultiple instances are grouped into bags and each bag contains information\nabout the proportion of each class. However, in practical applications,\nobtaining precise supervisory information regarding the proportion of instances\nin a specific class is challenging. To better align with real-world application\nscenarios and effectively leverage the proportional constraints of instances\nwithin tuples, this paper proposes a generalized learning framework\n\\emph{MDPU}. Specifically, we first mathematically model the distribution of\ninstances within tuples of arbitrary size, under the constraint that the number\nof positive instances is no less than that of negative instances. Then we\nderive an unbiased risk estimator that satisfies risk consistency based on the\nempirical risk minimization (ERM) method. To mitigate the inevitable\noverfitting issue during training, a risk correction method is introduced,\nleading to the development of a corrected risk estimator. The generalization\nerror bounds of the unbiased risk estimator theoretically demonstrate the\nconsistency of the proposed method. Extensive experiments on multiple datasets\nand comparisons with other relevant baseline methods comprehensively validate\nthe effectiveness of the proposed learning framework.", "AI": {"tldr": "本文提出了一种广义学习框架MDPU，用于解决标签比例学习（LLP）中实例比例信息不精确的问题，通过数学建模和风险校正方法提升分类性能。", "motivation": "在实际应用中，获取实例比例的精确监督信息具有挑战性，因此需要一种更贴合实际场景的学习框架。", "method": "提出MDPU框架，数学建模实例分布，基于ERM方法推导无偏风险估计器，并引入风险校正方法避免过拟合。", "result": "理论证明了无偏风险估计器的泛化误差界限，实验验证了MDPU在多数据集上的有效性。", "conclusion": "MDPU框架在LLP问题中表现出色，为实际应用提供了有效的解决方案。"}}
{"id": "2506.16095", "categories": ["eess.SY", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2506.16095", "abs": "https://arxiv.org/abs/2506.16095", "authors": ["Xun Liu", "Xiaobin Wu", "Jiaqi He", "Rajan Das Gupta"], "title": "Intelligent Operation and Maintenance and Prediction Model Optimization for Improving Wind Power Generation Efficiency", "comment": "7 pages, 3 figures", "summary": "This study explores the effectiveness of predictive maintenance models and\nthe optimization of intelligent Operation and Maintenance (O&M) systems in\nimproving wind power generation efficiency. Through qualitative research,\nstructured interviews were conducted with five wind farm engineers and\nmaintenance managers, each with extensive experience in turbine operations.\nUsing thematic analysis, the study revealed that while predictive maintenance\nmodels effectively reduce downtime by identifying major faults, they often\nstruggle with detecting smaller, gradual failures. Key challenges identified\ninclude false positives, sensor malfunctions, and difficulties in integrating\nnew models with older turbine systems. Advanced technologies such as digital\ntwins, SCADA systems, and condition monitoring have significantly enhanced\nturbine maintenance practices. However, these technologies still require\nimprovements, particularly in AI refinement and real-time data integration. The\nfindings emphasize the need for continuous development to fully optimize wind\nturbine performance and support the broader adoption of renewable energy.", "AI": {"tldr": "研究探讨了预测性维护模型和智能运维系统在提高风力发电效率中的有效性，揭示了其优势与挑战。", "motivation": "优化风力发电的运维系统，提升发电效率，支持可再生能源的广泛应用。", "method": "通过定性研究，对五位经验丰富的风电场工程师和维护经理进行结构化访谈，并采用主题分析法。", "result": "预测性维护模型能有效减少停机时间，但对小故障检测不足；数字孪生等技术提升了维护实践，但仍需改进。", "conclusion": "需持续开发技术以优化风机性能，推动可再生能源的普及。"}}
{"id": "2506.16253", "categories": ["cs.LG", "cs.GT", "cs.IT", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16253", "abs": "https://arxiv.org/abs/2506.16253", "authors": ["Hadar Tal", "Oron Sabag"], "title": "Optimal Online Bookmaking for Any Number of Outcomes", "comment": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2025", "summary": "We study the Online Bookmaking problem, where a bookmaker dynamically updates\nbetting odds on the possible outcomes of an event. In each betting round, the\nbookmaker can adjust the odds based on the cumulative betting behavior of\ngamblers, aiming to maximize profit while mitigating potential loss. We show\nthat for any event and any number of betting rounds, in a worst-case setting\nover all possible gamblers and outcome realizations, the bookmaker's optimal\nloss is the largest root of a simple polynomial. Our solution shows that\nbookmakers can be as fair as desired while avoiding financial risk, and the\nexplicit characterization reveals an intriguing relation between the\nbookmaker's regret and Hermite polynomials. We develop an efficient algorithm\nthat computes the optimal bookmaking strategy: when facing an optimal gambler,\nthe algorithm achieves the optimal loss, and in rounds where the gambler is\nsuboptimal, it reduces the achieved loss to the optimal opportunistic loss, a\nnotion that is related to subgame perfect Nash equilibrium. The key technical\ncontribution to achieve these results is an explicit characterization of the\nBellman-Pareto frontier, which unifies the dynamic programming updates for\nBellman's value function with the multi-criteria optimization framework of the\nPareto frontier in the context of vector repeated games.", "AI": {"tldr": "研究了在线博彩问题，庄家动态调整赔率以最大化利润并减少潜在损失，最优损失为多项式最大根，揭示了与Hermite多项式的关系，并开发了高效算法。", "motivation": "探讨庄家如何在动态调整赔率时平衡公平性与财务风险，同时应对赌徒的最优策略。", "method": "通过动态规划和多目标优化框架，结合Bellman-Pareto前沿，开发高效算法计算最优策略。", "result": "最优损失为多项式最大根，算法在面对最优赌徒时实现最优损失，对次优赌徒减少损失。", "conclusion": "庄家可实现公平性与财务安全的平衡，算法高效且理论结果揭示了与Hermite多项式的关系。"}}
{"id": "2506.15815", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15815", "abs": "https://arxiv.org/abs/2506.15815", "authors": ["Narayan Kandel", "Daljit Singh J. S. Dhillon"], "title": "GratNet: A Photorealistic Neural Shader for Diffractive Surfaces", "comment": null, "summary": "Structural coloration is commonly modeled using wave optics for reliable and\nphotorealistic rendering of natural, quasi-periodic and complex nanostructures.\nSuch models often rely on dense, preliminary or preprocessed data to accurately\ncapture the nuanced variations in diffractive surface reflectances. This heavy\ndata dependency warrants implicit neural representation which has not been\naddressed comprehensively in the current literature. In this paper, we present\na multi-layer perceptron (MLP) based method for data-driven rendering of\ndiffractive surfaces with high accuracy and efficiency. We primarily approach\nthis problem from a data compression perspective to devise a nuanced training\nand modeling method which is attuned to the domain and range characteristics of\ndiffractive reflectance datasets. Importantly, our approach avoids over-fitting\nand has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),\nStructural Similarity Index Measure (SSIM) and a flipping difference evaluator\n(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of\nthe ground-truth. In comparison to a recent state-of-the-art offline,\nwave-optical, forward modeling approach, our method reproduces subjectively\nsimilar results with significant performance gains. We reduce the memory\nfootprint of the raw datasets by two orders of magnitude in general. Lastly, we\ndepict the working of our method with actual surface renderings.", "AI": {"tldr": "提出了一种基于多层感知机（MLP）的数据驱动方法，用于高效准确地渲染衍射表面，显著减少数据依赖性和内存占用。", "motivation": "当前结构着色模型依赖密集预处理数据，缺乏对隐式神经表示的全面研究。", "method": "采用MLP从数据压缩角度建模，避免过拟合，具有鲁棒的重采样行为。", "result": "通过PSNR、SSIM和FLIP评估，高质量重建地面真实数据，内存占用减少两个数量级。", "conclusion": "该方法在性能上优于现有波光学模型，同时保持主观相似的结果。"}}
{"id": "2506.16795", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16795", "abs": "https://arxiv.org/abs/2506.16795", "authors": ["Chengpeng Hu", "Ziming Wang", "Bo Yuan", "Jialin Liu", "Chengqi Zhang", "Xin Yao"], "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning", "comment": null, "summary": "Dynamic material handling (DMH) involves the assignment of dynamically\narriving material transporting tasks to suitable vehicles in real time for\nminimising makespan and tardiness. In real-world scenarios, historical task\nrecords are usually available, which enables the training of a decision policy\non multiple instances consisting of historical records. Recently, reinforcement\nlearning has been applied to solve DMH. Due to the occurrence of dynamic events\nsuch as new tasks, adaptability is highly required. Solving DMH is challenging\nsince constraints including task delay should be satisfied. A feedback is\nreceived only when all tasks are served, which leads to sparse reward. Besides,\nmaking the best use of limited computational resources and historical records\nfor training a robust policy is crucial. The time allocated to different\nproblem instances would highly impact the learning process. To tackle those\nchallenges, this paper proposes a novel adaptive constrained evolutionary\nreinforcement learning (ACERL) approach, which maintains a population of actors\nfor diverse exploration. ACERL accesses each actor for tackling sparse rewards\nand constraint violation to restrict the behaviour of the policy. Moreover,\nACERL adaptively selects the most beneficial training instances for improving\nthe policy. Extensive experiments on eight training and eight unseen test\ninstances demonstrate the outstanding performance of ACERL compared with\nseveral state-of-the-art algorithms. Policies trained by ACERL can schedule the\nvehicles while fully satisfying the constraints. Additional experiments on 40\nunseen noised instances show the robust performance of ACERL. Cross-validation\nfurther presents the overall effectiveness of ACREL. Besides, a rigorous\nablation study highlights the coordination and benefits of each ingredient of\nACERL.", "AI": {"tldr": "本文提出了一种自适应约束进化强化学习（ACERL）方法，用于动态物料搬运（DMH）问题，以最小化完成时间和延迟。ACERL通过多样化的探索、稀疏奖励处理和约束满足，显著优于现有算法。", "motivation": "动态物料搬运（DMH）问题需要实时分配任务，同时满足约束条件（如任务延迟）。传统方法难以应对动态事件和稀疏奖励，因此需要一种适应性强的解决方案。", "method": "ACERL通过维护一组多样化的执行器（actors）进行探索，处理稀疏奖励和约束违反，并自适应选择最有价值的训练实例来优化策略。", "result": "在8个训练和8个未见测试实例上，ACERL表现优异，能够完全满足约束条件。在40个噪声实例上的实验进一步验证了其鲁棒性。", "conclusion": "ACERL在DMH问题中表现出色，其多样化和自适应机制显著提升了策略的鲁棒性和性能。"}}
{"id": "2506.16277", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16277", "abs": "https://arxiv.org/abs/2506.16277", "authors": ["Rico Schrage", "Jari Radler", "Astrid Nieße"], "title": "Coordination of Electrical and Heating Resources by Self-Interested Agents", "comment": null, "summary": "With the rise of distributed energy resources and sector coupling,\ndistributed optimization can be a sensible approach to coordinate decentralized\nenergy resources. Further, district heating, heat pumps, cogeneration, and\nsharing concepts like local energy communities introduce the potential to\noptimize heating and electricity output simultaneously. To solve this issue, we\ntackle the distributed multi-energy scheduling optimization problem, which\ndescribes the optimization of distributed energy generators over multiple time\nsteps to reach a specific target schedule. This work describes a novel\ndistributed hybrid algorithm as a solution approach. This approach is based on\nthe heuristics of gossiping and local search and can simultaneously optimize\nthe private objective of the participants and the collective objective,\nconsidering multiple energy sectors. We show that the algorithm finds globally\nnear-optimal solutions while protecting the stakeholders' economic goals and\nthe plants' technical properties. Two test cases representing pure electrical\nand gas-based technologies are evaluated.", "AI": {"tldr": "提出了一种分布式混合算法，用于协调分布式能源资源的多能源调度优化问题，同时优化私人和集体目标。", "motivation": "随着分布式能源资源和跨部门耦合的兴起，需要一种方法协调分散的能源资源，同时优化电力和热力输出。", "method": "基于八卦算法和局部搜索的启发式方法，提出了一种分布式混合算法。", "result": "算法能够找到全局接近最优解，同时保护利益相关者的经济目标和设备的技术特性。通过纯电力和燃气技术的测试案例验证了其有效性。", "conclusion": "该算法为多能源调度问题提供了一种有效的分布式解决方案，兼顾了私人和集体目标。"}}
{"id": "2506.16020", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16020", "abs": "https://arxiv.org/abs/2506.16020", "authors": ["Zijing Zhao", "Kai Wang", "Hao Huang", "Ying Hu", "Liang He", "Jichen Yang"], "title": "VS-Singer: Vision-Guided Stereo Singing Voice Synthesis with Consistency Schrödinger Bridge", "comment": "Accepted by Interspeech 2025", "summary": "To explore the potential advantages of utilizing spatial cues from images for\ngenerating stereo singing voices with room reverberation, we introduce\nVS-Singer, a vision-guided model designed to produce stereo singing voices with\nroom reverberation from scene images. VS-Singer comprises three modules:\nfirstly, a modal interaction network integrates spatial features into text\nencoding to create a linguistic representation enriched with spatial\ninformation. Secondly, the decoder employs a consistency Schr\\\"odinger bridge\nto facilitate one-step sample generation. Moreover, we utilize the SFE module\nto improve the consistency of audio-visual matching. To our knowledge, this\nstudy is the first to combine stereo singing voice synthesis with visual\nacoustic matching within a unified framework. Experimental results demonstrate\nthat VS-Singer can effectively generate stereo singing voices that align with\nthe scene perspective in a single step.", "AI": {"tldr": "VS-Singer是一个视觉引导模型，通过场景图像生成带有房间混响的立体声歌唱声音，结合了空间特征和文本编码，并采用一致性Schrödinger桥实现一步生成。", "motivation": "探索利用图像空间线索生成带有房间混响的立体声歌唱声音的潜在优势。", "method": "VS-Singer包含三个模块：模态交互网络整合空间特征与文本编码，一致性Schrödinger桥实现一步生成，SFE模块提升视听匹配一致性。", "result": "实验表明，VS-Singer能有效一步生成与场景视角一致的立体声歌唱声音。", "conclusion": "该研究首次将立体声歌唱声音合成与视觉声学匹配结合在一个统一框架中。"}}
{"id": "2506.15828", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15828", "abs": "https://arxiv.org/abs/2506.15828", "authors": ["Emanuele Musumeci", "Michele Brienza", "Francesco Argenziano", "Vincenzo Suriani", "Daniele Nardi", "Domenico D. Bloisi"], "title": "Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning", "comment": null, "summary": "Classical planning in AI and Robotics addresses complex tasks by shifting\nfrom imperative to declarative approaches (e.g., PDDL). However, these methods\noften fail in real scenarios due to limited robot perception and the need to\nground perceptions to planning predicates. This often results in heavily\nhard-coded behaviors that struggle to adapt, even with scenarios where goals\ncan be achieved through relaxed planning. Meanwhile, Large Language Models\n(LLMs) lead to planning systems that leverage commonsense reasoning but often\nat the cost of generating unfeasible and/or unsafe plans. To address these\nlimitations, we present an approach integrating classical planning with LLMs,\nleveraging their ability to extract commonsense knowledge and ground actions.\nWe propose a hierarchical formulation that enables robots to make unfeasible\ntasks tractable by defining functionally equivalent goals through gradual\nrelaxation. This mechanism supports partial achievement of the intended\nobjective, suited to the agent's specific context. Our method demonstrates its\nability to adapt and execute tasks effectively within environments modeled\nusing 3D Scene Graphs through comprehensive qualitative and quantitative\nevaluations. We also show how this method succeeds in complex scenarios where\nother benchmark methods are more likely to fail. Code, dataset, and additional\nmaterial are released to the community.", "AI": {"tldr": "论文提出了一种结合经典规划和大型语言模型（LLMs）的方法，以解决机器人规划中感知局限和动作落地问题，通过分层目标松弛实现任务适应性。", "motivation": "经典规划方法在真实场景中因感知局限和动作落地问题表现不佳，而LLMs虽能利用常识推理生成规划，但常导致不可行或不安全的计划。", "method": "提出了一种分层规划方法，结合经典规划和LLMs，通过目标松弛机制逐步调整任务目标，使其适应机器人能力。", "result": "方法在3D场景图中表现出高效的任务适应和执行能力，在复杂场景中优于基准方法。", "conclusion": "该方法通过结合经典规划和LLMs，有效解决了机器人规划中的适应性和可行性问题，为复杂任务提供了新思路。"}}
{"id": "2506.15883", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15883", "abs": "https://arxiv.org/abs/2506.15883", "authors": ["Jonathan Zong", "Isabella Pedraza Pineros", "Mengzhu Katie Chen", "Daniel Hajas", "Arvind Satyanarayan"], "title": "Semantic Scaffolding: Augmenting Textual Structures with Domain-Specific Groupings for Accessible Data Exploration", "comment": null, "summary": "Drawing connections between interesting groupings of data and their\nreal-world meaning is an important, yet difficult, part of encountering a new\ndataset. A lay reader might see an interesting visual pattern in a chart but\nlack the domain expertise to explain its meaning. Or, a reader might be\nfamiliar with a real-world concept but struggle to express it in terms of a\ndataset's fields. In response, we developed semantic scaffolding, a technique\nfor using domain-specific information from large language models (LLMs) to\nidentify, explain, and formalize semantically meaningful data groupings. We\npresent groupings in two ways: as semantic bins, which segment a field into\ndomain-specific intervals and categories; and data highlights, which annotate\nsubsets of data records with their real-world meaning. We demonstrate and\nevaluate this technique in Olli, an accessible visualization tool that\nexemplifies tensions around explicitly defining groupings while respecting the\nagency of readers to conduct independent data exploration. We conducted a study\nwith 15 blind and low-vision (BLV) users and found that readers used semantic\nscaffolds to quickly understand the meaning of the data, but were often also\ncritically aware of its influence on their interpretation.", "AI": {"tldr": "论文提出了一种名为“语义支架”的技术，利用大语言模型（LLMs）的领域知识来识别、解释和形式化数据分组的语义意义，帮助读者理解数据。", "motivation": "解决读者在遇到新数据集时难以理解数据分组与现实意义之间联系的困难。", "method": "开发了语义支架技术，包括语义分箱（semantic bins）和数据高亮（data highlights），并在可视化工具Olli中实现和评估。", "result": "15名盲人和低视力用户的实验表明，语义支架能快速帮助理解数据，但用户也意识到其对解释的影响。", "conclusion": "语义支架有效辅助数据理解，但需平衡明确分组与读者自主探索的需求。"}}
{"id": "2506.15757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15757", "abs": "https://arxiv.org/abs/2506.15757", "authors": ["Ruoyu Wang", "Tong Yu", "Junda Wu", "Yao Liu", "Julian McAuley", "Lina Yao"], "title": "Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation", "comment": null, "summary": "Visual Language Navigation (VLN) is a fundamental task within the field of\nEmbodied AI, focusing on the ability of agents to navigate complex environments\nbased on natural language instructions. Despite the progress made by existing\nmethods, these methods often present some common challenges. First, they rely\non pre-trained backbone models for visual perception, which struggle with the\ndynamic viewpoints in VLN scenarios. Second, the performance is limited when\nusing pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN\ndomain knowledge. Third, while fine-tuning LLMs and VLMs can improve results,\ntheir computational costs are higher than those without fine-tuning. To address\nthese limitations, we propose Weakly-supervised Partial Contrastive Learning\n(WPCL), a method that enhances an agent's ability to identify objects from\ndynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM\nknowledge into the perception process, without requiring VLM fine-tuning. Our\nmethod enhances the agent's ability to interpret and respond to environmental\ncues while ensuring computational efficiency. Experimental results have shown\nthat our method outperforms the baseline methods on multiple benchmarks, which\nvalidate the effectiveness, robustness and generalizability of our method.", "AI": {"tldr": "论文提出了一种弱监督部分对比学习（WPCL）方法，解决了视觉语言导航（VLN）中动态视角和预训练模型领域知识不足的问题，同时避免了高计算成本。", "motivation": "现有方法依赖预训练模型，难以应对动态视角，且未调优的LLMs/VLMs性能有限，调优则计算成本高。", "method": "提出WPCL，通过弱监督部分对比学习，将预训练VLM知识融入感知过程，无需调优。", "result": "实验表明，WPCL在多个基准测试中优于基线方法，验证了其有效性、鲁棒性和泛化性。", "conclusion": "WPCL在提升VLN任务性能的同时，保持了计算效率，为相关领域提供了新思路。"}}
{"id": "2506.15687", "categories": ["cs.LG", "stat.ML", "65M70, 65N99, 68U99, 68T07"], "pdf": "https://arxiv.org/pdf/2506.15687", "abs": "https://arxiv.org/abs/2506.15687", "authors": ["Yajie Ji", "Yanlai Chen", "Shawn Koohy"], "title": "S$^2$GPT-PINNs: Sparse and Small models for PDEs", "comment": "17 pages,6 figures", "summary": "We propose S$^2$GPT-PINN, a sparse and small model for solving parametric\npartial differential equations (PDEs). Similar to Small Language Models (SLMs),\nS$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and\ncharacterized by its compact architecture and minimal computational power.\nLeveraging a small amount of extremely high quality data via a mathematically\nrigorous greedy algorithm that is enabled by the large full-order models,\nS$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to\nachieve extremely high efficiency via two levels of customizations. The first\nis knowledge distillation via task-specific activation functions that are\ntransferred from Pre-Trained PINNs. The second is a judicious down-sampling\nwhen calculating the physics-informed loss of the network compressing the\nnumber of data sites by orders of magnitude to the size of the small model.", "AI": {"tldr": "S$^2$GPT-PINN是一种稀疏小型模型，用于求解参数化偏微分方程（PDEs），通过知识蒸馏和智能降采样实现高效计算。", "motivation": "针对特定领域的PDEs，设计一种计算资源需求低但效率高的模型，以解决传统PINNs参数过多的问题。", "method": "1. 通过预训练PINNs进行知识蒸馏，使用任务特定的激活函数；2. 在计算物理损失时进行智能降采样，大幅减少数据点数量。", "result": "S$^2$GPT-PINN用远少于PINNs的参数实现了极高的计算效率。", "conclusion": "S$^2$GPT-PINN为特定领域PDEs提供了一种高效且资源友好的解决方案。"}}
{"id": "2506.16254", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.16254", "abs": "https://arxiv.org/abs/2506.16254", "authors": ["Hossein Mohammadi Firouzjaei", "Rafaela Scaciota", "Sumudu Samarakoon"], "title": "Multi-Task Lifelong Reinforcement Learning for Wireless Sensor Networks", "comment": null, "summary": "Enhancing the sustainability and efficiency of wireless sensor networks (WSN)\nin dynamic and unpredictable environments requires adaptive communication and\nenergy harvesting strategies. We propose a novel adaptive control strategy for\nWSNs that optimizes data transmission and EH to minimize overall energy\nconsumption while ensuring queue stability and energy storing constraints under\ndynamic environmental conditions. The notion of adaptability therein is\nachieved by transferring the known environment-specific knowledge to new\nconditions resorting to the lifelong reinforcement learning concepts. We\nevaluate our proposed method against two baseline frameworks: Lyapunov-based\noptimization, and policy-gradient reinforcement learning (RL). Simulation\nresults demonstrate that our approach rapidly adapts to changing environmental\nconditions by leveraging transferable knowledge, achieving near-optimal\nperformance approximately $30\\%$ faster than the RL method and $60\\%$ faster\nthan the Lyapunov-based approach.", "AI": {"tldr": "提出了一种自适应控制策略，通过终身强化学习优化无线传感器网络的能量消耗和数据传输。", "motivation": "提升无线传感器网络在动态环境中的可持续性和效率。", "method": "结合终身强化学习，将已知环境知识迁移到新条件中。", "result": "比强化学习快30%，比Lyapunov优化快60%，实现近最优性能。", "conclusion": "自适应策略显著提升了动态环境下的网络性能。"}}
{"id": "2506.16736", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2506.16736", "abs": "https://arxiv.org/abs/2506.16736", "authors": ["John Lazarsfeld", "Georgios Piliouras", "Ryann Sim", "Stratis Skoulakis"], "title": "Optimism Without Regularization: Constant Regret in Zero-Sum Games", "comment": null, "summary": "This paper studies the optimistic variant of Fictitious Play for learning in\ntwo-player zero-sum games. While it is known that Optimistic FTRL -- a\nregularized algorithm with a bounded stepsize parameter -- obtains constant\nregret in this setting, we show for the first time that similar, optimal rates\nare also achievable without regularization: we prove for two-strategy games\nthat Optimistic Fictitious Play (using any tiebreaking rule) obtains only\nconstant regret, providing surprising new evidence on the ability of\nnon-no-regret algorithms for fast learning in games. Our proof technique\nleverages a geometric view of Optimistic Fictitious Play in the dual space of\npayoff vectors, where we show a certain energy function of the iterates remains\nbounded over time. Additionally, we also prove a regret lower bound of\n$\\Omega(\\sqrt{T})$ for Alternating Fictitious Play. In the unregularized\nregime, this separates the ability of optimism and alternation in achieving\n$o(\\sqrt{T})$ regret.", "AI": {"tldr": "本文研究了乐观虚构博弈在两人零和游戏中的学习性能，首次证明无正则化的乐观虚构博弈也能实现常数遗憾，并通过几何视角和能量函数分析其机制。", "motivation": "探索无正则化算法在零和游戏中的性能，验证乐观虚构博弈是否也能实现类似正则化算法的低遗憾。", "method": "采用乐观虚构博弈，通过双空间几何视角和能量函数分析其迭代过程。", "result": "证明乐观虚构博弈在双策略游戏中仅产生常数遗憾，同时证明交替虚构博弈的遗憾下界为Ω(√T)。", "conclusion": "乐观虚构博弈在无正则化条件下也能实现最优性能，而交替虚构博弈则无法达到次线性遗憾。"}}
{"id": "2506.15821", "categories": ["cs.GR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.15821", "abs": "https://arxiv.org/abs/2506.15821", "authors": ["Pham Khai Nguyen Do", "Bao Nguyen Tran", "Nam Nguyen", "Duc Dung Nguyen"], "title": "VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal", "comment": null, "summary": "Recent advances in Novel View Synthesis (NVS) and 3D generation have\nsignificantly improved editing tasks, with a primary emphasis on maintaining\ncross-view consistency throughout the generative process. Contemporary methods\ntypically address this challenge using a dual-strategy framework: performing\nconsistent 2D inpainting across all views guided by embedded priors either\nexplicitly in pixel space or implicitly in latent space; and conducting 3D\nreconstruction with additional consistency guidance. Previous strategies, in\nparticular, often require an initial 3D reconstruction phase to establish\ngeometric structure, introducing considerable computational overhead. Even with\nthe added cost, the resulting reconstruction quality often remains suboptimal.\nIn this paper, we present VEIGAR, a computationally efficient framework that\noutperforms existing methods without relying on an initial reconstruction\nphase. VEIGAR leverages a lightweight foundation model to reliably align priors\nexplicitly in the pixel space. In addition, we introduce a novel supervision\nstrategy based on scale-invariant depth loss, which removes the need for\ntraditional scale-and-shift operations in monocular depth regularization.\nThrough extensive experimentation, VEIGAR establishes a new state-of-the-art\nbenchmark in reconstruction quality and cross-view consistency, while achieving\na threefold reduction in training time compared to the fastest existing method,\nhighlighting its superior balance of efficiency and effectiveness.", "AI": {"tldr": "VEIGAR是一种高效的新视角合成框架，无需初始3D重建阶段，通过轻量级基础模型和尺度不变深度损失监督策略，显著提升了重建质量和跨视角一致性。", "motivation": "现有方法通常需要初始3D重建阶段，计算成本高且重建质量不佳，VEIGAR旨在解决这一问题。", "method": "VEIGAR利用轻量级基础模型在像素空间显式对齐先验，并引入尺度不变深度损失监督策略。", "result": "VEIGAR在重建质量和跨视角一致性上达到新SOTA，训练时间减少三倍。", "conclusion": "VEIGAR在效率和效果上实现了优越的平衡。"}}
{"id": "2506.17169", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17169", "abs": "https://arxiv.org/abs/2506.17169", "authors": ["Denis Larionov", "Nikolay Bazenkov", "Mikhail Kiselev"], "title": "Continual Learning with Columnar Spiking Neural Networks", "comment": "12 pages, 3 figures", "summary": "This study investigates columnar-organized spiking neural networks (SNNs) for\ncontinual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered\nNetwork), we show that microcolumns adapt most efficiently to new tasks when\nthey lack shared structure with prior learning. We demonstrate how CoLaNET\nhyperparameters govern the trade-off between retaining old knowledge\n(stability) and acquiring new information (plasticity). Our optimal\nconfiguration learns ten sequential MNIST tasks effectively, maintaining 92%\naccuracy on each. It shows low forgetting, with only 4% performance degradation\non the first task after training on nine subsequent tasks.", "AI": {"tldr": "研究了基于柱状结构的脉冲神经网络（SNNs）在持续学习和灾难性遗忘中的应用，通过CoLaNET展示了微柱在新任务中的高效适应能力。", "motivation": "探索柱状结构SNNs在持续学习中的表现，解决灾难性遗忘问题。", "method": "使用CoLaNET（柱状分层网络），研究微柱在新任务中的适应效率，分析超参数对稳定性和可塑性的影响。", "result": "最优配置在十个MNIST任务序列中保持92%的准确率，遗忘率仅4%。", "conclusion": "CoLaNET在持续学习中表现出色，平衡了稳定性和可塑性，有效减少了灾难性遗忘。"}}
{"id": "2506.16311", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16311", "abs": "https://arxiv.org/abs/2506.16311", "authors": ["Aijing Kong", "Chengkai Xu", "Xian Wu", "Xinbo Chen", "Peng Hang"], "title": "Towards Emergency Scenarios: An Integrated Decision-making Framework of Multi-lane Platoon Reorganization", "comment": null, "summary": "To enhance the ability for vehicle platoons to respond to emergency\nscenarios, a platoon distribution reorganization decision-making framework is\nproposed. This framework contains platoon distribution layer, vehicle\ncooperative decision-making layer and vehicle planning and control layer.\nFirstly, a reinforcement-learning-based platoon distribution model is\npresented, where a risk potential field is established to quantitatively assess\ndriving risks, and a reward function tailored to the platoon reorganization\nprocess is constructed. Then, a coalition-game-based vehicle cooperative\ndecision-making model is put forward, modeling the cooperative relationships\namong vehicles through dividing coalitions and generating the optimal decision\nresults for each vehicle. Additionally, a novel graph-theory-based Platoon\nDisposition Index (PDI) is incorporated into the game reward function to\nmeasure the platoon's distribution state during the reorganization process, in\norder to accelerating the reorganization process. Finally, the validation of\nthe proposed framework is conducted in two high-risk scenarios under random\ntraffic flows. The results show that, compared to the baseline models, the\nproposed method can significantly reduce the collision rate and improve driving\nefficiency. Moreover, the model with PDI can significantly decrease the platoon\nformation reorganization time and improve the reorganization efficiency.", "AI": {"tldr": "提出了一种基于强化学习和联盟博弈的车辆编队重组决策框架，显著降低碰撞率并提高效率。", "motivation": "提升车辆编队在紧急场景下的响应能力。", "method": "结合强化学习的编队分布模型、联盟博弈的车辆协同决策模型，以及基于图论的编队分布指数（PDI）。", "result": "在随机交通流的高风险场景中验证，显著降低碰撞率、提高重组效率。", "conclusion": "提出的框架有效优化了编队重组过程，提升了安全性和效率。"}}
{"id": "2506.16127", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16127", "abs": "https://arxiv.org/abs/2506.16127", "authors": ["Shoutrik Das", "Nishant Singh", "Arjun Gangwar", "S Umesh"], "title": "Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching", "comment": "Accepted at Interspeech 2025", "summary": "Dysarthria is a neurological disorder that significantly impairs speech\nintelligibility, often rendering affected individuals unable to communicate\neffectively. This necessitates the development of robust dysarthric-to-regular\nspeech conversion techniques. In this work, we investigate the utility and\nlimitations of self-supervised learning (SSL) features and their quantized\nrepresentations as an alternative to mel-spectrograms for speech generation.\nAdditionally, we explore methods to mitigate speaker variability by generating\nclean speech in a single-speaker voice using features extracted from WavLM. To\nthis end, we propose a fully non-autoregressive approach that leverages\nConditional Flow Matching (CFM) with Diffusion Transformers to learn a direct\nmapping from dysarthric to clean speech. Our findings highlight the\neffectiveness of discrete acoustic units in improving intelligibility while\nachieving faster convergence compared to traditional mel-spectrogram-based\napproaches.", "AI": {"tldr": "该论文研究了利用自监督学习（SSL）特征及其量化表示替代梅尔频谱图进行语音生成的方法，提出了一种基于条件流匹配（CFM）和扩散变换器的非自回归方法，用于将构音障碍语音转换为清晰语音。", "motivation": "构音障碍严重影响语音清晰度，导致患者沟通困难，因此需要开发有效的语音转换技术。", "method": "采用自监督学习特征和量化表示，结合条件流匹配（CFM）与扩散变换器，提出非自回归方法。", "result": "离散声学单元显著提高了语音清晰度，且收敛速度优于传统梅尔频谱图方法。", "conclusion": "该方法在提升构音障碍语音清晰度方面具有潜力，且效率更高。"}}
{"id": "2506.15847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15847", "abs": "https://arxiv.org/abs/2506.15847", "authors": ["Arpit Bahety", "Arnav Balaji", "Ben Abbatematteo", "Roberto Martín-Martín"], "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation", "comment": null, "summary": "For robots to become efficient helpers in the home, they must learn to\nperform new mobile manipulation tasks simply by watching humans perform them.\nLearning from a single video demonstration from a human is challenging as the\nrobot needs to first extract from the demo what needs to be done and how,\ntranslate the strategy from a third to a first-person perspective, and then\nadapt it to be successful with its own morphology. Furthermore, to mitigate the\ndependency on costly human monitoring, this learning process should be\nperformed in a safe and autonomous manner. We present SafeMimic, a framework to\nlearn new mobile manipulation skills safely and autonomously from a single\nthird-person human video. Given an initial human video demonstration of a\nmulti-step mobile manipulation task, SafeMimic first parses the video into\nsegments, inferring both the semantic changes caused and the motions the human\nexecuted to achieve them and translating them to an egocentric reference. Then,\nit adapts the behavior to the robot's own morphology by sampling candidate\nactions around the human ones, and verifying them for safety before execution\nin a receding horizon fashion using an ensemble of safety Q-functions trained\nin simulation. When safe forward progression is not possible, SafeMimic\nbacktracks to previous states and attempts a different sequence of actions,\nadapting both the trajectory and the grasping modes when required for its\nmorphology. As a result, SafeMimic yields a strategy that succeeds in the\ndemonstrated behavior and learns task-specific actions that reduce exploration\nin future attempts. Our experiments show that our method allows robots to\nsafely and efficiently learn multi-step mobile manipulation behaviors from a\nsingle human demonstration, from different users, and in different\nenvironments, with improvements over state-of-the-art baselines across seven\ntasks", "AI": {"tldr": "SafeMimic框架通过单次人类视频演示，让机器人安全自主地学习移动操作技能。", "motivation": "解决机器人从人类视频中学习移动操作任务的挑战，减少对人类监控的依赖。", "method": "解析视频为语义和动作片段，转换为第一人称视角，并通过安全验证采样候选动作。", "result": "机器人能安全高效地学习多步骤移动操作行为，并在不同环境和用户中表现优于现有方法。", "conclusion": "SafeMimic为机器人学习复杂任务提供了一种安全、高效的解决方案。"}}
{"id": "2506.16008", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16008", "abs": "https://arxiv.org/abs/2506.16008", "authors": ["Yuichiro Fujimoto"], "title": "ChatAR: Conversation Support using Large Language Model and Augmented Reality", "comment": null, "summary": "Engaging in smooth conversations with others is a crucial social skill.\nHowever, differences in knowledge between conversation participants can\nsometimes hinder effective communication. To tackle this issue, this study\nproposes a real-time support system that integrates head-mounted display\n(HMD)-based augmented reality (AR) technology with large language models\n(LLMs). This system facilitates conversation by recognizing keywords during\ndialogue, generating relevant information using the LLM, reformatting it, and\npresenting it to the user via the HMD. A significant issue with this system is\nthat the user's eye movements may reveal to the conversation partner that they\nare reading the displayed text. This study also proposes a method for\npresenting information that takes into account appropriate eye movements during\nconversation. Two experiments were conducted to evaluate the effectiveness of\nthe proposed system. The first experiment revealed that the proposed\ninformation presentation method reduces the likelihood of the conversation\npartner noticing that the user is reading the displayed text. The second\nexperiment demonstrated that the proposed method led to a more balanced speech\nratio between the user and the conversation partner, as well as a increase in\nthe perceived excitement of the conversation.", "AI": {"tldr": "研究提出了一种结合头戴式显示器（HMD）和大型语言模型（LLM）的实时对话支持系统，通过关键词识别和信息生成改善沟通。同时提出了一种考虑眼神移动的信息展示方法，实验证明其有效性和对话平衡性。", "motivation": "解决对话中因知识差异导致的沟通障碍。", "method": "结合HMD和LLM，实时生成并展示相关信息，优化信息展示方式以减少被察觉的可能性。", "result": "实验表明，优化后的信息展示方法减少了被察觉的风险，并提升了对话的平衡性和兴奋感。", "conclusion": "该系统能有效改善对话质量，尤其在知识差异较大的场景中。"}}
{"id": "2506.15806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15806", "abs": "https://arxiv.org/abs/2506.15806", "authors": ["Akarshani Ramanayake", "Nihal Kodikara"], "title": "Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving", "comment": null, "summary": "In crowded urban environments where traffic is dense, current technologies\nstruggle to oversee tight navigation, but surface-level understanding allows\nautonomous vehicles to safely assess proximity to surrounding obstacles. 3D or\n2D scene mapping of the surrounding objects is an essential task in addressing\nthe above problem. Despite its importance in dense vehicle traffic conditions,\n3D scene reconstruction of object shapes with higher boundary level accuracy is\nnot yet entirely considered in current literature. The sign distance function\nrepresents any shape through parameters that calculate the distance from any\npoint in space to the closest obstacle surface, making it more efficient in\nterms of storage. In recent studies, researchers have started to formulate\nproblems with Implicit 3D reconstruction methods in the autonomous driving\ndomain, highlighting the possibility of using sign distance function to map\nobstacles effectively. This research addresses this gap by developing a\nlearning-based 3D scene reconstruction methodology that leverages LiDAR data\nand a deep neural network to build a the static Signed Distance Function (SDF)\nmaps. Unlike traditional polygonal representations, this approach has the\npotential to map 3D obstacle shapes with more boundary-level details. Our\npreliminary results demonstrate that this method would significantly enhance\ncollision detection performance, particularly in congested and dynamic\nenvironments.", "AI": {"tldr": "论文提出了一种基于学习的3D场景重建方法，利用LiDAR数据和深度神经网络构建静态SDF地图，以解决密集交通环境中障碍物边界精度不足的问题。", "motivation": "当前技术在密集城市交通环境中难以精确导航，3D场景重建的边界精度不足，而SDF在存储效率和形状表示上更具优势。", "method": "结合LiDAR数据和深度神经网络，开发了一种学习型3D场景重建方法，生成静态SDF地图。", "result": "初步结果表明，该方法能显著提升碰撞检测性能，尤其在拥堵和动态环境中。", "conclusion": "该方法通过SDF实现了更高精度的3D障碍物形状映射，为自动驾驶领域提供了更有效的解决方案。"}}
{"id": "2506.15688", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15688", "abs": "https://arxiv.org/abs/2506.15688", "authors": ["Hui Ma", "Kai Yang", "Man-On Pun"], "title": "Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism", "comment": null, "summary": "Cellular traffic prediction is of great importance for operators to manage\nnetwork resources and make decisions. Traffic is highly dynamic and influenced\nby many exogenous factors, which would lead to the degradation of traffic\nprediction accuracy. This paper proposes an end-to-end framework with two\nvariants to explicitly characterize the spatiotemporal patterns of cellular\ntraffic among neighboring cells. It uses convolutional neural networks with an\nattention mechanism to capture the spatial dynamics and Kalman filter for\ntemporal modelling. Besides, we can fully exploit the auxiliary information\nsuch as social activities to improve prediction performance. We conduct\nextensive experiments on three real-world datasets. The results show that our\nproposed models outperform the state-of-the-art machine learning techniques in\nterms of prediction accuracy.", "AI": {"tldr": "论文提出了一种端到端框架，通过卷积神经网络和注意力机制捕捉空间动态，结合卡尔曼滤波进行时间建模，显著提升了蜂窝流量预测的准确性。", "motivation": "蜂窝流量预测对运营商管理网络资源和决策至关重要，但流量的高度动态性和外部因素影响导致预测准确性下降。", "method": "提出两种变体的端到端框架，利用卷积神经网络和注意力机制捕捉空间动态，卡尔曼滤波建模时间动态，并利用辅助信息（如社交活动）提升性能。", "result": "在三个真实数据集上的实验表明，所提模型在预测准确性上优于现有机器学习技术。", "conclusion": "该框架能有效捕捉时空模式，结合辅助信息显著提升预测性能。"}}
{"id": "2506.16257", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.16257", "abs": "https://arxiv.org/abs/2506.16257", "authors": ["Francesco Conte", "Fernando Mancilla-David", "Amritansh Sagar", "Chendan Li", "Federico Silvestro", "Samuele Grillo"], "title": "Detailed Small-Signal Stability Analysis of the Cigré High-Voltage Network Penetrated by Grid-Following Inverter-Based Resources", "comment": null, "summary": "This paper presents a detailed small-signal stability analysis of a modified\nversion of the Cigr\\'e European high-voltage network, where one of the\nsynchronous generators is replaced by a grid-following inverter-based resource\n(IBR). The analysis focuses on the influence of the parameters defining the\ngrid-following IBR control scheme on the stability of the system. Given a set\nof potential grid configurations and the value of the IBR control parameters,\nstability is verified by the direct eigenvalue analysis of a high-detailed\nlinearized model of the overall Cigr\\'e network. Starting from this procedure,\nwe propose an adaptive sampling method for training a support vector machine\nclassifier able to estimate the probability of stability of the power system\nover a domain defined by candidate intervals of the considered parameters. The\ntraining of the classifier is refined to identify with more accuracy the\nboundaries of the parameters' stability regions. The obtained results are then\ncompared with those obtained by representing the grid with the classical\nTh\\'evenin equivalent. Results suggest that, when the Th\\'evenin equivalent is\naccurate, the predicted stability region is conservative yet contained within\nthat of the full network.", "AI": {"tldr": "论文分析了改进版Cigré欧洲高压电网的小信号稳定性，重点研究了基于逆变器的资源（IBR）控制参数对系统稳定性的影响，并提出了一种自适应采样方法训练支持向量机分类器来预测稳定性概率。", "motivation": "研究电网中同步发电机被IBR替代后的稳定性问题，探索IBR控制参数对系统的影响。", "method": "通过高精度线性化模型进行直接特征值分析，提出自适应采样方法训练支持向量机分类器，并与Thévenin等效模型结果对比。", "result": "Thévenin等效模型在准确时预测的稳定性区域保守但包含在全网模型中。", "conclusion": "自适应采样方法能有效预测稳定性边界，Thévenin等效模型在特定条件下可用，但需谨慎使用。"}}
{"id": "2506.15860", "categories": ["cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15860", "abs": "https://arxiv.org/abs/2506.15860", "authors": ["Hasan Balci", "Augustin Luna"], "title": "User-Guided Force-Directed Graph Layout", "comment": null, "summary": "Visual analysis of relational data is essential for many real-world analytics\ntasks, with layout quality being key to interpretability. However, existing\nlayout algorithms often require users to navigate complex parameters to express\ntheir intent. We present a user-guided force-directed layout approach that\nenables intuitive control through freehand sketching. Our method uses classical\nimage analysis techniques to extract structural information from sketches,\nwhich is then used to generate positional constraints that guide the layout\nprocess. We evaluate the approach on various real and synthetic graphs ranging\nfrom small to medium scale, demonstrating its ability to produce layouts\naligned with user expectations. An implementation of our method along with\ndocumentation and a demo page is freely available on GitHub at\nhttps://github.com/sciluna/uggly.", "AI": {"tldr": "论文提出了一种用户引导的力导向布局方法，通过手绘草图实现直观控制，避免了复杂参数调整。", "motivation": "现有布局算法通常需要用户调整复杂参数来表达意图，影响了用户体验和布局质量。", "method": "采用经典图像分析技术从草图中提取结构信息，生成位置约束以指导布局过程。", "result": "在多种真实和合成图形上验证了该方法，能够生成符合用户预期的布局。", "conclusion": "该方法直观易用，布局效果良好，已在GitHub上开源实现。"}}
{"id": "2506.17040", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.17040", "abs": "https://arxiv.org/abs/2506.17040", "authors": ["Lorenzo Tausani", "Paolo Muratore", "Morgan B. Talbot", "Giacomo Amerio", "Gabriel Kreiman", "Davide Zoccolan"], "title": "Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance", "comment": "21 pages, 9 figures", "summary": "Uncovering which features' combinations high-level visual units encode is\ncritical to understand how images are transformed into representations that\nsupport recognition. While existing feature visualization approaches typically\ninfer a unit's most exciting images, this is insufficient to reveal the\nmanifold of transformations under which responses remain invariant, which is\nkey to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),\nan unbiased, model-agnostic, and gradient-free framework to systematically\ncharacterize a unit's invariance landscape and its vulnerability to adversarial\nperturbations in both biological and artificial visual systems. SnS frames\nthese transformations as bi-objective optimization problems. To probe\ninvariance, SnS seeks image perturbations that maximally alter the\nrepresentation of a reference stimulus in a given processing stage while\npreserving unit activation. To probe adversarial sensitivity, SnS seeks\nperturbations that minimally alter the stimulus while suppressing unit\nactivation. Applied to convolutional neural networks (CNNs), SnS revealed image\nvariations that were further from a reference image in pixel-space than those\nproduced by affine transformations, while more strongly preserving the target\nunit's response. The discovered invariant images differed dramatically\ndepending on the choice of image representation used for optimization:\npixel-level changes primarily affected luminance and contrast, while stretching\nmid- and late-layer CNN representations altered texture and pose respectively.\nNotably, the invariant images from robust networks were more recognizable by\nhuman subjects than those from standard networks, supporting the higher\nfidelity of robust CNNs as models of the visual system.", "AI": {"tldr": "SnS框架通过双目标优化系统表征视觉单元的不变性及其对对抗扰动的脆弱性，揭示了不同层级CNN表征对图像变换的响应差异。", "motivation": "理解高级视觉单元如何编码特征组合以支持图像识别，并揭示其不变性变换的多样性。", "method": "提出Stretch-and-Squeeze（SnS）框架，通过双目标优化问题探究单元的不变性和对抗敏感性。", "result": "SnS发现CNN中不同层级的表征对图像变换（如亮度、纹理、姿态）的响应不同，且鲁棒网络的生成图像更易被人类识别。", "conclusion": "SnS为视觉系统的建模提供了新工具，支持鲁棒CNN作为视觉系统模型的更高保真度。"}}
{"id": "2506.16718", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16718", "abs": "https://arxiv.org/abs/2506.16718", "authors": ["Chenxu Wang", "Yonggang Jin", "Cheng Hu", "Youpeng Zhao", "Zipeng Dai", "Jian Zhao", "Shiyu Huang", "Liuyu Xiang", "Junge Zhang", "Zhaofeng He"], "title": "Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation", "comment": "This manuscript is under submission to Neurocomputing", "summary": "Adapting a single agent to a new multi-agent system brings challenges,\nnecessitating adjustments across various tasks, environments, and interactions\nwith unknown teammates and opponents. Addressing this challenge is highly\ncomplex, and researchers have proposed two simplified scenarios, Multi-agent\nreinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on\nthese foundations, we propose a more comprehensive setting, Agent\nCollaborative-Competitive Adaptation (ACCA), which evaluates an agent to\ngeneralize across diverse scenarios, tasks, and interactions with both\nunfamiliar opponents and teammates. In ACCA, agents adjust to task and\nenvironmental changes, collaborate with unseen teammates, and compete against\nunknown opponents. We introduce a new modeling approach, Multi-Retrieval and\nDynamic Generation (MRDG), that effectively models both teammates and opponents\nusing their behavioral trajectories. This method incorporates a positional\nencoder for varying team sizes and a hypernetwork module to boost agents'\nlearning and adaptive capabilities. Additionally, a viewpoint alignment module\nharmonizes the observational perspectives of retrieved teammates and opponents\nwith the learning agent. Extensive tests in benchmark scenarios like SMAC,\nOvercooked-AI, and Melting Pot show that MRDG significantly improves robust\ncollaboration and competition with unseen teammates and opponents, surpassing\nestablished baselines. Our code is available at:\nhttps://github.com/vcis-wangchenxu/MRDG.git", "AI": {"tldr": "论文提出了一种新的多智能体协作-竞争适应（ACCA）场景，并开发了多检索与动态生成（MRDG）方法，以提升智能体在未知队友和对手环境中的适应能力。", "motivation": "解决智能体在新多智能体系统中适应复杂任务、环境和未知交互的挑战。", "method": "提出MRDG方法，利用行为轨迹建模队友和对手，结合位置编码、超网络模块和视角对齐模块。", "result": "在SMAC、Overcooked-AI和Melting Pot等基准测试中，MRDG显著优于现有基线。", "conclusion": "MRDG方法在多智能体协作与竞争中表现出色，为复杂适应问题提供了有效解决方案。"}}
{"id": "2506.16225", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16225", "abs": "https://arxiv.org/abs/2506.16225", "authors": ["Jiale Liu", "Dandan Peng", "Huan Wang", "Chenyu Liu", "Yan-Fu Li", "Min Xie"], "title": "AeroGPT: Leveraging Large-Scale Audio Model for Aero-Engine Bearing Fault Diagnosis", "comment": null, "summary": "Aerospace engines, as critical components in aviation and aerospace\nindustries, require continuous and accurate fault diagnosis to ensure\noperational safety and prevent catastrophic failures. While deep learning\ntechniques have been extensively studied in this context, they output logits or\nconfidence scores, necessitating post-processing to derive actionable insights.\nFurthermore, the potential of large-scale audio models in this domain remains\nlargely untapped. To address these limitations, this paper proposes AeroGPT, a\nnovel framework that transfers knowledge from general audio domain to\naero-engine bearing fault diagnosis. AeroGPT is a framework based on\nlarge-scale audio model that incorporates Vibration Signal Alignment (VSA) to\nadapt general audio knowledge to domain-specific vibration patterns, and\ncombines Generative Fault Classification (GFC) to directly output interpretable\nfault labels. This approach eliminates the need for post-processing of fault\nlabels, supports interactive, interpretable, and actionable fault diagnosis,\nthereby greatly enhancing industrial applicability. Through comprehensive\nexperimental validation on two aero-engine bearing datasets, AeroGPT achieved\nexceptional performance with 98.94% accuracy on the DIRG dataset and perfect\n100% classification on the HIT bearing dataset, surpassing traditional deep\nlearning approaches. Additional Qualitative analysis validates the\neffectiveness of our approach and highlights the potential of large-scale\nmodels to revolutionize fault diagnosis.", "AI": {"tldr": "AeroGPT是一个基于大规模音频模型的框架，通过振动信号对齐和生成式故障分类，直接输出可解释的故障标签，显著提升航空发动机故障诊断的准确性和实用性。", "motivation": "传统深度学习方法需要后处理输出，且大规模音频模型在航空发动机故障诊断中的应用尚未充分探索。", "method": "提出AeroGPT框架，结合振动信号对齐（VSA）和生成式故障分类（GFC），直接输出可解释的故障标签。", "result": "在两个数据集上表现优异，DIRG数据集准确率98.94%，HIT轴承数据集达到100%。", "conclusion": "AeroGPT展示了大规模模型在故障诊断中的潜力，显著提升了工业适用性。"}}
{"id": "2506.15849", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15849", "abs": "https://arxiv.org/abs/2506.15849", "authors": ["Kirill Muravyev", "Vasily Yuryev", "Oleg Bulichev", "Dmitry Yudin", "Konstantin Yakovlev"], "title": "PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps", "comment": "This version was submitted and rejected from IROS 2025 conference", "summary": "Localization in the environment is one of the crucial tasks of navigation of\na mobile robot or a self-driving vehicle. For long-range routes, performing\nlocalization within a dense global lidar map in real time may be difficult, and\nthe creation of such a map may require much memory. To this end, leveraging\ntopological maps may be useful. In this work, we propose PRISM-Loc -- a\ntopological map-based approach for localization in large environments. The\nproposed approach leverages a twofold localization pipeline, which consists of\nglobal place recognition and estimation of the local pose inside the found\nlocation. For local pose estimation, we introduce an original lidar scan\nmatching algorithm, which is based on 2D features and point-based optimization.\nWe evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and\ncompare it against the state-of-the-art metric map-based and place\nrecognition-based competitors. The results of the experiments show that the\nproposed method outperforms its competitors both quality-wise and\ncomputationally-wise.", "AI": {"tldr": "PRISM-Loc是一种基于拓扑地图的定位方法，用于大型环境中的实时定位，结合全局地点识别和局部位姿估计，性能优于现有方法。", "motivation": "在长距离路径中，使用密集全局激光雷达地图进行实时定位可能困难且内存消耗大，因此需要利用拓扑地图。", "method": "提出PRISM-Loc方法，采用双重定位流程：全局地点识别和局部位姿估计。局部位姿估计使用基于2D特征和点优化的激光雷达扫描匹配算法。", "result": "在3公里路径的ITLP-Campus数据集上测试，PRISM-Loc在质量和计算效率上均优于现有方法。", "conclusion": "PRISM-Loc是一种高效且性能优越的拓扑地图定位方法，适用于大型环境。"}}
{"id": "2506.16010", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16010", "abs": "https://arxiv.org/abs/2506.16010", "authors": ["Xiangyang He", "Jiale Li", "Jiahao Chen", "Yang Yang", "Mingming Fan"], "title": "SimuPanel: A Novel Immersive Multi-Agent System to Simulate Interactive Expert Panel Discussion", "comment": null, "summary": "Panel discussion allows the audience to learn different perspectives through\ninteractive discussions among experts moderated by a host and a Q&A session\nwith the audience. Despite its benefits, panel discussion in the real world is\ninaccessible to many who do not have the privilege to participate due to\ngeographical, financial, and time constraints. We present SimuPanel, which\nsimulates panel discussions among academic experts through LLM-based\nmulti-agent interaction. It enables users to define topics of interest for the\npanel, observe the expert discussion, engage in Q&A, and take notes. SimuPanel\nemploys a host-expert architecture where each panel member is simulated by an\nagent with specialized expertise, and the panel is visualized in an immersive\n3D environment to enhance engagement. Traditional dialogue generation struggles\nto capture the depth and interactivity of real-world panel discussions. To\naddress this limitation, we propose a novel multi-agent interaction framework\nthat simulates authentic panel dynamics by modeling reasoning strategies and\npersonas of experts grounded in multimedia sources. This framework enables\nagents to dynamically recall and contribute to the discussion based on past\nexperiences from diverse perspectives. Our technical evaluation and the user\nstudy with university students show that SimuPanel was able to simulate more\nin-depth discussions and engage participants to interact with and reflect on\nthe discussions. As a first step in this direction, we offer design\nimplications for future avenues to improve and harness the power of panel\ndiscussion for multimedia learning.", "AI": {"tldr": "SimuPanel是一个基于LLM的多智能体交互系统，模拟学术专家小组讨论，解决现实小组讨论的地理、时间和经济限制问题。", "motivation": "现实中的小组讨论因地理、财务和时间限制难以普及，SimuPanel旨在通过技术手段解决这一问题。", "method": "采用多智能体交互框架，模拟专家讨论，结合3D可视化环境增强互动性，并基于多媒体资源建模专家推理策略和角色。", "result": "技术评估和用户研究表明，SimuPanel能模拟深入讨论并提升参与者互动与反思。", "conclusion": "SimuPanel为多媒体学习中的小组讨论提供了改进方向，展示了未来潜力。"}}
{"id": "2506.15837", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15837", "abs": "https://arxiv.org/abs/2506.15837", "authors": ["Fatmah AlHindaassi", "Mohammed Talha Alam", "Fakhri Karray"], "title": "ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions", "comment": "Under-review at IEEE SMC 2025", "summary": "Adverse weather conditions, particularly fog, pose a significant challenge to\nautonomous vehicles, surveillance systems, and other safety-critical\napplications by severely degrading visual information. We introduce\nADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly\noptimizes image restoration and object detection under varying fog intensities.\nA lightweight Haze Density Estimation Network (HDEN) classifies each input as\nlight, medium, or heavy fog. Based on this score, the system dynamically routes\nthe image through one of three CORUN branches: Light, Medium, or Complex, each\ntailored to its haze regime. A novel adaptive loss balances physical-model\ncoherence and perceptual fidelity, ensuring both accurate defogging and\npreservation of fine details. On Cityscapes and the real-world RTTS benchmark,\nADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and\nincreases object detection mAP by up to 13 points, while cutting inference time\nby 20 percent. These results highlight the importance of intensity-specific\nprocessing and seamless integration with downstream vision tasks. Code\navailable at: https://github.com/talha-alam/ADAM-Dehaze.", "AI": {"tldr": "ADAM-Dehaze是一种自适应、密度感知的去雾框架，通过动态路由和自适应损失优化图像恢复和目标检测，显著提升性能。", "motivation": "雾天严重影响视觉信息，对自动驾驶等安全关键应用构成挑战。", "method": "使用HDEN网络分类雾密度，动态路由至不同CORUN分支，结合自适应损失平衡物理模型和感知保真度。", "result": "在Cityscapes和RTTS基准上，PSNR提升2.1 dB，FADE降低30%，目标检测mAP提高13点，推理时间减少20%。", "conclusion": "ADAM-Dehaze证明了雾密度特异性处理和与下游视觉任务无缝集成的重要性。"}}
{"id": "2506.15689", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15689", "abs": "https://arxiv.org/abs/2506.15689", "authors": ["Liulu He", "Shenli Zhen", "Karwei Sun", "Yijiang Liu", "Yufei Zhao", "Chongkang Tan", "Huanrui Yang", "Yuan Du", "Li Du"], "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models", "comment": null, "summary": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon.", "AI": {"tldr": "论文提出了BASE-Q方法，通过结合偏置校正和非对称缩放，有效减少旋转量化中的舍入和裁剪误差，并支持分块优化，避免内存密集型全模型反向传播。", "motivation": "当前旋转量化方法存在两个问题：(i) 旋转未能对齐通道均值，导致量化边界更宽和舍入误差增加；(ii) 旋转使激活分布更接近高斯分布，增加了裁剪误差的能量损失。", "method": "引入BASE-Q方法，结合偏置校正和非对称缩放，减少舍入和裁剪误差，并支持分块优化。", "result": "实验表明，BASE-Q将精度差距缩小了50.5%、42.9%和29.2%（相比QuaRot、SpinQuant和OSTQuant）。", "conclusion": "BASE-Q是一种简单有效的方法，显著提升了旋转量化的性能，并降低了内存需求。"}}
{"id": "2506.16454", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.16454", "abs": "https://arxiv.org/abs/2506.16454", "authors": ["Haotian Yao", "Vahid Hakimian", "Mostafa Farrokhabadi", "Hamidreza Zareipour"], "title": "Emission-Aware Operation of Electrical Energy Storage Systems", "comment": null, "summary": "Since the beginning of this century, there has been a growing body of\nresearch and developments supporting the participation of energy storage\nsystems (ESS) in the emission reduction mandates. However, regardless of these\nefforts and despite the need for an accelerated energy transition, we have yet\nto see a practical framework for operational carbon accounting and credit\ntrading for energy storage systems. In this context, this paper proposes an\nemission performance credits (EPCs) framework that allows ESS, down to the\nprosumer level, to participate in the carbon market. Thus, a mechanism is\nproposed, for the first time, to calculate the grid's real-time marginal\nemission intensity (MEI). The MEI is then used to optimize the cumulative\noperational emission of ESS through carbon-aware dispatch. Consequently, the\nframework tracks the operational emissions and converts them into EPCs, which\nare then sold to regulated entities under compliance programs. Simulation\nresults support the potential of ESS, regardless of their size, to participate\nin the broader carbon mitigation objectives.", "AI": {"tldr": "本文提出了一种排放绩效积分（EPCs）框架，使能源存储系统（ESS）能够参与碳市场，通过实时边际排放强度（MEI）优化排放，并生成可交易的EPCs。", "motivation": "尽管能源存储系统在减排任务中发挥重要作用，但缺乏实际可行的碳核算和信用交易框架。本文旨在填补这一空白。", "method": "提出了一种基于实时MEI的碳感知调度机制，用于优化ESS的累积运行排放，并将其转化为EPCs。", "result": "模拟结果表明，无论规模大小，ESS均能有效参与碳减排目标。", "conclusion": "该框架为ESS参与碳市场提供了可行方案，支持更广泛的减排目标。"}}
{"id": "2506.16627", "categories": ["cs.GR", "cs.CV", "cs.LG", "65D18, 68U05, 68T07, 53A07", "I.3.5; I.3.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.16627", "abs": "https://arxiv.org/abs/2506.16627", "authors": ["Haotian Yin", "Aleksander Plocharski", "Michal Jan Wlodarczyk", "Mikolaj Kida", "Przemyslaw Musialski"], "title": "FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models", "comment": "12 page, 10 figures, preprint", "summary": "Neural signed-distance fields (SDFs) have become a versatile backbone for\ngeometric learning, yet enforcing developable, CAD-style behavior still hinges\non Gaussian curvature penalties that require full Hessian evaluation and\nsecond-order automatic differentiation, both of which are costly in memory and\nruntime. We present a curvature proxy that regularizes only the mixed\nsecond-order term (Weingarten term), allowing the two principal curvatures to\nadapt freely to data while suppressing unwanted warp. Two complementary\ninstantiations realize this idea: (i) a finite-difference proxy that replaces\neach Hessian entry with four forward SDF evaluations and a single first-order\ngradient, and (ii) an autodiff proxy that computes the same mixed derivative\nvia one Hessian-vector product, sidestepping explicit full Hessian assembly and\nremaining faster in practice. Both variants converge to the exact mixed second\nderivative, thus preserving the intended geometric bias without incurring full\nsecond-order graphs. On the ABC benchmarks, the proxies match or exceed the\nreconstruction fidelity of Hessian-based baselines while reducing GPU memory\nuse and wall-clock time by a factor of two. Because the method is drop-in and\nframework-agnostic, it opens a practical path toward scalable, curvature-aware\nSDF learning for engineering-grade shape reconstruction.", "AI": {"tldr": "提出了一种新的曲率代理方法，通过仅正则化混合二阶项（Weingarten项）来优化神经SDF，避免了昂贵的完整Hessian计算，显著降低了内存和计算成本。", "motivation": "现有的神经SDF方法在实现可展曲面（CAD风格）时依赖于高斯曲率惩罚，需要完整的Hessian计算和二阶自动微分，导致高内存和运行时成本。", "method": "提出了两种曲率代理实现：(i) 有限差分代理，用四个前向SDF评估和一个一阶梯度替换Hessian条目；(ii) 自动微分代理，通过一个Hessian-向量积计算混合导数，避免显式Hessian组装。", "result": "在ABC基准测试中，代理方法在重建保真度上与基于Hessian的基线相当或更优，同时将GPU内存使用和运行时间减少了一半。", "conclusion": "该方法是一种即插即用且框架无关的解决方案，为可扩展的、曲率感知的SDF学习提供了实用路径，适用于工程级形状重建。"}}
{"id": "2506.16836", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16836", "abs": "https://arxiv.org/abs/2506.16836", "authors": ["Arpitha Srivathsa Malavalli", "Karthik Sama", "Janvi Chhabra", "Pooja Bassin", "Srinath Srinivasa"], "title": "Engineering Resilience: An Energy-Based Approach to Sustainable Behavioural Interventions", "comment": null, "summary": "Addressing complex societal challenges, such as improving public health,\nfostering honesty in workplaces, or encouraging eco-friendly behaviour requires\neffective nudges to influence human behaviour at scale. Intervention science\nseeks to design such nudges within complex societal systems. While\ninterventions primarily aim to shift the system toward a desired state, less\nattention is given to the sustainability of that state, which we define in\nterms of resilience: the system's ability to retain the desired state even\nunder perturbations. In this work, we offer a more holistic perspective to\nintervention design by incorporating a nature-inspired postulate i.e., lower\nenergy states tend to exhibit greater resilience, as a regularization mechanism\nwithin intervention optimization to ensure that the resulting state is also\nsustainable. Using a simple agent-based simulation where commuters are nudged\nto choose eco-friendly options (e.g., cycles) over individually attractive but\nless eco-friendly ones (e.g., cars), we demonstrate how embedding lower energy\npostulate into intervention design induces resilience. The system energy is\ndefined in terms of motivators that drive its agent's behaviour. By inherently\nensuring that agents are not pushed into actions that contradict their\nmotivators, the energy-based approach helps design effective interventions that\ncontribute to resilient behavioural states.", "AI": {"tldr": "论文提出了一种结合自然启发的低能量状态假设的干预设计方法，旨在提高行为干预的可持续性和韧性。", "motivation": "解决复杂社会挑战（如公共健康、职场诚信和环保行为）时，现有干预设计多关注短期效果，而忽略了状态的可持续性。", "method": "通过将低能量状态假设作为正则化机制嵌入干预优化中，设计了一种基于代理的模拟，以通勤者选择环保交通工具为例验证方法。", "result": "结果表明，低能量假设的嵌入能显著提高系统的韧性，确保行为状态在扰动下仍能保持。", "conclusion": "能量为基础的干预设计方法不仅有效，还能促进可持续的行为状态，为复杂社会系统干预提供了新视角。"}}
{"id": "2506.16538", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16538", "abs": "https://arxiv.org/abs/2506.16538", "authors": ["Yunkee Chae", "Kyogu Lee"], "title": "Towards Bitrate-Efficient and Noise-Robust Speech Coding with Variable Bitrate RVQ", "comment": "Accepted to Interspeech 2025", "summary": "Residual Vector Quantization (RVQ) has become a dominant approach in neural\nspeech and audio coding, providing high-fidelity compression. However, speech\ncoding presents additional challenges due to real-world noise, which degrades\ncompression efficiency. Standard codecs allocate bits uniformly, wasting\nbitrate on noise components that do not contribute to intelligibility. This\npaper introduces a Variable Bitrate RVQ (VRVQ) framework for noise-robust\nspeech coding, dynamically adjusting bitrate per frame to optimize\nrate-distortion trade-offs. Unlike constant bitrate (CBR) RVQ, our method\nprioritizes critical speech components while suppressing residual noise.\nAdditionally, we integrate a feature denoiser to further improve noise\nrobustness. Experimental results show that VRVQ improves rate-distortion\ntrade-offs over conventional methods, achieving better compression efficiency\nand perceptual quality in noisy conditions. Samples are available at our\nproject page: https://yoongi43.github.io/noise_robust_vrvq/.", "AI": {"tldr": "论文提出了一种可变比特率残差向量量化（VRVQ）框架，用于噪声鲁棒的语音编码，动态调整每帧比特率以优化率失真权衡。", "motivation": "传统语音编码在噪声环境下效率降低，均匀分配比特率会浪费资源在不影响可懂度的噪声成分上。", "method": "引入VRVQ框架，动态调整比特率，优先处理关键语音成分并抑制噪声，同时集成特征去噪器提升鲁棒性。", "result": "实验表明，VRVQ在噪声条件下优于传统方法，实现了更高的压缩效率和感知质量。", "conclusion": "VRVQ为噪声环境下的语音编码提供了更优的解决方案，显著提升了压缩效率和鲁棒性。"}}
{"id": "2506.15851", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15851", "abs": "https://arxiv.org/abs/2506.15851", "authors": ["Qiyuan Wu", "Mark Campbell"], "title": "Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles", "comment": "Accepted by ICRA 2025", "summary": "The uncertainty quantification of sensor measurements coupled with deep\nlearning networks is crucial for many robotics systems, especially for\nsafety-critical applications such as self-driving cars. This paper develops an\nuncertainty quantification approach in the context of visual localization for\nautonomous driving, where locations are selected based on images. Key to our\napproach is to learn the measurement uncertainty using light-weight sensor\nerror model, which maps both image feature and semantic information to\n2-dimensional error distribution. Our approach enables uncertainty estimation\nconditioned on the specific context of the matched image pair, implicitly\ncapturing other critical, unannotated factors (e.g., city vs highway, dynamic\nvs static scenes, winter vs summer) in a latent manner. We demonstrate the\naccuracy of our uncertainty prediction framework using the Ithaca365 dataset,\nwhich includes variations in lighting and weather (sunny, night, snowy). Both\nthe uncertainty quantification of the sensor+network is evaluated, along with\nBayesian localization filters using unique sensor gating method. Results show\nthat the measurement error does not follow a Gaussian distribution with poor\nweather and lighting conditions, and is better predicted by our Gaussian\nMixture model.", "AI": {"tldr": "论文提出了一种用于自动驾驶视觉定位的轻量级传感器误差模型，通过学习图像特征和语义信息预测二维误差分布，并验证了其在复杂环境下的不确定性量化效果。", "motivation": "在自动驾驶等安全关键应用中，传感器测量与深度学习网络的不确定性量化至关重要。", "method": "采用轻量级传感器误差模型，结合图像特征和语义信息预测误差分布，并利用高斯混合模型改进不确定性预测。", "result": "在Ithaca365数据集上验证，结果显示在恶劣天气和光照条件下，误差分布更符合高斯混合模型而非高斯分布。", "conclusion": "该方法能有效捕捉未标注的上下文因素，为自动驾驶定位提供更准确的不确定性量化。"}}
{"id": "2506.16044", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16044", "abs": "https://arxiv.org/abs/2506.16044", "authors": ["MH Farhadi", "Ali Rabiee", "Sima Ghafoori", "Anna Cetera", "Wei Xu", "Reza Abiri"], "title": "Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications", "comment": null, "summary": "With recent advancements in AI and computational tools, intelligent paradigms\nhave emerged to enhance fields like shared autonomy and human-machine teaming\nin healthcare. Advanced AI algorithms (e.g., reinforcement learning) can\nautonomously make decisions to achieve planning and motion goals. However, in\nhealthcare, where human intent is crucial, fully independent machine decisions\nmay not be ideal. This chapter presents a comprehensive review of\nhuman-centered shared autonomy AI frameworks, focusing on upper limb\nbiosignal-based machine interfaces and associated motor control systems,\nincluding computer cursors, robotic arms, and planar platforms. We examine\nmotor planning, learning (rehabilitation), and control, covering conceptual\nfoundations of human-machine teaming in reach-and-grasp tasks and analyzing\nboth theoretical and practical implementations. Each section explores how human\nand machine inputs can be blended for shared autonomy in healthcare\napplications. Topics include human factors, biosignal processing for intent\ndetection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,\nassistive robotics, and Large Language Models (LLMs) as the next frontier. We\npropose adaptive shared autonomy AI as a high-performance paradigm for\ncollaborative human-AI systems, identify key implementation challenges, and\noutline future directions, particularly regarding AI reasoning agents. This\nanalysis aims to bridge neuroscientific insights with robotics to create more\nintuitive, effective, and ethical human-machine teaming frameworks.", "AI": {"tldr": "综述了以人为中心的共享自主AI框架，重点关注上肢生物信号接口及其在医疗中的应用，提出自适应共享自主AI作为高效协作范式。", "motivation": "在医疗领域，完全独立的机器决策可能不理想，需结合人类意图，因此探索人机协作的共享自主AI框架。", "method": "通过综述上肢生物信号接口（如脑机接口）及相关运动控制系统，分析人机协作的理论与实践。", "result": "提出自适应共享自主AI作为高性能协作范式，并识别关键挑战与未来方向。", "conclusion": "旨在结合神经科学与机器人技术，构建更直观、高效且伦理的人机协作框架。"}}
{"id": "2506.15838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15838", "abs": "https://arxiv.org/abs/2506.15838", "authors": ["Jiahao Wang", "Hualian Sheng", "Sijia Cai", "Weizhan Zhang", "Caixia Yan", "Yachuang Feng", "Bing Deng", "Jieping Ye"], "title": "EchoShot: Multi-Shot Portrait Video Generation", "comment": null, "summary": "Video diffusion models substantially boost the productivity of artistic\nworkflows with high-quality portrait video generative capacity. However,\nprevailing pipelines are primarily constrained to single-shot creation, while\nreal-world applications urge for multiple shots with identity consistency and\nflexible content controllability. In this work, we propose EchoShot, a native\nand scalable multi-shot framework for portrait customization built upon a\nfoundation video diffusion model. To start with, we propose shot-aware position\nembedding mechanisms within video diffusion transformer architecture to model\ninter-shot variations and establish intricate correspondence between multi-shot\nvisual content and their textual descriptions. This simple yet effective design\nenables direct training on multi-shot video data without introducing additional\ncomputational overhead. To facilitate model training within multi-shot\nscenario, we construct PortraitGala, a large-scale and high-fidelity\nhuman-centric video dataset featuring cross-shot identity consistency and\nfine-grained captions such as facial attributes, outfits, and dynamic motions.\nTo further enhance applicability, we extend EchoShot to perform reference\nimage-based personalized multi-shot generation and long video synthesis with\ninfinite shot counts. Extensive evaluations demonstrate that EchoShot achieves\nsuperior identity consistency as well as attribute-level controllability in\nmulti-shot portrait video generation. Notably, the proposed framework\ndemonstrates potential as a foundational paradigm for general multi-shot video\nmodeling.", "AI": {"tldr": "EchoShot是一个基于视频扩散模型的多镜头肖像定制框架，通过改进位置嵌入机制和构建高质量数据集，实现了身份一致性和内容可控性。", "motivation": "现实应用需要多镜头生成且保持身份一致性和内容可控性，而现有方法局限于单镜头生成。", "method": "提出shot-aware位置嵌入机制，构建PortraitGala数据集，支持基于参考图像的个性化生成和长视频合成。", "result": "EchoShot在多镜头肖像视频生成中表现出优越的身份一致性和属性级可控性。", "conclusion": "EchoShot可作为通用多镜头视频建模的基础范式。"}}
{"id": "2506.15690", "categories": ["cs.LG", "cs.AI", "cs.SI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2506.15690", "abs": "https://arxiv.org/abs/2506.15690", "authors": ["Tianyu Wang", "Lingyou Pang", "Akira Horiguchi", "Carey E. Priebe"], "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs", "comment": null, "summary": "The increasing use of synthetic data from the public Internet has enhanced\ndata usage efficiency in large language model (LLM) training. However, the\npotential threat of model collapse remains insufficiently explored. Existing\nstudies primarily examine model collapse in a single model setting or rely\nsolely on statistical surrogates. In this work, we introduce LLM Web Dynamics\n(LWD), an efficient framework for investigating model collapse at the network\nlevel. By simulating the Internet with a retrieval-augmented generation (RAG)\ndatabase, we analyze the convergence pattern of model outputs. Furthermore, we\nprovide theoretical guarantees for this convergence by drawing an analogy to\ninteracting Gaussian Mixture Models.", "AI": {"tldr": "论文提出LLM Web Dynamics（LWD）框架，研究网络级模型崩溃问题，通过模拟互联网和理论分析验证收敛模式。", "motivation": "公共互联网合成数据的使用增加，但模型崩溃的潜在威胁未被充分探索。", "method": "引入LWD框架，利用RAG数据库模拟互联网，分析模型输出的收敛模式，并通过高斯混合模型理论验证。", "result": "揭示了模型崩溃的网络级特征，并提供了理论收敛保证。", "conclusion": "LWD框架为研究模型崩溃提供了有效工具，填补了现有研究的空白。"}}
{"id": "2506.16620", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.16620", "abs": "https://arxiv.org/abs/2506.16620", "authors": ["Luca Spagnuolo", "Gabriel Giribaldi", "Filippo Perli", "Alberto Corigliano", "Luca Colombo", "Matteo Rinaldi"], "title": "Power Handling Improvement in Cross-Sectional Lame Mode Resonators Operating in the Ku-band", "comment": "5 pages, 3 figures, IFCS2025 Queretaro", "summary": "This study presents power handling improvements in cross-sectional Lame-Mode\nResonators (CLMRs) designed for operation in the Ku-band. Previously fabricated\nCLMR devices failed at approximately 8 dBm of input power, primarily due to\nelectromigration in the aluminum interdigitated electrodes (IDTs). To better\nunderstand this mechanism in CLMRs, a data driven thermal model is developed to\nanalyze localized heating effects within the resonator body, which are known to\naccelerate electromigration. Based on insights from this model, Aluminum\nSilicon Copper (AlSiCu) was selected for the IDTs due to its superior thermal\nstability and resistance to electromigration. Devices fabricated with AlSiCu\nexhibited no signs of performance degradation, with the best-performing\nresonator achieving a mechanical quality factor (Qm) of 360, a maximum Bode\nquality factor (QBode) of 500, and an electromechanical coupling coefficient\n(kt2) of 6.3%. Moreover, the use of AlSiCu significantly increased the maximum\ninput power the device can withstand, showing an improvement of up to 6 dBm\nover previous devices. These improvements in power handling make the devices\nstrong candidates for high-power Ku-band filtering applications.", "AI": {"tldr": "研究通过改进CLMRs的电极材料（AlSiCu）提升功率处理能力，显著提高设备性能。", "motivation": "解决CLMRs在Ku波段因铝电极电迁移导致的功率处理能力不足问题。", "method": "开发数据驱动的热模型分析局部加热效应，并选用AlSiCu作为电极材料。", "result": "AlSiCu电极的CLMRs无性能退化，Qm达360，QBode达500，kt2为6.3%，功率处理能力提升6 dBm。", "conclusion": "AlSiCu电极显著提升CLMRs的功率处理能力，适用于高功率Ku波段滤波应用。"}}
{"id": "2506.16827", "categories": ["cs.GR", "cs.CV", "cs.LG", "I.2.6; I.4.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2506.16827", "abs": "https://arxiv.org/abs/2506.16827", "authors": ["Grzegorz Gruszczynski", "Michal Jan Wlodarczyk", "Jakub J Meixner", "Przemyslaw Musialski"], "title": "Beyond Blur: A Fluid Perspective on Generative Diffusion Models", "comment": "11 pages, 8 figures, pre-print, supplementary pseudocode in appendix", "summary": "We propose a novel PDE-driven corruption process for generative image\nsynthesis based on advection-diffusion processes which generalizes existing\nPDE-based approaches. Our forward pass formulates image corruption via a\nphysically motivated PDE that couples directional advection with isotropic\ndiffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,\nFourier). We implement this PDE numerically through a GPU-accelerated custom\nLattice Boltzmann solver for fast evaluation. To induce realistic turbulence,\nwe generate stochastic velocity fields that introduce coherent motion and\ncapture multi-scale mixing. In the generative process, a neural network learns\nto reverse the advection-diffusion operator thus constituting a novel\ngenerative model. We discuss how previous methods emerge as specific cases of\nour operator, demonstrating that our framework generalizes prior PDE-based\ncorruption techniques. We illustrate how advection improves the diversity and\nquality of the generated images while keeping the overall color palette\nunaffected. This work bridges fluid dynamics, dimensionless PDE theory, and\ndeep generative modeling, offering a fresh perspective on physically informed\nimage corruption processes for diffusion-based synthesis.", "AI": {"tldr": "本文提出了一种基于PDE的图像生成方法，通过物理驱动的腐蚀过程结合对流-扩散和噪声，实现了高质量的图像生成。", "motivation": "旨在通过物理启发的PDE方法改进现有图像生成技术，结合流体动力学和深度学习，提升生成图像的多样性和质量。", "method": "采用GPU加速的Lattice Boltzmann求解器实现PDE数值模拟，结合随机速度场生成湍流，并通过神经网络学习反转腐蚀过程。", "result": "该方法能够生成多样且高质量的图像，同时保持色彩一致性，并推广了现有的PDE腐蚀技术。", "conclusion": "本文为基于扩散的图像生成提供了新的物理启发视角，结合了流体动力学和深度学习，具有广泛的应用潜力。"}}
{"id": "2506.16729", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16729", "abs": "https://arxiv.org/abs/2506.16729", "authors": ["Shoichi Koyama", "Kenji Ishizuka"], "title": "Learning Magnitude Distribution of Sound Fields via Conditioned Autoencoder", "comment": "To appear in Forum Acusticum 2025", "summary": "A learning-based method for estimating the magnitude distribution of sound\nfields from spatially sparse measurements is proposed. Estimating the magnitude\ndistribution of acoustic transfer function (ATF) is useful when phase\nmeasurements are unreliable or inaccessible and has a wide range of\napplications related to spatial audio. We propose a neural-network-based method\nfor the ATF magnitude estimation. The key feature of our network architecture\nis the input and output layers conditioned on source and receiver positions and\nfrequency and the aggregation module of latent variables, which can be\ninterpreted as an autoencoder-based extension of the basis expansion of the\nsound field. Numerical simulation results indicated that the ATF magnitude is\naccurately estimated with a small number of receivers by our proposed method.", "AI": {"tldr": "提出了一种基于学习的方法，通过稀疏空间测量估计声场的幅度分布。", "motivation": "在相位测量不可靠或不可用时，估计声学传递函数（ATF）的幅度分布对空间音频应用具有重要意义。", "method": "采用神经网络方法，网络架构的关键特点是输入输出层基于声源和接收器位置及频率，并包含潜在变量的聚合模块，可视为基于自编码器的声场基扩展。", "result": "数值模拟结果表明，该方法能以少量接收器准确估计ATF幅度。", "conclusion": "该方法为稀疏测量下的声场幅度估计提供了一种有效解决方案。"}}
{"id": "2506.15865", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15865", "abs": "https://arxiv.org/abs/2506.15865", "authors": ["Viral Rasik Galaiya"], "title": "Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples", "comment": "Thesis", "summary": "To use robots in more unstructured environments, we have to accommodate for\nmore complexities. Robotic systems need more awareness of the environment to\nadapt to uncertainty and variability. Although cameras have been predominantly\nused in robotic tasks, the limitations that come with them, such as occlusion,\nvisibility and breadth of information, have diverted some focus to tactile\nsensing. In this thesis, we explore the use of tactile sensing to determine the\npose of the object using the temporal features. We then use reinforcement\nlearning with tactile collisions to reduce the number of attempts required to\ngrasp an object resulting from positional uncertainty from camera estimates.\nFinally, we use information provided by these tactile sensors to a\nreinforcement learning agent to determine the trajectory to take to remove an\nobject from a restricted passage while reducing training time by pertaining\nfrom human examples.", "AI": {"tldr": "论文探讨了在非结构化环境中使用触觉传感和强化学习改进机器人抓取和移动物体的方法。", "motivation": "在非结构化环境中，机器人需要更高的环境感知能力以应对不确定性，而传统摄像头存在遮挡和视野限制，因此转向触觉传感。", "method": "利用触觉传感确定物体姿态，结合强化学习减少抓取尝试次数，并通过人类示例缩短训练时间。", "result": "触觉传感和强化学习的结合提高了机器人对物体姿态的估计精度，并减少了训练时间。", "conclusion": "触觉传感与强化学习的结合为机器人在复杂环境中的操作提供了有效解决方案。"}}
{"id": "2506.16107", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16107", "abs": "https://arxiv.org/abs/2506.16107", "authors": ["Mariann Kornelia Smith", "Jacqueline Meijer-Irons", "Andrew Millar"], "title": "From 600 Tools to 1 Console: A UX-Driven Transformation", "comment": null, "summary": "In 2021 the Technical Infrastructure (TI) User Experience (UX) team sent a\nsurvey to 10,000 Google Developers (Googlers) and uncovered that Google's\ninternal infrastructure tools were fragmented and inefficient, hindering\ndevelopers' productivity. Using user centered research and design methodologies\nthe team first created a story map and service blueprint to visualize the\nrelationship between internal applications, then formulated a strategic vision\nto consolidate tools, streamline workflows, and measure the impact of their\nwork. We secured executive buy-in and delivered incremental improvements.", "AI": {"tldr": "Google内部工具碎片化影响开发效率，团队通过用户研究和设计方法整合工具并优化流程。", "motivation": "发现Google内部基础设施工具分散且低效，阻碍开发者生产力。", "method": "使用用户研究和设计方法，创建故事地图和服务蓝图，制定战略愿景。", "result": "获得高管支持并逐步改进工具和流程。", "conclusion": "通过用户中心方法成功优化内部工具，提升开发效率。"}}
{"id": "2506.15852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15852", "abs": "https://arxiv.org/abs/2506.15852", "authors": ["Dominic Akt", "Marco Peer", "Florian Kleber"], "title": "Assessing the impact of Binarization for Writer Identification in Greek Papyrus", "comment": "Accepted for publication for AIROV 2025", "summary": "This paper tackles the task of writer identification for Greek papyri. A\ncommon preprocessing step in writer identification pipelines is image\nbinarization, which prevents the model from learning background features. This\nis challenging in historical documents, in our case Greek papyri, as background\nis often non-uniform, fragmented, and discolored with visible fiber structures.\nWe compare traditional binarization methods to state-of-the-art Deep Learning\n(DL) models, evaluating the impact of binarization quality on subsequent writer\nidentification performance. DL models are trained with and without a custom\ndata augmentation technique, as well as different model selection criteria are\napplied. The performance of these binarization methods, is then systematically\nevaluated on the DIBCO 2019 dataset. The impact of binarization on writer\nidentification is subsequently evaluated using a state-of-the-art approach for\nwriter identification. The results of this analysis highlight the influence of\ndata augmentation for DL methods. Furthermore, findings indicate a strong\ncorrelation between binarization effectiveness on papyri documents of DIBCO\n2019 and downstream writer identification performance.", "AI": {"tldr": "本文研究了希腊纸草文献的作者识别任务，重点探讨了图像二值化预处理对识别性能的影响，比较了传统方法与深度学习方法，并评估了数据增强的作用。", "motivation": "希腊纸草文献的背景不均匀、碎片化且变色，传统二值化方法难以处理，需探索深度学习方法及其优化策略。", "method": "比较传统二值化方法与DL模型，引入自定义数据增强技术，并在DIBCO 2019数据集上评估性能。", "result": "数据增强显著提升DL方法性能，二值化质量与作者识别效果强相关。", "conclusion": "DL方法结合数据增强在希腊纸草文献的二值化及作者识别中表现优异，为历史文献分析提供了有效工具。"}}
{"id": "2506.15691", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15691", "abs": "https://arxiv.org/abs/2506.15691", "authors": ["Chuheng Zhang", "Tim Pearce", "Pushi Zhang", "Kaixin Wang", "Xiaoyu Chen", "Wei Shen", "Li Zhao", "Jiang Bian"], "title": "What Do Latent Action Models Actually Learn?", "comment": null, "summary": "Latent action models (LAMs) aim to learn action-relevant changes from\nunlabeled videos by compressing changes between frames as latents. However,\ndifferences between video frames can be caused by controllable changes as well\nas exogenous noise, leading to an important concern -- do latents capture the\nchanges caused by actions or irrelevant noise? This paper studies this issue\nanalytically, presenting a linear model that encapsulates the essence of LAM\nlearning, while being tractable.This provides several insights, including\nconnections between LAM and principal component analysis (PCA), desiderata of\nthe data-generating policy, and justification of strategies to encourage\nlearning controllable changes using data augmentation, data cleaning, and\nauxiliary action-prediction. We also provide illustrative results based on\nnumerical simulation, shedding light on the specific structure of observations,\nactions, and noise in data that influence LAM learning.", "AI": {"tldr": "本文通过线性模型分析潜在动作模型（LAMs）学习中的核心问题，探讨其与PCA的联系，并提出数据增强等策略以优化学习可控变化。", "motivation": "研究LAMs是否捕捉到由动作引起的变化还是无关噪声，以提升模型对可控变化的识别能力。", "method": "提出一个可分析的线性模型，结合数据增强、数据清理和辅助动作预测策略。", "result": "揭示了观测数据、动作和噪声结构对LAM学习的影响，并通过数值模拟验证。", "conclusion": "线性模型为LAM学习提供了理论支持，数据增强等策略有助于优化学习可控变化。"}}
{"id": "2506.16797", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.16797", "abs": "https://arxiv.org/abs/2506.16797", "authors": ["Chenjun Liu", "Jason J. R. Liu", "Zhan Shu", "James Lam"], "title": "Distributed Affine Formation Control of Linear Multi-agent Systems with Adaptive Event-triggering", "comment": null, "summary": "Concerning general multi-agent systems with limited communication, this paper\nproposes distributed formation control protocols under adaptive event-triggered\nschemes to operate affine transformations of nominal formations. To accommodate\nmore practical system mechanics, we develop an event-triggered controller that\ndrives the leader to a desired state by bringing in the compensation term.\nBased on triggering instants' state information, an affine formation control\nmethod with adaptive event-triggering is designed for each follower, making the\nwhole protocol effective in refraining from successive communication while not\nrelying on predefined global information. In particular, mitigating the effect\nof partial state availability, an output-based control solution is presented to\nexpand the protocol's serviceable range. Finally, we perform numerical\nsimulations on the formation and its affine transformations to verify the\neffectiveness of the control protocol and the feasibility of the\nevent-triggered mechanism.", "AI": {"tldr": "本文提出了一种基于自适应事件触发的分布式编队控制协议，用于多智能体系统在有限通信下的仿射变换编队控制。", "motivation": "针对多智能体系统在有限通信条件下的编队控制问题，提出了一种自适应事件触发机制，以减少连续通信需求并避免依赖全局信息。", "method": "设计了基于事件触发时刻状态信息的自适应事件触发控制器，引入补偿项驱动领导者至期望状态，并为跟随者设计了仿射编队控制方法。此外，提出了基于输出的控制方案以应对部分状态不可用的情况。", "result": "数值仿真验证了控制协议的有效性和事件触发机制的可行性。", "conclusion": "所提出的协议在减少通信需求的同时，能够有效实现仿射编队控制，扩展了协议的应用范围。"}}
{"id": "2506.17206", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17206", "abs": "https://arxiv.org/abs/2506.17206", "authors": ["Yukun Huang", "Yanning Zhou", "Jianan Wang", "Kaiyi Huang", "Xihui Liu"], "title": "DreamCube: 3D Panorama Generation via Multi-plane Synchronization", "comment": "Project page: https://yukun-huang.github.io/DreamCube/", "summary": "3D panorama synthesis is a promising yet challenging task that demands\nhigh-quality and diverse visual appearance and geometry of the generated\nomnidirectional content. Existing methods leverage rich image priors from\npre-trained 2D foundation models to circumvent the scarcity of 3D panoramic\ndata, but the incompatibility between 3D panoramas and 2D single views limits\ntheir effectiveness. In this work, we demonstrate that by applying multi-plane\nsynchronization to the operators from 2D foundation models, their capabilities\ncan be seamlessly extended to the omnidirectional domain. Based on this design,\nwe further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D\npanorama generation, which maximizes the reuse of 2D foundation model priors to\nachieve diverse appearances and accurate geometry while maintaining multi-view\nconsistency. Extensive experiments demonstrate the effectiveness of our\napproach in panoramic image generation, panoramic depth estimation, and 3D\nscene generation.", "AI": {"tldr": "论文提出了一种通过多平面同步技术将2D基础模型能力扩展到全景领域的方法，并介绍了DreamCube模型，用于生成高质量3D全景内容。", "motivation": "解决现有方法因3D全景数据稀缺而依赖2D基础模型时的不兼容性问题，提升全景内容的多样性和几何准确性。", "method": "采用多平面同步技术扩展2D基础模型能力，并设计DreamCube模型，利用RGB-D扩散模型生成3D全景内容。", "result": "实验证明该方法在全景图像生成、深度估计和3D场景生成中表现优异。", "conclusion": "通过多平面同步和DreamCube模型，成功实现了高质量3D全景内容的生成，同时保持了多视图一致性。"}}
{"id": "2506.16833", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16833", "abs": "https://arxiv.org/abs/2506.16833", "authors": ["Jianyuan Feng", "Guangzheng Li", "Yangfei Xu"], "title": "Hybrid-Sep: Language-queried audio source separation via pre-trained Model Fusion and Adversarial Diffusion Training", "comment": "Submitted to WASAA 2025", "summary": "Language-queried Audio Separation (LASS) employs linguistic queries to\nisolate target sounds based on semantic descriptions. However, existing methods\nface challenges in aligning complex auditory features with linguistic context\nwhile preserving separation precision. Current research efforts focus primarily\non text description augmentation and architectural innovations, yet the\npotential of integrating pre-trained self-supervised learning (SSL) audio\nmodels and Contrastive Language-Audio Pretraining (CLAP) frameworks, capable of\nextracting cross-modal audio-text relationships, remains underexplored. To\naddress this, we present HybridSep, a two-stage LASS framework that synergizes\nSSL-based acoustic representations with CLAP-derived semantic embeddings. Our\nframework introduces Adversarial Consistent Training (ACT), a novel\noptimization strategy that treats diffusion as an auxiliary regularization loss\nwhile integrating adversarial training to enhance separation fidelity.\nExperiments demonstrate that HybridSep achieves significant performance\nimprovements over state-of-the-art baselines (e.g., AudioSep, FlowSep) across\nmultiple metrics, establishing new benchmarks for LASS tasks.", "AI": {"tldr": "HybridSep结合了自监督学习和对比语言-音频预训练，通过两阶段框架和对抗一致性训练提升语言查询音频分离性能。", "motivation": "现有方法在复杂听觉特征与语言上下文对齐及分离精度上存在不足，自监督学习和CLAP框架的潜力尚未充分挖掘。", "method": "提出HybridSep框架，结合SSL音频表征和CLAP语义嵌入，引入对抗一致性训练（ACT）优化策略。", "result": "实验显示HybridSep在多项指标上显著优于现有基准（如AudioSep、FlowSep）。", "conclusion": "HybridSep为语言查询音频分离任务设定了新基准。"}}
{"id": "2506.15868", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15868", "abs": "https://arxiv.org/abs/2506.15868", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jia Hu", "Jiaqi Ma"], "title": "CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction", "comment": "IROS2025", "summary": "Risk quantification is a critical component of safe autonomous driving,\nhowever, constrained by the limited perception range and occlusion of\nsingle-vehicle systems in complex and dense scenarios. Vehicle-to-everything\n(V2X) paradigm has been a promising solution to sharing complementary\nperception information, nevertheless, how to ensure the risk interpretability\nwhile understanding multi-agent interaction with V2X remains an open question.\nIn this paper, we introduce the first V2X-enabled risk quantification pipeline,\nCooperRisk, to fuse perception information from multiple agents and quantify\nthe scenario driving risk in future multiple timestamps. The risk is\nrepresented as a scenario risk map to ensure interpretability based on risk\nseverity and exposure, and the multi-agent interaction is captured by the\nlearning-based cooperative prediction model. We carefully design a\nrisk-oriented transformer-based prediction model with multi-modality and\nmulti-agent considerations. It aims to ensure scene-consistent future behaviors\nof multiple agents and avoid conflicting predictions that could lead to overly\nconservative risk quantification and cause the ego vehicle to become overly\nhesitant to drive. Then, the temporal risk maps could serve to guide a model\npredictive control planner. We evaluate the CooperRisk pipeline in a real-world\nV2X dataset V2XPnP, and the experiments demonstrate its superior performance in\nrisk quantification, showing a 44.35% decrease in conflict rate between the ego\nvehicle and background traffic participants.", "AI": {"tldr": "论文提出了一种基于V2X的风险量化框架CooperRisk，通过多智能体感知信息融合和未来时间戳的风险量化，生成可解释的场景风险地图，并利用基于学习的协同预测模型捕捉多智能体交互。", "motivation": "单车辆系统在复杂密集场景中的感知范围有限且易受遮挡，V2X技术虽能共享感知信息，但如何确保风险可解释性并理解多智能体交互仍是一个开放问题。", "method": "设计了基于Transformer的风险导向预测模型，考虑多模态和多智能体因素，确保场景一致的行为预测，避免冲突预测导致过度保守的风险量化。", "result": "在V2XPnP数据集上评估，冲突率降低了44.35%，验证了其在风险量化上的优越性能。", "conclusion": "CooperRisk框架通过多智能体协同感知和风险量化，显著提升了自动驾驶的安全性和可解释性。"}}
{"id": "2506.16168", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16168", "abs": "https://arxiv.org/abs/2506.16168", "authors": ["Thomas Barbera", "Jacopo Burger", "Alessandro D'Amelio", "Simone Zini", "Simone Bianco", "Raffaella Lanzarotti", "Paolo Napoletano", "Giuseppe Boccignone", "Jose Luis Contreras-Vidal"], "title": "On using AI for EEG-based BCI applications: problems, current challenges and future trends", "comment": null, "summary": "Imagine unlocking the power of the mind to communicate, create, and even\ninteract with the world around us. Recent breakthroughs in Artificial\nIntelligence (AI), especially in how machines \"see\" and \"understand\" language,\nare now fueling exciting progress in decoding brain signals from scalp\nelectroencephalography (EEG). Prima facie, this opens the door to revolutionary\nbrain-computer interfaces (BCIs) designed for real life, moving beyond\ntraditional uses to envision Brain-to-Speech, Brain-to-Image, and even a\nBrain-to-Internet of Things (BCIoT).\n  However, the journey is not as straightforward as it was for Computer Vision\n(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based\nBCIs, particularly in building powerful foundational models, presents unique\nand intricate hurdles that could affect their reliability.\n  Here, we unfold a guided exploration of this dynamic and rapidly evolving\nresearch area. Rather than barely outlining a map of current endeavors and\nresults, the goal is to provide a principled navigation of this hot and\ncutting-edge research landscape. We consider the basic paradigms that emerge\nfrom a causal perspective and the attendant challenges presented to AI-based\nmodels. Looking ahead, we then discuss promising research avenues that could\novercome today's technological, methodological, and ethical limitations. Our\naim is to lay out a clear roadmap for creating truly practical and effective\nEEG-based BCI solutions that can thrive in everyday environments.", "AI": {"tldr": "论文探讨了AI在解码脑电图（EEG）信号中的应用，展望了脑机接口（BCI）的未来潜力，同时分析了当前的技术挑战和研究方向。", "motivation": "通过AI技术解码EEG信号，推动脑机接口（如脑到语音、脑到图像等）的实际应用，解决传统方法的局限性。", "method": "从因果视角分析基本范式，探讨AI模型在EEG-BCI中的挑战，并提出未来研究方向。", "result": "揭示了当前技术的可靠性问题，并提出了克服技术、方法和伦理限制的可能途径。", "conclusion": "论文旨在为开发实用且高效的EEG-BCI解决方案提供清晰的路线图，推动其在日常环境中的应用。"}}
{"id": "2506.15854", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15854", "abs": "https://arxiv.org/abs/2506.15854", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak", "Ahmad Patooghy"], "title": "Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation", "comment": null, "summary": "Connected and Autonomous Vehicles (CAVs) rely on a range of devices that\noften process privacy-sensitive data. Among these, roadside units play a\ncritical role particularly through the use of AI-equipped (AIE) cameras for\napplications such as violation detection. However, the privacy risks associated\nwith captured imagery remain a major concern, as such data can be misused for\nidentity theft, profiling, or unauthorized commercial purposes. While\ntraditional techniques such as face blurring and obfuscation have been applied\nto mitigate privacy risks, individual privacy remains at risk, as individuals\ncan still be tracked using other features such as their clothing. This paper\nintroduces a novel privacy-preserving framework that leverages feedback-based\nreinforcement learning (RL) and vision-language models (VLMs) to protect\nsensitive visual information captured by AIE cameras. The main idea is to\nconvert images into semantically equivalent textual descriptions, ensuring that\nscene-relevant information is retained while visual privacy is preserved. A\nhierarchical RL strategy is employed to iteratively refine the generated text,\nenhancing both semantic accuracy and privacy. Evaluation results demonstrate\nsignificant improvements in both privacy protection and textual quality, with\nthe Unique Word Count increasing by approximately 77\\% and Detail Density by\naround 50\\% compared to existing approaches.", "AI": {"tldr": "论文提出了一种基于反馈强化学习和视觉语言模型的新框架，用于保护AI摄像头捕获的敏感视觉信息，将图像转换为语义等效的文本描述，显著提升了隐私保护和文本质量。", "motivation": "CAV设备（如AI摄像头）处理隐私敏感数据时存在隐私泄露风险，传统方法（如模糊处理）无法完全保护隐私。", "method": "采用反馈强化学习和视觉语言模型，将图像转换为语义等效文本，并通过分层RL策略迭代优化文本生成。", "result": "相比现有方法，隐私保护和文本质量显著提升，独特词数增加约77%，细节密度提升约50%。", "conclusion": "该框架有效解决了AI摄像头隐私保护问题，同时保留了场景相关信息。"}}
{"id": "2506.15692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15692", "abs": "https://arxiv.org/abs/2506.15692", "authors": ["Jaehyun Nam", "Jinsung Yoon", "Jiefeng Chen", "Jinwoo Shin", "Sercan Ö. Arık", "Tomas Pfister"], "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement", "comment": null, "summary": "Agents based on large language models (LLMs) for machine learning engineering\n(MLE) can automatically implement ML models via code generation. However,\nexisting approaches to build such agents often rely heavily on inherent LLM\nknowledge and employ coarse exploration strategies that modify the entire code\nstructure at once. This limits their ability to select effective task-specific\nmodels and perform deep exploration within specific components, such as\nexperimenting extensively with feature engineering options. To overcome these,\nwe propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first\nleverages external knowledge by using a search engine to retrieve effective\nmodels from the web, forming an initial solution, then iteratively refines it\nby exploring various strategies targeting specific ML components. This\nexploration is guided by ablation studies analyzing the impact of individual\ncode blocks. Furthermore, we introduce a novel ensembling method using an\neffective strategy suggested by MLE-STAR. Our experimental results show that\nMLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,\nsignificantly outperforming the best alternative.", "AI": {"tldr": "MLE-STAR是一种基于LLM的MLE代理，通过结合外部知识和针对性探索策略，显著提升模型性能。", "motivation": "现有基于LLM的MLE代理依赖固有知识且探索策略粗糙，无法深入优化特定组件。", "method": "MLE-STAR利用搜索引擎获取初始模型，通过针对性探索和消融研究迭代优化，并引入新的集成方法。", "result": "MLE-STAR在44%的Kaggle竞赛中表现优异，显著优于其他方法。", "conclusion": "MLE-STAR通过结合外部知识和精细化探索，为MLE代理提供了更高效的解决方案。"}}
{"id": "2506.16870", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.16870", "abs": "https://arxiv.org/abs/2506.16870", "authors": ["Marcelo Jacinto", "Rita Cunha"], "title": "Vision-Based Multirotor Control for Spherical Target Tracking: A Bearing-Angle Approach", "comment": "This paper has been accepted for presentation at the 2025 IEEE\n  European Control Conference (ECC)", "summary": "This work addresses the problem of designing a visual servo controller for a\nmultirotor vehicle, with the end goal of tracking a moving spherical target\nwith unknown radius. To address this problem, we first transform two bearing\nmeasurements provided by a camera sensor into a bearing-angle pair. We then use\nthis information to derive the system's dynamics in a new set of coordinates,\nwhere the angle measurement is used to quantify a relative distance to the\ntarget. Building on this system representation, we design an adaptive nonlinear\ncontrol algorithm that takes advantage of the properties of the new system\ngeometry and assumes that the target follows a constant acceleration model.\nSimulation results illustrate the performance of the proposed control\nalgorithm.", "AI": {"tldr": "设计了一种基于视觉伺服的控制器，用于多旋翼飞行器跟踪未知半径的移动球形目标。", "motivation": "解决多旋翼飞行器在未知半径条件下跟踪移动球形目标的控制问题。", "method": "将相机传感器的两个方位测量转换为方位-角度对，并利用新坐标系下的系统动态设计自适应非线性控制算法。", "result": "仿真结果表明所提出的控制算法性能良好。", "conclusion": "通过新坐标系和自适应控制算法，成功实现了对未知半径移动目标的跟踪。"}}
{"id": "2506.17032", "categories": ["cs.HC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.17032", "abs": "https://arxiv.org/abs/2506.17032", "authors": ["Abdulhaq Adetunji Salako", "Christian Tominski"], "title": "Toward Understanding Similarity of Visualization Techniques", "comment": null, "summary": "The literature describes many visualization techniques for different types of\ndata, tasks, and application contexts, and new techniques are proposed on a\nregular basis. Visualization surveys try to capture the immense space of\ntechniques and structure it with meaningful categorizations. Yet, it remains\ndifficult to understand the similarity of visualization techniques in general.\nWe approach this open research question from two angles. First, we follow a\nmodel-driven approach that is based on defining the signature of visualization\ntechniques and interpreting the similarity of signatures as the similarity of\ntheir associated techniques. Second, following an expert-driven approach, we\nasked visualization experts in a small online study for their ad-hoc intuitive\nassessment of the similarity of pairs visualization techniques. From both\napproaches, we gain insight into the similarity of a set of 13 basic and\nadvanced visualizations for different types of data. While our results are so\nfar preliminary and academic, they are first steps toward better understanding\nthe similarity of visualization techniques.", "AI": {"tldr": "论文探讨了可视化技术的相似性，通过模型驱动和专家驱动两种方法分析了13种基本和高级可视化技术的相似性。", "motivation": "现有研究难以全面理解可视化技术的相似性，因此作者希望通过两种方法填补这一空白。", "method": "采用模型驱动（定义技术签名）和专家驱动（在线专家调查）两种方法。", "result": "初步结果揭示了13种可视化技术的相似性，为未来研究奠定了基础。", "conclusion": "研究为理解可视化技术相似性提供了初步探索，未来需进一步验证和扩展。"}}
{"id": "2506.16336", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16336", "abs": "https://arxiv.org/abs/2506.16336", "authors": ["Yiou Huang"], "title": "Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections", "comment": null, "summary": "Reinforcement learning (RL) exhibits remarkable potential in addressing\nautonomous driving tasks. However, it is difficult to train a sample-efficient\nand safe policy in complex scenarios. In this article, we propose a novel\nhierarchical reinforcement learning (HRL) framework with a goal-conditioned\ncollision prediction (GCCP) module. In the hierarchical structure, the GCCP\nmodule predicts collision risks according to different potential subgoals of\nthe ego vehicle. A high-level decision-maker choose the best safe subgoal. A\nlow-level motion-planner interacts with the environment according to the\nsubgoal. Compared to traditional RL methods, our algorithm is more\nsample-efficient, since its hierarchical structure allows reusing the policies\nof subgoals across similar tasks for various navigation scenarios. In\nadditional, the GCCP module's ability to predict both the ego vehicle's and\nsurrounding vehicles' future actions according to different subgoals, ensures\nthe safety of the ego vehicle throughout the decision-making process.\nExperimental results demonstrate that the proposed method converges to an\noptimal policy faster and achieves higher safety than traditional RL methods.", "AI": {"tldr": "提出了一种分层强化学习框架（HRL）结合目标条件碰撞预测模块（GCCP），用于自动驾驶任务，提高了样本效率和安全性。", "motivation": "传统强化学习在复杂场景中难以高效训练安全策略，需改进。", "method": "采用HRL框架，高层决策选择安全子目标，低层执行；GCCP模块预测碰撞风险。", "result": "实验表明，该方法收敛更快，安全性优于传统RL方法。", "conclusion": "HRL结合GCCP显著提升了自动驾驶任务的样本效率和安全性。"}}
{"id": "2506.16889", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16889", "abs": "https://arxiv.org/abs/2506.16889", "authors": ["Junghyun Koo", "Marco A. Martinez-Ramirez", "Wei-Hsiang Liao", "Giorgio Fabbro", "Michele Mancusi", "Yuki Mitsufuji"], "title": "ITO-Master: Inference-Time Optimization for Audio Effects Modeling of Music Mastering Processors", "comment": "ISMIR 2025", "summary": "Music mastering style transfer aims to model and apply the mastering\ncharacteristics of a reference track to a target track, simulating the\nprofessional mastering process. However, existing methods apply fixed\nprocessing based on a reference track, limiting users' ability to fine-tune the\nresults to match their artistic intent. In this paper, we introduce the\nITO-Master framework, a reference-based mastering style transfer system that\nintegrates Inference-Time Optimization (ITO) to enable finer user control over\nthe mastering process. By optimizing the reference embedding during inference,\nour approach allows users to refine the output dynamically, making micro-level\nadjustments to achieve more precise mastering results. We explore both\nblack-box and white-box methods for modeling mastering processors and\ndemonstrate that ITO improves mastering performance across different styles.\nThrough objective evaluation, subjective listening tests, and qualitative\nanalysis using text-based conditioning with CLAP embeddings, we validate that\nITO enhances mastering style similarity while offering increased adaptability.\nOur framework provides an effective and user-controllable solution for\nmastering style transfer, allowing users to refine their results beyond the\ninitial style transfer.", "AI": {"tldr": "论文提出了ITO-Master框架，通过推理时间优化（ITO）实现用户对音乐母带风格迁移的精细控制，提升了结果的可调性和精准度。", "motivation": "现有方法基于固定处理限制了用户根据艺术意图微调结果的能力，因此需要一种更灵活可控的解决方案。", "method": "引入ITO-Master框架，结合推理时间优化（ITO）动态优化参考嵌入，支持用户微调；探索了黑盒和白盒方法建模母带处理器。", "result": "通过客观评估、主观听测和CLAP嵌入的定性分析，验证ITO提升了风格相似性和适应性。", "conclusion": "ITO-Master为用户提供了高效且可控的母带风格迁移解决方案，支持结果进一步优化。"}}
{"id": "2506.15870", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15870", "abs": "https://arxiv.org/abs/2506.15870", "authors": ["Hossein Maghsoumi", "Yaser Fallah"], "title": "A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices", "comment": null, "summary": "Small-scale autonomous vehicle platforms provide a cost-effective environment\nfor developing and testing advanced driving systems. However, specific\nconfigurations within this scale are underrepresented, limiting full awareness\nof their potential. This paper focuses on a one-sixth-scale setup, offering a\nhigh-level overview of its design, hardware and software integration, and\ntypical challenges encountered during development. We discuss methods for\naddressing mechanical and electronic issues common to this scale and propose\nguidelines for improving reliability and performance. By sharing these\ninsights, we aim to expand the utility of small-scale vehicles for testing\nautonomous driving algorithms and to encourage further research in this domain.", "AI": {"tldr": "本文探讨了六分之一比例的小型自动驾驶车辆平台的设计与开发，提出了解决常见机械和电子问题的方法，并分享了提高可靠性和性能的指南。", "motivation": "小型自动驾驶车辆平台成本效益高，但某些配置的研究不足，限制了对其潜力的全面认识。本文旨在填补这一空白。", "method": "通过设计、硬件和软件集成，以及解决开发中的典型挑战，提出改进方法。", "result": "提出了提高小型车辆平台可靠性和性能的指南。", "conclusion": "本文旨在扩展小型车辆平台在自动驾驶算法测试中的实用性，并鼓励进一步研究。"}}
{"id": "2506.16199", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16199", "abs": "https://arxiv.org/abs/2506.16199", "authors": ["Mohammad Naiseh", "Huseyin Dogan", "Stephen Giff", "Nan Jiang"], "title": "Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) plays a critical role in fostering\nuser trust and understanding in AI-driven systems. However, the design of\neffective XAI interfaces presents significant challenges, particularly for UX\nprofessionals who may lack technical expertise in AI or machine learning.\nExisting explanation methods, such as SHAP, LIME, and counterfactual\nexplanations, often rely on complex technical language and assumptions that are\ndifficult for non-expert users to interpret. To address these gaps, we propose\na UX Research (UXR) Playbook for XAI - a practical framework aimed at\nsupporting UX professionals in designing accessible, transparent, and\ntrustworthy AI experiences. Our playbook offers actionable guidance to help\nbridge the gap between technical explainability methods and user centred\ndesign, empowering designers to create AI interactions that foster better\nunderstanding, trust, and responsible AI adoption.", "AI": {"tldr": "提出了一种面向UX专业人士的XAI设计框架（UXR Playbook），旨在解决现有解释方法对非技术用户不友好的问题。", "motivation": "现有XAI解释方法（如SHAP、LIME）过于技术化，非专家用户难以理解，影响了AI系统的透明度和用户信任。", "method": "开发了一个UXR Playbook框架，提供实用指导，帮助UX专业人士设计更易理解、透明且可信的AI交互。", "result": "该框架为设计师提供了工具，以弥合技术解释方法与用户中心设计之间的差距。", "conclusion": "UXR Playbook有助于提升AI系统的可理解性和用户信任，推动负责任的AI采用。"}}
{"id": "2506.15871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15871", "abs": "https://arxiv.org/abs/2506.15871", "authors": ["Rim Assouel", "Declan Campbell", "Taylor Webb"], "title": "Visual symbolic mechanisms: Emergent symbol processing in vision language models", "comment": null, "summary": "To accurately process a visual scene, observers must bind features together\nto represent individual objects. This capacity is necessary, for instance, to\ndistinguish an image containing a red square and a blue circle from an image\ncontaining a blue square and a red circle. Recent work has found that language\nmodels solve this 'binding problem' via a set of symbol-like,\ncontent-independent indices, but it is unclear whether similar mechanisms are\nemployed by vision language models (VLMs). This question is especially\nrelevant, given the persistent failures of VLMs on tasks that require binding.\nHere, we identify a set of emergent symbolic mechanisms that support binding in\nVLMs via a content-independent, spatial indexing scheme. Moreover, we find that\nbinding errors can be traced directly to failures in these mechanisms. Taken\ntogether, these results shed light on the mechanisms that support symbol-like\nprocessing in VLMs, and suggest possible avenues for addressing the persistent\nbinding failures exhibited by these models.", "AI": {"tldr": "研究发现视觉语言模型（VLMs）通过内容无关的空间索引机制解决特征绑定问题，并揭示了绑定错误的根源。", "motivation": "探索视觉语言模型（VLMs）是否采用类似语言模型的符号化机制解决特征绑定问题，以解释其在此类任务中的持续失败。", "method": "识别VLMs中支持绑定的内容无关空间索引机制，并分析绑定错误的来源。", "result": "发现VLMs通过符号化机制实现特征绑定，绑定错误源于这些机制的失效。", "conclusion": "研究揭示了VLMs中符号化处理机制，为解决其绑定失败问题提供了潜在方向。"}}
{"id": "2506.15693", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15693", "abs": "https://arxiv.org/abs/2506.15693", "authors": ["Jiaxing Li", "Hanjiang Hu", "Yujie Yang", "Changliu Liu"], "title": "Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks", "comment": "6 pages, 3 figures", "summary": "Recent learning-based safety filters have outperformed conventional methods,\nsuch as hand-crafted Control Barrier Functions (CBFs), by effectively adapting\nto complex constraints. However, these learning-based approaches lack formal\nsafety guarantees. In this work, we introduce a verifiable model-free safety\nfilter based on Hamilton-Jacobi reachability analysis. Our primary\ncontributions include: 1) extending verifiable self-consistency properties for\nQ value functions, 2) proposing a multiplicative Q-network structure to\nmitigate zero-sublevel-set shrinkage issues, and 3) developing a verification\npipeline capable of soundly verifying these self-consistency properties. Our\nproposed approach successfully synthesizes formally verified, model-free safety\ncertificates across four standard safe-control benchmarks.", "AI": {"tldr": "论文提出了一种基于Hamilton-Jacobi可达性分析的可验证无模型安全过滤器，解决了学习型安全过滤器缺乏形式化安全保证的问题。", "motivation": "学习型安全过滤器虽优于传统方法（如手工设计的CBFs），但缺乏形式化安全保证，因此需要一种可验证的无模型方法。", "method": "1）扩展Q值函数的可验证自一致性属性；2）提出乘法Q网络结构以减少零子集收缩问题；3）开发验证流程以验证自一致性。", "result": "方法在四个标准安全控制基准测试中成功生成了形式化验证的无模型安全证书。", "conclusion": "该研究为学习型安全过滤器提供了形式化安全保证，解决了现有方法的局限性。"}}
{"id": "2506.16971", "categories": ["cs.SY", "cs.AI", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16971", "abs": "https://arxiv.org/abs/2506.16971", "authors": ["Oliver Schön", "Sofie Haesaert", "Sadegh Soudjani"], "title": "Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)", "comment": "26 pages, 5 figures, extended version of paper accepted for\n  publication at QEST 2025", "summary": "The requirement for identifying accurate system representations has not only\nbeen a challenge to fulfill, but it has compromised the scalability of formal\nmethods, as the resulting models are often too complex for effective decision\nmaking with formal correctness and performance guarantees. Focusing on\nprobabilistic simulation relations and surrogate models of stochastic systems,\nwe propose an approach that significantly enhances the scalability and\npractical applicability of such simulation relations by eliminating the need to\ncompute error bounds directly. As a result, we provide an abstraction-based\ntechnique that scales effectively to higher dimensions while addressing complex\nnonlinear agent-environment interactions with infinite-horizon temporal logic\nguarantees amidst uncertainty. Our approach trades scalability for conservatism\nfavorably, as demonstrated on a complex high-dimensional vehicle intersection\ncase study.", "AI": {"tldr": "提出了一种基于概率模拟关系和替代模型的方法，显著提升了随机系统模拟关系的可扩展性和实用性，无需直接计算误差界限。", "motivation": "现有系统表示方法复杂且难以扩展，影响了形式化方法的实用性和决策效率。", "method": "采用概率模拟关系和替代模型，避免直接计算误差界限，提供抽象化技术以处理高维复杂非线性交互。", "result": "方法在复杂高维车辆交叉案例中验证了其可扩展性和保守性之间的良好权衡。", "conclusion": "该方法为随机系统提供了高效且可扩展的形式化保证，适用于复杂非线性交互场景。"}}
{"id": "2506.16535", "categories": ["cs.RO", "cs.MA", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16535", "abs": "https://arxiv.org/abs/2506.16535", "authors": ["Tyler Landle", "Jordan Rapp", "Dean Blank", "Chandramouli Amarnath", "Abhijit Chatterjee", "Alex Daglis", "Umakishore Ramachandran"], "title": "eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles", "comment": null, "summary": "As autonomous vehicles edge closer to widespread adoption, enhancing road\nsafety through collision avoidance and minimization of collateral damage\nbecomes imperative. Vehicle-to-everything (V2X) technologies, which include\nvehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud\n(V2C), are being proposed as mechanisms to achieve this safety improvement.\n  Simulation-based testing is crucial for early-stage evaluation of Connected\nAutonomous Vehicle (CAV) control systems, offering a safer and more\ncost-effective alternative to real-world tests. However, simulating large 3D\nenvironments with many complex single- and multi-vehicle sensors and\ncontrollers is computationally intensive. There is currently no evaluation\nframework that can effectively evaluate realistic scenarios involving large\nnumbers of autonomous vehicles.\n  We propose eCAV -- an efficient, modular, and scalable evaluation platform to\nfacilitate both functional validation of algorithmic approaches to increasing\nroad safety, as well as performance prediction of algorithms of various V2X\ntechnologies, including a futuristic Vehicle-to-Edge control plane and\ncorrespondingly designed control algorithms. eCAV can model up to 256 vehicles\nrunning individual control algorithms without perception enabled, which is\n$8\\times$ more vehicles than what is possible with state-of-the-art\nalternatives. %faster than state-of-the-art alternatives that can simulate\n$8\\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64\nvehicles with a step time under 800ms, which is $4\\times$ more and $1.5\\times$\nfaster than the state-of-the-art OpenCDA framework.", "AI": {"tldr": "eCAV是一个高效、模块化和可扩展的评估平台，用于验证自动驾驶车辆（CAV）的安全算法和V2X技术性能，支持模拟更多车辆且速度更快。", "motivation": "随着自动驾驶车辆的普及，提高道路安全性和减少碰撞损害变得至关重要，但现有仿真框架无法有效评估大规模车辆场景。", "method": "提出eCAV平台，支持多达256辆无感知车辆的模拟，以及64辆有感知车辆的快速仿真，性能优于现有方案。", "result": "eCAV在无感知模式下支持8倍于现有方案的车辆数量，有感知模式下支持4倍车辆且速度快1.5倍。", "conclusion": "eCAV为自动驾驶车辆的安全算法验证和大规模场景仿真提供了高效解决方案。"}}
{"id": "2506.17055", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17055", "abs": "https://arxiv.org/abs/2506.17055", "authors": ["Charilaos Papaioannou", "Emmanouil Benetos", "Alexandros Potamianos"], "title": "Universal Music Representations? Evaluating Foundation Models on World Music Corpora", "comment": "Accepted at ISMIR 2025", "summary": "Foundation models have revolutionized music information retrieval, but\nquestions remain about their ability to generalize across diverse musical\ntraditions. This paper presents a comprehensive evaluation of five\nstate-of-the-art audio foundation models across six musical corpora spanning\nWestern popular, Greek, Turkish, and Indian classical traditions. We employ\nthree complementary methodologies to investigate these models' cross-cultural\ncapabilities: probing to assess inherent representations, targeted supervised\nfine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource\nscenarios. Our analysis shows varying cross-cultural generalization, with\nlarger models typically outperforming on non-Western music, though results\ndecline for culturally distant traditions. Notably, our approaches achieve\nstate-of-the-art performance on five out of six evaluated datasets,\ndemonstrating the effectiveness of foundation models for world music\nunderstanding. We also find that our targeted fine-tuning approach does not\nconsistently outperform probing across all settings, suggesting foundation\nmodels already encode substantial musical knowledge. Our evaluation framework\nand benchmarking results contribute to understanding how far current models are\nfrom achieving universal music representations while establishing metrics for\nfuture progress.", "AI": {"tldr": "该论文评估了五种音频基础模型在六种不同音乐传统中的跨文化泛化能力，发现较大模型在非西方音乐上表现更好，但文化距离较远的传统表现下降。", "motivation": "研究基础模型是否能泛化到多样化的音乐传统中，填补跨文化音乐信息检索的空白。", "method": "采用三种方法：探测模型固有表示、针对性监督微调1-2层、多标签少样本学习。", "result": "在六种数据集中五种达到最优性能，但针对性微调不总是优于探测。", "conclusion": "基础模型已编码大量音乐知识，但仍需改进以实现通用音乐表示。"}}
{"id": "2506.15890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15890", "abs": "https://arxiv.org/abs/2506.15890", "authors": ["Thomas Manzini", "Priyankari Perali", "Robin R. Murphy", "David Merrick"], "title": "Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene", "comment": "6 pages, 5 Figures, 1 Table", "summary": "This paper details four principal challenges encountered with machine\nlearning (ML) damage assessment using small uncrewed aerial systems (sUAS) at\nHurricanes Debby and Helene that prevented, degraded, or delayed the delivery\nof data products during operations and suggests three research directions for\nfuture real-world deployments. The presence of these challenges is not\nsurprising given that a review of the literature considering both datasets and\nproposed ML models suggests this is the first sUAS-based ML system for disaster\ndamage assessment actually deployed as a part of real-world operations. The\nsUAS-based ML system was applied by the State of Florida to Hurricanes Helene\n(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne\nsUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a\nWintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial\nimagery of inland flood damage resulting from post-tropical remnants of\nHurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing\nfurther insights into the advantages and limitations of sUAS for disaster\nresponse. The four challenges (variationin spatial resolution of input imagery,\nspatial misalignment between imagery and geospatial data, wireless\nconnectivity, and data product format) lead to three recommendations that\nspecify research needed to improve ML model capabilities to accommodate the\nwide variation of potential spatial resolutions used in practice, handle\nspatial misalignment, and minimize the dependency on wireless connectivity.\nThese recommendations are expected to improve the effective operational use of\nsUAS and sUAS-based ML damage assessment systems for disaster response.", "AI": {"tldr": "论文总结了在飓风Debby和Helene中使用小型无人机（sUAS）进行机器学习（ML）损害评估时遇到的四个主要挑战，并提出了未来实际部署的三个研究方向。", "motivation": "研究旨在解决在真实灾害响应中首次部署的基于sUAS的ML损害评估系统所面临的操作性挑战，以提升其效率和实用性。", "method": "通过分析在佛罗里达州飓风Helene和Debby中收集的无人机影像数据（包括2次飞行任务和3.0千兆像素的影像），以及宾夕法尼亚州飓风Debby后的人机影像数据（436张正射影像，136.5千兆像素），识别出四个主要挑战。", "result": "研究发现四个挑战（输入影像空间分辨率的变化、影像与地理空间数据的空间不对齐、无线连接问题、数据产品格式）影响了数据产品的交付。", "conclusion": "提出了三个研究方向，以改进ML模型的能力，适应实际中广泛的空间分辨率变化、处理空间不对齐问题，并减少对无线连接的依赖，从而提升sUAS在灾害响应中的操作效率。"}}
{"id": "2506.16312", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16312", "abs": "https://arxiv.org/abs/2506.16312", "authors": ["Angxuan Chen"], "title": "When learning analytics dashboard is explainable: An exploratory study on the effect of GenAI-supported learning analytics dashboard", "comment": null, "summary": "This study investigated the impact of a theory-driven, explainable Learning\nAnalytics Dashboard (LAD) on university students' human-AI collaborative\nacademic abstract writing task. Grounded in Self-Regulated Learning (SRL)\ntheory and incorporating Explainable AI (XAI) principles, our LAD featured a\nthree-layered design (Visual, Explainable, Interactive). In an experimental\nstudy, participants were randomly assigned to either an experimental group\n(using the full explainable LAD) or a control group (using a visual-only LAD)\nto collaboratively write an academic abstract with a Generative AI. While\nquantitative analysis revealed no significant difference in the quality of\nco-authored abstracts between the two groups, a significant and noteworthy\ndifference emerged in conceptual understanding: students in the explainable LAD\ngroup demonstrated a superior grasp of abstract writing principles, as\nevidenced by their higher scores on a knowledge test (p= .026). These findings\nhighlight that while basic AI-generated feedback may suffice for immediate task\ncompletion, the provision of explainable feedback is crucial for fostering\ndeeper learning, enhancing conceptual understanding, and developing\ntransferable skills fundamental to self-regulated learning in academic writing\ncontexts.", "AI": {"tldr": "研究探讨了理论驱动的可解释学习分析仪表盘（LAD）对大学生与AI协作撰写学术摘要的影响。结果显示，可解释LAD组在概念理解上表现更优。", "motivation": "探究可解释AI反馈在学术写作中对学生概念理解和自我调节学习的影响。", "method": "基于自我调节学习理论和可解释AI原则设计三层次LAD（视觉、可解释、交互），实验组使用完整LAD，对照组仅用视觉LAD。", "result": "两组摘要质量无显著差异，但可解释LAD组在概念理解测试中得分更高（p=0.026）。", "conclusion": "可解释反馈对深化学习和培养可迁移技能至关重要，尤其在学术写作中。"}}
{"id": "2506.15908", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15908", "abs": "https://arxiv.org/abs/2506.15908", "authors": ["Elif Keles", "Merve Yazol", "Gorkem Durak", "Ziliang Hong", "Halil Ertugrul Aktas", "Zheyuan Zhang", "Linkai Peng", "Onkar Susladkar", "Necati Guzelyel", "Oznur Leman Boyunaga", "Cemal Yazici", "Mark Lowe", "Aliye Uc", "Ulas Bagci"], "title": "Pediatric Pancreas Segmentation from MRI Scans with Deep Learning", "comment": "Code and MRI data available for public", "summary": "Objective: Our study aimed to evaluate and validate PanSegNet, a deep\nlearning (DL) algorithm for pediatric pancreas segmentation on MRI in children\nwith acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.\nMethods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T\nSiemens Aera/Verio) from children aged 2-19 years at Gazi University\n(2015-2024). The dataset includes healthy children as well as patients\ndiagnosed with AP or CP based on clinical criteria. Pediatric and general\nradiologists manually segmented the pancreas, then confirmed by a senior\npediatric radiologist. PanSegNet-generated segmentations were assessed using\nDice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance\n(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W\nscans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)\nand 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved\nDSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98\nmm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86\n(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and\n0.81. Strong agreement was observed between automated and manual volumes (R^2 =\n0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.\nConclusion: PanSegNet represents the first validated deep learning solution for\npancreatic MRI segmentation, achieving expert-level performance across healthy\nand diseased states. This tool, algorithm, along with our annotated dataset,\nare freely available on GitHub and OSF, advancing accessible, radiation-free\npediatric pancreatic imaging and fostering collaborative research in this\nunderserved domain.", "AI": {"tldr": "PanSegNet是一种深度学习算法，用于儿童MRI胰腺分割，在健康和患病状态下表现优异。", "motivation": "评估和验证PanSegNet在儿童急性胰腺炎、慢性胰腺炎和健康对照中的MRI胰腺分割性能。", "method": "收集84例儿童MRI扫描，手动分割胰腺后，用PanSegNet生成分割结果，评估指标包括DSC和HD95。", "result": "PanSegNet在健康儿童中DSC为88%，AP为81%，CP为80%，HD95分别为3.98mm、9.85mm和15.67mm，表现出高可靠性。", "conclusion": "PanSegNet是首个验证的深度学习胰腺MRI分割工具，性能达到专家水平，工具和数据集已开源。"}}
{"id": "2506.15694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15694", "abs": "https://arxiv.org/abs/2506.15694", "authors": ["Iliyas Ibrahim Iliyas", "Souley Boukari", "Abdulsalam Yau Gital"], "title": "Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction", "comment": null, "summary": "This study introduces a framework that integrates nonlinear feature\nextraction, classification, and efficient optimization. First, kernel principal\ncomponent analysis with a radial basis function kernel reduces dimensionality\nwhile preserving 95% of the variance. Second, a multilayer perceptron (MLP)\nlearns to predict disease status. Finally, a modified multiprocessing genetic\nalgorithm (MIGA) optimizes MLP hyperparameters in parallel over ten\ngenerations. We evaluated this approach on three datasets: the Wisconsin\nDiagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and\nthe chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best\naccuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%\nfor chronic kidney disease. These results outperform those of other methods,\nsuch as grid search, random search, and Bayesian optimization. Compared with a\nstandard genetic algorithm, kernel PCA revealed nonlinear relationships that\nimproved classification, and the MIGA's parallel fitness evaluations reduced\nthe tuning time by approximately 60%. The genetic algorithm incurs high\ncomputational cost from sequential fitness evaluations, but our multiprocessing\ninterface GA (MIGA) parallelizes this step, slashing the tuning time and\nsteering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for\nbreast cancer, Parkinson's disease, and CKD, respectively.", "AI": {"tldr": "该研究提出了一种结合非线性特征提取、分类和高效优化的框架，通过核主成分分析降维、多层感知机分类，以及改进的多处理遗传算法优化超参数，在多个数据集上取得了优于其他方法的准确率。", "motivation": "解决传统方法在非线性特征提取和超参数优化中效率低下的问题，提升分类模型的性能。", "method": "1. 使用核主成分分析降维；2. 多层感知机进行分类；3. 改进的多处理遗传算法并行优化超参数。", "result": "在乳腺癌、帕金森病和慢性肾病数据集上的准确率分别为99.12%、94.87%和100%，优化时间减少约60%。", "conclusion": "该框架显著提升了分类性能，优化效率高，适用于多种医学数据集。"}}
{"id": "2506.17005", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17005", "abs": "https://arxiv.org/abs/2506.17005", "authors": ["Ram Milan Kumar Verma", "Shashi Ranjan Kumar", "Hemendra Arya"], "title": "Trajectory tracking control of USV with actuator constraints in the presence of disturbances", "comment": null, "summary": "All practical systems often pose a problem of finite control capability,\nwhich can notably degrade the performance if not properly addressed. Since\nactuator input bounds are typically known, integrating actuator saturation\nconsiderations into the control law design process can lead to enhanced\nperformance and more precise trajectory tracking. Also, the actuators cannot\nprovide the demanded forces or torques instantaneously; hence, there is a\nlimitation on the rate of magnitude. This work proposes nonlinear feedback\ncontroller designs developed using the Lyapunov stability and backstepping\nmethod while actively considering the actuator magnitude and rate constraints.\nThe system dynamics are augmented with a smooth control input saturation model.\nAdditionally, an observer is incorporated to estimate the disturbance vector.\nThrough Lyapunov stability analysis, we demonstrate the system's stability\nunder the proposed controller for the Uncrewed Surface Vessel (USV), ensuring\nadherence to actuator constraints provided their initial values fall within the\nprescribed bounds. Extensive numerical simulations performed by considering\nvarious trajectories and multiple initial conditions demonstrate the\neffectiveness of the controller in maintaining tracking performance without\nviolating actuator constraints. This work also relaxes the assumption of\nequally capable actuators to be used to control the motion of USVs, affirming\nthe viability of the controller in practical applications.", "AI": {"tldr": "论文提出了一种考虑执行器幅值和速率约束的非线性反馈控制器设计方法，通过Lyapunov稳定性和反步法实现无人水面艇（USV）的稳定控制。", "motivation": "实际系统中执行器的有限控制能力会显著降低性能，需在设计控制律时考虑执行器饱和问题以提高性能。", "method": "结合Lyapunov稳定性和反步法设计控制器，引入平滑的输入饱和模型和扰动观测器。", "result": "通过数值仿真验证了控制器在多种轨迹和初始条件下能保持跟踪性能且不违反执行器约束。", "conclusion": "该控制器放宽了对执行器能力均等的假设，适用于实际应用。"}}
{"id": "2506.16710", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16710", "abs": "https://arxiv.org/abs/2506.16710", "authors": ["Aditya Bhatt", "Mary Katherine Corra", "Franklin Merlo", "Prajit KrisshnaKumar", "Souma Chowdhury"], "title": "Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms", "comment": "to be published in IDETC 2025 conference proceedings", "summary": "Signal source localization has been a problem of interest in the multi-robot\nsystems domain given its applications in search \\& rescue and hazard\nlocalization in various industrial and outdoor settings. A variety of\nmulti-robot search algorithms exist that usually formulate and solve the\nassociated autonomous motion planning problem as a heuristic model-free or\nbelief model-based optimization process. Most of these algorithms however\nremains tested only in simulation, thereby losing the opportunity to generate\nknowledge about how such algorithms would compare/contrast in a real physical\nsetting in terms of search performance and real-time computing performance. To\naddress this gap, this paper presents a new lab-scale physical setup and\nassociated open-source software pipeline to evaluate and benchmark multi-robot\nsearch algorithms. The presented physical setup innovatively uses an acoustic\nsource (that is safe and inexpensive) and small ground robots (e-pucks)\noperating in a standard motion-capture environment. This setup can be easily\nrecreated and used by most robotics researchers. The acoustic source also\npresents interesting uncertainty in terms of its noise-to-signal ratio, which\nis useful to assess sim-to-real gaps. The overall software pipeline is designed\nto readily interface with any multi-robot search algorithm with minimal effort\nand is executable in parallel asynchronous form. This pipeline includes a\nframework for distributed implementation of multi-robot or swarm search\nalgorithms, integrated with a ROS (Robotics Operating System)-based software\nstack for motion capture supported localization. The utility of this novel\nsetup is demonstrated by using it to evaluate two state-of-the-art multi-robot\nsearch algorithms, based on swarm optimization and batch-Bayesian Optimization\n(called Bayes-Swarm), as well as a random walk baseline.", "AI": {"tldr": "论文提出了一种新的实验室规模物理设置和开源软件管道，用于评估和基准测试多机器人搜索算法，填补了算法在真实物理环境中性能测试的空白。", "motivation": "现有大多数多机器人搜索算法仅在仿真中测试，缺乏在真实物理环境中的性能比较，因此需要一种可复现的实验设置来评估算法的实际表现。", "method": "使用声学源和小型地面机器人（e-pucks）在标准运动捕捉环境中构建实验设置，并开发开源软件管道，支持分布式多机器人搜索算法的并行异步执行。", "result": "实验设置成功评估了两种先进的多机器人搜索算法（群体优化和Bayes-Swarm）以及随机游走基线，展示了其在实际环境中的性能。", "conclusion": "该实验设置和软件管道为机器人研究者提供了一种易于复现的工具，用于评估多机器人搜索算法在真实环境中的性能，填补了仿真与现实的差距。"}}
{"id": "2506.15912", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15912", "abs": "https://arxiv.org/abs/2506.15912", "authors": ["Zifei Xu", "Sayeh Sharify", "Hesham Mostafa", "Tristan Webb", "Wanzin Yazar", "Xin Wang"], "title": "Early Attentive Sparsification Accelerates Neural Speech Transcription", "comment": null, "summary": "Transformer-based neural speech processing has achieved state-of-the-art\nperformance. Since speech audio signals are known to be highly compressible,\nhere we seek to accelerate neural speech transcription by time-domain signal\nsparsification early in the neural encoding stage, taking advantage of the\ninterpretability of the self-attention mechanism in transformer audio encoders.\nWith the Whisper family of models, we perform a systematic architecture search\nover the joint space of sparsification stage (a certain encoder layer) and\ncompression ratio (sparsity). We found that the best resulting solutions under\n1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity\nat an early encoding stage, and thereby achieve up to 1.6x runtime acceleration\nin English speech transcription tasks on Nvidia GPUs without any fine-tuning.", "AI": {"tldr": "通过时间域信号稀疏化加速神经语音转录，利用Transformer编码器的自注意力机制，在Whisper模型上实现1.6倍运行时加速。", "motivation": "语音信号高度可压缩，利用Transformer的自注意力机制可解释性，加速神经语音转录。", "method": "在Whisper模型上进行架构搜索，探索稀疏化阶段和压缩比的联合空间。", "result": "最佳方案在准确率下降1%内，隐藏状态稀疏化40-60%，实现1.6倍运行时加速。", "conclusion": "早期编码阶段的稀疏化可显著加速语音转录，无需微调。"}}
{"id": "2506.15899", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15899", "abs": "https://arxiv.org/abs/2506.15899", "authors": ["Israel Charles", "Hossein Maghsoumi", "Yaser Fallah"], "title": "Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform", "comment": null, "summary": "The RoboRacer (F1TENTH) platform has emerged as a leading testbed for\nadvancing autonomous driving research, offering a scalable, cost-effective, and\ncommunity-driven environment for experimentation. This paper presents a\ncomprehensive survey of the platform, analyzing its modular hardware and\nsoftware architecture, diverse research applications, and role in autonomous\nsystems education. We examine critical aspects such as bridging the\nsimulation-to-reality (Sim2Real) gap, integration with simulation environments,\nand the availability of standardized datasets and benchmarks. Furthermore, the\nsurvey highlights advancements in perception, planning, and control algorithms,\nas well as insights from global competitions and collaborative research\nefforts. By consolidating these contributions, this study positions RoboRacer\nas a versatile framework for accelerating innovation and bridging the gap\nbetween theoretical research and real-world deployment. The findings underscore\nthe platform's significance in driving forward developments in autonomous\nracing and robotics.", "AI": {"tldr": "本文对RoboRacer（F1TENTH）平台进行了全面调查，分析了其硬件和软件架构、研究应用及教育作用，强调了其在自动驾驶研究中的重要性。", "motivation": "RoboRacer平台为自动驾驶研究提供了一个可扩展、经济高效且社区驱动的实验环境，本文旨在总结其贡献和潜力。", "method": "通过分析平台的模块化架构、仿真与现实（Sim2Real）的桥梁、数据集和基准测试，以及感知、规划和控制算法的进展。", "result": "研究发现RoboRacer是一个多功能框架，能加速创新并弥合理论研究与实际部署之间的差距。", "conclusion": "RoboRacer在推动自动驾驶赛车和机器人技术发展中具有重要意义。"}}
{"id": "2506.16345", "categories": ["cs.HC", "H.5.2"], "pdf": "https://arxiv.org/pdf/2506.16345", "abs": "https://arxiv.org/abs/2506.16345", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Bruna Capeleti", "Rafael Ferreira Mello", "André Freire", "Luciana Zaina"], "title": "Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation", "comment": "Paper accepted at the 20th IFIP TC13 International Conference on\n  Human-Computer Interaction (INTERACT) 2025", "summary": "Heuristic evaluation is a widely used method in Human-Computer Interaction\n(HCI) to inspect interfaces and identify issues based on heuristics. Recently,\nLarge Language Models (LLMs), such as GPT-4o, have been applied in HCI to\nassist in persona creation, the ideation process, and the analysis of\nsemi-structured interviews. However, considering the need to understand\nheuristics and the high degree of abstraction required to evaluate them, LLMs\nmay have difficulty conducting heuristic evaluation. However, prior research\nhas not investigated GPT-4o's performance in heuristic evaluation compared to\nHCI experts in web-based systems. In this context, this study aims to compare\nthe results of a heuristic evaluation performed by GPT-4o and human experts. To\nthis end, we selected a set of screenshots from a web system and asked GPT-4o\nto perform a heuristic evaluation based on Nielsen's Heuristics from a\nliterature-grounded prompt. Our results indicate that only 21.2% of the issues\nidentified by human experts were also identified by GPT-4o, despite it found 27\nnew issues. We also found that GPT-4o performed better for heuristics related\nto aesthetic and minimalist design and match between system and real world,\nwhereas it has difficulty identifying issues in heuristics related to\nflexibility, control, and user efficiency. Additionally, we noticed that GPT-4o\ngenerated several false positives due to hallucinations and attempts to predict\nissues. Finally, we highlight five takeaways for the conscious use of GPT-4o in\nheuristic evaluations.", "AI": {"tldr": "GPT-4o在启发式评估中表现不如人类专家，仅识别了21.2%的人类专家发现的问题，但发现了27个新问题。它在美学和现实匹配方面表现较好，但在灵活性和用户效率方面较差。", "motivation": "研究GPT-4o在启发式评估中的表现，并与人类专家对比，以探索其在HCI中的应用潜力。", "method": "使用Nielsen的启发式方法，通过GPT-4o和人类专家评估同一组网页截图。", "result": "GPT-4o识别的问题与人类专家重叠率低，且存在误报。", "conclusion": "GPT-4o在启发式评估中可作为辅助工具，但需谨慎使用以避免误报。"}}
{"id": "2506.15929", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.15929", "abs": "https://arxiv.org/abs/2506.15929", "authors": ["Liangyan Li", "Yimo Ning", "Kevin Le", "Wei Dong", "Yunzhe Li", "Jun Chen", "Xiaohong Liu"], "title": "MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior", "comment": null, "summary": "This paper introduces a novel framework for image and video demoir\\'eing by\nintegrating Maximum A Posteriori (MAP) estimation with advanced deep learning\ntechniques. Demoir\\'eing addresses inherently nonlinear degradation processes,\nwhich pose significant challenges for existing methods.\n  Traditional supervised learning approaches either fail to remove moir\\'e\npatterns completely or produce overly smooth results. This stems from\nconstrained model capacity and scarce training data, which inadequately\nrepresent the clean image distribution and hinder accurate reconstruction of\nground-truth images. While generative models excel in image restoration for\nlinear degradations, they struggle with nonlinear cases such as demoir\\'eing\nand often introduce artifacts.\n  To address these limitations, we propose a hybrid MAP-based framework that\nintegrates two complementary components. The first is a supervised learning\nmodel enhanced with efficient linear attention Test-Time Training (TTT)\nmodules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\\'eing.\nThe second is a Truncated Flow Matching Prior (TFMP) that further refines the\noutputs by aligning them with the clean image distribution, effectively\nrestoring high-frequency details and suppressing artifacts. These two\ncomponents combine the computational efficiency of linear attention with the\nrefinement abilities of generative models, resulting in improved restoration\nperformance.", "AI": {"tldr": "本文提出了一种结合MAP估计和深度学习的图像和视频去摩尔纹框架，解决了非线性退化问题。", "motivation": "传统方法无法完全去除摩尔纹或导致结果过于平滑，生成模型在非线性退化中表现不佳。", "method": "提出混合MAP框架，结合监督学习模型和TFMP先验，前者学习非线性映射，后者优化输出。", "result": "框架结合了线性注意力的计算效率和生成模型的优化能力，提升了恢复性能。", "conclusion": "该框架有效解决了去摩尔纹问题，恢复了高频细节并抑制了伪影。"}}
{"id": "2506.15695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15695", "abs": "https://arxiv.org/abs/2506.15695", "authors": ["Xinxing Ren", "Qianbo Zang", "Zekun Guo"], "title": "SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have shown impressive\nperformance in mathematical reasoning and code generation. However, LLMs still\nstruggle in the simulation domain, particularly in generating Simulink models,\nwhich are essential tools in engineering and scientific research. Our\npreliminary experiments indicate that LLM agents often fail to produce reliable\nand complete Simulink simulation code from text-only inputs, likely due to the\nlack of Simulink-specific data in their pretraining. To address this challenge,\nwe propose SimuGen, a multimodal agent-based framework that automatically\ngenerates accurate Simulink simulation code by leveraging both the visual\nSimulink diagram and domain knowledge. SimuGen coordinates several specialized\nagents, including an investigator, unit test reviewer, code generator,\nexecutor, debug locator, and report writer, supported by a domain-specific\nknowledge base. This collaborative and modular design enables interpretable,\nrobust, and reproducible Simulink simulation generation. Our source code is\npublicly available at https://github.com/renxinxing123/SimuGen_beta.", "AI": {"tldr": "论文提出SimuGen框架，通过多模态代理生成准确的Simulink仿真代码，解决LLMs在Simulink领域的不足。", "motivation": "LLMs在数学推理和代码生成方面表现优异，但在Simulink模型生成上表现不佳，缺乏相关预训练数据。", "method": "提出SimuGen框架，结合视觉Simulink图和领域知识，通过多个协作代理（如代码生成器、调试定位器等）生成仿真代码。", "result": "SimuGen能够生成可靠、完整的Simulink仿真代码，支持可解释性和可重复性。", "conclusion": "SimuGen通过多模态代理和领域知识，显著提升了Simulink仿真代码的生成能力。"}}
{"id": "2506.17044", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17044", "abs": "https://arxiv.org/abs/2506.17044", "authors": ["Orcun Karaca", "Irina Subotic", "Lennart Harnefors", "Ioannis Tsoumas"], "title": "On the input admittance of a universal power synchronization controller with droop controllers", "comment": null, "summary": "Recent work has proposed a universal framework that integrates the\nwell-established power synchronization control into vector current control.\nUsing this controller for the parallel operation of grid-forming converters,\nand/or with loads that have strong voltage magnitude sensitivity, requires\nadditional loops manipulating the voltage magnitude, e.g., $QV$ and $PV$\nvoltage-power droop controllers. This paper derives the input admittance of the\nresulting overall scheme. Sensitivity analyses based on the passivity index\ndemonstrate the benefits of the proportional components of $QV$ and $PV$\ncontrol. A case study is presented where a grid-forming converter is operated\nin parallel with a generator.", "AI": {"tldr": "本文提出了一种将功率同步控制与矢量电流控制集成的通用框架，并分析了其输入导纳及比例控制的优势。", "motivation": "研究电网形成变流器并联运行时，电压幅值控制的必要性及其对系统稳定性的影响。", "method": "通过推导整体控制方案的输入导纳，并基于无源性指数进行灵敏度分析。", "result": "比例控制的QV和PV控制器在提升系统性能方面具有显著优势。", "conclusion": "比例控制组件在电网形成变流器与发电机并联运行时能有效提升系统稳定性。"}}
{"id": "2506.16748", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16748", "abs": "https://arxiv.org/abs/2506.16748", "authors": ["Arjo Chakravarty", "Michael X. Grey", "M. A. Viraj J. Muthugala", "Mohan Rajesh Elara"], "title": "A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT", "comment": null, "summary": "Free-space multi-agent path planning remains challenging at large scales.\nMost existing methods either offer optimality guarantees but do not scale\nbeyond a few dozen agents, or rely on grid-world assumptions that do not\ngeneralize well to continuous space. In this work, we propose a hybrid,\nrule-based planning framework that combines Priority Inheritance with\nBacktracking (PiBT) with a novel safety-aware path smoothing method. Our\napproach extends PiBT to 8-connected grids and selectively applies\nstring-pulling based smoothing while preserving collision safety through local\ninteraction awareness and a fallback collision resolution step based on Safe\nInterval Path Planning (SIPP). This design allows us to reduce overall path\nlengths while maintaining real-time performance. We demonstrate that our method\ncan scale to over 500 agents in large free-space environments, outperforming\nexisting any-angle and optimal methods in terms of runtime, while producing\nnear-optimal trajectories in sparse domains. Our results suggest this framework\nis a promising building block for scalable, real-time multi-agent navigation in\nrobotics systems operating beyond grid constraints.", "AI": {"tldr": "提出了一种结合优先级继承回溯（PiBT）和安全感知路径平滑的混合规划框架，适用于大规模自由空间多智能体路径规划。", "motivation": "现有方法在大规模多智能体路径规划中难以兼顾最优性和可扩展性，或依赖于网格假设。", "method": "扩展PiBT至8连通网格，结合安全感知路径平滑和局部交互感知，采用SIPP进行碰撞解决。", "result": "方法可扩展到500+智能体，运行时优于现有方法，在稀疏环境中生成接近最优的轨迹。", "conclusion": "该框架是机器人系统中可扩展、实时多智能体导航的有前景的解决方案。"}}
{"id": "2506.16173", "categories": ["cs.RO", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16173", "abs": "https://arxiv.org/abs/2506.16173", "authors": ["Jiang Wang", "Runwu Shi", "Benjamin Yen", "He Kong", "Kazuhiro Nakadai"], "title": "Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments", "comment": "This paper was accepted and going to appear in the 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)", "summary": "Accurately estimating sound source positions is crucial for robot audition.\nHowever, existing sound source localization methods typically rely on a\nmicrophone array with at least two spatially preconfigured microphones. This\nrequirement hinders the applicability of microphone-based robot audition\nsystems and technologies. To alleviate these challenges, we propose an online\nsound source localization method that uses a single microphone mounted on a\nmobile robot in reverberant environments. Specifically, we develop a\nlightweight neural network model with only 43k parameters to perform real-time\ndistance estimation by extracting temporal information from reverberant\nsignals. The estimated distances are then processed using an extended Kalman\nfilter to achieve online sound source localization. To the best of our\nknowledge, this is the first work to achieve online sound source localization\nusing a single microphone on a moving robot, a gap that we aim to fill in this\nwork. Extensive experiments demonstrate the effectiveness and merits of our\napproach. To benefit the broader research community, we have open-sourced our\ncode at https://github.com/JiangWAV/single-mic-SSL.", "AI": {"tldr": "提出了一种基于单麦克风的移动机器人实时声源定位方法，填补了单麦克风在线定位的空白。", "motivation": "现有声源定位方法依赖多麦克风阵列，限制了机器人听觉系统的适用性。", "method": "开发了一个轻量级神经网络（43k参数）提取混响信号的时间信息，结合扩展卡尔曼滤波器实现在线定位。", "result": "实验验证了方法的有效性和优势。", "conclusion": "该方法首次实现了移动机器人单麦克风在线声源定位，并开源了代码。"}}
{"id": "2506.15920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15920", "abs": "https://arxiv.org/abs/2506.15920", "authors": ["Liang Qin", "Weiwei Wan", "Jun Takahashi", "Ryo Negishi", "Masaki Matsushita", "Kensuke Harada"], "title": "Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency", "comment": null, "summary": "This work proposes a learning method to accelerate robotic pick-and-place\nplanning by predicting shared grasps. Shared grasps are defined as grasp poses\nfeasible to both the initial and goal object configurations in a pick-and-place\ntask. Traditional analytical methods for solving shared grasps evaluate grasp\ncandidates separately, leading to substantial computational overhead as the\ncandidate set grows. To overcome the limitation, we introduce an Energy-Based\nModel (EBM) that predicts shared grasps by combining the energies of feasible\ngrasps at both object poses. This formulation enables early identification of\npromising candidates and significantly reduces the search space. Experiments\nshow that our method improves grasp selection performance, offers higher data\nefficiency, and generalizes well to unseen grasps and similarly shaped objects.", "AI": {"tldr": "提出了一种基于能量模型（EBM）的学习方法，用于加速机器人抓取-放置任务中的共享抓取预测。", "motivation": "传统分析方法在解决共享抓取问题时需单独评估每个候选抓取，计算开销大。", "method": "引入能量模型（EBM），通过结合初始和目标物体位姿的可行抓取能量，预测共享抓取。", "result": "实验表明，该方法提升了抓取选择性能，数据效率更高，并能泛化到未见过的抓取和相似形状物体。", "conclusion": "该方法显著减少了搜索空间，提高了机器人抓取-放置任务的效率。"}}
{"id": "2506.16468", "categories": ["cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16468", "abs": "https://arxiv.org/abs/2506.16468", "authors": ["Vlad Cnejevici", "Matthias Ponfick", "Raul C. Sîmpetru", "Alessandro Del Vecchio"], "title": "Closed-Loop Control of Electrical Stimulation through Spared Motor Unit Ensembles Restores Foot Movements after Spinal Cord Injury", "comment": "26 pages, 7 figures", "summary": "Restoring movement of a paralyzed foot is a key challenge in helping\nindividuals with neurological conditions such as spinal cord injury (SCI) to\nimprove their quality of life. Neuroprostheses based on functional electrical\nstimulation (FES) can restore the physiological range of motion by stimulating\nthe affected muscles using surface electrodes. We have previously shown that,\ndespite chronic motor-complete SCI, it is possible to capture paralyzed hand\nmovements in individuals with tetraplegia using spared and modulated motor unit\n(MU) activity decoded with non-invasive electromyography (EMG) sensors. This\nstudy investigated whether a wearable high-density surface EMG system could\ncapture and control paralyzed foot kinematics in closed-loop control with an\nFES system. We found that all our participants with SCI (2 with chronic SCI and\n3 with acute SCI) retained distinct spared EMG activity for at least three\nankle movements, which allowed them to reliably control a digital cursor using\ntheir spared tibialis anterior and triceps surae MU activity. Movement\nseparability was further reconfirmed by extracting task-modulated MU activity\nduring foot flexion/extension (3-7 modulated MUs/participant). Three\nparticipants were further able to modulate and maintain their foot\nflexion/extension EMG levels with an accuracy of >70%. Lastly, we show that\nreal-time control of a FES system using EMG from the affected limb can restore\nfoot movements in a highly intuitive way, significantly improving the lost or\npathological foot range of motion. Our system provides an intuitive approach\nfor closed-loop control of FES that has the potential to assist individuals\nwith SCI in regaining lost motor functions.", "AI": {"tldr": "研究探讨了通过高密度表面EMG系统结合FES系统，实现对瘫痪足部运动的闭环控制，发现SCI患者保留的EMG活动可用于准确控制足部运动。", "motivation": "帮助脊髓损伤（SCI）患者恢复足部运动，提高生活质量。", "method": "使用可穿戴高密度表面EMG系统解码保留的MU活动，结合FES系统实现闭环控制。", "result": "所有参与者均保留至少三种踝关节运动的EMG活动，部分参与者能准确控制足部屈伸，且FES系统能显著恢复运动范围。", "conclusion": "该系统为SCI患者提供了一种直观的闭环控制方法，有望帮助恢复运动功能。"}}
{"id": "2506.15937", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.15937", "abs": "https://arxiv.org/abs/2506.15937", "authors": ["Yosub Shin", "Igor Molybog"], "title": "Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization", "comment": null, "summary": "Video synchronization-aligning multiple video streams capturing the same\nevent from different angles-is crucial for applications such as reality TV show\nproduction, sports analysis, surveillance, and autonomous systems. Prior work\nhas heavily relied on audio cues or specific visual events, limiting\napplicability in diverse settings where such signals may be unreliable or\nabsent. Additionally, existing benchmarks for video synchronization lack\ngenerality and reproducibility, restricting progress in the field. In this\nwork, we introduce VideoSync, a video synchronization framework that operates\nindependently of specific feature extraction methods, such as human pose\nestimation, enabling broader applicability across different content types. We\nevaluate our system on newly composed datasets covering single-human,\nmulti-human, and non-human scenarios, providing both the methodology and code\nfor dataset creation to establish reproducible benchmarks. Our analysis reveals\nbiases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,\nleading to inflated performance claims. We correct these biases and propose a\nmore rigorous evaluation framework, demonstrating that VideoSync outperforms\nexisting approaches, including SeSyn-Net, under fair experimental conditions.\nAdditionally, we explore various synchronization offset prediction methods,\nidentifying a convolutional neural network (CNN)-based model as the most\neffective. Our findings advance video synchronization beyond domain-specific\nconstraints, making it more generalizable and robust for real-world\napplications.", "AI": {"tldr": "VideoSync是一个独立于特定特征提取方法的视频同步框架，适用于多种内容类型，并通过新的数据集和评估框架提升了性能。", "motivation": "现有视频同步方法依赖音频或特定视觉事件，限制了通用性；现有基准缺乏可重复性，阻碍了领域发展。", "method": "提出VideoSync框架，不依赖特定特征提取方法，使用新数据集（单人类、多人类、非人类场景）和CNN模型进行同步偏移预测。", "result": "VideoSync在公平实验条件下优于现有方法（如SeSyn-Net），并纠正了先前工作中的偏差。", "conclusion": "VideoSync提升了视频同步的通用性和鲁棒性，适用于现实应用。"}}
{"id": "2506.15696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15696", "abs": "https://arxiv.org/abs/2506.15696", "authors": ["Haipeng Zhou", "Sicheng Yang", "Sihan Yang", "Jing Qin", "Lei Chen", "Lei Zhu"], "title": "CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction", "comment": null, "summary": "Survival prediction aims to evaluate the risk level of cancer patients.\nExisting methods primarily rely on pathology and genomics data, either\nindividually or in combination. From the perspective of cancer pathogenesis,\nepigenetic changes, such as methylation data, could also be crucial for this\ntask. Furthermore, no previous endeavors have utilized textual descriptions to\nguide the prediction. To this end, we are the first to explore the use of four\nmodalities, including three clinical modalities and language, for conducting\nsurvival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)\nto propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and\ninter-learning. We encode the clinical data as the raw features, which remain\ndomain-specific knowledge for intra-learning. In terms of inter-learning, we\nuse language to prompt the raw features and introduce an Autoregressive Mutual\nTraction module for synergistic representation. This tailored framework\nfacilitates joint learning among multiple modalities. Our approach is evaluated\nacross five public cancer datasets, and extensive experiments validate the\neffectiveness of our methods and proposed designs, leading to producing \\sota\nresults. Codes will be released.", "AI": {"tldr": "论文提出了一种名为Chain-of-Cancer (CoC)的多模态框架，结合临床数据和语言描述进行癌症患者生存预测，通过内学习和互学习实现多模态联合学习，实验验证了其有效性。", "motivation": "现有方法主要依赖病理和基因组数据，忽略了表观遗传变化（如甲基化数据）和语言描述的潜力。论文首次探索了四种模态（包括语言）的联合使用。", "method": "提出CoC框架，包括内学习（临床数据编码为原始特征）和互学习（语言提示原始特征，引入自回归互牵引模块）。", "result": "在五个公共癌症数据集上验证了方法的有效性，取得了最先进的结果。", "conclusion": "CoC框架通过多模态联合学习显著提升了生存预测性能，代码将开源。"}}
{"id": "2506.17048", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17048", "abs": "https://arxiv.org/abs/2506.17048", "authors": ["Noël Hallemans", "Philipp Dechent", "David Howey", "Simon Clark", "Mona Faraji Niri", "James Marco", "Patrick S. Grant", "Stephen R. Duncan"], "title": "Opportunities for real-time process control of electrode properties in lithium-ion battery manufacturing", "comment": null, "summary": "Lithium-ion batteries (LIBs) have an important role in the shift required to\nachieve a global net-zero carbon target of 2050. Electrode manufacture is\namongst the most expensive steps of the LIB manufacturing process and, despite\nits apparent maturity, optimised manufacturing conditions are arrived at by\nlargely trial and error. Currently, LIB manufacturing plants are controlled to\nfollow the fixed \"recipe\" obtained by trial and error, which may nonetheless be\nsuboptimal. Moreover, regulating the process as a whole to conform to the set\nconditions is not widespread. Inspired by control approaches used in other film\nand sheet processes, we discuss opportunities for implementing real-time\nprocess control of electrode-related products, which has the potential to\nreduce the electrode manufacturing cost, CO2 emissions, usage of resources by\nincreases in process yield, and throughput. We highlight the challenges and\nsignificant opportunities of implementing real-time process control in LIB\nelectrode production lines.", "AI": {"tldr": "探讨锂离子电池电极制造中实时过程控制的潜力与挑战，以优化成本、减少排放并提高效率。", "motivation": "锂离子电池电极制造是成本高且依赖试错优化的过程，现有固定“配方”可能非最优，需改进以实现更高效、环保的生产。", "method": "借鉴其他薄膜和板材过程的控制方法，提出在电极制造中实施实时过程控制的策略。", "result": "实时控制有望降低电极制造成本、减少CO2排放、提高资源利用率和生产效率。", "conclusion": "实施实时过程控制面临挑战，但在锂离子电池电极制造中具有显著潜力，值得进一步探索。"}}
{"id": "2506.16310", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.16310", "abs": "https://arxiv.org/abs/2506.16310", "authors": ["Pranav Pawar", "Akshansh Dwivedi", "Jenish Boricha", "Himanshu Gohil", "Aditya Dubey"], "title": "Optimizing Multilingual Text-To-Speech with Accents & Emotions", "comment": "12 pages, 8 figures", "summary": "State-of-the-art text-to-speech (TTS) systems realize high naturalness in\nmonolingual environments, synthesizing speech with correct multilingual accents\n(especially for Indic languages) and context-relevant emotions still poses\ndifficulty owing to cultural nuance discrepancies in current frameworks. This\npaper introduces a new TTS architecture integrating accent along with\npreserving transliteration with multi-scale emotion modelling, in particularly\ntuned for Hindi and Indian English accent. Our approach extends the Parler-TTS\nmodel by integrating A language-specific phoneme alignment hybrid\nencoder-decoder architecture, and culture-sensitive emotion embedding layers\ntrained on native speaker corpora, as well as incorporating a dynamic accent\ncode switching with residual vector quantization. Quantitative tests\ndemonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction\nfrom 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native\nlisteners, surpassing METTS and VECL-TTS baselines. The novelty of the system\nis that it can mix code in real time - generating statements such as \"Namaste,\nlet's talk about <Hindi phrase>\" with uninterrupted accent shifts while\npreserving emotional consistency. Subjective evaluation with 200 users reported\na mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than\nexisting multilingual systems (p<0.01). This research makes cross-lingual\nsynthesis more feasible by showcasing scalable accent-emotion disentanglement,\nwith direct application in South Asian EdTech and accessibility software.", "AI": {"tldr": "该论文提出了一种新的TTS架构，结合口音和多尺度情感建模，特别针对印地语和印度英语口音优化，显著提升了口音准确性和情感识别能力。", "motivation": "当前TTS系统在多语言环境下（尤其是印度语言）难以正确合成口音和上下文相关的情感，主要由于文化差异问题。", "method": "扩展Parler-TTS模型，集成语言特定音素对齐的混合编码器-解码器架构、文化敏感的情感嵌入层，以及动态口音代码切换技术。", "result": "口音准确性提升23.7%（WER从15.4%降至11.8%），情感识别准确率达85.3%，用户评价MOS为4.2/5。", "conclusion": "该系统通过可扩展的口音-情感解耦技术，为跨语言合成提供了更可行的解决方案，适用于南亚教育科技和辅助软件。"}}
{"id": "2506.15945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15945", "abs": "https://arxiv.org/abs/2506.15945", "authors": ["Kowndinya Boyalakuntla", "Abdeslam Boularias", "Jingjin Yu"], "title": "KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping", "comment": null, "summary": "We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic\nobject tracking and grasping over eye-on-hand (EoH) systems, significantly\nexpanding such systems capabilities in challenging, realistic environments. In\ncomparison to the previous state-of-the-art, KARL (1) incorporates a novel\nsix-stage RL curriculum that doubles the system's motion range, thereby greatly\nenhancing the system's grasping performance, (2) integrates a robust Kalman\nfilter layer between the perception and reinforcement learning (RL) control\nmodules, enabling the system to maintain an uncertain but continuous 6D pose\nestimate even when the target object temporarily exits the camera's\nfield-of-view or undergoes rapid, unpredictable motion, and (3) introduces\nmechanisms to allow retries to gracefully recover from unavoidable policy\nexecution failures. Extensive evaluations conducted in both simulation and\nreal-world experiments qualitatively and quantitatively corroborate KARL's\nadvantage over earlier systems, achieving higher grasp success rates and faster\nrobot execution speed. Source code and supplementary materials for KARL will be\nmade available at: https://github.com/arc-l/karl.", "AI": {"tldr": "KARL是一种结合卡尔曼滤波与强化学习的动态物体跟踪与抓取方法，显著提升了眼手系统的性能。", "motivation": "解决动态物体跟踪与抓取在复杂环境中的挑战，尤其是在目标物体短暂离开视野或快速移动时。", "method": "采用六阶段强化学习课程、卡尔曼滤波层和失败恢复机制。", "result": "在仿真和实际实验中，KARL表现出更高的抓取成功率和更快的执行速度。", "conclusion": "KARL通过结合滤波与强化学习，显著提升了动态物体抓取的性能。"}}
{"id": "2506.16473", "categories": ["cs.HC", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.16473", "abs": "https://arxiv.org/abs/2506.16473", "authors": ["Sophie Chiang", "Guy Laban", "Hatice Gunes"], "title": "Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support", "comment": null, "summary": "As conversational agents increasingly engage in emotionally supportive\ndialogue, it is important to understand how closely their interactions resemble\nthose in traditional therapy settings. This study investigates whether the\nconcerns shared with a robot align with those shared in human-to-human (H2H)\ntherapy sessions, and whether robot responses semantically mirror those of\nhuman therapists. We analyzed two datasets: one of interactions between users\nand professional therapists (Hugging Face's NLP Mental Health Conversations),\nand another involving supportive conversations with a social robot (QTrobot\nfrom LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence\nembeddings and K-means clustering, we assessed cross-agent thematic alignment\nby applying a distance-based cluster-fitting method that evaluates whether\nresponses from one agent type map to clusters derived from the other, and\nvalidated it using Euclidean distances. Results showed that 90.88% of robot\nconversation disclosures could be mapped to clusters from the human therapy\ndataset, suggesting shared topical structure. For matched clusters, we compared\nthe subjects as well as therapist and robot responses using Transformer,\nWord2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'\ndisclosures in both datasets, as well as in the responses given to similar\nhuman disclosure themes across agent types (robot vs. human therapist). These\nfindings highlight both the parallels and boundaries of robot-led support\nconversations and their potential for augmenting mental health interventions.", "AI": {"tldr": "研究比较了机器人对话与传统心理治疗对话的相似性，发现90.88%的机器人对话主题可映射到人类治疗对话的聚类中，且语义高度重叠。", "motivation": "随着对话机器人在情感支持中的应用增多，需评估其与人类治疗对话的相似性。", "method": "分析两个数据集（人类治疗对话与机器人对话），使用句子嵌入和K-means聚类评估主题对齐，并通过距离方法验证。", "result": "机器人对话主题与人类治疗对话高度匹配，语义分析显示响应也相似。", "conclusion": "机器人支持对话与传统治疗有显著相似性，可能辅助心理健康干预。"}}
{"id": "2506.15940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15940", "abs": "https://arxiv.org/abs/2506.15940", "authors": ["Zhongchen Zhao", "Chaodong Xiao", "Hui Lin", "Qi Xie", "Lei Zhang", "Deyu Meng"], "title": "Polyline Path Masked Attention for Vision Transformer", "comment": null, "summary": "Global dependency modeling and spatial position modeling are two core issues\nof the foundational architecture design in current deep learning frameworks.\nRecently, Vision Transformers (ViTs) have achieved remarkable success in\ncomputer vision, leveraging the powerful global dependency modeling capability\nof the self-attention mechanism. Furthermore, Mamba2 has demonstrated its\nsignificant potential in natural language processing tasks by explicitly\nmodeling the spatial adjacency prior through the structured mask. In this\npaper, we propose Polyline Path Masked Attention (PPMA) that integrates the\nself-attention mechanism of ViTs with an enhanced structured mask of Mamba2,\nharnessing the complementary strengths of both architectures. Specifically, we\nfirst ameliorate the traditional structured mask of Mamba2 by introducing a 2D\npolyline path scanning strategy and derive its corresponding structured mask,\npolyline path mask, which better preserves the adjacency relationships among\nimage tokens. Notably, we conduct a thorough theoretical analysis on the\nstructural characteristics of the proposed polyline path mask and design an\nefficient algorithm for the computation of the polyline path mask. Next, we\nembed the polyline path mask into the self-attention mechanism of ViTs,\nenabling explicit modeling of spatial adjacency prior. Extensive experiments on\nstandard benchmarks, including image classification, object detection, and\nsegmentation, demonstrate that our model outperforms previous state-of-the-art\napproaches based on both state-space models and Transformers. For example, our\nproposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K\nsemantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,\nrespectively. Code is available at https://github.com/zhongchenzhao/PPMA.", "AI": {"tldr": "提出了一种结合ViTs自注意力机制和Mamba2结构化掩码的PPMA方法，通过2D折线路径扫描策略改进掩码，并在多个视觉任务中表现优异。", "motivation": "结合ViTs的全局依赖建模能力和Mamba2的空间邻接先验建模能力，提升视觉任务的性能。", "method": "改进Mamba2的结构化掩码为2D折线路径掩码，并嵌入ViTs的自注意力机制中。", "result": "在图像分类、目标检测和分割任务中超越现有方法，如ADE20K上PPMA-T/S/B模型mIoU提升0.7%/1.3%/0.3%。", "conclusion": "PPMA有效结合了两种架构的优势，显著提升了视觉任务的性能。"}}
{"id": "2506.15697", "categories": ["cs.AR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15697", "abs": "https://arxiv.org/abs/2506.15697", "authors": ["Yi Liu", "Hongji Zhang", "Yunhao Zhou", "Zhengyuan Shi", "Changran Xu", "Qiang Xu"], "title": "DeepRTL2: A Versatile Model for RTL-Related Tasks", "comment": "ACL 2025 Findings", "summary": "The integration of large language models (LLMs) into electronic design\nautomation (EDA) has significantly advanced the field, offering transformative\nbenefits, particularly in register transfer level (RTL) code generation and\nunderstanding. While previous studies have demonstrated the efficacy of\nfine-tuning LLMs for these generation-based tasks, embedding-based tasks, which\nare equally critical to EDA workflows, have been largely overlooked. These\ntasks, including natural language code search, RTL code functionality\nequivalence checking, and performance prediction, are essential for\naccelerating and optimizing the hardware design process. To address this gap,\nwe present DeepRTL2, a family of versatile LLMs that unifies both generation-\nand embedding-based tasks related to RTL. By simultaneously tackling a broad\nrange of tasks, DeepRTL2 represents the first model to provide a comprehensive\nsolution to the diverse challenges in EDA. Through extensive experiments, we\nshow that DeepRTL2 achieves state-of-the-art performance across all evaluated\ntasks.", "AI": {"tldr": "DeepRTL2是一个多功能LLM家族，首次统一了EDA中的生成和嵌入任务，实现了最先进的性能。", "motivation": "现有研究主要关注LLM在EDA中的生成任务，而忽略了同样关键的嵌入任务，如代码搜索和功能等价检查。", "method": "提出了DeepRTL2模型，同时处理生成和嵌入任务，覆盖RTL相关的多样化需求。", "result": "实验表明，DeepRTL2在所有评估任务中均达到最先进性能。", "conclusion": "DeepRTL2为EDA提供了全面的解决方案，填补了嵌入任务的空白。"}}
{"id": "2506.17059", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17059", "abs": "https://arxiv.org/abs/2506.17059", "authors": ["Martin Cornejo", "Melina Graner", "Holger Hesse", "Andreas Jossen"], "title": "Evaluating the Impact of Model Accuracy for Optimizing Battery Energy Storage Systems", "comment": "5 pages, 4 figures. Submitted to IEEE ISGT Europe 2025", "summary": "This study investigates two models of varying complexity for optimizing\nintraday arbitrage energy trading of a battery energy storage system using a\nmodel predictive control approach. Scenarios reflecting different stages of the\nsystem's lifetime are analyzed. The findings demonstrate that the\nequivalent-circuit-model-based non-linear optimization model outperforms the\nsimpler linear model by delivering more accurate predictions of energy losses\nand system capabilities. This enhanced accuracy enables improved operational\nstrategies, resulting in increased roundtrip efficiency and revenue,\nparticularly in systems with batteries exhibiting high internal resistance,\nsuch as second-life batteries. However, to fully leverage the model's benefits,\nit is essential to identify the correct parameters.", "AI": {"tldr": "研究比较了两种不同复杂度的模型，用于优化电池储能系统的日内套利能源交易，发现非线性模型优于线性模型，但需正确参数化。", "motivation": "探索不同复杂度模型对电池储能系统日内套利能源交易的优化效果，特别是在系统寿命不同阶段的表现。", "method": "采用模型预测控制方法，分析等效电路模型（非线性）与线性模型的表现。", "result": "非线性模型能更准确预测能量损失和系统能力，提升运行策略，增加效率和收益，尤其适用于高内阻电池。", "conclusion": "非线性模型表现更优，但需正确参数化才能充分发挥其优势。"}}
{"id": "2506.15953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15953", "abs": "https://arxiv.org/abs/2506.15953", "authors": ["Liang Heng", "Haoran Geng", "Kaifeng Zhang", "Pieter Abbeel", "Jitendra Malik"], "title": "ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation", "comment": null, "summary": "Dexterous manipulation is a cornerstone capability for robotic systems aiming\nto interact with the physical world in a human-like manner. Although\nvision-based methods have advanced rapidly, tactile sensing remains crucial for\nfine-grained control, particularly in unstructured or visually occluded\nsettings. We present ViTacFormer, a representation-learning approach that\ncouples a cross-attention encoder to fuse high-resolution vision and touch with\nan autoregressive tactile prediction head that anticipates future contact\nsignals. Building on this architecture, we devise an easy-to-challenging\ncurriculum that steadily refines the visual-tactile latent space, boosting both\naccuracy and robustness. The learned cross-modal representation drives\nimitation learning for multi-fingered hands, enabling precise and adaptive\nmanipulation. Across a suite of challenging real-world benchmarks, our method\nachieves approximately 50% higher success rates than prior state-of-the-art\nsystems. To our knowledge, it is also the first to autonomously complete\nlong-horizon dexterous manipulation tasks that demand highly precise control\nwith an anthropomorphic hand, successfully executing up to 11 sequential stages\nand sustaining continuous operation for 2.5 minutes.", "AI": {"tldr": "ViTacFormer是一种结合视觉与触觉的表示学习方法，通过跨注意力编码器和自回归触觉预测头提升机器人灵巧操作的精度与鲁棒性。", "motivation": "灵巧操作是机器人系统实现类人交互的核心能力，而触觉感知在非结构化或视觉遮挡环境中对精细控制至关重要。", "method": "提出ViTacFormer架构，结合跨注意力编码器融合视觉与触觉信息，并设计由易到难的课程学习策略优化表示空间。", "result": "在真实世界基准测试中，成功率比现有技术高50%，首次实现长时程灵巧操作任务（11个连续阶段，持续2.5分钟）。", "conclusion": "ViTacFormer通过视觉-触觉融合与课程学习，显著提升了机器人灵巧操作的性能与适应性。"}}
{"id": "2506.16542", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16542", "abs": "https://arxiv.org/abs/2506.16542", "authors": ["Nathalia Gomez", "S. Sue Batham", "Mathias Volonte", "Tiffany D. Do"], "title": "Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence", "comment": "6 pages, To Appear in Companion Publication of the 2025 Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW Companion '25)", "summary": "Technical interviews are a critical yet stressful step in the hiring process\nfor computer science graduates, often hindered by limited access to practice\nopportunities. This formative qualitative study (n=20) explores whether a\nmultimodal AI system can realistically simulate technical interviews and\nsupport confidence-building among candidates. Participants engaged with an\nAI-driven mock interview tool featuring whiteboarding tasks and real-time\nfeedback. Many described the experience as realistic and helpful, noting\nincreased confidence and improved articulation of problem-solving decisions.\nHowever, challenges with conversational flow and timing were noted. These\nfindings demonstrate the potential of AI-driven technical interviews as\nscalable and realistic preparation tools, suggesting that future research could\nexplore variations in interviewer behavior and their potential effects on\ncandidate preparation.", "AI": {"tldr": "研究探讨了多模态AI系统是否能模拟技术面试并提升候选人信心，结果显示AI工具在模拟面试中有效，但仍存在对话流畅性和时间管理问题。", "motivation": "技术面试对计算机科学毕业生至关重要，但缺乏练习机会，研究旨在探索AI是否能提供逼真的模拟面试体验。", "method": "采用定性研究方法（n=20），参与者使用AI驱动的模拟面试工具，完成白板任务并接收实时反馈。", "result": "多数参与者认为AI模拟面试逼真且有用，提升了信心和问题解决表达能力，但对话流畅性和时间管理仍需改进。", "conclusion": "AI驱动的技术面试具有潜力作为可扩展的准备工具，未来可研究不同面试行为对候选人的影响。"}}
{"id": "2506.15971", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15971", "abs": "https://arxiv.org/abs/2506.15971", "authors": ["Jiawen Yang", "Shuhao Chen", "Yucong Duan", "Ke Tang", "Yu Zhang"], "title": "Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging", "comment": null, "summary": "Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps\nbut become struggled when the source and target domains belong to entirely\ndistinct modalities. To address this limitation, we propose a novel setting\ncalled Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which\nenables knowledge transfer between completely different modalities by\nleveraging a bridge domain containing unlabeled samples from both modalities.\nTo learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a\nspecialized framework designed for the semantic segmentation task.\nSpecifically, LSB utilizes a dual-branch architecture, incorporating a feature\nconsistency loss to align representations across modalities and a domain\nalignment loss to reduce discrepancies between class centroids across domains.\nExtensive experiments conducted on six benchmark datasets demonstrate that LSB\nachieves state-of-the-art performance.", "AI": {"tldr": "提出了一种名为HMUDA的新设置，通过桥接域实现不同模态间的无监督域适应，并设计了LSB框架用于语义分割任务。", "motivation": "解决源域和目标域属于完全不同模态时无监督域适应方法的局限性。", "method": "提出LSB框架，采用双分支架构，结合特征一致性损失和域对齐损失。", "result": "在六个基准数据集上实现了最先进的性能。", "conclusion": "LSB框架有效解决了跨模态无监督域适应问题。"}}
{"id": "2506.15698", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15698", "abs": "https://arxiv.org/abs/2506.15698", "authors": ["Yunhak Oh", "Junseok Lee", "Yeongmin Kim", "Sangwoo Seo", "Namkyeong Lee", "Chanyoung Park"], "title": "Global Context-aware Representation Learning for Spatially Resolved Transcriptomics", "comment": "ICML 2025", "summary": "Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that\ncaptures the spatial context of cells within tissues, enabling the study of\ncomplex biological networks. Recent graph-based methods leverage both gene\nexpression and spatial information to identify relevant spatial domains.\nHowever, these approaches fall short in obtaining meaningful spot\nrepresentations, especially for spots near spatial domain boundaries, as they\nheavily emphasize adjacent spots that have minimal feature differences from an\nanchor node. To address this, we propose Spotscape, a novel framework that\nintroduces the Similarity Telescope module to capture global relationships\nbetween multiple spots. Additionally, we propose a similarity scaling strategy\nto regulate the distances between intra- and inter-slice spots, facilitating\neffective multi-slice integration. Extensive experiments demonstrate the\nsuperiority of Spotscape in various downstream tasks, including single-slice\nand multi-slice scenarios. Our code is available at the following link: https:\n//github.com/yunhak0/Spotscape.", "AI": {"tldr": "Spotscape是一个新的框架，通过引入相似性望远镜模块和相似性缩放策略，改进了空间转录组学中基于图的方法，提升了边界区域的识别能力。", "motivation": "现有的基于图的方法在空间域边界附近的点识别能力不足，因为它们过于依赖相邻点的相似性。", "method": "提出Spotscape框架，包括相似性望远镜模块和相似性缩放策略，以捕捉全局关系并优化多切片整合。", "result": "实验表明，Spotscape在单切片和多切片场景下的下游任务中表现优异。", "conclusion": "Spotscape通过全局关系捕捉和距离调节，显著提升了空间转录组学分析的准确性。"}}
{"id": "2506.17060", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17060", "abs": "https://arxiv.org/abs/2506.17060", "authors": ["Orcun Karaca", "Ioannis Tsoumas", "Mario Schweizer", "Ognjen Stanojev", "Lennart Harnefors"], "title": "Robust black start of an offshore wind farm with DRU based HVDC link using power synchronization control", "comment": null, "summary": "This paper introduces a universal power synchronization controller for\ngrid-side control of the wind turbine conversion systems in an offshore wind\nfarm with a diode rectifier in the offshore substation of the HVDC link. The\ncontroller incorporates voltage-power droop controllers in the outer loop to\nenable the operation of this setup. To effectively handle the impact of large\ndelays during black start and power ramp phases, virtual active and reactive\npower quantities are defined. These quantities are computed based on the\ncurrent references prior to any modifications that might be needed to meet\nconverter current and voltage limits or source constraints. Utilizing them in\nthe outer loop ensures a balanced power sharing and a stable operation whenever\nthe original (unmodified) current references are not realized. Case studies\nconfirm the robustness of the proposed controller.", "AI": {"tldr": "本文提出了一种用于风电转换系统电网侧控制的通用功率同步控制器，适用于带二极管整流器的海上风电场HVDC链路。控制器通过外环电压-功率下垂控制实现稳定运行，并定义虚拟有功和无功功率以处理大延迟影响。", "motivation": "解决海上风电场HVDC链路中二极管整流器带来的控制挑战，特别是在黑启动和功率爬坡阶段的大延迟问题。", "method": "设计外环电压-功率下垂控制器，定义虚拟有功和无功功率，基于未修改的电流参考值计算，确保功率均衡和稳定运行。", "result": "案例研究验证了控制器的鲁棒性。", "conclusion": "提出的控制器能有效应对大延迟问题，确保风电转换系统的稳定运行。"}}
{"id": "2506.15983", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15983", "abs": "https://arxiv.org/abs/2506.15983", "authors": ["Jianzhu Huai", "Yuxin Shao", "Yujia Zhang", "Alper Yilmaz"], "title": "A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone", "comment": "ISPRS GSW2025 Dubai UAE", "summary": "The rapid advancement of the metaverse, digital twins, and robotics\nunderscores the demand for low-cost, portable mapping systems for reality\ncapture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped\nsmartphones, either come at a high cost or are limited in range and accuracy.\nLeveraging the proliferation and technological evolution of mobile devices\nalongside recent advancements in lidar technology, we introduce a novel,\nlow-cost, portable mobile mapping system. Our system integrates a lidar unit,\nan Android smartphone, and an RTK-GNSS stick. Running on the Android platform,\nit features lidar-inertial odometry built with the NDK, and logs data from the\nlidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)\ncost under 2,000 USD and a weight of about 1 kilogram, the system achieves a\ngood balance between affordability and portability. We detail the system\ndesign, multisensor calibration, synchronization, and evaluate its performance\nfor tracking and mapping. To further contribute to the community, the system's\ndesign and software are made open source at:\nhttps://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1", "AI": {"tldr": "提出了一种低成本、便携的移动测绘系统，结合激光雷达、智能手机和RTK-GNSS技术，总成本低于2000美元，重量约1公斤。", "motivation": "当前移动测绘系统成本高或性能有限，无法满足元宇宙、数字孪生和机器人技术对低成本便携解决方案的需求。", "method": "系统集成了激光雷达、Android智能手机和RTK-GNSS模块，通过NDK构建激光雷达惯性里程计，并记录多传感器数据。", "result": "系统在2000美元预算内实现了良好的跟踪和测绘性能，设计及软件已开源。", "conclusion": "该系统为低成本便携测绘提供了可行方案，开源设计有助于社区进一步发展。"}}
{"id": "2506.16571", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16571", "abs": "https://arxiv.org/abs/2506.16571", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Capturing Visualization Design Rationale", "comment": null, "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.", "AI": {"tldr": "本文提出了一种新的数据集和方法，通过自然语言探究可视化设计的原理，利用学生创建的可视化笔记本和大型语言模型（LLMs）生成问题-答案-原理三元组。", "motivation": "现有自然语言数据集多关注可视化解读而非设计原理，且依赖人工构建的环境。本文旨在填补这一空白，通过真实世界的可视化笔记本探究设计决策背后的原理。", "method": "利用学生创建的可视化笔记本作为数据源，结合LLMs生成问题-答案-原理三元组，并通过验证和筛选构建数据集。", "result": "构建了一个捕捉学生可视化设计选择及其原理的数据集，为研究设计决策提供了新资源。", "conclusion": "该方法为理解可视化设计原理提供了新视角，数据集可用于进一步研究设计决策的自然语言表达。"}}
{"id": "2506.15976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15976", "abs": "https://arxiv.org/abs/2506.15976", "authors": ["Jingwei Zhang", "Xi Han", "Hong Qin", "Mahdi S. Hosseini", "Dimitris Samaras"], "title": "LBMamba: Locally Bi-directional Mamba", "comment": "Submitted to TMLR", "summary": "Mamba, a State Space Model (SSM) that accelerates training by recasting\nrecurrence as a parallel selective scan, has recently emerged as a\nlinearly-scaling, efficient alternative to self-attention. Because of its\nunidirectional nature, each state in Mamba only has information of its previous\nstates and is blind to states after. Current Mamba-based computer-vision\nmethods typically overcome this limitation by augmenting Mamba's global forward\nscan with a global backward scan, forming a bi-directional scan that restores a\nfull receptive field. However, this operation doubles the computational load,\neroding much of the efficiency advantage that originally Mamba have. To\neliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM\nblock that embeds a lightweight locally backward scan inside the forward\nselective scan and executes it entirely in per-thread registers. Building on\nLBMamba, we present LBVim, a scalable vision backbone that alternates scan\ndirections every two layers to recover a global receptive field without extra\nbackward sweeps. We validate the versatility of our approach on both natural\nimages and whole slide images (WSIs). We show that our LBVim constantly offers\na superior performance-throughput trade-off. That is under the same throughput,\nLBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K\nclassification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic\nsegmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection\ndataset. We also integrate LBMamba into the SOTA pathology multiple instance\nlearning (MIL) approach, MambaMIL, which uses single directional scan.\nExperiments on 3 public WSI classification datasets for show that our method\nachieves a relative improvement of up to 3.06% better AUC, 3.39% better F1,\n1.67% better accuracy.", "AI": {"tldr": "LBMamba是一种局部双向状态空间模型，通过在正向扫描中嵌入轻量级反向扫描，避免了传统双向扫描的计算开销，提升了效率。基于LBMamba的LBVim在多个视觉任务中表现出色。", "motivation": "Mamba模型因其单向性限制了全局信息获取，传统双向扫描方法虽能解决但计算成本高。LBMamba旨在消除额外扫描，保持高效性。", "method": "提出LBMamba块，在正向选择性扫描中嵌入局部反向扫描，无需额外计算。LBVim通过交替扫描方向恢复全局感受野。", "result": "LBVim在ImageNet、ADE20K和COCO等数据集上性能优于基线，病理图像分类任务中AUC、F1和准确率也有显著提升。", "conclusion": "LBMamba和LBVim在保持高效的同时显著提升了性能，适用于多种视觉任务。"}}
{"id": "2506.15699", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15699", "abs": "https://arxiv.org/abs/2506.15699", "authors": ["Shengyuan Hu", "Neil Kale", "Pratiksha Thaker", "Yiwei Fu", "Steven Wu", "Virginia Smith"], "title": "BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap", "comment": null, "summary": "Machine unlearning has the potential to improve the safety of large language\nmodels (LLMs) by removing sensitive or harmful information post hoc. A key\nchallenge in unlearning involves balancing between forget quality (effectively\nunlearning undesirable information) and retain quality (maintaining good\nperformance on other, general tasks). Unfortunately, as we show, current LLM\nunlearning benchmarks contain highly disparate forget and retain sets --\npainting a false picture of the effectiveness of LLM unlearning methods. This\ncan be particularly problematic because it opens the door for benign\nperturbations, such as relearning attacks, to easily reveal supposedly\nunlearned knowledge once models are deployed. To address this, we present\n$\\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic\nscenarios of forget-retain overlap. $\\texttt{BLUR}$ significantly expands on\nexisting unlearning benchmarks by providing extended evaluation tasks, combined\nforget/retain queries, and relearning datasets of varying degrees of\ndifficulty. Despite the benign nature of the queries considered, we find that\nthe performance of existing methods drops significantly when evaluated on\n$\\texttt{BLUR}$, with simple approaches performing better on average than more\nrecent methods. These results highlight the importance of robust evaluation and\nsuggest several important directions of future study. Our benchmark is publicly\navailable at: https://huggingface.co/datasets/forgelab/BLUR", "AI": {"tldr": "论文提出了一个名为BLUR的新基准测试，用于更真实地评估大型语言模型（LLM）的遗忘能力，解决了现有基准测试中遗忘与保留数据集差异过大的问题。", "motivation": "现有LLM遗忘基准测试中遗忘与保留数据集差异过大，导致评估结果失真，可能掩盖模型在实际部署中的潜在风险。", "method": "提出BLUR基准测试，通过扩展评估任务、结合遗忘/保留查询以及提供不同难度的重新学习数据集，更真实地模拟遗忘与保留的重叠场景。", "result": "现有遗忘方法在BLUR上的表现显著下降，简单方法平均优于新方法，表明现有评估不够稳健。", "conclusion": "BLUR为LLM遗忘研究提供了更真实的评估工具，并指出了未来研究的重要方向。"}}
{"id": "2506.17112", "categories": ["eess.SY", "cs.ET", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17112", "abs": "https://arxiv.org/abs/2506.17112", "authors": ["Lukas Brand", "Fardad Vakilipoor", "Sören Botsch", "Timo Jakumeit", "Sebastian Lotter", "Robert Schober", "Maximilian Schäfer"], "title": "Closed-Loop Molecular Communication with Local and Global Degradation: Modeling and ISI Analysis", "comment": null, "summary": "This paper presents a novel physics-based model for signal propagation in\nclosed-loop molecular communication (MC) systems, which are particularly\nrelevant for many envisioned biomedical applications, such as health monitoring\nor drug delivery within the closed-loop human cardiovascular system (CVS).\nCompared to open-loop systems, which are mostly considered in MC, closed-loop\nsystems exhibit different characteristic effects influencing signaling molecule\n(SM) propagation. One key phenomenon are the periodic SM arrivals at the\nreceiver (RX), leading to various types of inter-symbol interference (ISI)\ninherent to closed-loop system. To capture these characteristic effects, we\npropose an analytical model for the SM propagation inside closed-loop systems.\nThe model accounts for arbitrary spatio-temporal SM release patterns at the\ntransmitter (TX), and incorporates several environmental effects such as fluid\nflow, SM diffusion, and SM degradation. Moreover, to capture a wide range of\npractically relevant degradation and clearance mechanisms, the model includes\nboth local removal (e.g., due to SM absorption into organs) and global removal\n(e.g., due to chemical degradation) of SMs. The accuracy of the proposed model\nis validated with three-dimensional (3-D) particle-based simulations (PBSs).\nMoreover, we utilize the proposed model to develop a rigorous characterization\nof the various types of ISI encountered in closed-loop MC systems.", "AI": {"tldr": "本文提出了一种基于物理的闭环分子通信信号传播模型，适用于生物医学应用，如健康监测或药物输送。", "motivation": "闭环分子通信系统在生物医学应用中具有潜力，但现有研究多关注开环系统，闭环系统的特性（如周期性信号分子到达）尚未充分研究。", "method": "提出了一种分析模型，考虑了信号分子的时空释放模式、流体流动、扩散和降解等环境效应，并验证了模型的准确性。", "result": "模型通过3D粒子模拟验证，并用于严格表征闭环系统中的符号间干扰类型。", "conclusion": "该模型为闭环分子通信系统提供了理论基础，有助于未来生物医学应用的设计与优化。"}}
{"id": "2506.16012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16012", "abs": "https://arxiv.org/abs/2506.16012", "authors": ["Boyu Li", "Siyuan He", "Hang Xu", "Haoqi Yuan", "Yu Zang", "Liwei Hu", "Junpeng Yue", "Zhenxiong Jiang", "Pengbo Hu", "Börje F. Karlsson", "Yehui Tang", "Zongqing Lu"], "title": "DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning", "comment": null, "summary": "Developing embodied agents capable of performing complex interactive tasks in\nreal-world scenarios remains a fundamental challenge in embodied AI. Although\nrecent advances in simulation platforms have greatly enhanced task diversity to\ntrain embodied Vision Language Models (VLMs), most platforms rely on simplified\nrobot morphologies and bypass the stochastic nature of low-level execution,\nwhich limits their transferability to real-world robots. To address these\nissues, we present a physics-based simulation platform DualTHOR for complex\ndual-arm humanoid robots, built upon an extended version of AI2-THOR. Our\nsimulator includes real-world robot assets, a task suite for dual-arm\ncollaboration, and inverse kinematics solvers for humanoid robots. We also\nintroduce a contingency mechanism that incorporates potential failures through\nphysics-based low-level execution, bridging the gap to real-world scenarios.\nOur simulator enables a more comprehensive evaluation of the robustness and\ngeneralization of VLMs in household environments. Extensive evaluations reveal\nthat current VLMs struggle with dual-arm coordination and exhibit limited\nrobustness in realistic environments with contingencies, highlighting the\nimportance of using our simulator to develop more capable VLMs for embodied\ntasks. The code is available at https://github.com/ds199895/DualTHOR.git.", "AI": {"tldr": "DualTHOR是一个基于物理的仿真平台，专注于双臂人形机器人，旨在解决现有仿真平台在真实世界机器人任务中的局限性。", "motivation": "现有仿真平台简化了机器人形态并忽略了低级执行的随机性，限制了其在真实世界中的可转移性。", "method": "扩展AI2-THOR，引入真实机器人资产、双臂协作任务套件、逆运动学求解器和基于物理的故障机制。", "result": "当前视觉语言模型在双臂协调和真实环境中的鲁棒性表现不佳，凸显了DualTHOR的重要性。", "conclusion": "DualTHOR为开发更强大的视觉语言模型提供了更全面的仿真环境，有助于提升其在真实世界任务中的表现。"}}
{"id": "2506.16677", "categories": ["cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16677", "abs": "https://arxiv.org/abs/2506.16677", "authors": ["Hao Guo", "Wei Fan", "Shaohui Liu", "Feng Jiang", "Chunzhi Yi"], "title": "PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration", "comment": null, "summary": "Trust prediction is a key issue in human-robot collaboration, especially in\nconstruction scenarios where maintaining appropriate trust calibration is\ncritical for safety and efficiency. This paper introduces the\nPerformance-guided Physiological signal-based Trust Prediction (PPTP), a novel\nframework designed to improve trust assessment. We designed a human-robot\nconstruction scenario with three difficulty levels to induce different trust\nstates. Our approach integrates synchronized multimodal physiological signals\n(ECG, GSR, and EMG) with collaboration performance evaluation to predict human\ntrust levels. Individual physiological signals are processed using\ncollaboration performance information as guiding cues, leveraging the\nstandardized nature of collaboration performance to compensate for individual\nvariations in physiological responses. Extensive experiments demonstrate the\nefficacy of our cross-modality fusion method in significantly improving trust\nclassification performance. Our model achieves over 81% accuracy in three-level\ntrust classification, outperforming the best baseline method by 6.7%, and\nnotably reaches 74.3% accuracy in high-resolution seven-level classification,\nwhich is a first in trust prediction research. Ablation experiments further\nvalidate the superiority of physiological signal processing guided by\ncollaboration performance assessment.", "AI": {"tldr": "PPTP框架通过结合生理信号和协作表现，显著提升了人机协作中的信任预测准确率。", "motivation": "在建筑场景中，信任预测对安全和效率至关重要，但现有方法难以准确评估信任水平。", "method": "设计多难度人机协作场景，融合ECG、GSR和EMG等生理信号与协作表现评估，进行信任预测。", "result": "模型在三级别信任分类中准确率达81%，七级别分类中达74.3%，优于基线方法。", "conclusion": "协作表现引导的生理信号处理显著提升了信任预测性能，为研究提供了新方向。"}}
{"id": "2506.15977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15977", "abs": "https://arxiv.org/abs/2506.15977", "authors": ["Sungrae Hong", "Hyeongmin Park", "Youngsin Ko", "Sol Lee", "Bryan Wong", "Mun Yong Yi"], "title": "Towards Classifying Histopathological Microscope Images as Time Series Data", "comment": "5 pages, 4 figures, Accepted by International Symposium on Biomedical\n  Imaging (ISBI) 2025", "summary": "As the frontline data for cancer diagnosis, microscopic pathology images are\nfundamental for providing patients with rapid and accurate treatment. However,\ndespite their practical value, the deep learning community has largely\noverlooked their usage. This paper proposes a novel approach to classifying\nmicroscopy images as time series data, addressing the unique challenges posed\nby their manual acquisition and weakly labeled nature. The proposed method fits\nimage sequences of varying lengths to a fixed-length target by leveraging\nDynamic Time-series Warping (DTW). Attention-based pooling is employed to\npredict the class of the case simultaneously. We demonstrate the effectiveness\nof our approach by comparing performance with various baselines and showcasing\nthe benefits of using various inference strategies in achieving stable and\nreliable results. Ablation studies further validate the contribution of each\ncomponent. Our approach contributes to medical image analysis by not only\nembracing microscopic images but also lifting them to a trustworthy level of\nperformance.", "AI": {"tldr": "本文提出了一种将显微镜图像作为时间序列数据分类的新方法，解决了手动获取和弱标签带来的挑战，并通过动态时间规整和注意力池化实现了稳定可靠的性能。", "motivation": "显微镜病理图像是癌症诊断的前线数据，但其在深度学习领域的应用被忽视。本文旨在填补这一空白，提升其分类性能。", "method": "利用动态时间规整（DTW）将不同长度的图像序列拟合为固定长度目标，并结合注意力池化同时预测病例类别。", "result": "通过与多种基线比较和消融实验，验证了方法的有效性，展示了稳定可靠的分类性能。", "conclusion": "该方法不仅将显微镜图像纳入医学图像分析，还将其性能提升至可信水平。"}}
{"id": "2506.15700", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15700", "abs": "https://arxiv.org/abs/2506.15700", "authors": ["Minjae Cho", "Hiroyasu Tsukamoto", "Huy Trong Tran"], "title": "Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking", "comment": null, "summary": "Control contraction metrics (CCMs) provide a framework to co-synthesize a\ncontroller and a corresponding contraction metric -- a positive-definite\nRiemannian metric under which a closed-loop system is guaranteed to be\nincrementally exponentially stable. However, the synthesized controller only\nensures that all the trajectories of the system converge to one single\ntrajectory and, as such, does not impose any notion of optimality across an\nentire trajectory. Furthermore, constructing CCMs requires a known dynamics\nmodel and non-trivial effort in solving an infinite-dimensional convex\nfeasibility problem, which limits its scalability to complex systems featuring\nhigh dimensionality with uncertainty. To address these issues, we propose to\nintegrate CCMs into reinforcement learning (RL), where CCMs provide\ndynamics-informed feedback for learning control policies that minimize\ncumulative tracking error under unknown dynamics. We show that our algorithm,\ncalled contraction actor-critic (CAC), formally enhances the capability of CCMs\nto provide a set of contracting policies with the long-term optimality of RL in\na fully automated setting. Given a pre-trained dynamics model, CAC\nsimultaneously learns a contraction metric generator (CMG) -- which generates a\ncontraction metric -- and uses an actor-critic algorithm to learn an optimal\ntracking policy guided by that metric. We demonstrate the effectiveness of our\nalgorithm relative to established baselines through extensive empirical\nstudies, including simulated and real-world robot experiments, and provide a\ntheoretical rationale for incorporating contraction theory into RL.", "AI": {"tldr": "论文提出了一种结合控制收缩度量（CCM）和强化学习（RL）的方法，称为收缩演员-评论家（CAC），以解决CCM在动态模型已知和长期最优性方面的局限性。", "motivation": "CCM虽然能保证闭环系统的增量指数稳定性，但缺乏对轨迹最优性的考虑，且依赖于已知动态模型和复杂的无限维凸可行性问题。", "method": "通过将CCM与RL结合，CAC算法学习收缩度量生成器（CMG）和最优跟踪策略，利用CCM提供动态信息反馈。", "result": "实验表明，CAC在模拟和真实机器人任务中优于基线方法，并提供了将收缩理论融入RL的理论依据。", "conclusion": "CAC成功扩展了CCM的能力，实现了在未知动态下的长期最优控制。"}}
{"id": "2506.17146", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2506.17146", "abs": "https://arxiv.org/abs/2506.17146", "authors": ["Collin R. Johnson", "Kerstin Wohlgemuth", "Sergio Lucia"], "title": "A tutorial overview of model predictive control for continuous crystallization: current possibilities and future perspectives", "comment": null, "summary": "This paper presents a systematic approach to the advanced control of\ncontinuous crystallization processes using model predictive control. We provide\na tutorial introduction to controlling complex particle size distributions by\nintegrating population balance equations with detailed models of various\ncontinuous crystallizers. Since these high-fidelity models are often too\ncomplex for online optimization, we propose the use of data-driven surrogate\nmodels that enable efficient optimization-based control. Through two case\nstudies, one with a low-complexity system allowing direct comparison with\ntraditional methods and another involving a spatially distributed crystallizer,\nwe demonstrate how our approach enables real-time model predictive control\nwhile maintaining accuracy. The presented methodology facilitates the use of\ncomplex models in a model-based control framework, allowing precise control of\nkey particle size distribution characteristics, such as the median particle\nsize $d_{50}$ and the width $d_{90} - d_{10}$. This addresses a critical\nchallenge in pharmaceutical and fine chemical manufacturing, where product\nquality depends on tight control of particle characteristics.", "AI": {"tldr": "本文提出了一种基于模型预测控制的连续结晶过程高级控制方法，通过数据驱动的替代模型实现高效优化控制，并在两个案例中验证了其准确性和实时性。", "motivation": "解决制药和精细化工中因颗粒特性控制不足导致的产品质量问题。", "method": "整合种群平衡方程与连续结晶器详细模型，并使用数据驱动的替代模型进行在线优化控制。", "result": "在两个案例中验证了方法的有效性，实现了对颗粒尺寸分布关键特性的精确控制。", "conclusion": "该方法为复杂模型在基于模型的控制框架中的应用提供了可行方案，解决了颗粒特性控制的挑战。"}}
{"id": "2506.16050", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16050", "abs": "https://arxiv.org/abs/2506.16050", "authors": ["Jiawen Yu", "Jieji Ren", "Yang Chang", "Qiaojun Yu", "Xuan Tong", "Boyang Wang", "Yan Song", "You Li", "Xinji Mai", "Wenqiang Zhang"], "title": "Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments", "comment": "IROS 2025 Oral", "summary": "Anomaly detection and localization in automated industrial manufacturing can\nsignificantly enhance production efficiency and product quality. Existing\nmethods are capable of detecting surface defects in pre-defined or controlled\nimaging environments. However, accurately detecting workpiece defects in\ncomplex and unstructured industrial environments with varying views, poses and\nillumination remains challenging. We propose a novel anomaly detection and\nlocalization method specifically designed to handle inputs with perturbative\npatterns. Our approach introduces a new framework based on a collaborative\ndistillation heterogeneous teacher network (HetNet), an adaptive local-global\nfeature fusion module, and a local multivariate Gaussian noise generation\nmodule. HetNet can learn to model the complex feature distribution of normal\npatterns using limited information about local disruptive changes. We conducted\nextensive experiments on mainstream benchmarks. HetNet demonstrates superior\nperformance with approximately 10% improvement across all evaluation metrics on\nMSC-AD under industrial conditions, while achieving state-of-the-art results on\nother datasets, validating its resilience to environmental fluctuations and its\ncapability to enhance the reliability of industrial anomaly detection systems\nacross diverse scenarios. Tests in real-world environments further confirm that\nHetNet can be effectively integrated into production lines to achieve robust\nand real-time anomaly detection. Codes, images and videos are published on the\nproject website at: https://zihuatanejoyu.github.io/HetNet/", "AI": {"tldr": "提出了一种基于协作蒸馏异构教师网络（HetNet）的新方法，用于复杂工业环境中的异常检测与定位，性能提升显著。", "motivation": "现有方法在复杂、非结构化的工业环境中难以准确检测工件缺陷，亟需一种能适应多变视角、姿态和光照的解决方案。", "method": "采用HetNet框架，结合自适应局部-全局特征融合模块和局部多元高斯噪声生成模块，学习正常模式的复杂特征分布。", "result": "在主流基准测试中，HetNet性能提升约10%，并在其他数据集上达到最优，验证了其对环境波动的鲁棒性。", "conclusion": "HetNet能有效集成到生产线中，实现实时、鲁棒的异常检测，提升工业检测系统的可靠性。"}}
{"id": "2506.16716", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16716", "abs": "https://arxiv.org/abs/2506.16716", "authors": ["Qixin Wang", "Songtao Zhou", "Zeyu Jin", "Chenglin Guo", "Shikun Sun", "Xiaoyu Qin"], "title": "V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos", "comment": "Accepted by IJCNN 2025", "summary": "Automatic video commentary systems are widely used on multimedia social media\nplatforms to extract factual information about video content. However, current\nsystems may overlook essential para-linguistic cues, including emotion and\nattitude, which are critical for fully conveying the meaning of visual content.\nThe absence of these cues can limit user understanding or, in some cases,\ndistort the video's original intent. Expressive speech effectively conveys\nthese cues and enhances the user's comprehension of videos. Building on these\ninsights, this paper explores the usage of vision-context-aware expressive\nspeech in enhancing users' understanding of videos in video commentary systems.\nFirstly, our formatting study indicates that semantic-only speech can lead to\nambiguity, and misaligned emotions between speech and visuals may distort\ncontent interpretation. To address this, we propose a method called\nvision-context-aware speech synthesis (V-CASS). It analyzes para-linguistic\ncues from visuals using a vision-language model and leverages a\nknowledge-infused language model to guide the expressive speech model in\ngenerating context-aligned speech. User studies show that V-CASS enhances\nemotional and attitudinal resonance, as well as user audio-visual understanding\nand engagement, with 74.68% of participants preferring the system. Finally, we\nexplore the potential of our method in helping blind and low-vision users\nnavigate web videos, improving universal accessibility.", "AI": {"tldr": "论文提出了一种基于视觉上下文感知的语音合成方法（V-CASS），通过分析视频中的副语言线索生成情感和态度一致的语音，提升用户对视频内容的理解和参与度。", "motivation": "现有视频解说系统常忽略情感和态度等副语言线索，导致用户理解受限或内容意图被扭曲。", "method": "提出V-CASS方法，结合视觉语言模型分析视频中的副语言线索，并利用知识增强的语言模型指导语音合成。", "result": "用户研究表明，V-CASS显著提升了情感共鸣和用户理解，74.68%的参与者更偏好该系统。", "conclusion": "V-CASS不仅提升了视频解说的效果，还为视障用户提供了更好的视频访问体验。"}}
{"id": "2506.15980", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15980", "abs": "https://arxiv.org/abs/2506.15980", "authors": ["Cong Wang", "Zexuan Deng", "Zhiwei Jiang", "Fei Shen", "Yafeng Yin", "Shiwei Gan", "Zifeng Cheng", "Shiping Ge", "Qing Gu"], "title": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization", "comment": null, "summary": "Sign Language Video Generation (SLVG) seeks to generate identity-preserving\nsign language videos from spoken language texts. Existing methods primarily\nrely on the single coarse condition (\\eg, skeleton sequences) as the\nintermediary to bridge the translation model and the video generation model,\nwhich limits both the naturalness and expressiveness of the generated videos.\nTo overcome these limitations, we propose SignViP, a novel SLVG framework that\nincorporates multiple fine-grained conditions for improved generation fidelity.\nRather than directly translating error-prone high-dimensional conditions,\nSignViP adopts a discrete tokenization paradigm to integrate and represent\nfine-grained conditions (\\ie, fine-grained poses and 3D hands). SignViP\ncontains three core components. (1) Sign Video Diffusion Model is jointly\ntrained with a multi-condition encoder to learn continuous embeddings that\nencapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization\n(FSQ) Autoencoder is further trained to compress and quantize these embeddings\ninto discrete tokens for compact representation of the conditions. (3)\nMulti-Condition Token Translator is trained to translate spoken language text\nto discrete multi-condition tokens. During inference, Multi-Condition Token\nTranslator first translates the spoken language text into discrete\nmulti-condition tokens. These tokens are then decoded to continuous embeddings\nby FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion\nModel to guide video generation. Experimental results show that SignViP\nachieves state-of-the-art performance across metrics, including video quality,\ntemporal coherence, and semantic fidelity. The code is available at\nhttps://github.com/umnooob/signvip/.", "AI": {"tldr": "SignViP是一种新的手语视频生成框架，通过多细粒度条件提升生成质量，采用离散标记化方法，结合扩散模型和量化自编码器，实现高性能生成。", "motivation": "现有方法依赖单一粗粒度条件（如骨架序列），限制了生成视频的自然性和表现力。", "method": "SignViP包含三个核心组件：1) 联合训练的视频扩散模型和多条件编码器；2) 量化自编码器压缩嵌入为离散标记；3) 多条件标记翻译器将文本翻译为离散标记。", "result": "实验表明SignViP在视频质量、时间一致性和语义保真度上达到最优性能。", "conclusion": "SignViP通过多细粒度条件和离散标记化方法显著提升了手语视频生成的质量和表现力。"}}
{"id": "2506.15701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15701", "abs": "https://arxiv.org/abs/2506.15701", "authors": ["Haolin Pan", "Hongyu Lin", "Haoran Luo", "Yang Liu", "Kaichun Yao", "Libo Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning", "comment": null, "summary": "Compiler auto-tuning optimizes pass sequences to improve performance metrics\nsuch as Intermediate Representation (IR) instruction count. Although recent\nadvances leveraging Large Language Models (LLMs) have shown promise in\nautomating compiler tuning, two significant challenges still remain: the\nabsence of high-quality reasoning datasets for agents training, and limited\neffective interactions with the compilation environment. In this work, we\nintroduce Compiler-R1, the first reinforcement learning (RL)-driven framework\nspecifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1\nfeatures a curated, high-quality reasoning dataset and a novel two-stage\nend-to-end RL training pipeline, enabling efficient environment exploration and\nlearning through an outcome-based reward. Extensive experiments across seven\ndatasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction\ncount reduction compared to opt -Oz, showcasing the strong potential of\nRL-trained LLMs for compiler optimization. Our code and datasets are publicly\navailable at https://github.com/Panhaolin2001/Compiler-R1.", "AI": {"tldr": "Compiler-R1是一个基于强化学习的框架，通过增强LLM能力优化编译器自动调优，显著减少IR指令数。", "motivation": "解决编译器自动调优中缺乏高质量推理数据集和有限编译环境交互的问题。", "method": "引入两阶段端到端强化学习训练流程，结合高质量推理数据集和结果导向的奖励机制。", "result": "在七个数据集上平均减少8.46%的IR指令数，优于opt -Oz。", "conclusion": "Compiler-R1展示了RL训练的LLM在编译器优化中的巨大潜力。"}}
{"id": "2506.17183", "categories": ["eess.SY", "cs.SY", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2506.17183", "abs": "https://arxiv.org/abs/2506.17183", "authors": ["Junaid Ali", "Gregory Shaver", "Anil Bajaj"], "title": "A Set-valued Impact Law Approach for Modeling and Analysis of Rigid Contact Universal Joint with Clearance", "comment": null, "summary": "This study presents a dynamic model of a universal joint (U-Joint) with\nradial clearance, focusing on the rigid unilateral frictional contacts at the\ncrosspiece and yoke interfaces. Unlike previous models that neglect crosspiece\ninertia and interface friction, this work incorporates these effects using a\nset-valued impact law based on Signorini's condition with Coulomb friction,\ncapturing the complex non-smooth dynamics introduced by radial clearance.\nNumerical simulations of a 2 degrees-of-freedom (DOF) shaft system reveal the\ncritical influence of clearance on U-Joint dynamic behavior, including\nimpact-induced oscillations, quasi-periodic motion, and chaotic dynamics, which\nare essential for accurate driveline modeling and real-time control in\nautomotive, aerospace, and precision medical applications.", "AI": {"tldr": "本文提出了一种考虑径向间隙的万向节动态模型，重点关注十字轴与叉架界面的刚性单边摩擦接触，并引入了十字轴惯性和界面摩擦效应。", "motivation": "以往模型常忽略十字轴惯性和界面摩擦，导致对万向节动态行为的描述不准确。本文旨在填补这一空白，为汽车、航空航天和精密医疗等领域的实时控制和精确建模提供支持。", "method": "采用基于Signorini条件和库仑摩擦的多值冲击定律，捕捉径向间隙引入的非光滑动力学特性，并通过2自由度轴系统的数值模拟验证模型。", "result": "模拟结果显示间隙对万向节动态行为有显著影响，包括冲击诱导振荡、准周期运动和混沌动力学。", "conclusion": "该模型为精确建模和实时控制提供了重要工具，尤其适用于高精度应用领域。"}}
{"id": "2506.16079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16079", "abs": "https://arxiv.org/abs/2506.16079", "authors": ["Prakrut Kotecha", "Aditya Shirwatkar", "Shishir Kolathaya"], "title": "Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion", "comment": "6 pages, 5 figures, Accepted at Advances in Robotics (AIR) Conference\n  2025", "summary": "Lagrangian Neural Networks (LNNs) present a principled and interpretable\nframework for learning the system dynamics by utilizing inductive biases. While\ntraditional dynamics models struggle with compounding errors over long\nhorizons, LNNs intrinsically preserve the physical laws governing any system,\nenabling accurate and stable predictions essential for sustainable locomotion.\nThis work evaluates LNNs for infinite horizon planning in quadrupedal robots\nthrough four dynamics models: (1) full-order forward dynamics (FD) training and\ninference, (2) diagonalized representation of Mass Matrix in full order FD, (3)\nfull-order inverse dynamics (ID) training with FD inference, (4) reduced-order\nmodeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that\nLNNs bring improvements in sample efficiency (10x) and superior prediction\naccuracy (up to 2-10x) compared to baseline methods. Notably, the\ndiagonalization approach of LNNs reduces computational complexity while\nretaining some interpretability, enabling real-time receding horizon control.\nThese findings highlight the advantages of LNNs in capturing the underlying\nstructure of system dynamics in quadrupeds, leading to improved performance and\nefficiency in locomotion planning and control. Additionally, our approach\nachieves a higher control frequency than previous LNN methods, demonstrating\nits potential for real-world deployment on quadrupeds.", "AI": {"tldr": "LNNs利用归纳偏置学习系统动力学，优于传统方法，在四足机器人中实现高效准确预测。", "motivation": "传统动力学模型在长期预测中误差累积，LNNs通过保留物理规律解决这一问题。", "method": "评估四种动力学模型：全阶正向动力学、对角化质量矩阵、全阶逆向动力学与降阶模型。", "result": "LNNs样本效率提升10倍，预测精度提高2-10倍，计算复杂度降低。", "conclusion": "LNNs在四足机器人动力学建模中表现优越，适合实时控制与部署。"}}
{"id": "2506.16851", "categories": ["cs.HC", "H.5.0"], "pdf": "https://arxiv.org/pdf/2506.16851", "abs": "https://arxiv.org/abs/2506.16851", "authors": ["Ankolika De", "Kelley Cotter", "Shaheen Kanthawala", "Haley McAtee", "Amy Ritchart", "Gahana Kadur"], "title": "\"Whoever needs to see it, will see it\": Motivations and Labor of Creating Algorithmic Conspirituality Content on TikTok", "comment": "27 pages, Proc. ACM Hum.-Comput. Interact. 8", "summary": "Recent studies show that users often interpret social media algorithms as\nmystical or spiritual because of their unpredictability. This invites new\nquestions about how such perceptions affect the content that creators create\nand the communities they form online. In this study, 14 creators of algorithmic\nconspirituality content on TikTok were interviewed to explore their\ninterpretations and creation processes influenced by the platform's For You\nPage algorithm. We illustrate how creators' beliefs interact with TikTok's\nalgorithmic mediation to reinforce and shape their spiritual or relational\nthemes. Furthermore, we show how algorithmic conspirituality content impacts\nviewers, highlighting its role in generating significant emotional and\naffective labor for creators, stemming from complex relational dynamics\ninherent in this content creation. We discuss implications for design to\nsupport creators aimed at recognizing the unexpected spiritual and religious\nexperiences algorithms prompt, as well as supporting creators in effectively\nmanaging these challenges.", "AI": {"tldr": "研究探讨了社交媒体算法被用户视为神秘或灵性现象的现象，以及这种认知如何影响内容创作者及其在线社区。", "motivation": "研究动机源于用户对社交媒体算法不可预测性的神秘化解读，及其对创作者内容和社区形成的影响。", "method": "通过访谈14位在TikTok上创作算法灵性内容的创作者，分析其创作过程和算法解读。", "result": "研究发现创作者的信仰与算法互动，强化了灵性或关系主题，并对观众产生情感影响。", "conclusion": "研究建议设计应支持创作者应对算法引发的灵性体验及管理相关挑战。"}}
{"id": "2506.15988", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15988", "abs": "https://arxiv.org/abs/2506.15988", "authors": ["Connor Malone", "Owen Claxton", "Iman Shames", "Michael Milford"], "title": "Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation", "comment": null, "summary": "Stand-alone Visual Place Recognition (VPR) systems have little defence\nagainst a well-designed adversarial attack, which can lead to disastrous\nconsequences when deployed for robot navigation. This paper extensively\nanalyzes the effect of four adversarial attacks common in other perception\ntasks and four novel VPR-specific attacks on VPR localization performance. We\nthen propose how to close the loop between VPR, an Adversarial Attack Detector\n(AAD), and active navigation decisions by demonstrating the performance benefit\nof simulated AADs in a novel experiment paradigm -- which we detail for the\nrobotics community to use as a system framework. In the proposed experiment\nparadigm, we see the addition of AADs across a range of detection accuracies\ncan improve performance over baseline; demonstrating a significant improvement\n-- such as a ~50% reduction in the mean along-track localization error -- can\nbe achieved with True Positive and False Positive detection rates of only 75%\nand up to 25% respectively. We examine a variety of metrics including:\nAlong-Track Error, Percentage of Time Attacked, Percentage of Time in an\n`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on\nthese results, we provide the first investigation into the efficacy of the Fast\nGradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this\nwork highlights the need for AADs in real-world systems for trustworthy\nnavigation, and informs quantitative requirements for system design.", "AI": {"tldr": "论文分析了视觉地点识别（VPR）系统对对抗攻击的脆弱性，提出了一种结合对抗攻击检测器（AAD）和主动导航决策的系统框架，显著提升了定位性能。", "motivation": "由于VPR系统在面对对抗攻击时缺乏防御能力，可能导致机器人导航灾难性后果，因此需要研究如何提升其鲁棒性。", "method": "论文分析了四种常见对抗攻击和四种VPR特有攻击，提出了一种结合AAD和导航决策的实验框架，并评估了多种性能指标。", "result": "实验表明，加入AAD后，定位性能显著提升（如沿轨定位误差减少约50%），即使检测准确率仅为75%。", "conclusion": "研究强调了AAD在实际系统中的必要性，并为系统设计提供了量化要求。"}}
{"id": "2506.15702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15702", "abs": "https://arxiv.org/abs/2506.15702", "authors": ["Peter Belcak", "Greg Heinrich", "Jan Kautz", "Pavlo Molchanov"], "title": "Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation", "comment": null, "summary": "Finetuning language models for a new domain inevitably leads to the\ndeterioration of their general performance. This becomes more pronounced the\nmore limited the finetuning data resource.\n  We introduce minifinetuning (MFT), a method for language model domain\nadaptation that considerably reduces the effects of overfitting-induced\ndegeneralization in low-data settings and which does so in the absence of any\npre-training data for replay. MFT demonstrates 2-10x more favourable\nspecialization-to-degeneralization ratios than standard finetuning across a\nwide range of models and domains and exhibits an intrinsic robustness to\noverfitting when data in the new domain is scarce and down to as little as 500\nsamples.\n  Employing corrective self-distillation that is individualized on the sample\nlevel, MFT outperforms parameter-efficient finetuning methods, demonstrates\nreplay-like degeneralization mitigation properties, and is composable with\neither for a combined effect.", "AI": {"tldr": "论文提出了一种名为minifinetuning（MFT）的方法，用于在低数据环境下减少语言模型微调时的性能退化问题，且无需预训练数据。", "motivation": "微调语言模型在新领域时会导致其通用性能下降，尤其是在数据资源有限的情况下。", "method": "MFT通过样本级别的自我蒸馏技术，减少过拟合引起的性能退化，并在低至500样本的情况下仍表现稳健。", "result": "MFT在多种模型和领域中表现出比标准微调更好的性能，且能与参数高效微调方法结合使用。", "conclusion": "MFT是一种高效且稳健的领域适应方法，特别适用于数据稀缺的场景。"}}
{"id": "2506.16143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16143", "abs": "https://arxiv.org/abs/2506.16143", "authors": ["Stephane Ngnepiepaye Wembe", "Vincent Rousseau", "Johann Laconte", "Roland Lenain"], "title": "From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Modern agriculture faces escalating challenges: increasing demand for food,\nlabor shortages, and the urgent need to reduce environmental impact.\nAgricultural robotics has emerged as a promising response to these pressures,\nenabling the automation of precise and suitable field operations. In\nparticular, robots equipped with implements for tasks such as weeding or sowing\nmust interact delicately and accurately with the crops and soil. Unlike robots\nin other domains, these agricultural platforms typically use rigidly mounted\nimplements, where the implement's position is more critical than the robot's\ncenter in determining task success. Yet, most control strategies in the\nliterature focus on the vehicle body, often neglecting the acctual working\npoint of the system. This is particularly important when considering new\nagriculture practices where crops row are not necessary straights. This paper\npresents a predictive control strategy targeting the implement's reference\npoint. The method improves tracking performance by anticipating the motion of\nthe implement, which, due to its offset from the vehicle's center of rotation,\nis prone to overshooting during turns if not properly accounted for.", "AI": {"tldr": "本文提出了一种针对农业机器人工具的预测控制策略，通过预测工具的运动来提升跟踪性能，解决了工具偏移导致的转弯时超调问题。", "motivation": "现代农业面临劳动力短缺和环境压力，农业机器人成为解决方案，但现有控制策略多关注车辆而非工具的实际工作点。", "method": "提出一种预测控制策略，专注于工具参考点，通过预测工具运动来优化跟踪性能。", "result": "该方法有效减少了工具在转弯时的超调，提升了作业精度。", "conclusion": "专注于工具参考点的预测控制策略能显著提升农业机器人的作业效率与准确性。"}}
{"id": "2506.16874", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16874", "abs": "https://arxiv.org/abs/2506.16874", "authors": ["Zhiqing Wang", "Haoxiang Fan", "Shiwei Wu", "Qiaoyi Chen", "Yongqi Liang", "Zhenhui Peng"], "title": "Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools", "comment": null, "summary": "The integration of Generative Artificial Intelligence (GenAI) in K-6\nproject-based art courses presents both opportunities and challenges for\nenhancing creativity, engagement, and group collaboration. This study\nintroduces a four-phase field study, involving in total two experienced K-6 art\nteachers and 132 students in eight offline course sessions, to investigate the\nusage and impact of GenAI. Specifically, based on findings in Phases 1 and 2,\nwe developed AskArt, an interactive interface that combines DALL-E and GPT and\nis tailored to support elementary school students in their art projects, and\ndeployed it in Phases 3 and 4. Our findings revealed the benefits of GenAI in\nproviding background information, inspirations, and personalized guidance.\nHowever, challenges in query formulation for generating expected content were\nalso observed. Moreover, students employed varied collaboration strategies, and\nteachers noted increased engagement alongside concerns regarding misuse and\ninterface suitability. This study offers insights into the effective\nintegration of GenAI in elementary education, presents AskArt as a practical\ntool, and provides recommendations for educators and researchers to enhance\nproject-based learning with GenAI technologies.", "AI": {"tldr": "研究探讨了GenAI在K-6艺术课程中的整合，开发了AskArt工具，发现其能提升创造力与协作，但也面临查询生成和界面适配的挑战。", "motivation": "探索GenAI在小学艺术课程中如何增强创造力、参与度和团队协作。", "method": "四阶段实地研究，涉及2名教师和132名学生，开发并测试AskArt交互界面。", "result": "GenAI提供背景信息和灵感，但查询生成存在困难；学生协作策略多样，教师关注滥用问题。", "conclusion": "AskArt是有效的工具，研究为GenAI在基础教育中的整合提供了实践建议。"}}
{"id": "2506.16006", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16006", "abs": "https://arxiv.org/abs/2506.16006", "authors": ["Weiwei Duan", "Michael P. Gerlek", "Steven N. Minton", "Craig A. Knoblock", "Fandel Lin", "Theresa Chen", "Leeje Jang", "Sofia Kirsanova", "Zekun Li", "Yijun Lin", "Yao-Yi Chiang"], "title": "DIGMAPPER: A Modular System for Automated Geologic Map Digitization", "comment": null, "summary": "Historical geologic maps contain rich geospatial information, such as rock\nunits, faults, folds, and bedding planes, that is critical for assessing\nmineral resources essential to renewable energy, electric vehicles, and\nnational security. However, digitizing maps remains a labor-intensive and\ntime-consuming task. We present DIGMAPPER, a modular, scalable system developed\nin collaboration with the United States Geological Survey (USGS) to automate\nthe digitization of geologic maps. DIGMAPPER features a fully dockerized,\nworkflow-orchestrated architecture that integrates state-of-the-art deep\nlearning models for map layout analysis, feature extraction, and\ngeoreferencing. To overcome challenges such as limited training data and\ncomplex visual content, our system employs innovative techniques, including\nin-context learning with large language models, synthetic data generation, and\ntransformer-based models. Evaluations on over 100 annotated maps from the\nDARPA-USGS dataset demonstrate high accuracy across polygon, line, and point\nfeature extraction, and reliable georeferencing performance. Deployed at USGS,\nDIGMAPPER significantly accelerates the creation of analysis-ready geospatial\ndatasets, supporting national-scale critical mineral assessments and broader\ngeoscientific applications.", "AI": {"tldr": "DIGMAPPER是一个自动化地质地图数字化的系统，采用模块化、可扩展的架构，结合深度学习模型，解决了传统数字化任务耗时的问题。", "motivation": "地质地图中的地理信息对可再生能源、电动汽车和国家安全至关重要，但传统数字化方法效率低下。", "method": "系统采用Docker化、工作流编排架构，整合深度学习模型，利用上下文学习、合成数据生成和Transformer模型解决数据不足和复杂视觉内容问题。", "result": "在DARPA-USGS数据集上的评估显示，系统在多类特征提取和地理配准方面表现优异。", "conclusion": "DIGMAPPER显著提升了地质地图数字化的效率，支持国家级矿产评估和地质科学应用。"}}
{"id": "2506.15703", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15703", "abs": "https://arxiv.org/abs/2506.15703", "authors": ["Guoqing Chao", "Zhenghao Zhang", "Lei Meng", "Jie Wen", "Dianhui Chu"], "title": "Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance", "comment": null, "summary": "Federated multi-view clustering has been proposed to mine the valuable\ninformation within multi-view data distributed across different devices and has\nachieved impressive results while preserving the privacy. Despite great\nprogress, most federated multi-view clustering methods only used global\npseudo-labels to guide the downstream clustering process and failed to exploit\nthe global information when extracting features. In addition, missing data\nproblem in federated multi-view clustering task is less explored. To address\nthese problems, we propose a novel Federated Incomplete Multi-view Clustering\nmethod with globally Fused Graph guidance (FIMCFG). Specifically, we designed a\ndual-head graph convolutional encoder at each client to extract two kinds of\nunderlying features containing global and view-specific information.\nSubsequently, under the guidance of the fused graph, the two underlying\nfeatures are fused into high-level features, based on which clustering is\nconducted under the supervision of pseudo-labeling. Finally, the high-level\nfeatures are uploaded to the server to refine the graph fusion and\npseudo-labeling computation. Extensive experimental results demonstrate the\neffectiveness and superiority of FIMCFG. Our code is publicly available at\nhttps://github.com/PaddiHunter/FIMCFG.", "AI": {"tldr": "提出了一种新的联邦不完全多视图聚类方法FIMCFG，通过全局融合图指导解决现有方法在特征提取和缺失数据问题上的不足。", "motivation": "现有联邦多视图聚类方法仅使用全局伪标签指导聚类，未能充分利用全局信息，且对缺失数据问题研究较少。", "method": "设计双头图卷积编码器提取全局和视图特定特征，通过融合图指导特征融合，并在伪标签监督下进行聚类。", "result": "实验证明FIMCFG的有效性和优越性。", "conclusion": "FIMCFG成功解决了现有方法的不足，并在联邦多视图聚类任务中表现出色。"}}
{"id": "2506.17011", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.17011", "abs": "https://arxiv.org/abs/2506.17011", "authors": ["Bruno Campos"], "title": "Juicy or Dry? A Comparative Study of User Engagement and Information Retention in Interactive Infographics", "comment": null, "summary": "This study compares the impact of \"juiciness\" on user engagement and\nshort-term information retention in interactive infographics. Juicy designs\ngenerally showed a slight advantage in overall user engagement scores compared\nto dry designs. Specifically, the juicy version of the Burcalories infographic\nhad the highest engagement score. However, the differences in engagement were\noften small. Regarding information retention, the results were mixed. The juicy\nversions of The Daily Routines of Famous Creative People and The Main Chakras\ninfographics showed marginally better average recall and more participants with\nhigher recall. Conversely, the dry version of Burcalories led to more correct\nanswers in multiple-choice questions. The study suggests that while juicy\ndesign elements can enhance user engagement and, in some cases, short-term\ninformation retention, their effectiveness depends on careful implementation.\nExcessive juiciness could be overwhelming or distracting, while\nwell-implemented juicy elements contributed to a more entertaining experience.\nThe findings emphasize the importance of balancing engaging feedback with\nclarity and usability.", "AI": {"tldr": "研究比较了“多汁”设计对用户参与度和短期信息保留的影响，发现多汁设计在用户参与度上略有优势，但对信息保留的效果因设计而异。", "motivation": "探讨多汁设计元素在交互式信息图表中对用户行为和记忆的影响。", "method": "比较多汁设计和干燥设计的用户参与度和信息保留效果，分析具体案例。", "result": "多汁设计在用户参与度上表现略好，但对信息保留的效果不一；某些情况下干燥设计更优。", "conclusion": "多汁设计能提升用户参与度，但需平衡清晰度和实用性，避免过度设计。"}}
{"id": "2506.16017", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16017", "abs": "https://arxiv.org/abs/2506.16017", "authors": ["Liangjing Shao", "Linxin Bai", "Chenkang Du", "Xinrong Chen"], "title": "EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training", "comment": "Accepted by IROS 2025", "summary": "Monocular depth estimation and ego-motion estimation are significant tasks\nfor scene perception and navigation in stable, accurate and efficient\nrobot-assisted endoscopy. To tackle lighting variations and sparse textures in\nendoscopic scenes, multiple techniques including optical flow, appearance flow\nand intrinsic image decomposition have been introduced into the existing\nmethods. However, the effective training strategy for multiple modules are\nstill critical to deal with both illumination issues and information\ninterference for self-supervised depth estimation in endoscopy. Therefore, a\nnovel framework with multistep efficient finetuning is proposed in this work.\nIn each epoch of end-to-end training, the process is divided into three steps,\nincluding optical flow registration, multiscale image decomposition and\nmultiple transformation alignments. At each step, only the related networks are\ntrained without interference of irrelevant information. Based on\nparameter-efficient finetuning on the foundation model, the proposed method\nachieves state-of-the-art performance on self-supervised depth estimation on\nSCARED dataset and zero-shot depth estimation on Hamlyn dataset, with\n4\\%$\\sim$10\\% lower error. The evaluation code of this work has been published\non https://github.com/BaymaxShao/EndoMUST.", "AI": {"tldr": "提出了一种多步微调框架，用于内窥镜场景下的自监督深度估计，解决了光照变化和稀疏纹理问题，性能优于现有方法。", "motivation": "内窥镜场景中的光照变化和稀疏纹理对深度估计和自运动估计提出了挑战，现有方法需要更有效的训练策略。", "method": "采用三步训练策略：光流注册、多尺度图像分解和多变换对齐，每步仅训练相关网络以减少信息干扰。", "result": "在SCARED和Hamlyn数据集上实现了4%~10%的误差降低，达到SOTA性能。", "conclusion": "多步微调框架显著提升了内窥镜场景下的自监督深度估计性能。"}}
{"id": "2506.15704", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15704", "abs": "https://arxiv.org/abs/2506.15704", "authors": ["Feiyu Yao", "Qian Wang"], "title": "Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding", "comment": null, "summary": "As large language models (LLMs) continue to support increasingly longer\ncontexts, the memory demand for key-value (KV) caches during decoding grows\nrapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe\nbandwidth. Sparse attention mechanisms alleviate this issue by computing\nattention weights only for selected key-value pairs. However, their indexing\ncomputation typically requires traversing all key vectors, resulting in\nsignificant computational and data transfer overhead. To reduce the cost of\nindex retrieval, existing methods often treat each decoding step as an\nindependent process, failing to exploit the temporal correlations embedded in\nhistorical decoding information. To this end, we propose LFPS(Learn From the\nPast for Sparse Indexing), an acceleration method that dynamically constructs\nsparse indexing candidates based on historical attention patterns. LFPS\ncaptures two prevalent trends in decoder attention -vertical patterns\n(attending to fixed positions) and slash patterns (attending to relative\npositions) -and incorporates a positional expansion strategy to effectively\npredict the Top-k indices for the current step. We validate LFPS on challenging\nlong-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as\nthe base model. Experimental results show that LFPS achieves up to 22.8$\\times$\nspeedup over full attention and 9.6$\\times$ speedup over exact Top-k retrieval\non an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,\nwhile preserving generation accuracy. These results demonstrate that LFPS\noffers a practical and efficient solution for decoding optimization in\nlong-context LLM inference.", "AI": {"tldr": "LFPS是一种基于历史注意力模式的稀疏索引加速方法，显著提升了长上下文LLM的解码效率。", "motivation": "解决KV缓存内存需求大和稀疏注意力机制索引检索开销高的问题。", "method": "动态构建稀疏索引候选，利用历史注意力模式（垂直和斜线模式）和位置扩展策略预测Top-k索引。", "result": "在RTX 4090 GPU和Xeon Gold 6430 CPU上分别实现22.8倍和9.6倍加速，同时保持生成准确性。", "conclusion": "LFPS为长上下文LLM推理提供了一种高效且实用的解码优化方案。"}}
{"id": "2506.16201", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16201", "abs": "https://arxiv.org/abs/2506.16201", "authors": ["Sen Wang", "Le Wang", "Sanping Zhou", "Jingyi Tian", "Jiayi Li", "Haowen Sun", "Wei Tang"], "title": "FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation in high-precision tasks is essential for numerous\nindustrial and real-world applications where accuracy and speed are required.\nYet current diffusion-based policy learning methods generally suffer from low\ncomputational efficiency due to the iterative denoising process during\ninference. Moreover, these methods do not fully explore the potential of\ngenerative models for enhancing information exploration in 3D environments. In\nresponse, we propose FlowRAM, a novel framework that leverages generative\nmodels to achieve region-aware perception, enabling efficient multimodal\ninformation processing. Specifically, we devise a Dynamic Radius Schedule,\nwhich allows adaptive perception, facilitating transitions from global scene\ncomprehension to fine-grained geometric details. Furthermore, we integrate\nstate space models to integrate multimodal information, while preserving linear\ncomputational complexity. In addition, we employ conditional flow matching to\nlearn action poses by regressing deterministic vector fields, simplifying the\nlearning process while maintaining performance. We verify the effectiveness of\nthe FlowRAM in the RLBench, an established manipulation benchmark, and achieve\nstate-of-the-art performance. The results demonstrate that FlowRAM achieves a\nremarkable improvement, particularly in high-precision tasks, where it\noutperforms previous methods by 12.0% in average success rate. Additionally,\nFlowRAM is able to generate physically plausible actions for a variety of\nreal-world tasks in less than 4 time steps, significantly increasing inference\nspeed.", "AI": {"tldr": "FlowRAM是一种新型框架，利用生成模型实现区域感知，提升3D环境中的信息探索效率，显著提高高精度任务的性能。", "motivation": "当前基于扩散的策略学习方法在推理过程中计算效率低，且未充分利用生成模型在3D环境中的潜力。", "method": "提出FlowRAM框架，采用动态半径调度实现自适应感知，结合状态空间模型处理多模态信息，并使用条件流匹配学习动作位姿。", "result": "在RLBench基准测试中，FlowRAM平均成功率提升12.0%，推理速度显著加快（少于4个时间步骤）。", "conclusion": "FlowRAM在高精度任务中表现出色，为机器人操作提供了高效且性能优越的解决方案。"}}
{"id": "2506.16054", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.16054", "abs": "https://arxiv.org/abs/2506.16054", "authors": ["Tianchen Zhao", "Ke Hong", "Xinhao Yang", "Xuefeng Xiao", "Huixia Li", "Feng Ling", "Ruiqi Xie", "Siqi Chen", "Hongyu Zhu", "Yichong Zhang", "Yu Wang"], "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models", "comment": "project page: https://a-suozhang.xyz/paroattn.github.io", "summary": "In visual generation, the quadratic complexity of attention mechanisms\nresults in high memory and computational costs, especially for longer token\nsequences required in high-resolution image or multi-frame video generation. To\naddress this, prior research has explored techniques such as sparsification and\nquantization. However, these techniques face significant challenges under low\ndensity and reduced bitwidths. Through systematic analysis, we identify that\nthe core difficulty stems from the dispersed and irregular characteristics of\nvisual attention patterns. Therefore, instead of introducing specialized\nsparsification and quantization design to accommodate such patterns, we propose\nan alternative strategy: *reorganizing* the attention pattern to alleviate the\nchallenges. Inspired by the local aggregation nature of visual feature\nextraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**\ntechnique, which unifies the diverse attention patterns into a\nhardware-friendly block-wise pattern. This unification substantially simplifies\nand enhances both sparsification and quantization. We evaluate the\nperformance-efficiency trade-offs of various design choices and finalize a\nmethodology tailored for the unified pattern. Our approach, **PAROAttention**,\nachieves video and image generation with lossless metrics, and nearly identical\nresults from full-precision (FP) baselines, while operating at notably lower\ndensity (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to\n**2.7x** end-to-end latency speedup.", "AI": {"tldr": "提出了一种名为PARO的技术，通过重组注意力模式来降低视觉生成中的计算和内存成本，同时保持性能。", "motivation": "解决视觉生成中注意力机制的高计算和内存成本问题，尤其是在高分辨率图像或多帧视频生成中。", "method": "设计了一种Pattern-Aware token ReOrdering (PARO)技术，将分散的注意力模式统一为硬件友好的块状模式，简化并增强了稀疏化和量化。", "result": "PAROAttention在低密度（20%-30%）和低比特宽度（INT8/INT4）下实现了与全精度基线几乎相同的性能，端到端延迟加速了1.9x至2.7x。", "conclusion": "PARO技术通过重组注意力模式，显著降低了计算成本，同时保持了视觉生成的高质量。"}}
{"id": "2506.15705", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15705", "abs": "https://arxiv.org/abs/2506.15705", "authors": ["Jittarin Jetwiriyanon", "Teo Susnjak", "Surangika Ranathunga"], "title": "Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models", "comment": null, "summary": "This study investigates zero-shot forecasting capabilities of Time Series\nFoundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to\nforecasting economic indicators under univariate conditions, bypassing the need\nfor train bespoke econometric models using and extensive training datasets. Our\nexperiments were conducted on a case study dataset, without additional\ncustomisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,\nTimeGPT and Moirai) under data-scarce conditions and structural breaks. Our\nresults demonstrate that appropriately engineered TSFMs can internalise rich\neconomic dynamics, accommodate regime shifts, and deliver well-behaved\nuncertainty estimates out of the box, while matching state-of-the-art\nmultivariate models on this domain. Our findings suggest that, without any\nfine-tuning, TSFMs can match or exceed classical models during stable economic\nconditions. However, they are vulnerable to degradation in performances during\nperiods of rapid shocks. The findings offer guidance to practitioners on when\nzero-shot deployments are viable for macroeconomic monitoring and strategic\nplanning.", "AI": {"tldr": "研究探讨了时间序列基础模型（TSFMs）在宏观经济指标零样本预测中的能力，发现其在稳定经济条件下表现优异，但在快速冲击时期性能下降。", "motivation": "探索TSFMs在无需训练定制模型的情况下预测宏观经济指标的潜力，为实践者提供零样本部署的指导。", "method": "在数据稀缺和结构断裂条件下，对三种先进TSFMs（Chronos、TimeGPT和Moirai）进行严格回测。", "result": "TSFMs能够内化经济动态、适应制度变化，并在稳定条件下匹配或超越传统模型，但在快速冲击时期表现不佳。", "conclusion": "TSFMs在零样本部署中具有潜力，但需注意其在经济快速变化时期的局限性。"}}
{"id": "2506.15907", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.15907", "abs": "https://arxiv.org/abs/2506.15907", "authors": ["Hang Yang", "Yusheng Hu", "Yong Liu", "Cong", "Hao"], "title": "Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI", "comment": "7 pages, 4 figures, 1 table, submitted", "summary": "Accurate graph similarity is critical for knowledge transfer in VLSI design,\nenabling the reuse of prior solutions to reduce engineering effort and\nturnaround time. We propose Pieceformer, a scalable, self-supervised similarity\nassessment framework, equipped with a hybrid message-passing and graph\ntransformer encoder. To address transformer scalability, we incorporate a\nlinear transformer backbone and introduce a partitioned training pipeline for\nefficient memory and parallelism management. Evaluations on synthetic and\nreal-world CircuitNet datasets show that Pieceformer reduces mean absolute\nerror (MAE) by 24.9% over the baseline and is the only method to correctly\ncluster all real-world design groups. We further demonstrate the practical\nusage of our model through a case study on a partitioning task, achieving up to\n89% runtime reduction. These results validate the framework's effectiveness for\nscalable, unbiased design reuse in modern VLSI systems.", "AI": {"tldr": "Pieceformer是一个可扩展的自监督图相似性评估框架，用于VLSI设计中的知识转移，显著降低误差并提升效率。", "motivation": "解决VLSI设计中知识转移的准确性问题，以减少工程努力和缩短周转时间。", "method": "结合混合消息传递和图变换器编码器，采用线性变换器主干和分区训练管道以提高可扩展性。", "result": "在合成和真实数据集上，MAE降低24.9%，并能正确聚类所有真实设计组；分区任务中实现89%的运行时减少。", "conclusion": "Pieceformer在VLSI系统中实现了可扩展且无偏的设计重用，验证了其有效性。"}}
{"id": "2506.16211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16211", "abs": "https://arxiv.org/abs/2506.16211", "authors": ["Puhao Li", "Yingying Wu", "Ziheng Xi", "Wanlin Li", "Yuzhe Huang", "Zhiyuan Zhang", "Yinghan Chen", "Jianan Wang", "Song-Chun Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models", "comment": "Website: https://controlvla.github.io", "summary": "Learning real-world robotic manipulation is challenging, particularly when\nlimited demonstrations are available. Existing methods for few-shot\nmanipulation often rely on simulation-augmented data or pre-built modules like\ngrasping and pose estimation, which struggle with sim-to-real gaps and lack\nextensibility. While large-scale imitation pre-training shows promise, adapting\nthese general-purpose policies to specific tasks in data-scarce settings\nremains unexplored. To achieve this, we propose ControlVLA, a novel framework\nthat bridges pre-trained VLA models with object-centric representations via a\nControlNet-style architecture for efficient fine-tuning. Specifically, to\nintroduce object-centric conditions without overwriting prior knowledge,\nControlVLA zero-initializes a set of projection layers, allowing them to\ngradually adapt the pre-trained manipulation policies. In real-world\nexperiments across 6 diverse tasks, including pouring cubes and folding\nclothes, our method achieves a 76.7% success rate while requiring only 10-20\ndemonstrations -- a significant improvement over traditional approaches that\nrequire more than 100 demonstrations to achieve comparable success. Additional\nexperiments highlight ControlVLA's extensibility to long-horizon tasks and\nrobustness to unseen objects and backgrounds.", "AI": {"tldr": "ControlVLA是一种新框架，通过结合预训练的VLA模型和以对象为中心的表示，实现了在少量演示下的高效微调，显著提升了机器人操作的性能。", "motivation": "解决现有方法在少量演示下机器人操作任务中的局限性，如仿真数据依赖和模块化方法的扩展性问题。", "method": "提出ControlVLA框架，通过ControlNet风格架构将预训练的VLA模型与对象中心表示结合，使用零初始化投影层逐步适应预训练策略。", "result": "在6个真实任务中，仅需10-20次演示即达到76.7%的成功率，优于传统方法（需100次以上演示）。", "conclusion": "ControlVLA在少量演示下表现出高效性和扩展性，适用于长时任务和未知对象与背景的鲁棒性。"}}
{"id": "2506.17116", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.17116", "abs": "https://arxiv.org/abs/2506.17116", "authors": ["Samuel Rhys Cox", "Helena Bøjer Djernæs", "Niels van Berkel"], "title": "Reflecting Human Values in XAI: Emotional and Reflective Benefits in Creativity Support Tools", "comment": "Workshop paper presented at XAIxArts'25 - the third international\n  workshop on eXplainable AI for the Arts, held in conjunction with the ACM\n  Creativity and Cognition conference 2025, June 23rd, 2025. 3 pages", "summary": "In this workshop paper, we discuss the potential for measures of user-centric\nbenefits (such as emotional well-being) that could be explored when evaluating\nexplainable AI (XAI) systems within the arts. As a background to this, we draw\nfrom our recent review of creativity support tool (CST) evaluations, that found\na paucity of studies evaluating CSTs for user-centric measures that benefit the\nuser themselves. Specifically, we discuss measures of: (1) developing intrinsic\nabilities, (2) emotional well-being, (3) self-reflection, and (4)\nself-perception. By discussing these user-centric measures within the context\nof XAI and the arts, we wish to provoke discussion regarding the potential of\nsuch measures.", "AI": {"tldr": "探讨在艺术领域评估可解释AI（XAI）系统时，用户中心化指标（如情感福祉）的潜力。", "motivation": "现有创造力支持工具（CST）评估中缺乏关注用户自身受益的研究，作者希望通过讨论用户中心化指标填补这一空白。", "method": "基于近期CST评估综述，提出四项用户中心化指标：内在能力发展、情感福祉、自我反思和自我认知。", "result": "通过将这些指标与XAI和艺术结合，激发相关讨论。", "conclusion": "呼吁在XAI和艺术领域进一步探索用户中心化评估指标的价值。"}}
{"id": "2506.16058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16058", "abs": "https://arxiv.org/abs/2506.16058", "authors": ["Yong Liu", "SongLi Wu", "Sule Bai", "Jiahao Wang", "Yitong Wang", "Yansong Tang"], "title": "Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation", "comment": null, "summary": "Open-vocabulary segmentation aims to achieve segmentation of arbitrary\ncategories given unlimited text inputs as guidance. To achieve this, recent\nworks have focused on developing various technical routes to exploit the\npotential of large-scale pre-trained vision-language models and have made\nsignificant progress on existing benchmarks. However, we find that existing\ntest sets are limited in measuring the models' comprehension of\n``open-vocabulary\" concepts, as their semantic space closely resembles the\ntraining space, even with many overlapping categories. To this end, we present\na new benchmark named OpenBench that differs significantly from the training\nsemantics. It is designed to better assess the model's ability to understand\nand segment a wide range of real-world concepts. When testing existing methods\non OpenBench, we find that their performance diverges from the conclusions\ndrawn on existing test sets. In addition, we propose a method named OVSNet to\nimprove the segmentation performance for diverse and open scenarios. Through\nelaborate fusion of heterogeneous features and cost-free expansion of the\ntraining space, OVSNet achieves state-of-the-art results on both existing\ndatasets and our proposed OpenBench. Corresponding analysis demonstrate the\nsoundness and effectiveness of our proposed benchmark and method.", "AI": {"tldr": "论文提出新基准OpenBench以评估开放词汇分割模型的泛化能力，并提出OVSNet方法提升性能。", "motivation": "现有测试集无法充分衡量模型对开放词汇概念的理解，因其语义空间与训练集相似。", "method": "提出OVSNet，通过异构特征融合和训练空间扩展提升性能。", "result": "OVSNet在现有数据集和OpenBench上均取得最佳结果。", "conclusion": "OpenBench和OVSNet的有效性得到验证，为开放词汇分割提供新方向。"}}
{"id": "2506.15706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15706", "abs": "https://arxiv.org/abs/2506.15706", "authors": ["Yunze Lin"], "title": "MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning", "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) as it requires ensuring the correctness of each reasoning step.\nResearchers have been strengthening the mathematical reasoning abilities of\nLLMs through supervised fine-tuning, but due to the inability to suppress\nincorrect outputs, illusions can easily arise. Recently, Direct Preference\nOptimization (DPO) has been widely adopted for aligning human intent by using\npreference data to prevent LLMs from generating incorrect outputs. However, it\nhas shown limited benefits in long-chain mathematical reasoning, mainly because\nDPO struggles to effectively capture the differences between accepted and\nrejected answers from preferences in long-chain data. The inconsistency between\nDPO training and LLMs' generation metrics also affects the effectiveness of\nsuppressing incorrect outputs. We propose the Multi-Granularity Direct\nPreference Optimization (MDPO) method, optimizing the mathematical reasoning of\nLLMs at three granularities: Solution2Solution, Inference2Inference, and\nStep2Step. Solution2Solution focuses on the correctness of entire long-chain\nreasoning; Inference2Inference concentrates on logical reasoning between steps;\nStep2Step corrects computational errors in steps, enhancing the computational\ncapabilities of LLMs. Additionally, we unify the training objectives of the\nthree granularities to align with the generation metrics. We conducted\nexperiments on the open-source models Qwen2 and Llama3, achieving improvements\nof 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,\noutperforming DPO and other DPO variant methods. Furthermore, we also provide a\npipeline for constructing MDPO training data that is simple and does not\nrequire manual annotation costs.", "AI": {"tldr": "论文提出了一种多粒度直接偏好优化（MDPO）方法，通过三个粒度优化LLMs的数学推理能力，并在实验中优于DPO及其变体。", "motivation": "LLMs在数学推理中容易产生幻觉，现有方法如DPO在长链推理中效果有限。", "method": "提出MDPO方法，在Solution2Solution、Inference2Inference和Step2Step三个粒度上优化，并统一训练目标。", "result": "在Qwen2和Llama3模型上，GSM8K和MATH数据集上性能提升显著。", "conclusion": "MDPO方法有效提升了LLMs的数学推理能力，并提供了低成本的数据构建流程。"}}
{"id": "2506.16392", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16392", "abs": "https://arxiv.org/abs/2506.16392", "authors": ["Gonçalo Granjal Cruz", "Balazs Renczes", "Mark C Runacres", "Jan Decuyper"], "title": "State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification", "comment": "Accepted for IEEE Control Systems Letters", "summary": "While accurate, black-box system identification models lack interpretability\nof the underlying system dynamics. This paper proposes State-Space\nKolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating\nKolmogorov-Arnold Networks within a state-space framework. The proposed model\nis validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein\nbenchmarks. Results show that SS-KAN provides enhanced interpretability due to\nsparsity-promoting regularization and the direct visualization of its learned\nunivariate functions, which reveal system nonlinearities at the cost of\naccuracy when compared to state-of-the-art black-box models, highlighting\nSS-KAN as a promising approach for interpretable nonlinear system\nidentification, balancing accuracy and interpretability of nonlinear system\ndynamics.", "AI": {"tldr": "论文提出SS-KAN模型，通过结合Kolmogorov-Arnold网络与状态空间框架，提升非线性系统辨识的可解释性，但牺牲部分准确性。", "motivation": "黑盒系统辨识模型缺乏对系统动态的可解释性，需要一种平衡准确性和可解释性的方法。", "method": "提出SS-KAN模型，利用稀疏正则化和单变量函数可视化，增强可解释性。", "result": "在Silverbox和Wiener-Hammerstein基准测试中，SS-KAN展示了更好的可解释性，但准确性略低于先进黑盒模型。", "conclusion": "SS-KAN是一种有前景的模型，平衡了非线性系统辨识的准确性和可解释性。"}}
{"id": "2506.16219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16219", "abs": "https://arxiv.org/abs/2506.16219", "authors": ["Amine Tourki", "Paul Prevel", "Nils Einecke", "Tim Puphal", "Alexandre Alahi"], "title": "Probabilistic Collision Risk Estimation for Pedestrian Navigation", "comment": null, "summary": "Intelligent devices for supporting persons with vision impairment are\nbecoming more widespread, but they are lacking behind the advancements in\nintelligent driver assistant system. To make a first step forward, this work\ndiscusses the integration of the risk model technology, previously used in\nautonomous driving and advanced driver assistance systems, into an assistance\ndevice for persons with vision impairment. The risk model computes a\nprobabilistic collision risk given object trajectories which has previously\nbeen shown to give better indications of an object's collision potential\ncompared to distance or time-to-contact measures in vehicle scenarios. In this\nwork, we show that the risk model is also superior in warning persons with\nvision impairment about dangerous objects. Our experiments demonstrate that the\nwarning accuracy of the risk model is 67% while both distance and\ntime-to-contact measures reach only 51% accuracy for real-world data.", "AI": {"tldr": "将自动驾驶中的风险模型技术应用于视觉障碍辅助设备，实验显示其警告准确率（67%）优于距离和时间接触测量（51%）。", "motivation": "智能设备在视觉障碍辅助领域落后于智能驾驶辅助系统，因此探索将自动驾驶中的风险模型技术引入视觉障碍辅助设备。", "method": "将风险模型技术（基于物体轨迹计算碰撞概率）应用于视觉障碍辅助设备，并与传统的距离和时间接触测量方法对比。", "result": "风险模型的警告准确率为67%，显著高于距离和时间接触测量的51%。", "conclusion": "风险模型技术在视觉障碍辅助设备中表现优越，具有实际应用潜力。"}}
{"id": "2506.17196", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2506.17196", "abs": "https://arxiv.org/abs/2506.17196", "authors": ["Shambhavi Bhushan", "Danielle R Thomas", "Conrad Borchers", "Isha Raghuvanshi", "Ralph Abboud", "Erin Gatz", "Shivang Gupta", "Kenneth Koedinger"], "title": "Detecting LLM-Generated Short Answers and Effects on Learner Performance", "comment": "Accepted for publication at the 19th European Conference on\n  Technology Enhanced Learning (ECTEL 2025). This is the author's accepted\n  manuscript", "summary": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases.", "AI": {"tldr": "研究比较了LLM生成文本的检测方法，提出了一种基于GPT-4o的微调模型，性能优于现有工具，并探讨了LLM滥用对学习的影响。", "motivation": "随着LLM的普及，其在线学习中的滥用问题引发担忧，但现有检测工具的可靠性参差不齐，且缺乏对滥用影响的评估。", "method": "定义LLM生成文本的标准，微调GPT-4o进行检测，并与GPTZero对比，同时评估滥用对学习的影响。", "result": "微调后的GPT-4o检测准确率达80%，F1分数0.78，优于GPTZero；滥用LLM的学习者在后续测试中表现异常。", "conclusion": "研究为改进LLM检测提供了结构化方法，并建议结合辅助统计指标，同时公开数据和代码以支持后续研究。"}}
{"id": "2506.16061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16061", "abs": "https://arxiv.org/abs/2506.16061", "authors": ["Yucheng Jin", "Jinyan Chen", "Ziyue He", "Baojun Han", "Furan An"], "title": "STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution", "comment": "14pages 3figures, alredy submiss to PRCV 2025", "summary": "Human pose estimation in low-resolution videos presents a fundamental\nchallenge in computer vision. Conventional methods either assume high-quality\ninputs or employ computationally expensive cascaded processing, which limits\ntheir deployment in resource-constrained environments. We propose STAR-Pose, a\nspatial-temporal adaptive super-resolution framework specifically designed for\nvideo-based human pose estimation. Our method features a novel spatial-temporal\nTransformer with LeakyReLU-modified linear attention, which efficiently\ncaptures long-range temporal dependencies. Moreover, it is complemented by an\nadaptive fusion module that integrates parallel CNN branch for local texture\nenhancement. We also design a pose-aware compound loss to achieve task-oriented\nsuper-resolution. This loss guides the network to reconstruct structural\nfeatures that are most beneficial for keypoint localization, rather than\noptimizing purely for visual quality. Extensive experiments on several\nmainstream video HPE datasets demonstrate that STAR-Pose outperforms existing\napproaches. It achieves up to 5.2% mAP improvement under extremely\nlow-resolution (64x48) conditions while delivering 2.8x to 4.4x faster\ninference than cascaded approaches.", "AI": {"tldr": "STAR-Pose是一种针对低分辨率视频中人体姿态估计的空间-时间自适应超分辨率框架，通过新型Transformer和自适应融合模块提升性能。", "motivation": "低分辨率视频中的人体姿态估计存在挑战，传统方法要么依赖高质量输入，要么计算成本高，难以在资源受限环境中部署。", "method": "提出STAR-Pose框架，结合空间-时间Transformer和自适应融合模块，设计任务导向的复合损失函数。", "result": "在多个主流数据集上表现优异，低分辨率条件下mAP提升5.2%，推理速度比级联方法快2.8x至4.4x。", "conclusion": "STAR-Pose在低分辨率视频中高效且准确地实现了人体姿态估计，优于现有方法。"}}
{"id": "2506.15707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15707", "abs": "https://arxiv.org/abs/2506.15707", "authors": ["Xinglin Wang", "Yiwei Li", "Shaoxiong Feng", "Peiwen Yuan", "Yueqi Zhang", "Jiayi Shi", "Chuyi Tan", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "comment": "preprint", "summary": "Test-Time Scaling (TTS) improves the performance of Large Language Models\n(LLMs) by using additional inference-time computation to explore multiple\nreasoning paths through search. Yet how to allocate a fixed rollout budget most\neffectively during search remains underexplored, often resulting in inefficient\nuse of compute at test time. To bridge this gap, we formulate test-time search\nas a resource allocation problem and derive the optimal allocation strategy\nthat maximizes the probability of obtaining a correct solution under a fixed\nrollout budget. Within this formulation, we reveal a core limitation of\nexisting search methods: solution-level allocation tends to favor reasoning\ndirections with more candidates, leading to theoretically suboptimal and\ninefficient use of compute. To address this, we propose Direction-Oriented\nResource Allocation (DORA), a provably optimal method that mitigates this bias\nby decoupling direction quality from candidate count and allocating resources\nat the direction level. To demonstrate DORA's effectiveness, we conduct\nextensive experiments on challenging mathematical reasoning benchmarks\nincluding MATH500, AIME2024, and AIME2025. The empirical results show that DORA\nconsistently outperforms strong baselines with comparable computational cost,\nachieving state-of-the-art accuracy. We hope our findings contribute to a\nbroader understanding of optimal TTS for LLMs.", "AI": {"tldr": "论文提出了一种名为DORA的资源分配方法，用于优化大型语言模型（LLMs）在测试时的搜索效率，通过方向级别的资源分配提升性能。", "motivation": "现有测试时搜索方法在资源分配上效率低下，倾向于支持候选数量多的推理方向，导致计算资源浪费。", "method": "将测试时搜索建模为资源分配问题，提出DORA方法，通过方向级别的资源分配优化搜索效率。", "result": "在MATH500、AIME2024和AIME2025等数学推理基准测试中，DORA表现优于基线方法，达到最优准确率。", "conclusion": "DORA通过优化资源分配策略，显著提升了LLMs在测试时的性能，为未来研究提供了新思路。"}}
{"id": "2506.16427", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16427", "abs": "https://arxiv.org/abs/2506.16427", "authors": ["Mohamad Hachem", "Clément Roos", "Thierry Miquel", "Murat Bronz"], "title": "Full-Pose Tracking via Robust Control for Over-Actuated Multirotors", "comment": null, "summary": "This paper presents a robust cascaded control architecture for over-actuated\nmultirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)\ncontrol combined with structured H_inf control, initially proposed for\nunder-actuated multirotors, to a broader range of multirotor configurations. To\nachieve precise and robust attitude and position tracking, we employ a weighted\nleast-squares geometric guidance control allocation method, formulated as a\nquadratic optimization problem, enabling full-pose tracking. The proposed\napproach effectively addresses key challenges, such as preventing infeasible\npose references and enhancing robustness against disturbances, as well as\nconsidering multirotor's actual physical limitations. Numerical simulations\nwith an over-actuated hexacopter validate the method's effectiveness,\ndemonstrating its adaptability to diverse mission scenarios and its potential\nfor real-world aerial applications.", "AI": {"tldr": "本文提出了一种针对过驱动多旋翼的级联控制架构，结合INDI和结构化H∞控制，通过几何制导分配方法实现精确姿态与位置跟踪。", "motivation": "扩展INDI和H∞控制在多旋翼中的应用，解决过驱动配置下的精确跟踪和鲁棒性问题。", "method": "采用加权最小二乘几何制导控制分配方法，将其建模为二次优化问题，实现全姿态跟踪。", "result": "数值模拟验证了方法在过驱动六旋翼中的有效性，展示了其适应性和实际应用潜力。", "conclusion": "该方法能有效应对不可行姿态参考和干扰，同时考虑物理限制，适用于多样化任务场景。"}}
{"id": "2506.16263", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16263", "abs": "https://arxiv.org/abs/2506.16263", "authors": ["Xiting He", "Mingwu Su", "Xinqi Jiang", "Long Bai", "Jiewen Lai", "Hongliang Ren"], "title": "CapsDT: Diffusion-Transformer for Capsule Robot Manipulation", "comment": "IROS 2025", "summary": "Vision-Language-Action (VLA) models have emerged as a prominent research\narea, showcasing significant potential across a variety of applications.\nHowever, their performance in endoscopy robotics, particularly endoscopy\ncapsule robots that perform actions within the digestive system, remains\nunexplored. The integration of VLA models into endoscopy robots allows more\nintuitive and efficient interactions between human operators and medical\ndevices, improving both diagnostic accuracy and treatment outcomes. In this\nwork, we design CapsDT, a Diffusion Transformer model for capsule robot\nmanipulation in the stomach. By processing interleaved visual inputs, and\ntextual instructions, CapsDT can infer corresponding robotic control signals to\nfacilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot\nsystem, a capsule robot controlled by a robotic arm-held magnet, addressing\ndifferent levels of four endoscopy tasks and creating corresponding capsule\nrobot datasets within the stomach simulator. Comprehensive evaluations on\nvarious robotic tasks indicate that CapsDT can serve as a robust\nvision-language generalist, achieving state-of-the-art performance in various\nlevels of endoscopy tasks while achieving a 26.25% success rate in real-world\nsimulation manipulation.", "AI": {"tldr": "CapsDT是一种基于扩散变换器的模型，用于胶囊机器人在胃部的操作，结合视觉和文本输入生成控制信号，提升内窥镜任务的效率和准确性。", "motivation": "探索Vision-Language-Action模型在内窥镜胶囊机器人中的应用潜力，以改善人机交互和医疗效果。", "method": "设计CapsDT模型，处理视觉和文本输入，生成机器人控制信号，并开发胶囊内窥镜机器人系统。", "result": "CapsDT在多种内窥镜任务中表现优异，真实模拟操作成功率达26.25%。", "conclusion": "CapsDT作为视觉-语言通用模型，在内窥镜任务中具有显著潜力。"}}
{"id": "2506.16073", "categories": ["cs.CV", "I.4.8; I.5.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.16073", "abs": "https://arxiv.org/abs/2506.16073", "authors": ["Byung Hoon Lee", "Wooseok Shin", "Sung Won Han"], "title": "TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network for Lipreading", "comment": "15 pages, 6 figures", "summary": "The word-level lipreading approach typically employs a two-stage framework\nwith separate frontend and backend architectures to model dynamic lip\nmovements. Each component has been extensively studied, and in the backend\narchitecture, temporal convolutional networks (TCNs) have been widely adopted\nin state-of-the-art methods. Recently, dense skip connections have been\nintroduced in TCNs to mitigate the limited density of the receptive field,\nthereby improving the modeling of complex temporal representations. However,\ntheir performance remains constrained owing to potential information loss\nregarding the continuous nature of lip movements, caused by blind spots in the\nreceptive field. To address this limitation, we propose TD3Net, a temporal\ndensely connected multi-dilated convolutional network that combines dense skip\nconnections and multi-dilated temporal convolutions as the backend\narchitecture. TD3Net covers a wide and dense receptive field without blind\nspots by applying different dilation factors to skip-connected features.\nExperimental results on a word-level lipreading task using two large publicly\navailable datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that\nthe proposed method achieves performance comparable to state-of-the-art\nmethods. It achieved higher accuracy with fewer parameters and lower\nfloating-point operations compared to existing TCN-based backend architectures.\nMoreover, visualization results suggest that our approach effectively utilizes\ndiverse temporal features while preserving temporal continuity, presenting\nnotable advantages in lipreading systems. The code is available at our GitHub\nrepository:\nhttps://github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading", "AI": {"tldr": "论文提出TD3Net，一种结合密集跳跃连接和多扩张时间卷积的后端架构，用于解决唇读任务中时间连续性问题，性能优于现有方法。", "motivation": "现有方法在建模唇部运动时存在时间连续性信息丢失的问题，限制了性能。", "method": "提出TD3Net，结合密集跳跃连接和多扩张时间卷积，覆盖更广且无盲区的感受野。", "result": "在两个大型公开数据集上表现优异，参数和计算量更少，准确率更高。", "conclusion": "TD3Net有效利用多样化时间特征，保持时间连续性，在唇读系统中具有显著优势。"}}
{"id": "2506.15708", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15708", "abs": "https://arxiv.org/abs/2506.15708", "authors": ["Falih Gozi Febrinanto", "Adonia Simango", "Chengpei Xu", "Jingjing Zhou", "Jiangang Ma", "Sonika Tyagi", "Feng Xia"], "title": "Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification", "comment": null, "summary": "Graph neural networks (GNNs) have been developed to model the relationship\nbetween regions of interest (ROIs) in brains and have shown significant\nimprovement in detecting brain diseases. However, most of these frameworks do\nnot consider the intrinsic relationship of causality factor between brain ROIs,\nwhich is arguably more essential to observe cause and effect interaction\nbetween signals rather than typical correlation values. We propose a novel\nframework called CGB (Causal Graphs for Brains) for brain disease\nclassification/detection, which models refined brain networks based on the\ncausal discovery method, transfer entropy, and geometric curvature strategy.\nCGB unveils causal relationships between ROIs that bring vital information to\nenhance brain disease classification performance. Furthermore, CGB also\nperforms a graph rewiring through a geometric curvature strategy to refine the\ngenerated causal graph to become more expressive and reduce potential\ninformation bottlenecks when GNNs model it. Our extensive experiments show that\nCGB outperforms state-of-the-art methods in classification tasks on brain\ndisease datasets, as measured by average F1 scores.", "AI": {"tldr": "论文提出了一种名为CGB的新框架，通过因果发现方法和几何曲率策略改进脑网络建模，提升脑疾病分类性能。", "motivation": "现有GNN框架未充分考虑脑区间的因果关系，而因果关系比传统相关性更能揭示信号间的因果交互。", "method": "结合因果发现方法（转移熵）和几何曲率策略，构建因果图并进行图重连。", "result": "CGB在脑疾病分类任务中表现优于现有方法，F1分数更高。", "conclusion": "CGB通过建模因果关系和优化图结构，显著提升了脑疾病分类的准确性和表达能力。"}}
{"id": "2506.16301", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16301", "abs": "https://arxiv.org/abs/2506.16301", "authors": ["Nadine Imholz", "Maurice Brunner", "Nicolas Baumann", "Edoardo Ghignone", "Michele Magno"], "title": "M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing", "comment": null, "summary": "Unrestricted multi-agent racing presents a significant research challenge,\nrequiring decision-making at the limits of a robot's operational capabilities.\nWhile previous approaches have either ignored spatiotemporal information in the\ndecision-making process or been restricted to single-opponent scenarios, this\nwork enables arbitrary multi-opponent head-to-head racing while considering the\nopponents' future intent. The proposed method employs a KF-based multi-opponent\ntracker to effectively perform opponent ReID by associating them across\nobservations. Simultaneously, spatial and velocity GPR is performed on all\nobserved opponent trajectories, providing predictive information to compute the\novertaking maneuvers. This approach has been experimentally validated on a\nphysical 1:10 scale autonomous racing car, achieving an overtaking success rate\nof up to 91.65% and demonstrating an average 10.13%-point improvement in safety\nat the same speed as the previous SotA. These results highlight its potential\nfor high-performance autonomous racing.", "AI": {"tldr": "本文提出了一种基于KF的多对手跟踪器和GPR的方法，用于多对手赛车场景，显著提高了超车成功率和安全性。", "motivation": "多对手赛车是一个复杂的研究挑战，需要在高动态环境中考虑对手的未来意图，而现有方法要么忽略时空信息，要么仅限于单对手场景。", "method": "使用KF多对手跟踪器进行对手重识别，并结合空间和速度GPR预测对手轨迹，以计算超车策略。", "result": "在1:10比例自动驾驶赛车上实验验证，超车成功率高达91.65%，安全性平均提高10.13%。", "conclusion": "该方法在高性能自动驾驶赛车中具有潜力，显著提升了多对手场景下的表现。"}}
{"id": "2506.16051", "categories": ["cs.LG", "cs.DB", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16051", "abs": "https://arxiv.org/abs/2506.16051", "authors": ["Zhiwei Li", "Carl Kesselman", "Tran Huy Nguyen", "Benjamin Yixing Xu", "Kyle Bolo", "Kimberley Yu"], "title": "From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience", "comment": null, "summary": "Reproducibility remains a central challenge in machine learning (ML),\nespecially in collaborative eScience projects where teams iterate over data,\nfeatures, and models. Current ML workflows are often dynamic yet fragmented,\nrelying on informal data sharing, ad hoc scripts, and loosely connected tools.\nThis fragmentation impedes transparency, reproducibility, and the adaptability\nof experiments over time. This paper introduces a data-centric framework for\nlifecycle-aware reproducibility, centered around six structured artifacts:\nDataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These\nartifacts formalize the relationships between data, code, and decisions,\nenabling ML experiments to be versioned, interpretable, and traceable over\ntime. The approach is demonstrated through a clinical ML use case of glaucoma\ndetection, illustrating how the system supports iterative exploration, improves\nreproducibility, and preserves the provenance of collaborative decisions across\nthe ML lifecycle.", "AI": {"tldr": "论文提出了一种数据为中心的框架，通过六个结构化工件（数据集、特征、工作流、执行、资产和控制词汇）提升机器学习实验的可重复性和可追溯性。", "motivation": "当前机器学习工作流程动态但碎片化，缺乏透明度和可重复性，尤其在协作项目中。", "method": "引入一个数据为中心的框架，定义六个结构化工件，用于版本化、解释和追踪ML实验。", "result": "通过青光眼检测的临床案例展示了该框架如何支持迭代探索、提升可重复性并保留决策来源。", "conclusion": "该框架为协作机器学习项目提供了生命周期感知的可重复性解决方案。"}}
{"id": "2506.16082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16082", "abs": "https://arxiv.org/abs/2506.16082", "authors": ["Yizhe Li", "Sanping Zhou", "Zheng Qin", "Le Wang"], "title": "PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning", "comment": null, "summary": "Dense video captioning is a challenging task that aims to localize and\ncaption multiple events in an untrimmed video. Recent studies mainly follow the\ntransformer-based architecture to jointly perform the two sub-tasks, i.e.,\nevent localization and caption generation, in an end-to-end manner. Based on\nthe general philosophy of detection transformer, these methods implicitly learn\nthe event locations and event semantics, which requires a large amount of\ntraining data and limits the model's performance in practice. In this paper, we\npropose a novel dense video captioning framework, named PR-DETR, which injects\nthe explicit position and relation prior into the detection transformer to\nimprove the localization accuracy and caption quality, simultaneously. On the\none hand, we first generate a set of position-anchored queries to provide the\nscene-specific position and semantic information about potential events as\nposition prior, which serves as the initial event search regions to eliminate\nthe implausible event proposals. On the other hand, we further design an event\nrelation encoder to explicitly calculate the relationship between event\nboundaries as relation prior to guide the event interaction to improve the\nsemantic coherence of the captions. Extensive ablation studies are conducted to\nverify the effectiveness of the position and relation prior. Experimental\nresults also show the competitive performance of our method on ActivityNet\nCaptions and YouCook2 datasets.", "AI": {"tldr": "PR-DETR框架通过显式位置和关系先验改进密集视频描述任务，提升事件定位和字幕生成质量。", "motivation": "现有基于Transformer的方法隐式学习事件位置和语义，依赖大量训练数据且性能受限。", "method": "引入位置锚定查询和事件关系编码器，显式注入位置和关系先验。", "result": "在ActivityNet Captions和YouCook2数据集上表现优异。", "conclusion": "显式先验显著提升密集视频描述任务的性能。"}}
{"id": "2506.15709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15709", "abs": "https://arxiv.org/abs/2506.15709", "authors": ["Pedro C. Vieira", "Miguel E. P. Silva", "Pedro Manuel Pinto Ribeiro"], "title": "Studying and Improving Graph Neural Network-based Motif Estimation", "comment": "This manuscript represents a revised version from the paper on\n  https://openreview.net/forum?id=PZVVOeu6xx. Still a work in progress.\n  Comments are welcome! 23 pages (12 main text + references), 9 figures, 5\n  tables", "summary": "Graph Neural Networks (GNNs) are a predominant method for graph\nrepresentation learning. However, beyond subgraph frequency estimation, their\napplication to network motif significance-profile (SP) prediction remains\nunder-explored, with no established benchmarks in the literature. We propose to\naddress this problem, framing SP estimation as a task independent of subgraph\nfrequency estimation. Our approach shifts from frequency counting to direct SP\nestimation and modulates the problem as multitarget regression. The\nreformulation is optimised for interpretability, stability and scalability on\nlarge graphs. We validate our method using a large synthetic dataset and\nfurther test it on real-world graphs. Our experiments reveal that 1-WL limited\nmodels struggle to make precise estimations of SPs. However, they can\ngeneralise to approximate the graph generation processes of networks by\ncomparing their predicted SP with the ones originating from synthetic\ngenerators. This first study on GNN-based motif estimation also hints at how\nusing direct SP estimation can help go past the theoretical limitations that\nmotif estimation faces when performed through subgraph counting.", "AI": {"tldr": "该论文提出了一种直接估计网络模体显著性分布（SP）的方法，避免了传统子图频率估计的限制，并通过多目标回归优化了可解释性、稳定性和可扩展性。", "motivation": "现有图神经网络（GNNs）在网络模体SP预测方面缺乏研究，且没有基准测试。论文旨在填补这一空白，将SP估计独立于子图频率估计。", "method": "将SP估计重新定义为多目标回归问题，直接从图中学习SP，而非依赖子图频率计数。方法优化了可解释性、稳定性和大规模图的可扩展性。", "result": "实验表明，1-WL受限模型在精确估计SP方面表现不佳，但能通过比较预测SP与合成生成器的SP，近似网络生成过程。", "conclusion": "这是首个基于GNN的模体估计研究，表明直接SP估计可以突破子图计数带来的理论限制。"}}
{"id": "2506.16112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16112", "abs": "https://arxiv.org/abs/2506.16112", "authors": ["Yuan Zhang", "Chun-Kai Fan", "Tao Huang", "Ming Lu", "Sicheng Yu", "Junwen Pan", "Kuan Cheng", "Qi She", "Shanghang Zhang"], "title": "AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models", "comment": "19 pages", "summary": "Inspired by text prompts in large language models (LLMs), visual prompts have\nbeen explored to enhance the reasoning capabilities of large vision-language\nmodels (LVLMs). Current methods design heuristic visual prompts, such as\noverlaying a text-query-guided attention heatmap on the original input image.\nHowever, designing effective prompts manually is challenging and\ntime-consuming, and it often fails to explore the benefits of different visual\nprompts, leading to sub-optimal performance. To this end, we propose\n\\textbf{AutoV} that learns to automatically select the optimal visual prompt\nfrom various candidates based on given textual queries and the input image. To\ntrain AutoV, we developed an automatic data collection and labeling pipeline\nthat evaluates various visual prompts with a pre-trained LVLM. We input a set\nof visual prompts into the LVLM and rank them according to the prediction\nlosses generated by the model. Using the ranking as a supervision signal, we\ntrain AutoV to automatically choose the optimal visual prompt from various\nvisual prompts for LVLMs. Experimental results indicate that AutoV enhances the\nperformance of various LVLMs across multiple popular image understanding tasks.\nFor instance, LLaVA-OV with AutoV achieves $\\textbf{1.7}\\%$ accuracy gain on\nLLaVA$^{\\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\\textbf{1.9}\\%$ on MMMU,\nhighlighting its potential as an optimal visual prompting method for LVLMs.", "AI": {"tldr": "AutoV是一种自动选择最优视觉提示的方法，通过训练模型从多个候选提示中选择最佳选项，显著提升了大型视觉语言模型（LVLM）的性能。", "motivation": "手动设计视觉提示耗时且效果有限，无法充分利用不同提示的潜力，因此需要一种自动化的方法来优化提示选择。", "method": "提出AutoV，通过自动数据收集和标注流程，利用预训练的LVLM评估和排名视觉提示，训练模型自动选择最优提示。", "result": "实验表明，AutoV显著提升了多种LVLM的性能，例如LLaVA-OV和Qwen2.5-VL在多个任务中准确率分别提高了1.7%和1.9%。", "conclusion": "AutoV作为一种自动视觉提示方法，具有潜力成为LVLM的最优提示选择工具。"}}
{"id": "2506.15710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15710", "abs": "https://arxiv.org/abs/2506.15710", "authors": ["Siru Ouyang", "Xinyu Zhu", "Zilin Xiao", "Minhao Jiang", "Yu Meng", "Jiawei Han"], "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer", "comment": null, "summary": "Reinforcement learning (RL) has become a powerful approach for improving the\nreasoning capabilities of large language models (LLMs), as evidenced by recent\nsuccesses such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale\nremains intimidatingly resource-intensive, requiring multiple model copies and\nextensive GPU workloads. On the other hand, while being powerful, recent\nstudies suggest that RL does not fundamentally endow models with new knowledge;\nrather, it primarily reshapes the model's output distribution to activate\nreasoning capabilities latent in the base model. Building on this insight, we\nhypothesize that the changes in output probabilities induced by RL are largely\nmodel-size invariant, opening the door to a more efficient paradigm: training a\nsmall model with RL and transferring its induced probability shifts to larger\nbase models. To verify our hypothesis, we conduct a token-level analysis of\ndecoding trajectories and find high alignment in RL-induced output\ndistributions across model scales, validating our hypothesis. Motivated by\nthis, we propose RAST, a simple yet effective method that transfers reasoning\nbehaviors by injecting RL-induced probability adjustments from a small\nRL-trained model into larger models. Experiments across multiple mathematical\nreasoning benchmarks show that RAST substantially and consistently enhances the\nreasoning capabilities of base models while requiring significantly lower GPU\nmemory than direct RL training, sometimes even yielding better performance than\nthe RL-trained counterparts. Our findings offer new insights into the nature of\nRL-driven reasoning and practical strategies for scaling its benefits without\nincurring its full computational cost. The project page of RAST is available at\nhttps://ozyyshr.github.io/RAST/.", "AI": {"tldr": "论文提出了一种名为RAST的高效方法，通过将小模型RL训练后的概率调整转移到大型基础模型中，显著提升了推理能力，同时降低了计算资源需求。", "motivation": "现有RL方法在提升LLM推理能力时资源消耗巨大，且RL并未赋予模型新知识，而是调整输出分布。作者假设RL诱导的概率变化与模型规模无关，从而探索更高效的训练范式。", "method": "提出RAST方法，通过分析解码轨迹验证假设，并将小模型RL训练后的概率调整注入大型基础模型。", "result": "实验表明，RAST显著提升了基础模型的推理能力，且计算资源需求远低于直接RL训练，有时性能甚至优于RL训练模型。", "conclusion": "研究揭示了RL驱动推理的本质，并提供了一种无需高计算成本的实用策略。"}}
{"id": "2506.16537", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16537", "abs": "https://arxiv.org/abs/2506.16537", "authors": ["Sreeja Roy-Singh", "Alan P. Li", "Vinay Ravindra", "Roderick Lammers", "Marc Sanchez Net"], "title": "Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods", "comment": null, "summary": "Fully re-orientable small spacecraft are now supported by commercial\ntechnologies, allowing them to point their instruments in any direction and\ncapture images, with short notice. When combined with improved onboard\nprocessing, and implemented on a constellation of inter-communicable\nsatellites, this intelligent agility can significantly increase responsiveness\nto transient or evolving phenomena. We demonstrate a ground-based and onboard\nalgorithmic framework that combines orbital mechanics, attitude control,\ninter-satellite communication, intelligent prediction and planning to schedule\nthe time-varying, re-orientation of agile, small satellites in a constellation.\nPlanner intelligence is improved by updating the predictive value of future\nspace-time observations based on shared observations of evolving episodic\nprecipitation and urban flood forecasts. Reliable inter-satellite communication\nwithin a fast, dynamic constellation topology is modeled in the physical,\naccess control and network layer. We apply the framework on a representative\n24-satellite constellation observing 5 global regions. Results show\nappropriately low latency in information exchange (average within 1/3rd\navailable time for implicit consensus), enabling the onboard scheduler to\nobserve ~7% more flood magnitude than a ground-based implementation. Both\nonboard and offline versions performed ~98% better than constellations without\nagility.", "AI": {"tldr": "论文提出了一种基于地面和星载算法的框架，用于调度可灵活调整方向的小型卫星星座，以提高对瞬态或动态现象的响应能力。", "motivation": "利用商业技术支持的完全可定向小型航天器，结合星载处理和星座内通信，提升对动态现象的观测效率。", "method": "结合轨道力学、姿态控制、卫星间通信、智能预测和规划，设计算法框架，并通过24颗卫星的星座进行验证。", "result": "星载调度器观测到的洪水幅度比地面实现多7%，且两种实现方式比非灵活星座表现好98%。", "conclusion": "该框架显著提升了卫星星座对动态现象的响应能力，验证了智能灵活调度的有效性。"}}
{"id": "2506.16356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16356", "abs": "https://arxiv.org/abs/2506.16356", "authors": ["Aman Singh", "Deepak Kapa", "Prasham Chedda", "Shishir N. Y. Kolathaya"], "title": "Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots", "comment": "6 pages, 5 figures, Accepted at Advances in Robotics 2025", "summary": "Legged robots, such as quadrupeds and humanoids, require high-performance\nactuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with\nsingle-stage planetary gearboxes offer low inertia, high efficiency, and\ntransparency. Among planetary gearbox architectures, Internal (ISSPG) and\nExternal Single-Stage Planetary Gearbox (ESSPG) are the two predominant\ndesigns. While ISSPG is often preferred for its compactness and high torque\ndensity at certain gear ratios, no objective comparison between the two\narchitectures exists. Additionally, existing designs rely on heuristics rather\nthan systematic optimization. This paper presents a design framework for\noptimally selecting actuator parameters based on given performance requirements\nand motor specifications. Using this framework, we generate and analyze various\noptimized gearbox designs for both architectures. Our results demonstrate that\nfor the T-motor U12, ISSPG is the superior choice within the lower gear ratio\nrange of 5:1 to 7:1, offering a lighter design. However, for gear ratios\nexceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the\n7:1 to 11:1 range. To validate our approach, we designed and optimized two\nactuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with\na 7.2:1 gear ratio. Their respective masses closely align with our optimization\nmodel predictions, confirming the effectiveness of our methodology.", "AI": {"tldr": "本文提出了一种用于优化选择执行器参数的设计框架，比较了两种行星齿轮箱架构（ISSPG和ESSPG）的性能，并通过实验验证了其有效性。", "motivation": "缺乏对ISSPG和ESSPG两种行星齿轮箱架构的客观比较，且现有设计依赖启发式方法而非系统优化。", "method": "提出设计框架，基于性能需求和电机规格优化执行器参数，生成并分析两种架构的优化齿轮箱设计。", "result": "对于T-motor U12，ISSPG在5:1至7:1的低传动比范围内更优，而ESSPG在7:1至11:1范围内更优。实验验证了优化模型的有效性。", "conclusion": "设计框架能够有效优化执行器参数，为不同传动比范围选择最佳架构提供了依据。"}}
{"id": "2506.16119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16119", "abs": "https://arxiv.org/abs/2506.16119", "authors": ["Chengyu Bai", "Yuming Li", "Zhongyu Zhao", "Jintao Chen", "Peidong Jia", "Qi She", "Ming Lu", "Shanghang Zhang"], "title": "FastInit: Fast Noise Initialization for Temporally Consistent Video Generation", "comment": null, "summary": "Video generation has made significant strides with the development of\ndiffusion models; however, achieving high temporal consistency remains a\nchallenging task. Recently, FreeInit identified a training-inference gap and\nintroduced a method to iteratively refine the initial noise during inference.\nHowever, iterative refinement significantly increases the computational cost\nassociated with video generation. In this paper, we introduce FastInit, a fast\nnoise initialization method that eliminates the need for iterative refinement.\nFastInit learns a Video Noise Prediction Network (VNPNet) that takes random\nnoise and a text prompt as input, generating refined noise in a single forward\npass. Therefore, FastInit greatly enhances the efficiency of video generation\nwhile achieving high temporal consistency across frames. To train the VNPNet,\nwe create a large-scale dataset consisting of pairs of text prompts, random\nnoise, and refined noise. Extensive experiments with various text-to-video\nmodels show that our method consistently improves the quality and temporal\nconsistency of the generated videos. FastInit not only provides a substantial\nimprovement in video generation but also offers a practical solution that can\nbe applied directly during inference. The code and dataset will be released.", "AI": {"tldr": "FastInit提出了一种快速噪声初始化方法，通过单次前向传播生成高质量视频噪声，显著提升视频生成的效率和时序一致性。", "motivation": "现有方法如FreeInit通过迭代优化噪声提高了时序一致性，但计算成本高，FastInit旨在解决这一问题。", "method": "FastInit训练了一个视频噪声预测网络（VNPNet），输入随机噪声和文本提示，单次生成优化后的噪声。", "result": "实验表明，FastInit显著提升了生成视频的质量和时序一致性，且计算效率更高。", "conclusion": "FastInit为视频生成提供了一种高效且实用的解决方案，可直接应用于推理阶段。"}}
{"id": "2506.15711", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15711", "abs": "https://arxiv.org/abs/2506.15711", "authors": ["Le Jiang", "Liyan Ma", "Guang Yang"], "title": "Shadow defense against gradient inversion attack in federated learning", "comment": null, "summary": "Federated learning (FL) has emerged as a transformative framework for\nprivacy-preserving distributed training, allowing clients to collaboratively\ntrain a global model without sharing their local data. This is especially\ncrucial in sensitive fields like healthcare, where protecting patient data is\nparamount. However, privacy leakage remains a critical challenge, as the\ncommunication of model updates can be exploited by potential adversaries.\nGradient inversion attacks (GIAs), for instance, allow adversaries to\napproximate the gradients used for training and reconstruct training images,\nthus stealing patient privacy. Existing defense mechanisms obscure gradients,\nyet lack a nuanced understanding of which gradients or types of image\ninformation are most vulnerable to such attacks. These indiscriminate\ncalibrated perturbations result in either excessive privacy protection\ndegrading model accuracy, or insufficient one failing to safeguard sensitive\ninformation. Therefore, we introduce a framework that addresses these\nchallenges by leveraging a shadow model with interpretability for identifying\nsensitive areas. This enables a more targeted and sample-specific noise\ninjection. Specially, our defensive strategy achieves discrepancies of 3.73 in\nPSNR and 0.2 in SSIM compared to the circumstance without defense on the\nChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,\nit minimizes adverse effects on model performance, with less than 1\\% F1\nreduction compared to SOTA methods. Our extensive experiments, conducted across\ndiverse types of medical images, validate the generalization of the proposed\nframework. The stable defense improvements for FedAvg are consistently over\n1.5\\% times in LPIPS and SSIM. It also offers a universal defense against\nvarious GIA types, especially for these sensitive areas in images.", "AI": {"tldr": "论文提出了一种针对联邦学习中梯度反转攻击的防御框架，通过影子模型识别敏感区域并针对性注入噪声，平衡隐私保护与模型性能。", "motivation": "联邦学习中梯度反转攻击可能导致隐私泄露，现有防御方法缺乏针对性，影响模型准确性或保护不足。", "method": "利用可解释的影子模型识别敏感区域，进行样本特定的噪声注入。", "result": "在ChestXRay和EyePACS数据集上，PSNR和SSIM指标显著优于无防御情况，模型性能损失小于1%。", "conclusion": "该框架在多种医疗图像上验证了通用性，能有效防御不同类型的梯度反转攻击，尤其保护图像敏感区域。"}}
{"id": "2506.16546", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16546", "abs": "https://arxiv.org/abs/2506.16546", "authors": ["Liyang Yu", "Tianyi Wang", "Junfeng Jiao", "Fengwu Shan", "Hongqing Chu", "Bingzhao Gao"], "title": "BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios", "comment": "6 pages, 3 figures, 4 tables, accepted for IEEE Intelligent Vehicles\n  (IV) Symposium 2025", "summary": "In complex real-world traffic environments, autonomous vehicles (AVs) need to\ninteract with other traffic participants while making real-time and\nsafety-critical decisions accordingly. The unpredictability of human behaviors\nposes significant challenges, particularly in dynamic scenarios, such as\nmulti-lane highways and unsignalized T-intersections. To address this gap, we\ndesign a bi-level interaction decision-making algorithm (BIDA) that integrates\ninteractive Monte Carlo tree search (MCTS) with deep reinforcement learning\n(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs\nin dynamic key traffic scenarios. Specifically, we adopt three types of DRL\nalgorithms to construct a reliable value network and policy network, which\nguide the online deduction process of interactive MCTS by assisting in value\nupdate and node selection. Then, a dynamic trajectory planner and a trajectory\ntracking controller are designed and implemented in CARLA to ensure smooth\nexecution of planned maneuvers. Experimental evaluations demonstrate that our\nBIDA not only enhances interactive deduction and reduces computational costs,\nbut also outperforms other latest benchmarks, which exhibits superior safety,\nefficiency and interaction rationality under varying traffic conditions.", "AI": {"tldr": "提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互理性、效率和安全性。", "motivation": "自动驾驶车辆在复杂交通环境中需实时与人类交互并做出安全决策，但人类行为的不可预测性带来了挑战。", "method": "采用三种DRL算法构建值网络和策略网络，指导交互式MCTS的在线推理过程，并结合动态轨迹规划器和跟踪控制器在CARLA中实现。", "result": "实验表明BIDA提升了交互推理效率，降低了计算成本，并在多种交通条件下表现出更高的安全性、效率和交互理性。", "conclusion": "BIDA为自动驾驶车辆在动态交通场景中的交互决策提供了有效解决方案。"}}
{"id": "2506.16386", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16386", "abs": "https://arxiv.org/abs/2506.16386", "authors": ["Leesai Park", "Keunwoo Jang", "Sanghyun Kim"], "title": "CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance", "comment": null, "summary": "This paper proposes Constrained Sampling Cluster Model Predictive Path\nIntegral (CSC-MPPI), a novel constrained formulation of MPPI designed to\nenhance trajectory optimization while enforcing strict constraints on system\nstates and control inputs. Traditional MPPI, which relies on a probabilistic\nsampling process, often struggles with constraint satisfaction and generates\nsuboptimal trajectories due to the weighted averaging of sampled trajectories.\nTo address these limitations, the proposed framework integrates a primal-dual\ngradient-based approach and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) to steer sampled input trajectories into feasible regions\nwhile mitigating risks associated with weighted averaging. First, to ensure\nthat sampled trajectories remain within the feasible region, the primal-dual\ngradient method is applied to iteratively shift sampled inputs while enforcing\nstate and control constraints. Then, DBSCAN groups the sampled trajectories,\nenabling the selection of representative control inputs within each cluster.\nFinally, among the representative control inputs, the one with the lowest cost\nis chosen as the optimal action. As a result, CSC-MPPI guarantees constraint\nsatisfaction, improves trajectory selection, and enhances robustness in complex\nenvironments. Simulation and real-world experiments demonstrate that CSC-MPPI\noutperforms traditional MPPI in obstacle avoidance, achieving improved\nreliability and efficiency. The experimental videos are available at\nhttps://cscmppi.github.io", "AI": {"tldr": "CSC-MPPI是一种改进的MPPI方法，通过结合原始-对偶梯度方法和DBSCAN聚类，优化轨迹并严格满足约束条件。", "motivation": "传统MPPI在约束满足和轨迹优化上表现不佳，CSC-MPPI旨在解决这些问题。", "method": "结合原始-对偶梯度方法确保约束满足，使用DBSCAN聚类选择代表性控制输入。", "result": "CSC-MPPI在避障任务中优于传统MPPI，提高了可靠性和效率。", "conclusion": "CSC-MPPI通过严格约束和聚类优化，显著提升了轨迹优化性能。"}}
{"id": "2506.16129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16129", "abs": "https://arxiv.org/abs/2506.16129", "authors": ["Stefano Colamonaco", "David Debot", "Giuseppe Marra"], "title": "Neurosymbolic Object-Centric Learning with Distant Supervision", "comment": null, "summary": "Relational learning enables models to generalize across structured domains by\nreasoning over objects and their interactions. While recent advances in\nneurosymbolic reasoning and object-centric learning bring us closer to this\ngoal, existing systems rely either on object-level supervision or on a\npredefined decomposition of the input into objects. In this work, we propose a\nneurosymbolic formulation for learning object-centric representations directly\nfrom raw unstructured perceptual data and using only distant supervision. We\ninstantiate this approach in DeepObjectLog, a neurosymbolic model that\nintegrates a perceptual module, which extracts relevant object representations,\nwith a symbolic reasoning layer based on probabilistic logic programming. By\nenabling sound probabilistic logical inference, the symbolic component\nintroduces a novel learning signal that further guides the discovery of\nmeaningful objects in the input. We evaluate our model across a diverse range\nof generalization settings, including unseen object compositions, unseen tasks,\nand unseen number of objects. Experimental results show that our method\noutperforms neural and neurosymbolic baselines across the tested settings.", "AI": {"tldr": "提出了一种从原始非结构化感知数据中直接学习对象中心表示的方法DeepObjectLog，结合感知模块和符号推理层，无需对象级监督或预定义分解。", "motivation": "现有系统依赖对象级监督或预定义输入分解，限制了泛化能力。", "method": "结合感知模块提取对象表示与基于概率逻辑编程的符号推理层，通过概率逻辑推理提供学习信号。", "result": "在多种泛化场景（如未见对象组合、任务和数量）中表现优于神经和神经符号基线。", "conclusion": "DeepObjectLog通过神经符号方法有效学习对象中心表示，提升了泛化能力。"}}
{"id": "2506.15712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15712", "abs": "https://arxiv.org/abs/2506.15712", "authors": ["Songqi Zhou", "Ruixue Liu", "Yixing Wang", "Jia Lu", "Benben Jiang"], "title": "BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling", "comment": null, "summary": "Accurate fault detection in lithium-ion batteries is essential for the safe\nand reliable operation of electric vehicles and energy storage systems.\nHowever, existing methods often struggle to capture complex temporal\ndependencies and cannot fully leverage abundant unlabeled data. Although large\nlanguage models (LLMs) exhibit strong representation capabilities, their\narchitectures are not directly suited to the numerical time-series data common\nin industrial settings. To address these challenges, we propose a novel\nframework that adapts BERT-style pretraining for battery fault detection by\nextending the standard BERT architecture with a customized time-series-to-token\nrepresentation module and a point-level Masked Signal Modeling (point-MSM)\npretraining task tailored to battery applications. This approach enables\nself-supervised learning on sequential current, voltage, and other\ncharge-discharge cycle data, yielding distributionally robust, context-aware\ntemporal embeddings. We then concatenate these embeddings with battery metadata\nand feed them into a downstream classifier for accurate fault classification.\nExperimental results on a large-scale real-world dataset show that models\ninitialized with our pretrained parameters significantly improve both\nrepresentation quality and classification accuracy, achieving an AUROC of 0.945\nand substantially outperforming existing approaches. These findings validate\nthe effectiveness of BERT-style pretraining for time-series fault detection.", "AI": {"tldr": "论文提出了一种基于BERT风格预训练的新框架，用于锂离子电池故障检测，通过定制时间序列表示模块和点级掩码信号建模任务，显著提升了分类精度。", "motivation": "现有方法难以捕捉复杂时间依赖关系且无法充分利用未标记数据，而大型语言模型（LLMs）的架构不适用于工业场景中的数值时间序列数据。", "method": "扩展标准BERT架构，引入时间序列到令牌表示模块和点级掩码信号建模（point-MSM）预训练任务，结合电池元数据进行下游分类。", "result": "在真实数据集上，预训练参数显著提升了表示质量和分类精度，AUROC达到0.945，优于现有方法。", "conclusion": "验证了BERT风格预训练在时间序列故障检测中的有效性。"}}
{"id": "2506.16555", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16555", "abs": "https://arxiv.org/abs/2506.16555", "authors": ["Melih Özcan", "Ozgur S. Oguz"], "title": "An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation", "comment": "8 pages, 8 figures, accepted for oral presentation at IROS 2025.\n  Supplementary site: https://sites.google.com/view/komo-force/home", "summary": "Robotic manipulation demands precise control over both contact forces and\nmotion trajectories. While force control is essential for achieving compliant\ninteraction and high-frequency adaptation, it is limited to operations in close\nproximity to the manipulated object and often fails to maintain stable\norientation during extended motion sequences. Conversely, optimization-based\nmotion planning excels in generating collision-free trajectories over the\nrobot's configuration space but struggles with dynamic interactions where\ncontact forces play a crucial role. To address these limitations, we propose a\nmulti-modal control framework that combines force control and\noptimization-augmented motion planning to tackle complex robotic manipulation\ntasks in a sequential manner, enabling seamless switching between control modes\nbased on task requirements. Our approach decomposes complex tasks into\nsubtasks, each dynamically assigned to one of three control modes: Pure\noptimization for global motion planning, pure force control for precise\ninteraction, or hybrid control for tasks requiring simultaneous trajectory\ntracking and force regulation. This framework is particularly advantageous for\nbimanual and multi-arm manipulation, where synchronous motion and coordination\namong arms are essential while considering both the manipulated object and\nenvironmental constraints. We demonstrate the versatility of our method through\na range of long-horizon manipulation tasks, including single-arm, bimanual, and\nmulti-arm applications, highlighting its ability to handle both free-space\nmotion and contact-rich manipulation with robustness and precision.", "AI": {"tldr": "提出了一种结合力控制和优化运动规划的多模态控制框架，用于复杂机器人操作任务。", "motivation": "力控制在高频适应和柔顺交互中表现优异，但在远距离运动和稳定方向上受限；优化运动规划擅长生成无碰撞轨迹，但在动态交互中表现不足。", "method": "将任务分解为子任务，动态分配三种控制模式：纯优化、纯力控制或混合控制。", "result": "在单臂、双臂和多臂操作任务中展示了方法的鲁棒性和精确性。", "conclusion": "该框架能够无缝切换控制模式，适用于自由空间运动和接触丰富的操作任务。"}}
{"id": "2506.16141", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16141", "abs": "https://arxiv.org/abs/2506.16141", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Junhao Cheng", "Ying Shan", "Xihui Liu"], "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning", "comment": "Code released at: https://github.com/TencentARC/GRPO-CARE", "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.", "AI": {"tldr": "论文提出GRPO-CARE框架，通过双重奖励机制提升多模态大语言模型（MLLMs）的答案准确性和推理一致性，并在SEED-Bench-R1基准测试中表现优于标准GRPO。", "motivation": "现有强化学习方法（如GRPO）在MLLMs中的适应性和评估不足，尤其是推理步骤与答案的逻辑一致性。", "method": "提出GRPO-CARE框架，结合基础答案正确性奖励和自适应一致性奖励，替代KL惩罚，优化推理路径。", "result": "GRPO-CARE在SEED-Bench-R1上性能提升6.7%，一致性提高24.5%，并展示出强迁移能力。", "conclusion": "GRPO-CARE为MLLMs的后训练提供了可推广的框架，推动了更可解释和鲁棒的模型发展。"}}
{"id": "2506.15713", "categories": ["cs.LG", "physics.data-an", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2506.15713", "abs": "https://arxiv.org/abs/2506.15713", "authors": ["Michael T. M. B. Morris-Thomas", "Marius Martens"], "title": "An application of machine learning to the motion response prediction of floating assets", "comment": "17 pages, 6 figures", "summary": "The real-time prediction of floating offshore asset behavior under stochastic\nmetocean conditions remains a significant challenge in offshore engineering.\nWhile traditional empirical and frequency-domain methods work well in benign\nconditions, they struggle with both extreme sea states and nonlinear responses.\nThis study presents a supervised machine learning approach using multivariate\nregression to predict the nonlinear motion response of a turret-moored vessel\nin 400 m water depth. We developed a machine learning workflow combining a\ngradient-boosted ensemble method with a custom passive weathervaning solver,\ntrained on approximately $10^6$ samples spanning 100 features. The model\nachieved mean prediction errors of less than 5% for critical mooring parameters\nand vessel heading accuracy to within 2.5 degrees across diverse metocean\nconditions, significantly outperforming traditional frequency-domain methods.\nThe framework has been successfully deployed on an operational facility,\ndemonstrating its efficacy for real-time vessel monitoring and operational\ndecision-making in offshore environments.", "AI": {"tldr": "本文提出了一种基于监督机器学习的多元回归方法，用于预测浮式海上资产在随机海洋条件下的非线性运动响应，显著优于传统频域方法。", "motivation": "实时预测浮式海上资产在随机海洋条件下的行为是海上工程中的重大挑战，传统方法在极端海况和非线性响应中表现不佳。", "method": "采用梯度提升集成方法与自定义被动转向求解器结合的机器学习工作流程，训练了约100万样本和100个特征。", "result": "模型对关键系泊参数的平均预测误差小于5%，船舶航向精度在2.5度以内，显著优于传统方法。", "conclusion": "该框架已成功应用于实际设施，证明了其在实时船舶监测和海上操作决策中的有效性。"}}
{"id": "2506.16593", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16593", "abs": "https://arxiv.org/abs/2506.16593", "authors": ["Nicolas Samson", "William Larrivée-Hardy", "William Dubois", "Élie Roy-Brouard", "Edith Brotherton", "Dominic Baril", "Julien Lépine", "François Pomerleau"], "title": "DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty", "comment": "This version is the preprint of a journal article with the same\n  title, accepted in the IEEE Transactions on Field Robotics. To have a look at\n  the early access version, use the following link\n  https://ieeexplore.ieee.org/document/11037776", "summary": "Off-road autonomous navigation is a challenging task as it is mainly\ndependent on the accuracy of the motion model. Motion model performances are\nlimited by their ability to predict the interaction between the terrain and the\nUGV, which an onboard sensor can not directly measure. In this work, we propose\nusing the DRIVE protocol to standardize the collection of data for system\nidentification and characterization of the slip state space. We validated this\nprotocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on\nsix terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9\nhours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to\nexplore the velocity command space and identify the reachable velocities for\nterrain-robot interactions. We investigated the transfer function between the\ncommand velocity space and the resulting steady-state slip for an SSMR. An\nunpredictability metric is proposed to estimate command uncertainty and help\nassess risk likelihood and severity in deployment. Finally, we share our\nlessons learned on running system identification on large UGV to help the\ncommunity.", "AI": {"tldr": "论文提出使用DRIVE协议标准化数据收集，用于系统识别和滑移状态空间表征，验证了协议在多种地形和平台上的有效性，并提出了不可预测性度量以评估风险。", "motivation": "越野自主导航依赖于运动模型的准确性，但运动模型预测地形与无人地面车辆（UGV）交互的能力有限，需要标准化数据收集方法。", "method": "使用DRIVE协议收集数据，验证其在两种平台（75 kg至470 kg）和六种地形上的有效性，分析速度命令空间与稳态滑移的传递函数。", "result": "DRIVE协议能有效探索速度命令空间并识别可达速度，提出了不可预测性度量以评估风险。", "conclusion": "DRIVE协议为UGV系统识别提供了标准化方法，并分享了在大型UGV上运行系统识别的经验。"}}
{"id": "2506.16475", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16475", "abs": "https://arxiv.org/abs/2506.16475", "authors": ["Yaru Niu", "Yunzhe Zhang", "Mingyang Yu", "Changyi Lin", "Chenhao Li", "Yikai Wang", "Yuxiang Yang", "Wenhao Yu", "Tingnan Zhang", "Bingqing Chen", "Jonathan Francis", "Zhenzhen Li", "Jie Tan", "Ding Zhao"], "title": "Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining", "comment": null, "summary": "Quadrupedal robots have demonstrated impressive locomotion capabilities in\ncomplex environments, but equipping them with autonomous versatile manipulation\nskills in a scalable way remains a significant challenge. In this work, we\nintroduce a cross-embodiment imitation learning system for quadrupedal\nmanipulation, leveraging data collected from both humans and LocoMan, a\nquadruped equipped with multiple manipulation modes. Specifically, we develop a\nteleoperation and data collection pipeline, which unifies and modularizes the\nobservation and action spaces of the human and the robot. To effectively\nleverage the collected data, we propose an efficient modularized architecture\nthat supports co-training and pretraining on structured modality-aligned data\nacross different embodiments. Additionally, we construct the first manipulation\ndataset for the LocoMan robot, covering various household tasks in both\nunimanual and bimanual modes, supplemented by a corresponding human dataset. We\nvalidate our system on six real-world manipulation tasks, where it achieves an\naverage success rate improvement of 41.9% overall and 79.7% under\nout-of-distribution (OOD) settings compared to the baseline. Pretraining with\nhuman data contributes a 38.6% success rate improvement overall and 82.7% under\nOOD settings, enabling consistently better performance with only half the\namount of robot data. Our code, hardware, and data are open-sourced at:\nhttps://human2bots.github.io.", "AI": {"tldr": "本文提出了一种跨体现模仿学习系统，用于四足机器人的多功能操作，通过结合人类和机器人数据，显著提升了任务成功率。", "motivation": "四足机器人虽然在复杂环境中表现出色，但其多功能自主操作能力仍面临挑战，需要一种可扩展的解决方案。", "method": "开发了统一的遥操作和数据收集流程，提出模块化架构支持跨体现数据的联合训练和预训练，并构建了首个四足机器人操作数据集。", "result": "在六项真实任务中，系统平均成功率提升41.9%，OOD场景下提升79.7%；预训练人类数据进一步带来38.6%和82.7%的提升。", "conclusion": "该系统通过跨体现数据利用显著提升了四足机器人的操作能力，且开源了代码、硬件和数据。"}}
{"id": "2506.16157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16157", "abs": "https://arxiv.org/abs/2506.16157", "authors": ["Xingbai Chen", "Tingchao Fu", "Renyang Liu", "Wei Zhou", "Chao Yi"], "title": "MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models", "comment": "17 pages, 5pages", "summary": "Referring Expression Segmentation (RES) enables precise object segmentation\nin images based on natural language descriptions, offering high flexibility and\nbroad applicability in real-world vision tasks. Despite its impressive\nperformance, the robustness of RES models against adversarial examples remains\nlargely unexplored. While prior adversarial attack methods have explored\nadversarial robustness on conventional segmentation models, they perform poorly\nwhen directly applied to RES, failing to expose vulnerabilities in its\nmultimodal structure. Moreover, in practical open-world scenarios, users\ntypically issue multiple, diverse referring expressions to interact with the\nsame image, highlighting the need for adversarial examples that generalize\nacross varied textual inputs. To address these multimodal challenges, we\npropose a novel adversarial attack strategy termed \\textbf{Multimodal\nBidirectional Attack}, tailored for RES models. Our method introduces learnable\nproxy textual embedding perturbation and jointly performs visual-aligned\noptimization on the image modality and textual-adversarial optimization on the\ntextual modality during attack generation. This dual optimization framework\nencourages adversarial images to actively adapt to more challenging text\nembedding during optimization, thereby enhancing their cross-text\ntransferability, which refers to the ability of adversarial examples to remain\neffective under a variety of unseen or semantically diverse textual inputs.\nExtensive experiments conducted on multiple RES models and benchmark datasets\ndemonstrate the superior effectiveness of our method compared to existing\nmethods.", "AI": {"tldr": "提出了一种针对RES模型的多模态双向攻击方法，通过联合优化图像和文本模态，提升对抗样本的跨文本迁移能力。", "motivation": "RES模型在多模态场景下的对抗鲁棒性尚未充分研究，现有攻击方法在RES上表现不佳，且实际应用中需要对抗样本能适应多样化的文本输入。", "method": "提出多模态双向攻击策略，结合可学习的代理文本嵌入扰动和视觉对齐优化，同时优化图像和文本模态。", "result": "实验证明该方法在多个RES模型和数据集上优于现有方法。", "conclusion": "该方法有效提升了对抗样本的跨文本迁移能力，填补了RES模型对抗鲁棒性研究的空白。"}}
{"id": "2506.15714", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15714", "abs": "https://arxiv.org/abs/2506.15714", "authors": ["Andrew Kiruluta"], "title": "Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention", "comment": null, "summary": "We propose an innovative, learnable two-sided short-time Laplace transform\n(STLT) mechanism to supplant the traditional self attention in\ntransformer-based LLMs. Our STLT introduces trainable parameters for each\nLaplace node, enabling end-to-end learning of decay rates , oscillatory\nfrequencies, and window bandwidth T. This flexibility allows the model to\ndynamically adapt token relevance half lives and frequency responses during\ntraining. By selecting S learnable nodes and leveraging fast recursive\nconvolution, we achieve an effective complexity of in time and memory. We\nfurther incorporate an efficient FFT-based computation of the relevance matrix\nand an adaptive node allocation mechanism to dynamically adjust the number of\nactive Laplace nodes. Empirical results on language modeling (WikiText\\-103,\nProject Gutenberg), machine translation (WMT'14 En\\-De), and long document\nquestion answering (NarrativeQA) demonstrate that our learnable STLT achieves\nperplexities and scores on par with or better than existing efficient\ntransformers while naturally extending to context lengths exceeding 100k tokens\nor more limited only by available hardware. Ablation studies confirm the\nimportance of learnable parameters and adaptive node allocation. The proposed\napproach combines interpretability, through explicit decay and frequency\nparameters, with scalability and robustness, offering a pathway towards\nultra-long-sequence language modeling without the computational bottleneck of\nself-attention.", "AI": {"tldr": "提出了一种可学习的双短时拉普拉斯变换（STLT）机制，替代传统Transformer中的自注意力，实现动态调整token相关性和频率响应，支持超长序列建模。", "motivation": "传统自注意力机制在长序列建模中存在计算瓶颈，STLT通过可学习参数和自适应节点分配，提供了一种可解释且高效的方法。", "method": "引入可训练的拉普拉斯节点参数，结合快速递归卷积和FFT计算，动态调整节点数量和带宽，实现高效复杂度。", "result": "在语言建模、机器翻译和长文档问答任务中，STLT性能与现有高效Transformer相当或更优，支持超过100k tokens的上下文长度。", "conclusion": "STLT结合了可解释性、可扩展性和鲁棒性，为超长序列语言建模提供了一种无需自注意力计算瓶颈的新途径。"}}
{"id": "2506.16892", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.16892", "abs": "https://arxiv.org/abs/2506.16892", "authors": ["Partha Chowdhury", "Harsha M", "Ayush Gupta", "Sanat K Biswas"], "title": "Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform", "comment": "This work has been already submitted for STEP-IPSC 2025 Conference\n  Proceedings", "summary": "This work presents an indigenous web based platform Orbital Collision (OrCo),\ncreated by the Space Systems Laboratory at IIIT Delhi, to enhance Space\nSituational Awareness (SSA) by predicting collision probabilities of space\nobjects using Two Line Elements (TLE) data. The work highlights the growing\nchallenges of congestion in the Earth's orbital environment, mainly due to\nspace debris and defunct satellites, which increase collision risks. It employs\nseveral methods for propagating orbital uncertainty and calculating the\ncollision probability. The performance of the platform is evaluated through\naccuracy assessments and efficiency metrics, in order to improve the tracking\nof space objects and ensure the safety of the satellite in congested space.", "AI": {"tldr": "OrCo平台通过TLE数据预测空间物体碰撞概率，提升空间态势感知能力。", "motivation": "地球轨道环境日益拥挤，空间碎片和失效卫星增加了碰撞风险，需提高空间态势感知。", "method": "利用多种方法传播轨道不确定性并计算碰撞概率。", "result": "通过准确性和效率评估验证平台性能，改善空间物体追踪和卫星安全。", "conclusion": "OrCo平台有效提升空间态势感知，保障卫星在拥挤空间中的安全。"}}
{"id": "2506.16493", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16493", "abs": "https://arxiv.org/abs/2506.16493", "authors": ["Mehreen Naeem", "Andrew Melnik", "Michael Beetz"], "title": "Grounding Language Models with Semantic Digital Twins for Robotic Planning", "comment": null, "summary": "We introduce a novel framework that integrates Semantic Digital Twins (SDTs)\nwith Large Language Models (LLMs) to enable adaptive and goal-driven robotic\ntask execution in dynamic environments. The system decomposes natural language\ninstructions into structured action triplets, which are grounded in contextual\nenvironmental data provided by the SDT. This semantic grounding allows the\nrobot to interpret object affordances and interaction rules, enabling action\nplanning and real-time adaptability. In case of execution failures, the LLM\nutilizes error feedback and SDT insights to generate recovery strategies and\niteratively revise the action plan. We evaluate our approach using tasks from\nthe ALFRED benchmark, demonstrating robust performance across various household\nscenarios. The proposed framework effectively combines high-level reasoning\nwith semantic environment understanding, achieving reliable task completion in\nthe face of uncertainty and failure.", "AI": {"tldr": "提出了一种结合语义数字孪生（SDT）和大语言模型（LLM）的框架，用于动态环境中机器人任务的适应性执行。", "motivation": "解决动态环境中机器人任务执行的不确定性和失败问题，通过语义理解和实时适应提高任务完成率。", "method": "将自然语言指令分解为结构化动作三元组，结合SDT提供的环境数据进行语义接地，利用LLM生成恢复策略并迭代调整动作计划。", "result": "在ALFRED基准测试中表现出色，适用于多种家庭场景，实现了高可靠性的任务完成。", "conclusion": "该框架成功结合了高层推理与语义环境理解，有效应对不确定性和失败。"}}
{"id": "2506.16159", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2506.16159", "abs": "https://arxiv.org/abs/2506.16159", "authors": ["Taisei Omine", "Naoyuki Kawabata", "Fuminori Homma"], "title": "Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters", "comment": "Accepted to SIGGRAPH 2025 Poster", "summary": "With the advancement of conversational AI, research on bodily expressions,\nincluding gestures and facial expressions, has also progressed. However, many\nexisting studies focus on photorealistic avatars, making them unsuitable for\nnon-photorealistic characters, such as those found in anime. This study\nproposes methods for expressing emotions, including exaggerated expressions\nunique to non-photorealistic characters, by utilizing expression data extracted\nfrom comics and dialogue-specific semantic gestures. A user study demonstrated\nsignificant improvements across multiple aspects when compared to existing\nresearch.", "AI": {"tldr": "研究提出了一种为非写实角色（如动漫角色）设计情感表达的方法，结合漫画中的表情数据和对话语义手势，显著优于现有研究。", "motivation": "现有研究多关注写实化虚拟角色，缺乏对非写实角色（如动漫角色）情感表达的支持。", "method": "利用漫画中的表情数据和对话语义手势，设计非写实角色的情感表达方法。", "result": "用户研究表明，该方法在多个方面显著优于现有研究。", "conclusion": "该方法为非写实角色的情感表达提供了有效解决方案。"}}
{"id": "2506.15715", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15715", "abs": "https://arxiv.org/abs/2506.15715", "authors": ["Hanyu Pei", "Jing-Xiao Liao", "Qibin Zhao", "Ting Gao", "Shijun Zhang", "Xiaoge Zhang", "Feng-Lei Fan"], "title": "NeuronSeek: On Stability and Expressivity of Task-driven Neurons", "comment": "14 pages, 10 figures", "summary": "Drawing inspiration from our human brain that designs different neurons for\ndifferent tasks, recent advances in deep learning have explored modifying a\nnetwork's neurons to develop so-called task-driven neurons. Prototyping\ntask-driven neurons (referred to as NeuronSeek) employs symbolic regression\n(SR) to discover the optimal neuron formulation and construct a network from\nthese optimized neurons. Along this direction, this work replaces symbolic\nregression with tensor decomposition (TD) to discover optimal neuronal\nformulations, offering enhanced stability and faster convergence. Furthermore,\nwe establish theoretical guarantees that modifying the aggregation functions\nwith common activation functions can empower a network with a fixed number of\nparameters to approximate any continuous function with an arbitrarily small\nerror, providing a rigorous mathematical foundation for the NeuronSeek\nframework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD\nframework not only achieves superior stability, but also is competitive\nrelative to the state-of-the-art models across diverse benchmarks. The code is\navailable at https://github.com/HanyuPei22/NeuronSeek.", "AI": {"tldr": "论文提出了一种基于张量分解（TD）的任务驱动神经元优化方法（NeuronSeek-TD），取代符号回归（SR），以提高稳定性和收敛速度。理论证明该方法能逼近任意连续函数，实验验证其性能优于现有模型。", "motivation": "受人类大脑不同任务对应不同神经元的启发，探索通过优化神经元设计提升网络性能。", "method": "使用张量分解（TD）替代符号回归（SR）优化神经元结构，并理论证明其逼近能力。", "result": "NeuronSeek-TD框架在稳定性和性能上优于现有模型，实验验证了其有效性。", "conclusion": "张量分解为任务驱动神经元优化提供了更稳定高效的解决方案，理论支持其广泛适用性。"}}
{"id": "2506.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16160", "abs": "https://arxiv.org/abs/2506.16160", "authors": ["Jiyao Wang", "Xiao Yang", "Hao Lu", "Dengbo He", "Kaishun Wu"], "title": "Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization", "comment": null, "summary": "Multi-source synsemantic domain generalization (MSSDG) for multi-task remote\nphysiological measurement seeks to enhance the generalizability of these\nmetrics and attracts increasing attention. However, challenges like partial\nlabeling and environmental noise may disrupt task-specific accuracy. Meanwhile,\ngiven that real-time adaptation is necessary for personalized products, the\ntest-time personalized adaptation (TTPA) after MSSDG is also worth exploring,\nwhile the gap between previous generalization and personalization methods is\nsignificant and hard to fuse. Thus, we proposed a unified framework for\nMSSD\\textbf{G} and TTP\\textbf{A} employing \\textbf{P}riors (\\textbf{GAP}) in\nbiometrics and remote photoplethysmography (rPPG). We first disentangled\ninformation from face videos into invariant semantics, individual bias, and\nnoise. Then, multiple modules incorporating priors and our observations were\napplied in different stages and for different facial information. Then, based\non the different principles of achieving generalization and personalization,\nour framework could simultaneously address MSSDG and TTPA under multi-task\nremote physiological estimation with minimal adjustments. We expanded the MSSDG\nbenchmark to the TTPA protocol on six publicly available datasets and\nintroduced a new real-world driving dataset with complete labeling. Extensive\nexperiments that validated our approach, and the codes along with the new\ndataset will be released.", "AI": {"tldr": "论文提出了一种统一框架GAP，用于多源同步语义域泛化（MSSDG）和测试时个性化适应（TTPA），通过先验信息和解耦面部视频数据提升多任务远程生理测量的泛化性和个性化能力。", "motivation": "解决多任务远程生理测量中部分标注和环境噪声对任务特定准确性的干扰，以及泛化与个性化方法之间的融合难题。", "method": "将面部视频信息解耦为不变语义、个体偏差和噪声，并利用先验信息设计多模块框架，同时处理MSSDG和TTPA。", "result": "在六个公开数据集和新引入的真实驾驶数据集上验证了框架的有效性。", "conclusion": "GAP框架能够以最小调整同时实现泛化和个性化，代码和新数据集将公开。"}}
{"id": "2506.17184", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.17184", "abs": "https://arxiv.org/abs/2506.17184", "authors": ["Albert H. Li", "Brandon Hung", "Aaron D. Ames", "Jiuguang Wang", "Simon Le Cleac'h", "Preston Culbertson"], "title": "Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control", "comment": "Accepted at the 2025 RSS Workshop on Fast Motion Planning and Control\n  in the Era of Parallelism. 5 Pages", "summary": "Recent advancements in parallel simulation and successful robotic\napplications are spurring a resurgence in sampling-based model predictive\ncontrol. To build on this progress, however, the robotics community needs\ncommon tooling for prototyping, evaluating, and deploying sampling-based\ncontrollers. We introduce Judo, a software package designed to address this\nneed. To facilitate rapid prototyping and evaluation, Judo provides robust\nimplementations of common sampling-based MPC algorithms and standardized\nbenchmark tasks. It further emphasizes usability with simple but extensible\ninterfaces for controller and task definitions, asynchronous execution for\nstraightforward simulation-to-hardware transfer, and a highly customizable\ninteractive GUI for tuning controllers interactively. While written in Python,\nthe software leverages MuJoCo as its physics backend to achieve real-time\nperformance, which we validate across both consumer and server-grade hardware.\nCode at https://github.com/bdaiinstitute/judo.", "AI": {"tldr": "Judo是一个用于快速原型设计和评估采样基于MPC算法的软件包，提供标准化任务和易用接口。", "motivation": "机器人社区需要通用工具来支持采样基于控制器的开发与部署。", "method": "Judo提供常见算法的实现、标准化任务、异步执行和交互式GUI，基于Python和MuJoCo。", "result": "Judo在消费级和服务器级硬件上实现实时性能。", "conclusion": "Judo为采样基于MPC的快速开发和评估提供了有效工具。"}}
{"id": "2506.16186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16186", "abs": "https://arxiv.org/abs/2506.16186", "authors": ["Zhenghao Xi", "Xiang Liu", "Yaqi Liu", "Yitong Cai", "Yangyu Zheng"], "title": "Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis", "comment": null, "summary": "Accident detection using Closed Circuit Television (CCTV) footage is one of\nthe most imperative features for enhancing transport safety and efficient\ntraffic control. To this end, this research addresses the issues of supervised\nmonitoring and data deficiency in accident detection systems by adapting\nexcellent deep learning technologies. The motivation arises from rising\nstatistics in the number of car accidents worldwide; this calls for innovation\nand the establishment of a smart, efficient and automated way of identifying\naccidents and calling for help to save lives. Addressing the problem of the\nscarcity of data, the presented framework joins Generative Adversarial Networks\n(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model\ntraining. Video frames for accidents and non-accidents are collected from\nYouTube videos, and we perform resizing, image enhancement and image\nnormalisation pixel range adjustments. Three models are used: CNN, Fine-tuned\nConvolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best\nfor detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,\nwhile the CNN model obtained 88%. Such results show that the proposed framework\nsuits traffic safety applications due to its high real-time accident detection\ncapabilities and broad-scale applicability. This work lays the foundation for\nintelligent surveillance systems in the future for real-time traffic\nmonitoring, smart city framework, and integration of intelligent surveillance\nsystems into emergency management systems.", "AI": {"tldr": "该研究利用深度学习技术（GANs和CNN）解决交通事故检测中的数据不足问题，提出了一种高效框架，准确率达94%-95%。", "motivation": "全球交通事故数量上升，需智能、高效的自动化事故检测系统以拯救生命。", "method": "结合GANs生成数据和CNN训练模型，使用三种模型（CNN、FTCNN、VIT）检测事故，并对视频帧进行预处理。", "result": "FTCNN和VIT模型准确率分别为94%和95%，CNN为88%，证明框架适用于实时事故检测。", "conclusion": "该框架为智能监控系统奠定基础，适用于实时交通监测和智慧城市应用。"}}
{"id": "2506.15717", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15717", "abs": "https://arxiv.org/abs/2506.15717", "authors": ["Zhengze Zhang", "Shiqi Wang", "Yiqun Shen", "Simin Guo", "Dahua Lin", "Xiaoliang Wang", "Nguyen Cam-Tu", "Fei Tan"], "title": "daDPO: Distribution-Aware DPO for Distilling Conversational Abilities", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional performance across\nvarious applications, but their conversational abilities decline sharply as\nmodel size decreases, presenting a barrier to their deployment in\nresource-constrained environments. Knowledge distillation with Direct\nPreference Optimization (dDPO) has emerged as a promising approach to enhancing\nthe conversational abilities of smaller models using a larger teacher model.\nHowever, current methods primarily focus on 'black-box' KD, which only uses the\nteacher's responses, overlooking the output distribution offered by the\nteacher. This paper addresses this gap by introducing daDPO (Distribution-Aware\nDPO), a unified method for preference optimization and distribution-based\ndistillation. We provide rigorous theoretical analysis and empirical\nvalidation, showing that daDPO outperforms existing methods in restoring\nperformance for pruned models and enhancing smaller LLM models. Notably, in\nin-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve\nnear-teacher performance (-7.3% preference rate compared to that of dDPO's\n-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model\n(14.0% win rate).", "AI": {"tldr": "论文提出了一种名为daDPO（Distribution-Aware DPO）的方法，通过结合偏好优化和基于分布的蒸馏，显著提升了小型语言模型的对话能力。", "motivation": "大型语言模型（LLMs）在资源受限环境中表现不佳，现有知识蒸馏方法忽视教师模型的输出分布信息。", "method": "引入daDPO方法，结合偏好优化和分布蒸馏，利用教师模型的输出分布提升小型模型性能。", "result": "实验表明，daDPO在修剪模型和小型LLM上表现优于现有方法，例如修剪后的Vicuna1.5-7B性能接近教师模型，Qwen2.5-1.5B甚至偶尔超越7B教师模型。", "conclusion": "daDPO为资源受限环境中的LLM部署提供了一种高效解决方案，显著提升了小型模型的对话能力。"}}
{"id": "2506.16209", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16209", "abs": "https://arxiv.org/abs/2506.16209", "authors": ["Annajoyce Mariani", "Kira Maag", "Hanno Gottschalk"], "title": "VideoGAN-based Trajectory Proposal for Automated Vehicles", "comment": null, "summary": "Being able to generate realistic trajectory options is at the core of\nincreasing the degree of automation of road vehicles. While model-driven,\nrule-based, and classical learning-based methods are widely used to tackle\nthese tasks at present, they can struggle to effectively capture the complex,\nmultimodal distributions of future trajectories. In this paper we investigate\nwhether a generative adversarial network (GAN) trained on videos of bird's-eye\nview (BEV) traffic scenarios can generate statistically accurate trajectories\nthat correctly capture spatial relationships between the agents. To this end,\nwe propose a pipeline that uses low-resolution BEV occupancy grid videos as\ntraining data for a video generative model. From the generated videos of\ntraffic scenarios we extract abstract trajectory data using single-frame object\ndetection and frame-to-frame object matching. We particularly choose a GAN\narchitecture for the fast training and inference times with respect to\ndiffusion models. We obtain our best results within 100 GPU hours of training,\nwith inference times under 20\\,ms. We demonstrate the physical realism of the\nproposed trajectories in terms of distribution alignment of spatial and dynamic\nparameters with respect to the ground truth videos from the Waymo Open Motion\nDataset.", "AI": {"tldr": "该论文提出了一种基于GAN的方法，通过鸟瞰视角视频生成统计上准确的交通轨迹，以解决传统方法难以捕捉复杂多模态轨迹分布的问题。", "motivation": "传统方法（如模型驱动、规则驱动或经典学习方法）难以有效捕捉未来轨迹的复杂多模态分布，因此需要一种更高效的方法。", "method": "使用低分辨率鸟瞰视角占用网格视频训练GAN模型，生成交通场景视频，并通过单帧目标检测和帧间目标匹配提取轨迹数据。", "result": "在100 GPU小时内完成训练，推理时间低于20毫秒，生成的轨迹在空间和动态参数上与真实数据分布一致。", "conclusion": "GAN方法能够快速生成物理上真实的交通轨迹，适用于自动驾驶领域。"}}
{"id": "2506.15718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15718", "abs": "https://arxiv.org/abs/2506.15718", "authors": ["Yu Guo", "Hongji Fang", "Tianyu Fang", "Zhe Cui"], "title": "BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata", "comment": null, "summary": "With the rise of artificial intelligence, the automatic generation of\nbuilding-scale 3-D objects has become an active research topic, yet training\nsuch models still demands large, clean and richly annotated datasets. We\nintroduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)\nbuildings (about 10 GB) produced by a shape-grammar-driven pipeline that\nencodes established building-design principles. Every sample consists of a\ngeometrically exact B-rep solid-covering floors, walls, slabs and rule-based\nopenings-together with a fast-loading .npy metadata file that records detailed\nper-floor parameters. The generator incorporates constraints on spatial scale,\ndaylight optimisation and interior layout, and the resulting objects pass\nmulti-stage filters that remove Boolean failures, undersized rooms and extreme\naspect ratios, ensuring compliance with architectural standards. To verify the\ndataset's learnability we trained two lightweight PointNet baselines. (i)\nMulti-attribute regression. A single encoder predicts storey count, total\nrooms, per-storey vector and mean room area from a 4 000-point cloud. On 100\nunseen buildings it attains 0.37-storey MAE (87 \\% within $\\pm1$), 5.7-room\nMAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same\nbackbone we classify GOOD versus DEFECT; on a balanced 100-model set the\nnetwork reaches 54 \\% accuracy, recalling 82 \\% of true defects at 53 \\%\nprecision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K\nis learnable yet non-trivial for both geometric regression and topological\nquality assessment", "AI": {"tldr": "BuildingBRep-11K是一个包含11,978个多楼层建筑的数据集，用于训练3D建筑生成模型，支持几何回归和拓扑质量评估。", "motivation": "解决3D建筑自动生成研究中缺乏大规模、高质量标注数据集的问题。", "method": "通过形状语法驱动的流水线生成建筑，并结合空间尺度、日光优化等约束，确保符合建筑标准。", "result": "训练了两个轻量级PointNet基线模型，分别用于多属性回归和缺陷检测，结果显示数据集具有学习潜力但仍有挑战。", "conclusion": "BuildingBRep-11K是一个可学习但非简单的数据集，适用于几何和拓扑任务。"}}
{"id": "2506.16218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16218", "abs": "https://arxiv.org/abs/2506.16218", "authors": ["Xinting Liao", "Weiming Liu", "Jiaming Qian", "Pengyang Zhou", "Jiahe Xu", "Wenjie Wang", "Chaochao Chen", "Xiaolin Zheng", "Tat-Seng Chua"], "title": "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models", "comment": "Accepted by ICML25", "summary": "Federated prompt learning (FPL) for vision-language models is a powerful\napproach to collaboratively adapt models across distributed clients while\npreserving data privacy. However, existing FPL approaches suffer from a\ntrade-off between performance and robustness, particularly in\nout-of-distribution (OOD) shifts, limiting their reliability in real-world\nscenarios. The inherent in-distribution (ID) data heterogeneity among different\nclients makes it more challenging to maintain this trade-off. To fill this gap,\nwe introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,\nwhich captures diverse distributions among clients using ID global prompts,\nlocal prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of\nprompts to create both class-level and distribution-level separations, which\nadapt to OOD shifts through bi-level distributionally robust optimization.\nAdditionally, FOCoOp improves the discrimination consistency among clients,\ni.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by\nsemi-unbalanced optimal transport. The extensive experiments on real-world\ndatasets demonstrate that FOCoOp effectively captures decentralized\nheterogeneous distributions and enhances robustness of different OOD shifts.\nThe project is available at GitHub.", "AI": {"tldr": "FOCoOp框架通过全局、局部和OOD提示优化联邦提示学习，提升分布偏移下的鲁棒性和性能。", "motivation": "现有联邦提示学习方法在性能和鲁棒性之间存在权衡，尤其在分布偏移下表现不佳，限制了实际应用。", "method": "FOCoOp利用三类提示（全局、局部、OOD）进行双层分布鲁棒优化，并通过半不平衡最优传输校准提示。", "result": "实验表明FOCoOp能有效捕捉分布式异构分布，增强对不同OOD偏移的鲁棒性。", "conclusion": "FOCoOp填补了联邦提示学习在分布偏移下的性能与鲁棒性权衡的空白，具有实际应用潜力。"}}
{"id": "2506.15719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15719", "abs": "https://arxiv.org/abs/2506.15719", "authors": ["Manal Rahal", "Bestoun S. Ahmed", "Roger Renstrom", "Robert Stener", "Albrecht Wurtz"], "title": "Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems", "comment": "33 pages accepted in Neural Networks and Applications", "summary": "Heat pumps (HPs) have emerged as a cost-effective and clean technology for\nsustainable energy systems, but their efficiency in producing hot water remains\nrestricted by conventional threshold-based control methods. Although machine\nlearning (ML) has been successfully implemented for various HP applications,\noptimization of household hot water demand forecasting remains understudied.\nThis paper addresses this problem by introducing a novel approach that combines\npredictive ML with anomaly detection to create adaptive hot water production\nstrategies based on household-specific consumption patterns. Our key\ncontributions include: (1) a composite approach combining ML and isolation\nforest (iForest) to forecast household demand for hot water and steer\nresponsive HP operations; (2) multi-step feature selection with advanced\ntime-series analysis to capture complex usage patterns; (3) application and\ntuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long\nShort-Term Memory (LSTM), and Bi-directional LSTM with the self-attention\nmechanism on data from different types of real HP installations; and (4)\nexperimental validation on six real household installations. Our experiments\nshow that the best-performing model LightGBM achieves superior performance,\nwith RMSE improvements of up to 9.37\\% compared to LSTM variants with $R^2$\nvalues between 0.748-0.983. For anomaly detection, our iForest implementation\nachieved an F1-score of 0.87 with a false alarm rate of only 5.2\\%,\ndemonstrating strong generalization capabilities across different household\ntypes and consumption patterns, making it suitable for real-world HP\ndeployments.", "AI": {"tldr": "论文提出了一种结合机器学习和异常检测的新方法，用于优化家用热泵的热水生产策略，显著提升了效率和适应性。", "motivation": "传统热泵控制方法效率有限，机器学习在热水需求预测中的应用尚未充分研究，因此需要一种更智能的解决方案。", "method": "结合机器学习（LightGBM、LSTM、BiLSTM）和隔离森林（iForest），通过多步特征选择和高级时间序列分析，预测热水需求并优化热泵操作。", "result": "LightGBM表现最佳，RMSE提升9.37%，iForest异常检测F1-score为0.87，误报率仅5.2%。", "conclusion": "该方法在真实家庭环境中表现出色，适用于实际热泵部署。"}}
{"id": "2506.16565", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16565", "abs": "https://arxiv.org/abs/2506.16565", "authors": ["Yuxin Chen", "Jianglan Wei", "Chenfeng Xu", "Boyi Li", "Masayoshi Tomizuka", "Andrea Bajcsy", "Ran Tian"], "title": "Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control", "comment": null, "summary": "World models enable robots to \"imagine\" future observations given current\nobservations and planned actions, and have been increasingly adopted as\ngeneralized dynamics models to facilitate robot learning. Despite their\npromise, these models remain brittle when encountering novel visual distractors\nsuch as objects and background elements rarely seen during training.\nSpecifically, novel distractors can corrupt action outcome predictions, causing\ndownstream failures when robots rely on the world model imaginations for\nplanning or action verification. In this work, we propose Reimagination with\nObservation Intervention (ReOI), a simple yet effective test-time strategy that\nenables world models to predict more reliable action outcomes in open-world\nscenarios where novel and unanticipated visual distractors are inevitable.\nGiven the current robot observation, ReOI first detects visual distractors by\nidentifying which elements of the scene degrade in physically implausible ways\nduring world model prediction. Then, it modifies the current observation to\nremove these distractors and bring the observation closer to the training\ndistribution. Finally, ReOI \"reimagines\" future outcomes with the modified\nobservation and reintroduces the distractors post-hoc to preserve visual\nconsistency for downstream planning and verification. We validate our approach\non a suite of robotic manipulation tasks in the context of action verification,\nwhere the verifier needs to select desired action plans based on predictions\nfrom a world model. Our results show that ReOI is robust to both\nin-distribution and out-of-distribution visual distractors. Notably, it\nimproves task success rates by up to 3x in the presence of novel distractors,\nsignificantly outperforming action verification that relies on world model\npredictions without imagination interventions.", "AI": {"tldr": "论文提出了一种名为ReOI的方法，通过检测并干预视觉干扰物，提升世界模型在开放世界中的预测可靠性。", "motivation": "世界模型在遇到训练中未见的视觉干扰物时表现脆弱，影响动作预测和规划。", "method": "ReOI通过检测干扰物、修改观测并重新预测，提升模型鲁棒性。", "result": "实验表明，ReOI显著提高任务成功率，尤其在新型干扰物下表现优异。", "conclusion": "ReOI是一种简单有效的测试时策略，适用于开放世界中的机器人任务。"}}
{"id": "2506.16262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16262", "abs": "https://arxiv.org/abs/2506.16262", "authors": ["Weeyoung Kwon", "Jeahun Sung", "Minkyu Jeon", "Chanho Eom", "Jihyong Oh"], "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision", "comment": "Please visit our project page at\n  https://github.com/CMLab-Korea/Awesome-3D-Low-Level-Vision", "summary": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have achieved significant progress in photorealistic\n3D scene reconstruction and novel view synthesis. However, most existing models\nassume clean and high-resolution (HR) multi-view inputs, which limits their\nrobustness under real-world degradations such as noise, blur, low-resolution\n(LR), and weather-induced artifacts. To address these limitations, the emerging\nfield of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision\ntasks including super-resolution (SR), deblurring, weather degradation removal,\nrestoration, and enhancement into the 3D spatial domain. This survey, referred\nto as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust\nrendering, restoration, and enhancement for 3D LLV by formalizing the\ndegradation-aware rendering problem and identifying key challenges related to\nspatio-temporal consistency and ill-posed optimization. Recent methods that\nintegrate LLV into neural rendering frameworks are categorized to illustrate\nhow they enable high-fidelity 3D reconstruction under adverse conditions.\nApplication domains such as autonomous driving, AR/VR, and robotics are also\ndiscussed, where reliable 3D perception from degraded inputs is critical. By\nreviewing representative methods, datasets, and evaluation protocols, this work\npositions 3D LLV as a fundamental direction for robust 3D content generation\nand scene-level reconstruction in real-world environments.", "AI": {"tldr": "该论文综述了3D低层视觉（3D LLV）领域，探讨了如何在退化条件下实现高保真3D重建，并提出了相关挑战和方法。", "motivation": "现有神经渲染方法（如NeRF和3DGS）假设输入为干净高分辨率图像，无法应对现实中的噪声、模糊等退化问题，因此需要研究3D LLV以提升鲁棒性。", "method": "通过形式化退化感知渲染问题，分类整合低层视觉任务的神经渲染方法，并讨论时空一致性和病态优化等关键挑战。", "result": "综述了代表性方法、数据集和评估协议，展示了3D LLV在自动驾驶、AR/VR等领域的应用潜力。", "conclusion": "3D LLV是实现现实环境中鲁棒3D内容生成和场景重建的重要方向。"}}
{"id": "2506.15720", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15720", "abs": "https://arxiv.org/abs/2506.15720", "authors": ["Juntae Lee", "Munawar Hayat", "Sungrack Yun"], "title": "Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning", "comment": "Accepted at CVPR 2025", "summary": "Few-shot class incremental learning (FSCIL) enables the continual learning of\nnew concepts with only a few training examples. In FSCIL, the model undergoes\nsubstantial updates, making it prone to forgetting previous concepts and\noverfitting to the limited new examples. Most recent trend is typically to\ndisentangle the learning of the representation from the classification head of\nthe model. A well-generalized feature extractor on the base classes (many\nexamples and many classes) is learned, and then fixed during incremental\nlearning. Arguing that the fixed feature extractor restricts the model's\nadaptability to new classes, we introduce a novel FSCIL method to effectively\naddress catastrophic forgetting and overfitting issues. Our method enables to\nseamlessly update the entire model with a few examples. We mainly propose a\ntripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,\nimmediately previous, and current models in weight-space, especially for the\nclassification heads of the models. Then, it collaboratively maintains\nknowledge from the base and previous models. In addition, we recognize the\nchallenges of distilling generalized representations from the previous model\nfrom scarce data. Hence, we suggest a regularization loss term using amplified\ndata knowledge distillation. Simply intermixing the few-shot data, we can\nproduce richer data enabling the distillation of critical knowledge from the\nprevious model. Consequently, we attain state-of-the-art results on the\nminiImageNet, CUB200, and CIFAR100 datasets.", "AI": {"tldr": "论文提出了一种新的少样本类增量学习（FSCIL）方法，通过三部分权重空间集成（Tri-WE）和放大数据知识蒸馏，解决了固定特征提取器限制模型适应性的问题。", "motivation": "固定特征提取器限制了模型对新类的适应性，导致灾难性遗忘和过拟合问题。", "method": "提出Tri-WE方法，在权重空间中插值基础模型、前一个模型和当前模型，并结合放大数据知识蒸馏正则化损失。", "result": "在miniImageNet、CUB200和CIFAR100数据集上取得了最先进的结果。", "conclusion": "该方法有效解决了FSCIL中的灾难性遗忘和过拟合问题，提升了模型性能。"}}
{"id": "2506.16265", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2506.16265", "abs": "https://arxiv.org/abs/2506.16265", "authors": ["Zhaoyi Wang", "Jemil Avers Butt", "Shengyu Huang", "Tomislav Medic", "Andreas Wieser"], "title": "Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images", "comment": "20 pages, 16 figures. Preprint under peer review. Example data and\n  code available at [GitHub](https://github.com/zhaoyiww/fusion4landslide)", "summary": "Landslide monitoring is essential for understanding geohazards and mitigating\nassociated risks. However, existing point cloud-based methods typically rely on\neither geometric or radiometric information and often yield sparse or non-3D\ndisplacement estimates. In this paper, we propose a hierarchical\npartition-based coarse-to-fine approach that fuses 3D point clouds and\nco-registered RGB images to estimate dense 3D displacement vector fields. We\nconstruct patch-level matches using both 3D geometry and 2D image features.\nThese matches are refined via geometric consistency checks, followed by rigid\ntransformation estimation per match. Experimental results on two real-world\nlandslide datasets demonstrate that our method produces 3D displacement\nestimates with high spatial coverage (79% and 97%) and high accuracy.\nDeviations in displacement magnitude with respect to external measurements\n(total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets,\nrespectively, and only 0.07 m and 0.20 m compared to manually derived\nreferences. These values are below the average scan resolutions (0.08 m and\n0.30 m). Our method outperforms the state-of-the-art method F2S3 in spatial\ncoverage while maintaining comparable accuracy. Our approach offers a practical\nand adaptable solution for TLS-based landslide monitoring and is extensible to\nother types of point clouds and monitoring tasks. Our example data and source\ncode are publicly available at https://github.com/zhaoyiww/fusion4landslide.", "AI": {"tldr": "提出了一种基于3D点云和RGB图像融合的分层分区方法，用于估计密集3D位移矢量场，显著提高了空间覆盖率和精度。", "motivation": "现有方法通常仅依赖几何或辐射信息，导致稀疏或非3D位移估计，无法满足滑坡监测需求。", "method": "采用分层分区粗到细的方法，结合3D几何和2D图像特征构建匹配，并通过几何一致性检查和刚性变换估计进行优化。", "result": "在两个真实滑坡数据集上，空间覆盖率达79%和97%，位移幅度偏差分别为0.15m和0.25m，优于现有方法F2S3。", "conclusion": "该方法为基于TLS的滑坡监测提供了实用且适应性强的解决方案，并可扩展至其他点云和监测任务。"}}
{"id": "2506.15721", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15721", "abs": "https://arxiv.org/abs/2506.15721", "authors": ["Junqi Gao", "Zhichang Guo", "Dazhi Zhang", "Dong Li", "Runze Liu", "Pengfei Li", "Kai Tian", "Biqing Qi"], "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration", "comment": null, "summary": "Heterogeneous Large Language Model (LLM) fusion integrates the strengths of\nmultiple source LLMs with different architectures into a target LLM with low\ncomputational overhead. While promising, existing methods suffer from two major\nlimitations: 1) reliance on real data from limited domain for knowledge fusion,\npreventing the target LLM from fully acquiring knowledge across diverse\ndomains, and 2) fixed data allocation proportions across domains, failing to\ndynamically adjust according to the target LLM's varying capabilities across\ndomains, leading to a capability imbalance. To overcome these limitations, we\npropose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.\nThrough the organization of knowledge domains into a hierarchical tree\nstructure, Bohdi enables automatic domain exploration and multi-domain data\ngeneration through multi-model collaboration, thereby comprehensively\nextracting knowledge from source LLMs. By formalizing domain expansion and data\nsampling proportion allocation on the knowledge tree as a Hierarchical\nMulti-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism\nto adaptively adjust sampling proportions based on the target LLM's performance\nfeedback across domains. Integrated with our proposed Introspection-Rebirth\n(IR) mechanism, DynaBranches dynamically tracks capability shifts during target\nLLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),\nfurther enhancing its online adaptation capability. Comparative experimental\nresults on a comprehensive suite of benchmarks demonstrate that Bohdi\nsignificantly outperforms existing baselines on multiple target LLMs, exhibits\nhigher data efficiency, and virtually eliminates the imbalance in the target\nLLM's capabilities. Our code is available at\nhttps://github.com/gjq100/Bohdi.git.", "AI": {"tldr": "Bohdi是一个仅使用合成数据的异构大语言模型（LLM）融合框架，通过层次化知识树和多模型协作实现自动领域探索和多领域数据生成，解决了现有方法依赖有限领域数据和固定数据分配比例的局限性。", "motivation": "现有异构LLM融合方法依赖有限领域数据和固定数据分配比例，导致目标LLM无法全面获取多样领域的知识，并出现能力不平衡问题。", "method": "Bohdi通过层次化知识树和多模型协作生成合成数据，将领域扩展和数据采样比例分配形式化为层次化多臂老虎机问题，并利用DynaBranches机制动态调整采样比例。结合Introspection-Rebirth机制，通过滑动窗口二项似然比测试跟踪能力变化。", "result": "实验表明，Bohdi在多个目标LLM上显著优于现有基线，数据效率更高，并几乎消除了能力不平衡问题。", "conclusion": "Bohdi通过合成数据和动态调整机制，有效解决了异构LLM融合中的领域限制和能力不平衡问题。"}}
{"id": "2506.16623", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16623", "abs": "https://arxiv.org/abs/2506.16623", "authors": ["Mobin Habibpour", "Fatemeh Afghah"], "title": "History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation", "comment": null, "summary": "Object Goal Navigation (ObjectNav) challenges robots to find objects in\nunseen environments, demanding sophisticated reasoning. While Vision-Language\nModels (VLMs) show potential, current ObjectNav methods often employ them\nsuperficially, primarily using vision-language embeddings for object-scene\nsimilarity checks rather than leveraging deeper reasoning. This limits\ncontextual understanding and leads to practical issues like repetitive\nnavigation behaviors. This paper introduces a novel zero-shot ObjectNav\nframework that pioneers the use of dynamic, history-aware prompting to more\ndeeply integrate VLM reasoning into frontier-based exploration. Our core\ninnovation lies in providing the VLM with action history context, enabling it\nto generate semantic guidance scores for navigation actions while actively\navoiding decision loops. We also introduce a VLM-assisted waypoint generation\nmechanism for refining the final approach to detected objects. Evaluated on the\nHM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and\n24.8% Success weighted by Path Length (SPL). These results are comparable to\nstate-of-the-art zero-shot methods, demonstrating the significant potential of\nour history-augmented VLM prompting strategy for more robust and context-aware\nrobotic navigation.", "AI": {"tldr": "论文提出了一种基于动态历史感知提示的零样本目标导航框架，通过深度整合视觉语言模型的推理能力，解决了现有方法在上下文理解和导航行为上的局限性。", "motivation": "当前目标导航方法对视觉语言模型的利用较浅，仅用于对象-场景相似性检查，缺乏深度推理能力，导致上下文理解不足和重复导航行为。", "method": "采用动态、历史感知提示技术，为视觉语言模型提供动作历史上下文，生成语义导航评分，并引入视觉语言模型辅助的路径点生成机制。", "result": "在HM3D数据集上的实验显示，成功率为46%，路径长度加权的成功率为24.8%，与现有零样本方法相当。", "conclusion": "动态历史感知提示策略显著提升了导航的鲁棒性和上下文感知能力，展示了视觉语言模型在目标导航中的潜力。"}}
{"id": "2506.16273", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.16273", "abs": "https://arxiv.org/abs/2506.16273", "authors": ["Xin Jiang", "Meiqi Cao", "Hao Tang", "Fei Shen", "Zechao Li"], "title": "Fine-grained Image Retrieval via Dual-Vision Adaptation", "comment": null, "summary": "Fine-Grained Image Retrieval~(FGIR) faces challenges in learning\ndiscriminative visual representations to retrieve images with similar\nfine-grained features. Current leading FGIR solutions typically follow two\nregimes: enforce pairwise similarity constraints in the semantic embedding\nspace, or incorporate a localization sub-network to fine-tune the entire model.\nHowever, such two regimes tend to overfit the training data while forgetting\nthe knowledge gained from large-scale pre-training, thus reducing their\ngeneralization ability. In this paper, we propose a Dual-Vision Adaptation\n(DVA) approach for FGIR, which guides the frozen pre-trained model to perform\nFGIR through collaborative sample and feature adaptation. Specifically, we\ndesign Object-Perceptual Adaptation, which modifies input samples to help the\npre-trained model perceive critical objects and elements within objects that\nare helpful for category prediction. Meanwhile, we propose In-Context\nAdaptation, which introduces a small set of parameters for feature adaptation\nwithout modifying the pre-trained parameters. This makes the FGIR task using\nthese adjusted features closer to the task solved during the pre-training.\nAdditionally, to balance retrieval efficiency and performance, we propose\nDiscrimination Perception Transfer to transfer the discriminative knowledge in\nthe object-perceptual adaptation to the image encoder using the knowledge\ndistillation mechanism. Extensive experiments show that DVA has fewer learnable\nparameters and performs well on three in-distribution and three\nout-of-distribution fine-grained datasets.", "AI": {"tldr": "提出了一种双视觉适应（DVA）方法，通过样本和特征协作适应，解决了细粒度图像检索（FGIR）中预训练模型过拟合和泛化能力不足的问题。", "motivation": "当前FGIR方法容易过拟合训练数据，忽视预训练知识，导致泛化能力下降。", "method": "设计了对象感知适应和上下文适应，分别调整输入样本和特征，同时提出判别感知迁移以平衡检索效率和性能。", "result": "在三个分布内和三个分布外细粒度数据集上表现优异，且参数较少。", "conclusion": "DVA方法有效提升了FGIR任务的性能，同时保持了预训练模型的优势。"}}
{"id": "2506.15722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15722", "abs": "https://arxiv.org/abs/2506.15722", "authors": ["Wangzhi Zhan", "Jianpeng Chen", "Dongqi Fu", "Dawei Zhou"], "title": "UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation", "comment": null, "summary": "Metamaterials are artificial materials that are designed to meet unseen\nproperties in nature, such as ultra-stiffness and negative materials indices.\nIn mechanical metamaterial design, three key modalities are typically involved,\ni.e., 3D topology, density condition, and mechanical property. Real-world\ncomplex application scenarios place the demanding requirements on machine\nlearning models to consider all three modalities together. However, a\ncomprehensive literature review indicates that most existing works only\nconsider two modalities, e.g., predicting mechanical properties given the 3D\ntopology or generating 3D topology given the required properties. Therefore,\nthere is still a significant gap for the state-of-the-art machine learning\nmodels capturing the whole. Hence, we propose a unified model named UNIMATE,\nwhich consists of a modality alignment module and a synergetic diffusion\ngeneration module. Experiments indicate that UNIMATE outperforms the other\nbaseline models in topology generation task, property prediction task, and\ncondition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We\nopensource our proposed UNIMATE model and corresponding results at\nhttps://github.com/wzhan24/UniMate.", "AI": {"tldr": "论文提出了一种名为UNIMATE的统一模型，用于同时处理机械超材料设计中的三种模态（3D拓扑、密度条件和机械性能），填补了现有研究仅关注两种模态的空白。", "motivation": "现实应用场景要求机器学习模型同时考虑机械超材料设计中的三种模态，但现有研究大多仅关注其中两种，因此需要一种统一模型来解决这一问题。", "method": "UNIMATE模型包含模态对齐模块和协同扩散生成模块，能够同时处理三种模态。", "result": "实验表明，UNIMATE在拓扑生成、性能预测和条件确认任务中分别优于基线模型80.2%、5.1%和50.2%。", "conclusion": "UNIMATE填补了现有研究的空白，并在多个任务中表现出色，已开源模型和结果。"}}
{"id": "2506.16643", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.16643", "abs": "https://arxiv.org/abs/2506.16643", "authors": ["Matthew Ebisu", "Hang Yu", "Reuben Aronson", "Elaine Short"], "title": "See What I Mean? Expressiveness and Clarity in Robot Display Design", "comment": null, "summary": "Nonverbal visual symbols and displays play an important role in communication\nwhen humans and robots work collaboratively. However, few studies have\ninvestigated how different types of non-verbal cues affect objective task\nperformance, especially in a dynamic environment that requires real time\ndecision-making. In this work, we designed a collaborative navigation task\nwhere the user and the robot only had partial information about the map on each\nend and thus the users were forced to communicate with a robot to complete the\ntask. We conducted our study in a public space and recruited 37 participants\nwho randomly passed by our setup. Each participant collaborated with a robot\nutilizing either animated anthropomorphic eyes and animated icons, or static\nanthropomorphic eyes and static icons. We found that participants that\ninteracted with a robot with animated displays reported the greatest level of\ntrust and satisfaction; that participants interpreted static icons the best;\nand that participants with a robot with static eyes had the highest completion\nsuccess. These results suggest that while animation can foster trust with\nrobots, human-robot communication can be optimized by the addition of familiar\nstatic icons that may be easier for users to interpret. We published our code,\ndesigned symbols, and collected results online at:\nhttps://github.com/mattufts/huamn_Cozmo_interaction.", "AI": {"tldr": "研究探讨了非语言视觉符号在人与机器人协作中的作用，发现动画显示能增强信任，而静态图标更易理解。", "motivation": "探索不同类型非语言提示对动态环境中任务表现的影响。", "method": "设计协作导航任务，比较动画与静态显示的效果，招募37名参与者。", "result": "动画显示提升信任，静态图标更易理解，静态眼睛显示任务完成率最高。", "conclusion": "动画增强信任，但结合熟悉的静态图标可优化人机通信。"}}
{"id": "2506.16297", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16297", "abs": "https://arxiv.org/abs/2506.16297", "authors": ["Heng Zhang", "Zikang Wan", "Danilo Vasconcellos Vargas"], "title": "SycnMapV2: Robust and Adaptive Unsupervised Segmentation", "comment": null, "summary": "Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods.This superior performance extends across various types\nof corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%\nvs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,\nsupervision, or loss functions. It is based on a learning paradigm that uses\nself-organizing dynamical equations combined with concepts from random\nnetworks. Moreover,unlike conventional methods that require re-initialization\nfor each new input, SyncMapV2 adapts online, mimicking the continuous\nadaptability of human vision. Thus, we go beyond the accurate and robust\nresults, and present the first algorithm that can do all the above online,\nadapting to input rather than re-initializing. In adaptability tests, SyncMapV2\ndemonstrates near-zero performance degradation, which motivates and fosters a\nnew generation of robust and adaptive intelligence in the near future.", "AI": {"tldr": "SyncMapV2是一种无监督分割算法，具有卓越的鲁棒性，无需训练或监督，且能在线适应输入。", "motivation": "人类视觉在无显式训练下仍能稳健分割视觉线索，而现有AI算法在噪声增加时表现不佳。SyncMapV2旨在解决这一问题。", "method": "基于自组织动力学方程和随机网络概念，无需鲁棒训练或损失函数，支持在线适应。", "result": "在数字噪声、天气和模糊等干扰下，SyncMapV2性能下降极小（0.01%），远超现有方法（下降23.8%）。", "conclusion": "SyncMapV2展示了无监督、自适应分割的潜力，为未来鲁棒智能算法的发展提供了新方向。"}}
{"id": "2506.15724", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15724", "abs": "https://arxiv.org/abs/2506.15724", "authors": ["Kunxi Li", "Zhonghua Jiang", "Zhouzhou Shen", "Zhaode Wang", "Chengfei Lv", "Shengyu Zhang", "Fan Wu", "Fei Wu"], "title": "MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference", "comment": null, "summary": "This paper introduces MadaKV, a modality-adaptive key-value (KV) cache\neviction strategy designed to enhance the efficiency of multimodal large\nlanguage models (MLLMs) in long-context inference. In multimodal scenarios,\nattention heads exhibit varying preferences for different modalities, resulting\nin significant disparities in modality importance across attention heads.\nTraditional KV cache eviction methods, which are tailored for unimodal\nsettings, fail to capture modality-specific information, thereby yielding\nsuboptimal performance. MadaKV addresses these challenges through two key\ncomponents: modality preference adaptation and hierarchical compression\ncompensation. By dynamically sensing modality information within attention\nheads and adaptively retaining critical tokens, MadaKV achieves substantial\nreductions in KV cache memory footprint and model inference decoding latency\n(1.3 to 1.5 times improvement) while maintaining high accuracy across various\nmultimodal long-context tasks. Extensive experiments on representative MLLMs\nand the MileBench benchmark demonstrate the effectiveness of MadaKV compared to\nexisting KV cache eviction methods.", "AI": {"tldr": "MadaKV是一种模态自适应的KV缓存淘汰策略，旨在提升多模态大语言模型（MLLMs）在长上下文推理中的效率。", "motivation": "传统KV缓存淘汰方法在多模态场景中无法捕捉模态特定信息，导致性能不佳。", "method": "MadaKV通过模态偏好适应和分层压缩补偿两个关键组件，动态感知注意力头中的模态信息并自适应保留关键令牌。", "result": "MadaKV显著减少了KV缓存内存占用和模型推理解码延迟（提升1.3至1.5倍），同时在多模态长上下文任务中保持高准确性。", "conclusion": "实验证明MadaKV在MLLMs和MileBench基准测试中优于现有KV缓存淘汰方法。"}}
{"id": "2506.16652", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16652", "abs": "https://arxiv.org/abs/2506.16652", "authors": ["Guang Yin", "Yitong Li", "Yixuan Wang", "Dale McConachie", "Paarth Shah", "Kunimatsu Hashimoto", "Huan Zhang", "Katherine Liu", "Yunzhu Li"], "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity", "comment": "Accepted to Robotics: Science and Systems (RSS) 2025. The first three\n  authors contributed equally. Project Page:\n  https://robopil.github.io/code-diffuser/", "summary": "Natural language instructions for robotic manipulation tasks often exhibit\nambiguity and vagueness. For instance, the instruction \"Hang a mug on the mug\ntree\" may involve multiple valid actions if there are several mugs and branches\nto choose from. Existing language-conditioned policies typically rely on\nend-to-end models that jointly handle high-level semantic understanding and\nlow-level action generation, which can result in suboptimal performance due to\ntheir lack of modularity and interpretability. To address these challenges, we\nintroduce a novel robotic manipulation framework that can accomplish tasks\nspecified by potentially ambiguous natural language. This framework employs a\nVision-Language Model (VLM) to interpret abstract concepts in natural language\ninstructions and generates task-specific code - an interpretable and executable\nintermediate representation. The generated code interfaces with the perception\nmodule to produce 3D attention maps that highlight task-relevant regions by\nintegrating spatial and semantic information, effectively resolving ambiguities\nin instructions. Through extensive experiments, we identify key limitations of\ncurrent imitation learning methods, such as poor adaptation to language and\nenvironmental variations. We show that our approach excels across challenging\nmanipulation tasks involving language ambiguity, contact-rich manipulation, and\nmulti-object interactions.", "AI": {"tldr": "论文提出了一种新型机器人操作框架，通过Vision-Language Model (VLM)解析自然语言指令中的模糊性，生成可执行代码，并结合感知模块解决指令歧义。", "motivation": "现有语言条件策略因缺乏模块化和可解释性，导致性能不佳。论文旨在解决自然语言指令的模糊性和歧义问题。", "method": "使用VLM解析自然语言指令，生成任务特定代码，结合感知模块生成3D注意力图，整合空间和语义信息。", "result": "实验表明，该方法在语言模糊性、接触密集操作和多物体交互任务中表现优异。", "conclusion": "提出的框架有效解决了自然语言指令的模糊性问题，提升了机器人操作的适应性和性能。"}}
{"id": "2506.16307", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.16307", "abs": "https://arxiv.org/abs/2506.16307", "authors": ["Xu Zhao", "Chen Zhao", "Xiantao Hu", "Hongliang Zhang", "Ying Tai", "Jian Yang"], "title": "Learning Multi-scale Spatial-frequency Features for Image Denoising", "comment": null, "summary": "Recent advancements in multi-scale architectures have demonstrated\nexceptional performance in image denoising tasks. However, existing\narchitectures mainly depends on a fixed single-input single-output Unet\narchitecture, ignoring the multi-scale representations of pixel level. In\naddition, previous methods treat the frequency domain uniformly, ignoring the\ndifferent characteristics of high-frequency and low-frequency noise. In this\npaper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for\nimage denoising. We use image pyramid inputs to restore noise-free results from\nlow-resolution images. In order to realize the interaction of high-frequency\nand low-frequency information, we design an adaptive spatial-frequency learning\nunit (ASFU), where a learnable mask is used to separate the information into\nhigh-frequency and low-frequency components. In the skip connections, we design\na global feature fusion block to enhance the features at different scales.\nExtensive experiments on both synthetic and real noisy image datasets verify\nthe effectiveness of MADNet compared with current state-of-the-art denoising\napproaches.", "AI": {"tldr": "提出了一种新型多尺度自适应双域网络（MADNet），通过图像金字塔输入和自适应空间频率学习单元（ASFU）解决图像去噪中多尺度表示和频率域特性差异的问题。", "motivation": "现有方法依赖固定单输入单输出的Unet架构，忽略了像素级多尺度表示，且对高、低频噪声特性处理不足。", "method": "使用图像金字塔输入，设计ASFU单元通过可学习掩模分离高、低频信息，并在跳跃连接中引入全局特征融合块。", "result": "在合成和真实噪声图像数据集上的实验验证了MADNet优于当前最先进的去噪方法。", "conclusion": "MADNet通过多尺度自适应和双域学习显著提升了图像去噪性能。"}}
{"id": "2506.15725", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15725", "abs": "https://arxiv.org/abs/2506.15725", "authors": ["Matteo Ninniri", "Marco Podda", "Davide Bacciu"], "title": "Graph Diffusion that can Insert and Delete", "comment": null, "summary": "Generative models of graphs based on discrete Denoising Diffusion\nProbabilistic Models (DDPMs) offer a principled approach to molecular\ngeneration by systematically removing structural noise through iterative atom\nand bond adjustments. However, existing formulations are fundamentally limited\nby their inability to adapt the graph size (that is, the number of atoms)\nduring the diffusion process, severely restricting their effectiveness in\nconditional generation scenarios such as property-driven molecular design,\nwhere the targeted property often correlates with the molecular size. In this\npaper, we reformulate the noising and denoising processes to support monotonic\ninsertion and deletion of nodes. The resulting model, which we call GrIDDD,\ndynamically grows or shrinks the chemical graph during generation. GrIDDD\nmatches or exceeds the performance of existing graph diffusion models on\nmolecular property targeting despite being trained on a more difficult problem.\nFurthermore, when applied to molecular optimization, GrIDDD exhibits\ncompetitive performance compared to specialized optimization models. This work\npaves the way for size-adaptive molecular generation with graph diffusion.", "AI": {"tldr": "论文提出了一种基于离散去噪扩散概率模型（DDPMs）的图生成模型GrIDDD，支持动态调整分子大小，解决了现有方法在条件生成中的局限性。", "motivation": "现有基于DDPMs的图生成模型无法在扩散过程中调整分子大小，限制了其在条件生成（如属性驱动分子设计）中的应用。", "method": "通过重新定义噪声和去噪过程，支持节点的单调插入和删除，实现了动态调整分子大小的GrIDDD模型。", "result": "GrIDDD在分子属性目标生成中表现优于现有图扩散模型，并在分子优化中展现出竞争力。", "conclusion": "GrIDDD为尺寸自适应的分子生成提供了新思路，推动了图扩散模型的发展。"}}
{"id": "2506.16685", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16685", "abs": "https://arxiv.org/abs/2506.16685", "authors": ["Xiaomeng Xu", "Yifan Hou", "Zeyi Liu", "Shuran Song"], "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections", "comment": null, "summary": "We address key challenges in Dataset Aggregation (DAgger) for real-world\ncontact-rich manipulation: how to collect informative human correction data and\nhow to effectively update policies with this new data. We introduce Compliant\nResidual DAgger (CR-DAgger), which contains two novel components: 1) a\nCompliant Intervention Interface that leverages compliance control, allowing\nhumans to provide gentle, accurate delta action corrections without\ninterrupting the ongoing robot policy execution; and 2) a Compliant Residual\nPolicy formulation that learns from human corrections while incorporating force\nfeedback and force control. Our system significantly enhances performance on\nprecise contact-rich manipulation tasks using minimal correction data,\nimproving base policy success rates by over 50\\% on two challenging tasks (book\nflipping and belt assembly) while outperforming both retraining-from-scratch\nand finetuning approaches. Through extensive real-world experiments, we provide\npractical guidance for implementing effective DAgger in real-world robot\nlearning tasks. Result videos are available at:\nhttps://compliant-residual-dagger.github.io/", "AI": {"tldr": "论文提出了Compliant Residual DAgger（CR-DAgger），通过合规干预接口和合规残差策略，解决了数据集聚合（DAgger）在接触丰富操作中的关键挑战，显著提升了任务成功率。", "motivation": "解决在真实世界接触丰富操作中，如何收集信息丰富的人类校正数据以及如何有效利用这些数据更新策略的挑战。", "method": "1) 合规干预接口，利用合规控制实现人类提供精确的动作校正；2) 合规残差策略，结合力反馈和力控制学习人类校正。", "result": "在书籍翻页和皮带组装任务中，基础策略成功率提升超过50%，优于从头训练和微调方法。", "conclusion": "CR-DAgger为真实世界机器人学习任务提供了有效的DAgger实现指南。"}}
{"id": "2506.16318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16318", "abs": "https://arxiv.org/abs/2506.16318", "authors": ["Carmelo Scribano", "Elena Govi", "Paolo bertellini", "Simone Parisi", "Giorgia Franchini", "Marko Bertogna"], "title": "Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation", "comment": "Acceptet at ICIAP 2025", "summary": "Accurate mapping of agricultural field boundaries is essential for the\nefficient operation of agriculture. Automatic extraction from high-resolution\nsatellite imagery, supported by computer vision techniques, can avoid costly\nground surveys. In this paper, we present a pipeline for field delineation\nbased on the Segment Anything Model (SAM), introducing a fine-tuning strategy\nto adapt SAM to this task. In addition to using published datasets, we describe\na method for acquiring a complementary regional dataset that covers areas\nbeyond current sources. Extensive experiments assess segmentation accuracy and\nevaluate the generalization capabilities. Our approach provides a robust\nbaseline for automated field delineation. The new regional dataset, known as\nERAS, is now publicly available.", "AI": {"tldr": "论文提出了一种基于Segment Anything Model（SAM）的农田边界自动提取方法，并通过微调策略优化模型，同时发布了一个新的区域数据集ERAS。", "motivation": "高效农业运营需要精确的农田边界映射，传统地面调查成本高，因此利用计算机视觉技术从高分辨率卫星图像中自动提取边界具有重要意义。", "method": "采用Segment Anything Model（SAM）进行农田边界提取，并引入微调策略优化模型性能，同时开发了一种获取补充区域数据集的方法。", "result": "实验验证了方法的准确性和泛化能力，新数据集ERAS填补了现有数据的空白。", "conclusion": "该方法为自动化农田边界提取提供了可靠基准，ERAS数据集已公开可用。"}}
{"id": "2506.15792", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2506.15792", "abs": "https://arxiv.org/abs/2506.15792", "authors": ["Jackson Burns", "Akshat Zalte", "William Green"], "title": "Descriptor-based Foundation Models for Molecular Property Prediction", "comment": null, "summary": "Fast and accurate prediction of molecular properties with machine learning is\npivotal to scientific advancements across myriad domains. Foundation models in\nparticular have proven especially effective, enabling accurate training on\nsmall, real-world datasets. This study introduces CheMeleon, a novel molecular\nfoundation model pre-trained on deterministic molecular descriptors from the\nMordred package, leveraging a Directed Message-Passing Neural Network to\npredict these descriptors in a noise-free setting. Unlike conventional\napproaches relying on noisy experimental data or biased quantum mechanical\nsimulations, CheMeleon uses low-noise molecular descriptors to learn rich\nmolecular representations. Evaluated on 58 benchmark datasets from Polaris and\nMoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,\noutperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop\n(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)\nand other foundation models. However, it struggles to distinguish activity\ncliffs like many of the tested models. The t-SNE projection of CheMeleon's\nlearned representations demonstrates effective separation of chemical series,\nhighlighting its ability to capture structural nuances. These results\nunderscore the potential of descriptor-based pre-training for scalable and\neffective molecular property prediction, opening avenues for further\nexploration of descriptor sets and unlabeled datasets.", "AI": {"tldr": "CheMeleon是一种基于分子描述符预训练的分子基础模型，通过无噪声环境下的Directed Message-Passing Neural Network预测分子性质，在多个基准测试中表现优异。", "motivation": "传统方法依赖噪声实验数据或偏向量子力学模拟，而CheMeleon利用低噪声分子描述符学习更丰富的分子表示。", "method": "使用Mordred包中的确定性分子描述符预训练CheMeleon，采用Directed Message-Passing Neural Network进行预测。", "result": "在Polaris和MoleculeACE的58个基准测试中，CheMeleon分别取得79%和97%的胜率，优于其他基线模型。", "conclusion": "基于描述符的预训练为分子性质预测提供了可扩展且高效的方法，未来可进一步探索描述符集和无标签数据集。"}}
{"id": "2506.16703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16703", "abs": "https://arxiv.org/abs/2506.16703", "authors": ["Sinuo Cheng", "Ruyi Zhou", "Wenhao Feng", "Huaiguang Yang", "Haibo Gao", "Zongquan Deng", "Liang Ding"], "title": "VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation", "comment": "accepted by IROS 2025", "summary": "The increasingly complex and diverse planetary exploration environment\nrequires more adaptable and flexible rover navigation strategy. In this study,\nwe propose a VLM-empowered multi-mode system to achieve efficient while safe\nautonomous navigation for planetary rovers. Vision-Language Model (VLM) is used\nto parse scene information by image inputs to achieve a human-level\nunderstanding of terrain complexity. Based on the complexity classification,\nthe system switches to the most suitable navigation mode, composing of\nperception, mapping and planning modules designed for different terrain types,\nto traverse the terrain ahead before reaching the next waypoint. By integrating\nthe local navigation system with a map server and a global waypoint generation\nmodule, the rover is equipped to handle long-distance navigation tasks in\ncomplex scenarios. The navigation system is evaluated in various simulation\nenvironments. Compared to the single-mode conservative navigation method, our\nmulti-mode system is able to bootstrap the time and energy efficiency in a\nlong-distance traversal with varied type of obstacles, enhancing efficiency by\n79.5%, while maintaining its avoidance capabilities against terrain hazards to\nguarantee rover safety. More system information is shown at\nhttps://chengsn1234.github.io/multi-mode-planetary-navigation/.", "AI": {"tldr": "提出了一种基于视觉语言模型（VLM）的多模式行星探测车导航系统，通过地形复杂度分类切换导航模式，显著提升效率和安全性。", "motivation": "行星探测环境复杂多样，需要更灵活、适应性强的导航策略。", "method": "利用VLM解析场景信息，根据地形复杂度分类切换导航模式（感知、建图、规划），并与全局路径规划模块集成。", "result": "在模拟环境中，多模式系统比单模式保守方法效率提升79.5%，同时保持对地形危险的规避能力。", "conclusion": "该系统显著提升了行星探测车在复杂环境中的导航效率和安全性。"}}
{"id": "2506.16319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16319", "abs": "https://arxiv.org/abs/2506.16319", "authors": ["Arpit Jadon", "Haoran Wang", "Phillip Thomas", "Michael Stanley", "S. Nathaniel Cibik", "Rachel Laurat", "Omar Maher", "Lukas Hoyer", "Ozan Unal", "Dengxin Dai"], "title": "RealDriveSim: A Realistic Multi-Modal Multi-Task Synthetic Dataset for Autonomous Driving", "comment": "Accepted at the IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "As perception models continue to develop, the need for large-scale datasets\nincreases. However, data annotation remains far too expensive to effectively\nscale and meet the demand. Synthetic datasets provide a solution to boost model\nperformance with substantially reduced costs. However, current synthetic\ndatasets remain limited in their scope, realism, and are designed for specific\ntasks and applications. In this work, we present RealDriveSim, a realistic\nmulti-modal synthetic dataset for autonomous driving that not only supports\npopular 2D computer vision applications but also their LiDAR counterparts,\nproviding fine-grained annotations for up to 64 classes. We extensively\nevaluate our dataset for a wide range of applications and domains,\ndemonstrating state-of-the-art results compared to existing synthetic\nbenchmarks. The dataset is publicly available at\nhttps://realdrivesim.github.io/.", "AI": {"tldr": "RealDriveSim是一个多模态合成数据集，用于自动驾驶，支持2D计算机视觉和LiDAR应用，提供64类精细标注，性能优于现有合成基准。", "motivation": "解决数据标注成本高、现有合成数据集范围有限且不够真实的问题。", "method": "开发RealDriveSim数据集，支持多模态（2D和LiDAR）并提供精细标注。", "result": "在多种应用和领域中表现优异，优于现有合成基准。", "conclusion": "RealDriveSim为自动驾驶研究提供了高效且低成本的数据解决方案。"}}
{"id": "2506.15809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15809", "abs": "https://arxiv.org/abs/2506.15809", "authors": ["Deyi Li", "Zijun Yao", "Muxuan Liang", "Mei Liu"], "title": "DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling", "comment": null, "summary": "In recent years, graph learning has gained significant interest for modeling\ncomplex interactions among medical events in structured Electronic Health\nRecord (EHR) data. However, existing graph-based approaches often work in a\nstatic manner, either restricting interactions within individual encounters or\ncollapsing all historical encounters into a single snapshot. As a result, when\nit is necessary to identify meaningful groups of medical events spanning\nlongitudinal encounters, existing methods are inadequate in modeling\ninteractions cross encounters while accounting for temporal dependencies. To\naddress this limitation, we introduce Deep Patient Journey (DeepJ), a novel\ngraph convolutional transformer model with differentiable graph pooling to\neffectively capture intra-encounter and inter-encounter medical event\ninteractions. DeepJ can identify groups of temporally and functionally related\nmedical events, offering valuable insights into key event clusters pertinent to\npatient outcome prediction. DeepJ significantly outperformed five\nstate-of-the-art baseline models while enhancing interpretability,\ndemonstrating its potential for improved patient risk stratification.", "AI": {"tldr": "DeepJ是一种新型图卷积变换模型，通过可微分图池化捕捉医疗事件在单次和多次就诊中的交互关系，显著优于现有基线模型。", "motivation": "现有图学习方法无法有效建模跨就诊的医疗事件交互及时间依赖性，限制了其对患者风险分层的实用性。", "method": "提出DeepJ模型，结合图卷积和变换器技术，利用可微分图池化捕捉医疗事件的时空交互。", "result": "DeepJ在性能上显著优于五种基线模型，并能识别与患者预后相关的关键事件群。", "conclusion": "DeepJ为患者风险分层提供了更优的建模工具，同时增强了模型的可解释性。"}}
{"id": "2506.16330", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16330", "abs": "https://arxiv.org/abs/2506.16330", "authors": ["Ji Zhang", "Jingkuan Song", "Lianli Gao", "Nicu Sebe", "Heng Tao Shen"], "title": "Reliable Few-shot Learning under Dual Noises", "comment": "17 pages, 6 figures,", "summary": "Recent advances in model pre-training give rise to task adaptation-based\nfew-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic\nmodel for capturing task-specific knowledge with a few-labeled support samples\nof the target task.Nevertheless, existing approaches may still fail in the open\nworld due to the inevitable in-distribution (ID) and out-of-distribution (OOD)\nnoise from both support and query samples of the target task. With limited\nsupport samples available, i) the adverse effect of the dual noises can be\nseverely amplified during task adaptation, and ii) the adapted model can\nproduce unreliable predictions on query samples in the presence of the dual\nnoises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable\nFSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate\nimage and region weights for support samples, based on which a clean prototype\nloss and a noise entropy maximization loss are proposed to achieve noise-robust\ntask adaptation. Additionally,DETA++ employs a memory bank to store and refine\nclean regions for each inner-task class, based on which a Local Nearest\nCentroid Classifier (LocalNCC) is devised to yield noise-robust predictions on\nquery samples. Moreover, DETA++ utilizes an Intra-class Region Swapping\n(IntraSwap) strategy to rectify ID class prototypes during task adaptation,\nenhancing the model's robustness to the dual noises. Extensive experiments\ndemonstrate the effectiveness and flexibility of DETA++.", "AI": {"tldr": "DETA++提出了一种去噪任务适应方法，通过对比相关聚合模块和噪声熵最大化损失，提升小样本学习在开放世界中的鲁棒性。", "motivation": "现有方法在开放世界中可能因支持样本和查询样本中的分布内外噪声而失效，DETA++旨在解决这一问题。", "method": "DETA++使用CoRA模块计算支持样本权重，提出干净原型损失和噪声熵最大化损失，并利用LocalNCC和IntraSwap策略增强鲁棒性。", "result": "实验证明DETA++在噪声环境下表现出色，具有较高的灵活性和有效性。", "conclusion": "DETA++通过去噪和鲁棒性增强策略，显著提升了小样本学习在开放世界中的性能。"}}
{"id": "2506.15817", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15817", "abs": "https://arxiv.org/abs/2506.15817", "authors": ["Jason Tandiary"], "title": "Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions", "comment": null, "summary": "This paper studies Vickrey first-price auctions under binary feedback.\nLeveraging the enhanced performance of machine learning algorithms, the new\nalgorithm uses past information to improve the regret bounds of the BROAD-OMD\nalgorithm. Motivated by the growing relevance of first-price auctions and the\npredictive capabilities of machine learning models, this paper proposes a new\nalgorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages\npredictions of the highest competing bid. This paper's main contribution is an\nalgorithm that achieves zero regret under accurate predictions. Additionally, a\nbounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain\nnormality conditions.", "AI": {"tldr": "本文研究二元反馈下的Vickrey第一价格拍卖，提出一种新算法，利用机器学习提升BROAD-OMD算法的遗憾边界。", "motivation": "受第一价格拍卖日益重要和机器学习预测能力的启发，研究如何利用历史信息改进拍卖算法。", "method": "在BROAD-OMD框架内提出新算法，利用最高竞争出价的预测信息。", "result": "算法在准确预测下实现零遗憾，并在特定条件下建立O(T^(3/4) * Vt^(1/4))的遗憾边界。", "conclusion": "新算法显著提升了拍卖性能，尤其在预测准确时表现优异。"}}
{"id": "2506.16720", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16720", "abs": "https://arxiv.org/abs/2506.16720", "authors": ["Weitao Zhou", "Bo Zhang", "Zhong Cao", "Xiang Li", "Qian Cheng", "Chunyang Liu", "Yaqin Zhang", "Diange Yang"], "title": "DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy", "comment": null, "summary": "With the increasing presence of automated vehicles on open roads under driver\nsupervision, disengagement cases are becoming more prevalent. While some\ndata-driven planning systems attempt to directly utilize these disengagement\ncases for policy improvement, the inherent scarcity of disengagement data\n(often occurring as a single instances) restricts training effectiveness.\nFurthermore, some disengagement data should be excluded since the disengagement\nmay not always come from the failure of driving policies, e.g. the driver may\ncasually intervene for a while. To this end, this work proposes\ndisengagement-reason-augmented reinforcement learning (DRARL), which enhances\ndriving policy improvement process according to the reason of disengagement\ncases. Specifically, the reason of disengagement is identified by a\nout-of-distribution (OOD) state estimation model. When the reason doesn't\nexist, the case will be identified as a casual disengagement case, which\ndoesn't require additional policy adjustment. Otherwise, the policy can be\nupdated under a reason-augmented imagination environment, improving the policy\nperformance of disengagement cases with similar reasons. The method is\nevaluated using real-world disengagement cases collected by autonomous driving\nrobotaxi. Experimental results demonstrate that the method accurately\nidentifies policy-related disengagement reasons, allowing the agent to handle\nboth original and semantically similar cases through reason-augmented training.\nFurthermore, the approach prevents the agent from becoming overly conservative\nafter policy adjustments. Overall, this work provides an efficient way to\nimprove driving policy performance with disengagement cases.", "AI": {"tldr": "论文提出了一种基于脱管原因增强的强化学习方法（DRARL），通过识别脱管原因来优化自动驾驶策略，避免无效数据干扰，提升策略性能。", "motivation": "随着自动驾驶车辆在开放道路上的增多，脱管案例日益普遍，但脱管数据稀缺且部分数据无效（如驾驶员随意干预），限制了策略改进效果。", "method": "提出DRARL方法，利用OOD状态估计模型识别脱管原因，排除无效案例，并在原因增强的想象环境中更新策略。", "result": "实验证明，该方法能准确识别策略相关脱管原因，提升策略对相似案例的处理能力，同时避免策略过度保守。", "conclusion": "DRARL为利用脱管案例优化驾驶策略提供了高效方法。"}}
{"id": "2506.16331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16331", "abs": "https://arxiv.org/abs/2506.16331", "authors": ["Viktoria Pundy", "Marco Peer", "Florian Kleber"], "title": "Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification", "comment": null, "summary": "Neural Networks are the state of the art for many tasks in the computer\nvision domain, including Writer Identification (WI) and Writer Verification\n(WV). The transparency of these \"black box\" systems is important for\nimprovements of performance and reliability. For this work, two transparency\ntechniques are applied to neural networks trained on WI and WV for the first\ntime in this domain. The first technique provides pixel-level saliency maps,\nwhile the point-specific saliency maps of the second technique provide\ninformation on similarities between two images. The transparency techniques are\nevaluated using deletion and insertion score metrics. The goal is to support\nforensic experts with information on similarities in handwritten text and to\nexplore the characteristics selected by a neural network for the identification\nprocess. For the qualitative evaluation, the highlights of the maps are\ncompared to the areas forensic experts consider during the identification\nprocess. The evaluation results show that the pixel-wise saliency maps\noutperform the point-specific saliency maps and are suitable for the support of\nforensic experts.", "AI": {"tldr": "论文研究了神经网络在笔迹识别和验证中的透明度问题，首次应用了两种透明度技术，并通过评估支持法医专家。", "motivation": "提高神经网络在笔迹识别和验证中的透明度和可靠性，支持法医专家的工作。", "method": "应用两种透明度技术：像素级显著性图和点特异性显著性图，并通过删除和插入评分指标进行评估。", "result": "像素级显著性图优于点特异性显著性图，适合支持法医专家。", "conclusion": "像素级显著性图在笔迹识别和验证中表现更优，有助于法医专家的分析。"}}
{"id": "2506.15823", "categories": ["cs.LG", "68T01, 68T05"], "pdf": "https://arxiv.org/pdf/2506.15823", "abs": "https://arxiv.org/abs/2506.15823", "authors": ["Chiara Razzetta", "Shahryar Noei", "Federico Barbarossa", "Edoardo Spairani", "Monica Roascio", "Elisa Barbi", "Giulia Ciacci", "Sara Sommariva", "Sabrina Guastavino", "Michele Piana", "Matteo Lenge", "Gabriele Arnulfo", "Giovanni Magenes", "Elvira Maranesi", "Giulio Amabili", "Anna Maria Massone", "Federico Benvenuto", "Giuseppe Jurman", "Diego Sona", "Cristina Campi"], "title": "AI-based modular warning machine for risk identification in proximity healthcare", "comment": null, "summary": "\"DHEAL-COM - Digital Health Solutions in Community Medicine\" is a research\nand technology project funded by the Italian Department of Health for the\ndevelopment of digital solutions of interest in proximity healthcare. The\nactivity within the DHEAL-COM framework allows scientists to gather a notable\namount of multi-modal data whose interpretation can be performed by means of\nmachine learning algorithms. The present study illustrates a general automated\npipeline made of numerous unsupervised and supervised methods that can ingest\nsuch data, provide predictive results, and facilitate model interpretations via\nfeature identification.", "AI": {"tldr": "DHEAL-COM项目开发数字健康解决方案，利用机器学习分析多模态数据，提出自动化流程以提供预测结果和特征解释。", "motivation": "意大利卫生部资助，旨在开发社区医疗中的数字健康解决方案。", "method": "结合无监督和监督学习方法，构建自动化数据处理流程。", "result": "能够处理多模态数据并提供预测结果，同时支持特征识别。", "conclusion": "该研究为社区医疗中的数字健康解决方案提供了高效的数据分析工具。"}}
{"id": "2506.16353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16353", "abs": "https://arxiv.org/abs/2506.16353", "authors": ["Chao He", "Hongxi Wei"], "title": "MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval", "comment": "Accepted by ICMR2025. arXiv admin note: text overlap with\n  arXiv:2405.07524", "summary": "Deep image hashing aims to enable effective large-scale image retrieval by\nmapping the input images into simple binary hash codes through deep neural\nnetworks. More recently, Vision Mamba with linear time complexity has attracted\nextensive attention from researchers by achieving outstanding performance on\nvarious computer tasks. Nevertheless, the suitability of Mamba for large-scale\nimage retrieval tasks still needs to be explored. Towards this end, we propose\na visual state space hashing model, called MambaHash. Concretely, we propose a\nbackbone network with stage-wise architecture, in which grouped Mamba operation\nis introduced to model local and global information by utilizing Mamba to\nperform multi-directional scanning along different groups of the channel.\nSubsequently, the proposed channel interaction attention module is used to\nenhance information communication across channels. Finally, we meticulously\ndesign an adaptive feature enhancement module to increase feature diversity and\nenhance the visual representation capability of the model. We have conducted\ncomprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and\nIMAGENET. The experimental results demonstrate that compared with the\nstate-of-the-art deep hashing methods, our proposed MambaHash has well\nefficiency and superior performance to effectively accomplish large-scale image\nretrieval tasks. Source code is available\nhttps://github.com/shuaichaochao/MambaHash.git", "AI": {"tldr": "MambaHash是一种基于视觉状态空间的哈希模型，用于大规模图像检索，结合了Mamba的高效性和分组操作，性能优于现有方法。", "motivation": "探索Mamba在大规模图像检索任务中的适用性，并提升哈希模型的效率和性能。", "method": "提出分阶段的主干网络，引入分组Mamba操作和多方向扫描，结合通道交互注意力模块和自适应特征增强模块。", "result": "在CIFAR-10、NUS-WIDE和IMAGENET数据集上表现优于现有深度哈希方法。", "conclusion": "MambaHash在大规模图像检索任务中表现出高效性和优越性能。"}}
{"id": "2506.15825", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15825", "abs": "https://arxiv.org/abs/2506.15825", "authors": ["Luiz Pereira", "M. Hadi Amini"], "title": "Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters", "comment": null, "summary": "In this paper, we first propose a novel algorithm for model fusion that\nleverages Wasserstein barycenters in training a global Deep Neural Network\n(DNN) in a distributed architecture. To this end, we divide the dataset into\nequal parts that are fed to \"agents\" who have identical deep neural networks\nand train only over the dataset fed to them (known as the local dataset). After\nsome training iterations, we perform an aggregation step where we combine the\nweight parameters of all neural networks using Wasserstein barycenters. These\nsteps form the proposed algorithm referred to as FedWB. Moreover, we leverage\nthe processes created in the first part of the paper to develop an algorithm to\ntackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test\nexperiment is the CartPole toy problem, where we vary the lengths of the poles\nto create heterogeneous environments. We train a deep Q-Network (DQN) in each\nenvironment to learn to control each cart, while occasionally performing a\nglobal aggregation step to generalize the local models; the end outcome is a\nglobal DQN that functions across all environments.", "AI": {"tldr": "提出了一种基于Wasserstein重心的新算法FedWB，用于分布式架构中全局DNN的模型融合，并扩展应用于异构联邦强化学习（HFRL）。", "motivation": "解决分布式训练中模型融合的挑战，并扩展至异构环境下的强化学习问题。", "method": "将数据集分发给多个代理训练本地DNN，通过Wasserstein重心聚合权重；在HFRL中，通过全局聚合泛化局部DQN模型。", "result": "在CartPole问题中验证了FedWB和HFRL算法的有效性，生成了适应异构环境的全局DQN。", "conclusion": "FedWB和HFRL算法在分布式和异构环境下表现良好，具有实际应用潜力。"}}
{"id": "2506.16822", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16822", "abs": "https://arxiv.org/abs/2506.16822", "authors": ["Daniel Frau-Alfaro", "Julio Castaño-Amoros", "Santiago Puente", "Pablo Gil", "Roberto Calandra"], "title": "Learning Dexterous Object Handover", "comment": "Paper accepted for presentation in RoMan 2025", "summary": "Object handover is an important skill that we use daily when interacting with\nother humans. To deploy robots in collaborative setting, like houses, being\nable to receive and handing over objects safely and efficiently becomes a\ncrucial skill. In this work, we demonstrate the use of Reinforcement Learning\n(RL) for dexterous object handover between two multi-finger hands. Key to this\ntask is the use of a novel reward function based on dual quaternions to\nminimize the rotation distance, which outperforms other rotation\nrepresentations such as Euler and rotation matrices. The robustness of the\ntrained policy is experimentally evaluated by testing w.r.t. objects that are\nnot included in the training distribution, and perturbations during the\nhandover process. The results demonstrate that the trained policy successfully\nperform this task, achieving a total success rate of 94% in the best-case\nscenario after 100 experiments, thereby showing the robustness of our policy\nwith novel objects. In addition, the best-case performance of the policy\ndecreases by only 13.8% when the other robot moves during the handover, proving\nthat our policy is also robust to this type of perturbation, which is common in\nreal-world object handovers.", "AI": {"tldr": "该论文研究了如何使用强化学习（RL）实现多指手之间的灵巧物体交接，提出了一种基于双四元数的新型奖励函数，以最小化旋转距离，并在实验中验证了其鲁棒性。", "motivation": "在协作环境中（如家庭），机器人需要安全高效地接收和交接物体，这是实现人机协作的关键技能。", "method": "使用强化学习（RL）训练多指手进行物体交接，提出了一种基于双四元数的奖励函数，优于欧拉角和旋转矩阵等表示方法。", "result": "训练的策略在未包含于训练分布中的物体和交接过程中的扰动下表现出色，最佳情况下成功率达到94%，且在另一机器人移动时性能仅下降13.8%。", "conclusion": "该策略在新型物体和常见扰动下均表现出鲁棒性，验证了其在现实场景中的实用性。"}}
{"id": "2506.16369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16369", "abs": "https://arxiv.org/abs/2506.16369", "authors": ["Pallabi Dutta", "Anubhab Maity", "Sushmita Mitra"], "title": "Prompt-based Dynamic Token Pruning to Guide Transformer Attention in Efficient Segmentation", "comment": null, "summary": "The high computational demands of Vision Transformers (ViTs), in processing a\nhuge number of tokens, often constrain their practical application in analyzing\nmedical images. This research proposes an adaptive prompt-guided pruning method\nto selectively reduce the processing of irrelevant tokens in the segmentation\npipeline. The prompt-based spatial prior helps to rank the tokens according to\ntheir relevance. Tokens with low-relevance scores are down-weighted, ensuring\nthat only the relevant ones are propagated for processing across subsequent\nstages. This data-driven pruning strategy facilitates end-to-end training,\nmaintains gradient flow, and improves segmentation accuracy by focusing\ncomputational resources on essential regions. The proposed framework is\nintegrated with several state-of-the-art models to facilitate the elimination\nof irrelevant tokens; thereby, enhancing computational efficiency while\npreserving segmentation accuracy. The experimental results show a reduction of\n$\\sim$ 35-55\\% tokens; thus reducing the computational costs relative to the\nbaselines. Cost-effective medical image processing, using our framework,\nfacilitates real-time diagnosis by expanding its applicability in\nresource-constrained environments.", "AI": {"tldr": "提出了一种基于提示的自适应剪枝方法，减少ViTs在医学图像分割中的计算负担，同时保持准确性。", "motivation": "Vision Transformers在医学图像处理中因高计算需求受限，需一种方法减少无关标记的处理。", "method": "使用提示引导的空间先验对标记进行排序，剪枝低相关性标记，仅保留相关标记进行后续处理。", "result": "实验显示减少了35-55%的标记，降低了计算成本，同时保持了分割准确性。", "conclusion": "该方法在资源受限环境中实现了高效、实时的医学图像处理。"}}
{"id": "2506.15840", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15840", "abs": "https://arxiv.org/abs/2506.15840", "authors": ["Kevin Yin", "Julia Gersey", "Pei Zhang"], "title": "In-field Calibration of Low-Cost Sensors through XGBoost $\\&$ Aggregate Sensor Data", "comment": "6 pages including citations", "summary": "Effective large-scale air quality monitoring necessitates distributed sensing\ndue to the pervasive and harmful nature of particulate matter (PM),\nparticularly in urban environments. However, precision comes at a cost: highly\naccurate sensors are expensive, limiting the spatial deployments and thus their\ncoverage. As a result, low-cost sensors have become popular, though they are\nprone to drift caused by environmental sensitivity and manufacturing\nvariability. This paper presents a model for in-field sensor calibration using\nXGBoost ensemble learning to consolidate data from neighboring sensors. This\napproach reduces dependence on the presumed accuracy of individual sensors and\nimproves generalization across different locations.", "AI": {"tldr": "提出了一种基于XGBoost集成学习的现场传感器校准模型，用于提高低成本传感器的精度和泛化能力。", "motivation": "大规模空气质量监测需要分布式传感，但高精度传感器成本高，而低成本传感器易受环境因素影响，导致数据漂移。", "method": "使用XGBoost集成学习整合邻近传感器数据，减少对单个传感器精度的依赖。", "result": "模型提高了传感器数据的准确性和跨位置泛化能力。", "conclusion": "该方法为低成本传感器在大规模空气质量监测中的应用提供了可行方案。"}}
{"id": "2506.16371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16371", "abs": "https://arxiv.org/abs/2506.16371", "authors": ["Yunhao Hou", "Bochao Zou", "Min Zhang", "Ran Chen", "Shangdong Yang", "Yanmei Zhang", "Junbao Zhuo", "Siheng Chen", "Jiansheng Chen", "Huimin Ma"], "title": "AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios", "comment": null, "summary": "By sharing information across multiple agents, collaborative perception helps\nautonomous vehicles mitigate occlusions and improve overall perception\naccuracy. While most previous work focus on vehicle-to-vehicle and\nvehicle-to-infrastructure collaboration, with limited attention to aerial\nperspectives provided by UAVs, which uniquely offer dynamic, top-down views to\nalleviate occlusions and monitor large-scale interactive environments. A major\nreason for this is the lack of high-quality datasets for aerial-ground\ncollaborative scenarios. To bridge this gap, we present AGC-Drive, the first\nlarge-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The\ndata collection platform consists of two vehicles, each equipped with five\ncameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and\na LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.\nConsisting of approximately 120K LiDAR frames and 440K images, the dataset\ncovers 14 diverse real-world driving scenarios, including urban roundabouts,\nhighway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic\ninteraction events, including vehicle cut-ins, cut-outs, and frequent lane\nchanges. AGC-Drive contains 400 scenes, each with approximately 100 frames and\nfully annotated 3D bounding boxes covering 13 object categories. We provide\nbenchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative\nperception and vehicle-to-UAV collaborative perception. Additionally, we\nrelease an open-source toolkit, including spatiotemporal alignment verification\ntools, multi-agent visualization systems, and collaborative annotation\nutilities. The dataset and code are available at\nhttps://github.com/PercepX/AGC-Drive.", "AI": {"tldr": "AGC-Drive是首个大规模真实世界数据集，用于空中-地面协同3D感知，填补了无人机视角数据集的空白。", "motivation": "现有研究多关注车与车或车与基础设施的协作，缺乏无人机视角的高质量数据集，限制了空中-地面协同感知的发展。", "method": "通过两辆配备多摄像头和LiDAR的车辆及一架搭载摄像头和LiDAR的无人机，采集多视角多代理数据。", "result": "数据集包含120K LiDAR帧和440K图像，覆盖14种驾驶场景，提供400个场景的3D标注和动态交互事件。", "conclusion": "AGC-Drive为空中-地面协同感知提供了基准和工具，推动了该领域的研究。"}}
{"id": "2506.15850", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15850", "abs": "https://arxiv.org/abs/2506.15850", "authors": ["Pedro Mendes", "Paolo Romano", "David Garlan"], "title": "Uncertainty Estimation by Human Perception versus Neural Models", "comment": null, "summary": "Modern neural networks (NNs) often achieve high predictive accuracy but\nremain poorly calibrated, producing overconfident predictions even when wrong.\nThis miscalibration poses serious challenges in applications where reliable\nuncertainty estimates are critical. In this work, we investigate how human\nperceptual uncertainty compares to uncertainty estimated by NNs. Using three\nvision benchmarks annotated with both human disagreement and crowdsourced\nconfidence, we assess the correlation between model-predicted uncertainty and\nhuman-perceived uncertainty. Our results show that current methods only weakly\nalign with human intuition, with correlations varying significantly across\ntasks and uncertainty metrics. Notably, we find that incorporating\nhuman-derived soft labels into the training process can improve calibration\nwithout compromising accuracy. These findings reveal a persistent gap between\nmodel and human uncertainty and highlight the potential of leveraging human\ninsights to guide the development of more trustworthy AI systems.", "AI": {"tldr": "现代神经网络预测准确但校准性差，人类感知不确定性与模型估计的关联较弱，引入人类软标签可改善校准性。", "motivation": "神经网络常产生过度自信的预测，其不确定性估计与人类感知的差异在关键应用中带来挑战。", "method": "通过三个视觉基准数据集，比较模型预测的不确定性与人类标注的不确定性之间的相关性。", "result": "当前方法与人类直觉的关联较弱，但引入人类软标签可提升校准性且不影响准确性。", "conclusion": "模型与人类不确定性存在差距，利用人类洞察可开发更可信的AI系统。"}}
{"id": "2506.16936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16936", "abs": "https://arxiv.org/abs/2506.16936", "authors": ["Shengpeng Wang", "Xin Luo", "Yulong Xie", "Wei Wang"], "title": "SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion", "comment": null, "summary": "Point cloud extraction (PCE) and ego velocity estimation (EVE) are key\ncapabilities gaining attention in 3D radar perception. However, existing work\ntypically treats these two tasks independently, which may neglect the interplay\nbetween radar's spatial and Doppler domain features, potentially introducing\nadditional bias. In this paper, we observe an underlying correlation between 3D\npoints and ego velocity, which offers reciprocal benefits for PCE and EVE. To\nfully unlock such inspiring potential, we take the first step to design a\nSpatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and\naccurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the\nconventional latent diffusion process in three major aspects. First, we\nintroduce a representation that embodies both spatial occupancy and Doppler\nfeatures. Second, we design a directional diffusion with radar priors to\nstreamline the sampling. Third, we propose Iterative Doppler Refinement to\nenhance the model's adaptability to density variations and ghosting effects.\nExtensive evaluations show that SDDiff significantly outperforms\nstate-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater\nin valid generation density while boosting PCE effectiveness and reliability.", "AI": {"tldr": "提出了一种名为SDDiff的模型，首次将点云提取（PCE）和自车速度估计（EVE）任务结合，利用空间-多普勒扩散方法提升性能。", "motivation": "现有工作通常独立处理PCE和EVE，忽略了雷达空间和多普勒域特征的相互作用，可能引入偏差。研究发现两者存在潜在关联，可相互促进。", "method": "设计了SDDiff模型，改进传统潜在扩散过程：1）引入结合空间占用和多普勒特征的表示；2）设计基于雷达先验的方向性扩散；3）提出迭代多普勒细化增强适应性。", "result": "SDDiff显著优于现有基线，EVE准确率提升59%，有效生成密度提高4倍，同时提升PCE的效果和可靠性。", "conclusion": "SDDiff通过结合PCE和EVE任务，充分利用空间和多普勒特征的关联，为雷达感知提供了更高效的解决方案。"}}
{"id": "2506.16385", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16385", "abs": "https://arxiv.org/abs/2506.16385", "authors": ["Santosh Patapati", "Trisanth Srinivasan", "Amith Adiraju"], "title": "CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset", "comment": null, "summary": "Micro-gesture recognition is a challenging task in affective computing due to\nthe subtle, involuntary nature of the gestures and their low movement\namplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based\narchitecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP\nmodel tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG\nintegrates human pose (skeleton) information into the CLIP-based recognition\npipeline through pose-guided semantic query generation and a gated multi-modal\nfusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These\nresults demonstrate both the potential of our approach and the remaining\ndifficulty in fully adapting vision-language models like CLIP for micro-gesture\nrecognition.", "AI": {"tldr": "本文提出了一种基于CLIP的微手势识别模型CLIP-MG，通过结合姿态信息和多模态融合机制，在iMiGUE数据集上取得了61.82%的Top-1准确率。", "motivation": "微手势因其细微、不自主的特性及低幅度运动，是情感计算中的一项挑战性任务。", "method": "提出了一种Pose-Guided Semantics-Aware CLIP-based架构（CLIP-MG），通过姿态引导的语义查询生成和门控多模态融合机制，将人体姿态信息整合到CLIP识别流程中。", "result": "模型在iMiGUE数据集上的Top-1准确率为61.82%。", "conclusion": "结果表明该方法具有潜力，但也揭示了将视觉语言模型（如CLIP）完全适配于微手势识别的难度。"}}
{"id": "2506.15864", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15864", "abs": "https://arxiv.org/abs/2506.15864", "authors": ["Xixi Hu", "Runlong Liao", "Keyang Xu", "Bo Liu", "Yeqing Li", "Eugene Ie", "Hongliang Fei", "Qiang Liu"], "title": "Improving Rectified Flow with Boundary Conditions", "comment": "14 pages", "summary": "Rectified Flow offers a simple and effective approach to high-quality\ngenerative modeling by learning a velocity field. However, we identify a\nlimitation in directly modeling the velocity with an unconstrained neural\nnetwork: the learned velocity often fails to satisfy certain boundary\nconditions, leading to inaccurate velocity field estimations that deviate from\nthe desired ODE. This issue is particularly critical during stochastic sampling\nat inference, as the score function's errors are amplified near the boundary.\nTo mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary\nRF Model), in which we enforce boundary conditions with a minimal code\nmodification. Boundary RF Model improves performance over vanilla RF model,\ndemonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and\n8.98% improvement using SDE sampling.", "AI": {"tldr": "Boundary-enforced Rectified Flow Model (Boundary RF Model)通过强制边界条件改进Rectified Flow，显著提升生成模型性能。", "motivation": "直接建模速度场时，未约束的神经网络可能无法满足边界条件，导致速度场估计不准确，尤其在边界附近误差放大。", "method": "提出Boundary RF Model，通过最小代码修改强制边界条件。", "result": "在ImageNet上，ODE采样FID分数提升8.01%，SDE采样提升8.98%。", "conclusion": "Boundary RF Model有效解决了边界条件问题，显著提升了生成模型的质量。"}}
{"id": "2506.16986", "categories": ["cs.RO", "68T40, 93C85, 70E60", "I.2.9; I.2.10; I.2.8"], "pdf": "https://arxiv.org/pdf/2506.16986", "abs": "https://arxiv.org/abs/2506.16986", "authors": ["Yuntao Ma", "Yang Liu", "Kaixian Qu", "Marco Hutter"], "title": "Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration", "comment": "8 pages, IROS 2025", "summary": "Throwing is a fundamental skill that enables robots to manipulate objects in\nways that extend beyond the reach of their arms. We present a control framework\nthat combines learning and model-based control for prehensile whole-body\nthrowing with legged mobile manipulators. Our framework consists of three\ncomponents: a nominal tracking policy for the end-effector, a high-frequency\nresidual policy to enhance tracking accuracy, and an optimization-based module\nto improve end-effector acceleration control. The proposed controller achieved\nthe average of 0.28 m landing error when throwing at targets located 6 m away.\nFurthermore, in a comparative study with university students, the system\nachieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,\nhitting small targets randomly placed at distances of 3-5 m while throwing at a\nspecified speed of 6 m/s. In contrast, humans have a success rate of only\n15.2%. This work provides an early demonstration of prehensile throwing with\nquantified accuracy on hardware, contributing to progress in dynamic whole-body\nmanipulation.", "AI": {"tldr": "论文提出了一种结合学习和模型控制的方法，用于腿式移动机械臂的抓握全身投掷，实现了较高的投掷精度。", "motivation": "投掷是机器人扩展手臂操作范围的基本技能，但现有技术难以实现高精度动态全身投掷。", "method": "框架包括末端执行器的名义跟踪策略、高频残差策略提升跟踪精度，以及基于优化的模块改进加速度控制。", "result": "投掷6米远目标时平均着陆误差0.28米；与大学生对比实验中，系统成功率为56.8%，人类仅为15.2%。", "conclusion": "该工作展示了硬件上量化精度的抓握投掷，推动了动态全身操作的发展。"}}
{"id": "2506.16398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16398", "abs": "https://arxiv.org/abs/2506.16398", "authors": ["Peixiang Huang", "Yanyan Huang", "Weiqin Zhao", "Junjun He", "Lequan Yu"], "title": "HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis", "comment": null, "summary": "Pathology is essential for cancer diagnosis, with multiple instance learning\n(MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural\nhierarchy -- patches, regions, and slides -- with distinct semantic\nassociations. While some methods attempt to leverage this hierarchy for\nimproved representation, they predominantly rely on Euclidean embeddings, which\nstruggle to fully capture semantic hierarchies. To address this limitation, we\npropose HyperPath, a novel method that integrates knowledge from textual\ndescriptions to guide the modeling of semantic hierarchies of WSIs in\nhyperbolic space, thereby enhancing WSI classification. Our approach adapts\nboth visual and textual features extracted by pathology vision-language\nfoundation models to the hyperbolic space. We design an Angular Modality\nAlignment Loss to ensure robust cross-modal alignment, while a Semantic\nHierarchy Consistency Loss further refines feature hierarchies through\nentailment and contradiction relationships and thus enhance semantic coherence.\nThe classification is performed with geodesic distance, which measures the\nsimilarity between entities in the hyperbolic semantic hierarchy. This\neliminates the need for linear classifiers and enables a geometry-aware\napproach to WSI analysis. Extensive experiments show that our method achieves\nsuperior performance across tasks compared to existing methods, highlighting\nthe potential of hyperbolic embeddings for WSI analysis.", "AI": {"tldr": "论文提出了一种名为HyperPath的新方法，利用双曲空间建模WSI的语义层次结构，结合文本描述知识，提升WSI分类性能。", "motivation": "现有方法主要依赖欧几里得嵌入，难以充分捕捉WSI的语义层次结构，限制了分类性能。", "method": "HyperPath通过双曲空间建模视觉和文本特征，设计角度模态对齐损失和语义层次一致性损失，利用测地距离进行分类。", "result": "实验表明，HyperPath在多个任务上优于现有方法，验证了双曲嵌入在WSI分析中的潜力。", "conclusion": "双曲空间能有效建模WSI的语义层次，结合跨模态对齐和层次一致性损失，显著提升分类性能。"}}
{"id": "2506.15872", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15872", "abs": "https://arxiv.org/abs/2506.15872", "authors": ["Sara Kangaslahti", "Elan Rosenfeld", "Naomi Saphra"], "title": "Hidden Breakthroughs in Language Model Training", "comment": "17 pages, 10 figures", "summary": "Loss curves are smooth during most of model training, so visible\ndiscontinuities stand out as possible conceptual breakthroughs. Studying these\nbreakthroughs enables a deeper understanding of learning dynamics, but only\nwhen they are properly identified. This paper argues that similar breakthroughs\noccur frequently throughout training but they are obscured by a loss metric\nthat collapses all variation into a single scalar. To find these hidden\ntransitions, we introduce POLCA, a method for decomposing changes in loss along\narbitrary bases of the low-rank training subspace. We use our method to\nidentify clusters of samples that share similar changes in loss during\ntraining, disaggregating the overall loss into that of smaller groups of\nconceptually similar data. We validate our method on synthetic arithmetic and\nnatural language tasks, showing that POLCA recovers clusters that represent\ninterpretable breakthroughs in the model's capabilities. We demonstrate the\npromise of these hidden phase transitions as a tool for unsupervised\ninterpretability.", "AI": {"tldr": "论文提出POLCA方法，通过分解损失变化来识别训练中的隐藏突破，验证了其在合成算术和自然语言任务中的有效性。", "motivation": "研究训练中的损失曲线不连续性以理解学习动态，但现有方法因标量损失指标而难以识别隐藏突破。", "method": "引入POLCA方法，在低秩训练子空间中沿任意基分解损失变化，识别样本聚类。", "result": "POLCA成功识别了模型能力中的可解释突破，验证了其在合成和自然语言任务中的有效性。", "conclusion": "隐藏的相变可作为无监督可解释性工具，POLCA为理解学习动态提供了新途径。"}}
{"id": "2506.17110", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17110", "abs": "https://arxiv.org/abs/2506.17110", "authors": ["Teng Guo", "Baichuan Huang", "Jingjin Yu"], "title": "Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping", "comment": "Accepted to IROS 2025", "summary": "Accurate 6D object pose estimation is a prerequisite for successfully\ncompleting robotic prehensile and non-prehensile manipulation tasks. At\npresent, 6D pose estimation for robotic manipulation generally relies on depth\nsensors based on, e.g., structured light, time-of-flight, and stereo-vision,\nwhich can be expensive, produce noisy output (as compared with RGB cameras),\nand fail to handle transparent objects. On the other hand, state-of-the-art\nmonocular depth estimation models (MDEMs) provide only affine-invariant depths\nup to an unknown scale and shift. Metric MDEMs achieve some successful\nzero-shot results on public datasets, but fail to generalize. We propose a\nnovel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover\nmetric depth from a single RGB image, through a one-shot adaptation building on\nMDEM techniques. MOMA performs scale-rotation-shift alignments during camera\ncalibration, guided by sparse ground-truth depth points, enabling accurate\ndepth estimation without additional data collection or model retraining on the\ntesting setup. MOMA supports fine-tuning the MDEM on transparent objects,\ndemonstrating strong generalization capabilities. Real-world experiments on\ntabletop 2-finger grasping and suction-based bin-picking applications show MOMA\nachieves high success rates in diverse tasks, confirming its effectiveness.", "AI": {"tldr": "论文提出了一种名为MOMA的单目RGB图像深度估计框架，通过一次性适配技术恢复度量深度，解决了现有方法在透明物体和泛化能力上的不足。", "motivation": "当前6D物体姿态估计依赖昂贵的深度传感器，且对透明物体效果不佳；单目深度估计模型（MDEMs）虽能提供深度信息，但缺乏度量精度和泛化能力。", "method": "提出MOMA框架，通过相机校准中的尺度-旋转-平移对齐，结合稀疏真实深度点，实现无需额外数据或模型重训练的单次适配。", "result": "MOMA在透明物体上表现优异，并在实际抓取和分拣任务中取得高成功率。", "conclusion": "MOMA通过一次性适配解决了单目深度估计的度量精度和泛化问题，适用于多种机器人操作任务。"}}
{"id": "2506.16407", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16407", "abs": "https://arxiv.org/abs/2506.16407", "authors": ["Dong Nguyen Tien", "Dung D. Le"], "title": "Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks", "comment": "8 pages, 1 figure, under review at EMNLP 2025", "summary": "Visual Document Understanding (VDU) systems have achieved strong performance\nin information extraction by integrating textual, layout, and visual signals.\nHowever, their robustness under realistic adversarial perturbations remains\ninsufficiently explored. We introduce the first unified framework for\ngenerating and evaluating multi-modal adversarial attacks on OCR-based VDU\nmodels. Our method covers six gradient-based layout attack scenarios,\nincorporating manipulations of OCR bounding boxes, pixels, and texts across\nboth word and line granularities, with constraints on layout perturbation\nbudget (e.g., IoU >= 0.6) to preserve plausibility.\n  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and\nsix model families demonstrate that line-level attacks and compound\nperturbations (BBox + Pixel + Text) yield the most severe performance\ndegradation. Projected Gradient Descent (PGD)-based BBox perturbations\noutperform random-shift baselines in all investigated models. Ablation studies\nfurther validate the impact of layout budget, text modification, and\nadversarial transferability.", "AI": {"tldr": "本文提出了首个统一框架，用于生成和评估基于OCR的视觉文档理解（VDU）模型的多模态对抗攻击，涵盖六种基于梯度的布局攻击场景，实验表明复合扰动和行级攻击对性能影响最大。", "motivation": "探索VDU系统在现实对抗扰动下的鲁棒性，填补现有研究的空白。", "method": "提出统一框架，结合OCR边界框、像素和文本的扰动，约束布局扰动预算以保持合理性。", "result": "实验显示行级攻击和复合扰动（BBox + Pixel + Text）导致性能下降最严重，PGD-based BBox扰动优于随机基线。", "conclusion": "布局预算、文本修改和对抗可转移性对VDU模型的鲁棒性有显著影响。"}}
{"id": "2506.15879", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15879", "abs": "https://arxiv.org/abs/2506.15879", "authors": ["Abdel Rahman Alsheyab", "Mohammad Alkhasawneh", "Nidal Shahin"], "title": "Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings", "comment": "8 pages, 5 figures, synthetic data only, experimental work", "summary": "This paper presents a machine learning methodology prototype using a large\nsynthetic dataset of job listings to identify trends, predict salaries, and\ngroup similar job roles. Employing techniques such as regression,\nclassification, clustering, and natural language processing (NLP) for\ntext-based feature extraction and representation, this study aims to uncover\nthe key features influencing job market dynamics and provide valuable insights\nfor job seekers, employers, and researchers. Exploratory data analysis was\nconducted to understand the dataset's characteristics. Subsequently, regression\nmodels were developed to predict salaries, classification models to predict job\ntitles, and clustering techniques were applied to group similar jobs. The\nanalyses revealed significant factors influencing salary and job roles, and\nidentified distinct job clusters based on the provided data. While the results\nare based on synthetic data and not intended for real-world deployment, the\nmethodology demonstrates a transferable framework for job market analysis.", "AI": {"tldr": "论文提出了一种基于合成数据的机器学习方法，用于分析职位市场趋势、预测薪资并分类相似职位。", "motivation": "研究旨在揭示影响职位市场动态的关键因素，为求职者、雇主和研究人员提供有价值的见解。", "method": "采用回归、分类、聚类和自然语言处理（NLP）技术，对合成数据集进行探索性分析，并构建预测模型。", "result": "分析揭示了影响薪资和职位角色的重要因素，并识别出基于数据的独特职位群集。", "conclusion": "虽然结果基于合成数据，但该方法为职位市场分析提供了一个可转移的框架。"}}
{"id": "2506.16418", "categories": ["cs.CV", "cs.AI", "eess.IV", "eess.SP", "68T07, 68T10, 94A08, 42C10"], "pdf": "https://arxiv.org/pdf/2506.16418", "abs": "https://arxiv.org/abs/2506.16418", "authors": ["Berk Yilmaz", "Daniel Fidel Harvey", "Prajit Dhuri"], "title": "Efficient Transformations in Deep Learning Convolutional Neural Networks", "comment": "All authors contributed equally to this work. 17 pages, 36\n  references, 10 figures, 1 appendix", "summary": "This study investigates the integration of signal processing transformations\n-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete\nCosine Transform (DCT) -- within the ResNet50 convolutional neural network\n(CNN) model for image classification. The primary objective is to assess the\ntrade-offs between computational efficiency, energy consumption, and\nclassification accuracy during training and inference. Using the CIFAR-100\ndataset (100 classes, 60,000 images), experiments demonstrated that\nincorporating WHT significantly reduced energy consumption while improving\naccuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy\nof 66%, consuming an average of 25,606 kJ per model. In contrast, a modified\nResNet50 incorporating WHT in the early convolutional layers achieved 74%\naccuracy, and an enhanced version with WHT applied to both early and late\nlayers achieved 79% accuracy, with an average energy consumption of only 39 kJ\nper model. These results demonstrate the potential of WHT as a highly efficient\nand effective approach for energy-constrained CNN applications.", "AI": {"tldr": "研究探讨了在ResNet50中集成FFT、WHT和DCT信号处理变换对图像分类的影响，发现WHT能显著降低能耗并提高准确率。", "motivation": "评估信号处理变换在CNN中的计算效率、能耗和分类准确性之间的权衡。", "method": "在ResNet50中集成FFT、WHT和DCT，使用CIFAR-100数据集进行实验。", "result": "WHT显著降低能耗（从25,606 kJ降至39 kJ），同时提高准确率（从66%提升至79%）。", "conclusion": "WHT是一种高效且有效的方法，适用于能耗受限的CNN应用。"}}
{"id": "2506.15881", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15881", "abs": "https://arxiv.org/abs/2506.15881", "authors": ["Alexey Yermakov", "David Zoro", "Mars Liyao Gao", "J. Nathan Kutz"], "title": "T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders", "comment": "16 pages, 5 figures, submitted to Transactions of the Royal Society\n  (Symbolic Regression in the Physical Sciences)", "summary": "SHallow REcurrent Decoders (SHRED) are effective for system identification\nand forecasting from sparse sensor measurements. Such models are light-weight\nand computationally efficient, allowing them to be trained on consumer laptops.\nSHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple\nMulti-Layer Perceptron (MLP) for the temporal encoding and spatial decoding\nrespectively. Despite the relatively simple structure of SHRED, they are able\nto predict chaotic dynamical systems on different physical, spatial, and\ntemporal scales directly from a sparse set of sensor measurements. In this\nwork, we improve SHRED by leveraging transformers (T-SHRED) for the temporal\nencoding which improves performance on next-step state prediction on large\ndatasets. We also introduce a sparse identification of nonlinear dynamics\n(SINDy) attention mechanism into T-SHRED to perform symbolic regression\ndirectly on the latent space as part of the model regularization architecture.\nSymbolic regression improves model interpretability by learning and\nregularizing the dynamics of the latent space during training. We analyze the\nperformance of T-SHRED on three different dynamical systems ranging from\nlow-data to high-data regimes. We observe that SINDy attention T-SHRED\naccurately predicts future frames based on an interpretable symbolic model\nacross all tested datasets.", "AI": {"tldr": "SHRED是一种轻量级模型，用于稀疏传感器数据的系统识别和预测。改进版T-SHRED引入Transformer和SINDy注意力机制，提升了预测性能和可解释性。", "motivation": "提升SHRED模型在稀疏传感器数据下的预测性能，并通过符号回归增强模型的可解释性。", "method": "使用Transformer替代RNN进行时间编码，并引入SINDy注意力机制进行符号回归。", "result": "T-SHRED在多种动态系统中表现优异，能够准确预测未来状态并生成可解释的符号模型。", "conclusion": "T-SHRED结合Transformer和SINDy注意力机制，显著提升了预测性能和模型可解释性。"}}
{"id": "2506.17198", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17198", "abs": "https://arxiv.org/abs/2506.17198", "authors": ["Jianglong Ye", "Keyi Wang", "Chengjing Yuan", "Ruihan Yang", "Yiquan Li", "Jiyue Zhu", "Yuzhe Qin", "Xueyan Zou", "Xiaolong Wang"], "title": "Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation", "comment": "Accepted to RSS 2025. Project page: https://jianglongye.com/dex1b", "summary": "Generating large-scale demonstrations for dexterous hand manipulation remains\nchallenging, and several approaches have been proposed in recent years to\naddress this. Among them, generative models have emerged as a promising\nparadigm, enabling the efficient creation of diverse and physically plausible\ndemonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and\nhigh-quality demonstration dataset produced with generative models. The dataset\ncontains one billion demonstrations for two fundamental tasks: grasping and\narticulation. To construct it, we propose a generative model that integrates\ngeometric constraints to improve feasibility and applies additional conditions\nto enhance diversity. We validate the model on both established and newly\nintroduced simulation benchmarks, where it significantly outperforms prior\nstate-of-the-art methods. Furthermore, we demonstrate its effectiveness and\nrobustness through real-world robot experiments. Our project page is at\nhttps://jianglongye.com/dex1b", "AI": {"tldr": "论文介绍了Dex1B数据集，通过生成模型高效创建了大规模、多样且高质量的灵巧手操作演示，显著优于现有方法。", "motivation": "灵巧手操作的大规模演示生成具有挑战性，生成模型为解决这一问题提供了新思路。", "method": "提出一种结合几何约束和多样性条件的生成模型，构建了包含十亿演示的Dex1B数据集。", "result": "在仿真和真实机器人实验中验证了模型的优越性和鲁棒性。", "conclusion": "Dex1B数据集和生成模型为灵巧手操作研究提供了高效工具。"}}
{"id": "2506.16421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16421", "abs": "https://arxiv.org/abs/2506.16421", "authors": ["Jan Skvrna", "Lukas Neumann"], "title": "Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution", "comment": null, "summary": "This paper presents the winning solution for the S23DR Challenge 2025, which\ninvolves predicting a house's 3D roof wireframe from a sparse point cloud and\nsemantic segmentations. Our method operates directly in 3D, first identifying\nvertex candidates from the COLMAP point cloud using Gestalt segmentations. We\nthen employ two PointNet-like models: one to refine and classify these\ncandidates by analyzing local cubic patches, and a second to predict edges by\nprocessing the cylindrical regions connecting vertex pairs. This two-stage, 3D\ndeep learning approach achieved a winning Hybrid Structure Score (HSS) of 0.43\non the private leaderboard.", "AI": {"tldr": "本文介绍了S23DR Challenge 2025的获胜方案，通过3D深度学习从稀疏点云和语义分割预测房屋的3D屋顶线框。", "motivation": "解决从稀疏点云和语义分割中预测3D屋顶线框的挑战。", "method": "直接在3D空间中操作，首先利用Gestalt分割从COLMAP点云识别顶点候选，然后使用两个PointNet-like模型：一个用于通过分析局部立方体补丁来细化和分类候选顶点，另一个用于通过处理连接顶点对的圆柱区域来预测边缘。", "result": "在私有排行榜上获得了0.43的混合结构分数（HSS）。", "conclusion": "两阶段的3D深度学习方法在预测3D屋顶线框任务中表现出色。"}}
{"id": "2506.15882", "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15882", "abs": "https://arxiv.org/abs/2506.15882", "authors": ["Sheng Liu", "Tianlang Chen", "Pan Lu", "Haotian Ye", "Yizheng Chen", "Lei Xing", "James Zou"], "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute", "comment": "18 pages, 5 figures, Project website:\n  https://shengliu66.github.io/fractreason/", "summary": "Test-time compute has emerged as a powerful paradigm for improving the\nperformance of large language models (LLMs), where generating multiple outputs\nor refining individual chains can significantly boost answer accuracy. However,\nexisting methods like Best-of-N, majority voting, and self-reflection typically\napply reasoning in a uniform way across inputs, overlooking the fact that\ndifferent problems may require different levels of reasoning depth. In this\nwork, we propose Fractional Reasoning, a training-free and model-agnostic\nframework that enables continuous control over reasoning intensity at inference\ntime, going beyond the limitations of fixed instructional prompts. Our method\noperates by extracting the latent steering vector associated with deeper\nreasoning and reapplying it with a tunable scaling factor, allowing the model\nto tailor its reasoning process to the complexity of each input. This supports\ntwo key modes of test-time scaling: (1) improving output quality in\nbreadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing\nthe correctness of individual reasoning chains in depth-based strategies (e.g.,\nself-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that\nFractional Reasoning consistently improves performance across diverse reasoning\ntasks and models.", "AI": {"tldr": "提出了Fractional Reasoning框架，通过动态调整推理强度提升LLM性能，适用于不同复杂度的输入。", "motivation": "现有方法（如Best-of-N、多数投票等）对输入统一处理，忽略了问题所需的推理深度差异。", "method": "提取与深度推理相关的潜在导向向量，并通过可调缩放因子动态调整推理强度。", "result": "在GSM8K、MATH500和GPQA等任务上，性能显著提升。", "conclusion": "Fractional Reasoning为测试时计算提供了灵活且高效的解决方案。"}}
{"id": "2506.16450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16450", "abs": "https://arxiv.org/abs/2506.16450", "authors": ["Giuseppe Lando", "Rosario Forte", "Giovanni Maria Farinella", "Antonino Furnari"], "title": "How Far Can Off-the-Shelf Multimodal Large Language Models Go in Online Episodic Memory Question Answering?", "comment": null, "summary": "We investigate whether off-the-shelf Multimodal Large Language Models (MLLMs)\ncan tackle Online Episodic-Memory Video Question Answering (OEM-VQA) without\nadditional training. Our pipeline converts a streaming egocentric video into a\nlightweight textual memory, only a few kilobytes per minute, via an MLLM\ndescriptor module, and answers multiple-choice questions by querying this\nmemory with an LLM reasoner module. On the QAEgo4D-Closed benchmark, our best\nconfiguration attains 56.0% accuracy with 3.6 kB per minute storage, matching\nthe performance of dedicated state-of-the-art systems while being 10**4/10**5\ntimes more memory-efficient. Extensive ablations provides insights into the\nrole of each component and design choice, and highlight directions of\nimprovement for future research.", "AI": {"tldr": "研究探讨了现成的多模态大语言模型（MLLMs）能否无需额外训练即可处理在线情景记忆视频问答（OEM-VQA）。通过将流式第一人称视频转换为轻量级文本记忆，并利用LLM推理模块回答问题，系统在QAEgo4D-Closed基准上达到56.0%准确率，存储效率比现有系统高10^4/10^5倍。", "motivation": "探索现成MLLMs在OEM-VQA任务中的潜力，避免额外训练成本。", "method": "将流式视频转换为轻量级文本记忆（3.6 kB/分钟），通过MLLM描述模块和LLM推理模块回答问题。", "result": "在QAEgo4D-Closed基准上达到56.0%准确率，存储效率显著优于现有系统。", "conclusion": "该方法展示了现成MLLMs在OEM-VQA中的有效性，同时为未来研究提供了改进方向。"}}
{"id": "2506.15893", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15893", "abs": "https://arxiv.org/abs/2506.15893", "authors": ["Farnam Mansouri", "Hans U. Simon", "Adish Singla", "Yuxin Chen", "Sandra Zilles"], "title": "Formal Models of Active Learning from Contrastive Examples", "comment": null, "summary": "Machine learning can greatly benefit from providing learning algorithms with\npairs of contrastive training examples -- typically pairs of instances that\ndiffer only slightly, yet have different class labels. Intuitively, the\ndifference in the instances helps explain the difference in the class labels.\nThis paper proposes a theoretical framework in which the effect of various\ntypes of contrastive examples on active learners is studied formally. The focus\nis on the sample complexity of learning concept classes and how it is\ninfluenced by the choice of contrastive examples. We illustrate our results\nwith geometric concept classes and classes of Boolean functions. Interestingly,\nwe reveal a connection between learning from contrastive examples and the\nclassical model of self-directed learning.", "AI": {"tldr": "论文研究了对比训练样本对机器学习的影响，提出了一个理论框架分析其对主动学习样本复杂度的作用。", "motivation": "通过对比样本（仅轻微差异但标签不同的实例）帮助解释标签差异，从而提升学习效果。", "method": "提出理论框架，研究不同类型对比样本对主动学习的影响，重点关注样本复杂度。", "result": "揭示了对比样本学习与自导向学习模型的联系，并通过几何概念类和布尔函数类验证结果。", "conclusion": "对比样本能显著影响学习效率，且与自导向学习存在理论关联。"}}
{"id": "2506.16497", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.16497", "abs": "https://arxiv.org/abs/2506.16497", "authors": ["Riccardo Ziglio", "Cecilia Pasquini", "Silvio Ranise"], "title": "Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors", "comment": "8 pages, 4 figures, workshop paper", "summary": "Face swapping manipulations in video streams represents an increasing threat\nin remote video communications, due to advances\n  in automated and real-time tools. Recent literature proposes to characterize\nand exploit visual artifacts introduced in video frames\n  by swapping algorithms when dealing with challenging physical scenes, such as\nface occlusions. This paper investigates the\n  effectiveness of this approach by benchmarking CNN-based data-driven models\non two data corpora (including a newly collected\n  one) and analyzing generalization capabilities with respect to different\nacquisition sources and swapping algorithms. The results\n  confirm excellent performance of general-purpose CNN architectures when\noperating within the same data source, but a significant\n  difficulty in robustly characterizing occlusion-based visual cues across\ndatasets. This highlights the need for specialized detection\n  strategies to deal with such artifacts.", "AI": {"tldr": "论文研究了视频流中换脸操作的检测方法，重点分析了遮挡场景下的视觉伪影，并测试了CNN模型在不同数据集上的泛化能力。", "motivation": "由于自动化实时换脸工具的进步，视频流中的换脸操作威胁增加，需要有效检测方法。", "method": "通过基准测试CNN模型在两个数据集（包括一个新收集的数据集）上的表现，分析其对不同采集源和换脸算法的泛化能力。", "result": "CNN模型在同一数据源下表现优异，但在跨数据集时难以鲁棒地捕捉遮挡相关的视觉线索。", "conclusion": "需要开发专门的检测策略以应对换脸操作中的视觉伪影。"}}
{"id": "2506.15896", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15896", "abs": "https://arxiv.org/abs/2506.15896", "authors": ["Yu Zhang", "Gaoshan Bi", "Simon Jeffery", "Max Davis", "Yang Li", "Qing Xue", "Po Yang"], "title": "KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction", "comment": "8 pages, 4 figures", "summary": "Precision soil greenhouse gas (GHG) flux prediction is essential in\nagricultural systems for assessing environmental impacts, developing emission\nmitigation strategies and promoting sustainable agriculture. Due to the lack of\nadvanced sensor and network technologies on majority of farms, there are\nchallenges in obtaining comprehensive and diverse agricultural data. As a\nresult, the scarcity of agricultural data seriously obstructs the application\nof machine learning approaches in precision soil GHG flux prediction. This\nresearch proposes a knowledge-guided graph neural network framework that\naddresses the above challenges by integrating knowledge embedded in an\nagricultural process-based model and graph neural network techniques.\nSpecifically, we utilise the agricultural process-based model to simulate and\ngenerate multi-dimensional agricultural datasets for 47 countries that cover a\nwide range of agricultural variables. To extract key agricultural features and\nintegrate correlations among agricultural features in the prediction process,\nwe propose a machine learning framework that integrates the autoencoder and\nmulti-target multi-graph based graph neural networks, which utilises the\nautoencoder to selectively extract significant agricultural features from the\nagricultural process-based model simulation data and the graph neural network\nto integrate correlations among agricultural features for accurately predict\nfertilisation-oriented soil GHG fluxes. Comprehensive experiments were\nconducted with both the agricultural simulation dataset and real-world\nagricultural dataset to evaluate the proposed approach in comparison with\nwell-known baseline and state-of-the-art regression methods. The results\ndemonstrate that our proposed approach provides superior accuracy and stability\nin fertilisation-oriented soil GHG prediction.", "AI": {"tldr": "论文提出了一种知识引导的图神经网络框架，用于解决农业数据稀缺问题，提高土壤温室气体通量预测的精度。", "motivation": "由于大多数农场缺乏先进的传感器和网络技术，农业数据稀缺，阻碍了机器学习在土壤温室气体通量预测中的应用。", "method": "结合农业过程模型和图神经网络技术，利用自动编码器提取关键特征，并通过多目标多图神经网络整合特征相关性。", "result": "实验表明，该方法在施肥导向的土壤温室气体预测中具有更高的准确性和稳定性。", "conclusion": "该框架为农业环境评估和可持续发展提供了有效的工具。"}}
{"id": "2506.16504", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16504", "abs": "https://arxiv.org/abs/2506.16504", "authors": ["Zeqiang Lai", "Yunfei Zhao", "Haolin Liu", "Zibo Zhao", "Qingxiang Lin", "Huiwen Shi", "Xianghui Yang", "Mingxin Yang", "Shuhui Yang", "Yifei Feng", "Sheng Zhang", "Xin Huang", "Di Luo", "Fan Yang", "Fang Yang", "Lifu Wang", "Sicong Liu", "Yixuan Tang", "Yulin Cai", "Zebin He", "Tian Liu", "Yuhong Liu", "Jie Jiang", "Linus", "Jingwei Huang", "Chunchao Guo"], "title": "Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details", "comment": "Technical report", "summary": "In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion\nmodels aimed at generating high-fidelity and detailed textured 3D assets.\nHunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D\n2.0, while demonstrating substantial advancements in both shape and texture\ngeneration. In terms of shape generation, we introduce a new shape foundation\nmodel -- LATTICE, which is trained with scaled high-quality datasets,\nmodel-size, and compute. Our largest model reaches 10B parameters and generates\nsharp and detailed 3D shape with precise image-3D following while keeping mesh\nsurface clean and smooth, significantly closing the gap between generated and\nhandcrafted 3D shapes. In terms of texture generation, it is upgraded with\nphyiscal-based rendering (PBR) via a novel multi-view architecture extended\nfrom Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D\n2.5 significantly outperforms previous methods in both shape and end-to-end\ntexture generation.", "AI": {"tldr": "Hunyuan3D 2.5是一套强大的3D扩散模型，通过两阶段流程生成高保真纹理3D资产，在形状和纹理生成上均有显著提升。", "motivation": "旨在缩小生成3D形状与手工制作3D形状之间的差距，并提升纹理生成的质量。", "method": "采用两阶段流程，引入新的形状基础模型LATTICE（10B参数），并升级纹理生成技术，使用基于物理的渲染（PBR）和多视图架构。", "result": "在形状和端到端纹理生成上显著优于先前方法。", "conclusion": "Hunyuan3D 2.5在3D资产生成领域取得了重要进展，为高质量3D内容生成提供了有效工具。"}}
{"id": "2506.15898", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15898", "abs": "https://arxiv.org/abs/2506.15898", "authors": ["Xiao Zhang", "Xingyu Zhao", "Hong Xia", "Yuan Cao", "Guiyuan Jiang", "Junyu Dong", "Yanwei Yu"], "title": "TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation", "comment": null, "summary": "With the proliferation of location-tracking technologies, massive volumes of\ntrajectory data are continuously being collected. As a fundamental task in\ntrajectory data mining, trajectory similarity computation plays a critical role\nin a wide range of real-world applications. However, existing learning-based\nmethods face three challenges: First, they ignore the semantic gap between GPS\nand grid features in trajectories, making it difficult to obtain meaningful\ntrajectory embeddings. Second, the noise inherent in the trajectories, as well\nas the noise introduced during grid discretization, obscures the true motion\npatterns of the trajectories. Third, existing methods focus solely on\npoint-wise and pair-wise losses, without utilizing the global ranking\ninformation obtained by sorting all trajectories according to their similarity\nto a given trajectory. To address the aforementioned challenges, we propose a\nnovel trajectory similarity computation framework, named TrajDiff.\nSpecifically, the semantic alignment module relies on cross-attention and an\nattention score mask mechanism with adaptive fusion, effectively eliminating\nsemantic discrepancies between data at two scales and generating a unified\nrepresentation. Additionally, the DDBM-based Noise-robust Pre-Training\nintroduces the transfer patterns between any two trajectories into the model\ntraining process, enhancing the model's noise robustness. Finally, the overall\nranking-aware regularization shifts the model's focus from a local to a global\nperspective, enabling it to capture the holistic ordering information among\ntrajectories. Extensive experiments on three publicly available datasets show\nthat TrajDiff consistently outperforms state-of-the-art baselines. In\nparticular, it achieves an average HR@1 gain of 33.38% across all three\nevaluation metrics and datasets.", "AI": {"tldr": "论文提出了一种名为TrajDiff的新框架，用于解决轨迹相似性计算中的语义差距、噪声问题和全局排序信息利用不足的挑战。", "motivation": "随着位置跟踪技术的普及，轨迹数据量激增，但现有学习方法在语义对齐、噪声鲁棒性和全局排序信息利用方面存在不足。", "method": "TrajDiff框架包含语义对齐模块（跨注意力和自适应融合）、基于DDBM的噪声鲁棒预训练和全局排序感知正则化。", "result": "在三个公开数据集上的实验表明，TrajDiff显著优于现有方法，平均HR@1提升33.38%。", "conclusion": "TrajDiff通过语义对齐、噪声鲁棒性和全局排序优化，有效提升了轨迹相似性计算的性能。"}}
{"id": "2506.16531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16531", "abs": "https://arxiv.org/abs/2506.16531", "authors": ["Mei Qi Tang", "Sean Sedwards", "Chengjie Huang", "Krzysztof Czarnecki"], "title": "How Hard Is Snow? A Paired Domain Adaptation Dataset for Clear and Snowy Weather: CADC+", "comment": "IEEE IV 2025", "summary": "The impact of snowfall on 3D object detection performance remains\nunderexplored. Conducting such an evaluation requires a dataset with sufficient\nlabelled data from both weather conditions, ideally captured in the same\ndriving environment. Current driving datasets with LiDAR point clouds either do\nnot provide enough labelled data in both snowy and clear weather conditions, or\nrely on de-snowing methods to generate synthetic clear weather. Synthetic data\noften lacks realism and introduces an additional domain shift that confounds\naccurate evaluations. To address these challenges, we present CADC+, the first\npaired weather domain adaptation dataset for autonomous driving in winter\nconditions. CADC+ extends the Canadian Adverse Driving Conditions dataset\n(CADC) using clear weather data that was recorded on the same roads and in the\nsame period as CADC. To create CADC+, we pair each CADC sequence with a clear\nweather sequence that matches the snowy sequence as closely as possible. CADC+\nthus minimizes the domain shift resulting from factors unrelated to the\npresence of snow. We also present some preliminary results using CADC+ to\nevaluate the effect of snow on 3D object detection performance. We observe that\nsnow introduces a combination of aleatoric and epistemic uncertainties, acting\nas both noise and a distinct data domain.", "AI": {"tldr": "CADC+是首个用于冬季自动驾驶的配对天气域适应数据集，解决了现有数据集中雪天和晴天数据不足或合成数据不真实的问题。", "motivation": "研究降雪对3D物体检测性能的影响，但现有数据集缺乏足够的雪天和晴天配对数据，或依赖不真实的合成数据。", "method": "通过扩展加拿大恶劣驾驶条件数据集（CADC），创建CADC+，将雪天序列与同路段、同时间的晴天序列配对，减少非雪因素引起的域偏移。", "result": "初步结果显示，降雪同时引入了随机性和认知不确定性，既表现为噪声，又作为独立的数据域。", "conclusion": "CADC+为研究雪天对3D物体检测的影响提供了更真实的数据基础，揭示了降雪对检测性能的双重影响。"}}
{"id": "2506.15901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15901", "abs": "https://arxiv.org/abs/2506.15901", "authors": ["Li Sun", "Shuheng Chen", "Yong Si", "Junyi Fan", "Maryam Pishgar", "Elham Pishgar", "Kamiar Alaei", "Greg Placencia"], "title": "Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach", "comment": null, "summary": "Background: Patients with both diabetes mellitus (DM) and atrial fibrillation\n(AF) face elevated mortality in intensive care units (ICUs), yet models\ntargeting this high-risk group remain limited.\n  Objective: To develop an interpretable machine learning (ML) model predicting\n28-day mortality in ICU patients with concurrent DM and AF using early-phase\nclinical data.\n  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF\nwas extracted from the MIMIC-IV database. Data preprocessing involved\nmedian/mode imputation, z-score normalization, and early temporal feature\nengineering. A two-step feature selection pipeline-univariate filtering (ANOVA\nF-test) and Random Forest-based multivariate ranking-yielded 19 interpretable\nfeatures. Seven ML models were trained with stratified 5-fold cross-validation\nand SMOTE oversampling. Interpretability was assessed via ablation and\nAccumulated Local Effects (ALE) analysis.\n  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%\nCI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,\nage, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects\nsuch as age-related risk acceleration and bilirubin thresholds.\n  Conclusion: This interpretable ML model offers accurate risk prediction and\nclinical insights for early ICU triage in patients with DM and AF.", "AI": {"tldr": "开发了一种可解释的机器学习模型，用于预测ICU中糖尿病和房颤患者的28天死亡率，逻辑回归表现最佳。", "motivation": "糖尿病和房颤患者ICU死亡率高，但针对该群体的预测模型有限。", "method": "使用MIMIC-IV数据库中的1,535例患者数据，通过特征选择和多种机器学习模型训练，评估模型性能和可解释性。", "result": "逻辑回归表现最佳（AUROC: 0.825），关键预测因子包括RAS、年龄、胆红素和拔管。", "conclusion": "该模型为糖尿病和房颤患者的早期ICU分诊提供了准确的风险预测和临床见解。"}}
{"id": "2506.16563", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16563", "abs": "https://arxiv.org/abs/2506.16563", "authors": ["Keyhan Najafian", "Farhad Maleki", "Lingling Jin", "Ian Stavness"], "title": "From Semantic To Instance: A Semi-Self-Supervised Learning Approach", "comment": null, "summary": "Instance segmentation is essential for applications such as automated\nmonitoring of plant health, growth, and yield. However, extensive effort is\nrequired to create large-scale datasets with pixel-level annotations of each\nobject instance for developing instance segmentation models that restrict the\nuse of deep learning in these areas. This challenge is more significant in\nimages with densely packed, self-occluded objects, which are common in\nagriculture. To address this challenge, we propose a semi-self-supervised\nlearning approach that requires minimal manual annotation to develop a\nhigh-performing instance segmentation model. We design GLMask, an image-mask\nrepresentation for the model to focus on shape, texture, and pattern while\nminimizing its dependence on color features. We develop a pipeline to generate\nsemantic segmentation and then transform it into instance-level segmentation.\nThe proposed approach substantially outperforms the conventional instance\nsegmentation models, establishing a state-of-the-art wheat head instance\nsegmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed\nmethodology on the general-purpose Microsoft COCO dataset, achieving a\nsignificant performance improvement of over 12.6% mAP@50. This highlights that\nthe utility of our proposed approach extends beyond precision agriculture and\napplies to other domains, specifically those with similar data characteristics.", "AI": {"tldr": "提出了一种半自监督学习方法GLMask，用于实例分割，减少了对大量手动标注的依赖，并在农业和通用数据集上表现优异。", "motivation": "解决农业图像中密集、自遮挡对象的实例分割问题，减少标注需求。", "method": "设计GLMask图像-掩码表示，关注形状、纹理和模式；通过语义分割生成实例分割。", "result": "在小麦头部实例分割上达到98.5% mAP@50，在COCO数据集上提升12.6%。", "conclusion": "该方法不仅适用于农业，还可推广到其他类似数据特征的领域。"}}
{"id": "2506.15903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15903", "abs": "https://arxiv.org/abs/2506.15903", "authors": ["Josef Kuchař", "Marek Kadlčík", "Michal Spiegel", "Michal Štefánik"], "title": "VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics", "comment": null, "summary": "We introduce a large-scale dataset for instruction-guided vector image\nediting, consisting of over 270,000 pairs of SVG images paired with natural\nlanguage edit instructions. Our dataset enables training and evaluation of\nmodels that modify vector graphics based on textual commands. We describe the\ndata collection process, including image pairing via CLIP similarity and\ninstruction generation with vision-language models. Initial experiments with\nstate-of-the-art large language models reveal that current methods struggle to\nproduce accurate and valid edits, underscoring the challenge of this task. To\nfoster research in natural language-driven vector graphic generation and\nediting, we make our resources created within this work publicly available.", "AI": {"tldr": "论文介绍了一个大规模的数据集，用于指令引导的矢量图像编辑，包含27万对SVG图像和自然语言编辑指令。", "motivation": "推动基于自然语言的矢量图形生成与编辑研究，提供训练和评估模型的资源。", "method": "通过CLIP相似性配对图像，并利用视觉语言模型生成指令。", "result": "实验表明现有大型语言模型难以生成准确有效的编辑结果。", "conclusion": "公开数据集以促进相关研究。"}}
{"id": "2506.16744", "categories": ["cs.LG", "cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.16744", "abs": "https://arxiv.org/abs/2506.16744", "authors": ["Eion Tyacke", "Kunal Gupta", "Jay Patel", "Rui Li"], "title": "IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification", "comment": null, "summary": "Hand gestures are a primary output of the human motor system, yet the\ndecoding of their neuromuscular signatures remains a bottleneck for basic\nneuroscience and assistive technologies such as prosthetics. Traditional\nhuman-machine interface pipelines rely on a single biosignal modality, but\nmultimodal fusion can exploit complementary information from sensors. We\nsystematically compare linear and attention-based fusion strategies across\nthree architectures: a Multimodal MLP, a Multimodal Transformer, and a\nHierarchical Transformer, evaluating performance on scenarios with unimodal and\nmultimodal inputs. Experiments use two publicly available datasets: NinaPro DB2\n(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).\nAcross both datasets, the Hierarchical Transformer with attention-based fusion\nconsistently achieved the highest accuracy, surpassing the multimodal and best\nsingle-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%\non HD-sEMG. To investigate how modalities interact, we introduce an Isolation\nNetwork that selectively silences unimodal or cross-modal attention pathways,\nquantifying each group of token interactions' contribution to downstream\ndecisions. Ablations reveal that cross-modal interactions contribute\napproximately 30% of the decision signal across transformer layers,\nhighlighting the importance of attention-driven fusion in harnessing\ncomplementary modality information. Together, these findings reveal when and\nhow multimodal fusion would enhance biosignal classification and also provides\nmechanistic insights of human muscle activities. The study would be beneficial\nin the design of sensor arrays for neurorobotic systems.", "AI": {"tldr": "论文研究了多模态融合策略在手势解码中的效果，发现基于注意力的分层Transformer性能最优，并揭示了跨模态交互的重要性。", "motivation": "手势解码是神经科学和辅助技术的关键瓶颈，传统方法依赖单一生物信号模态，多模态融合可提供互补信息。", "method": "比较了线性与基于注意力的融合策略，测试了三种架构（Multimodal MLP、Multimodal Transformer、Hierarchical Transformer），并使用公开数据集评估性能。", "result": "Hierarchical Transformer在两种数据集上表现最佳，准确率分别提升10%和3.7%。跨模态交互贡献了约30%的决策信号。", "conclusion": "研究揭示了多模态融合的优势和机制，为神经机器人系统的传感器设计提供了指导。"}}
{"id": "2506.16578", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16578", "abs": "https://arxiv.org/abs/2506.16578", "authors": ["Tongan Cai", "Haomiao Ni", "Wenchao Ma", "Yuan Xue", "Qian Ma", "Rachel Leicht", "Kelvin Wong", "John Volpi", "Stephen T. C. Wong", "James Z. Wang", "Sharon X. Huang"], "title": "SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage", "comment": "IPMI 2025", "summary": "Effective stroke triage in emergency settings often relies on clinicians'\nability to identify subtle abnormalities in facial muscle coordination. While\nrecent AI models have shown promise in detecting such patterns from patient\nfacial videos, their reliance on real patient data raises significant ethical\nand privacy challenges -- especially when training robust and generalizable\nmodels across institutions. To address these concerns, we propose SafeTriage, a\nnovel method designed to de-identify patient facial videos while preserving\nessential motion cues crucial for stroke diagnosis. SafeTriage leverages a\npretrained video motion transfer (VMT) model to map the motion characteristics\nof real patient faces onto synthetic identities. This approach retains\ndiagnostically relevant facial dynamics without revealing the patients'\nidentities. To mitigate the distribution shift between normal population\npre-training videos and patient population test videos, we introduce a\nconditional generative model for visual prompt tuning, which adapts the input\nspace of the VMT model to ensure accurate motion transfer without needing to\nfine-tune the VMT model backbone. Comprehensive evaluation, including\nquantitative metrics and clinical expert assessments, demonstrates that\nSafeTriage-produced synthetic videos effectively preserve stroke-relevant\nfacial patterns, enabling reliable AI-based triage. Our evaluations also show\nthat SafeTriage provides robust privacy protection while maintaining diagnostic\naccuracy, offering a secure and ethically sound foundation for data sharing and\nAI-driven clinical analysis in neurological disorders.", "AI": {"tldr": "SafeTriage是一种新方法，通过去识别化患者面部视频保留关键运动特征，解决AI模型在卒中分诊中的隐私和伦理问题。", "motivation": "解决AI模型依赖真实患者数据带来的隐私和伦理挑战，同时确保卒中诊断的准确性。", "method": "使用预训练视频运动转移模型将患者面部运动特征映射到合成身份上，并引入条件生成模型调整输入空间。", "result": "合成视频有效保留卒中相关面部模式，提供可靠的AI分诊，同时确保隐私保护。", "conclusion": "SafeTriage为神经疾病数据共享和AI临床分析提供了安全、伦理合规的基础。"}}
{"id": "2506.16753", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16753", "abs": "https://arxiv.org/abs/2506.16753", "authors": ["Kosuke Nakanishi", "Akihiro Kubo", "Yuji Yasui", "Shin Ishii"], "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation", "comment": "ICML2025 poster, 39 pages, 6 figures, 13 tables. arXiv admin note:\n  text overlap with arXiv:2409.00418", "summary": "Recently, robust reinforcement learning (RL) methods designed to handle\nadversarial input observations have received significant attention, motivated\nby RL's inherent vulnerabilities. While existing approaches have demonstrated\nreasonable success, addressing worst-case scenarios over long time horizons\nrequires both minimizing the agent's cumulative rewards for adversaries and\ntraining agents to counteract them through alternating learning. However, this\nprocess introduces mutual dependencies between the agent and the adversary,\nmaking interactions with the environment inefficient and hindering the\ndevelopment of off-policy methods. In this work, we propose a novel off-policy\nmethod that eliminates the need for additional environmental interactions by\nreformulating adversarial learning as a soft-constrained optimization problem.\nOur approach is theoretically supported by the symmetric property of policy\nevaluation between the agent and the adversary. The implementation is available\nat https://github.com/nakanakakosuke/VALT_SAC.", "AI": {"tldr": "提出了一种新的离策略方法，通过将对抗学习重新表述为软约束优化问题，消除了额外环境交互的需求。", "motivation": "现有方法在处理长期最坏情况时效率低下，且阻碍了离策略方法的发展。", "method": "将对抗学习重新表述为软约束优化问题，利用策略评估的对称性。", "result": "理论支持了方法的有效性，并提供了实现代码。", "conclusion": "新方法解决了现有方法的效率问题，为对抗强化学习提供了更优的解决方案。"}}
{"id": "2506.16589", "categories": ["cs.CV", "cs.AI", "cs.PF", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16589", "abs": "https://arxiv.org/abs/2506.16589", "authors": ["Tal Zeevi", "Eléonore V. Lieffrig", "Lawrence H. Staib", "John A. Onofrey"], "title": "Spatially-Aware Evaluation of Segmentation Uncertainty", "comment": "Presented at the 4th Workshop on Uncertainty Quantification for\n  Computer Vision (CVPR 2025), June 11, 2025. This version is not included in\n  the official proceedings", "summary": "Uncertainty maps highlight unreliable regions in segmentation predictions.\nHowever, most uncertainty evaluation metrics treat voxels independently,\nignoring spatial context and anatomical structure. As a result, they may assign\nidentical scores to qualitatively distinct patterns (e.g., scattered vs.\nboundary-aligned uncertainty). We propose three spatially aware metrics that\nincorporate structural and boundary information and conduct a thorough\nvalidation on medical imaging data from the prostate zonal segmentation\nchallenge within the Medical Segmentation Decathlon. Our results demonstrate\nimproved alignment with clinically important factors and better discrimination\nbetween meaningful and spurious uncertainty patterns.", "AI": {"tldr": "论文提出三种空间感知指标，用于评估医学图像分割中的不确定性，考虑了结构和边界信息，优于传统独立像素评估方法。", "motivation": "传统不确定性评估指标忽略空间上下文和解剖结构，导致对定性不同模式（如分散与边界对齐的不确定性）评分相同。", "method": "提出三种结合结构和边界信息的空间感知指标，并在前列腺分区分割挑战数据上验证。", "result": "新指标与临床重要因素更一致，能更好区分有意义和虚假的不确定性模式。", "conclusion": "空间感知指标在医学图像分割中更有效，能提供更可靠的不确定性评估。"}}
{"id": "2506.16826", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16826", "abs": "https://arxiv.org/abs/2506.16826", "authors": ["Sattwik Sahu", "Agamdeep Singh", "Karthik Nambiar", "Srikanth Saripalli", "P. B. Sujit"], "title": "AnyTraverse: An off-road traversability framework with VLM and human operator in the loop", "comment": null, "summary": "Off-road traversability segmentation enables autonomous navigation with\napplications in search-and-rescue, military operations, wildlife exploration,\nand agriculture. Current frameworks struggle due to significant variations in\nunstructured environments and uncertain scene changes, and are not adaptive to\nbe used for different robot types. We present AnyTraverse, a framework\ncombining natural language-based prompts with human-operator assistance to\ndetermine navigable regions for diverse robotic vehicles. The system segments\nscenes for a given set of prompts and calls the operator only when encountering\npreviously unexplored scenery or unknown class not part of the prompt in its\nregion-of-interest, thus reducing active supervision load while adapting to\nvarying outdoor scenes. Our zero-shot learning approach eliminates the need for\nextensive data collection or retraining. Our experimental validation includes\ntesting on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate\nreal-world deployment on multiple robot platforms. The results show that\nAnyTraverse performs better than GA-NAV and Off-seg while offering a\nvehicle-agnostic approach to off-road traversability that balances automation\nwith targeted human supervision.", "AI": {"tldr": "AnyTraverse是一个结合自然语言提示和人类操作员辅助的框架，用于分割可导航区域，适应多样化机器人类型，减少主动监督负担。", "motivation": "当前框架在非结构化环境中表现不佳，且无法适应不同机器人类型，需要一种更灵活的方法。", "method": "结合自然语言提示和人类操作员辅助，采用零样本学习，无需大量数据收集或重新训练。", "result": "在多个数据集和机器人平台上验证，性能优于GA-NAV和Off-seg。", "conclusion": "AnyTraverse提供了一种车辆无关的越野可通行性方法，平衡自动化与针对性人类监督。"}}
{"id": "2506.16601", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.16601", "abs": "https://arxiv.org/abs/2506.16601", "authors": ["Muhammad Azeem Aslam", "Muhammad Hamza", "Nisar Ahmed", "Gulshan Saleem", "Zhu Shuangtong", "Hu Hongfei", "Xu Wei", "Saba Aslam", "Wang Jun"], "title": "MetaQAP -- A Meta-Learning Approach for Quality-Aware Pretraining in Image Quality Assessment", "comment": null, "summary": "Image Quality Assessment (IQA) is a critical task in a wide range of\napplications but remains challenging due to the subjective nature of human\nperception and the complexity of real-world image distortions. This study\nproposes MetaQAP, a novel no-reference IQA model designed to address these\nchallenges by leveraging quality-aware pre-training and meta-learning. The\nmodel performs three key contributions: pre-training Convolutional Neural\nNetworks (CNNs) on a quality-aware dataset, implementing a quality-aware loss\nfunction to optimize predictions, and integrating a meta-learner to form an\nensemble model that effectively combines predictions from multiple base models.\nExperimental evaluations were conducted on three benchmark datasets: LiveCD,\nKonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional\nperformance with Pearson Linear Correlation Coefficient (PLCC) and Spearman\nRank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD,\n0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing\nIQA methods. Cross-dataset evaluations further demonstrated the\ngeneralizability of the model, with PLCC and SROCC scores ranging from 0.6721\nto 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The\nablation study confirmed the significance of each model component, revealing\nsubstantial performance degradation when critical elements such as the\nmeta-learner or quality-aware loss function were omitted. MetaQAP not only\naddresses the complexities of authentic distortions but also establishes a\nrobust and generalizable framework for practical IQA applications. By advancing\nthe state-of-the-art in no-reference IQA, this research provides valuable\ninsights and methodologies for future improvements and extensions in the field.", "AI": {"tldr": "MetaQAP是一种新型无参考图像质量评估模型，通过质量感知预训练和元学习解决IQA挑战，性能优于现有方法。", "motivation": "解决图像质量评估中主观性和复杂失真的挑战。", "method": "结合质量感知预训练CNN、质量感知损失函数和元学习器集成多个基模型。", "result": "在LiveCD、KonIQ-10K和BIQ2021数据集上表现优异，PLCC和SROCC分数显著高于现有方法。", "conclusion": "MetaQAP为无参考IQA提供了鲁棒且可推广的框架，推动了该领域的进步。"}}
{"id": "2506.15923", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.15923", "abs": "https://arxiv.org/abs/2506.15923", "authors": ["Liangyan Li", "Yangyi Liu", "Yimo Ning", "Stefano Rini", "Jun Chen"], "title": "PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning", "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for leveraging\ndiverse datasets from multiple sources while preserving data privacy by\navoiding centralized storage. However, many existing approaches fail to account\nfor the intricate gradient correlations between remote clients, a limitation\nthat becomes especially problematic in data heterogeneity scenarios. In this\nwork, we propose a novel FL framework utilizing Power-Norm Cosine Similarity\n(PNCS) to improve client selection for model aggregation. By capturing\nhigher-order gradient moments, PNCS addresses non-IID data challenges,\nenhancing convergence speed and accuracy. Additionally, we introduce a simple\nalgorithm ensuring diverse client selection through a selection history queue.\nExperiments with a VGG16 model across varied data partitions demonstrate\nconsistent improvements over state-of-the-art methods.", "AI": {"tldr": "本文提出了一种基于Power-Norm余弦相似度（PNCS）的联邦学习框架，通过捕捉高阶梯度矩来解决非独立同分布数据问题，提升收敛速度和准确性。", "motivation": "现有联邦学习方法未充分考虑远程客户端间的梯度相关性，尤其在数据异构场景下表现不佳。", "method": "使用PNCS改进客户端选择，并引入选择历史队列算法以确保多样性。", "result": "在VGG16模型上的实验显示，该方法在多种数据分区下均优于现有技术。", "conclusion": "PNCS框架有效解决了数据异构问题，提升了联邦学习的性能。"}}
{"id": "2506.16842", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.16842", "abs": "https://arxiv.org/abs/2506.16842", "authors": ["Chaehyeon Song", "Dongjae Lee", "Jongwoo Lim", "Ayoung Kim"], "title": "Camera Calibration via Circular Patterns: A Comprehensive Framework with Measurement Uncertainty and Unbiased Projection Model", "comment": null, "summary": "Camera calibration using planar targets has been widely favored, and two\ntypes of control points have been mainly considered as measurements: the\ncorners of the checkerboard and the centroid of circles. Since a centroid is\nderived from numerous pixels, the circular pattern provides more precise\nmeasurements than the checkerboard. However, the existing projection model of\ncircle centroids is biased under lens distortion, resulting in low performance.\nTo surmount this limitation, we propose an unbiased projection model of the\ncircular pattern and demonstrate its superior accuracy compared to the\ncheckerboard. Complementing this, we introduce uncertainty into circular\npatterns to enhance calibration robustness and completeness. Defining centroid\nuncertainty improves the performance of calibration components, including\npattern detection, optimization, and evaluation metrics. We also provide\nguidelines for performing good camera calibration based on the evaluation\nmetric. The core concept of this approach is to model the boundary points of a\ntwo-dimensional shape as a Markov random field, considering its connectivity.\nThe shape distribution is propagated to the centroid uncertainty through an\nappropriate shape representation based on the Green theorem. Consequently, the\nresulting framework achieves marked gains in calibration accuracy and\nrobustness. The complete source code and demonstration video are available at\nhttps://github.com/chaehyeonsong/discocal.", "AI": {"tldr": "论文提出了一种无偏的圆形图案投影模型，解决了现有圆形图案在镜头畸变下的偏差问题，并通过引入不确定性提升了校准的鲁棒性和准确性。", "motivation": "现有圆形图案的投影模型在镜头畸变下存在偏差，导致校准性能较低。", "method": "提出无偏的圆形图案投影模型，引入不确定性以优化校准的各个组件（如模式检测、优化和评估指标），并基于马尔可夫随机场建模二维形状的边界点。", "result": "新框架显著提高了校准的准确性和鲁棒性。", "conclusion": "通过无偏投影模型和不确定性定义，论文实现了更精确和鲁棒的相机校准，并提供了相关指南和开源代码。"}}
{"id": "2506.16647", "categories": ["cs.CV", "68T05 (Primary), 68T01 (Secondary)", "I.2.10; C.3; J.2"], "pdf": "https://arxiv.org/pdf/2506.16647", "abs": "https://arxiv.org/abs/2506.16647", "authors": ["Ajesh Thangaraj Nadar", "Gabriel Nixon Raj", "Soham Chandane", "Sushant Bhat"], "title": "Leveraging CNN and IoT for Effective E-Waste Management", "comment": "6 pages, 4 figures, published in 2023 7th International Conference on\n  I-SMAC IoT in Social Mobile Analytics and Cloud. Conference held in Kirtipur\n  Nepal from 11 to 13 October 2023", "summary": "The increasing proliferation of electronic devices in the modern era has led\nto a significant surge in electronic waste (e-waste). Improper disposal and\ninsufficient recycling of e-waste pose serious environmental and health risks.\nThis paper proposes an IoT-enabled system combined with a lightweight CNN-based\nclassification pipeline to enhance the identification, categorization, and\nrouting of e-waste materials. By integrating a camera system and a digital\nweighing scale, the framework automates the classification of electronic items\nbased on visual and weight-based attributes. The system demonstrates how\nreal-time detection of e-waste components such as circuit boards, sensors, and\nwires can facilitate smart recycling workflows and improve overall waste\nprocessing efficiency.", "AI": {"tldr": "论文提出了一种结合物联网和轻量级CNN的分类系统，用于电子废物的智能识别与分类。", "motivation": "现代电子设备激增导致电子废物问题严重，不当处理带来环境和健康风险。", "method": "集成摄像头和数字称重系统，通过视觉和重量特征自动化分类电子废物。", "result": "系统能实时检测电路板、传感器等组件，提升回收效率。", "conclusion": "该框架为智能回收流程提供了高效解决方案。"}}
{"id": "2506.15926", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.15926", "abs": "https://arxiv.org/abs/2506.15926", "authors": ["Soumya Basu"], "title": "Competing Bandits in Matching Markets via Super Stability", "comment": null, "summary": "We study bandit learning in matching markets with two-sided reward\nuncertainty, extending prior research primarily focused on single-sided\nuncertainty. Leveraging the concept of `super-stability' from Irving (1994), we\ndemonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the\nstandard GS algorithm in achieving true stable matchings under incomplete\ninformation. By employing the Extended GS algorithm, our centralized algorithm\nattains a logarithmic pessimal stable regret dependent on an instance-dependent\nadmissible gap parameter. This algorithm is further adapted to a decentralized\nsetting with a constant regret increase. Finally, we establish a novel\ncentralized instance-dependent lower bound for binary stable regret,\nelucidating the roles of the admissible gap and super-stable matching in\ncharacterizing the complexity of stable matching with bandit feedback.", "AI": {"tldr": "研究了双边奖励不确定性的匹配市场中强盗学习，扩展了之前主要关注单边不确定性的研究。通过引入Irving (1994)的“超稳定性”概念，展示了扩展Gale-Shapley (GS)算法在实现不完全信息下真实稳定匹配的优势。", "motivation": "现有研究多关注单边不确定性，而双边不确定性的匹配市场研究较少，因此探索其强盗学习问题具有重要意义。", "method": "采用扩展GS算法，提出了一种集中式算法，其遗憾与实例相关的可接受间隙参数呈对数关系，并进一步适应了去中心化设置。", "result": "集中式算法实现了对数级的最差稳定遗憾，去中心化设置下遗憾仅增加常数。同时，建立了新的集中式实例相关下界。", "conclusion": "扩展GS算法在双边不确定性匹配市场中表现优越，可接受间隙和超稳定匹配在稳定匹配复杂度中起关键作用。"}}
{"id": "2506.16663", "categories": ["cs.CV", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2506.16663", "abs": "https://arxiv.org/abs/2506.16663", "authors": ["Michael Gyimadu", "Gregory Bell"], "title": "A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques", "comment": null, "summary": "High-dimensional image data often require dimensionality reduction before\nfurther analysis. This paper provides a purely analytical comparison of two\nlinear techniques-Principal Component Analysis (PCA) and Singular Value\nDecomposition (SVD). After the derivation of each algorithm from first\nprinciples, we assess their interpretability, numerical stability, and\nsuitability for differing matrix shapes. building on classical and recent\nnumerical literature, We synthesize rule-of-thumb guidelines for choosing one\nout of the two algorithms without empirical benchmarking, building on classical\nand recent numerical literature. Limitations and directions for future\nexperimental work are outlined at the end.", "AI": {"tldr": "本文对PCA和SVD两种线性降维技术进行了纯理论比较，评估了它们的可解释性、数值稳定性及适用性，并提出了选择指南。", "motivation": "高维图像数据通常需要降维处理，但缺乏对PCA和SVD的纯理论比较。", "method": "从基本原理推导算法，评估其特性，并基于文献提出选择指南。", "result": "提供了选择PCA或SVD的实用建议，无需依赖经验基准测试。", "conclusion": "总结了两种技术的优缺点，并指出未来实验工作的方向。"}}
{"id": "2506.15933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15933", "abs": "https://arxiv.org/abs/2506.15933", "authors": ["Esther Rodriguez", "Monica Welfert", "Samuel McDowell", "Nathan Stromberg", "Julian Antolin Camarena", "Lalitha Sankar"], "title": "CORAL: Disentangling Latent Representations in Long-Tailed Diffusion", "comment": null, "summary": "Diffusion models have achieved impressive performance in generating\nhigh-quality and diverse synthetic data. However, their success typically\nassumes a class-balanced training distribution. In real-world settings,\nmulti-class data often follow a long-tailed distribution, where standard\ndiffusion models struggle -- producing low-diversity and lower-quality samples\nfor tail classes. While this degradation is well-documented, its underlying\ncause remains poorly understood. In this work, we investigate the behavior of\ndiffusion models trained on long-tailed datasets and identify a key issue: the\nlatent representations (from the bottleneck layer of the U-Net) for tail class\nsubspaces exhibit significant overlap with those of head classes, leading to\nfeature borrowing and poor generation quality. Importantly, we show that this\nis not merely due to limited data per class, but that the relative class\nimbalance significantly contributes to this phenomenon. To address this, we\npropose COntrastive Regularization for Aligning Latents (CORAL), a contrastive\nlatent alignment framework that leverages supervised contrastive losses to\nencourage well-separated latent class representations. Experiments demonstrate\nthat CORAL significantly improves both the diversity and visual quality of\nsamples generated for tail classes relative to state-of-the-art methods.", "AI": {"tldr": "论文研究了扩散模型在长尾数据分布下的表现问题，提出了一种对比潜在对齐框架（CORAL）以改善尾部类别的生成质量。", "motivation": "现实世界中的多类数据通常遵循长尾分布，而标准扩散模型在此类数据上表现不佳，尾部类别的生成样本多样性和质量较低。", "method": "通过分析扩散模型在长尾数据集上的行为，发现尾部类别的潜在表示与头部类别重叠严重。为此，提出了CORAL框架，利用监督对比损失来分离潜在类别表示。", "result": "实验表明，CORAL显著提高了尾部类别生成样本的多样性和视觉质量。", "conclusion": "CORAL通过对比潜在对齐有效解决了长尾数据分布下扩散模型的问题。"}}
{"id": "2506.17119", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17119", "abs": "https://arxiv.org/abs/2506.17119", "authors": ["Teng Guo", "Jingjin Yu"], "title": "RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking", "comment": "Accepted to IROS 2025", "summary": "We introduce a robust framework, RGBTrack, for real-time 6D pose estimation\nand tracking that operates solely on RGB data, thereby eliminating the need for\ndepth input for such dynamic and precise object pose tracking tasks. Building\non the FoundationPose architecture, we devise a novel binary search strategy\ncombined with a render-and-compare mechanism to efficiently infer depth and\ngenerate robust pose hypotheses from true-scale CAD models. To maintain stable\ntracking in dynamic scenarios, including rapid movements and occlusions,\nRGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman\nfilter and a state machine for proactive object pose recovery. In addition,\nRGBTrack's scale recovery module dynamically adapts CAD models of unknown scale\nusing an initial depth estimate, enabling seamless integration with modern\ngenerative reconstruction techniques. Extensive evaluations on benchmark\ndatasets demonstrate that RGBTrack's novel depth-free approach achieves\ncompetitive accuracy and real-time performance, making it a promising practical\nsolution candidate for application areas including robotics, augmented reality,\nand computer vision.\n  The source code for our implementation will be made publicly available at\nhttps://github.com/GreatenAnoymous/RGBTrack.git.", "AI": {"tldr": "RGBTrack是一个基于RGB数据的实时6D姿态估计与跟踪框架，无需深度输入，通过新颖的二进制搜索和渲染比较机制实现高效深度推断和姿态假设生成。", "motivation": "解决动态和精确物体姿态跟踪任务中依赖深度输入的问题，提供一种更灵活且高效的解决方案。", "method": "结合FoundationPose架构，采用二进制搜索策略和渲染比较机制，集成2D对象跟踪（XMem）、卡尔曼滤波和状态机以实现稳定跟踪。", "result": "在基准数据集上表现出竞争性精度和实时性能，适用于机器人、增强现实和计算机视觉领域。", "conclusion": "RGBTrack是一种无需深度输入的实用解决方案，具有广泛的应用潜力。"}}
{"id": "2506.16673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16673", "abs": "https://arxiv.org/abs/2506.16673", "authors": ["Ruiming Chen", "Junming Yang", "Shiyu Xia", "Xu Yang", "Jing Wang", "Xin Geng"], "title": "Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge", "comment": null, "summary": "CLIP (Contrastive Language-Image Pre-training) has attracted widespread\nattention for its multimodal generalizable knowledge, which is significant for\ndownstream tasks. However, the computational overhead of a large number of\nparameters and large-scale pre-training poses challenges of pre-training a\ndifferent scale of CLIP. Learngene extracts the generalizable components termed\nas learngene from an ancestry model and initializes diverse descendant models\nwith it. Previous Learngene paradigms fail to handle the generalizable\nknowledge in multimodal scenarios. In this paper, we put forward the idea of\nutilizing a multimodal block to extract the multimodal generalizable knowledge,\nwhich inspires us to propose MM-LG (Multimodal Learngene), a novel framework\ndesigned to extract and leverage generalizable components from CLIP.\nSpecifically, we first establish multimodal and unimodal blocks to extract the\nmultimodal and unimodal generalizable knowledge in a weighted-sum manner.\nSubsequently, we employ these components to numerically initialize descendant\nmodels of varying scales and modalities. Extensive experiments demonstrate\nMM-LG's effectiveness, which achieves performance gains over existing learngene\napproaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and\ncomparable or superior results to the pre-training and fine-tuning paradigm\n(e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG\nrequires only around 25% of the parameter storage while reducing around 2.8\ntimes pre-training costs for diverse model scales compared to the pre-training\nand fine-tuning paradigm, making it particularly suitable for efficient\ndeployment across diverse downstream tasks.", "AI": {"tldr": "MM-LG提出了一种从CLIP中提取多模态通用知识的新框架，通过加权和方式提取多模态和单模态知识，显著提升了性能并降低了计算成本。", "motivation": "解决现有Learngene方法在多模态场景中提取通用知识的不足，以及CLIP大规模预训练的计算开销问题。", "method": "设计多模态和单模态块，以加权和方式提取通用知识，并用于初始化不同规模和模态的模型。", "result": "在多个数据集上性能优于现有方法（如Oxford-IIIT PET提升3.1%），同时参数存储减少25%，预训练成本降低2.8倍。", "conclusion": "MM-LG是一种高效的多模态通用知识提取框架，适用于多样化下游任务部署。"}}
{"id": "2506.15943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15943", "abs": "https://arxiv.org/abs/2506.15943", "authors": ["Bruce Huang", "Ruida Zhou", "Lin F. Yang", "Suhas Diggavi"], "title": "On the optimal regret of collaborative personalized linear bandits", "comment": "30 pages, 4 figures", "summary": "Stochastic linear bandits are a fundamental model for sequential decision\nmaking, where an agent selects a vector-valued action and receives a noisy\nreward with expected value given by an unknown linear function. Although well\nstudied in the single-agent setting, many real-world scenarios involve multiple\nagents solving heterogeneous bandit problems, each with a different unknown\nparameter. Applying single agent algorithms independently ignores cross-agent\nsimilarity and learning opportunities. This paper investigates the optimal\nregret achievable in collaborative personalized linear bandits. We provide an\ninformation-theoretic lower bound that characterizes how the number of agents,\nthe interaction rounds, and the degree of heterogeneity jointly affect regret.\nWe then propose a new two-stage collaborative algorithm that achieves the\noptimal regret. Our analysis models heterogeneity via a hierarchical Bayesian\nframework and introduces a novel information-theoretic technique for bounding\nregret. Our results offer a complete characterization of when and how\ncollaboration helps with a optimal regret bound $\\tilde{O}(d\\sqrt{mn})$,\n$\\tilde{O}(dm^{1-\\gamma}\\sqrt{n})$, $\\tilde{O}(dm\\sqrt{n})$ for the number of\nrounds $n$ in the range of $(0, \\frac{d}{m \\sigma^2})$, $[\\frac{d}{m^{2\\gamma}\n\\sigma^2}, \\frac{d}{\\sigma^2}]$ and $(\\frac{d}{\\sigma^2}, \\infty)$\nrespectively, where $\\sigma$ measures the level of heterogeneity, $m$ is the\nnumber of agents, and $\\gamma\\in[0, 1/2]$ is an absolute constant. In contrast,\nagents without collaboration achieve a regret bound $O(dm\\sqrt{n})$ at best.", "AI": {"tldr": "该论文研究了多智能体协作的个性化线性赌博机问题，提出了信息论下界和最优遗憾的两阶段协作算法，分析了协作在不同条件下的效果。", "motivation": "现实场景中多智能体解决异构赌博机问题时，独立应用单智能体算法会忽略跨智能体的相似性和学习机会，因此需要研究协作的优化效果。", "method": "提出了一种新的两阶段协作算法，通过分层贝叶斯框架建模异构性，并引入信息论技术来限制遗憾。", "result": "论文给出了协作在不同条件下的最优遗憾界限，证明了协作能显著降低遗憾。", "conclusion": "协作在多智能体个性化线性赌博机问题中是有效的，尤其是在一定条件下能显著优于独立学习。"}}
{"id": "2506.17212", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17212", "abs": "https://arxiv.org/abs/2506.17212", "authors": ["Tianjiao Yu", "Vedant Shah", "Muntasir Wahed", "Ying Shen", "Kiet A. Nguyen", "Ismini Lourentzou"], "title": "Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting", "comment": null, "summary": "Articulated objects are common in the real world, yet modeling their\nstructure and motion remains a challenging task for 3D reconstruction methods.\nIn this work, we introduce Part$^{2}$GS, a novel framework for modeling\narticulated digital twins of multi-part objects with high-fidelity geometry and\nphysically consistent articulation. Part$^{2}$GS leverages a part-aware 3D\nGaussian representation that encodes articulated components with learnable\nattributes, enabling structured, disentangled transformations that preserve\nhigh-fidelity geometry. To ensure physically consistent motion, we propose a\nmotion-aware canonical representation guided by physics-based constraints,\nincluding contact enforcement, velocity consistency, and vector-field\nalignment. Furthermore, we introduce a field of repel points to prevent part\ncollisions and maintain stable articulation paths, significantly improving\nmotion coherence over baselines. Extensive evaluations on both synthetic and\nreal-world datasets show that Part$^{2}$GS consistently outperforms\nstate-of-the-art methods by up to 10$\\times$ in Chamfer Distance for movable\nparts.", "AI": {"tldr": "Part$^{2}$GS是一个用于建模多部分物体高保真几何和物理一致运动的框架，通过部分感知的3D高斯表示和物理约束实现。", "motivation": "现实中的关节物体建模仍具挑战性，现有方法难以兼顾高保真几何和物理一致性。", "method": "采用部分感知3D高斯表示，结合物理约束（接触、速度、向量场对齐）和排斥点场防止碰撞。", "result": "在合成和真实数据集上，Part$^{2}$GS在可移动部分的Chamfer距离上优于现有方法10倍。", "conclusion": "Part$^{2}$GS为关节物体的高保真建模和物理一致运动提供了有效解决方案。"}}
{"id": "2506.16679", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16679", "abs": "https://arxiv.org/abs/2506.16679", "authors": ["Manuel Brack", "Sudeep Katakol", "Felix Friedrich", "Patrick Schramowski", "Hareesh Ravi", "Kristian Kersting", "Ajinkya Kale"], "title": "How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions", "comment": null, "summary": "Training data is at the core of any successful text-to-image models. The\nquality and descriptiveness of image text are crucial to a model's performance.\nGiven the noisiness and inconsistency in web-scraped datasets, recent works\nshifted towards synthetic training captions. While this setup is generally\nbelieved to produce more capable models, current literature does not provide\nany insights into its design choices. This study closes this gap by\nsystematically investigating how different synthetic captioning strategies\nimpact the downstream performance of text-to-image models. Our experiments\ndemonstrate that dense, high-quality captions enhance text alignment but may\nintroduce trade-offs in output aesthetics and diversity. Conversely, captions\nof randomized lengths yield balanced improvements across aesthetics and\nalignment without compromising sample diversity. We also demonstrate that\nvarying caption distributions introduce significant shifts in the output bias\nof a trained model. Our findings underscore the importance of caption design in\nachieving optimal model performance and provide practical insights for more\neffective training data strategies in text-to-image generation.", "AI": {"tldr": "研究探讨了合成标注策略对文本到图像模型性能的影响，发现高质量标注提升文本对齐但可能牺牲美学和多样性，而随机长度标注则能平衡美学与对齐。", "motivation": "现有研究缺乏对合成标注设计选择的深入分析，本文旨在填补这一空白。", "method": "系统研究不同合成标注策略对模型性能的影响，包括标注密度、质量和长度变化。", "result": "高质量标注提升文本对齐但可能降低美学和多样性；随机长度标注能平衡美学与对齐；标注分布影响模型输出偏差。", "conclusion": "标注设计对模型性能至关重要，研究为文本到图像生成提供了更有效的训练数据策略。"}}
{"id": "2506.15954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15954", "abs": "https://arxiv.org/abs/2506.15954", "authors": ["Vinicius Yuiti Fukase", "Heitor Gama", "Barbara Bueno", "Lucas Libanio", "Anna Helena Reali Costa", "Artur Jordao"], "title": "One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks", "comment": null, "summary": "Critical Learning Periods comprehend an important phenomenon involving deep\nlearning, where early epochs play a decisive role in the success of many\ntraining recipes, such as data augmentation. Existing works confirm the\nexistence of this phenomenon and provide useful insights. However, the\nliterature lacks efforts to precisely identify when critical periods occur. In\nthis work, we fill this gap by introducing a systematic approach for\nidentifying critical periods during the training of deep neural networks,\nfocusing on eliminating computationally intensive regularization techniques and\neffectively applying mechanisms for reducing computational costs, such as data\npruning. Our method leverages generalization prediction mechanisms to pinpoint\ncritical phases where training recipes yield maximum benefits to the predictive\nability of models. By halting resource-intensive recipes beyond these periods,\nwe significantly accelerate the learning phase and achieve reductions in\ntraining time, energy consumption, and CO$_2$ emissions. Experiments on\nstandard architectures and benchmarks confirm the effectiveness of our method.\nSpecifically, we achieve significant milestones by reducing the training time\nof popular architectures by up to 59.67%, leading to a 59.47% decrease in\nCO$_2$ emissions and a 60% reduction in financial costs, without compromising\nperformance. Our work enhances understanding of training dynamics and paves the\nway for more sustainable and efficient deep learning practices, particularly in\nresource-constrained environments. In the era of the race for foundation\nmodels, we believe our method emerges as a valuable framework. The repository\nis available at https://github.com/baunilhamarga/critical-periods", "AI": {"tldr": "本文提出了一种系统方法来识别深度神经网络训练中的关键学习期，通过减少计算成本和资源消耗，显著提升了训练效率。", "motivation": "现有研究确认了关键学习期的存在，但缺乏精确识别其发生时间的方法。本文旨在填补这一空白。", "method": "利用泛化预测机制识别关键期，停止资源密集型训练方法，结合数据剪枝等技术降低计算成本。", "result": "实验显示，训练时间减少59.67%，CO$_2$排放减少59.47%，成本降低60%，且不影响性能。", "conclusion": "该方法为深度学习提供了更高效、可持续的实践方案，特别适用于资源受限环境。"}}
{"id": "2506.17213", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.17213", "abs": "https://arxiv.org/abs/2506.17213", "authors": ["Xiuyu Yang", "Shuhan Tan", "Philipp Krähenbühl"], "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation", "comment": "Preprint. Project page: https://orangesodahub.github.io/InfGen Code:\n  https://github.com/OrangeSodahub/infgen", "summary": "An ideal traffic simulator replicates the realistic long-term point-to-point\ntrip that a self-driving system experiences during deployment. Prior models and\nbenchmarks focus on closed-loop motion simulation for initial agents in a\nscene. This is problematic for long-term simulation. Agents enter and exit the\nscene as the ego vehicle enters new regions. We propose InfGen, a unified\nnext-token prediction model that performs interleaved closed-loop motion\nsimulation and scene generation. InfGen automatically switches between\nclosed-loop motion simulation and scene generation mode. It enables stable\nlong-term rollout simulation. InfGen performs at the state-of-the-art in\nshort-term (9s) traffic simulation, and significantly outperforms all other\nmethods in long-term (30s) simulation. The code and model of InfGen will be\nreleased at https://orangesodahub.github.io/InfGen", "AI": {"tldr": "InfGen是一个统一的下一代预测模型，用于交替进行闭环运动模拟和场景生成，显著提升了长期交通模拟的性能。", "motivation": "现有模型和基准测试主要关注场景中初始代理的闭环运动模拟，难以支持长期模拟中代理的进出场景需求。", "method": "提出InfGen模型，通过交替进行闭环运动模拟和场景生成，实现稳定的长期模拟。", "result": "InfGen在短期（9秒）交通模拟中达到最先进水平，在长期（30秒）模拟中显著优于其他方法。", "conclusion": "InfGen为长期交通模拟提供了有效解决方案，代码和模型将开源。"}}
{"id": "2506.16690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16690", "abs": "https://arxiv.org/abs/2506.16690", "authors": ["Yun Xing", "Yue Cao", "Nhat Chung", "Jie Zhang", "Ivor Tsang", "Ming-Ming Cheng", "Yang Liu", "Lei Ma", "Qing Guo"], "title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches", "comment": null, "summary": "Stereo Depth estimation is a critical task in autonomous driving and\nrobotics, where inaccuracies (such as misidentifying nearby objects as distant)\ncan lead to dangerous situations. Adversarial attacks against stereo depth\nestimation can help reveal vulnerabilities before deployment. Previous work has\nshown that repeating optimized textures can effectively mislead stereo depth\nestimation in digital settings. However, our research reveals that these\nnaively repeated texture structures perform poorly in physical-world\nimplementations, i.e., when deployed as patches, limiting their practical\nutility for testing stereo depth estimation systems. In this work, for the\nfirst time, we discover that introducing regular intervals between repeated\ntextures, creating a striped structure, significantly enhances the patch attack\neffectiveness. Through extensive experimentation, we analyze how variations of\nthis novel structure influence the performance. Based on these insights, we\ndevelop a novel stereo depth attack that jointly optimizes both the striped\nstructure and texture elements. Our generated adversarial patches can be\ninserted into any scenes and successfully attack state-of-the-art stereo depth\nestimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can\nalso attack commercial RGB-D cameras (Intel RealSense) in real-world\nconditions, demonstrating their practical relevance for security assessment of\nstereo systems.", "AI": {"tldr": "论文提出了一种新型的条纹结构对抗补丁，显著提升了物理世界中立体深度估计攻击的效果。", "motivation": "立体深度估计在自动驾驶和机器人技术中至关重要，但其易受对抗攻击。现有方法在物理世界中效果不佳，限制了实际应用。", "method": "通过引入条纹结构（在重复纹理中加入间隔），并联合优化条纹结构和纹理元素，生成对抗补丁。", "result": "生成的补丁能有效攻击主流立体深度估计方法（如RAFT-Stereo和STTR）及商用RGB-D相机（Intel RealSense）。", "conclusion": "条纹结构显著提升了对抗补丁的物理世界攻击效果，为立体系统的安全评估提供了实用工具。"}}
{"id": "2506.15963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15963", "abs": "https://arxiv.org/abs/2506.15963", "authors": ["Jingyi Cui", "Qi Zhang", "Yifei Wang", "Yisen Wang"], "title": "On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond", "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting\nfeatures learned by large language models (LLMs). It aims to recover complex\nsuperposed polysemantic features into interpretable monosemantic ones through\nfeature reconstruction via sparsely activated neural networks. Despite the wide\napplications of SAEs, it remains unclear under what conditions an SAE can fully\nrecover the ground truth monosemantic features from the superposed polysemantic\nones. In this paper, through theoretical analysis, we for the first time\npropose the necessary and sufficient conditions for identifiable SAEs (SAEs\nthat learn unique and ground truth monosemantic features), including 1) extreme\nsparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)\nenough hidden dimensions of SAEs. Moreover, when the identifiable conditions\nare not fully met, we propose a reweighting strategy to improve the\nidentifiability. Specifically, following the theoretically suggested weight\nselection principle, we prove that the gap between the loss functions of SAE\nreconstruction and monosemantic feature reconstruction can be narrowed, so that\nthe reweighted SAEs have better reconstruction of the ground truth monosemantic\nfeatures than the uniformly weighted ones. In experiments, we validate our\ntheoretical findings and show that our weighted SAE significantly improves\nfeature monosemanticity and interpretability.", "AI": {"tldr": "本文提出了稀疏自编码器（SAE）在恢复单语义特征时的必要和充分条件，并提出了一种加权策略以提高可识别性。实验验证了理论发现。", "motivation": "尽管SAE在解释大型语言模型特征方面应用广泛，但其在何种条件下能完全恢复单语义特征尚不明确。", "method": "通过理论分析提出SAE可识别的三个条件，并提出加权策略优化损失函数。", "result": "实验证明加权SAE显著提高了特征的单语义性和可解释性。", "conclusion": "本文为SAE的可识别性提供了理论支持，并通过加权策略改进了特征恢复效果。"}}
{"id": "2506.16691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16691", "abs": "https://arxiv.org/abs/2506.16691", "authors": ["Tongtian Yue", "Longteng Guo", "Yepeng Tang", "Zijia Zhao", "Xinxin Zhu", "Hua Huang", "Jing Liu"], "title": "LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation", "comment": null, "summary": "Despite the impressive advancements of Large Vision-Language Models (LVLMs),\nexisting approaches suffer from a fundamental bottleneck: inefficient\nvisual-language integration. Current methods either disrupt the model's\ninherent structure or introduce severe long-context computational burden,\nseverely limiting scalability and efficiency. In this paper, we rethink\nmultimodal integration and present LaVi, a novel LVLM that enables seamless and\nefficient vision-language fusion through internal feature modulation within the\nLarge Language Models (LLMs). Unlike dominant LVLMs that rely on visual token\nconcatenation, LaVi bypasses long-context expansion by introducing a\nlightweight and adaptive transformation, which incorporates visual context by\ninjecting token-wise vision-conditioned deltas into the affine parameters of\nlayer normalization. This mechanism directly modulates linguistic hidden states\nbased on visual input, ensuring precise vision-language alignment while\npreserving the LLM's linguistic priors and drastically reducing computational\ncosts. Extensive evaluations across 15 image and video benchmarks demonstrate\nthat LaVi not only achieves state-of-the-art multimodal performance but also\ndramatically enhances efficiency. Compared to LLaVA-OV-7B, LaVi reduces FLOPs\nby 94.0%, improves inference speed by 3.1 times, and cuts memory usage in half\n- establishing LaVi as a scalable and practical solution for real-time\nmultimodal reasoning. The code and models will be released soon.", "AI": {"tldr": "LaVi提出了一种新型大型视觉语言模型，通过内部特征调制实现高效视觉语言融合，显著提升性能和效率。", "motivation": "现有大型视觉语言模型在视觉语言集成上效率低下，限制了可扩展性和效率。", "method": "LaVi通过轻量级自适应变换，将视觉条件化的增量注入层归一化的仿射参数中，避免长上下文扩展。", "result": "在15个图像和视频基准测试中，LaVi实现了最先进的多模态性能，FLOPs减少94.0%，推理速度提升3.1倍，内存使用减半。", "conclusion": "LaVi是一种可扩展且实用的实时多模态推理解决方案。"}}
{"id": "2506.15969", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15969", "abs": "https://arxiv.org/abs/2506.15969", "authors": ["Haoyue Zhang", "Hualei Zhang", "Xiaosong Ma", "Jie Zhang", "Song Guo"], "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning", "comment": null, "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.", "AI": {"tldr": "论文提出LazyEviction框架，通过延迟KV缓存淘汰减少内存占用，同时保持推理性能。", "motivation": "现有KV缓存压缩方法在长推理任务中表现不佳，未能捕捉Token Importance Recurrence现象。", "method": "提出LazyEviction框架，包含Recurrence Interval Tracking和Maximum Recurrence Interval-Centric Eviction Policy。", "result": "实验显示LazyEviction将KV缓存减少50%，同时保持数学推理数据集的准确性。", "conclusion": "保留周期性关键token对多步推理任务的知识连续性至关重要。"}}
{"id": "2506.16701", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16701", "abs": "https://arxiv.org/abs/2506.16701", "authors": ["Xiaodan Hu", "Chuhang Zou", "Suchen Wang", "Jaechul Kim", "Narendra Ahuja"], "title": "Language-driven Description Generation and Common Sense Reasoning for Video Action Recognition", "comment": null, "summary": "Recent video action recognition methods have shown excellent performance by\nadapting large-scale pre-trained language-image models to the video domain.\nHowever, language models contain rich common sense priors - the scene contexts\nthat humans use to constitute an understanding of objects, human-object\ninteractions, and activities - that have not been fully exploited. In this\npaper, we introduce a framework incorporating language-driven common sense\npriors to identify cluttered video action sequences from monocular views that\nare often heavily occluded. We propose: (1) A video context summary component\nthat generates candidate objects, activities, and the interactions between\nobjects and activities; (2) A description generation module that describes the\ncurrent scene given the context and infers subsequent activities, through\nauxiliary prompts and common sense reasoning; (3) A multi-modal activity\nrecognition head that combines visual and textual cues to recognize video\nactions. We demonstrate the effectiveness of our approach on the challenging\nAction Genome and Charades datasets.", "AI": {"tldr": "该论文提出了一种结合语言驱动常识先验的框架，用于从单目视角识别遮挡严重的视频动作序列，通过多模态方法提升动作识别性能。", "motivation": "现有方法未充分利用语言模型中丰富的常识先验（如场景上下文），而这些先验对人类理解对象、交互和活动至关重要。", "method": "1. 视频上下文总结组件生成候选对象、活动及交互；2. 描述生成模块通过辅助提示和常识推理描述场景并推断后续活动；3. 多模态活动识别头结合视觉和文本线索识别动作。", "result": "在Action Genome和Charades数据集上验证了方法的有效性。", "conclusion": "结合语言驱动的常识先验能显著提升复杂视频动作识别的性能。"}}
{"id": "2506.16001", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16001", "abs": "https://arxiv.org/abs/2506.16001", "authors": ["Qianru Zhang", "Honggang Wen", "Ming Li", "Dong Huang", "Siu-Ming Yiu", "Christian S. Jensen", "Pietro Liò"], "title": "AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction", "comment": "14 pages", "summary": "Time series forecasting requires architectures that simultaneously achieve\nthree competing objectives: (1) strict temporal causality for reliable\npredictions, (2) sub-quadratic complexity for practical scalability, and (3)\nmulti-scale pattern recognition for accurate long-horizon forecasting. We\nintroduce AutoHFormer, a hierarchical autoregressive transformer that addresses\nthese challenges through three key innovations: 1) Hierarchical Temporal\nModeling: Our architecture decomposes predictions into segment-level blocks\nprocessed in parallel, followed by intra-segment sequential refinement. This\ndual-scale approach maintains temporal coherence while enabling efficient\ncomputation. 2) Dynamic Windowed Attention: The attention mechanism employs\nlearnable causal windows with exponential decay, reducing complexity while\npreserving precise temporal relationships. This design avoids both the\nanti-causal violations of standard transformers and the sequential bottlenecks\nof RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system\nis adopted to capture time patterns at multiple scales. It combines fixed\noscillating patterns for short-term variations with learnable decay rates for\nlong-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X\nfaster training and 6.06X memory reduction compared to PatchTST on PEMS08,\nwhile maintaining consistent accuracy across 96-720 step horizons in most of\ncases. These breakthroughs establish new benchmarks for efficient and precise\ntime series modeling. Implementations of our method and all baselines in\nhierarchical autoregressive mechanism are available at\nhttps://github.com/lizzyhku/Autotime.", "AI": {"tldr": "AutoHFormer是一种层次自回归Transformer，通过分层时间建模、动态窗口注意力和自适应时间编码解决了时间序列预测的三个关键挑战，实现了高效且精确的预测。", "motivation": "时间序列预测需要同时满足严格的时间因果性、次二次复杂度和多尺度模式识别，现有方法难以兼顾这些目标。", "method": "1) 分层时间建模：并行处理段级块，再顺序细化；2) 动态窗口注意力：使用可学习因果窗口；3) 自适应时间编码：结合固定振荡模式和可学习衰减率。", "result": "AutoHFormer在PEMS08上训练速度提升10.76倍，内存减少6.06倍，同时在96-720步预测范围内保持高精度。", "conclusion": "AutoHFormer为高效且精确的时间序列建模设定了新基准。"}}
{"id": "2506.16728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16728", "abs": "https://arxiv.org/abs/2506.16728", "authors": ["Yunhan Ren", "Feng Luo", "Siyu Huang"], "title": "Few-Shot Generalized Category Discovery With Retrieval-Guided Decision Boundary Enhancement", "comment": "Accepted by ICMR 2025", "summary": "While existing Generalized Category Discovery (GCD) models have achieved\nsignificant success, their performance with limited labeled samples and a small\nnumber of known categories remains largely unexplored. In this work, we\nintroduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming\nto achieve competitive performance in GCD tasks under conditions of known\ninformation scarcity. To tackle this challenge, we propose a decision boundary\nenhancement framework with affinity-based retrieval. Our framework is designed\nto learn the decision boundaries of known categories and transfer these\nboundaries to unknown categories. First, we use a decision boundary\npre-training module to mitigate the overfitting of pre-trained information on\nknown category boundaries and improve the learning of these decision boundaries\nusing labeled samples. Second, we implement a two-stage retrieval-guided\ndecision boundary optimization strategy. Specifically, this strategy further\nenhances the severely limited known boundaries by using affinity-retrieved\npseudo-labeled samples. Then, these refined boundaries are applied to unknown\nclusters via guidance from affinity-based feature retrieval. Experimental\nresults demonstrate that our proposed method outperforms existing methods on\nsix public GCD benchmarks under the FSGCD setting. The codes are available at:\nhttps://github.com/Ryh1218/FSGCD", "AI": {"tldr": "本文提出了Few-shot Generalized Category Discovery (FSGCD)任务，旨在在已知信息稀缺的情况下提升GCD性能，并提出了一种基于决策边界增强和亲和力检索的框架。", "motivation": "现有GCD模型在有限标记样本和少量已知类别下的性能尚未充分研究，FSGCD任务旨在填补这一空白。", "method": "提出决策边界增强框架，包括预训练模块和两阶段检索优化策略，利用亲和力检索伪标记样本优化边界。", "result": "在六个公共GCD基准测试中，该方法在FSGCD设置下优于现有方法。", "conclusion": "该方法通过决策边界增强和亲和力检索，显著提升了FSGCD任务的性能。"}}
{"id": "2506.16009", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16009", "abs": "https://arxiv.org/abs/2506.16009", "authors": ["Hamdi Altaheri", "Fakhri Karray", "Md. Milon Islam", "S M Taslim Uddin Raju", "Amir-Hossein Karimi"], "title": "Bridging Brain with Foundation Models through Self-Supervised Learning", "comment": null, "summary": "Foundation models (FMs), powered by self-supervised learning (SSL), have\nredefined the capabilities of artificial intelligence, demonstrating\nexceptional performance in domains like natural language processing and\ncomputer vision. These advances present a transformative opportunity for brain\nsignal analysis. Unlike traditional supervised learning, which is limited by\nthe scarcity of labeled neural data, SSL offers a promising solution by\nenabling models to learn meaningful representations from unlabeled data. This\nis particularly valuable in addressing the unique challenges of brain signals,\nincluding high noise levels, inter-subject variability, and low signal-to-noise\nratios. This survey systematically reviews the emerging field of bridging brain\nsignals with foundation models through the innovative application of SSL. It\nexplores key SSL techniques, the development of brain-specific foundation\nmodels, their adaptation to downstream tasks, and the integration of brain\nsignals with other modalities in multimodal SSL frameworks. The review also\ncovers commonly used evaluation metrics and benchmark datasets that support\ncomparative analysis. Finally, it highlights key challenges and outlines future\nresearch directions. This work aims to provide researchers with a structured\nunderstanding of this rapidly evolving field and a roadmap for developing\ngeneralizable brain foundation models powered by self-supervision.", "AI": {"tldr": "该论文综述了自监督学习在脑信号分析中的应用，探讨了基础模型的潜力、关键技术与挑战，并提出了未来研究方向。", "motivation": "传统监督学习受限于标记数据的稀缺性，而自监督学习能够从未标记数据中学习有意义的表示，为脑信号分析提供了新机遇。", "method": "系统回顾了自监督学习技术、脑信号基础模型的开发、下游任务适配以及多模态自监督框架的整合。", "result": "总结了常用评估指标和基准数据集，并分析了自监督学习在脑信号分析中的潜力与挑战。", "conclusion": "该研究为开发通用脑信号基础模型提供了结构化理解和未来路线图。"}}
{"id": "2506.16730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16730", "abs": "https://arxiv.org/abs/2506.16730", "authors": ["Mingrui Zhu", "Xiru Chen", "Xin Wei", "Nannan Wang", "Xinbo Gao"], "title": "TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion", "comment": "11 pages, 6 figures", "summary": "Infrared and visible image fusion (IVF) aims to combine complementary\ninformation from both image modalities, producing more informative and\ncomprehensive outputs. Recently, text-guided IVF has shown great potential due\nto its flexibility and versatility. However, the effective integration and\nutilization of textual semantic information remains insufficiently studied. To\ntackle these challenges, we introduce textual semantics at two levels: the mask\nsemantic level and the text semantic level, both derived from textual\ndescriptions extracted by large Vision-Language Models (VLMs). Building on\nthis, we propose Textual Semantic Guidance for infrared and visible image\nfusion, termed TeSG, which guides the image synthesis process in a way that is\noptimized for downstream tasks such as detection and segmentation.\nSpecifically, TeSG consists of three core components: a Semantic Information\nGenerator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven\nAttentional Fusion (TDAF) module. The SIG generates mask and text semantics\nbased on textual descriptions. The MGCA module performs initial attention-based\nfusion of visual features from both infrared and visible images, guided by mask\nsemantics. Finally, the TDAF module refines the fusion process with gated\nattention driven by text semantics. Extensive experiments demonstrate the\ncompetitiveness of our approach, particularly in terms of performance on\ndownstream tasks, compared to existing state-of-the-art methods.", "AI": {"tldr": "论文提出了一种基于文本语义引导的红外与可见光图像融合方法（TeSG），通过多级语义信息提升融合效果，并在下游任务中表现优异。", "motivation": "当前文本引导的红外与可见光图像融合方法对文本语义信息的利用不足，限制了融合效果和下游任务性能。", "method": "TeSG方法包含三个核心模块：语义信息生成器（SIG）、掩码引导的交叉注意力模块（MGCA）和文本驱动的注意力融合模块（TDAF），分别生成语义信息、初步融合视觉特征和优化融合过程。", "result": "实验表明，TeSG在下游任务（如检测和分割）中表现优于现有方法。", "conclusion": "TeSG通过多级文本语义引导，显著提升了红外与可见光图像融合的效果和实用性。"}}
{"id": "2506.16014", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16014", "abs": "https://arxiv.org/abs/2506.16014", "authors": ["Jina Kim", "Youjin Jang", "Jeongjin Han"], "title": "VRAIL: Vectorized Reward-based Attribution for Interpretable Learning", "comment": null, "summary": "We propose VRAIL (Vectorized Reward-based Attribution for Interpretable\nLearning), a bi-level framework for value-based reinforcement learning (RL)\nthat learns interpretable weight representations from state features. VRAIL\nconsists of two stages: a deep learning (DL) stage that fits an estimated value\nfunction using state features, and an RL stage that uses this to shape learning\nvia potential-based reward transformations. The estimator is modeled in either\nlinear or quadratic form, allowing attribution of importance to individual\nfeatures and their interactions. Empirical results on the Taxi-v3 environment\ndemonstrate that VRAIL improves training stability and convergence compared to\nstandard DQN, without requiring environment modifications. Further analysis\nshows that VRAIL uncovers semantically meaningful subgoals, such as passenger\npossession, highlighting its ability to produce human-interpretable behavior.\nOur findings suggest that VRAIL serves as a general, model-agnostic framework\nfor reward shaping that enhances both learning and interpretability.", "AI": {"tldr": "VRAIL是一个基于向量化奖励的双层框架，通过深度学习和强化学习阶段提升RL的可解释性和训练稳定性。", "motivation": "解决基于价值的强化学习中可解释性不足的问题，同时提升训练稳定性和收敛性。", "method": "VRAIL分为DL阶段（拟合价值函数）和RL阶段（通过奖励转换塑造学习），支持线性和二次模型以解释特征重要性。", "result": "在Taxi-v3环境中，VRAIL比标准DQN表现更优，无需环境修改即可揭示语义子目标（如乘客状态）。", "conclusion": "VRAIL是一个通用的、模型无关的奖励塑造框架，显著提升学习和可解释性。"}}
{"id": "2506.16735", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.16735", "abs": "https://arxiv.org/abs/2506.16735", "authors": ["Yunshan Li", "Wenwu Gong", "Qianqian Wang", "Chao Wang", "Lili Yang"], "title": "3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting", "comment": null, "summary": "Recent approaches based on transform-based tensor nuclear norm (TNN) have\ndemonstrated notable effectiveness in hyperspectral image (HSI) inpainting by\nleveraging low-rank structures in latent representations. Recent developments\nincorporate deep transforms to improve low-rank tensor representation; however,\nexisting approaches typically restrict the transform to the spectral mode,\nneglecting low-rank properties along other tensor modes. In this paper, we\npropose a novel 3-directional deep low-rank tensor representation (3DeepRep)\nmodel, which performs deep nonlinear transforms along all three modes of the\nHSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of\nmode-i frontal slices in the corresponding latent space for each direction\n(i=1,2,3), forming a 3-directional TNN regularization. The outputs from the\nthree directional branches are subsequently fused via a learnable aggregation\nmodule to produce the final result. An efficient gradient-based optimization\nalgorithm is developed to solve the model in a self-supervised manner.\nExtensive experiments on real-world HSI datasets demonstrate that the proposed\nmethod achieves superior inpainting performance compared to existing\nstate-of-the-art techniques, both qualitatively and quantitatively.", "AI": {"tldr": "提出了一种新型3方向深度低秩张量表示模型（3DeepRep），通过在所有三个HSI张量模式上执行深度非线性变换，显著提升了高光谱图像修复性能。", "motivation": "现有方法通常仅限制变换在光谱模式上，忽略了其他张量模式的低秩特性，因此需要一种更全面的方法。", "method": "提出3DeepRep模型，通过三个方向的深度非线性变换和核范数最小化，结合可学习的聚合模块，以自监督方式优化。", "result": "在真实HSI数据集上的实验表明，该方法在定性和定量上均优于现有技术。", "conclusion": "3DeepRep模型通过全面利用张量模式的低秩特性，显著提升了高光谱图像修复的效果。"}}
{"id": "2506.16032", "categories": ["cs.LG", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16032", "abs": "https://arxiv.org/abs/2506.16032", "authors": ["Zhen Qin", "Michael B. Wakin", "Zhihui Zhu"], "title": "A Scalable Factorization Approach for High-Order Structured Tensor Recovery", "comment": null, "summary": "Tensor decompositions, which represent an $N$-order tensor using\napproximately $N$ factors of much smaller dimensions, can significantly reduce\nthe number of parameters. This is particularly beneficial for high-order\ntensors, as the number of entries in a tensor grows exponentially with the\norder. Consequently, they are widely used in signal recovery and data analysis\nacross domains such as signal processing, machine learning, and quantum\nphysics. A computationally and memory-efficient approach to these problems is\nto optimize directly over the factors using local search algorithms such as\ngradient descent, a strategy known as the factorization approach in matrix and\ntensor optimization. However, the resulting optimization problems are highly\nnonconvex due to the multiplicative interactions between factors, posing\nsignificant challenges for convergence analysis and recovery guarantees.\n  In this paper, we present a unified framework for the factorization approach\nto solving various tensor decomposition problems. Specifically, by leveraging\nthe canonical form of tensor decompositions--where most factors are constrained\nto be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient\ndescent (RGD) to optimize these orthonormal factors on the Stiefel manifold.\nUnder a mild condition on the loss function, we establish a Riemannian\nregularity condition for the factorized objective and prove that RGD converges\nto the ground-truth tensor at a linear rate when properly initialized. Notably,\nboth the initialization requirement and the convergence rate scale polynomially\nrather than exponentially with $N$, improving upon existing results for Tucker\nand tensor-train format tensors.", "AI": {"tldr": "论文提出了一种统一的框架，通过黎曼梯度下降（RGD）优化张量分解问题，证明了在适当初始化下，RGD能以线性速率收敛到真实张量。", "motivation": "张量分解能显著减少参数数量，但优化问题高度非凸，现有方法在收敛性和恢复保证方面存在挑战。", "method": "利用张量分解的规范形式，将正交约束因素优化在Stiefel流形上，应用RGD方法。", "result": "在损失函数的温和条件下，证明了RGD能以线性速率收敛，且初始化和收敛速率与张量阶数多项式相关。", "conclusion": "该方法在Tucker和张量链格式上优于现有结果，为张量分解问题提供了高效解决方案。"}}
{"id": "2506.16737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16737", "abs": "https://arxiv.org/abs/2506.16737", "authors": ["Liu Zongzhen", "Luo Hui", "Wang Zhixing", "Wei Yuxing", "Zuo Haorui", "Zhang Jianlin"], "title": "Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection", "comment": null, "summary": "Unmanned aerial vehicle (UAV) object detection plays a vital role in\napplications such as environmental monitoring and urban security. To improve\nrobustness, recent studies have explored multimodal detection by fusing visible\n(RGB) and infrared (IR) imagery. However, due to UAV platform motion and\nasynchronous imaging, spatial misalignment frequently occurs between\nmodalities, leading to weak alignment. This introduces two major challenges:\nsemantic inconsistency at corresponding spatial locations and modality conflict\nduring feature fusion. Existing methods often address these issues in\nisolation, limiting their effectiveness. In this paper, we propose Cross-modal\nOffset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that\njointly tackles both challenges in weakly aligned UAV-based object detection.\nCoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA),\nwhich estimates attention-based spatial offsets and uses deformable convolution\nguided by a shared semantic space to align features more precisely; and the\nDynamic Attention-guided Fusion Module (DAFM), which adaptively balances\nmodality contributions through gating and refines fused features via\nspatial-channel dual attention. By integrating alignment and fusion in a\nunified design, CoDAF enables robust UAV object detection. Experiments on\nstandard benchmarks validate the effectiveness of our approach, with CoDAF\nachieving a mAP of 78.6% on the DroneVehicle dataset.", "AI": {"tldr": "论文提出了一种名为CoDAF的统一框架，用于解决无人机（UAV）多模态目标检测中的弱对齐问题，通过联合处理语义不一致和模态冲突，显著提升了检测性能。", "motivation": "无人机多模态目标检测中，由于平台运动和异步成像导致的弱对齐问题，引发语义不一致和模态冲突，现有方法未能有效联合解决这些问题。", "method": "CoDAF框架包含两个模块：OSA模块通过估计空间偏移和可变形卷积对齐特征；DAFM模块通过动态注意力平衡模态贡献并优化融合特征。", "result": "在DroneVehicle数据集上，CoDAF达到了78.6%的mAP，验证了其有效性。", "conclusion": "CoDAF通过统一设计解决了弱对齐问题，显著提升了无人机多模态目标检测的鲁棒性。"}}
{"id": "2506.16035", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.16035", "abs": "https://arxiv.org/abs/2506.16035", "authors": ["Vishesh Tripathi", "Tanmay Odapally", "Indraneel Das", "Uday Allu", "Biddwan Ahmed"], "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding", "comment": "11 pages, 1 Figure, 1 Table", "summary": "Retrieval-Augmented Generation (RAG) systems have revolutionized information\nretrieval and question answering, but traditional text-based chunking methods\nstruggle with complex document structures, multi-page tables, embedded figures,\nand contextual dependencies across page boundaries. We present a novel\nmultimodal document chunking approach that leverages Large Multimodal Models\n(LMMs) to process PDF documents in batches while maintaining semantic coherence\nand structural integrity. Our method processes documents in configurable page\nbatches with cross-batch context preservation, enabling accurate handling of\ntables spanning multiple pages, embedded visual elements, and procedural\ncontent. We evaluate our approach on a curated dataset of PDF documents with\nmanually crafted queries, demonstrating improvements in chunk quality and\ndownstream RAG performance. Our vision-guided approach achieves better accuracy\ncompared to traditional vanilla RAG systems, with qualitative analysis showing\nsuperior preservation of document structure and semantic coherence.", "AI": {"tldr": "提出了一种基于多模态模型的新型文档分块方法，解决了传统文本分块在处理复杂文档结构时的局限性。", "motivation": "传统文本分块方法在处理多页表格、嵌入图形和跨页上下文依赖时表现不佳，需要一种更高效的方法。", "method": "利用大型多模态模型（LMMs）对PDF文档进行批处理，保持语义连贯性和结构完整性，支持跨批次上下文保留。", "result": "在精心策划的数据集上评估，显示分块质量和下游RAG性能均有提升，优于传统RAG系统。", "conclusion": "多模态文档分块方法显著提高了处理复杂文档的准确性和语义连贯性。"}}
{"id": "2506.16742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16742", "abs": "https://arxiv.org/abs/2506.16742", "authors": ["Md Nahiduzzaman", "Ruwan Tennakoon", "Steven Korevaar", "Zongyuan Ge", "Alireza Bab-Hadiashar"], "title": "Uncertainty-Aware Variational Information Pursuit for Interpretable Medical Image Analysis", "comment": null, "summary": "In medical imaging, AI decision-support systems must balance accuracy and\ninterpretability to build user trust and support effective clinical\ndecision-making. Recently, Variational Information Pursuit (V-IP) and its\nvariants have emerged as interpretable-by-design modeling techniques, aiming to\nexplain AI decisions in terms of human-understandable, clinically relevant\nconcepts. However, existing V-IP methods overlook instance-level uncertainties\nin query-answer generation, which can arise from model limitations (epistemic\nuncertainty) or variability in expert responses (aleatoric uncertainty).\n  This paper introduces Uncertainty-Aware V-IP (UAV-IP), a novel framework that\nintegrates uncertainty quantification into the V-IP process. We evaluate UAV-IP\nacross four medical imaging datasets, PH2, Derm7pt, BrEaST, and SkinCon,\ndemonstrating an average AUC improvement of approximately 3.2% while generating\n20% more concise explanations compared to baseline V-IP, without sacrificing\ninformativeness. These findings highlight the importance of uncertainty-aware\nreasoning in interpretable by design models for robust and reliable medical\ndecision-making.", "AI": {"tldr": "论文提出了一种不确定性感知的V-IP框架（UAV-IP），在医疗影像中提升了解释性AI的准确性和解释简洁性。", "motivation": "现有V-IP方法忽视了查询-答案生成中的实例级不确定性，影响了模型的可靠性和临床决策支持效果。", "method": "UAV-IP将不确定性量化融入V-IP过程，通过四个医疗影像数据集（PH2、Derm7pt、BrEaST、SkinCon）进行评估。", "result": "UAV-IP平均AUC提升约3.2%，生成的解释比基线V-IP简洁20%，且信息量未减少。", "conclusion": "不确定性感知推理对设计可解释的医疗AI模型至关重要，能提升其鲁棒性和可靠性。"}}
{"id": "2506.16743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16743", "abs": "https://arxiv.org/abs/2506.16743", "authors": ["Weinan Guan", "Wei Wang", "Bo Peng", "Ziwen He", "Jing Dong", "Haonan Cheng"], "title": "Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention", "comment": "Accepted by TIFS 2025. Our code is availabel at\n  https://github.com/WeinanGuan/NASA-Swin", "summary": "With the rapid development of image generation technologies, especially the\nadvancement of Diffusion Models, the quality of synthesized images has\nsignificantly improved, raising concerns among researchers about information\nsecurity. To mitigate the malicious abuse of diffusion models,\ndiffusion-generated image detection has proven to be an effective\ncountermeasure.However, a key challenge for forgery detection is generalising\nto diffusion models not seen during training. In this paper, we address this\nproblem by focusing on image noise. We observe that images from different\ndiffusion models share similar noise patterns, distinct from genuine images.\nBuilding upon this insight, we introduce a novel Noise-Aware Self-Attention\n(NASA) module that focuses on noise regions to capture anomalous patterns. To\nimplement a SOTA detection model, we incorporate NASA into Swin Transformer,\nforming an novel detection architecture NASA-Swin. Additionally, we employ a\ncross-modality fusion embedding to combine RGB and noise images, along with a\nchannel mask strategy to enhance feature learning from both modalities.\nExtensive experiments demonstrate the effectiveness of our approach in\nenhancing detection capabilities for diffusion-generated images. When\nencountering unseen generation methods, our approach achieves the\nstate-of-the-art performance.Our code is available at\nhttps://github.com/WeinanGuan/NASA-Swin.", "AI": {"tldr": "论文提出了一种基于噪声感知自注意力模块（NASA）的检测方法，用于识别扩散模型生成的图像，并在未见过的生成方法上表现优异。", "motivation": "随着扩散模型生成图像质量的提升，信息安全隐患增加，需要一种能够泛化到未见过扩散模型的检测方法。", "method": "通过分析图像噪声模式，引入NASA模块并结合Swin Transformer，提出NASA-Swin架构，同时采用跨模态融合和通道掩码策略。", "result": "实验证明该方法在检测扩散生成图像方面表现优异，尤其在未见过的生成方法上达到SOTA性能。", "conclusion": "NASA-Swin通过噪声分析和跨模态学习，显著提升了检测扩散生成图像的能力。"}}
{"id": "2506.16056", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16056", "abs": "https://arxiv.org/abs/2506.16056", "authors": ["Puchun Liu", "C. L. Philip Chen", "Yubin He", "Tong Zhang"], "title": "CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations", "comment": null, "summary": "The difficulty of extracting deep features from EEG data and effectively\nintegrating information from multiple views presents significant challenges for\ndeveloping a generalizable pretraining framework for EEG representation\nlearning. However, most existing pre-training methods rely solely on the\ncontextual semantics of a single view, failing to capture the complex and\nsynergistic interactions among different perspectives, limiting the\nexpressiveness and generalization of learned representations. To address these\nissues, this paper proposes CRIA, an adaptive framework that utilizes\nvariable-length and variable-channel coding to achieve a unified representation\nof EEG data across different datasets. In this work, we define cross-view\ninformation as the integrated representation that emerges from the interaction\namong temporal, spectral, and spatial views of EEG signals. The model employs a\ncross-attention mechanism to fuse temporal, spectral, and spatial features\neffectively, and combines an attention matrix masking strategy based on the\ninformation bottleneck principle with a novel viewpoint masking pre-training\nscheme. Experimental results on the Temple University EEG corpus and the\nCHB-MIT dataset show that CRIA outperforms existing methods with the same\npre-training conditions, achieving a balanced accuracy of 57.02% for\nmulti-class event classification and 80.03% for anomaly detection, highlighting\nits strong generalization ability.", "AI": {"tldr": "CRIA提出了一种自适应框架，通过多视角融合和注意力机制提升EEG表示学习的泛化能力。", "motivation": "现有预训练方法仅依赖单一视角，难以捕捉EEG信号的多维交互，限制了表示的表达能力和泛化性。", "method": "采用变长变通道编码统一EEG表示，通过跨注意力机制融合时空谱特征，并结合信息瓶颈原则的掩码策略。", "result": "在Temple University EEG和CHB-MIT数据集上，CRIA在事件分类和异常检测任务中表现优于现有方法。", "conclusion": "CRIA通过多视角融合和注意力机制显著提升了EEG表示学习的泛化能力。"}}
{"id": "2506.16745", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.16745", "abs": "https://arxiv.org/abs/2506.16745", "authors": ["Qi-Ying Sun", "Wan-Lei Zhao", "Yi-Bo Miao", "Chong-Wah Ngo"], "title": "Class Agnostic Instance-level Descriptor for Visual Instance Search", "comment": null, "summary": "Despite the great success of the deep features in content-based image\nretrieval, the visual instance search remains challenging due to the lack of\neffective instance level feature representation. Supervised or weakly\nsupervised object detection methods are not among the options due to their poor\nperformance on the unknown object categories. In this paper, based on the\nfeature set output from self-supervised ViT, the instance level region\ndiscovery is modeled as detecting the compact feature subsets in a hierarchical\nfashion. The hierarchical decomposition results in a hierarchy of feature\nsubsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the\nvarious instance regions in an image of different semantic scales. The\nhierarchical decomposition well addresses the problem of object embedding and\nocclusions, which are widely observed in the real scenarios. The features\nderived from the nodes on the hierarchy make up a comprehensive representation\nfor the latent instances in the image. Our instance-level descriptor remains\neffective on both the known and unknown object categories. Empirical studies on\nthree instance search benchmarks show that it outperforms state-of-the-art\nmethods considerably.", "AI": {"tldr": "论文提出了一种基于自监督ViT的分层特征子集检测方法，用于解决视觉实例搜索中实例级特征表示的挑战。", "motivation": "由于现有监督或弱监督方法在未知对象类别上表现不佳，需要一种更有效的实例级特征表示方法。", "method": "利用自监督ViT输出的特征集，通过分层方式检测紧凑特征子集，形成层次化的实例区域表示。", "result": "在三个实例搜索基准测试中，该方法显著优于现有技术。", "conclusion": "分层分解方法有效解决了对象嵌入和遮挡问题，适用于已知和未知对象类别。"}}
{"id": "2506.16065", "categories": ["cs.LG", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.16065", "abs": "https://arxiv.org/abs/2506.16065", "authors": ["Geonho Hwang", "Wonyeol Lee", "Yeachan Park", "Sejun Park", "Feras Saad"], "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators", "comment": "70 pages, 4 figures. Appearing in CAV 2025", "summary": "The classical universal approximation (UA) theorem for neural networks\nestablishes mild conditions under which a feedforward neural network can\napproximate a continuous function $f$ with arbitrary accuracy. A recent result\nshows that neural networks also enjoy a more general interval universal\napproximation (IUA) theorem, in the sense that the abstract interpretation\nsemantics of the network using the interval domain can approximate the direct\nimage map of $f$ (i.e., the result of applying $f$ to a set of inputs) with\narbitrary accuracy. These theorems, however, rest on the unrealistic assumption\nthat the neural network computes over infinitely precise real numbers, whereas\ntheir software implementations in practice compute over finite-precision\nfloating-point numbers. An open question is whether the IUA theorem still holds\nin the floating-point setting.\n  This paper introduces the first IUA theorem for floating-point neural\nnetworks that proves their remarkable ability to perfectly capture the direct\nimage map of any rounded target function $f$, showing no limits exist on their\nexpressiveness. Our IUA theorem in the floating-point setting exhibits material\ndifferences from the real-valued setting, which reflects the fundamental\ndistinctions between these two computational models. This theorem also implies\nsurprising corollaries, which include (i) the existence of provably robust\nfloating-point neural networks; and (ii) the computational completeness of the\nclass of straight-line programs that use only floating-point additions and\nmultiplications for the class of all floating-point programs that halt.", "AI": {"tldr": "本文提出了首个浮点神经网络的区间通用逼近定理（IUA），证明其在浮点计算环境下仍能完美逼近目标函数的直接图像映射，展现了浮点神经网络的强大表达能力。", "motivation": "经典UA和IUA定理基于无限精度实数计算，而实际神经网络使用有限精度浮点数计算。本文旨在探讨IUA定理在浮点计算环境下是否成立。", "method": "通过理论分析，证明浮点神经网络在浮点计算环境下仍能实现区间通用逼近，并揭示其与实数计算模型的本质区别。", "result": "浮点神经网络在浮点环境下仍具有强大的表达能力，能够完美逼近目标函数的直接图像映射，并衍生出一些意外推论。", "conclusion": "浮点神经网络的IUA定理不仅填补了理论空白，还揭示了浮点计算模型的独特性，为神经网络的理论和应用提供了新视角。"}}
{"id": "2506.16773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16773", "abs": "https://arxiv.org/abs/2506.16773", "authors": ["Shuchen Sun", "Ligen Shi", "Chang Liu", "Lina Wu", "Jun Qiu"], "title": "Infrared and Visible Image Fusion Based on Implicit Neural Representations", "comment": null, "summary": "Infrared and visible light image fusion aims to combine the strengths of both\nmodalities to generate images that are rich in information and fulfill visual\nor computational requirements. This paper proposes an image fusion method based\non Implicit Neural Representations (INR), referred to as INRFuse. This method\nparameterizes a continuous function through a neural network to implicitly\nrepresent the multimodal information of the image, breaking through the\ntraditional reliance on discrete pixels or explicit features. The normalized\nspatial coordinates of the infrared and visible light images serve as inputs,\nand multi-layer perceptrons is utilized to adaptively fuse the features of both\nmodalities, resulting in the output of the fused image. By designing multiple\nloss functions, the method jointly optimizes the similarity between the fused\nimage and the original images, effectively preserving the thermal radiation\ninformation of the infrared image while maintaining the texture details of the\nvisible light image. Furthermore, the resolution-independent characteristic of\nINR allows for the direct fusion of images with varying resolutions and\nachieves super-resolution reconstruction through high-density coordinate\nqueries. Experimental results indicate that INRFuse outperforms existing\nmethods in both subjective visual quality and objective evaluation metrics,\nproducing fused images with clear structures, natural details, and rich\ninformation without the necessity for a training dataset.", "AI": {"tldr": "本文提出了一种基于隐式神经表示（INR）的红外与可见光图像融合方法INRFuse，通过神经网络参数化连续函数，自适应融合多模态信息，无需训练数据集即可生成高质量融合图像。", "motivation": "结合红外与可见光图像的优势，生成信息丰富且满足视觉或计算需求的融合图像。", "method": "利用归一化空间坐标作为输入，通过多层感知机自适应融合特征，设计多损失函数联合优化相似性。", "result": "INRFuse在主观视觉质量和客观评价指标上优于现有方法，融合图像结构清晰、细节自然、信息丰富。", "conclusion": "INRFuse通过INR的特性实现了分辨率无关的图像融合和超分辨率重建，无需训练数据集即可高效生成高质量融合图像。"}}
{"id": "2506.16072", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16072", "abs": "https://arxiv.org/abs/2506.16072", "authors": ["Kexuan Wang", "An Liu"], "title": "A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems", "comment": null, "summary": "Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for\nits near-optimal weighted sum rate performance. However, its practical\ndeployment in massive multi-user (MU) multiple-input multiple-output (MIMO)\northogonal frequency-division multiplexing (OFDM) systems is hindered by the\nassumption of perfect channel state information (CSI) and high computational\ncomplexity. To address these issues, we first develop a wideband stochastic\nWMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted\nsum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight\nreinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),\nwhere each SWMMSE iteration is mapped to a network layer. Specifically, its DU\nmodule integrates approximation techniques and leverages beam-domain sparsity\nas well as frequency-domain subcarrier correlation, significantly accelerating\nconvergence and reducing computational overhead. Furthermore, the RL module\nadaptively adjusts the network depth and generates compensation matrices to\nmitigate approximation errors. Simulation results under imperfect CSI\ndemonstrate that RLDDU-Net outperforms existing baselines in EWSR performance\nwhile offering superior computational and convergence efficiency.", "AI": {"tldr": "论文提出了一种基于强化学习的深度展开网络（RLDDU-Net），用于解决WMMSE预编码在OFDM系统中的高计算复杂性和不完美CSI问题。", "motivation": "WMMSE预编码在OFDM系统中的实际应用受到完美CSI假设和高计算复杂性的限制。", "method": "开发了宽频随机WMMSE（SWMMSE）算法，并基于此设计了RL驱动的深度展开网络（RLDDU-Net），利用波束域稀疏性和频域子载波相关性加速收敛。", "result": "仿真结果表明，RLDDU-Net在不完美CSI下优于现有基线，具有更高的EWSR性能和计算效率。", "conclusion": "RLDDU-Net为大规模MU-MIMO OFDM系统提供了一种高效且适应性强的解决方案。"}}
{"id": "2506.16776", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16776", "abs": "https://arxiv.org/abs/2506.16776", "authors": ["Beomseok Ko", "Hyeryung Jang"], "title": "PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model", "comment": "10 pages, 6 figures", "summary": "Diffusion models excel in image generation but are computational and\nresource-intensive due to their reliance on iterative Markov chain processes,\nleading to error accumulation and limiting the effectiveness of naive\ncompression techniques. In this paper, we propose PQCAD-DM, a novel hybrid\ncompression framework combining Progressive Quantization (PQ) and\nCalibration-Assisted Distillation (CAD) to address these challenges. PQ employs\na two-stage quantization with adaptive bit-width transitions guided by a\nmomentum-based mechanism, reducing excessive weight perturbations in\nlow-precision. CAD leverages full-precision calibration datasets during\ndistillation, enabling the student to match full-precision performance even\nwith a quantized teacher. As a result, PQCAD-DM achieves a balance between\ncomputational efficiency and generative quality, halving inference time while\nmaintaining competitive performance. Extensive experiments validate PQCAD-DM's\nsuperior generative capabilities and efficiency across diverse datasets,\noutperforming fixed-bit quantization methods.", "AI": {"tldr": "PQCAD-DM是一种结合渐进量化（PQ）和校准辅助蒸馏（CAD）的混合压缩框架，旨在解决扩散模型的计算和资源消耗问题，同时保持生成质量。", "motivation": "扩散模型在图像生成中表现出色，但其依赖迭代马尔可夫链过程导致计算和资源消耗大，且误差累积限制了压缩技术的效果。", "method": "PQCAD-DM采用两阶段渐进量化（PQ）和校准辅助蒸馏（CAD），PQ通过动量机制自适应调整位宽，CAD利用全精度校准数据集进行蒸馏。", "result": "PQCAD-DM在计算效率和生成质量之间取得平衡，推理时间减半且性能保持竞争力，优于固定位量化方法。", "conclusion": "PQCAD-DM通过混合压缩框架显著提升了扩散模型的效率，同时保持了生成质量，为实际应用提供了可行方案。"}}
{"id": "2506.16074", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16074", "abs": "https://arxiv.org/abs/2506.16074", "authors": ["Kexuan Wang", "An Liu"], "title": "Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach", "comment": null, "summary": "6G wireless networks are expected to support diverse quality-of-service (QoS)\ndemands while maintaining high energy efficiency. Weighted Minimum Mean Square\nError (WMMSE) precoding with fixed user priorities and transmit power is widely\nrecognized for enhancing overall system performance but lacks flexibility to\nadapt to user-specific QoS requirements and time-varying channel conditions. To\naddress this, we propose a novel constrained reinforcement learning (CRL)\nalgorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy\nnetwork to dynamically allocate user priorities and power for WMMSE precoding.\nSpecifically, CAAC integrates a Constrained Stochastic Successive Convex\nApproximation (CSSCA) method to optimize the policy, enabling more effective\nhandling of energy efficiency goals and satisfaction of stochastic non-convex\nQoS constraints compared to traditional and existing CRL methods. Moreover,\nCAAC employs lightweight attention-enhanced Q-networks to evaluate policy\nupdates without prior environment model knowledge. The network architecture not\nonly enhances representational capacity but also boosts learning efficiency.\nSimulation results show that CAAC outperforms baselines in both energy\nefficiency and QoS satisfaction.", "AI": {"tldr": "论文提出了一种基于约束强化学习（CRL）的算法CAAC，用于动态分配用户优先级和功率，以优化6G网络中的WMMSE预编码，提高能效和满足QoS需求。", "motivation": "传统WMMSE预编码缺乏灵活性，无法适应动态用户需求和时变信道条件，因此需要一种自适应方法。", "method": "提出CAAC算法，结合CSSCA方法优化策略，并使用轻量级注意力增强Q网络评估策略更新。", "result": "仿真结果表明，CAAC在能效和QoS满足方面优于基线方法。", "conclusion": "CAAC为6G网络提供了一种灵活高效的预编码解决方案。"}}
{"id": "2506.16784", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.16784", "abs": "https://arxiv.org/abs/2506.16784", "authors": ["Xiaoyu Shi", "Rahul Kumar Jain", "Yinhao Li", "Ruibo Hou", "Jingliang Cheng", "Jie Bai", "Guohua Zhao", "Lanfen Lin", "Rui Xu", "Yen-wei Chen"], "title": "TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration", "comment": null, "summary": "Deep learning has demonstrated remarkable success in medical image\nsegmentation and computer-aided diagnosis. In particular, numerous advanced\nmethods have achieved state-of-the-art performance in brain tumor segmentation\nfrom MRI scans. While recent studies in other medical imaging domains have\nrevealed that integrating textual reports with visual data can enhance\nsegmentation accuracy, the field of brain tumor analysis lacks a comprehensive\ndataset that combines radiological images with corresponding textual\nannotations. This limitation has hindered the exploration of multimodal\napproaches that leverage both imaging and textual data.\n  To bridge this critical gap, we introduce the TextBraTS dataset, the first\npublicly available volume-level multimodal dataset that contains paired MRI\nvolumes and rich textual annotations, derived from the widely adopted BraTS2020\nbenchmark. Building upon this novel dataset, we propose a novel baseline\nframework and sequential cross-attention method for text-guided volumetric\nmedical image segmentation. Through extensive experiments with various\ntext-image fusion strategies and templated text formulations, our approach\ndemonstrates significant improvements in brain tumor segmentation accuracy,\noffering valuable insights into effective multimodal integration techniques.\n  Our dataset, implementation code, and pre-trained models are publicly\navailable at https://github.com/Jupitern52/TextBraTS.", "AI": {"tldr": "论文介绍了TextBraTS数据集，首个结合MRI图像和文本注释的多模态数据集，并提出了一种基于文本引导的医学图像分割方法，显著提高了脑肿瘤分割的准确性。", "motivation": "当前脑肿瘤分析领域缺乏结合图像和文本的多模态数据集，限制了多模态方法的研究。", "method": "提出了TextBraTS数据集，并开发了一种基于序列交叉注意力的文本引导体积医学图像分割框架。", "result": "实验表明，该方法显著提高了脑肿瘤分割的准确性，并提供了多模态融合的有效策略。", "conclusion": "TextBraTS数据集和提出的方法为脑肿瘤分割的多模态研究提供了重要资源和技术支持。"}}
{"id": "2506.16078", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.16078", "abs": "https://arxiv.org/abs/2506.16078", "authors": ["Tianle Gu", "Kexin Huang", "Zongqi Wang", "Yixu Wang", "Jie Li", "Yuanqi Yao", "Yang Yao", "Yujiu Yang", "Yan Teng", "Yingchun Wang"], "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations", "comment": null, "summary": "Safety alignment is a key requirement for building reliable Artificial\nGeneral Intelligence. Despite significant advances in safety alignment, we\nobserve that minor latent shifts can still trigger unsafe responses in aligned\nmodels. We argue that this stems from the shallow nature of existing alignment\nmethods, which focus on surface-level refusal behaviors without sufficiently\naltering internal representations. Consequently, small shifts in hidden\nactivations can re-trigger harmful behaviors embedded in the latent space. To\nexplore the robustness of safety alignment to latent perturbations, we\nintroduce a probing method that measures the Negative Log-Likelihood of the\noriginal response generated by the model. This probe quantifies local\nsensitivity in the latent space, serving as a diagnostic tool for identifying\nvulnerable directions. Based on this signal, we construct effective jailbreak\ntrajectories, giving rise to the Activation Steering Attack (ASA). More\nimportantly, these insights offer a principled foundation for improving\nalignment robustness. To this end, we introduce Layer-wise Adversarial Patch\nTraining~(LAPT), a fine-tuning strategy that inject controlled perturbations\ninto hidden representations during training. Experimental results highlight\nthat LAPT strengthen alignment robustness without compromising general\ncapabilities. Our findings reveal fundamental flaws in current alignment\nparadigms and call for representation-level training strategies that move\nbeyond surface-level behavior supervision. Codes and results are available at\nhttps://github.com/Carol-gutianle/LatentSafety.", "AI": {"tldr": "论文探讨了现有安全对齐方法的浅层性，提出了一种探测潜在扰动的方法（ASA），并通过LAPT策略提升对齐鲁棒性。", "motivation": "现有安全对齐方法仅关注表面拒绝行为，未充分改变内部表征，导致微小潜在偏移仍可能触发不安全响应。", "method": "引入负对数似然探测潜在空间敏感性，提出ASA攻击方法，并设计LAPT策略在训练中注入受控扰动。", "result": "实验表明LAPT能增强对齐鲁棒性且不影响模型通用能力。", "conclusion": "当前对齐范式存在根本缺陷，需转向表征级训练策略。"}}
{"id": "2506.16796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16796", "abs": "https://arxiv.org/abs/2506.16796", "authors": ["Junbo Qiao", "Miaomiao Cai", "Wei Li", "Yutong Liu", "Xudong Huang", "Gaoqi He", "Jiao Xie", "Jie Hu", "Xinghao Chen", "Shaohui Lin"], "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought", "comment": null, "summary": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation.", "AI": {"tldr": "论文提出RealSR-R1，结合视觉与语言推理的VLCoT框架，并引入GRPO优化方法，提升真实世界图像超分辨率任务的性能。", "motivation": "现有方法在理解退化图像内容上表现不佳，导致重建结果低保真且不自然。", "method": "提出VLCoT框架模拟人类处理退化图像的过程，结合GRPO优化方法，设计四种奖励函数。", "result": "实验证明RealSR-R1能生成真实细节并准确理解图像内容，尤其在语义丰富或严重退化场景中表现优异。", "conclusion": "VLCoT-GRPO框架有效提升了真实世界图像超分辨率的性能，生成结果更自然且高保真。"}}
{"id": "2506.16096", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16096", "abs": "https://arxiv.org/abs/2506.16096", "authors": ["Qianqian Liao", "Wuque Cai", "Hongze Sun", "Dongze Liu", "Duo Chen", "Dezhong Yao", "Daqing Guo"], "title": "A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders", "comment": "16 pages, 7 figures, 13 tables; this paper has been submitted for\n  possible publication", "summary": "Recent developed graph-based methods for diagnosing brain disorders using\nfunctional connectivity highly rely on predefined brain atlases, but overlook\nthe rich information embedded within atlases and the confounding effects of\nsite and phenotype variability. To address these challenges, we propose a\ntwo-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates\nthe semantic similarity of brain regions and condition-based population graph\nmodeling. In the first stage, termed brain representation learning, we leverage\nbrain atlas knowledge from GPT-4 to enrich the graph representation and refine\nthe brain graph through an adaptive node reassignment graph attention network.\nIn the second stage, termed population disorder diagnosis, phenotypic data is\nincorporated into population graph construction and feature fusion to mitigate\nconfounding effects and enhance diagnosis performance. Experiments on the ABIDE\nI, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms\nstate-of-the-art methods in prediction accuracy while enhancing\ninterpretability. Overall, our proposed framework offers a reliable and\npersonalized approach to brain disorder diagnosis, advancing clinical\napplicability.", "AI": {"tldr": "提出了一种两阶段图学习框架B2P-GL，结合脑区语义相似性和基于条件的群体图建模，提升脑疾病诊断性能。", "motivation": "现有基于图的脑疾病诊断方法依赖预定义脑图谱，但忽略了图谱内丰富信息及站点和表型变异的混杂效应。", "method": "第一阶段（脑表征学习）利用GPT-4知识丰富图谱表示，并通过自适应节点重分配图注意力网络优化脑图；第二阶段（群体疾病诊断）整合表型数据构建群体图并融合特征以减少混杂效应。", "result": "在ABIDE I、ADHD-200和Rest-meta-MDD数据集上，B2P-GL在预测准确性和可解释性上优于现有方法。", "conclusion": "B2P-GL为脑疾病诊断提供了可靠且个性化的方法，推动了临床应用。"}}
{"id": "2506.16802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16802", "abs": "https://arxiv.org/abs/2506.16802", "authors": ["Riccardo Corvi", "Davide Cozzolino", "Ekta Prashnani", "Shalini De Mello", "Koki Nagano", "Luisa Verdoliva"], "title": "Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation", "comment": null, "summary": "Synthetic video generation is progressing very rapidly. The latest models can\nproduce very realistic high-resolution videos that are virtually\nindistinguishable from real ones. Although several video forensic detectors\nhave been recently proposed, they often exhibit poor generalization, which\nlimits their applicability in a real-world scenario. Our key insight to\novercome this issue is to guide the detector towards seeing what really\nmatters. In fact, a well-designed forensic classifier should focus on\nidentifying intrinsic low-level artifacts introduced by a generative\narchitecture rather than relying on high-level semantic flaws that characterize\na specific model. In this work, first, we study different generative\narchitectures, searching and identifying discriminative features that are\nunbiased, robust to impairments, and shared across models. Then, we introduce a\nnovel forensic-oriented data augmentation strategy based on the wavelet\ndecomposition and replace specific frequency-related bands to drive the model\nto exploit more relevant forensic cues. Our novel training paradigm improves\nthe generalizability of AI-generated video detectors, without the need for\ncomplex algorithms and large datasets that include multiple synthetic\ngenerators. To evaluate our approach, we train the detector using data from a\nsingle generative model and test it against videos produced by a wide range of\nother models. Despite its simplicity, our method achieves a significant\naccuracy improvement over state-of-the-art detectors and obtains excellent\nresults even on very recent generative models, such as NOVA and FLUX. Code and\ndata will be made publicly available.", "AI": {"tldr": "论文提出了一种改进AI生成视频检测器泛化能力的方法，通过关注生成架构的低级特征而非高级语义缺陷，并结合小波分解的数据增强策略。", "motivation": "现有视频伪造检测器泛化能力差，限制了其在实际场景中的应用。", "method": "研究不同生成架构的特征，提出基于小波分解的数据增强策略，引导检测器关注更相关的伪造线索。", "result": "方法显著提升了检测器的泛化能力，在多种生成模型上表现优异。", "conclusion": "该方法简单有效，无需复杂算法或大规模数据集，即可显著提升检测器的性能。"}}
{"id": "2506.16110", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16110", "abs": "https://arxiv.org/abs/2506.16110", "authors": ["Langzhang Liang", "Fanchen Bu", "Zixing Song", "Zenglin Xu", "Shirui Pan", "Kijung Shin"], "title": "Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification", "comment": "Published as a conference paper at ICML 2025", "summary": "The message-passing paradigm of Graph Neural Networks often struggles with\nexchanging information across distant nodes typically due to structural\nbottlenecks in certain graph regions, a limitation known as\n\\textit{over-squashing}. To reduce such bottlenecks, \\textit{graph rewiring},\nwhich modifies graph topology, has been widely used. However, existing graph\nrewiring techniques often overlook the need to preserve critical properties of\nthe original graph, e.g., \\textit{spectral properties}. Moreover, many\napproaches rely on increasing edge count to improve connectivity, which\nintroduces significant computational overhead and exacerbates the risk of\nover-smoothing. In this paper, we propose a novel graph rewiring method that\nleverages \\textit{spectrum-preserving} graph \\textit{sparsification}, for\nmitigating over-squashing. Our method generates graphs with enhanced\nconnectivity while maintaining sparsity and largely preserving the original\ngraph spectrum, effectively balancing structural bottleneck reduction and graph\nproperty preservation. Experimental results validate the effectiveness of our\napproach, demonstrating its superiority over strong baseline methods in\nclassification accuracy and retention of the Laplacian spectrum.", "AI": {"tldr": "论文提出了一种基于谱保持的图稀疏化方法，用于缓解图神经网络中的过挤压问题，同时保持图的稀疏性和谱特性。", "motivation": "现有图重布线方法常忽略保留原始图的谱特性，且增加边数会导致计算开销和过平滑风险。", "method": "提出一种谱保持的图稀疏化方法，增强连通性同时保持稀疏性和谱特性。", "result": "实验表明该方法在分类准确性和拉普拉斯谱保留上优于基线方法。", "conclusion": "该方法有效平衡了结构瓶颈减少和图特性保留，具有优越性能。"}}
{"id": "2506.16805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16805", "abs": "https://arxiv.org/abs/2506.16805", "authors": ["Chao Chen", "Nobel Dang", "Juexiao Zhang", "Wenkai Sun", "Pengfei Zheng", "Xuhang He", "Yimeng Ye", "Taarun Srinivas", "Chen Feng"], "title": "Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes", "comment": null, "summary": "Humans exhibit a remarkable ability to recognize co-visibility-the\noverlapping regions visible in multiple images-even when these images are\nsparsely distributed across a complex scene. This capability is foundational in\n3D vision and robotic perception. Despite significant progress in vision\nlearning, it remains unclear whether current vision models have reached\nhuman-level proficiency in co-visibility analysis. In this work, we introduce\nthe Co-Visibility reasONing (Co-VisiON) benchmark, designed to directly\nevaluate co-visibility reasoning on sparse image sets across over 1000 indoor\nscenarios. Our experiments reveal that while co-visibility is typically treated\nas a low-level feature matching task, it poses a significant challenge for\nexisting vision models under sparse conditions. Notably, a proprietary\nvision-language model outperforms all purely vision-based approaches, with all\nmodels lagging substantially behind human performance. This gap underscores the\nneed for more than basic pairwise vision processing-it calls for a\ncomprehensive spatial understanding through high-level reasoning across\nmultiple views. Inspired by human visual cognition, we propose a novel\nmulti-view baseline, Covis, which achieves top performance among pure vision\nmodels and narrows the gap to the proprietary VLM. We hope our benchmark and\nfindings will spur further advancements in developing vision models capable of\nrobust, high-level reasoning in challenging, sparse environments. Our dataset\nand source code can be found at: https://ai4ce.github.io/CoVISION", "AI": {"tldr": "论文提出了Co-VisiON基准测试，用于评估稀疏图像集中的共视性推理能力，发现现有视觉模型在此任务上表现不佳，并提出新基线模型Covis。", "motivation": "研究人类共视性识别能力在稀疏图像条件下的表现，并评估当前视觉模型是否达到人类水平。", "method": "引入Co-VisiON基准测试，覆盖1000多个室内场景，比较现有视觉模型与人类表现，并提出多视图基线模型Covis。", "result": "专有视觉语言模型表现最佳，但所有模型均远低于人类水平；Covis在纯视觉模型中表现最优。", "conclusion": "共视性推理需要高层次多视图空间理解，Covis缩小了与专有模型的差距，为未来研究提供方向。"}}
{"id": "2506.16170", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16170", "abs": "https://arxiv.org/abs/2506.16170", "authors": ["Simardeep Singh"], "title": "From Teacher to Student: Tracking Memorization Through Model Distillation", "comment": "5 pages, in-proceedings L2M2 @ ACL 2025", "summary": "Large language models (LLMs) are known to memorize parts of their training\ndata, raising important concerns around privacy and security. While previous\nresearch has focused on studying memorization in pre-trained models, much less\nis known about how knowledge distillation (KD) affects memorization.In this\nstudy, we explore how different KD methods influence the memorization of\nfine-tuned task data when a large teacher model is distilled into smaller\nstudent variants.This study demonstrates that distilling a larger teacher\nmodel, fine-tuned on a dataset, into a smaller variant not only lowers\ncomputational costs and model size but also significantly reduces the\nmemorization risks compared to standard fine-tuning approaches.", "AI": {"tldr": "研究探讨知识蒸馏（KD）对大型语言模型（LLM）记忆训练数据的影响，发现蒸馏能降低记忆风险。", "motivation": "了解知识蒸馏如何影响LLM对任务数据的记忆，以解决隐私和安全问题。", "method": "比较不同KD方法在将大型教师模型蒸馏为小型学生模型时对记忆任务数据的影响。", "result": "蒸馏不仅降低计算成本和模型大小，还显著减少记忆风险。", "conclusion": "知识蒸馏是减少LLM记忆风险的有效方法。"}}
{"id": "2506.16806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16806", "abs": "https://arxiv.org/abs/2506.16806", "authors": ["Fan Yang", "Yousong Zhu", "Xin Li", "Yufei Zhan", "Hongyin Zhao", "Shurong Zheng", "Yaowei Wang", "Ming Tang", "Jinqiao Wang"], "title": "FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation", "comment": null, "summary": "Recent Large Vision Language Models (LVLMs) demonstrate promising\ncapabilities in unifying visual understanding and generative modeling, enabling\nboth accurate content understanding and flexible editing. However, current\napproaches treat \"what to see\" and \"how to edit\" separately: they either\nperform isolated object segmentation or utilize segmentation masks merely as\nconditional prompts for local edit generation tasks, often relying on multiple\ndisjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM\nthat integrates segmentation-aware perception and controllable object-centric\ngeneration within an end-to-end framework. FOCUS employs a dual-branch visual\nencoder to simultaneously capture global semantic context and fine-grained\nspatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to\nproduce discrete visual tokens that enhance generation quality. To enable\naccurate and controllable image editing, we propose a progressive multi-stage\ntraining pipeline, where segmentation masks are jointly optimized and used as\nspatial condition prompts to guide the diffusion decoder. This strategy aligns\nvisual encoding, segmentation, and generation modules, effectively bridging\nsegmentation-aware perception with fine-grained visual synthesis. Extensive\nexperiments across three core tasks, including multimodal understanding,\nreferring segmentation accuracy, and controllable image generation, demonstrate\nthat FOCUS achieves strong performance by jointly optimizing visual perception\nand generative capabilities.", "AI": {"tldr": "FOCUS是一个统一的LVLM模型，集成了分割感知和可控对象生成，通过端到端框架提升视觉理解和生成能力。", "motivation": "当前方法将视觉理解和编辑分离，依赖多个独立模型，无法统一处理。", "method": "采用双分支视觉编码器捕获全局和局部信息，结合MoVQGAN视觉分词器和多阶段训练策略。", "result": "在多项任务中表现优异，实现了视觉感知与生成能力的联合优化。", "conclusion": "FOCUS通过统一框架有效解决了视觉理解和生成的分割问题。"}}
{"id": "2506.16174", "categories": ["cs.LG", "I.5.4"], "pdf": "https://arxiv.org/pdf/2506.16174", "abs": "https://arxiv.org/abs/2506.16174", "authors": ["Ismo Horppu", "Frederick Ayala", "Erlin Gulbenkoglu"], "title": "Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song", "comment": "15 pages, 10 figures", "summary": "All languages are peculiar. Some of them are considered more challenging to\nunderstand than others. The Finnish Language is known to be a complex language.\nAlso, when languages are used by artists, the pronunciation and meaning might\nbe more tricky to understand. Therefore, we are putting AI to a fun, yet\nchallenging trial: translating a Finnish rap song to text. We will compare the\nFaster Whisperer algorithm and YouTube's internal speech-to-text functionality.\nThe reference truth will be Finnish rap lyrics, which the main author's little\nbrother, Mc Timo, has written. Transcribing the lyrics will be challenging\nbecause the artist raps over synth music player by Syntikka Janne. The\nhallucination level and mishearing of AI speech-to-text extractions will be\nmeasured by comparing errors made against the original Finnish lyrics. The\nerror function is informal but still works for our case.", "AI": {"tldr": "论文探讨了使用AI翻译芬兰说唱歌曲的挑战，比较了Faster Whisperer算法和YouTube的语音转文字功能，并以芬兰说唱歌词为基准。", "motivation": "芬兰语复杂且艺术家使用时更难以理解，因此测试AI在翻译芬兰说唱歌曲中的表现。", "method": "使用Faster Whisperer和YouTube的语音转文字功能，对比其与芬兰说唱歌词的差异。", "result": "通过错误函数测量AI的幻觉和误听水平。", "conclusion": "实验为AI在复杂语言和艺术表达中的应用提供了有趣但非正式的评估。"}}
{"id": "2506.16819", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16819", "abs": "https://arxiv.org/abs/2506.16819", "authors": ["Yuchu Jiang", "Jiaming Chu", "Jian Zhao", "Xin Zhang", "Xu Yang", "Lei Jin", "Chi Zhang", "Xuelong Li"], "title": "Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection", "comment": "6 pages, 2 figures, accepted by IJCAI 2025 workshop", "summary": "The proliferation of generative models has raised serious concerns about\nvisual content forgery. Existing deepfake detection methods primarily target\neither image-level classification or pixel-wise localization. While some\nachieve high accuracy, they often suffer from limited generalization across\nmanipulation types or rely on complex architectures. In this paper, we propose\nLoupe, a lightweight yet effective framework for joint deepfake detection and\nlocalization. Loupe integrates a patch-aware classifier and a segmentation\nmodule with conditional queries, allowing simultaneous global authenticity\nclassification and fine-grained mask prediction. To enhance robustness against\ndistribution shifts of test set, Loupe introduces a pseudo-label-guided\ntest-time adaptation mechanism by leveraging patch-level predictions to\nsupervise the segmentation head. Extensive experiments on the DDL dataset\ndemonstrate that Loupe achieves state-of-the-art performance, securing the\nfirst place in the IJCAI 2025 Deepfake Detection and Localization Challenge\nwith an overall score of 0.846. Our results validate the effectiveness of the\nproposed patch-level fusion and conditional query design in improving both\nclassification accuracy and spatial localization under diverse forgery\npatterns. The code is available at https://github.com/Kamichanw/Loupe.", "AI": {"tldr": "Loupe是一个轻量级框架，用于联合深度伪造检测和定位，通过全局分类和细粒度掩码预测实现高性能。", "motivation": "生成模型的普及引发了对视觉内容伪造的担忧，现有方法在泛化性和架构复杂性上存在不足。", "method": "Loupe结合了补丁感知分类器和条件查询的分割模块，并引入伪标签引导的测试时适应机制。", "result": "在DDL数据集上表现优异，IJCAI 2025挑战赛中得分0.846，排名第一。", "conclusion": "Loupe的补丁级融合和条件查询设计有效提升了分类和定位性能。"}}
{"id": "2506.16196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16196", "abs": "https://arxiv.org/abs/2506.16196", "authors": ["Xun Wang", "Jing Xu", "Franziska Boenisch", "Michael Backes", "Christopher A. Choquette-Choo", "Adam Dziedzic"], "title": "Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs", "comment": "Accepted at ICML2025", "summary": "Prompting has become a dominant paradigm for adapting large language models\n(LLMs). While discrete (textual) prompts are widely used for their\ninterpretability, soft (parameter) prompts have recently gained traction in\nAPIs. This is because they can encode information from more training samples\nwhile minimizing the user's token usage, leaving more space in the context\nwindow for task-specific input. However, soft prompts are tightly coupled to\nthe LLM they are tuned on, limiting their generalization to other LLMs. This\nconstraint is particularly problematic for efficiency and privacy: (1) tuning\nprompts on each LLM incurs high computational costs, especially as LLMs\ncontinue to grow in size. Additionally, (2) when the LLM is hosted externally,\nsoft prompt tuning often requires sharing private data with the LLM provider.\nFor instance, this is the case with the NVIDIA NeMo API. To address these\nissues, we propose POST (Privacy Of Soft prompt Transfer), a framework that\nenables private tuning of soft prompts on a small model and subsequently\ntransfers these prompts to a larger LLM. POST uses knowledge distillation to\nderive a small model directly from the large LLM to improve prompt\ntransferability, tunes the soft prompt locally, optionally with differential\nprivacy guarantees, and transfers it back to the larger LLM using a small\npublic dataset. Our experiments show that POST reduces computational costs,\npreserves privacy, and effectively transfers high-utility soft prompts.", "AI": {"tldr": "POST框架通过在小模型上私有调优软提示并转移到大模型，解决了软提示的计算成本和隐私问题。", "motivation": "软提示虽高效但受限于特定LLM，导致计算成本高和隐私泄露风险。", "method": "POST通过知识蒸馏、本地调优和差分隐私，实现软提示的私有化转移。", "result": "POST降低了计算成本，保护了隐私，并成功转移高效软提示。", "conclusion": "POST为软提示的私有化和高效转移提供了可行方案。"}}
{"id": "2506.16821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16821", "abs": "https://arxiv.org/abs/2506.16821", "authors": ["Can Lin", "Daniele Affinita", "Marco E. P. Zimmatore", "Daniele Nardi", "Domenico D. Bloisi", "Vincenzo Suriani"], "title": "Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots", "comment": null, "summary": "Robust and accurate ball detection is a critical component for autonomous\nhumanoid soccer robots, particularly in dynamic and challenging environments\nsuch as RoboCup outdoor fields. However, traditional supervised approaches\nrequire extensive manual annotation, which is costly and time-intensive. To\novercome this problem, we present a self-supervised learning framework for\ndomain-adaptive feature extraction to enhance ball detection performance. The\nproposed approach leverages a general-purpose pretrained model to generate\npseudo-labels, which are then used in a suite of self-supervised pretext tasks\n-- including colorization, edge detection, and triplet loss -- to learn robust\nvisual features without relying on manual annotations. Additionally, a\nmodel-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid\nadaptation to new deployment scenarios with minimal supervision. A new dataset\ncomprising 10,000 labeled images from outdoor RoboCup SPL matches is\nintroduced, used to validate the method, and made available to the community.\nExperimental results demonstrate that the proposed pipeline outperforms\nbaseline models in terms of accuracy, F1 score, and IoU, while also exhibiting\nfaster convergence.", "AI": {"tldr": "提出了一种自监督学习框架，用于自适应特征提取以提升球检测性能，避免了传统监督学习的高成本标注需求。", "motivation": "在动态和挑战性环境中（如RoboCup户外场地），传统监督方法需要大量手动标注，成本高且耗时。", "method": "利用预训练模型生成伪标签，通过自监督任务（如着色、边缘检测和三元组损失）学习鲁棒视觉特征，并结合MAML策略快速适应新场景。", "result": "实验表明，该方法在准确性、F1分数和IoU上优于基线模型，且收敛更快。", "conclusion": "自监督学习框架有效提升了球检测性能，减少了标注需求，适用于动态环境。"}}
{"id": "2506.16216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16216", "abs": "https://arxiv.org/abs/2506.16216", "authors": ["Charbel Bou Chaaya", "Abanoub M. Girgis", "Mehdi Bennis"], "title": "From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management", "comment": null, "summary": "In this work, we aim to optimize the radio resource management of a\ncommunication system between a remote controller and its device, whose state is\nrepresented through image frames, without compromising the performance of the\ncontrol task. We propose a novel machine learning (ML) technique to jointly\nmodel and predict the dynamics of the control system as well as the wireless\npropagation environment in latent space. Our method leverages two coupled\njoint-embedding predictive architectures (JEPAs): a control JEPA models the\ncontrol dynamics and guides the predictions of a wireless JEPA, which captures\nthe dynamics of the device's channel state information (CSI) through\ncross-modal conditioning. We then train a deep reinforcement learning (RL)\nalgorithm to derive a control policy from latent control dynamics and a power\npredictor to estimate scheduling intervals with favorable channel conditions\nbased on latent CSI representations. As such, the controller minimizes the\nusage of radio resources by utilizing the coupled JEPA networks to imagine the\ndevice's trajectory in latent space. We present simulation results on synthetic\nmultimodal data and show that our proposed approach reduces transmit power by\nover 50% while maintaining control performance comparable to baseline methods\nthat do not account for wireless optimization.", "AI": {"tldr": "提出了一种联合建模控制动态和无线传播环境的机器学习方法，通过耦合JEPA网络和强化学习优化无线资源管理，显著降低传输功率。", "motivation": "优化远程控制器与设备间的无线资源管理，同时不牺牲控制任务性能。", "method": "使用两个耦合的JEPA网络分别建模控制动态和无线传播环境，结合强化学习生成控制策略和功率预测器。", "result": "仿真结果显示，传输功率降低50%以上，同时保持与基线方法相当的控制性能。", "conclusion": "该方法有效平衡了无线资源优化和控制性能，具有实际应用潜力。"}}
{"id": "2506.16234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16234", "abs": "https://arxiv.org/abs/2506.16234", "authors": ["Prakhar Verma", "David Arbour", "Sunav Choudhary", "Harshita Chopra", "Arno Solin", "Atanu R. Sinha"], "title": "Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data", "comment": "24 pages, preprint", "summary": "Causal discovery from observational data typically assumes full access to\ndata and availability of domain experts. In practice, data often arrive in\nbatches, and expert knowledge is scarce. Language Models (LMs) offer a\nsurrogate but come with their own issues-hallucinations, inconsistencies, and\nbias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid\nBayesian framework that bridges these gaps by adaptively integrating sequential\nbatch data with LM-derived noisy, expert knowledge while accounting for both\ndata-induced and LM-induced biases. Our proposed representation shift from\nDirected Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates\nambiguities within a coherent Bayesian framework, allowing grounding the global\nLM knowledge in local observational data. To guide LM interaction, we use a\nsequential optimization scheme that adaptively queries the most informative\nedges. Across varied datasets, BLANCE outperforms prior work in structural\naccuracy and extends to Bayesian parameter estimation, showing robustness to LM\nnoise.", "AI": {"tldr": "BLANCE是一种混合贝叶斯框架，通过自适应整合序列批数据和语言模型（LM）生成的噪声专家知识，解决因果发现中的数据稀缺和专家知识不足问题。", "motivation": "实践中，数据通常分批到达且专家知识稀缺，而语言模型虽能替代但存在幻觉、不一致性和偏见问题。", "method": "BLANCE采用从有向无环图（DAG）到部分祖先图（PAG）的表示转换，结合贝叶斯框架和序列优化方案，自适应查询最有信息量的边。", "result": "BLANCE在结构准确性和贝叶斯参数估计方面优于现有方法，对LM噪声具有鲁棒性。", "conclusion": "BLANCE通过结合LM知识和局部观测数据，为因果发现提供了一种鲁棒且高效的解决方案。"}}
{"id": "2506.16237", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16237", "abs": "https://arxiv.org/abs/2506.16237", "authors": ["Jacopo Iollo", "Geoffroy Oudoumanessah", "Carole Lartizien", "Michel Dojat", "Florence Forbes"], "title": "Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design", "comment": null, "summary": "A key challenge in maximizing the benefits of Magnetic Resonance Imaging\n(MRI) in clinical settings is to accelerate acquisition times without\nsignificantly degrading image quality. This objective requires a balance\nbetween under-sampling the raw k-space measurements for faster acquisitions and\ngathering sufficient raw information for high-fidelity image reconstruction and\nanalysis tasks. To achieve this balance, we propose to use sequential Bayesian\nexperimental design (BED) to provide an adaptive and task-dependent selection\nof the most informative measurements. Measurements are sequentially augmented\nwith new samples selected to maximize information gain on a posterior\ndistribution over target images. Selection is performed via a gradient-based\noptimization of a design parameter that defines a subsampling pattern. In this\nwork, we introduce a new active BED procedure that leverages diffusion-based\ngenerative models to handle the high dimensionality of the images and employs\nstochastic optimization to select among a variety of patterns while meeting the\nacquisition process constraints and budget. So doing, we show how our setting\ncan optimize, not only standard image reconstruction, but also any associated\nimage analysis task. The versatility and performance of our approach are\ndemonstrated on several MRI acquisitions.", "AI": {"tldr": "提出了一种基于贝叶斯实验设计的自适应MRI采样方法，通过最大化信息增益优化采样模式，兼顾图像质量和采集速度。", "motivation": "解决MRI采集时间与图像质量之间的平衡问题，提升临床效率。", "method": "采用序列贝叶斯实验设计，结合扩散生成模型和高维图像处理，通过梯度优化选择最优采样模式。", "result": "方法在多种MRI采集任务中表现出高效性和多功能性。", "conclusion": "该方法不仅优化了图像重建，还能适应多种图像分析任务，具有广泛的应用潜力。"}}
{"id": "2506.16852", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16852", "abs": "https://arxiv.org/abs/2506.16852", "authors": ["Chaonan Ji", "Jinwei Qi", "Peng Zhang", "Bang Zhang", "Liefeng Bo"], "title": "Controllable and Expressive One-Shot Video Head Swapping", "comment": "Project page: https://humanaigc.github.io/SwapAnyHead/", "summary": "In this paper, we propose a novel diffusion-based multi-condition\ncontrollable framework for video head swapping, which seamlessly transplant a\nhuman head from a static image into a dynamic video, while preserving the\noriginal body and background of target video, and further allowing to tweak\nhead expressions and movements during swapping as needed. Existing\nface-swapping methods mainly focus on localized facial replacement neglecting\nholistic head morphology, while head-swapping approaches struggling with\nhairstyle diversity and complex backgrounds, and none of these methods allow\nusers to modify the transplanted head expressions after swapping. To tackle\nthese challenges, our method incorporates several innovative strategies through\na unified latent diffusion paradigm. 1) Identity-preserving context fusion: We\npropose a shape-agnostic mask strategy to explicitly disentangle foreground\nhead identity features from background/body contexts, combining hair\nenhancement strategy to achieve robust holistic head identity preservation\nacross diverse hair types and complex backgrounds. 2) Expression-aware landmark\nretargeting and editing: We propose a disentangled 3DMM-driven retargeting\nmodule that decouples identity, expression, and head poses, minimizing the\nimpact of original expressions in input images and supporting expression\nediting. While a scale-aware retargeting strategy is further employed to\nminimize cross-identity expression distortion for higher transfer precision.\nExperimental results demonstrate that our method excels in seamless background\nintegration while preserving the identity of the source portrait, as well as\nshowcasing superior expression transfer capabilities applicable to both real\nand virtual characters.", "AI": {"tldr": "提出了一种基于扩散的多条件可控视频头部替换框架，支持静态图像头部无缝移植到动态视频中，并允许调整表情和动作。", "motivation": "现有方法主要关注局部面部替换，忽略了整体头部形态，且无法在替换后修改表情。本文旨在解决这些问题。", "method": "采用统一潜在扩散范式，包括身份保留上下文融合和表情感知地标重定向与编辑模块。", "result": "实验表明，该方法在背景无缝融合和身份保留方面表现出色，同时具备优越的表情转移能力。", "conclusion": "该方法在视频头部替换中实现了更高的精确度和可控性，适用于真实和虚拟角色。"}}
{"id": "2506.16243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16243", "abs": "https://arxiv.org/abs/2506.16243", "authors": ["Abdulvahap Mutlu", "Şengül Doğan", "Türker Tuncer"], "title": "Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping", "comment": "The code is available on GitHub:\n  https://github.com/abdulvahapmutlu/als-synthetic-data-augmentation-wgan", "summary": "Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and\nhigh-quality EEG data from ALS patients are scarce. This data scarcity, coupled\nwith severe class imbalance between ALS and healthy control recordings, poses a\nchallenge for training reliable machine learning classifiers. In this work, we\naddress these issues by generating synthetic EEG signals for ALS patients using\na Conditional Wasserstein Generative Adversarial Network (CWGAN). We train\nCWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of\nALS EEG signals and produce realistic synthetic samples. We preprocess and\nnormalize EEG recordings, and train a CWGAN model to generate synthetic ALS\nsignals. The CWGAN architecture and training routine are detailed, with key\nhyperparameters chosen for stable training. Qualitative evaluation of generated\nsignals shows that they closely mimic real ALS EEG patterns. The CWGAN training\nconverged with generator and discriminator loss curves stabilizing, indicating\nsuccessful learning. The synthetic EEG signals appear realistic and have\npotential use as augmented data for training classifiers, helping to mitigate\nclass imbalance and improve ALS detection accuracy. We discuss how this\napproach can facilitate data sharing and enhance diagnostic models.", "AI": {"tldr": "使用条件Wasserstein生成对抗网络（CWGAN）生成ALS患者的合成EEG信号，以解决数据稀缺和类别不平衡问题。", "motivation": "ALS是一种罕见神经退行性疾病，高质量EEG数据稀缺且类别不平衡，影响机器学习分类器的可靠性。", "method": "通过CWGAN学习ALS EEG信号的分布，生成合成样本，并详细描述了模型架构和训练过程。", "result": "生成的合成信号与真实ALS EEG模式相似，训练稳定，可用于增强分类器训练。", "conclusion": "该方法有助于缓解数据稀缺和类别不平衡，提升ALS检测准确性，并促进数据共享。"}}
{"id": "2506.16856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16856", "abs": "https://arxiv.org/abs/2506.16856", "authors": ["Jun Fu", "Bin Tian", "Haonan Chen", "Shi Meng", "Tingting Yao"], "title": "ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control", "comment": null, "summary": "Autonomous parking plays a vital role in intelligent vehicle systems,\nparticularly in constrained urban environments where high-precision control is\nrequired. While traditional rule-based parking systems struggle with\nenvironmental uncertainties and lack adaptability in crowded or dynamic scenes,\nhuman drivers demonstrate the ability to park intuitively without explicit\nmodeling. Inspired by this observation, we propose a Transformer-based\nend-to-end framework for autonomous parking that learns from expert\ndemonstrations. The network takes as input surround-view camera images,\ngoal-point representations, ego vehicle motion, and pedestrian trajectories. It\noutputs discrete control sequences including throttle, braking, steering, and\ngear selection. A novel cross-attention module integrates BEV features with\ntarget points, and a GRU-based pedestrian predictor enhances safety by modeling\ndynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both\nvertical and parallel parking scenarios. Experiments show our model achieves a\nhigh success rate of 96.57\\%, with average positional and orientation errors of\n0.21 meters and 0.41 degrees, respectively. The ablation studies further\ndemonstrate the effectiveness of key modules such as pedestrian prediction and\ngoal-point attention fusion. The code and dataset will be released at:\nhttps://github.com/little-snail-f/ParkFormer.", "AI": {"tldr": "提出了一种基于Transformer的端到端自动驾驶停车框架，通过学习专家演示实现高精度控制，成功率达96.57%。", "motivation": "传统基于规则的停车系统难以应对环境不确定性和动态场景，而人类驾驶员能直观停车，因此受启发提出新方法。", "method": "使用Transformer框架，输入包括环视摄像头图像、目标点表示、车辆运动和行人轨迹，输出离散控制序列。引入跨注意力模块和GRU行人预测器。", "result": "在CARLA模拟器中验证，成功率达96.57%，位置和方向误差分别为0.21米和0.41度。", "conclusion": "该方法在复杂环境中表现优异，关键模块如行人预测和目标点注意力融合效果显著。"}}
{"id": "2506.16895", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16895", "abs": "https://arxiv.org/abs/2506.16895", "authors": ["Fabian Gröger", "Shuo Wen", "Huyen Le", "Maria Brbić"], "title": "With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You", "comment": null, "summary": "Multimodal models have demonstrated powerful capabilities in complex tasks\nrequiring multimodal alignment including zero-shot classification and\ncross-modal retrieval. However, existing models typically rely on millions of\npaired multimodal samples, which are prohibitively expensive or infeasible to\nobtain in many domains. In this work, we explore the feasibility of building\nmultimodal models with limited amount of paired data by aligning pretrained\nunimodal foundation models. We show that high-quality alignment is possible\nwith as few as tens of thousands of paired samples$\\unicode{x2013}$less than\n$1\\%$ of the data typically used in the field. To achieve this, we introduce\nSTRUCTURE, an effective regularization technique that preserves the\nneighborhood geometry of the latent space of unimodal encoders. Additionally,\nwe show that aligning last layers is often suboptimal and demonstrate the\nbenefits of aligning the layers with the highest representational similarity\nacross modalities. These two components can be readily incorporated into\nexisting alignment methods, yielding substantial gains across 24 zero-shot\nimage classification and retrieval benchmarks, with average relative\nimprovement of $51.6\\%$ in classification and $91.8\\%$ in retrieval tasks. Our\nresults highlight the effectiveness and broad applicability of our framework\nfor limited-sample multimodal learning and offer a promising path forward for\nresource-constrained domains.", "AI": {"tldr": "提出了一种名为STRUCTURE的正则化技术，通过少量配对数据（少于1%）实现多模态模型的高质量对齐，显著提升零样本分类和跨模态检索性能。", "motivation": "现有模型依赖大量配对数据，成本高且难以获取，探索在有限配对数据下构建多模态模型的可行性。", "method": "对齐预训练单模态基础模型，引入STRUCTURE正则化技术保护潜在空间几何结构，并优化对齐层选择。", "result": "在24个基准测试中，分类任务平均提升51.6%，检索任务提升91.8%。", "conclusion": "该框架在资源受限领域具有广泛适用性，为有限样本多模态学习提供了有效路径。"}}
{"id": "2506.16288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16288", "abs": "https://arxiv.org/abs/2506.16288", "authors": ["Leo Gagnon", "Eric Elmoznino", "Sarthak Mittal", "Tom Marty", "Tejas Kasetty", "Dhanya Sridhar", "Guillaume Lajoie"], "title": "Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective", "comment": null, "summary": "The rapid adaptation ability of auto-regressive foundation models is often\nattributed to the diversity of their pre-training data. This is because, from a\nBayesian standpoint, minimizing prediction error in such settings requires\nintegrating over all plausible latent hypotheses consistent with observations.\nWhile this behavior is desirable in principle, it often proves too ambitious in\npractice: under high ambiguity, the number of plausible latent alternatives\nmakes Bayes-optimal prediction computationally intractable. Cognitive science\nhas long recognized this limitation, suggesting that under such conditions,\nheuristics or information-seeking strategies are preferable to exhaustive\ninference. Translating this insight to next-token prediction, we hypothesize\nthat low- and high-ambiguity predictions pose different computational demands,\nmaking ambiguity-agnostic next-token prediction a detrimental inductive bias.\nTo test this, we introduce MetaHMM, a synthetic sequence meta-learning\nbenchmark with rich compositional structure and a tractable Bayesian oracle. We\nshow that Transformers indeed struggle with high-ambiguity predictions across\nmodel sizes. Motivated by cognitive theories, we propose a method to convert\npre-trained models into Monte Carlo predictors that decouple task inference\nfrom token prediction. Preliminary results show substantial gains in ambiguous\ncontexts through improved capacity allocation and test-time scalable inference,\nthough challenges remain.", "AI": {"tldr": "论文探讨了自回归基础模型在高模糊性预测中的局限性，提出了一种基于蒙特卡洛预测的方法来改进性能。", "motivation": "自回归模型在高模糊性预测中表现不佳，因为贝叶斯最优预测在计算上难以实现。认知科学启发我们，在这种情况下，启发式或信息搜索策略更为有效。", "method": "提出了MetaHMM合成序列元学习基准，并设计了一种方法，将预训练模型转换为蒙特卡洛预测器，以解耦任务推断和标记预测。", "result": "实验表明，Transformer模型在高模糊性预测中表现不佳，而蒙特卡洛预测方法在模糊情境中显著提升了性能。", "conclusion": "蒙特卡洛预测方法在高模糊性预测中具有潜力，但仍需进一步研究以解决剩余挑战。"}}
{"id": "2506.16940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16940", "abs": "https://arxiv.org/abs/2506.16940", "authors": ["Annika Thomas", "Robaire Galliath", "Aleksander Garbuz", "Luke Anger", "Cormac O'Neill", "Trevor Johst", "Dami Thomas", "George Lordos", "Jonathan P. How"], "title": "LunarLoc: Segment-Based Global Localization on the Moon", "comment": null, "summary": "Global localization is necessary for autonomous operations on the lunar\nsurface where traditional Earth-based navigation infrastructure, such as GPS,\nis unavailable. As NASA advances toward sustained lunar presence under the\nArtemis program, autonomous operations will be an essential component of tasks\nsuch as robotic exploration and infrastructure deployment. Tasks such as\nexcavation and transport of regolith require precise pose estimation, but\nproposed approaches such as visual-inertial odometry (VIO) accumulate odometry\ndrift over long traverses. Precise pose estimation is particularly important\nfor upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on\nautonomous agents to operate over extended timescales and varied terrain. To\nhelp overcome odometry drift over long traverses, we propose LunarLoc, an\napproach to global localization that leverages instance segmentation for\nzero-shot extraction of boulder landmarks from onboard stereo imagery. Segment\ndetections are used to construct a graph-based representation of the terrain,\nwhich is then aligned with a reference map of the environment captured during a\nprevious session using graph-theoretic data association. This method enables\naccurate and drift-free global localization in visually ambiguous settings.\nLunarLoc achieves sub-cm level accuracy in multi-session global localization\nexperiments, significantly outperforming the state of the art in lunar global\nlocalization. To encourage the development of further methods for global\nlocalization on the Moon, we release our datasets publicly with a playback\nmodule: https://github.com/mit-acl/lunarloc-data.", "AI": {"tldr": "LunarLoc提出了一种基于实例分割的月球全局定位方法，通过提取岩石标志物并构建图表示，实现高精度、无漂移的定位。", "motivation": "月球表面缺乏GPS等导航基础设施，传统视觉惯性里程计（VIO）在长距离任务中会累积漂移误差，影响自主任务的精确性。", "method": "利用实例分割从立体图像中提取岩石标志物，构建图表示，并通过图论数据关联与参考地图对齐。", "result": "在多会话全局定位实验中达到亚厘米级精度，显著优于现有技术。", "conclusion": "LunarLoc为月球任务提供了高精度定位解决方案，并公开数据集以促进进一步研究。"}}
{"id": "2506.16950", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16950", "abs": "https://arxiv.org/abs/2506.16950", "authors": ["Fanfei Li", "Thomas Klein", "Wieland Brendel", "Robert Geirhos", "Roland S. Zimmermann"], "title": "LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models", "comment": "ICML 2025 camera ready version", "summary": "Out-of-distribution (OOD) robustness is a desired property of computer vision\nmodels. Improving model robustness requires high-quality signals from\nrobustness benchmarks to quantify progress. While various benchmark datasets\nsuch as ImageNet-C were proposed in the ImageNet era, most ImageNet-C\ncorruption types are no longer OOD relative to today's large, web-scraped\ndatasets, which already contain common corruptions such as blur or JPEG\ncompression artifacts. Consequently, these benchmarks are no longer well-suited\nfor evaluating OOD robustness in the era of web-scale datasets. Indeed, recent\nmodels show saturating scores on ImageNet-era OOD benchmarks, indicating that\nit is unclear whether models trained on web-scale datasets truly become better\nat OOD generalization or whether they have simply been exposed to the test\ndistortions during training. To address this, we introduce LAION-C as a\nbenchmark alternative for ImageNet-C. LAION-C consists of six novel distortion\ntypes specifically designed to be OOD, even for web-scale datasets such as\nLAION. In a comprehensive evaluation of state-of-the-art models, we find that\nthe LAION-C dataset poses significant challenges to contemporary models,\nincluding MLLMs such as Gemini and GPT-4o. We additionally conducted a\npsychophysical experiment to evaluate the difficulty of our corruptions for\nhuman observers, enabling a comparison of models to lab-quality human\nrobustness data. We observe a paradigm shift in OOD generalization: from humans\noutperforming models, to the best models now matching or outperforming the best\nhuman observers.", "AI": {"tldr": "论文提出了LAION-C作为ImageNet-C的替代基准，旨在解决现有基准在评估OOD鲁棒性时因数据规模扩大而失效的问题。", "motivation": "现有OOD基准（如ImageNet-C）在Web规模数据集时代已不再适用，因为其失真类型已包含在训练数据中，无法有效评估模型的OOD泛化能力。", "method": "设计了六种新型失真类型构建LAION-C，确保其对Web规模数据集（如LAION）仍为OOD，并评估了包括MLLMs在内的先进模型。", "result": "LAION-C对当代模型（如Gemini和GPT-4o）构成显著挑战，且最佳模型的OOD泛化能力已接近或超越人类观察者。", "conclusion": "LAION-C为OOD鲁棒性评估提供了更有效的基准，揭示了模型在OOD泛化上的范式转变。"}}
{"id": "2506.16313", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.8; G.3"], "pdf": "https://arxiv.org/pdf/2506.16313", "abs": "https://arxiv.org/abs/2506.16313", "authors": ["Sajan Muhammad", "Salem Lahlou"], "title": "Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks", "comment": "Accepted to the EXAIT Workshop at ICML 2025", "summary": "Efficiently identifying the right trajectories for training remains an open\nproblem in GFlowNets. To address this, it is essential to prioritize\nexploration in regions of the state space where the reward distribution has not\nbeen sufficiently learned. This calls for uncertainty-driven exploration, in\nother words, the agent should be aware of what it does not know. This attribute\ncan be measured by joint predictions, which are particularly important for\ncombinatorial and sequential decision problems. In this research, we integrate\nepistemic neural networks (ENN) with the conventional architecture of GFlowNets\nto enable more efficient joint predictions and better uncertainty\nquantification, thereby improving exploration and the identification of optimal\ntrajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the\nbaseline method in GFlownets and evaluated in grid environments and structured\nsequence generation in various settings, demonstrating both its efficacy and\nefficiency.", "AI": {"tldr": "论文提出了一种结合ENN与GFlowNets的方法（ENN-GFN-Enhanced），通过不确定性驱动的探索优化轨迹训练。", "motivation": "GFlowNets中高效识别训练轨迹的问题尚未解决，需在未充分学习的奖励分布区域优先探索。", "method": "将ENN与传统GFlowNets架构结合，提升联合预测能力与不确定性量化。", "result": "在网格环境和结构化序列生成中，ENN-GFN-Enhanced优于基线方法。", "conclusion": "ENN-GFN-Enhanced通过不确定性驱动探索，显著提升了轨迹识别的效率与效果。"}}
{"id": "2506.16960", "categories": ["cs.CV", "68U10", "I.4.4"], "pdf": "https://arxiv.org/pdf/2506.16960", "abs": "https://arxiv.org/abs/2506.16960", "authors": ["Wenyang Luo", "Haina Qin", "Zewen Chen", "Libin Wang", "Dandan Zheng", "Yuming Li", "Yufan Liu", "Bing Li", "Weiming Hu"], "title": "Visual-Instructed Degradation Diffusion for All-in-One Image Restoration", "comment": "CVPR2025 Final Version; Corresponding Author: Bing Li", "summary": "Image restoration tasks like deblurring, denoising, and dehazing usually need\ndistinct models for each degradation type, restricting their generalization in\nreal-world scenarios with mixed or unknown degradations. In this work, we\npropose \\textbf{Defusion}, a novel all-in-one image restoration framework that\nutilizes visual instruction-guided degradation diffusion. Unlike existing\nmethods that rely on task-specific models or ambiguous text-based priors,\nDefusion constructs explicit \\textbf{visual instructions} that align with the\nvisual degradation patterns. These instructions are grounded by applying\ndegradations to standardized visual elements, capturing intrinsic degradation\nfeatures while agnostic to image semantics. Defusion then uses these visual\ninstructions to guide a diffusion-based model that operates directly in the\ndegradation space, where it reconstructs high-quality images by denoising the\ndegradation effects with enhanced stability and generalizability. Comprehensive\nexperiments demonstrate that Defusion outperforms state-of-the-art methods\nacross diverse image restoration tasks, including complex and real-world\ndegradations.", "AI": {"tldr": "Defusion是一种新型的全能图像恢复框架，通过视觉指令引导的退化扩散，解决了传统方法需要针对每种退化类型单独建模的问题。", "motivation": "传统图像恢复方法需要为每种退化类型（如去模糊、去噪、去雾）设计独立模型，限制了其在混合或未知退化场景中的泛化能力。", "method": "Defusion利用视觉指令引导退化扩散，通过将退化应用于标准化视觉元素构建明确的视觉指令，从而指导扩散模型在退化空间中直接操作。", "result": "实验表明，Defusion在多种图像恢复任务中优于现有方法，包括复杂和真实世界的退化场景。", "conclusion": "Defusion通过视觉指令引导的退化扩散，实现了更高的稳定性和泛化能力，为全能图像恢复提供了有效解决方案。"}}
{"id": "2506.16314", "categories": ["cs.LG", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2506.16314", "abs": "https://arxiv.org/abs/2506.16314", "authors": ["Emmanuel Gangler", "Emille E. O. Ishida", "Matwey V. Kornilov", "Vladimir Korolev", "Anastasia Lavrukhina", "Konstantin Malanchev", "Maria V. Pruzhinskaya", "Etienne Russeil", "Timofey Semenikhin", "Sreevarsha Sreejith", "Alina A. Volnova"], "title": "Signatures to help interpretability of anomalies", "comment": "7 pages, 3 figure, proceedings of the International Conference on\n  Machine Learning for Astrophysics (ML4ASTRO2)", "summary": "Machine learning is often viewed as a black box when it comes to\nunderstanding its output, be it a decision or a score. Automatic anomaly\ndetection is no exception to this rule, and quite often the astronomer is left\nto independently analyze the data in order to understand why a given event is\ntagged as an anomaly. We introduce here idea of anomaly signature, whose aim is\nto help the interpretability of anomalies by highlighting which features\ncontributed to the decision.", "AI": {"tldr": "论文提出了一种称为“异常签名”的方法，旨在通过突出显示影响决策的特征来提高异常检测的可解释性。", "motivation": "机器学习通常被视为黑盒，难以理解其输出（如决策或分数），天文学家需要独立分析数据以理解异常事件的原因。", "method": "引入“异常签名”的概念，通过突出显示对决策有贡献的特征来帮助解释异常。", "result": "该方法提高了异常检测的可解释性，使天文学家更容易理解异常事件的原因。", "conclusion": "“异常签名”是一种有效的方法，能够增强机器学习在异常检测中的透明度和可解释性。"}}
{"id": "2506.16961", "categories": ["cs.CV", "eess.IV", "68U10", "I.4.4"], "pdf": "https://arxiv.org/pdf/2506.16961", "abs": "https://arxiv.org/abs/2506.16961", "authors": ["Haina Qin", "Wenyang Luo", "Libin Wang", "Dandan Zheng", "Jingdong Chen", "Ming Yang", "Bing Li", "Weiming Hu"], "title": "Reversing Flow for Image Restoration", "comment": "CVPR2025 Final Version; Corresponding Author: Bing Li", "summary": "Image restoration aims to recover high-quality (HQ) images from degraded\nlow-quality (LQ) ones by reversing the effects of degradation. Existing\ngenerative models for image restoration, including diffusion and score-based\nmodels, often treat the degradation process as a stochastic transformation,\nwhich introduces inefficiency and complexity. In this work, we propose ResFlow,\na novel image restoration framework that models the degradation process as a\ndeterministic path using continuous normalizing flows. ResFlow augments the\ndegradation process with an auxiliary process that disambiguates the\nuncertainty in HQ prediction to enable reversible modeling of the degradation\nprocess. ResFlow adopts entropy-preserving flow paths and learns the augmented\ndegradation flow by matching the velocity field. ResFlow significantly improves\nthe performance and speed of image restoration, completing the task in fewer\nthan four sampling steps. Extensive experiments demonstrate that ResFlow\nachieves state-of-the-art results across various image restoration benchmarks,\noffering a practical and efficient solution for real-world applications.", "AI": {"tldr": "ResFlow是一种新的图像恢复框架，通过确定性路径建模退化过程，显著提升了性能和速度。", "motivation": "现有生成模型将退化过程视为随机变换，导致效率低下和复杂性增加。", "method": "ResFlow使用连续归一化流建模退化过程，引入辅助过程消除预测不确定性，并采用熵保持流路径匹配速度场。", "result": "ResFlow在少于4步采样中完成任务，在多个基准测试中达到最优效果。", "conclusion": "ResFlow为图像恢复提供了一种高效实用的解决方案。"}}
{"id": "2506.16316", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16316", "abs": "https://arxiv.org/abs/2506.16316", "authors": ["Huy Hoang Nguyen", "Han Zhou", "Matthew B. Blaschko", "Aleksei Tiulpin"], "title": "Bayesian Optimization over Bounded Domains with the Beta Product Kernel", "comment": "Accepted as a conference paper at UAI 2025", "summary": "Bayesian optimization with Gaussian processes (GP) is commonly used to\noptimize black-box functions. The Mat\\'ern and the Radial Basis Function (RBF)\ncovariance functions are used frequently, but they do not make any assumptions\nabout the domain of the function, which may limit their applicability in\nbounded domains. To address the limitation, we introduce the Beta kernel, a\nnon-stationary kernel induced by a product of Beta distribution density\nfunctions. Such a formulation allows our kernel to naturally model functions on\nbounded domains. We present statistical evidence supporting the hypothesis that\nthe kernel exhibits an exponential eigendecay rate, based on empirical analyses\nof its spectral properties across different settings. Our experimental results\ndemonstrate the robustness of the Beta kernel in modeling functions with optima\nlocated near the faces or vertices of the unit hypercube. The experiments show\nthat our kernel consistently outperforms a wide range of kernels, including the\nwell-known Mat\\'ern and RBF, in different problems, including synthetic\nfunction optimization and the compression of vision and language models.", "AI": {"tldr": "论文提出了一种新的Beta核函数，用于贝叶斯优化中的高斯过程，特别适用于有界域的函数建模，并在实验中表现优于常见的Matérn和RBF核。", "motivation": "现有的Matérn和RBF核函数未考虑函数的有界域特性，限制了其适用性。", "method": "引入基于Beta分布密度函数的非平稳Beta核，通过其谱性质分析验证其指数特征衰减率。", "result": "Beta核在有界域（如单位超立方体的面或顶点附近）的函数建模中表现稳健，并在合成函数优化和模型压缩任务中优于其他核。", "conclusion": "Beta核为有界域的函数优化提供了更有效的工具，扩展了高斯过程的应用范围。"}}
{"id": "2506.16962", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.16962", "abs": "https://arxiv.org/abs/2506.16962", "authors": ["Haoran Sun", "Yankai Jiang", "Wenjie Lou", "Yujie Zhang", "Wenjie Li", "Lilong Wang", "Mianxin Liu", "Lei Liu", "Xiaosong Wang"], "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs", "comment": null, "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs", "AI": {"tldr": "论文提出了一种名为MICS的新方法，用于生成高质量的医疗链式思维（CoT）数据，并通过实验验证了其有效性。", "motivation": "当前多模态大语言模型（MLLMs）在医疗领域的应用尚处于早期阶段，缺乏有效的推理路径搜索和评估框架。", "method": "提出MICS方法，通过导师-实习生协作搜索生成严格的医疗CoT数据，并构建多任务医疗推理数据集MMRP和新型医疗MLLM Chiron-o1。", "result": "实验表明，基于MICS构建的CoT数据集训练的Chiron-o1在医疗视觉问答和推理任务中达到最先进性能。", "conclusion": "MICS方法为医疗MLLMs提供了有效的推理路径生成框架，显著提升了模型性能。"}}
{"id": "2506.16349", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16349", "abs": "https://arxiv.org/abs/2506.16349", "authors": ["Nikola Jovanović", "Ismail Labiad", "Tomáš Souček", "Martin Vechev", "Pierre Fernandez"], "title": "Watermarking Autoregressive Image Generation", "comment": "Code: https://github.com/facebookresearch/wmar", "summary": "Watermarking the outputs of generative models has emerged as a promising\napproach for tracking their provenance. Despite significant interest in\nautoregressive image generation models and their potential for misuse, no prior\nwork has attempted to watermark their outputs at the token level. In this work,\nwe present the first such approach by adapting language model watermarking\ntechniques to this setting. We identify a key challenge: the lack of reverse\ncycle-consistency (RCC), wherein re-tokenizing generated image tokens\nsignificantly alters the token sequence, effectively erasing the watermark. To\naddress this and to make our method robust to common image transformations,\nneural compression, and removal attacks, we introduce (i) a custom\ntokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a\ncomplementary watermark synchronization layer. As our experiments demonstrate,\nour approach enables reliable and robust watermark detection with theoretically\ngrounded p-values.", "AI": {"tldr": "本文首次提出了一种在自回归图像生成模型的输出中进行令牌级水印的方法，解决了令牌序列因重新标记化而改变的问题，并通过改进的反向循环一致性和水印同步层提高了鲁棒性。", "motivation": "随着生成模型的普及，追踪其输出的来源变得尤为重要，但目前缺乏对自回归图像生成模型输出的令牌级水印方法。", "method": "通过调整语言模型水印技术，提出了一种令牌级水印方法，包括改进的反向循环一致性（RCC）和水印同步层。", "result": "实验表明，该方法能够可靠且鲁棒地检测水印，并提供理论支持的p值。", "conclusion": "该方法为自回归图像生成模型的输出水印提供了一种有效的解决方案，具有实际应用潜力。"}}
{"id": "2506.16991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16991", "abs": "https://arxiv.org/abs/2506.16991", "authors": ["Binbin Xiang", "Maciej Wielgosz", "Stefano Puliti", "Kamil Král", "Martin Krůček", "Azim Missarov", "Rasmus Astrup"], "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds", "comment": null, "summary": "The segmentation of forest LiDAR 3D point clouds, including both individual\ntree and semantic segmentation, is fundamental for advancing forest management\nand ecological research. However, current approaches often struggle with the\ncomplexity and variability of natural forest environments. We present\nForestFormer3D, a new unified and end-to-end framework designed for precise\nindividual tree and semantic segmentation. ForestFormer3D incorporates\nISA-guided query point selection, a score-based block merging strategy during\ninference, and a one-to-many association mechanism for effective training. By\ncombining these new components, our model achieves state-of-the-art performance\nfor individual tree segmentation on the newly introduced FOR-instanceV2\ndataset, which spans diverse forest types and regions. Additionally,\nForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),\nshowcasing its robustness across different forest conditions and sensor\nmodalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be\nreleased soon.", "AI": {"tldr": "ForestFormer3D是一种新的端到端框架，用于精确的树木分割和语义分割，通过创新组件在复杂森林环境中实现最先进性能。", "motivation": "森林LiDAR点云分割对林业管理和生态研究至关重要，但现有方法难以应对自然环境的复杂性和多样性。", "method": "ForestFormer3D结合了ISA引导的查询点选择、基于分数的块合并策略和一对多关联机制。", "result": "模型在FOR-instanceV2数据集上表现优异，并在未见测试集（Wytham woods和LAUTx）上展示出强泛化能力。", "conclusion": "ForestFormer3D为森林点云分割提供了高效解决方案，其数据集和代码将公开。"}}
{"id": "2506.16352", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16352", "abs": "https://arxiv.org/abs/2506.16352", "authors": ["Theo Zangato", "Aomar Osmani", "Pegah Alizadeh"], "title": "Data-Driven Policy Mapping for Safe RL-based Energy Management Systems", "comment": null, "summary": "Increasing global energy demand and renewable integration complexity have\nplaced buildings at the center of sustainable energy management. We present a\nthree-step reinforcement learning(RL)-based Building Energy Management System\n(BEMS) that combines clustering, forecasting, and constrained policy learning\nto address scalability, adaptability, and safety challenges. First, we cluster\nnon-shiftable load profiles to identify common consumption patterns, enabling\npolicy generalization and transfer without retraining for each new building.\nNext, we integrate an LSTM based forecasting module to anticipate future\nstates, improving the RL agents' responsiveness to dynamic conditions. Lastly,\ndomain-informed action masking ensures safe exploration and operation,\npreventing harmful decisions. Evaluated on real-world data, our approach\nreduces operating costs by up to 15% for certain building types, maintains\nstable environmental performance, and quickly classifies and optimizes new\nbuildings with limited data. It also adapts to stochastic tariff changes\nwithout retraining. Overall, this framework delivers scalable, robust, and\ncost-effective building energy management.", "AI": {"tldr": "提出了一种基于强化学习的三步建筑能源管理系统（BEMS），通过聚类、预测和约束策略学习解决可扩展性、适应性和安全性问题，显著降低运营成本并提升性能。", "motivation": "全球能源需求增长和可再生能源整合复杂性增加，建筑成为可持续能源管理的核心，需要一种可扩展且安全的解决方案。", "method": "1. 聚类非可转移负载以识别通用模式；2. 集成LSTM预测模块；3. 使用领域知识约束策略确保安全。", "result": "在真实数据上验证，运营成本降低15%，环境性能稳定，并能快速适应新建筑和电价变化。", "conclusion": "该框架提供了可扩展、稳健且经济高效的建筑能源管理方案。"}}
{"id": "2506.16994", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16994", "abs": "https://arxiv.org/abs/2506.16994", "authors": ["Yasir Ali Farrukh", "Syed Wali", "Irfan Khan", "Nathaniel D. Bastian"], "title": "Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments", "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world\nvision systems, especially in resource-constrained environments like drones,\nwhere memory and computation are limited. Existing prompt-driven UDA methods\ntypically rely on large vision-language models and require full access to\nsource-domain data during adaptation, limiting their applicability. In this\nwork, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain\nadaptation framework built around a teacher-student paradigm guided by\nprompt-based feature alignment. At the core of our method is a distilled and\nfine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A\nsmall set of low-level source features is aligned to the target domain\nsemantics-specified only through a natural language prompt-via Prompt-driven\nInstance Normalization (PIN). These semantically steered features are used to\nbriefly fine-tune the detection head of the teacher model. The adapted teacher\nthen generates high-quality pseudo-labels, which guide the on-the-fly\nadaptation of a compact student model. Experiments on the MDS-A dataset\ndemonstrate that Prmpt2Adpt achieves competitive detection performance compared\nto state-of-the-art methods, while delivering up to 7x faster adaptation and 5x\nfaster inference speed using few source images-making it a practical and\nscalable solution for real-time adaptation in low-resource domains.", "AI": {"tldr": "Prmpt2Adpt是一种轻量级、高效的零样本域适应框架，通过基于提示的特征对齐和教师-学生范式实现快速适应，适用于资源受限环境。", "motivation": "解决现有提示驱动的无监督域适应方法依赖大型视觉语言模型和完整源域数据访问的问题，提升在资源受限环境（如无人机）中的适用性。", "method": "使用蒸馏和微调的CLIP模型作为Faster R-CNN教师的冻结主干，通过提示驱动的实例归一化（PIN）对齐低层源特征与目标域语义，生成高质量伪标签指导紧凑学生模型适应。", "result": "在MDS-A数据集上，Prmpt2Adpt实现了与最先进方法相当的检测性能，同时适应速度快7倍，推理速度快5倍。", "conclusion": "Prmpt2Adpt是一种实用且可扩展的解决方案，适用于低资源领域的实时域适应。"}}
{"id": "2506.16380", "categories": ["cs.LG", "I.5.1; I.5.4; I.2.10; I.2.6; C.3; J.2; H.4.2"], "pdf": "https://arxiv.org/pdf/2506.16380", "abs": "https://arxiv.org/abs/2506.16380", "authors": ["Druva Dhakshinamoorthy", "Avikshit Jha", "Sabyasachi Majumdar", "Devdulal Ghosh", "Ranjita Chakraborty", "Hena Ray"], "title": "Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data", "comment": "6 pages, 5 figures. Druva Dhakshinamoorthy and Avikshit Jha\n  contributed equally as co-first authors. Work conducted during a summer\n  internship at CDAC Kolkata by students of BITS Pilani", "summary": "This paper presents a novel system for monitoring cattle behavior and\ndetecting estrus (heat) periods using sensor data and machine learning. We\ndesigned and deployed a low-cost Bluetooth-based neck collar equipped with\naccelerometer and gyroscope sensors to capture real-time behavioral data from\nreal cows, which was synced to the cloud. A labeled dataset was created using\nsynchronized CCTV footage to annotate behaviors such as feeding, rumination,\nlying, and others. We evaluated multiple machine learning models -- Support\nVector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks\n(CNN) -- for behavior classification. Additionally, we implemented a Long\nShort-Term Memory (LSTM) model for estrus detection using behavioral patterns\nand anomaly detection. Our system achieved over 93% behavior classification\naccuracy and 96% estrus detection accuracy on a limited test set. The approach\noffers a scalable and accessible solution for precision livestock monitoring,\nespecially in resource-constrained environments.", "AI": {"tldr": "论文提出了一种基于传感器数据和机器学习的牛行为监测与发情期检测系统，使用低成本蓝牙颈圈和多种机器学习模型，取得了高准确率。", "motivation": "为精准畜牧业提供低成本、可扩展的解决方案，特别是在资源有限的环境中。", "method": "设计蓝牙颈圈采集行为数据，结合CCTV标注数据集，评估SVM、RF、CNN等模型进行分类，并使用LSTM进行发情检测。", "result": "行为分类准确率超过93%，发情检测准确率达96%。", "conclusion": "该系统为畜牧业提供了一种高效且易于推广的监测方案。"}}
{"id": "2506.17004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17004", "abs": "https://arxiv.org/abs/2506.17004", "authors": ["Hanlin Wu", "Pengfei Lin", "Ehsan Javanmardi", "Naren Bao", "Bo Qian", "Hao Si", "Manabu Tsukada"], "title": "A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving", "comment": null, "summary": "3D semantic occupancy prediction is an emerging perception paradigm in\nautonomous driving, providing a voxel-level representation of both geometric\ndetails and semantic categories. However, the perception capability of a single\nvehicle is inherently constrained by occlusion, restricted sensor range, and\nnarrow viewpoints. To address these limitations, collaborative perception\nenables the exchange of complementary information, thereby enhancing the\ncompleteness and accuracy. In the absence of a dedicated dataset for\ncollaborative 3D semantic occupancy prediction, we augment an existing\ncollaborative perception dataset by replaying it in CARLA with a\nhigh-resolution semantic voxel sensor to provide dense and comprehensive\noccupancy annotations. In addition, we establish benchmarks with varying\nprediction ranges designed to systematically assess the impact of spatial\nextent on collaborative prediction. We further develop a baseline model that\nperforms inter-agent feature fusion via spatial alignment and attention\naggregation. Experimental results demonstrate that our baseline model\nconsistently outperforms single-agent models, with increasing gains observed as\nthe prediction range expands.", "AI": {"tldr": "论文提出了一种通过协作感知增强3D语义占用预测的方法，填补了数据集空白并建立了基准测试。", "motivation": "单车辆的感知能力受限于遮挡、传感器范围和视角，协作感知可以交换互补信息以提高准确性和完整性。", "method": "通过CARLA重放现有协作感知数据集，提供密集的语义体素标注；开发了基于空间对齐和注意力聚合的基线模型。", "result": "基线模型在扩展预测范围时表现优于单车辆模型，增益随范围增加而提升。", "conclusion": "协作感知能显著提升3D语义占用预测的性能，尤其在长距离预测中效果更明显。"}}
{"id": "2506.17027", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.17027", "abs": "https://arxiv.org/abs/2506.17027", "authors": ["Yiyang Tie", "Hong Zhu", "Yunyun Luo", "Jing Shi"], "title": "Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns", "comment": null, "summary": "The training of real-world super-resolution reconstruction models heavily\nrelies on datasets that reflect real-world degradation patterns. Extracting and\nmodeling degradation patterns for super-resolution reconstruction using only\nreal-world low-resolution (LR) images remains a challenging task. When\nsynthesizing datasets to simulate real-world degradation, relying solely on\ndegradation extraction methods fails to capture both blur and diverse noise\ncharacteristics across varying LR distributions, as well as more implicit\ndegradations such as color gamut shifts. Conversely, domain translation alone\ncannot accurately approximate real-world blur characteristics due to the\nsignificant degradation domain gap between synthetic and real data. To address\nthese challenges, we propose a novel TripleGAN framework comprising two\nstrategically designed components: The FirstGAN primarily focuses on narrowing\nthe domain gap in blur characteristics, while the SecondGAN performs\ndomain-specific translation to approximate target-domain blur properties and\nlearn additional degradation patterns. The ThirdGAN is trained on pseudo-real\ndata generated by the FirstGAN and SecondGAN to reconstruct real-world LR\nimages. Extensive experiments on the RealSR and DRealSR datasets demonstrate\nthat our method exhibits clear advantages in quantitative metrics while\nmaintaining sharp reconstructions without over-smoothing artifacts. The\nproposed framework effectively learns real-world degradation patterns from LR\nobservations and synthesizes aligned datasets with corresponding degradation\ncharacteristics, thereby enabling the trained network to achieve superior\nperformance in reconstructing high-quality SR images from real-world LR inputs.", "AI": {"tldr": "提出TripleGAN框架，通过三个GAN组件分别解决模糊域差距、学习额外退化模式及重建真实低分辨率图像，显著提升超分辨率重建性能。", "motivation": "真实世界超分辨率重建依赖反映真实退化模式的数据集，但现有方法难以同时捕捉模糊、噪声和隐式退化。", "method": "TripleGAN框架包含三个GAN：FirstGAN缩小模糊域差距，SecondGAN学习目标域模糊特性和额外退化模式，ThirdGAN重建真实低分辨率图像。", "result": "在RealSR和DRealSR数据集上，方法在定量指标上表现优越，重建图像清晰无过度平滑。", "conclusion": "TripleGAN能有效学习真实退化模式并合成对齐数据集，显著提升超分辨率重建质量。"}}
{"id": "2506.16396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16396", "abs": "https://arxiv.org/abs/2506.16396", "authors": ["Alexey Zakharov", "Shimon Whiteson"], "title": "GoalLadder: Incremental Goal Discovery with Vision-Language Models", "comment": null, "summary": "Natural language can offer a concise and human-interpretable means of\nspecifying reinforcement learning (RL) tasks. The ability to extract rewards\nfrom a language instruction can enable the development of robotic systems that\ncan learn from human guidance; however, it remains a challenging problem,\nespecially in visual environments. Existing approaches that employ large,\npretrained language models either rely on non-visual environment\nrepresentations, require prohibitively large amounts of feedback, or generate\nnoisy, ill-shaped reward functions. In this paper, we propose a novel method,\n$\\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL\nagents from a single language instruction in visual environments. GoalLadder\nworks by incrementally discovering states that bring the agent closer to\ncompleting a task specified in natural language. To do so, it queries a VLM to\nidentify states that represent an improvement in agent's task progress and to\nrank them using pairwise comparisons. Unlike prior work, GoalLadder does not\ntrust VLM's feedback completely; instead, it uses it to rank potential goal\nstates using an ELO-based rating system, thus reducing the detrimental effects\nof noisy VLM feedback. Over the course of training, the agent is tasked with\nminimising the distance to the top-ranked goal in a learned embedding space,\nwhich is trained on unlabelled visual data. This key feature allows us to\nbypass the need for abundant and accurate feedback typically required to train\na well-shaped reward function. We demonstrate that GoalLadder outperforms\nexisting related methods on classic control and robotic manipulation\nenvironments with the average final success rate of $\\sim$95% compared to only\n$\\sim$45% of the best competitor.", "AI": {"tldr": "GoalLadder利用视觉语言模型（VLM）从单一语言指令中训练强化学习（RL）代理，通过逐步发现任务进展状态并减少噪声反馈的影响，显著优于现有方法。", "motivation": "自然语言可以为强化学习任务提供简洁且人类可理解的规范，但从语言指令中提取奖励仍具挑战性，尤其是在视觉环境中。现有方法依赖非视觉环境表示或需要大量反馈，且生成的奖励函数可能噪声较大。", "method": "GoalLadder通过查询VLM识别任务进展状态并利用ELO评级系统对潜在目标状态进行排序，减少噪声反馈的影响。代理通过最小化与排名最高目标的距离来学习，无需大量准确反馈。", "result": "GoalLadder在经典控制和机器人操作环境中表现优异，平均最终成功率约为95%，而最佳竞争对手仅为45%。", "conclusion": "GoalLadder通过结合VLM和ELO评级系统，有效解决了从语言指令中提取奖励的挑战，显著提升了RL代理的性能。"}}
{"id": "2506.16404", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16404", "abs": "https://arxiv.org/abs/2506.16404", "authors": ["Alba Carballo-Castro", "Manuel Madeira", "Yiming Qin", "Dorina Thanou", "Pascal Frossard"], "title": "Generating Directed Graphs with Dual Attention and Asymmetric Encoding", "comment": null, "summary": "Directed graphs naturally model systems with asymmetric, ordered\nrelationships, essential to applications in biology, transportation, social\nnetworks, and visual understanding. Generating such graphs enables tasks such\nas simulation, data augmentation and novel instance discovery; however,\ndirected graph generation remains underexplored. We identify two key factors\nlimiting progress in this direction: first, modeling edge directionality\nintroduces a substantially larger dependency space, making the underlying\ndistribution harder to learn; second, the absence of standardized benchmarks\nhinders rigorous evaluation. Addressing the former requires more expressive\nmodels that are sensitive to directional topologies. We propose Directo, the\nfirst generative model for directed graphs built upon the discrete flow\nmatching framework. Our approach combines: (i) principled positional encodings\ntailored to asymmetric pairwise relations, (ii) a dual-attention mechanism\ncapturing both incoming and outgoing dependencies, and (iii) a robust, discrete\ngenerative framework. To support evaluation, we introduce a benchmark suite\ncovering synthetic and real-world datasets. It shows that our method performs\nstrongly across diverse settings and even competes with specialized models for\nparticular classes, such as directed acyclic graphs. Our results highlight the\neffectiveness and generality of our approach, establishing a solid foundation\nfor future research in directed graph generation.", "AI": {"tldr": "论文提出了Directo，首个基于离散流匹配框架的有向图生成模型，解决了有向图生成中的依赖空间大和缺乏标准化基准的问题。", "motivation": "有向图在多个领域有广泛应用，但其生成方法研究不足，主要受限于依赖空间大和缺乏标准化基准。", "method": "结合了针对非对称关系的编码、双注意力机制和离散生成框架。", "result": "在合成和真实数据集上表现优异，甚至在某些类别（如有向无环图）上与专用模型竞争。", "conclusion": "Directo为有向图生成研究奠定了坚实基础，展示了方法的有效性和通用性。"}}
{"id": "2506.17051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17051", "abs": "https://arxiv.org/abs/2506.17051", "authors": ["Florent Meyer", "Laurent Guichard", "Denis Coquenet", "Guillaume Gravier", "Yann Soullard", "Bertrand Coüasnon"], "title": "Relaxed syntax modeling in Transformers for future-proof license plate recognition", "comment": null, "summary": "Effective license plate recognition systems are required to be resilient to\nconstant change, as new license plates are released into traffic daily. While\nTransformer-based networks excel in their recognition at first sight, we\nobserve significant performance drop over time which proves them unsuitable for\ntense production environments. Indeed, such systems obtain state-of-the-art\nresults on plates whose syntax is seen during training. Yet, we show they\nperform similarly to random guessing on future plates where legible characters\nare wrongly recognized due to a shift in their syntax. After highlighting the\nflows of positional and contextual information in Transformer encoder-decoders,\nwe identify several causes for their over-reliance on past syntax. Following,\nwe devise architectural cut-offs and replacements which we integrate into SaLT,\nan attempt at a Syntax-Less Transformer for syntax-agnostic modeling of license\nplate representations. Experiments on both real and synthetic datasets show\nthat our approach reaches top accuracy on past syntax and most importantly\nnearly maintains performance on future license plates. We further demonstrate\nthe robustness of our architecture enhancements by way of various ablations.", "AI": {"tldr": "论文提出了一种名为SaLT的语法无关Transformer模型，用于解决车牌识别系统因语法变化导致的性能下降问题。", "motivation": "现有基于Transformer的车牌识别系统在面对新语法车牌时性能显著下降，无法适应生产环境的需求。", "method": "通过分析Transformer编码器-解码器中位置和上下文信息的流动，识别了模型对过去语法的过度依赖问题，并设计了架构改进方案。", "result": "SaLT模型在历史和未来车牌数据上均表现优异，保持了高准确率。", "conclusion": "SaLT通过语法无关建模提升了车牌识别的鲁棒性，适用于动态变化的实际场景。"}}
{"id": "2506.16406", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16406", "abs": "https://arxiv.org/abs/2506.16406", "authors": ["Zhiyuan Liang", "Dongwen Tang", "Yuhao Zhou", "Xuanlei Zhao", "Mingjia Shi", "Wangbo Zhao", "Zekai Li", "Peihao Wang", "Konstantin Schürholt", "Damian Borth", "Michael M. Bronstein", "Yang You", "Zhangyang Wang", "Kai Wang"], "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights", "comment": "We propose a method that can generate LoRA parameters in seconds", "summary": "Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank\nadaptation (LoRA) reduce the cost of customizing large language models (LLMs),\nyet still require a separate optimization run for every downstream dataset. We\nintroduce \\textbf{Drag-and-Drop LLMs (\\textit{DnD})}, a prompt-conditioned\nparameter generator that eliminates per-task training by mapping a handful of\nunlabeled task prompts directly to LoRA weight updates. A lightweight text\nencoder distills each prompt batch into condition embeddings, which are then\ntransformed by a cascaded hyper-convolutional decoder into the full set of LoRA\nmatrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD\nproduces task-specific parameters in seconds, yielding i) up to\n\\textbf{12,000$\\times$} lower overhead than full fine-tuning, ii) average gains\nup to \\textbf{30\\%} in performance over the strongest training LoRAs on unseen\ncommon-sense reasoning, math, coding, and multimodal benchmarks, and iii)\nrobust cross-domain generalization despite never seeing the target data or\nlabels. Our results demonstrate that prompt-conditioned parameter generation is\na viable alternative to gradient-based adaptation for rapidly specializing\nLLMs. Our project is available at\n\\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.", "AI": {"tldr": "DnD是一种基于提示的条件参数生成方法，无需为每个任务单独训练，通过轻量级文本编码器和超卷积解码器生成LoRA权重更新，显著降低计算开销并提升性能。", "motivation": "减少为每个下游任务单独优化大型语言模型的成本，提高效率。", "method": "使用提示条件参数生成器，通过文本编码器和超卷积解码器将任务提示映射到LoRA权重更新。", "result": "DnD在计算开销上比全微调低12,000倍，性能提升30%，且具有跨领域泛化能力。", "conclusion": "提示条件参数生成是梯度微调的有效替代方案，能够快速定制LLMs。"}}
{"id": "2506.17074", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17074", "abs": "https://arxiv.org/abs/2506.17074", "authors": ["Wang Zhao", "Yan-Pei Cao", "Jiale Xu", "Yuejiang Dong", "Ying Shan"], "title": "Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion", "comment": "Technical Report. Project page: https://assembler3d.github.io", "summary": "We present Assembler, a scalable and generalizable framework for 3D part\nassembly that reconstructs complete objects from input part meshes and a\nreference image. Unlike prior approaches that mostly rely on deterministic part\npose prediction and category-specific training, Assembler is designed to handle\ndiverse, in-the-wild objects with varying part counts, geometries, and\nstructures. It addresses the core challenges of scaling to general 3D part\nassembly through innovations in task formulation, representation, and data.\nFirst, Assembler casts part assembly as a generative problem and employs\ndiffusion models to sample plausible configurations, effectively capturing\nambiguities arising from symmetry, repeated parts, and multiple valid\nassemblies. Second, we introduce a novel shape-centric representation based on\nsparse anchor point clouds, enabling scalable generation in Euclidean space\nrather than SE(3) pose prediction. Third, we construct a large-scale dataset of\nover 320K diverse part-object assemblies using a synthesis and filtering\npipeline built on existing 3D shape repositories. Assembler achieves\nstate-of-the-art performance on PartNet and is the first to demonstrate\nhigh-quality assembly for complex, real-world objects. Based on Assembler, we\nfurther introduce an interesting part-aware 3D modeling system that generates\nhigh-resolution, editable objects from images, demonstrating potential for\ninteractive and compositional design. Project page:\nhttps://assembler3d.github.io", "AI": {"tldr": "Assembler是一个可扩展且通用的3D零件组装框架，通过扩散模型和稀疏锚点云表示，实现了高质量、多样化的零件组装。", "motivation": "解决现有方法在多样化、真实世界对象组装中的局限性，如确定性姿态预测和类别特定训练的不足。", "method": "将零件组装视为生成问题，使用扩散模型采样配置；引入基于稀疏锚点云的形状表示；构建大规模数据集。", "result": "在PartNet上达到最先进性能，首次实现复杂真实世界对象的高质量组装。", "conclusion": "Assembler为交互式和组合式设计提供了潜力，展示了从图像生成高分辨率可编辑对象的能力。"}}
{"id": "2506.16419", "categories": ["cs.LG", "cs.AI", "68T07, 68T45"], "pdf": "https://arxiv.org/pdf/2506.16419", "abs": "https://arxiv.org/abs/2506.16419", "authors": ["Daniel Fidel Harvey", "George Weale", "Berk Yilmaz"], "title": "Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models", "comment": "All authors contributed equally. 11 pages, 6 figures", "summary": "Mixture of Experts (MoE) architectures increase large language model\nscalability, yet their performance depends on the router module that moves\ntokens to specialized experts. Bad routing can load imbalance and reduced\naccuracy. This project designed and implemented different router architectures\nwithin Transformer models to fix these limitations. We experimented with six\ndistinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),\nHybrid, Hash, and our new MLP-Hadamard. We characterized these routers using\nBERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference\nlatency, routing entropy, and expert utilization patterns. Our evaluations\nshowed distinct trade-offs: Linear routers offer speed, while MLP and Attention\nrouters provide greater expressiveness. The MLP-Hadamard router shows a unique\ncapability for structured, sparse routing. We successfully replaced and\nfine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This\nwork provides a comparative analysis of MoE router designs and offers insights\ninto optimizing their performance for efficient and effective large-scale model\ndeployment.", "AI": {"tldr": "论文研究了混合专家（MoE）架构中路由器的设计，提出了六种路由器变体，并通过实验比较了它们的性能。", "motivation": "MoE架构的可扩展性依赖于路由器模块，但不良路由会导致负载不平衡和准确性下降，因此需要优化路由器设计。", "method": "设计了六种路由器变体（Linear、Attention、MLP、Hybrid、Hash和MLP-Hadamard），并在BERT和Qwen1.5-MoE模型上进行了实验评估。", "result": "不同路由器在速度、表达能力和稀疏路由方面表现出不同的权衡，MLP-Hadamard路由器在结构化稀疏路由中表现独特。", "conclusion": "研究为MoE路由器设计提供了比较分析，并为大规模模型部署的优化提供了见解。"}}
{"id": "2506.17101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17101", "abs": "https://arxiv.org/abs/2506.17101", "authors": ["Ke Li", "Chenyu Zhang", "Yuxin Ding", "Xianbiao Hu", "Ruwen Qin"], "title": "Acquiring and Accumulating Knowledge from Diverse Datasets for Multi-label Driving Scene Classification", "comment": null, "summary": "Driving scene identification, which assigns multiple non-exclusive class\nlabels to a scene, provides the contextual awareness necessary for enhancing\nautonomous vehicles' ability to understand, reason about, and interact with the\ncomplex driving environment. As a multi-label classification problem, it is\nbetter tackled via multitasking learning. However, directly training a\nmulti-label classification model for driving scene identification through\nmultitask learning presents two main challenges: acquiring a balanced,\ncomprehensively annotated multi-label dataset and balancing learning across\ndifferent tasks. This paper introduces a novel learning system that synergizes\nknowledge acquisition and accumulation (KAA) with consistency-based active\nlearning (CAL) to address those challenges. KAA acquires and accumulates\nknowledge about scene identification from various single-label datasets via\nmonotask learning. Subsequently, CAL effectively resolves the knowledge gap\ncaused by the discrepancy between the marginal distributions of individual\nattributes and their joint distribution. An ablation study on our Driving Scene\nIdentification (DSI) dataset demonstrates a 56.1% performance increase over the\nbaseline model pretrained on ImageNet. Of this, KAA accounts for 31.3% of the\ngain, and CAL contributes 24.8%. Moreover, KAA-CAL stands out as the best\nperformer when compared to state-of-the-art (SOTA) multi-label models on two\npublic datasets, BDD100K and HSD, achieving this while using 85% less data. The\nDSI dataset and the implementation code for KAA-CAL are available at\nhttps://github.com/KELISBU/KAA-CAL .", "AI": {"tldr": "论文提出了一种结合知识获取与积累（KAA）和基于一致性的主动学习（CAL）的新方法，用于解决驾驶场景多标签分类中的数据集不平衡和任务学习平衡问题。", "motivation": "提升自动驾驶车辆对复杂驾驶环境的理解能力，解决多标签分类中数据集获取和任务平衡的挑战。", "method": "通过KAA从单标签数据集中获取知识，再通过CAL解决属性分布差异导致的知识缺口。", "result": "在DSI数据集上性能提升56.1%，KAA和CAL分别贡献31.3%和24.8%。在BDD100K和HSD数据集上表现优于SOTA模型，且数据使用量减少85%。", "conclusion": "KAA-CAL方法在多标签驾驶场景识别中表现出色，显著提升了性能和数据效率。"}}
{"id": "2506.16428", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16428", "abs": "https://arxiv.org/abs/2506.16428", "authors": ["Dian Meng", "Zhiguang Cao", "Yaoxin Wu", "Yaqing Hou", "Hongwei Ge", "Qiang Zhang"], "title": "EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems", "comment": null, "summary": "Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely\non node coordinates as input, which may be less effective in practical\nscenarios where real cost metrics-such as edge-based distances-are more\nrelevant. To address this limitation, we introduce EFormer, an Edge-based\nTransformer model that uses edge as the sole input for VRPs. Our approach\nemploys a precoder module with a mixed-score attention mechanism to convert\nedge information into temporary node embeddings. We also present a parallel\nencoding strategy characterized by a graph encoder and a node encoder, each\nresponsible for processing graph and node embeddings in distinct feature\nspaces, respectively. This design yields a more comprehensive representation of\nthe global relationships among edges. In the decoding phase, parallel context\nembedding and multi-query integration are used to compute separate attention\nmechanisms over the two encoded embeddings, facilitating efficient path\nconstruction. We train EFormer using reinforcement learning in an\nautoregressive manner. Extensive experiments on the Traveling Salesman Problem\n(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer\noutperforms established baselines on synthetic datasets, including large-scale\nand diverse distributions. Moreover, EFormer demonstrates strong generalization\non real-world instances from TSPLib and CVRPLib. These findings confirm the\neffectiveness of EFormer's core design in solving VRPs.", "AI": {"tldr": "EFormer是一种基于边的Transformer模型，用于解决车辆路径问题（VRP），通过并行编码和解码策略提升性能。", "motivation": "传统神经启发式方法依赖节点坐标，而实际场景中基于边的距离更相关，因此需要更有效的输入方式。", "method": "EFormer使用边作为唯一输入，通过预编码模块和并行编码策略生成全局关系表示，解码时采用并行上下文嵌入和多查询集成。", "result": "在TSP和CVRP上，EFormer优于基线方法，并在真实数据集上表现出强泛化能力。", "conclusion": "EFormer的设计有效解决了VRP问题，验证了其核心方法的优越性。"}}
{"id": "2506.17113", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17113", "abs": "https://arxiv.org/abs/2506.17113", "authors": ["Shoubin Yu", "Yue Zhang", "Ziyang Wang", "Jaehong Yoon", "Mohit Bansal"], "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation", "comment": "The first two authors contributed equally; Github link:\n  https://github.com/Yui010206/MEXA", "summary": "Combining pre-trained expert models offers substantial potential for scalable\nmultimodal reasoning, but building a unified framework remains challenging due\nto the increasing diversity of input modalities and task complexity. For\ninstance, medical diagnosis requires precise reasoning over structured clinical\ntables, while financial forecasting depends on interpreting plot-based data to\nmake informed predictions. To tackle this challenge, we introduce MEXA, a\ntraining-free framework that performs modality- and task-aware aggregation of\nmultiple expert models to enable effective multimodal reasoning across diverse\nand distinct domains. MEXA dynamically selects expert models based on the input\nmodality and the task-specific reasoning demands (i.e., skills). Each expert\nmodel, specialized in a modality task pair, generates interpretable textual\nreasoning outputs. MEXA then aggregates and reasons over these outputs using a\nLarge Reasoning Model (LRM) to produce the final answer. This modular design\nallows flexible and transparent multimodal reasoning across diverse domains\nwithout additional training overhead. We extensively evaluate our approach on\ndiverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D\nUnderstanding, and Medical QA. MEXA consistently delivers performance\nimprovements over strong multimodal baselines, highlighting the effectiveness\nand broad applicability of our expert-driven selection and aggregation in\ndiverse multimodal reasoning tasks.", "AI": {"tldr": "MEXA是一个无需训练的框架，通过动态选择专家模型并结合大型推理模型（LRM）实现多模态推理，适用于多样化任务。", "motivation": "解决多模态输入和任务复杂性增加时，统一框架构建的挑战。", "method": "动态选择专家模型，生成可解释的文本推理输出，并通过LRM聚合推理。", "result": "在多种多模态基准测试中表现优于基线方法。", "conclusion": "MEXA展示了专家驱动选择和聚合在多模态推理任务中的有效性和广泛适用性。"}}
{"id": "2506.16436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16436", "abs": "https://arxiv.org/abs/2506.16436", "authors": ["Antonio Giulio Coretti", "Mattia Varile", "Mario Edoardo Bertaina"], "title": "An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras", "comment": "18th International Conference on Space Operations - Safety and\n  sustainability of Space Operations (SSU)", "summary": "Space debris poses a significant threat, driving research into active and\npassive mitigation strategies. This work presents an innovative collision\navoidance system utilizing event-based cameras - a novel imaging technology\nwell-suited for Space Situational Awareness (SSA) and Space Traffic Management\n(STM). The system, employing a Stack-CNN algorithm (previously used for meteor\ndetection), analyzes real-time event-based camera data to detect faint moving\nobjects. Testing on terrestrial data demonstrates the algorithm's ability to\nenhance signal-to-noise ratio, offering a promising approach for on-board space\nimaging and improving STM/SSA operations.", "AI": {"tldr": "论文提出了一种基于事件相机的碰撞避免系统，用于空间碎片监测，采用Stack-CNN算法提升信噪比。", "motivation": "空间碎片对航天器构成威胁，需要有效的监测和避免策略。", "method": "利用事件相机和Stack-CNN算法实时分析数据，检测微弱移动目标。", "result": "地面测试显示算法能显著提升信噪比，适用于空间成像和SSA/STM任务。", "conclusion": "该系统为空间碎片监测和避免提供了创新解决方案，具有实际应用潜力。"}}
{"id": "2506.16443", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.16443", "abs": "https://arxiv.org/abs/2506.16443", "authors": ["Jonas R. Naujoks", "Aleksander Krasowski", "Moritz Weckbecker", "Galip Ümit Yolcu", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek", "René P. Klausen"], "title": "Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks", "comment": "This article was presented at \"The 3rd World Conference on\n  eXplainable Artificial Intelligence\" (2025)", "summary": "Physics-informed neural networks (PINNs) offer a powerful approach to solving\npartial differential equations (PDEs), which are ubiquitous in the quantitative\nsciences. Applied to both forward and inverse problems across various\nscientific domains, PINNs have recently emerged as a valuable tool in the field\nof scientific machine learning. A key aspect of their training is that the data\n-- spatio-temporal points sampled from the PDE's input domain -- are readily\navailable. Influence functions, a tool from the field of explainable AI (XAI),\napproximate the effect of individual training points on the model, enhancing\ninterpretability. In the present work, we explore the application of influence\nfunction-based sampling approaches for the training data. Our results indicate\nthat such targeted resampling based on data attribution methods has the\npotential to enhance prediction accuracy in physics-informed neural networks,\ndemonstrating a practical application of an XAI method in PINN training.", "AI": {"tldr": "PINNs结合XAI中的影响函数进行数据采样，提升预测精度。", "motivation": "探索如何利用XAI中的影响函数改进PINNs的训练数据采样，以增强其预测能力。", "method": "应用影响函数对训练数据进行针对性重采样。", "result": "基于数据归因方法的采样策略能有效提升PINNs的预测精度。", "conclusion": "XAI方法在PINN训练中具有实际应用价值。"}}
{"id": "2506.17134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17134", "abs": "https://arxiv.org/abs/2506.17134", "authors": ["Md Sakibur Sajal", "Marc Dandin"], "title": "Dynamic Watermark Generation for Digital Images using Perimeter Gated SPAD Imager PUFs", "comment": "5 pages, 7 figures, accepted at MWSCAS 2025 Conference", "summary": "Digital image watermarks as a security feature can be derived from the\nimager's physically unclonable functions (PUFs) by utilizing the manufacturing\nvariations, i.e., the dark signal non-uniformity (DSNU). While a few\ndemonstrations focused on the CMOS image sensors (CIS) and active pixel sensors\n(APS), single photon avalanche diode (SPAD) imagers have never been\ninvestigated for this purpose. In this work, we have proposed a novel\nwatermarking technique using perimeter gated SPAD (pgSPAD) imagers. We utilized\nthe DSNU of three 64 x 64 pgSPAD imager chips, fabricated in a 0.35 {\\mu}m\nstandard CMOS process and analyzed the simulated watermarks for standard test\nimages from publicly available database. Our observation shows that both source\nidentification and tamper detection can be achieved using the proposed\nsource-scene-specific dynamic watermarks with a controllable\nsensitivity-robustness trade-off.", "AI": {"tldr": "提出了一种基于pgSPAD成像器的数字水印技术，利用制造差异（DSNU）生成物理不可克隆功能（PUF）水印，首次在SPAD成像器中实现。", "motivation": "现有研究主要集中在CMOS和APS成像器，而SPAD成像器尚未被探索用于水印技术。", "method": "利用三个64x64 pgSPAD芯片的DSNU，在标准CMOS工艺中制造，并通过公开数据库中的测试图像分析模拟水印。", "result": "提出的动态水印技术能实现来源识别和篡改检测，并具有可控的灵敏度-鲁棒性权衡。", "conclusion": "pgSPAD成像器在数字水印领域具有潜力，能够提供高安全性和灵活性。"}}
{"id": "2506.16448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16448", "abs": "https://arxiv.org/abs/2506.16448", "authors": ["Tri Duc Ly", "Gia H. Ngo"], "title": "Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach", "comment": "29 pages, 10 figures", "summary": "EEG is a non-invasive, safe, and low-risk method to record\nelectrophysiological signals inside the brain. Especially with recent\ntechnology developments like dry electrodes, consumer-grade EEG devices, and\nrapid advances in machine learning, EEG is commonly used as a resource for\nautomatic emotion recognition. With the aim to develop a deep learning model\nthat can perform EEG-based emotion recognition in a real-life context, we\npropose a novel approach to utilize multi-scale convolutional neural networks\nto accomplish such tasks. By implementing feature extraction kernels with many\nratio coefficients as well as a new type of kernel that learns key information\nfrom four separate areas of the brain, our model consistently outperforms the\nstate-of-the-art TSception model in predicting valence, arousal, and dominance\nscores across many performance evaluation metrics.", "AI": {"tldr": "提出了一种基于多尺度卷积神经网络的新方法，用于EEG情绪识别，优于现有技术。", "motivation": "开发一种能够在真实场景中基于EEG进行情绪识别的深度学习模型。", "method": "使用多尺度卷积神经网络，结合不同比例的卷积核和一种新型核，从大脑四个区域提取关键信息。", "result": "模型在预测效价、唤醒和支配分数方面优于现有的TSception模型。", "conclusion": "该方法在EEG情绪识别任务中表现优异，具有实际应用潜力。"}}
{"id": "2506.17136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17136", "abs": "https://arxiv.org/abs/2506.17136", "authors": ["Dongdong Meng", "Sheng Li", "Hao Wu", "Guoping Wang", "Xueqing Yan"], "title": "Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations", "comment": "10 pages, 2 figures, accepted at MICCAI 2025", "summary": "Semi-supervised learning addresses the issue of limited annotations in\nmedical images effectively, but its performance is often inadequate for complex\nbackgrounds and challenging tasks. Multi-modal fusion methods can significantly\nimprove the accuracy of medical image segmentation by providing complementary\ninformation. However, they face challenges in achieving significant\nimprovements under semi-supervised conditions due to the challenge of\neffectively leveraging unlabeled data. There is a significant need to create an\neffective and reliable multi-modal learning strategy for leveraging unlabeled\ndata in semi-supervised segmentation. To address these issues, we propose a\nnovel semi-supervised multi-modal medical image segmentation approach, which\nleverages complementary multi-modal information to enhance performance with\nlimited labeled data. Our approach employs a multi-stage multi-modal fusion and\nenhancement strategy to fully utilize complementary multi-modal information,\nwhile reducing feature discrepancies and enhancing feature sharing and\nalignment. Furthermore, we effectively introduce contrastive mutual learning to\nconstrain prediction consistency across modalities, thereby facilitating the\nrobustness of segmentation results in semi-supervised tasks. Experimental\nresults on two multi-modal datasets demonstrate the superior performance and\nrobustness of the proposed framework, establishing its valuable potential for\nsolving medical image segmentation tasks in complex scenarios.", "AI": {"tldr": "提出了一种新颖的半监督多模态医学图像分割方法，通过多阶段融合和对比互学习提升性能。", "motivation": "解决半监督条件下多模态融合方法难以有效利用未标记数据的问题。", "method": "采用多阶段多模态融合和增强策略，结合对比互学习约束预测一致性。", "result": "在两个多模态数据集上验证了方法的优越性能和鲁棒性。", "conclusion": "该方法在复杂场景下具有解决医学图像分割任务的潜力。"}}
{"id": "2506.16456", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16456", "abs": "https://arxiv.org/abs/2506.16456", "authors": ["Jun Qi", "Chen-Yu Liu", "Sabato Marco Siniscalchi", "Chao-Han Huck Yang", "Min-Hsiu Hsieh"], "title": "Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation", "comment": "Preprint. Under Review", "summary": "Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient\nfine-tuning of large-scale neural models. However, standard LoRA independently\noptimizes low-rank matrices, which inherently limits its expressivity and\ngeneralization capabilities. While classical tensor-train (TT) decomposition\ncan be separately employed on individual LoRA matrices, this work demonstrates\nthat the classical TT-based approach neither significantly improves parameter\nefficiency nor achieves substantial performance gains. This paper proposes\nTensorGuide, a novel tensor-train-guided adaptation framework to overcome these\nlimitations. TensorGuide generates two correlated low-rank LoRA matrices\nthrough a unified TT structure driven by controlled Gaussian noise. The\nresulting joint TT representation inherently provides structured, low-rank\nadaptations, significantly enhancing expressivity, generalization, and\nparameter efficiency without increasing the number of trainable parameters.\nTheoretically, we justify these improvements through neural tangent kernel\nanalyses, demonstrating superior optimization dynamics and enhanced\ngeneralization. Extensive experiments on quantum dot classification and GPT-2\nfine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently\noutperforms standard LoRA and TT-LoRA, achieving improved accuracy and\nscalability with fewer parameters.", "AI": {"tldr": "TensorGuide提出了一种基于张量列分解的LoRA改进框架，通过联合优化低秩矩阵，提升了表达能力和泛化性能。", "motivation": "标准LoRA独立优化低秩矩阵，限制了其表达能力和泛化性能，而传统的TT分解方法未能显著提升性能。", "method": "TensorGuide通过统一的TT结构生成两个相关的低秩LoRA矩阵，利用高斯噪声驱动优化。", "result": "实验表明，TensorGuide在量子点分类和GPT-2微调任务中优于标准LoRA和TT-LoRA，参数效率更高。", "conclusion": "TensorGuide显著提升了LoRA的表达能力和泛化性能，同时保持了参数效率。"}}
{"id": "2506.17137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17137", "abs": "https://arxiv.org/abs/2506.17137", "authors": ["Zhuonan Liang", "Dongnan Liu", "Jianan Fan", "Yaxuan Song", "Qiang Qu", "Yu Yao", "Peng Fu", "Weidong Cai"], "title": "On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting", "comment": "18 pages, 5 figures, 8 tables", "summary": "Object counting models suffer when deployed across domains with differing\ndensity variety, since density shifts are inherently task-relevant and violate\nstandard domain adaptation assumptions. To address this, we propose a\ntheoretical framework of conditional feature alignment. We first formalize the\nnotion of conditional divergence by partitioning each domain into subsets\n(e.g., object vs. background) and measuring divergences per condition. We then\nderive a joint error bound showing that, under discrete label spaces treated as\ncondition sets, aligning distributions conditionally leads to tighter bounds on\nthe combined source-target decision error than unconditional alignment. These\ninsights motivate a general conditional adaptation principle: by preserving\ntask-relevant variations while filtering out nuisance shifts, one can achieve\nsuperior cross-domain generalization for counting. We provide both defining\nconditional divergence then proving its benefit in lowering joint error and a\npractical adaptation strategy that preserves task-relevant information in\nunsupervised domain-adaptive counting. We demonstrate the effectiveness of our\napproach through extensive experiments on multiple counting datasets with\nvarying density distributions. The results show that our method outperforms\nexisting unsupervised domain adaptation methods, empirically validating the\ntheoretical insights on conditional feature alignment.", "AI": {"tldr": "论文提出了一种条件特征对齐的理论框架，通过分区域并测量条件差异，证明了条件对齐能降低联合误差，从而提升跨域计数任务的性能。", "motivation": "解决目标计数模型在跨域部署时因密度变化导致的性能下降问题。", "method": "提出条件特征对齐框架，包括条件差异的形式化定义和基于条件对齐的适应策略。", "result": "在多个密度分布不同的计数数据集上，方法优于现有无监督域适应方法。", "conclusion": "条件特征对齐能有效提升跨域计数的泛化能力，理论和实验均验证其优势。"}}
{"id": "2506.16460", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.16460", "abs": "https://arxiv.org/abs/2506.16460", "authors": ["John Abascal", "Nicolás Berrios", "Alina Oprea", "Jonathan Ullman", "Adam Smith", "Matthew Jagielski"], "title": "Black-Box Privacy Attacks on Shared Representations in Multitask Learning", "comment": "30 pages, 8 figures", "summary": "Multitask learning (MTL) has emerged as a powerful paradigm that leverages\nsimilarities among multiple learning tasks, each with insufficient samples to\ntrain a standalone model, to solve them simultaneously while minimizing data\nsharing across users and organizations. MTL typically accomplishes this goal by\nlearning a shared representation that captures common structure among the tasks\nby embedding data from all tasks into a common feature space. Despite being\ndesigned to be the smallest unit of shared information necessary to effectively\nlearn patterns across multiple tasks, these shared representations can\ninadvertently leak sensitive information about the particular tasks they were\ntrained on.\n  In this work, we investigate what information is revealed by the shared\nrepresentations through the lens of inference attacks. Towards this, we propose\na novel, black-box task-inference threat model where the adversary, given the\nembedding vectors produced by querying the shared representation on samples\nfrom a particular task, aims to determine whether that task was present when\ntraining the shared representation. We develop efficient, purely black-box\nattacks on machine learning models that exploit the dependencies between\nembeddings from the same task without requiring shadow models or labeled\nreference data. We evaluate our attacks across vision and language domains for\nmultiple use cases of MTL and demonstrate that even with access only to fresh\ntask samples rather than training data, a black-box adversary can successfully\ninfer a task's inclusion in training. To complement our experiments, we provide\ntheoretical analysis of a simplified learning setting and show a strict\nseparation between adversaries with training samples and fresh samples from the\ntarget task's distribution.", "AI": {"tldr": "该论文研究了多任务学习（MTL）中共享表示可能泄露任务敏感信息的问题，提出了一种黑盒任务推断攻击模型，并通过实验和理论分析验证了其有效性。", "motivation": "多任务学习通过共享表示提高任务性能，但可能泄露敏感信息。作者旨在探究共享表示泄露的信息，并提出攻击模型以评估其风险。", "method": "提出了一种黑盒任务推断攻击模型，利用任务样本的嵌入向量推断任务是否参与训练，无需影子模型或标记数据。", "result": "实验证明，攻击者在仅访问任务样本（而非训练数据）的情况下，仍能成功推断任务是否参与训练。理论分析进一步支持了这一发现。", "conclusion": "共享表示在多任务学习中存在信息泄露风险，需设计更安全的机制以保护隐私。"}}
{"id": "2506.17144", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17144", "abs": "https://arxiv.org/abs/2506.17144", "authors": ["Ritabrata Chakraborty", "Rajatsubhra Chakraborty", "Avijit Dasgupta", "Sandeep Chaurasia"], "title": "Do We Need Large VLMs for Spotting Soccer Actions?", "comment": "5 pages, 2 figures", "summary": "Traditional video-based tasks like soccer action spotting rely heavily on\nvisual inputs, often requiring complex and computationally expensive models to\nprocess dense video data. In this work, we propose a shift from this\nvideo-centric approach to a text-based task, making it lightweight and scalable\nby utilizing Large Language Models (LLMs) instead of Vision-Language Models\n(VLMs). We posit that expert commentary, which provides rich, fine-grained\ndescriptions and contextual cues such as excitement and tactical insights,\ncontains enough information to reliably spot key actions in a match. To\ndemonstrate this, we use the SoccerNet Echoes dataset, which provides\ntimestamped commentary, and employ a system of three LLMs acting as judges\nspecializing in outcome, excitement, and tactics. Each LLM evaluates sliding\nwindows of commentary to identify actions like goals, cards, and substitutions,\ngenerating accurate timestamps for these events. Our experiments show that this\nlanguage-centric approach performs effectively in detecting critical match\nevents, providing a lightweight and training-free alternative to traditional\nvideo-based methods for action spotting.", "AI": {"tldr": "论文提出了一种基于文本的轻量级方法，利用大型语言模型（LLMs）替代传统的视觉语言模型（VLMs）来识别足球比赛中的关键动作。", "motivation": "传统视频处理方法复杂且计算成本高，作者希望通过利用专家评论的丰富信息，提供一种更轻量、可扩展的解决方案。", "method": "使用SoccerNet Echoes数据集中的时间戳评论，通过三个LLM分别评估结果、兴奋度和战术，识别关键动作（如进球、黄牌等）。", "result": "实验表明，这种基于语言的方法能有效检测关键比赛事件，且无需训练。", "conclusion": "该方法为动作识别提供了一种轻量级、无需训练的替代方案，优于传统视频方法。"}}
{"id": "2506.16471", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16471", "abs": "https://arxiv.org/abs/2506.16471", "authors": ["Tara Akhound-Sadegh", "Jungyoon Lee", "Avishek Joey Bose", "Valentin De Bortoli", "Arnaud Doucet", "Michael M. Bronstein", "Dominique Beaini", "Siamak Ravanbakhsh", "Kirill Neklyudov", "Alexander Tong"], "title": "Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities", "comment": null, "summary": "Sampling efficiently from a target unnormalized probability density remains a\ncore challenge, with relevance across countless high-impact scientific\napplications. A promising approach towards this challenge is the design of\namortized samplers that borrow key ideas, such as probability path design, from\nstate-of-the-art generative diffusion models. However, all existing\ndiffusion-based samplers remain unable to draw samples from distributions at\nthe scale of even simple molecular systems. In this paper, we propose\nProgressive Inference-Time Annealing (PITA), a novel framework to learn\ndiffusion-based samplers that combines two complementary interpolation\ntechniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion\nsmoothing. PITA trains a sequence of diffusion models from high to low\ntemperatures by sequentially training each model at progressively higher\ntemperatures, leveraging engineered easy access to samples of the\ntemperature-annealed target density. In the subsequent step, PITA enables\nsimulating the trained diffusion model to procure training samples at a lower\ntemperature for the next diffusion model through inference-time annealing using\na novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA\nenables, for the first time, equilibrium sampling of N-body particle systems,\nAlanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically\nlower energy function evaluations. Code available at:\nhttps://github.com/taraak/pita", "AI": {"tldr": "PITA提出了一种新的扩散采样框架，结合了Boltzmann分布的退火和扩散平滑技术，首次实现了N体粒子系统、Alanine Dipeptide和三肽的平衡采样。", "motivation": "高效采样未归一化概率密度是核心挑战，现有扩散采样器无法处理简单分子系统的规模。", "method": "PITA通过逐步训练扩散模型，结合Boltzmann分布退火和扩散平滑，利用Feynman-Kac PDE和Sequential Monte Carlo进行推理时间退火。", "result": "PITA首次实现了N体粒子系统、Alanine Dipeptide和三肽的平衡采样，显著减少了能量函数评估次数。", "conclusion": "PITA为扩散采样提供了一种高效的新方法，适用于复杂分子系统的采样。"}}
{"id": "2506.17159", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17159", "abs": "https://arxiv.org/abs/2506.17159", "authors": ["Qing Xu", "Yuxiang Luo", "Wenting Duan", "Zhen Chen"], "title": "Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation", "comment": "Under Review", "summary": "Medical image analysis is critical yet challenged by the need of jointly\nsegmenting organs or tissues, and numerous instances for anatomical structures\nand tumor microenvironment analysis. Existing studies typically formulated\ndifferent segmentation tasks in isolation, which overlooks the fundamental\ninterdependencies between these tasks, leading to suboptimal segmentation\nperformance and insufficient medical image understanding. To address this\nissue, we propose a Co-Seg++ framework for versatile medical segmentation.\nSpecifically, we introduce a novel co-segmentation paradigm, allowing semantic\nand instance segmentation tasks to mutually enhance each other. We first devise\na spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial\nand temporal relationships between segmentation regions and image embeddings as\nprior spatial constraints. Moreover, we devise a multi-task collaborative\ndecoder (MTC-Decoder) that leverages cross-guidance to strengthen the\ncontextual consistency of both tasks, jointly computing semantic and instance\nsegmentation masks. Extensive experiments on diverse CT and histopathology\ndatasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts\nin the semantic, instance, and panoptic segmentation of dental anatomical\nstructures, histopathology tissues, and nuclei instances. The source code is\navailable at https://github.com/xq141839/Co-Seg-Plus.", "AI": {"tldr": "Co-Seg++框架通过联合语义和实例分割任务，提升医学图像分割性能。", "motivation": "现有研究通常孤立处理不同分割任务，忽视了任务间的相互依赖性，导致分割性能不佳。", "method": "提出STP-Encoder捕获空间和时间关系，MTC-Decoder通过跨任务指导增强上下文一致性。", "result": "在多种CT和组织病理学数据集上，Co-Seg++在语义、实例和全景分割中表现优于现有方法。", "conclusion": "Co-Seg++通过任务协同显著提升了医学图像分割的准确性和理解能力。"}}
{"id": "2506.16494", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.16494", "abs": "https://arxiv.org/abs/2506.16494", "authors": ["Amir Reza Vazifeh", "Jason W. Fleischer"], "title": "Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias", "comment": null, "summary": "Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart\nactivity and are well-established tools for detecting and monitoring\ncardiovascular disease. However, manual ECG analysis can be time-consuming and\nprone to errors. Machine learning has emerged as a promising approach for\nautomated heartbeat recognition and classification, but substantial variations\nin ECG signals make it challenging to develop generalizable models. ECG signals\ncan vary widely across individuals and leads, while datasets often follow\ndifferent labeling standards and may be biased, all of which greatly hinder\nsupervised methods. Conventional unsupervised methods, e.g. principal component\nanalysis, prioritize large (and often obvious) variances in the data and\ntypically overlook subtle yet clinically relevant patterns. If labels are\nmissing and/or variations are significant but small, both approaches fail.\nHere, we show that nonlinear dimensionality reduction (NLDR) can accommodate\nthese issues and identify medically relevant features in ECG signals, with no\nneed for training or prior information. Using the MLII and V1 leads of the\nMIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor\nembedding and uniform manifold approximation and projection can discriminate\nindividual recordings in mixed populations with >= 90% accuracy and distinguish\ndifferent arrhythmias in individual patients with a median accuracy of 98.96%\nand a median F1-score of 91.02%. The results show that NLDR holds much promise\nfor cardiac monitoring, including the limiting cases of single-lead ECG and the\ncurrent 12-lead standard of care, and for personalized health care beyond\ncardiology.", "AI": {"tldr": "论文提出非线性降维（NLDR）方法用于心电图（ECG）信号分析，解决了传统监督和无监督方法的局限性，展示了高准确率的心律失常分类能力。", "motivation": "ECG信号分析存在个体差异、标签标准不一致等问题，传统方法难以处理。NLDR无需训练或先验信息，能识别临床相关特征。", "method": "使用非线性降维技术（如t-SNE和UMAP）分析MIT-BIH数据集中的MLII和V1导联信号，区分不同心律失常。", "result": "NLDR在混合人群中区分个体记录的准确率≥90%，对心律失常分类的中位准确率为98.96%，F1分数为91.02%。", "conclusion": "NLDR在心脏监测和个性化医疗中具有潜力，适用于单导联和12导联ECG。"}}
{"id": "2506.17186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17186", "abs": "https://arxiv.org/abs/2506.17186", "authors": ["Ketil Malde"], "title": "YASMOT: Yet another stereo image multi-object tracker", "comment": "5 pages", "summary": "There now exists many popular object detectors based on deep learning that\ncan analyze images and extract locations and class labels for occurrences of\nobjects. For image time series (i.e., video or sequences of stills), tracking\nobjects over time and preserving object identity can help to improve object\ndetection performance, and is necessary for many downstream tasks, including\nclassifying and predicting behaviors, and estimating total abundances. Here we\npresent yasmot, a lightweight and flexible object tracker that can process the\noutput from popular object detectors and track objects over time from either\nmonoscopic or stereoscopic camera configurations. In addition, it includes\nfunctionality to generate consensus detections from ensembles of object\ndetectors.", "AI": {"tldr": "Yasmot是一个轻量级、灵活的对象跟踪器，用于处理从单目或立体相机配置中提取的对象检测结果，并支持生成多检测器的共识检测。", "motivation": "在图像时间序列中，跟踪对象并保持其身份有助于提升检测性能，并为下游任务（如行为分类和丰度估计）提供支持。", "method": "Yasmot处理流行对象检测器的输出，支持单目或立体相机配置，并能生成多检测器的共识检测。", "result": "Yasmot实现了对对象的持续跟踪，并提供了灵活的配置选项。", "conclusion": "Yasmot是一个高效的工具，适用于需要对象跟踪和身份保持的应用场景。"}}
{"id": "2506.16500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16500", "abs": "https://arxiv.org/abs/2506.16500", "authors": ["Samir Khaki", "Xiuyu Li", "Junxian Guo", "Ligeng Zhu", "Chenfeng Xu", "Konstantinos N. Plataniotis", "Amir Yazdanbakhsh", "Kurt Keutzer", "Song Han", "Zhijian Liu"], "title": "SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity", "comment": "ICML 2025. The first three authors contributed equally to this work.\n  Project page: https://z-lab.ai/projects/sparselora", "summary": "Fine-tuning LLMs is both computationally and memory-intensive. While\nparameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the\nnumber of trainable parameters and lower memory usage, they do not decrease\ncomputational cost. In some cases, they may even slow down fine-tuning. In this\npaper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning\nthrough contextual sparsity. We propose a lightweight, training-free SVD\nsparsity estimator that dynamically selects a sparse subset of weights for loss\nand gradient computation. Also, we systematically analyze and address\nsensitivity across layers, tokens, and training steps. Our experimental results\nshow that SparseLoRA reduces computational cost by up to 2.2 times and a\nmeasured speedup of up to 1.6 times while maintaining accuracy across various\ndownstream tasks, including commonsense and arithmetic reasoning, code\ngeneration, and instruction following.", "AI": {"tldr": "SparseLoRA是一种通过上下文稀疏性加速LLM微调的方法，减少计算成本并保持准确性。", "motivation": "现有参数高效微调方法（如QLoRA和DoRA）虽减少内存使用，但未降低计算成本，甚至可能减慢微调速度。", "method": "提出轻量级、无需训练的SVD稀疏性估计器，动态选择权重稀疏子集进行损失和梯度计算，并系统分析层、标记和训练步骤的敏感性。", "result": "实验显示SparseLoRA将计算成本降低至多2.2倍，实测加速1.6倍，同时在多种下游任务中保持准确性。", "conclusion": "SparseLoRA在减少计算成本的同时，不影响模型性能，为LLM微调提供了高效解决方案。"}}
{"id": "2506.17191", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17191", "abs": "https://arxiv.org/abs/2506.17191", "authors": ["Israel Juárez-Jiménez", "Tiffany Guadalupe Martínez Paredes", "Jesús García-Ramírez", "Eric Ramos Aguilar"], "title": "Facial Landmark Visualization and Emotion Recognition Through Neural Networks", "comment": "Best paper Award COMIA 2025", "summary": "Emotion recognition from facial images is a crucial task in human-computer\ninteraction, enabling machines to learn human emotions through facial\nexpressions. Previous studies have shown that facial images can be used to\ntrain deep learning models; however, most of these studies do not include a\nthrough dataset analysis. Visualizing facial landmarks can be challenging when\nextracting meaningful dataset insights; to address this issue, we propose\nfacial landmark box plots, a visualization technique designed to identify\noutliers in facial datasets. Additionally, we compare two sets of facial\nlandmark features: (i) the landmarks' absolute positions and (ii) their\ndisplacements from a neutral expression to the peak of an emotional expression.\nOur results indicate that a neural network achieves better performance than a\nrandom forest classifier.", "AI": {"tldr": "论文提出了一种面部标志箱线图可视化技术，用于识别面部数据集中的异常值，并比较了两种面部标志特征。神经网络表现优于随机森林分类器。", "motivation": "面部图像情感识别在人机交互中至关重要，但现有研究缺乏全面的数据集分析，且可视化面部标志提取有意义的数据洞察具有挑战性。", "method": "提出面部标志箱线图可视化技术，比较两种面部标志特征：绝对位置和从中性表情到情绪峰值的位移。", "result": "神经网络在性能上优于随机森林分类器。", "conclusion": "面部标志箱线图技术有助于数据集分析，神经网络是更优的情感识别模型。"}}
{"id": "2506.16506", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.16506", "abs": "https://arxiv.org/abs/2506.16506", "authors": ["Ronald Skorobogat", "Karsten Roth", "Mariana-Iuliana Georgescu", "Zeynep Akata"], "title": "Subspace-Boosted Model Merging", "comment": "21 pages (main + supp)", "summary": "Model merging enables the combination of multiple specialized expert models\ninto a single model capable of performing multiple tasks. However, the benefits\nof merging an increasing amount of specialized experts generally lead to\ndiminishing returns and reduced overall performance gains. In this work, we\noffer an explanation and analysis from a task arithmetic perspective; revealing\nthat as the merging process (across numerous existing merging methods)\ncontinues for more and more experts, the associated task vector space\nexperiences rank collapse. To mitigate this issue, we introduce Subspace\nBoosting, which operates on the singular value decomposed task vector space and\nmaintains task vector ranks. Subspace Boosting raises merging efficacy for up\nto 20 expert models by large margins of more than 10% when evaluated on vision\nbenchmarks. Moreover, we propose employing Higher-Order Generalized Singular\nValue Decomposition to further quantify task similarity, offering a new\ninterpretable perspective on model merging.", "AI": {"tldr": "论文提出了一种名为Subspace Boosting的方法，通过维持任务向量空间的秩来缓解模型合并中的性能下降问题，显著提升了合并效果。", "motivation": "随着合并的专家模型数量增加，性能提升逐渐减弱，任务向量空间出现秩崩溃现象。", "method": "采用奇异值分解的任务向量空间，提出Subspace Boosting方法维持秩，并引入高阶广义奇异值分解量化任务相似性。", "result": "Subspace Boosting在视觉基准测试中显著提升合并效果，性能提升超过10%。", "conclusion": "该方法不仅解决了秩崩溃问题，还为模型合并提供了新的可解释视角。"}}
{"id": "2506.17201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17201", "abs": "https://arxiv.org/abs/2506.17201", "authors": ["Jiaqi Li", "Junshu Tang", "Zhiyong Xu", "Longhuang Wu", "Yuan Zhou", "Shuai Shao", "Tianbao Yu", "Zhiguo Cao", "Qinglin Lu"], "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition", "comment": "Project page: https://hunyuan-gamecraft.github.io/", "summary": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.", "AI": {"tldr": "Hunyuan-GameCraft是一个用于游戏环境中高动态交互视频生成的新框架，解决了现有方法在动态性、通用性、长期一致性和效率上的不足。", "motivation": "当前基于扩散和可控视频生成的方法在动态性、通用性、长期一致性和效率上存在局限，影响了多样化游戏视频的生成能力。", "method": "通过统一键盘和鼠标输入到共享相机表示空间，提出混合历史条件训练策略，并采用模型蒸馏提升推理效率和可玩性。", "result": "在大规模数据集上训练和微调后，Hunyuan-GameCraft在视觉保真度、真实感和动作可控性上显著优于现有模型。", "conclusion": "Hunyuan-GameCraft提升了交互式游戏视频生成的现实感和可玩性，适合复杂交互环境的实时部署。"}}
{"id": "2506.16507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16507", "abs": "https://arxiv.org/abs/2506.16507", "authors": ["Pragya Srivastava", "Harman Singh", "Rahul Madhavan", "Gandharv Patil", "Sravanti Addepalli", "Arun Suggala", "Rengarajan Aravamudhan", "Soumya Sharma", "Anirban Laha", "Aravindan Raghuveer", "Karthikeyan Shanmugam", "Doina Precup"], "title": "Robust Reward Modeling via Causal Rubrics", "comment": null, "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k.", "AI": {"tldr": "Crome框架通过因果增强和中性增强解决奖励模型中的奖励黑客问题，显著提升性能。", "motivation": "奖励模型（RMs）常因奖励黑客问题而失效，即模型错误地将表面特征（如回答长度）与质量关联。", "method": "Crome通过因果增强（针对因果属性）和中性增强（针对虚假属性）训练奖励模型，无需虚假因素知识。", "result": "Crome在RewardBench上平均准确率提升5.4%，特定类别提升达13.2%和7.2%。", "conclusion": "Crome显著提升了奖励模型的鲁棒性和性能，适用于多种任务和基准。"}}
{"id": "2506.17202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17202", "abs": "https://arxiv.org/abs/2506.17202", "authors": ["Teng Li", "Quanfeng Lu", "Lirui Zhao", "Hao Li", "Xizhou Zhu", "Yu Qiao", "Jun Zhang", "Wenqi Shao"], "title": "UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation", "comment": "Code: https://github.com/tliby/UniFork", "summary": "Unified image understanding and generation has emerged as a promising\nparadigm in multimodal artificial intelligence. Despite recent progress, the\noptimal architectural design for such unified models remains an open challenge.\nIn this work, we start by analyzing the modality alignment behaviors of\ntask-specific expert models for understanding and generation, as well as\ncurrent unified models. Our analysis reveals a crucial observation:\nunderstanding tasks benefit from a progressively increasing modality alignment\nacross network depth, which helps build up semantic information for better\ncomprehension; In contrast, generation tasks follow a different trend: modality\nalignment increases in the early layers but decreases in the deep layers to\nrecover spatial details. These divergent alignment patterns create a\nfundamental conflict in fully shared Transformer backbones, where a uniform\nrepresentational flow often leads to performance compromises across two tasks.\nMotivated by this finding, we introduce UniFork, a novel Y-shaped architecture\nthat shares the shallow layers for cross-task representation learning, while\nemploying task-specific branches in deeper layers to avoid task interference.\nThis design effectively balances shared learning and task specialization.\nThrough extensive ablation experiments, we demonstrate that Unifork\nconsistently outperforms conventional fully shared Transformer architectures,\nand achieves performance on par with or better than task-specific models.", "AI": {"tldr": "UniFork提出了一种Y形架构，通过共享浅层和任务特定的深层来解决图像理解与生成任务中模态对齐的冲突，性能优于传统共享架构。", "motivation": "研究统一图像理解与生成模型的最优架构设计，发现任务间模态对齐模式存在冲突。", "method": "分析任务专家模型和现有统一模型的模态对齐行为，提出Y形架构UniFork，共享浅层并分离深层。", "result": "UniFork性能优于传统共享架构，与任务特定模型相当或更好。", "conclusion": "UniFork通过平衡共享学习和任务专业化，解决了统一模型中的冲突问题。"}}
{"id": "2506.16528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16528", "abs": "https://arxiv.org/abs/2506.16528", "authors": ["Bornali Phukon", "Xiuwen Zheng", "Mark Hasegawa-Johnson"], "title": "Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches", "comment": "5 pages, 2 figures, Interspeech 2025", "summary": "Traditional ASR metrics like WER and CER fail to capture intelligibility,\nespecially for dysarthric and dysphonic speech, where semantic alignment\nmatters more than exact word matches. ASR systems struggle with these speech\ntypes, often producing errors like phoneme repetitions and imprecise\nconsonants, yet the meaning remains clear to human listeners. We identify two\nkey challenges: (1) Existing metrics do not adequately reflect intelligibility,\nand (2) while LLMs can refine ASR output, their effectiveness in correcting ASR\ntranscripts of dysarthric speech remains underexplored. To address this, we\npropose a novel metric integrating Natural Language Inference (NLI) scores,\nsemantic similarity, and phonetic similarity. Our ASR evaluation metric\nachieves a 0.890 correlation with human judgments on Speech Accessibility\nProject data, surpassing traditional methods and emphasizing the need to\nprioritize intelligibility over error-based measures.", "AI": {"tldr": "论文提出了一种新指标，结合NLI分数、语义相似性和语音相似性，用于评估ASR系统在发音障碍语音中的可懂度，优于传统方法。", "motivation": "传统ASR指标（如WER和CER）无法准确反映发音障碍语音的可懂度，而ASR系统在处理此类语音时表现不佳。", "method": "提出了一种新指标，整合NLI分数、语义相似性和语音相似性。", "result": "新指标在Speech Accessibility Project数据上与人类判断的相关性达到0.890，优于传统方法。", "conclusion": "强调应优先考虑可懂度而非基于错误的指标，新指标在发音障碍语音评估中表现优异。"}}
{"id": "2506.16548", "categories": ["cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.16548", "abs": "https://arxiv.org/abs/2506.16548", "authors": ["Arjun Dosajh", "Mihika Sanghi"], "title": "Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU", "comment": "7 pages, 2 figures, to be published in SemEval-2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation. However, their tendency to\nmemorize training data raises concerns regarding privacy, copyright compliance,\nand security, particularly in cases involving Personally Identifiable\nInformation (PII). Effective machine unlearning techniques are essential to\nmitigate these risks, yet existing methods remain underdeveloped for LLMs due\nto their open-ended output space. In this work, we apply the Adaptive\nRepresentation Misdirection Unlearning (RMU) technique to unlearn sensitive\ninformation from LLMs. Through extensive experiments, we analyze the effects of\nunlearning across different decoder layers to determine the most effective\nregions for sensitive information removal. Our technique ranked 4th on the\nofficial leaderboard of both 1B parameter and 7B parameter models.", "AI": {"tldr": "论文提出了一种名为自适应表示误导遗忘（RMU）的技术，用于从大语言模型（LLMs）中删除敏感信息，并在实验中验证了其有效性。", "motivation": "大语言模型（LLMs）在自然语言理解和生成方面表现出色，但其对训练数据的记忆可能引发隐私、版权和安全问题，尤其是涉及个人身份信息（PII）时。", "method": "采用自适应表示误导遗忘（RMU）技术，通过分析不同解码层的影响，确定最有效的敏感信息删除区域。", "result": "该技术在1B和7B参数模型的官方排行榜上均排名第四。", "conclusion": "RMU技术为LLMs中的敏感信息遗忘提供了一种有效解决方案，但仍需进一步研究。"}}
{"id": "2506.16550", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16550", "abs": "https://arxiv.org/abs/2506.16550", "authors": ["Swagatam Das"], "title": "A Free Probabilistic Framework for Analyzing the Transformer-based Language Models", "comment": null, "summary": "We outline an operator-theoretic framework for analyzing transformer-based\nlanguage models using the tools of free probability theory. By representing\ntoken embeddings and attention mechanisms as self-adjoint operators in a racial\nprobability space, we reinterpret attention as a non-commutative convolution\nand view the layer-wise propagation of representations as an evolution governed\nby free additive convolution. This formalism reveals a spectral dynamical\nsystem underpinning deep transformer stacks and offers insight into their\ninductive biases, generalization behavior, and entropy dynamics. We derive a\ngeneralization bound based on free entropy and demonstrate that the spectral\ntrace of transformer layers evolves predictably with depth. Our approach\nbridges neural architecture with non-commutative harmonic analysis, enabling\nprincipled analysis of information flow and structural complexity in large\nlanguage models", "AI": {"tldr": "该论文提出了一种基于自由概率理论的算子理论框架，用于分析基于Transformer的语言模型，揭示了其谱动力学系统并提供了对归纳偏置、泛化行为和熵动态的见解。", "motivation": "通过将Transformer模型中的token嵌入和注意力机制表示为自伴算子，重新解释注意力机制为非交换卷积，从而更深入地理解模型的信息流和结构复杂性。", "method": "使用自由概率理论中的工具，将注意力机制和token嵌入建模为自伴算子，并通过自由加性卷积分析层间表示的传播。", "result": "提出了基于自由熵的泛化边界，并证明Transformer层的谱迹随深度可预测地演化。", "conclusion": "该框架将神经架构与非交换调和分析联系起来，为大型语言模型的信息流和结构复杂性提供了理论分析工具。"}}
{"id": "2506.17218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17218", "abs": "https://arxiv.org/abs/2506.17218", "authors": ["Zeyuan Yang", "Xueyang Yu", "Delin Chen", "Maohao Shen", "Chuang Gan"], "title": "Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens", "comment": "Project page: https://vlm-mirage.github.io/", "summary": "Vision-language models (VLMs) excel at multimodal understanding, yet their\ntext-only decoding forces them to verbalize visual reasoning, limiting\nperformance on tasks that demand visual imagination. Recent attempts train VLMs\nto render explicit images, but the heavy image-generation pre-training often\nhinders the reasoning ability. Inspired by the way humans reason with mental\nimagery-the internal construction and manipulation of visual cues-we\ninvestigate whether VLMs can reason through interleaved multimodal trajectories\nwithout producing explicit images. To this end, we present a Machine Mental\nImagery framework, dubbed as Mirage, which augments VLM decoding with latent\nvisual tokens alongside ordinary text. Concretely, whenever the model chooses\nto ``think visually'', it recasts its hidden states as next tokens, thereby\ncontinuing a multimodal trajectory without generating pixel-level images. Begin\nby supervising the latent tokens through distillation from ground-truth image\nembeddings, we then switch to text-only supervision to make the latent\ntrajectory align tightly with the task objective. A subsequent reinforcement\nlearning stage further enhances the multimodal reasoning capability.\nExperiments on diverse benchmarks demonstrate that Mirage unlocks stronger\nmultimodal reasoning without explicit image generation.", "AI": {"tldr": "Mirage框架通过引入潜在视觉标记增强视觉语言模型的解码能力，避免显式图像生成，从而提升多模态推理性能。", "motivation": "现有视觉语言模型在需要视觉想象的任务中表现受限，因为它们必须将视觉推理转化为文本。显式图像生成又可能损害推理能力。", "method": "提出Mirage框架，通过潜在视觉标记和文本交替解码，监督学习阶段从图像嵌入蒸馏开始，随后转向文本监督，最后通过强化学习增强推理能力。", "result": "实验表明，Mirage在不生成显式图像的情况下，显著提升了多模态推理能力。", "conclusion": "Mirage为视觉语言模型提供了一种更高效的多模态推理方法，避免了显式图像生成的负担。"}}
{"id": "2506.16553", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16553", "abs": "https://arxiv.org/abs/2506.16553", "authors": ["Soroush H. Zargarbashi", "Mohammad Sadegh Akhondzadeh", "Aleksandar Bojchevski"], "title": "One Sample is Enough to Make Conformal Prediction Robust", "comment": null, "summary": "Given any model, conformal prediction (CP) returns prediction sets guaranteed\nto include the true label with high adjustable probability. Robust CP (RCP)\nextends this to inputs with worst-case noise. A well-established approach is to\nuse randomized smoothing for RCP since it is applicable to any black-box model\nand provides smaller sets compared to deterministic methods. However, current\nsmoothing-based RCP requires many model forward passes per each input which is\ncomputationally expensive. We show that conformal prediction attains some\nrobustness even with a forward pass on a single randomly perturbed input. Using\nany binary certificate we propose a single sample robust CP (RCP1). Our\napproach returns robust sets with smaller average set size compared to SOTA\nmethods which use many (e.g. around 100) passes per input. Our key insight is\nto certify the conformal prediction procedure itself rather than individual\nscores. Our approach is agnostic to the setup (classification and regression).\nWe further extend our approach to smoothing-based robust conformal risk\ncontrol.", "AI": {"tldr": "本文提出了一种单样本鲁棒共形预测方法（RCP1），通过单次随机扰动输入实现鲁棒性，减少了计算开销，同时保持了较小的预测集大小。", "motivation": "传统基于平滑的鲁棒共形预测（RCP）需要多次模型前向传播，计算成本高。本文旨在通过单次扰动输入实现鲁棒性，降低计算负担。", "method": "利用任何二元证书，提出单样本鲁棒共形预测（RCP1），通过认证共形预测过程而非单个分数来实现鲁棒性。", "result": "RCP1在保持较小平均集大小的同时，显著减少了计算开销（如从100次前向传播减少到1次）。", "conclusion": "RCP1是一种高效且通用的鲁棒共形预测方法，适用于分类和回归任务，并可扩展到鲁棒共形风险控制。"}}
{"id": "2506.17220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17220", "abs": "https://arxiv.org/abs/2506.17220", "authors": ["Jisu Nam", "Soowon Son", "Dahyun Chung", "Jiyoung Kim", "Siyoon Jin", "Junhwa Hur", "Seungryong Kim"], "title": "Emergent Temporal Correspondences from Video Diffusion Transformers", "comment": "Project page is available at https:/cvlab-kaist.github.io/DiffTrack", "summary": "Recent advancements in video diffusion models based on Diffusion Transformers\n(DiTs) have achieved remarkable success in generating temporally coherent\nvideos. Yet, a fundamental question persists: how do these models internally\nestablish and represent temporal correspondences across frames? We introduce\nDiffTrack, the first quantitative analysis framework designed to answer this\nquestion. DiffTrack constructs a dataset of prompt-generated video with pseudo\nground-truth tracking annotations and proposes novel evaluation metrics to\nsystematically analyze how each component within the full 3D attention\nmechanism of DiTs (e.g., representations, layers, and timesteps) contributes to\nestablishing temporal correspondences. Our analysis reveals that query-key\nsimilarities in specific, but not all, layers play a critical role in temporal\nmatching, and that this matching becomes increasingly prominent during the\ndenoising process. We demonstrate practical applications of DiffTrack in\nzero-shot point tracking, where it achieves state-of-the-art performance\ncompared to existing vision foundation and self-supervised video models.\nFurther, we extend our findings to motion-enhanced video generation with a\nnovel guidance method that improves temporal consistency of generated videos\nwithout additional training. We believe our work offers crucial insights into\nthe inner workings of video DiTs and establishes a foundation for further\nresearch and applications leveraging their temporal understanding.", "AI": {"tldr": "DiffTrack是一个定量分析框架，用于研究视频扩散模型（DiTs）如何建立帧间时间对应关系，揭示了特定层在时间匹配中的关键作用，并展示了在零样本点跟踪和视频生成中的实际应用。", "motivation": "研究视频扩散模型内部如何建立和表示帧间时间对应关系，填补了这一领域的空白。", "method": "通过构建带有伪真实跟踪注释的数据集，并提出新的评估指标，系统分析了DiTs的3D注意力机制中各组件的作用。", "result": "发现特定层的查询-键相似性在时间匹配中起关键作用，且在去噪过程中逐渐显著；DiffTrack在零样本点跟踪中表现优异，并能改进视频生成的时间一致性。", "conclusion": "DiffTrack为理解视频DiTs的内部机制提供了重要见解，并为未来研究和应用奠定了基础。"}}
{"id": "2506.16590", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16590", "abs": "https://arxiv.org/abs/2506.16590", "authors": ["Zeyun Deng", "Jasorsi Ghosh", "Fiona Xie", "Yuzhe Lu", "Katia Sycara", "Joseph Campbell"], "title": "Energy-Based Transfer for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning algorithms often suffer from poor sample efficiency,\nmaking them challenging to apply in multi-task or continual learning settings.\nEfficiency can be improved by transferring knowledge from a previously trained\nteacher policy to guide exploration in new but related tasks. However, if the\nnew task sufficiently differs from the teacher's training task, the transferred\nguidance may be sub-optimal and bias exploration toward low-reward behaviors.\nWe propose an energy-based transfer learning method that uses\nout-of-distribution detection to selectively issue guidance, enabling the\nteacher to intervene only in states within its training distribution. We\ntheoretically show that energy scores reflect the teacher's state-visitation\ndensity and empirically demonstrate improved sample efficiency and performance\nacross both single-task and multi-task settings.", "AI": {"tldr": "提出了一种基于能量的迁移学习方法，通过分布外检测选择性提供指导，提高强化学习在多任务或持续学习中的样本效率。", "motivation": "强化学习算法在多任务或持续学习中样本效率低，迁移知识可能因任务差异导致次优指导。", "method": "使用基于能量的方法，结合分布外检测，选择性提供教师策略的指导。", "result": "理论证明能量分数反映教师的状态访问密度，实验显示在单任务和多任务中均提高了样本效率和性能。", "conclusion": "该方法有效解决了迁移学习中次优指导的问题，提升了强化学习的适用性。"}}
{"id": "2506.17221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.17221", "abs": "https://arxiv.org/abs/2506.17221", "authors": ["Zhangyang Qi", "Zhixiong Zhang", "Yizhou Yu", "Jiaqi Wang", "Hengshuang Zhao"], "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning", "comment": "project page: www.vlnr1.github.io", "summary": "Vision-Language Navigation (VLN) is a core challenge in embodied AI,\nrequiring agents to navigate real-world environments using natural language\ninstructions. Current language model-based navigation systems operate on\ndiscrete topological graphs, limiting path planning to predefined node\nconnections. We propose VLN-R1, an end-to-end framework that leverages Large\nVision-Language Models (LVLM) to directly translate egocentric video streams\ninto continuous navigation actions, adopting GRPO-based training inspired by\nDeepSeek-R1. To enable effective training, we first construct the VLN-Ego\ndataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling\nto balance historical and current observations. While large language models can\nsupervise complete textual instructions, they lack fine-grained action-level\ncontrol. Our framework employs a two-stage training approach: a) Supervised\nfine-tuning (SFT) to align the model's action sequence text predictions with\nexpert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced\nwith a Time-Decayed Reward (TDR) mechanism that strategically weights\nmulti-step future actions. Experimental results show VLN-R1 achieves strong\nperformance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied\nnavigation and enhance task-specific reasoning through data-efficient,\nreward-driven post-training.", "AI": {"tldr": "VLN-R1是一个端到端的框架，利用大型视觉语言模型（LVLM）将第一人称视频流直接转换为连续导航动作，通过GRPO训练和两阶段微调（SFT和RFT）提升性能。", "motivation": "当前基于语言模型的导航系统依赖于离散拓扑图，限制了路径规划的灵活性。VLN-R1旨在通过LVLM实现更直接的连续导航动作生成。", "method": "1. 构建VLN-Ego数据集；2. 采用Long-Short Memory Sampling平衡历史与当前观察；3. 两阶段训练：SFT对齐专家演示，RFT结合Time-Decayed Reward机制。", "result": "VLN-R1在VLN-CE基准测试中表现优异，证明LVLM能高效驱动具身导航。", "conclusion": "VLN-R1展示了LVLM在具身导航中的潜力，并通过数据高效、奖励驱动的后训练增强任务特定推理。"}}
{"id": "2506.16600", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16600", "abs": "https://arxiv.org/abs/2506.16600", "authors": ["Khiem Le", "Tuan Tran", "Ting Hua", "Nitesh V. Chawla"], "title": "FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE", "comment": null, "summary": "Existing resource-adaptive LoRA federated fine-tuning methods enable clients\nto fine-tune models using compressed versions of global LoRA matrices, in order\nto accommodate various compute resources across clients. This compression\nrequirement will lead to suboptimal performance due to information loss. To\naddress this, we propose FLAME, a novel federated learning framework based on\nthe Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,\nFLAME retains full (uncompressed) global LoRA matrices and achieves client-side\nadaptability by varying the number of activated experts per client. However,\nincorporating SMoE into federated learning introduces unique challenges,\nspecifically, the mismatch in output magnitude from partial expert activation\nand the imbalance in expert training quality across clients. FLAME tackles\nthese challenges through a lightweight rescaling mechanism and an\nactivation-aware aggregation scheme. Empirical results across diverse\ncomputational settings demonstrate that FLAME consistently outperforms existing\nmethods, providing a robust and effective solution for resource-adaptive\nfederated learning.", "AI": {"tldr": "FLAME是一种基于稀疏混合专家架构的新型联邦学习框架，通过动态激活专家数量实现客户端适应性，解决了现有方法因压缩导致性能下降的问题。", "motivation": "现有基于LoRA的联邦学习方法因压缩全局矩阵导致信息丢失和性能下降，需要一种更高效的方法。", "method": "FLAME采用稀疏混合专家架构，保留完整全局LoRA矩阵，通过动态激活专家数量和轻量级重缩放机制解决输出幅度不匹配问题。", "result": "实验表明，FLAME在不同计算环境下均优于现有方法，提供了稳健的资源自适应联邦学习方案。", "conclusion": "FLAME通过创新架构和机制，有效解决了联邦学习中的资源适应性问题，显著提升了性能。"}}
{"id": "2506.16602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16602", "abs": "https://arxiv.org/abs/2506.16602", "authors": ["Siddharth Viswanath", "Rahul Singh", "Yanlei Zhang", "J. Adam Noah", "Joy Hirsch", "Smita Krishnaswamy"], "title": "SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics", "comment": null, "summary": "Graph neural networks have been useful in machine learning on\ngraph-structured data, particularly for node classification and some types of\ngraph classification tasks. However, they have had limited use in representing\npatterning of signals over graphs. Patterning of signals over graphs and in\nsubgraphs carries important information in many domains including neuroscience.\nNeural signals are spatiotemporally patterned, high dimensional and difficult\nto decode. Graph signal processing and associated GCN models utilize the graph\nFourier transform and are unable to efficiently represent spatially or\nspectrally localized signal patterning on graphs. Wavelet transforms have shown\npromise here, but offer non-canonical representations and cannot be tightly\nconfined to subgraphs. Here we propose SlepNet, a novel GCN architecture that\nuses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian\nharmonics optimally concentrate signal energy on specifically relevant\nsubgraphs that are automatically learned with a mask. Thus, they can produce\ncanonical and highly resolved representations of neural activity, focusing\nenergy of harmonics on areas of the brain which are activated. We evaluated\nSlepNet across three fMRI datasets, spanning cognitive and visual tasks, and\ntwo traffic dynamics datasets, comparing its performance against conventional\nGNNs and graph signal processing constructs. SlepNet outperforms the baselines\nin all datasets. Moreover, the extracted representations of signal patterns\nfrom SlepNet offers more resolution in distinguishing between similar patterns,\nand thus represent brain signaling transients as informative trajectories. Here\nwe have shown that these extracted trajectory representations can be used for\nother downstream untrained tasks. Thus we establish that SlepNet is useful both\nfor prediction and representation learning in spatiotemporal data.", "AI": {"tldr": "SlepNet是一种新型图卷积网络架构，利用Slepian基代替图傅里叶谐波，专注于子图中的信号能量集中，在神经活动和交通动态数据中表现优于传统方法。", "motivation": "传统图神经网络和图信号处理方法在表示图信号模式时效率不足，尤其是在神经科学等领域中高维、时空模式的信号解码。", "method": "提出SlepNet，使用Slepian基自动学习相关子图，并通过掩码集中信号能量，生成规范且高分辨率的表示。", "result": "在三个fMRI数据集和两个交通动态数据集中，SlepNet均优于基线方法，并能更清晰地区分相似模式。", "conclusion": "SlepNet不仅适用于预测任务，还能用于表示学习，为时空数据的下游任务提供有用信息。"}}
{"id": "2506.16608", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16608", "abs": "https://arxiv.org/abs/2506.16608", "authors": ["Jiamin He", "A. Rupam Mahmood", "Martha White"], "title": "Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces", "comment": null, "summary": "We introduce a novel reinforcement learning (RL) framework that treats\ndistribution parameters as actions, redefining the boundary between agent and\nenvironment. This reparameterization makes the new action space continuous,\nregardless of the original action type (discrete, continuous, mixed, etc.).\nUnder this new parameterization, we develop a generalized deterministic policy\ngradient estimator, Distribution Parameter Policy Gradient (DPPG), which has\nlower variance than the gradient in the original action space. Although\nlearning the critic over distribution parameters poses new challenges, we\nintroduce interpolated critic learning (ICL), a simple yet effective strategy\nto enhance learning, supported by insights from bandit settings. Building on\nTD3, a strong baseline for continuous control, we propose a practical\nDPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).\nEmpirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from\nOpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance\non the same environments with discretized action spaces.", "AI": {"tldr": "提出了一种新的强化学习框架，将分布参数作为动作，重新定义了智能体与环境的边界，并开发了低方差的策略梯度估计器DPPG。", "motivation": "传统强化学习中动作空间的离散性或混合性限制了算法的通用性，通过将动作重新参数化为分布参数，可以统一处理不同类型的动作空间。", "method": "提出了DPPG策略梯度估计器和ICL插值评论家学习策略，基于TD3开发了DPAC算法。", "result": "DPAC在MuJoCo连续控制任务中优于TD3，并在离散化动作空间中表现竞争力。", "conclusion": "通过重新参数化动作空间，DPPG和DPAC算法在连续和离散任务中均表现出色，展示了框架的通用性和有效性。"}}
{"id": "2506.16629", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16629", "abs": "https://arxiv.org/abs/2506.16629", "authors": ["Eric V. Strobl"], "title": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data", "comment": "R code is available at github.com/ericstrobl/DEBIAS", "summary": "Causal inference in longitudinal biomedical data remains a central challenge,\nespecially in psychiatry, where symptom heterogeneity and latent confounding\nfrequently undermine classical estimators. Most existing methods for treatment\neffect estimation presuppose a fixed outcome variable and address confounding\nthrough observed covariate adjustment. However, the assumption of\nunconfoundedness may not hold for a fixed outcome in practice. To address this\nfoundational limitation, we directly optimize the outcome definition to\nmaximize causal identifiability. Our DEBIAS (Durable Effects with\nBackdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,\nclinically interpretable weights for outcome aggregation, maximizing durable\ntreatment effects and empirically minimizing both observed and latent\nconfounding by leveraging the time-limited direct effects of prior treatments\nin psychiatric longitudinal data. The algorithm also furnishes an empirically\nverifiable test for outcome unconfoundedness. DEBIAS consistently outperforms\nstate-of-the-art methods in recovering causal effects for clinically\ninterpretable composite outcomes across comprehensive experiments in depression\nand schizophrenia.", "AI": {"tldr": "DEBIAS算法通过优化结果定义以最大化因果可识别性，解决了精神病学纵向数据中因果推断的挑战。", "motivation": "精神病学中症状异质性和潜在混杂因素常削弱传统估计方法，现有方法假设固定结果变量且依赖观察到的协变量调整，但实践中无混杂假设可能不成立。", "method": "DEBIAS算法学习非负、临床可解释的权重以聚合结果，最大化持久治疗效果，并利用精神病学纵向数据中先前治疗的时滞直接效应最小化观察和潜在混杂。", "result": "DEBIAS在抑郁和精神分裂症的综合实验中，始终优于现有方法，恢复临床可解释复合结果的因果效应。", "conclusion": "DEBIAS通过优化结果定义和最小化混杂，显著提升了精神病学纵向数据中的因果推断能力。"}}
{"id": "2506.16644", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.16644", "abs": "https://arxiv.org/abs/2506.16644", "authors": ["Eren Akbiyik", "João Almeida", "Rik Melis", "Ritu Sriram", "Viviana Petrescu", "Vilhjálmur Vilhjálmsson"], "title": "Semantic Outlier Removal with Embedding Models and LLMs", "comment": "Accepted to the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025) Industry Track, 10 pages", "summary": "Modern text processing pipelines demand robust methods to remove extraneous\ncontent while preserving a document's core message. Traditional approaches such\nas HTML boilerplate extraction or keyword filters often fail in multilingual\nsettings and struggle with context-sensitive nuances, whereas Large Language\nModels (LLMs) offer improved quality at high computational cost. We introduce\nSORE (Semantic Outlier Removal), a cost-effective, transparent method that\nleverages multilingual sentence embeddings and approximate nearest-neighbor\nsearch to identify and excise unwanted text segments. By first identifying core\ncontent via metadata embedding and then flagging segments that either closely\nmatch predefined outlier groups or deviate significantly from the core, SORE\nachieves near-LLM extraction precision at a fraction of the cost. Experiments\non HTML datasets demonstrate that SORE outperforms structural methods and yield\nhigh precision in diverse scenarios. Our system is currently deployed in\nproduction, processing millions of documents daily across multiple languages\nwhile maintaining both efficiency and accuracy. To facilitate reproducibility\nand further research, we release our implementation and evaluation datasets.", "AI": {"tldr": "SORE是一种基于多语言句子嵌入和近似最近邻搜索的语义异常移除方法，高效且透明，能在多语言环境中精准去除无关文本。", "motivation": "传统方法（如HTML模板提取或关键词过滤）在多语言和上下文敏感场景中表现不佳，而大型语言模型（LLM）成本过高。", "method": "SORE通过元数据嵌入识别核心内容，并用近似最近邻搜索标记异常文本段。", "result": "实验显示SORE在HTML数据集上优于结构方法，且在多语言场景中保持高精度。", "conclusion": "SORE以低成本实现接近LLM的精度，已投入生产，支持多语言处理，并开源了实现和数据集。"}}
{"id": "2506.16651", "categories": ["cs.LG", "cs.CC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2506.16651", "abs": "https://arxiv.org/abs/2506.16651", "authors": ["Guy Blanc", "Jane Lange", "Carmen Strassle", "Li-Yang Tan"], "title": "A Distributional-Lifting Theorem for PAC Learning", "comment": "COLT 2025", "summary": "The apparent difficulty of efficient distribution-free PAC learning has led\nto a large body of work on distribution-specific learning. Distributional\nassumptions facilitate the design of efficient algorithms but also limit their\nreach and relevance. Towards addressing this, we prove a distributional-lifting\ntheorem: This upgrades a learner that succeeds with respect to a limited\ndistribution family $\\mathcal{D}$ to one that succeeds with respect to any\ndistribution $D^\\star$, with an efficiency overhead that scales with the\ncomplexity of expressing $D^\\star$ as a mixture of distributions in\n$\\mathcal{D}$.\n  Recent work of Blanc, Lange, Malik, and Tan considered the special case of\nlifting uniform-distribution learners and designed a lifter that uses a\nconditional sample oracle for $D^\\star$, a strong form of access not afforded\nby the standard PAC model. Their approach, which draws on ideas from\nsemi-supervised learning, first learns $D^\\star$ and then uses this information\nto lift.\n  We show that their approach is information-theoretically intractable with\naccess only to random examples, thereby giving formal justification for their\nuse of the conditional sample oracle. We then take a different approach that\nsidesteps the need to learn $D^\\star$, yielding a lifter that works in the\nstandard PAC model and enjoys additional advantages: it works for all base\ndistribution families, preserves the noise tolerance of learners, has better\nsample complexity, and is simpler.", "AI": {"tldr": "本文提出了一种分布提升定理，将有限分布族的学习器升级为适用于任意分布的学习器，并证明了现有方法在标准PAC模型中的信息理论不可行性，同时提出了一种更高效、更通用的新方法。", "motivation": "解决分布特定学习方法的局限性，提升学习器的通用性和效率。", "method": "提出分布提升定理，避免直接学习目标分布，设计了一种适用于标准PAC模型的提升方法。", "result": "新方法在标准PAC模型中有效，适用于所有基础分布族，具有更好的样本复杂性和噪声容忍性。", "conclusion": "新方法简化了现有技术，提供了更高效、更通用的分布提升解决方案。"}}
{"id": "2506.16654", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.16654", "abs": "https://arxiv.org/abs/2506.16654", "authors": ["Vijay Prakash Dwivedi", "Charilaos Kanatsoulis", "Shenyang Huang", "Jure Leskovec"], "title": "Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures", "comment": null, "summary": "Graph machine learning has led to a significant increase in the capabilities\nof models that learn on arbitrary graph-structured data and has been applied to\nmolecules, social networks, recommendation systems, and transportation, among\nother domains. Data in multi-tabular relational databases can also be\nconstructed as 'relational entity graphs' for Relational Deep Learning (RDL) -\na new blueprint that enables end-to-end representation learning without\ntraditional feature engineering. Compared to arbitrary graph-structured data,\nrelational entity graphs have key properties: (i) their structure is defined by\nprimary-foreign key relationships between entities in different tables, (ii)\nthe structural connectivity is a function of the relational schema defining a\ndatabase, and (iii) the graph connectivity is temporal and heterogeneous in\nnature. In this paper, we provide a comprehensive review of RDL by first\nintroducing the representation of relational databases as relational entity\ngraphs, and then reviewing public benchmark datasets that have been used to\ndevelop and evaluate recent GNN-based RDL models. We discuss key challenges\nincluding large-scale multi-table integration and the complexities of modeling\ntemporal dynamics and heterogeneous data, while also surveying foundational\nneural network methods and recent architectural advances specialized for\nrelational entity graphs. Finally, we explore opportunities to unify these\ndistinct modeling challenges, highlighting how RDL converges multiple\nsub-fields in graph machine learning towards the design of foundation models\nthat can transform the processing of relational data.", "AI": {"tldr": "该论文综述了关系深度学习（RDL），将多表关系数据库表示为关系实体图，并探讨了其关键特性、挑战及最新方法。", "motivation": "关系数据库的传统特征工程方法限制了端到端表示学习的能力，而关系实体图提供了一种新的学习范式。", "method": "通过将关系数据库建模为关系实体图，并利用图神经网络（GNN）进行表示学习，同时解决多表集成、时间动态和异构数据等挑战。", "result": "论文总结了RDL的公共基准数据集、关键挑战及最新架构进展，为关系数据的处理提供了新方向。", "conclusion": "RDL有望统一图机器学习的多个子领域，推动关系数据处理的基础模型设计。"}}
{"id": "2506.16656", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16656", "abs": "https://arxiv.org/abs/2506.16656", "authors": ["Yaozhong Shi", "Zachary E. Ross", "Domniki Asimaki", "Kamyar Azizzadenesheli"], "title": "Mesh-Informed Neural Operator : A Transformer Generative Approach", "comment": null, "summary": "Generative models in function spaces, situated at the intersection of\ngenerative modeling and operator learning, are attracting increasing attention\ndue to their immense potential in diverse scientific and engineering\napplications. While functional generative models are theoretically domain- and\ndiscretization-agnostic, current implementations heavily rely on the Fourier\nNeural Operator (FNO), limiting their applicability to regular grids and\nrectangular domains. To overcome these critical limitations, we introduce the\nMesh-Informed Neural Operator (MINO). By leveraging graph neural operators and\ncross-attention mechanisms, MINO offers a principled, domain- and\ndiscretization-agnostic backbone for generative modeling in function spaces.\nThis advancement significantly expands the scope of such models to more diverse\napplications in generative, inverse, and regression tasks. Furthermore, MINO\nprovides a unified perspective on integrating neural operators with general\nadvanced deep learning architectures. Finally, we introduce a suite of\nstandardized evaluation metrics that enable objective comparison of functional\ngenerative models, addressing another critical gap in the field.", "AI": {"tldr": "论文提出了一种名为Mesh-Informed Neural Operator（MINO）的新方法，解决了现有功能生成模型对规则网格和矩形域的依赖问题，扩展了应用范围。", "motivation": "现有功能生成模型（如FNO）受限于规则网格和矩形域，限制了其应用潜力。", "method": "MINO结合图神经算子和交叉注意力机制，提供了一种与域和离散化无关的生成建模框架。", "result": "MINO显著扩展了功能生成模型的应用范围，并提供了统一的视角来整合神经算子与深度学习架构。", "conclusion": "MINO填补了功能生成模型领域的空白，并提供了标准化评估指标，推动了该领域的发展。"}}
{"id": "2506.16659", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16659", "abs": "https://arxiv.org/abs/2506.16659", "authors": ["Athanasios Glentis", "Jiaxiang Li", "Andi Han", "Mingyi Hong"], "title": "A Minimalist Optimizer Design for LLM Pretraining", "comment": null, "summary": "Training large language models (LLMs) typically relies on adaptive optimizers\nsuch as Adam, which require significant memory to maintain first- and\nsecond-moment matrices, known as optimizer states. While recent works such as\nGaLore, Fira, and APOLLO have proposed state-compressed variants to reduce\nmemory consumption, a fundamental question remains: What is the minimal amount\nof optimizer state that is truly necessary to retain state-of-the-art\nperformance in LLM pretraining? In this work, we systematically investigate\nthis question using a bottom-up approach. We find that two memory- and\ncompute-efficient optimization techniques are particularly effective: (1)\ncolumn-wise gradient normalization significantly boosts the performance of\nplain SGD without requiring momentum; and (2) adding first-order momentum only\nto the output layer - where gradient variance is highest - yields performance\ncompetitive with fully adaptive methods such as Muon. Based on these insights,\nwe propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new\noptimizer that combines column-normalized SGD with last-layer momentum, where\ncolumn normalization refers to normalizing the gradient along the output\ndimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the\nperformance of Adam while using only 35-45% of the total memory. It also\nconsistently outperforms memory-efficient optimizers such as GaLore, Fira, and\nAPOLLO, making it a strong candidate for large-scale pretraining under memory\nconstraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art\nmethod APOLLO in terms of both perplexity and memory consumption. In addition,\nour method serves as a minimalist baseline for more sophisticated optimizer\ndesign.", "AI": {"tldr": "SCALE是一种新型优化器，结合列归一化SGD和最后一层动量，显著减少内存占用，同时保持或超越Adam的性能。", "motivation": "研究LLM预训练中优化器状态的最小需求，以降低内存消耗。", "method": "采用列归一化梯度和最后一层动量技术，提出SCALE优化器。", "result": "SCALE在多个LLaMA模型中性能优于Adam，内存占用仅为35-45%，且超越其他高效优化器。", "conclusion": "SCALE是内存受限大规模预训练的理想选择，并为优化器设计提供了简约基线。"}}
{"id": "2506.16661", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16661", "abs": "https://arxiv.org/abs/2506.16661", "authors": ["Felix Zhou", "Samson Zhou", "Vahab Mirrokni", "Alessandro Epasto", "Vincent Cohen-Addad"], "title": "Private Training & Data Generation by Clustering Embeddings", "comment": null, "summary": "Deep neural networks often use large, high-quality datasets to achieve high\nperformance on many machine learning tasks. When training involves potentially\nsensitive data, this process can raise privacy concerns, as large models have\nbeen shown to unintentionally memorize and reveal sensitive information,\nincluding reconstructing entire training samples. Differential privacy (DP)\nprovides a robust framework for protecting individual data and in particular, a\nnew approach to privately training deep neural networks is to approximate the\ninput dataset with a privately generated synthetic dataset, before any\nsubsequent training algorithm. We introduce a novel principled method for DP\nsynthetic image embedding generation, based on fitting a Gaussian Mixture Model\n(GMM) in an appropriate embedding space using DP clustering. Our method\nprovably learns a GMM under separation conditions. Empirically, a simple\ntwo-layer neural network trained on synthetically generated embeddings achieves\nstate-of-the-art (SOTA) classification accuracy on standard benchmark datasets.\nAdditionally, we demonstrate that our method can generate realistic synthetic\nimages that achieve downstream classification accuracy comparable to SOTA\nmethods. Our method is quite general, as the encoder and decoder modules can be\nfreely substituted to suit different tasks. It is also highly scalable,\nconsisting only of subroutines that scale linearly with the number of samples\nand/or can be implemented efficiently in distributed systems.", "AI": {"tldr": "论文提出了一种基于差分隐私（DP）的合成图像嵌入生成方法，通过高斯混合模型（GMM）在嵌入空间中实现隐私保护，并在分类任务中达到SOTA性能。", "motivation": "解决深度神经网络训练中隐私泄露问题，尤其是敏感数据的记忆和泄露风险。", "method": "使用DP聚类在嵌入空间中拟合GMM，生成合成数据集，再训练简单神经网络进行分类。", "result": "在标准基准数据集上达到SOTA分类准确率，并能生成逼真的合成图像。", "conclusion": "该方法通用性强、可扩展性好，适用于不同任务，且隐私保护效果显著。"}}
{"id": "2506.16688", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16688", "abs": "https://arxiv.org/abs/2506.16688", "authors": ["Zhiying Qiu", "Tao Lin"], "title": "Fast and Stable Diffusion Planning through Variational Adaptive Weighting", "comment": null, "summary": "Diffusion models have recently shown promise in offline RL. However, these\nmethods often suffer from high training costs and slow convergence,\nparticularly when using transformer-based denoising backbones. While several\noptimization strategies have been proposed -- such as modified noise schedules,\nauxiliary prediction targets, and adaptive loss weighting -- challenges remain\nin achieving stable and efficient training. In particular, existing loss\nweighting functions typically rely on neural network approximators, which can\nbe ineffective in early training phases due to limited generalization capacity\nof MLPs when exposed to sparse feedback in the early training stages. In this\nwork, we derive a variationally optimal uncertainty-aware weighting function\nand introduce a closed-form polynomial approximation method for its online\nestimation under the flow-based generative modeling framework. We integrate our\nmethod into a diffusion planning pipeline and evaluate it on standard offline\nRL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our\nmethod achieves competitive performance with up to 10 times fewer training\nsteps, highlighting its practical effectiveness.", "AI": {"tldr": "本文提出了一种基于变分最优不确定性感知的权重函数及其闭式多项式近似方法，用于提升扩散模型在离线强化学习中的训练效率和稳定性。", "motivation": "扩散模型在离线强化学习中表现出潜力，但存在训练成本高、收敛慢的问题，尤其是基于Transformer的去噪主干网络。现有方法在早期训练阶段因MLP泛化能力有限而效果不佳。", "method": "提出了一种变分最优不确定性感知权重函数，并引入闭式多项式近似方法进行在线估计，将其集成到扩散规划流程中。", "result": "在Maze2D和Kitchen任务上的实验表明，该方法能以10倍更少的训练步骤达到竞争性性能。", "conclusion": "该方法显著提升了扩散模型在离线强化学习中的训练效率和稳定性，具有实际应用价值。"}}
{"id": "2506.16698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16698", "abs": "https://arxiv.org/abs/2506.16698", "authors": ["Dinesh Ramasamy", "Shakti Kumar", "Chris Cadonic", "Jiaxin Yang", "Sohini Roychowdhury", "Esam Abdel Rhman", "Srihari Reddy"], "title": "SIDE: Semantic ID Embedding for effective learning from sequences", "comment": "7 pages, 4 images, 6 tables", "summary": "Sequence-based recommendations models are driving the state-of-the-art for\nindustrial ad-recommendation systems. Such systems typically deal with user\nhistories or sequence lengths ranging in the order of O(10^3) to O(10^4)\nevents. While adding embeddings at this scale is manageable in pre-trained\nmodels, incorporating them into real-time prediction models is challenging due\nto both storage and inference costs. To address this scaling challenge, we\npropose a novel approach that leverages vector quantization (VQ) to inject a\ncompact Semantic ID (SID) as input to the recommendation models instead of a\ncollection of embeddings. Our method builds on recent works of SIDs by\nintroducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ\nfusion that fuses multiple content embeddings and categorical predictions into\na single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding\nconversion technique, called SIDE, that is validated with two content embedding\ncollections, thereby eliminating the need for a large parameterized lookup\ntable; and (iii) a novel quantization method called Discrete-PCA (DPCA) which\ngeneralizes and enhances residual quantization techniques. The proposed\nenhancements when applied to a large-scale industrial ads-recommendation system\nachieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in\ndata footprint compared to traditional SID methods.", "AI": {"tldr": "论文提出了一种基于向量量化（VQ）的新方法，通过生成紧凑的语义ID（SID）替代传统嵌入，解决了工业广告推荐系统中存储和推理成本高的问题。", "motivation": "工业广告推荐系统通常需要处理大规模的用户历史数据，传统嵌入方法在实时预测中面临存储和推理成本高的挑战。", "method": "提出了三种创新：多任务VQ-VAE框架（VQ融合）、参数自由的SID-to-embedding转换技术（SIDE）和离散PCA（DPCA）量化方法。", "result": "在大规模工业广告推荐系统中，该方法实现了2.4倍的归一化熵增益提升和3倍的数据占用减少。", "conclusion": "该方法显著提升了推荐系统的效率和性能，为大规模实时推荐提供了可行的解决方案。"}}
{"id": "2506.16704", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16704", "abs": "https://arxiv.org/abs/2506.16704", "authors": ["Cynthia Dwork", "Lunjia Hu", "Han Shao"], "title": "How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension", "comment": null, "summary": "We study a fundamental question of domain generalization: given a family of\ndomains (i.e., data distributions), how many randomly sampled domains do we\nneed to collect data from in order to learn a model that performs reasonably\nwell on every seen and unseen domain in the family? We model this problem in\nthe PAC framework and introduce a new combinatorial measure, which we call the\ndomain shattering dimension. We show that this dimension characterizes the\ndomain sample complexity. Furthermore, we establish a tight quantitative\nrelationship between the domain shattering dimension and the classic VC\ndimension, demonstrating that every hypothesis class that is learnable in the\nstandard PAC setting is also learnable in our setting.", "AI": {"tldr": "论文研究了领域泛化的基本问题，提出了一个新的组合度量——领域破碎维度，并证明了其与经典VC维度的定量关系。", "motivation": "探索在给定领域族中，需要从多少随机采样的领域收集数据才能学习到一个在所有领域（包括未见领域）表现良好的模型。", "method": "在PAC框架下建模问题，引入领域破碎维度作为新的组合度量。", "result": "领域破碎维度刻画了领域样本复杂度，并与经典VC维度建立了紧密的定量关系。", "conclusion": "证明在标准PAC设置下可学习的假设类，在该设置下也可学习。"}}
{"id": "2506.16723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16723", "abs": "https://arxiv.org/abs/2506.16723", "authors": ["Yuping Yan", "Yizhi Wang", "Yuanshuai Li", "Yaochu Jin"], "title": "TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data", "comment": null, "summary": "Serial pipeline training is an efficient paradigm for handling data\nheterogeneity in cross-silo federated learning with low communication overhead.\nHowever, even without centralized aggregation, direct transfer of models\nbetween clients can violate privacy regulations and remain susceptible to\ngradient leakage and linkage attacks. Additionally, ensuring resilience against\nsemi-honest or malicious clients who may manipulate or misuse received models\nremains a grand challenge, particularly in privacy-sensitive domains such as\nhealthcare. To address these challenges, we propose TriCon-SF, a novel serial\nfederated learning framework that integrates triple shuffling and contribution\nawareness. TriCon-SF introduces three levels of randomization by shuffling\nmodel layers, data segments, and training sequences to break deterministic\nlearning patterns and disrupt potential attack vectors, thereby enhancing\nprivacy and robustness. In parallel, it leverages Shapley value methods to\ndynamically evaluate client contributions during training, enabling the\ndetection of dishonest behavior and enhancing system accountability. Extensive\nexperiments on non-IID healthcare datasets demonstrate that TriCon-SF\noutperforms standard serial and parallel federated learning in both accuracy\nand communication efficiency. Security analysis further supports its resilience\nagainst client-side privacy attacks.", "AI": {"tldr": "TriCon-SF是一种新型串行联邦学习框架，通过三重随机化和贡献感知提升隐私和鲁棒性，优于标准方法。", "motivation": "解决串行管道训练中的隐私泄露、梯度泄漏和恶意客户端问题，特别是在医疗等隐私敏感领域。", "method": "结合三重随机化（模型层、数据段、训练顺序）和Shapley值方法动态评估客户端贡献。", "result": "在非独立同分布医疗数据集上表现优于标准方法，提升准确性和通信效率。", "conclusion": "TriCon-SF在隐私保护和鲁棒性方面表现优异，适用于隐私敏感场景。"}}
{"id": "2506.16732", "categories": ["cs.LG", "cs.AI", "cs.DM", "math.PR"], "pdf": "https://arxiv.org/pdf/2506.16732", "abs": "https://arxiv.org/abs/2506.16732", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis", "comment": "2nd Workshop on Test-Time Adaptation: Putting Updates to the Test @\n  ICML 2025", "summary": "In unsupervised combinatorial optimization (UCO), during training, one aims\nto have continuous decisions that are promising in a probabilistic sense for\neach training instance, which enables end-to-end training on initially discrete\nand non-differentiable problems. At the test time, for each test instance,\nstarting from continuous decisions, derandomization is typically applied to\nobtain the final deterministic decisions. Researchers have developed more and\nmore powerful test-time derandomization schemes to enhance the empirical\nperformance and the theoretical guarantee of UCO methods. However, we notice a\nmisalignment between training and testing in the existing UCO methods.\nConsequently, lower training losses do not necessarily entail better\npost-derandomization performance, even for the training instances without any\ndata distribution shift. Empirically, we indeed observe such undesirable cases.\nWe explore a preliminary idea to better align training and testing in UCO by\nincluding a differentiable version of derandomization into training. Our\nempirical exploration shows that such an idea indeed improves training-test\nalignment, but also introduces nontrivial challenges into training.", "AI": {"tldr": "论文探讨了无监督组合优化（UCO）中训练与测试不对齐的问题，并提出了一种通过引入可微分的去随机化方法改进对齐的初步想法。", "motivation": "现有UCO方法中训练与测试存在不对齐问题，导致训练损失低不一定带来更好的去随机化性能。", "method": "在训练中引入可微分版本的测试时去随机化方法。", "result": "初步实验表明该方法改进了训练与测试的对齐，但也带来了训练上的挑战。", "conclusion": "研究揭示了UCO中训练与测试对齐的重要性，并提出了一种潜在解决方案，但需进一步解决训练中的挑战。"}}
{"id": "2506.16890", "categories": ["cs.LG", "cs.CV", "stat.AP", "62-06", "G.3; I.4; I.5"], "pdf": "https://arxiv.org/pdf/2506.16890", "abs": "https://arxiv.org/abs/2506.16890", "authors": ["Sebastian Hönel", "Jonas Nordqvist"], "title": "From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images", "comment": "18 pages, 7 figures, 1 table. Camera-ready version for the 2025\n  conference European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD '25)", "summary": "The detection and localization of quality-related problems in industrially\nmass-produced products has historically relied on manual inspection, which is\ncostly and error-prone. Machine learning has the potential to replace manual\nhandling. As such, the desire is to facilitate an unsupervised (or\nself-supervised) approach, as it is often impossible to specify all conceivable\ndefects ahead of time. A plethora of prior works have demonstrated the aptitude\nof common reconstruction-, embedding-, and synthesis-based methods in\nlaboratory settings. However, in practice, we observe that most methods do not\nhandle low data quality well or exude low robustness in unfavorable, but\ntypical real-world settings. For practitioners it may be very difficult to\nidentify the actual underlying problem when such methods underperform. Worse,\noften-reported metrics (e.g., AUROC) are rarely suitable in practice and may\ngive misleading results. In our setting, we attempt to identify subtle\nanomalies on the surface of blasted forged metal parts, using rather\nlow-quality RGB imagery only, which is a common industrial setting. We\nspecifically evaluate two types of state-of-the-art models that allow us to\nidentify and improve quality issues in production data, without having to\nobtain new data. Our contribution is to provide guardrails for practitioners\nthat allow them to identify problems related to, e.g., (lack of) robustness or\ninvariance, in either the chosen model or the data reliably in similar\nscenarios. Furthermore, we exemplify common pitfalls in and shortcomings of\nlikelihood-based approaches and outline a framework for proper empirical risk\nestimation that is more suitable for real-world scenarios.", "AI": {"tldr": "论文探讨了在工业大规模生产中使用无监督或自监督机器学习方法检测和定位质量问题的挑战，并提出了改进模型和数据质量问题的实用框架。", "motivation": "传统手动检测成本高且易出错，机器学习有潜力替代，但现有方法在低质量数据和实际环境中表现不佳，需要更实用的解决方案。", "method": "评估了两种最先进的模型，用于识别和改进生产数据中的质量问题，无需新数据，并提供了识别模型或数据问题的实用指南。", "result": "论文揭示了基于似然方法的常见缺陷，并提出了更适合实际场景的实证风险估计框架。", "conclusion": "为从业者提供了在实际工业场景中识别和改进质量问题的实用工具和方法，强调了模型和数据质量的重要性。"}}
{"id": "2506.16754", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.16754", "abs": "https://arxiv.org/abs/2506.16754", "authors": ["Jongmin Park", "Seunghoon Han", "Won-Yong Shin", "Sungsu Lim"], "title": "Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding", "comment": "14 pages, 9 figures", "summary": "The hyperbolic space, characterized by a constant negative curvature and\nexponentially expanding space, aligns well with the structural properties of\nheterogeneous graphs. However, although heterogeneous graphs inherently possess\ndiverse power-law structures, most hyperbolic heterogeneous graph embedding\nmodels rely on a single hyperbolic space. This approach may fail to effectively\ncapture the diverse power-law structures within heterogeneous graphs. To\naddress this limitation, we propose a Metapath-based Hyperbolic Contrastive\nLearning framework (MHCL), which uses multiple hyperbolic spaces to capture\ndiverse complex structures within heterogeneous graphs. Specifically, by\nlearning each hyperbolic space to describe the distribution of complex\nstructures corresponding to each metapath, it is possible to capture semantic\ninformation effectively. Since metapath embeddings represent distinct semantic\ninformation, preserving their discriminability is important when aggregating\nthem to obtain node representations. Therefore, we use a contrastive learning\napproach to optimize MHCL and improve the discriminability of metapath\nembeddings. In particular, our contrastive learning method minimizes the\ndistance between embeddings of the same metapath and maximizes the distance\nbetween those of different metapaths in hyperbolic space, thereby improving the\nseparability of metapath embeddings with distinct semantic information. We\nconduct comprehensive experiments to evaluate the effectiveness of MHCL. The\nexperimental results demonstrate that MHCL outperforms state-of-the-art\nbaselines in various graph machine learning tasks, effectively capturing the\ncomplex structures of heterogeneous graphs.", "AI": {"tldr": "论文提出了一种基于多双曲空间的异构图嵌入框架MHCL，通过对比学习优化，有效捕捉异构图中的多样复杂结构。", "motivation": "现有双曲异构图嵌入模型仅依赖单一双曲空间，难以有效捕捉异构图中多样的幂律结构。", "method": "提出MHCL框架，利用多双曲空间描述不同元路径对应的复杂结构分布，并通过对比学习优化元路径嵌入的区分性。", "result": "实验表明MHCL在多种图机器学习任务中优于现有基线，能有效捕捉异构图的复杂结构。", "conclusion": "MHCL通过多双曲空间和对比学习，显著提升了异构图嵌入的性能。"}}
{"id": "2506.16782", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.16782", "abs": "https://arxiv.org/abs/2506.16782", "authors": ["Youjin Kong"], "title": "What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity", "comment": "Accepted for presentation at ACM FAccT 2025; under final review\n  (minor revision) at an ACM journal", "summary": "Fairness in machine learning (ML) has become a rapidly growing area of\nresearch. But why, in the first place, is unfairness in ML morally wrong? And\nwhy should we care about improving fairness? Most fair-ML research implicitly\nappeals to distributive equality: the idea that desirable goods and benefits,\nsuch as opportunities (e.g., Barocas et al., 2023), should be equally\ndistributed across society. Unfair ML models, then, are seen as wrong because\nthey unequally distribute such benefits. This paper argues that this exclusive\nfocus on distributive equality offers an incomplete and potentially misleading\nethical foundation. Grounding ML fairness in egalitarianism -- the view that\nequality is a fundamental moral and social ideal -- requires challenging\nstructural inequality: systematic, institutional, and durable arrangements that\nprivilege some groups while disadvantaging others. Structural inequality\nmanifests through ML systems in two primary forms: allocative harms (e.g.,\neconomic loss) and representational harms (e.g., stereotypes, erasure). While\ndistributive equality helps address allocative harms, it fails to explain why\nrepresentational harms are wrong -- why it is wrong for ML systems to reinforce\nsocial hierarchies that stratify people into superior and inferior groups --\nand why ML systems should aim to foster a society where people relate as equals\n(i.e., relational equality). To address these limitations, the paper proposes a\nmultifaceted egalitarian framework for ML fairness that integrates both\ndistributive and relational equality. Drawing on critical social and political\nphilosophy, this framework offers a more comprehensive ethical foundation for\ntackling the full spectrum of harms perpetuated by ML systems. The paper also\noutlines practical pathways for implementing the framework across the ML\npipeline.", "AI": {"tldr": "论文探讨了机器学习公平性的伦理基础，指出仅关注分配平等是不完整的，提出了结合分配平等和关系平等的多元平等框架。", "motivation": "现有公平性研究主要基于分配平等，但忽视了结构性不平等和关系平等的重要性，导致伦理基础不完整。", "method": "提出一个结合分配平等和关系平等的多元平等框架，并借鉴批判社会和政治哲学。", "result": "该框架能更全面地解决机器学习系统中的分配性和代表性危害。", "conclusion": "多元平等框架为机器学习公平性提供了更全面的伦理基础，并提出了实际实施路径。"}}
{"id": "2506.16787", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16787", "abs": "https://arxiv.org/abs/2506.16787", "authors": ["Jiashun Cheng", "Aochuan Chen", "Nuo Chen", "Ziqi Gao", "Yuhan Li", "Jia Li", "Fugee Tsung"], "title": "Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps", "comment": "18 pages; Accepted to ACL 2025 Findings", "summary": "Low-Rank Adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large foundation models. Despite its successes, the substantial\nparameter redundancy, which limits the capacity and efficiency of LoRA, has\nbeen recognized as a bottleneck. In this work, we systematically investigate\nthe impact of redundancy in fine-tuning LoRA and reveal that reducing density\nredundancy does not degrade expressiveness. Based on this insight, we introduce\n\\underline{S}pectral-\\underline{e}ncoding \\underline{L}ow-\\underline{R}ank\n\\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of\nspectral bases to re-parameterize LoRA from a sparse spectral subspace.\nDesigned with simplicity, SeLoRA enables seamless integration with various LoRA\nvariants for performance boosting, serving as a scalable plug-and-play\nframework. Extensive experiments substantiate that SeLoRA achieves greater\nefficiency with fewer parameters, delivering superior performance enhancements\nover strong baselines on various downstream tasks, including commonsense\nreasoning, math reasoning, and code generation.", "AI": {"tldr": "SeLoRA通过稀疏谱子空间重新参数化LoRA，减少冗余参数，提升效率与性能。", "motivation": "LoRA在微调大型基础模型时存在参数冗余问题，限制了其容量和效率。", "method": "提出SeLoRA，利用谱基的鲁棒表达能力从稀疏谱子空间重新参数化LoRA。", "result": "实验证明SeLoRA在减少参数的同时提升了性能，优于基线模型。", "conclusion": "SeLoRA是一种高效、可扩展的插件框架，适用于多种下游任务。"}}
{"id": "2506.16790", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16790", "abs": "https://arxiv.org/abs/2506.16790", "authors": ["Senmiao Wang", "Yupeng Chen", "Yushun Zhang", "Ruoyu Sun", "Tian Ding"], "title": "Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective", "comment": "Published in TMLR (2025)", "summary": "Graph Neural Networks (GNNs) often suffer from performance degradation as the\nnetwork depth increases. This paper addresses this issue by introducing\ninitialization methods that enhance signal propagation (SP) within GNNs. We\npropose three key metrics for effective SP in GNNs: forward propagation,\nbackward propagation, and graph embedding variation (GEV). While the first two\nmetrics derive from classical SP theory, the third is specifically designed for\nGNNs. We theoretically demonstrate that a broad range of commonly used\ninitialization methods for GNNs, which exhibit performance degradation with\nincreasing depth, fail to control these three metrics simultaneously. To deal\nwith this limitation, a direct exploitation of the SP analysis--searching for\nweight initialization variances that optimize the three metrics--is shown to\nsignificantly enhance the SP in deep GCNs. This approach is called Signal\nPropagation on Graph-guided Initialization (SPoGInit). Our experiments\ndemonstrate that SPoGInit outperforms commonly used initialization methods on\nvarious tasks and architectures. Notably, SPoGInit enables performance\nimprovements as GNNs deepen, which represents a significant advancement in\naddressing depth-related challenges and highlights the validity and\neffectiveness of the SP analysis framework.", "AI": {"tldr": "论文提出了一种名为SPoGInit的初始化方法，通过优化信号传播（SP）的三个关键指标，解决了GNN深度增加时性能下降的问题。", "motivation": "GNN在深度增加时性能下降，现有初始化方法无法同时控制信号传播的三个关键指标。", "method": "提出SPoGInit方法，通过搜索优化权重初始化方差，同时优化前向传播、反向传播和图嵌入变化（GEV）三个指标。", "result": "实验证明SPoGInit在多种任务和架构中优于常用初始化方法，并能随着GNN深度增加提升性能。", "conclusion": "SPoGInit有效解决了GNN深度相关挑战，验证了信号传播分析框架的有效性。"}}
{"id": "2506.16791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16791", "abs": "https://arxiv.org/abs/2506.16791", "authors": ["Nick Erickson", "Lennart Purucker", "Andrej Tschalzev", "David Holzmüller", "Prateek Mutalik Desai", "and David Salinas", "Frank Hutter"], "title": "TabArena: A Living Benchmark for Machine Learning on Tabular Data", "comment": "51 pages. Code available at https://tabarena.ai/code; examples at\n  https://tabarena.ai/code-examples; dataset curation at\n  https://tabarena.ai/data-tabular-ml-iid-study and\n  https://tabarena.ai/dataset-curation", "summary": "With the growing popularity of deep learning and foundation models for\ntabular data, the need for standardized and reliable benchmarks is higher than\never. However, current benchmarks are static. Their design is not updated even\nif flaws are discovered, model versions are updated, or new models are\nreleased. To address this, we introduce TabArena, the first continuously\nmaintained living tabular benchmarking system. To launch TabArena, we manually\ncurate a representative collection of datasets and well-implemented models,\nconduct a large-scale benchmarking study to initialize a public leaderboard,\nand assemble a team of experienced maintainers. Our results highlight the\ninfluence of validation method and ensembling of hyperparameter configurations\nto benchmark models at their full potential. While gradient-boosted trees are\nstill strong contenders on practical tabular datasets, we observe that deep\nlearning methods have caught up under larger time budgets with ensembling. At\nthe same time, foundation models excel on smaller datasets. Finally, we show\nthat ensembles across models advance the state-of-the-art in tabular machine\nlearning and investigate the contributions of individual models. We launch\nTabArena with a public leaderboard, reproducible code, and maintenance\nprotocols to create a living benchmark available at https://tabarena.ai.", "AI": {"tldr": "TabArena是一个持续维护的动态表格数据基准测试系统，旨在解决现有静态基准的不足，通过标准化数据集和模型，提供公开排行榜和可复现代码。", "motivation": "当前表格数据的基准测试系统是静态的，无法适应模型更新或新模型发布的需求，TabArena旨在填补这一空白。", "method": "手动整理代表性数据集和模型，进行大规模基准测试，建立公开排行榜，并组建维护团队。", "result": "梯度提升树在实用数据集上表现优异，深度学习方法在大时间预算下通过集成追赶，基础模型在小数据集上表现突出。", "conclusion": "TabArena通过动态维护和公开资源，推动了表格机器学习的最新技术发展。"}}
{"id": "2506.16815", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16815", "abs": "https://arxiv.org/abs/2506.16815", "authors": ["Kai Yang", "Shaoyu Dou", "Pan Luo", "Xin Wang", "H. Vincent Poor"], "title": "Robust Group Anomaly Detection for Quasi-Periodic Network Time Series", "comment": "Published in IEEE Transactions on Network Science and Engineering", "summary": "Many real-world multivariate time series are collected from a network of\nphysical objects embedded with software, electronics, and sensors. The\nquasi-periodic signals generated by these objects often follow a similar\nrepetitive and periodic pattern, but have variations in the period, and come in\ndifferent lengths caused by timing (synchronization) errors. Given a multitude\nof such quasi-periodic time series, can we build machine learning models to\nidentify those time series that behave differently from the majority of the\nobservations? In addition, can the models help human experts to understand how\nthe decision was made? We propose a sequence to Gaussian Mixture Model\n(seq2GMM) framework. The overarching goal of this framework is to identify\nunusual and interesting time series within a network time series database. We\nfurther develop a surrogate-based optimization algorithm that can efficiently\ntrain the seq2GMM model. Seq2GMM exhibits strong empirical performance on a\nplurality of public benchmark datasets, outperforming state-of-the-art anomaly\ndetection techniques by a significant margin. We also theoretically analyze the\nconvergence property of the proposed training algorithm and provide numerical\nresults to substantiate our theoretical claims.", "AI": {"tldr": "论文提出了一种seq2GMM框架，用于识别网络时间序列数据库中的异常时间序列，并通过优化算法高效训练模型，性能优于现有技术。", "motivation": "现实中的多元时间序列常因同步误差导致周期和长度变化，需识别异常序列并解释决策过程。", "method": "提出seq2GMM框架，结合序列到高斯混合模型，并开发基于代理的优化算法进行训练。", "result": "在多个公共数据集上表现优异，显著优于现有异常检测技术，理论分析验证了算法收敛性。", "conclusion": "seq2GMM能有效识别异常时间序列，并通过理论分析和实验结果验证了其优越性。"}}
{"id": "2506.16824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16824", "abs": "https://arxiv.org/abs/2506.16824", "authors": ["Thomas Marwitz", "Alexander Colsmann", "Ben Breitung", "Christoph Brabec", "Christoph Kirchlechner", "Eva Blasco", "Gabriel Cadilha Marques", "Horst Hahn", "Michael Hirtz", "Pavel A. Levkin", "Yolita M. Eggeler", "Tobias Schlöder", "Pascal Friederich"], "title": "Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs", "comment": null, "summary": "Due to an exponential increase in published research articles, it is\nimpossible for individual scientists to read all publications, even within\ntheir own research field. In this work, we investigate the use of large\nlanguage models (LLMs) for the purpose of extracting the main concepts and\nsemantic information from scientific abstracts in the domain of materials\nscience to find links that were not noticed by humans and thus to suggest\ninspiring near/mid-term future research directions. We show that LLMs can\nextract concepts more efficiently than automated keyword extraction methods to\nbuild a concept graph as an abstraction of the scientific literature. A machine\nlearning model is trained to predict emerging combinations of concepts, i.e.\nnew research ideas, based on historical data. We demonstrate that integrating\nsemantic concept information leads to an increased prediction performance. The\napplicability of our model is demonstrated in qualitative interviews with\ndomain experts based on individualized model suggestions. We show that the\nmodel can inspire materials scientists in their creative thinking process by\npredicting innovative combinations of topics that have not yet been\ninvestigated.", "AI": {"tldr": "利用大语言模型（LLM）从材料科学摘要中提取主要概念和语义信息，构建概念图，预测新兴研究方向的组合。", "motivation": "由于研究文献数量激增，科学家难以全面阅读，需自动化工具提取关键信息并启发新研究方向。", "method": "使用LLM提取概念，构建概念图，训练机器学习模型预测新兴概念组合。", "result": "模型预测性能提升，专家访谈证实其能启发科学家创新思维。", "conclusion": "LLM结合语义信息可有效预测未探索的研究方向，助力科学创新。"}}
{"id": "2506.16840", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16840", "abs": "https://arxiv.org/abs/2506.16840", "authors": ["Zeyneddin Oz", "Shreyas Korde", "Marius Bock", "Kristof Van Laerhoven"], "title": "FedFitTech: A Baseline in Federated Learning for Fitness Tracking", "comment": "This submission includes a total of 7 pages and 6 figures", "summary": "Rapid evolution of sensors and resource-efficient machine learning models\nhave spurred the widespread adoption of wearable fitness tracking devices.\nEquipped with inertial sensors, such devices can continuously capture physical\nmovements for fitness technology (FitTech), enabling applications from sports\noptimization to preventive healthcare. Traditional centralized learning\napproaches to detect fitness activities struggle with privacy concerns,\nregulatory constraints, and communication inefficiencies. In contrast,\nFederated Learning (FL) enables a decentralized model training by communicating\nmodel updates rather than private wearable sensor data. Applying FL to FitTech\npresents unique challenges, such as data imbalance, lack of labelled data,\nheterogeneous user activity patterns, and trade-offs between personalization\nand generalization. To simplify research on FitTech in FL, we present the\nFedFitTech baseline, under the Flower framework, which is publicly available\nand widely used by both industry and academic researchers. Additionally, to\nillustrate its usage, this paper presents a case study that implements a system\nbased on the FedFitTech baseline, incorporating a client-side early stopping\nstrategy and comparing the results. For instance, this system allows wearable\ndevices to optimize the trade-off between capturing common fitness activity\npatterns and preserving individuals' nuances, thereby enhancing both the\nscalability and efficiency of privacy-aware fitness tracking applications.\nResults show that this reduces overall redundant communications by 13 percent,\nwhile maintaining the overall recognition performance at a negligible\nrecognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation\nfor a wide range of new research and development opportunities in FitTech, and\nit is available as open-source at:\nhttps://github.com/adap/flower/tree/main/baselines/fedfittech", "AI": {"tldr": "论文介绍了FedFitTech基线，用于解决可穿戴设备在联邦学习中的数据不平衡、标签缺失等问题，并通过案例展示了其减少通信冗余的效果。", "motivation": "传统集中式学习在隐私和效率上存在问题，联邦学习（FL）为可穿戴设备提供了一种去中心化的解决方案。", "method": "提出FedFitTech基线，结合客户端早期停止策略，优化活动模式识别与个性化平衡。", "result": "系统减少了13%的冗余通信，识别性能仅下降1%。", "conclusion": "FedFitTech为FitTech研究提供了开源基础，推动了隐私感知应用的发展。"}}
{"id": "2506.16844", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6; I.5.1; G.3"], "pdf": "https://arxiv.org/pdf/2506.16844", "abs": "https://arxiv.org/abs/2506.16844", "authors": ["Victor Alejandre", "Concha Bielza", "Pedro Larrañaga"], "title": "Bandwidth Selectors on Semiparametric Bayesian Networks", "comment": "37 pages, 15 figures. Submitted to Information Sciences", "summary": "Semiparametric Bayesian networks (SPBNs) integrate parametric and\nnon-parametric probabilistic models, offering flexibility in learning complex\ndata distributions from samples. In particular, kernel density estimators\n(KDEs) are employed for the non-parametric component. Under the assumption of\ndata normality, the normal rule is used to learn the bandwidth matrix for the\nKDEs in SPBNs. This matrix is the key hyperparameter that controls the\ntrade-off between bias and variance. However, real-world data often deviates\nfrom normality, potentially leading to suboptimal density estimation and\nreduced predictive performance. This paper first establishes the theoretical\nframework for the application of state-of-the-art bandwidth selectors and\nsubsequently evaluates their impact on SPBN performance. We explore the\napproaches of cross-validation and plug-in selectors, assessing their\neffectiveness in enhancing the learning capability and applicability of SPBNs.\nTo support this investigation, we have extended the open-source package\nPyBNesian for SPBNs with the additional bandwidth selection techniques and\nconducted extensive experimental analyses. Our results demonstrate that the\nproposed bandwidth selectors leverage increasing information more effectively\nthan the normal rule, which, despite its robustness, stagnates with more data.\nIn particular, unbiased cross-validation generally outperforms the normal rule,\nhighlighting its advantage in high sample size scenarios.", "AI": {"tldr": "论文研究了半参数贝叶斯网络（SPBNs）中带宽选择方法对性能的影响，提出了交叉验证和插件选择器，并验证其优于传统正态规则。", "motivation": "现实数据常偏离正态分布，传统基于正态假设的带宽选择方法可能导致次优密度估计和预测性能下降。", "method": "提出并评估了交叉验证和插件带宽选择器，扩展了PyBNesian工具包以支持这些方法。", "result": "实验表明，交叉验证方法在高样本量场景下优于传统正态规则。", "conclusion": "新型带宽选择器能更有效地利用数据信息，提升SPBNs的学习能力和适用性。"}}
{"id": "2506.16846", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16846", "abs": "https://arxiv.org/abs/2506.16846", "authors": ["Antonio Consoloa", "Edoardo Amaldi", "Emilio Carrizosa"], "title": "Soft decision trees for survival analysis", "comment": null, "summary": "Decision trees are popular in survival analysis for their interpretability\nand ability to model complex relationships. Survival trees, which predict the\ntiming of singular events using censored historical data, are typically built\nthrough heuristic approaches. Recently, there has been growing interest in\nglobally optimized trees, where the overall tree is trained by minimizing the\nerror function over all its parameters. We propose a new soft survival tree\nmodel (SST), with a soft splitting rule at each branch node, trained via a\nnonlinear optimization formulation amenable to decomposition. Since SSTs\nprovide for every input vector a specific survival function associated to a\nsingle leaf node, they satisfy the conditional computation property and inherit\nthe related benefits. SST and the training formulation combine flexibility with\ninterpretability: any smooth survival function (parametric, semiparametric, or\nnonparametric) estimated through maximum likelihood can be used, and each leaf\nnode of an SST yields a cluster of distinct survival functions which are\nassociated to the data points routed to it. Numerical experiments on 15\nwell-known datasets show that SSTs, with parametric and spline-based\nsemiparametric survival functions, trained using an adaptation of the\nnode-based decomposition algorithm proposed by Consolo et al. (2024) for soft\nregression trees, outperform three benchmark survival trees in terms of four\nwidely-used discrimination and calibration measures. SSTs can also be extended\nto consider group fairness.", "AI": {"tldr": "提出了一种新的软生存树模型（SST），通过非线性优化训练，结合灵活性和可解释性，在15个数据集上表现优于基准生存树。", "motivation": "生存树在生存分析中因其可解释性和建模复杂关系的能力而受欢迎，但传统方法多基于启发式。全局优化树的兴起激发了新模型的探索。", "method": "SST采用软分割规则，通过非线性优化训练，支持任何平滑生存函数（参数、半参数或非参数），每个叶节点生成独特的生存函数簇。", "result": "在15个数据集上，SST在判别和校准指标上优于三种基准生存树。", "conclusion": "SST结合了灵活性和可解释性，可扩展至群体公平性，是一种有效的生存分析工具。"}}
{"id": "2506.16853", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16853", "abs": "https://arxiv.org/abs/2506.16853", "authors": ["Semin Kim", "Yeonwoo Cha", "Jaehoon Yoo", "Seunghoon Hong"], "title": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models", "comment": "28 pages, Under review", "summary": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, using\nup to 3.5 times less inference budget, and, given sufficient inference budget,\nachieves performance comparable to learning-based baselines that require\nreward-specific fine-tuning. The code is available at\nhttps://github.com/seminkim/RATTPO.", "AI": {"tldr": "RATTPO是一种通用的测试时提示优化方法，适用于多种奖励场景，无需修改，通过迭代搜索优化提示，显著提升搜索效率。", "motivation": "现有自动提示工程方法针对特定奖励配置，在新场景中表现不佳，需要一种灵活的方法适应不同奖励模型。", "method": "RATTPO通过查询大型语言模型（LLMs）迭代优化提示，利用优化轨迹和奖励感知反馈信号（“提示”）作为上下文。", "result": "RATTPO在多种奖励设置下有效提升提示质量，搜索效率比其他基线高3.5倍，性能接近需要微调的学习基线。", "conclusion": "RATTPO是一种高效、通用的提示优化方法，适用于多样化奖励场景，无需特定任务描述或微调。"}}
{"id": "2506.16855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16855", "abs": "https://arxiv.org/abs/2506.16855", "authors": ["Shaoyu Dou", "Kai Yang", "Yang Jiao", "Chengbo Qiu", "Kui Ren"], "title": "Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning", "comment": "16 pages, 14 figures. Published in IEEE Transactions on Dependable\n  and Secure Computing. arXiv admin note: substantial text overlap with\n  arXiv:2207.08159", "summary": "Time series analysis has achieved great success in cyber security such as\nintrusion detection and device identification. Learning similarities among\nmultiple time series is a crucial problem since it serves as the foundation for\ndownstream analysis. Due to the complex temporal dynamics of the\nevent-triggered time series, it often remains unclear which similarity metric\nis appropriate for security-related tasks, such as anomaly detection and\nclustering. The overarching goal of this paper is to develop an unsupervised\nlearning framework that is capable of learning similarities among a set of\nevent-triggered time series. From the machine learning vantage point, the\nproposed framework harnesses the power of both hierarchical multi-resolution\nsequential autoencoders and the Gaussian Mixture Model (GMM) to effectively\nlearn the low-dimensional representations from the time series. Finally, the\nobtained similarity measure can be easily visualized for the explanation. The\nproposed framework aspires to offer a stepping stone that gives rise to a\nsystematic approach to model and learn similarities among a multitude of\nevent-triggered time series. Through extensive qualitative and quantitative\nexperiments, it is revealed that the proposed method outperforms\nstate-of-the-art methods considerably.", "AI": {"tldr": "提出了一种无监督学习框架，用于学习事件触发时间序列之间的相似性，结合了分层多分辨率自编码器和GMM，性能优于现有方法。", "motivation": "由于事件触发时间序列的复杂动态性，现有相似性度量方法在安全相关任务中表现不佳，需要一种更有效的无监督学习框架。", "method": "结合分层多分辨率自编码器和GMM，学习时间序列的低维表示，并可视化相似性度量。", "result": "通过实验验证，该方法在性能上显著优于现有技术。", "conclusion": "该框架为建模和学习事件触发时间序列相似性提供了系统化方法，具有实际应用潜力。"}}
{"id": "2506.16862", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2506.16862", "abs": "https://arxiv.org/abs/2506.16862", "authors": ["Qian Qi"], "title": "Optimal Depth of Neural Networks", "comment": null, "summary": "Determining the optimal depth of a neural network is a fundamental yet\nchallenging problem, typically resolved through resource-intensive\nexperimentation. This paper introduces a formal theoretical framework to\naddress this question by recasting the forward pass of a deep network,\nspecifically a Residual Network (ResNet), as an optimal stopping problem. We\nmodel the layer-by-layer evolution of hidden representations as a sequential\ndecision process where, at each layer, a choice is made between halting\ncomputation to make a prediction or continuing to a deeper layer for a\npotentially more refined representation. This formulation captures the\nintrinsic trade-off between accuracy and computational cost. Our primary\ntheoretical contribution is a proof that, under a plausible condition of\ndiminishing returns on the residual functions, the expected optimal stopping\ndepth is provably finite, even in an infinite-horizon setting. We leverage this\ninsight to propose a novel and practical regularization term, $\\mathcal{L}_{\\rm\ndepth}$, that encourages the network to learn representations amenable to\nefficient, early exiting. We demonstrate the generality of our framework by\nextending it to the Transformer architecture and exploring its connection to\ncontinuous-depth models via free-boundary problems. Empirical validation on\nImageNet confirms that our regularizer successfully induces the theoretically\npredicted behavior, leading to significant gains in computational efficiency\nwithout compromising, and in some cases improving, final model accuracy.", "AI": {"tldr": "本文提出了一种理论框架，将深度神经网络的前向传播建模为最优停止问题，证明了在残差函数收益递减条件下，最优停止深度是有限的，并提出了一种新的正则化项以提高计算效率。", "motivation": "解决神经网络深度优化的挑战，避免资源密集型实验。", "method": "将ResNet的前向传播建模为最优停止问题，提出正则化项$\\mathcal{L}_{\\rm depth}$，并扩展到Transformer架构。", "result": "理论证明了最优停止深度的有限性，实验验证了正则化项在提高计算效率的同时保持或提升模型准确性。", "conclusion": "该框架为神经网络深度优化提供了理论支持，并通过正则化项实现了高效且准确的模型训练。"}}
{"id": "2506.16884", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16884", "abs": "https://arxiv.org/abs/2506.16884", "authors": ["Jacopo Graldi", "Alessandro Breccia", "Giulia Lanzillotta", "Thomas Hofmann", "Lorenzo Noci"], "title": "The Importance of Being Lazy: Scaling Limits of Continual Learning", "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (2025). JG and AB contributed equally to this work", "summary": "Despite recent efforts, neural networks still struggle to learn in\nnon-stationary environments, and our understanding of catastrophic forgetting\n(CF) is far from complete. In this work, we perform a systematic study on the\nimpact of model scale and the degree of feature learning in continual learning.\nWe reconcile existing contradictory observations on scale in the literature, by\ndifferentiating between lazy and rich training regimes through a variable\nparameterization of the architecture. We show that increasing model width is\nonly beneficial when it reduces the amount of feature learning, yielding more\nlaziness. Using the framework of dynamical mean field theory, we then study the\ninfinite width dynamics of the model in the feature learning regime and\ncharacterize CF, extending prior theoretical results limited to the lazy\nregime. We study the intricate relationship between feature learning, task\nnon-stationarity, and forgetting, finding that high feature learning is only\nbeneficial with highly similar tasks. We identify a transition modulated by\ntask similarity where the model exits an effectively lazy regime with low\nforgetting to enter a rich regime with significant forgetting. Finally, our\nfindings reveal that neural networks achieve optimal performance at a critical\nlevel of feature learning, which depends on task non-stationarity and transfers\nacross model scales. This work provides a unified perspective on the role of\nscale and feature learning in continual learning.", "AI": {"tldr": "论文研究了模型规模和特征学习程度对持续学习的影响，揭示了模型宽度增加仅在减少特征学习时有益，并通过理论框架分析了特征学习与任务非平稳性之间的关系。", "motivation": "理解神经网络在非平稳环境中的学习困难，尤其是灾难性遗忘（CF）的机制。", "method": "通过变量参数化架构区分懒惰和丰富训练模式，利用动态平均场理论分析无限宽度模型动态。", "result": "发现高特征学习仅对高度相似任务有益，并识别了任务相似性调制的过渡点。", "conclusion": "神经网络在特征学习的临界水平达到最佳性能，该水平取决于任务非平稳性，并适用于不同模型规模。"}}
{"id": "2506.16929", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16929", "abs": "https://arxiv.org/abs/2506.16929", "authors": ["Mohon Raihan", "Plabon Kumar Saha", "Rajan Das Gupta", "A Z M Tahmidul Kabir", "Afia Anjum Tamanna", "Md. Harun-Ur-Rashid", "Adnan Bin Abdus Salam", "Md Tanvir Anjum", "A Z M Ahteshamul Kabir"], "title": "A deep learning and machine learning approach to predict neonatal death in the context of São Paulo", "comment": null, "summary": "Neonatal death is still a concerning reality for underdeveloped and even some\ndeveloped countries. Worldwide data indicate that 26.693 babies out of 1,000\nbirths die, according to Macro Trades. To reduce this number, early prediction\nof endangered babies is crucial. Such prediction enables the opportunity to\ntake ample care of the child and mother so that early child death can be\navoided. In this context, machine learning was used to determine whether a\nnewborn baby is at risk. To train the predictive model, historical data of 1.4\nmillion newborns was used. Machine learning and deep learning techniques such\nas logical regression, K-nearest neighbor, random forest classifier, extreme\ngradient boosting (XGBoost), convolutional neural network, and long short-term\nmemory (LSTM) were implemented using the dataset to identify the most accurate\nmodel for predicting neonatal mortality. Among the machine learning algorithms,\nXGBoost and random forest classifier achieved the best accuracy with 94%, while\namong the deep learning models, LSTM delivered the highest accuracy with 99%.\nTherefore, using LSTM appears to be the most suitable approach to predict\nwhether precautionary measures for a child are necessary.", "AI": {"tldr": "使用机器学习和深度学习技术预测新生儿死亡风险，LSTM模型表现最佳，准确率达99%。", "motivation": "全球新生儿死亡率高，早期预测可降低死亡风险。", "method": "使用逻辑回归、KNN、随机森林、XGBoost、CNN和LSTM等算法，基于140万新生儿历史数据训练模型。", "result": "XGBoost和随机森林准确率94%，LSTM准确率99%。", "conclusion": "LSTM是预测新生儿死亡风险的最优方法。"}}
{"id": "2506.16965", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.16965", "abs": "https://arxiv.org/abs/2506.16965", "authors": ["Çağatay Demirel"], "title": "RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics", "comment": "32 pages, 1 graphical abstract, 7 figures, 9 tables, 2 supplementary\n  figures", "summary": "Ensemble learning remains a cornerstone of machine learning, with stacking\nused to integrate predictions from multiple base learners through a meta-model.\nHowever, deep stacking remains rare, as most designs prioritize horizontal\ndiversity over recursive depth due to model complexity, feature redundancy, and\ncomputational burden. To address these challenges, RocketStack, a level-aware\nrecursive ensemble framework, is introduced and explored up to ten stacking\nlevels, extending beyond prior architectures. The framework incrementally\nprunes weaker learners at each level, enabling deeper stacking without\nexcessive complexity. To mitigate early performance saturation, mild Gaussian\nnoise is added to out-of-fold (OOF) scores before pruning, and compared against\nstrict OOF pruning. Further both per-level and periodic feature compressions\nare explored using attention-based selection, Simple, Fast, Efficient (SFE)\nfilter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),\nlinear-trend tests confirmed rising accuracy with depth in most variants, and\nthe top performing meta-model at each level increasingly outperformed the\nstrongest standalone ensemble. In the binary subset, periodic SFE with mild\nOOF-score randomization reached 97.08% at level 10, 5.14% above the\nstrict-pruning configuration and cut runtime by 10.5% relative to no\ncompression. In the multi-class subset, periodic attention selection reached\n98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing\nruntime by 56.1% and feature dimensionality by 74% compared to no compression.\nThese findings highlight mild randomization as an effective regularizer and\nperiodic compression as a stabilizer. Echoing the design of multistage rockets\nin aerospace (prune, compress, propel) RocketStack achieves deep recursive\nensembling with tractable complexity.", "AI": {"tldr": "RocketStack是一种基于递归深度集成的框架，通过逐层修剪弱学习器、添加高斯噪声和特征压缩，实现了更深的堆叠集成，显著提升了性能。", "motivation": "解决传统堆叠集成中因模型复杂性、特征冗余和计算负担而难以实现深度集成的问题。", "method": "提出RocketStack框架，包括逐层修剪、OOF分数随机化和周期性特征压缩（如注意力选择、SFE过滤和自动编码器）。", "result": "在33个数据集上验证，深度集成显著提升性能，二进制和多类任务分别达到97.08%和98.60%的准确率，同时减少运行时间和特征维度。", "conclusion": "RocketStack通过修剪、压缩和推进策略，实现了高效的深度递归集成，为机器学习集成方法提供了新思路。"}}
{"id": "2506.16975", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.16975", "abs": "https://arxiv.org/abs/2506.16975", "authors": ["Guan Zhe Hong", "Bhavya Vasudeva", "Vatsal Sharan", "Cyrus Rashtchian", "Prabhakar Raghavan", "Rina Panigrahy"], "title": "Latent Concept Disentanglement in Transformer-based Language Models", "comment": null, "summary": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks.", "AI": {"tldr": "论文探讨了大型语言模型（LLM）在上下文学习（ICL）中是否能够理解和利用潜在概念，并通过实验验证了模型在离散和连续潜在概念任务中的表现。", "motivation": "研究动机是探究Transformer模型在ICL中是否真正理解潜在概念，还是仅通过捷径解决问题。", "method": "方法包括分析模型在2跳推理任务中的表现，以及连续潜在概念任务中的低维子空间表示。", "result": "结果显示模型能够识别离散潜在概念并逐步组合，同时在连续概念任务中表现出与参数化一致的几何结构。", "conclusion": "结论表明Transformer模型在ICL中存在局部化结构，能够解耦潜在概念，深化了对ICL和模型表示的理解。"}}
{"id": "2506.17007", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17007", "abs": "https://arxiv.org/abs/2506.17007", "authors": ["Marco Jiralerspong", "Esther Derman", "Danilo Vucetic", "Nikolay Malkin", "Bilun Sun", "Tianyu Zhang", "Pierre-Luc Bacon", "Gauthier Gidel"], "title": "Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators", "comment": null, "summary": "A major bottleneck in scientific discovery involves narrowing a large\ncombinatorial set of objects, such as proteins or molecules, to a small set of\npromising candidates. While this process largely relies on expert knowledge,\nrecent methods leverage reinforcement learning (RL) to enhance this filtering.\nThey achieve this by estimating proxy reward functions from available datasets\nand using regularization to generate more diverse candidates. These reward\nfunctions are inherently uncertain, raising a particularly salient challenge\nfor scientific discovery. In this work, we show that existing methods, often\nframed as sampling proportional to a reward function, are inadequate and yield\nsuboptimal candidates, especially in large search spaces. To remedy this issue,\nwe take a robust RL approach and introduce a unified operator that seeks\nrobustness to the uncertainty of the proxy reward function. This general\noperator targets peakier sampling distributions while encompassing known soft\nRL operators. It also leads us to a novel algorithm that identifies\nhigher-quality, diverse candidates in both synthetic and real-world tasks.\nUltimately, our work offers a new, flexible perspective on discrete\ncompositional generation tasks. Code: https://github.com/marcojira/tgm.", "AI": {"tldr": "论文提出了一种基于强化学习的鲁棒方法，用于在科学发现中优化组合对象的筛选过程。", "motivation": "现有方法在处理代理奖励函数的不确定性时表现不佳，尤其在大型搜索空间中效果更差。", "method": "采用鲁棒强化学习方法，引入统一算子以应对奖励函数的不确定性，并设计新算法。", "result": "新算法在合成和实际任务中均能生成更高质量且多样化的候选对象。", "conclusion": "研究为离散组合生成任务提供了灵活的新视角。"}}
{"id": "2506.17016", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.17016", "abs": "https://arxiv.org/abs/2506.17016", "authors": ["Giulia Bertazzini", "Chiara Albisani", "Daniele Baracchi", "Dasara Shullani", "Roberto Verdecchia"], "title": "The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation", "comment": null, "summary": "With the growing adoption of AI image generation, in conjunction with the\never-increasing environmental resources demanded by AI, we are urged to answer\na fundamental question: What is the environmental impact hidden behind each\nimage we generate? In this research, we present a comprehensive empirical\nexperiment designed to assess the energy consumption of AI image generation.\nOur experiment compares 17 state-of-the-art image generation models by\nconsidering multiple factors that could affect their energy consumption, such\nas model quantization, image resolution, and prompt length. Additionally, we\nconsider established image quality metrics to study potential trade-offs\nbetween energy consumption and generated image quality. Results show that image\ngeneration models vary drastically in terms of the energy they consume, with up\nto a 46x difference. Image resolution affects energy consumption\ninconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.\nU-Net-based models tend to consume less than Transformer-based one. Model\nquantization instead results to deteriorate the energy efficiency of most\nmodels, while prompt length and content have no statistically significant\nimpact. Improving image quality does not always come at the cost of a higher\nenergy consumption, with some of the models producing the highest quality\nimages also being among the most energy efficient ones.", "AI": {"tldr": "该研究通过实验评估了17种AI图像生成模型的能耗，发现能耗差异显著（最高达46倍），并探讨了分辨率、模型类型等因素的影响。", "motivation": "随着AI图像生成的普及及其对环境的资源需求增加，研究旨在揭示生成每张图像背后的环境影响。", "method": "通过比较17种先进图像生成模型，分析模型量化、分辨率、提示长度等因素对能耗的影响，并结合图像质量指标。", "result": "能耗差异显著，U-Net模型更节能，模型量化降低能效，分辨率影响不一致，提示长度无显著影响。高质量图像不一定能耗更高。", "conclusion": "研究为AI图像生成的能耗提供了实证数据，表明优化模型设计可兼顾能效与图像质量。"}}
{"id": "2506.17029", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17029", "abs": "https://arxiv.org/abs/2506.17029", "authors": ["Leizhen Wang", "Peibo Duan", "Cheng Lyu", "Zewen Wang", "Zhiqiang He", "Nan Zheng", "Zhenliang Ma"], "title": "Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment", "comment": null, "summary": "The evolution of metropolitan cities and the increase in travel demands\nimpose stringent requirements on traffic assignment methods. Multi-agent\nreinforcement learning (MARL) approaches outperform traditional methods in\nmodeling adaptive routing behavior without requiring explicit system dynamics,\nwhich is beneficial for real-world deployment. However, MARL frameworks face\nchallenges in scalability and reliability when managing extensive networks with\nsubstantial travel demand, which limiting their practical applicability in\nsolving large-scale traffic assignment problems. To address these challenges,\nthis study introduces MARL-OD-DA, a new MARL framework for the traffic\nassignment problem, which redefines agents as origin-destination (OD) pair\nrouters rather than individual travelers, significantly enhancing scalability.\nAdditionally, a Dirichlet-based action space with action pruning and a reward\nfunction based on the local relative gap are designed to enhance solution\nreliability and improve convergence efficiency. Experiments demonstrate that\nthe proposed MARL framework effectively handles medium-sized networks with\nextensive and varied city-level OD demand, surpassing existing MARL methods.\nWhen implemented in the SiouxFalls network, MARL-OD-DA achieves better\nassignment solutions in 10 steps, with a relative gap that is 94.99% lower than\nthat of conventional methods.", "AI": {"tldr": "提出了一种新的多智能体强化学习框架MARL-OD-DA，通过将智能体定义为OD对路由器而非个体旅行者，显著提升了可扩展性，并设计了基于Dirichlet的动作空间和局部相对差距的奖励函数以提高可靠性和收敛效率。", "motivation": "大都市交通需求的增长对交通分配方法提出了更高要求，传统方法在建模自适应路由行为时需明确系统动态，而MARL方法虽优于传统方法，但在大规模网络中面临可扩展性和可靠性挑战。", "method": "MARL-OD-DA框架将智能体定义为OD对路由器，设计了基于Dirichlet的动作空间和局部相对差距的奖励函数。", "result": "实验表明，MARL-OD-DA能有效处理中等规模网络，并在SiouxFalls网络中比传统方法的相对差距降低了94.99%。", "conclusion": "MARL-OD-DA框架显著提升了MARL在大规模交通分配问题中的实用性和性能。"}}
{"id": "2506.17035", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17035", "abs": "https://arxiv.org/abs/2506.17035", "authors": ["João Matos", "Ben Van Calster", "Leo Anthony Celi", "Paula Dhiman", "Judy Wawira Gichoya", "Richard D. Riley", "Chris Russell", "Sara Khalid", "Gary S. Collins"], "title": "Critical Appraisal of Fairness Metrics in Clinical Predictive AI", "comment": "32 pages, 1 figure, 2 tables, 5 boxes, 4 linked supplementary\n  materials", "summary": "Predictive artificial intelligence (AI) offers an opportunity to improve\nclinical practice and patient outcomes, but risks perpetuating biases if\nfairness is inadequately addressed. However, the definition of \"fairness\"\nremains unclear. We conducted a scoping review to identify and critically\nappraise fairness metrics for clinical predictive AI. We defined a \"fairness\nmetric\" as a measure quantifying whether a model discriminates (societally)\nagainst individuals or groups defined by sensitive attributes. We searched five\ndatabases (2014-2024), screening 820 records, to include 41 studies, and\nextracted 62 fairness metrics. Metrics were classified by\nperformance-dependency, model output level, and base performance metric,\nrevealing a fragmented landscape with limited clinical validation and\noverreliance on threshold-dependent measures. Eighteen metrics were explicitly\ndeveloped for healthcare, including only one clinical utility metric. Our\nfindings highlight conceptual challenges in defining and quantifying fairness\nand identify gaps in uncertainty quantification, intersectionality, and\nreal-world applicability. Future work should prioritise clinically meaningful\nmetrics.", "AI": {"tldr": "该论文通过范围综述分析了临床预测AI中的公平性度量，揭示了定义模糊、验证不足及过度依赖阈值的问题。", "motivation": "探讨如何通过公平性度量改善临床预测AI，避免偏见延续。", "method": "对2014-2024年的820篇文献进行筛选，提取并分类62种公平性度量。", "result": "发现公平性度量碎片化、临床验证有限，且仅18种专为医疗设计。", "conclusion": "需优先开发具有临床意义的公平性度量，解决定义和量化挑战。"}}
{"id": "2506.17039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17039", "abs": "https://arxiv.org/abs/2506.17039", "authors": ["Elizabeth Fons", "Alejandro Sztrajman", "Yousef El-Laham", "Luciana Ferrer", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation", "comment": "In ICML 2025", "summary": "Time series with missing or irregularly sampled data are a persistent\nchallenge in machine learning. Many methods operate on the frequency-domain,\nrelying on the Fast Fourier Transform (FFT) which assumes uniform sampling,\ntherefore requiring prior interpolation that can distort the spectra. To\naddress this limitation, we introduce a differentiable Lomb--Scargle layer that\nenables a reliable computation of the power spectrum of irregularly sampled\ndata. We integrate this layer into a novel score-based diffusion model (LSCD)\nfor time series imputation conditioned on the entire signal spectrum.\nExperiments on synthetic and real-world benchmarks demonstrate that our method\nrecovers missing data more accurately than purely time-domain baselines, while\nsimultaneously producing consistent frequency estimates. Crucially, our method\ncan be easily integrated into learning frameworks, enabling broader adoption of\nspectral guidance in machine learning approaches involving incomplete or\nirregular data.", "AI": {"tldr": "提出了一种可微分的Lomb-Scargle层，结合扩散模型（LSCD），用于处理不规则采样时间序列数据，优于传统方法。", "motivation": "解决时间序列数据缺失或不规则采样的问题，避免传统FFT方法因插值导致的频谱失真。", "method": "引入可微分的Lomb-Scargle层计算不规则数据功率谱，结合基于分数的扩散模型（LSCD）进行数据填补。", "result": "在合成和真实数据上表现优于纯时域基线，同时提供一致的频率估计。", "conclusion": "该方法易于集成到学习框架中，为涉及不完整或不规则数据的机器学习方法提供频谱指导。"}}
{"id": "2506.17041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17041", "abs": "https://arxiv.org/abs/2506.17041", "authors": ["Joshua Schraven", "Alexander Windmann", "Oliver Niggemann"], "title": "MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection", "comment": "11 pages, 3 figures", "summary": "Benchmark datasets for network intrusion detection commonly rely on\nsynthetically generated traffic, which fails to reflect the statistical\nvariability and temporal drift encountered in operational environments. This\npaper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1\ndataset, designed to enable realistic and reproducible evaluation of anomaly\ndetection methods. A reproducible preprocessing pipeline is presented that\ntransforms raw packet captures into flow representations conforming to the\nCICFlowMeter format, while preserving MAWILab's original anomaly labels. The\nresulting datasets comprise temporally distinct samples from January 2011,\n2016, and 2021, drawn from trans-Pacific backbone traffic.\n  To establish reference baselines, traditional machine learning methods,\nincluding Decision Trees, Random Forests, XGBoost, and Logistic Regression, are\ncompared to a deep learning model based on a CNN-BiLSTM architecture. Empirical\nresults demonstrate that tree-based classifiers perform well on temporally\nstatic data but experience significant performance degradation over time. In\ncontrast, the CNN-BiLSTM model maintains better performance, thus showing\nimproved generalization. These findings underscore the limitations of synthetic\nbenchmarks and static models, and motivate the adoption of realistic datasets\nwith explicit temporal structure. All datasets, pipeline code, and model\nimplementations are made publicly available to foster transparency and\nreproducibility.", "AI": {"tldr": "MAWIFlow是一个基于MAWILab v1.1数据集的流量基准，用于真实且可复现的异常检测方法评估。研究发现，传统机器学习方法在静态数据上表现良好，但随时间性能下降，而CNN-BiLSTM模型表现更稳定。", "motivation": "现有网络入侵检测基准数据集多依赖合成流量，无法反映实际环境中的统计变异和时间漂移。", "method": "提出MAWIFlow数据集，并提供可复现的预处理流程，将原始数据包转换为CICFlowMeter格式的流表示。比较了传统机器学习方法和CNN-BiLSTM模型。", "result": "树基分类器在静态数据上表现良好，但随时间性能下降；CNN-BiLSTM模型表现更稳定，泛化能力更强。", "conclusion": "合成基准和静态模型存在局限性，需采用具有明确时间结构的真实数据集。所有数据和代码公开以促进透明性和可复现性。"}}
{"id": "2506.17047", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17047", "abs": "https://arxiv.org/abs/2506.17047", "authors": ["Haolin Liu", "Adrien Siproudhis", "Samuel Experton", "Peter Lorenz", "Christina Boura", "Thomas Peyrin"], "title": "Navigating the Deep: Signature Extraction on Deep Neural Networks", "comment": "26 pages", "summary": "Neural network model extraction has emerged in recent years as an important\nsecurity concern, as adversaries attempt to recover a network's parameters via\nblack-box queries. A key step in this process is signature extraction, which\naims to recover the absolute values of the network's weights layer by layer.\nPrior work, notably by Carlini et al. (2020), introduced a technique inspired\nby differential cryptanalysis to extract neural network parameters. However,\ntheir method suffers from several limitations that restrict its applicability\nto networks with a few layers only. Later works focused on improving sign\nextraction, but largely relied on the assumption that signature extraction\nitself was feasible.\n  In this work, we revisit and refine the signature extraction process by\nsystematically identifying and addressing for the first time critical\nlimitations of Carlini et al.'s signature extraction method. These limitations\ninclude rank deficiency and noise propagation from deeper layers. To overcome\nthese challenges, we propose efficient algorithmic solutions for each of the\nidentified issues, greatly improving the efficiency of signature extraction.\nOur approach permits the extraction of much deeper networks than was previously\npossible. We validate our method through extensive experiments on ReLU-based\nneural networks, demonstrating significant improvements in extraction depth and\naccuracy. For instance, our extracted network matches the target network on at\nleast 95% of the input space for each of the eight layers of a neural network\ntrained on the CIFAR-10 dataset, while previous works could barely extract the\nfirst three layers. Our results represent a crucial step toward practical\nattacks on larger and more complex neural network architectures.", "AI": {"tldr": "本文改进了神经网络模型提取中的签名提取方法，解决了Carlini等人方法的局限性，显著提高了提取深度和准确性。", "motivation": "神经网络模型提取是重要的安全问题，但现有方法（如Carlini等人的方法）存在局限性，仅适用于浅层网络。本文旨在解决这些限制。", "method": "提出高效算法解决签名提取中的秩不足和噪声传播问题，改进提取效率。", "result": "实验验证表明，新方法能提取更深层网络（如CIFAR-10数据集的8层网络），准确率高达95%，远超之前仅能提取3层的结果。", "conclusion": "本文方法为攻击更大、更复杂的神经网络架构提供了关键进展。"}}
{"id": "2506.17052", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.17052", "abs": "https://arxiv.org/abs/2506.17052", "authors": ["Jingtong Su", "Julia Kempe", "Karen Ullrich"], "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers", "comment": null, "summary": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet.", "AI": {"tldr": "论文提出了一种名为SAMD的方法，用于将复杂概念映射到Transformer模型的特定注意力头上，并通过SAMI策略调整这些模块的效果。实验表明该方法在语言和视觉任务中均有效。", "motivation": "当前Transformer模型的解释性研究主要关注简单概念，忽视了注意力机制的作用，且缺乏统一的分析方法。本文旨在填补这些空白。", "method": "提出SAMD方法，将概念表示为向量并与注意力头计算余弦相似度，选择TopK头构建概念相关模块；提出SAMI策略，通过标量参数调整模块效果。", "result": "实验证明模块位置在训练前后稳定，SAMI能显著提升或抑制特定概念的效果（如安全性和推理能力）。", "conclusion": "SAMD和SAMI为Transformer模型的解释性和行为控制提供了通用且有效的方法。"}}
{"id": "2506.17065", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17065", "abs": "https://arxiv.org/abs/2506.17065", "authors": ["Abdellah Rahmani", "Pascal Frossard"], "title": "Flow-Based Non-stationary Temporal Regime Causal Structure Learning", "comment": null, "summary": "Understanding causal relationships in multivariate time series is crucial in\nmany scenarios, such as those dealing with financial or neurological data. Many\nsuch time series exhibit multiple regimes, i.e., consecutive temporal segments\nwith a priori unknown boundaries, with each regime having its own causal\nstructure. Inferring causal dependencies and regime shifts is critical for\nanalyzing the underlying processes. However, causal structure learning in this\nsetting is challenging due to (1) non stationarity, i.e., each regime can have\nits own causal graph and mixing function, and (2) complex noise distributions,\nwhich may be non Gaussian or heteroscedastic. Existing causal discovery\napproaches cannot address these challenges, since generally assume stationarity\nor Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified\nframework for causal discovery that handles non stationary processes along with\nnon Gaussian and heteroscedastic noises. FANTOM simultaneously infers the\nnumber of regimes and their corresponding indices and learns each regime's\nDirected Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm\nthat maximizes the evidence lower bound of the data log likelihood. On the\ntheoretical side, we prove, under mild assumptions, that temporal\nheteroscedastic causal models, introduced in FANTOM's formulation, are\nidentifiable in both stationary and non stationary settings. In addition,\nextensive experiments on synthetic and real data show that FANTOM outperforms\nexisting methods.", "AI": {"tldr": "FANTOM是一个统一框架，用于处理非平稳过程和非高斯、异方差噪声的因果发现，同时推断多个因果结构和时间段的边界。", "motivation": "多变量时间序列中的因果关系分析在金融和神经科学等领域至关重要，但现有方法无法处理非平稳性和复杂噪声分布。", "method": "FANTOM使用贝叶斯期望最大化算法，最大化数据对数似然的证据下界，同时推断时间段的数量和对应的因果图。", "result": "理论证明FANTOM在平稳和非平稳设置下均可识别，实验表明其在合成和真实数据上优于现有方法。", "conclusion": "FANTOM为复杂时间序列中的因果发现提供了有效解决方案。"}}
{"id": "2506.17067", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.17067", "abs": "https://arxiv.org/abs/2506.17067", "authors": ["Zhuo Xu", "Tianyue Zheng", "Linglong Dai"], "title": "Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions", "comment": null, "summary": "The low-altitude economy (LAE) is gaining significant attention from academia\nand industry. Fortunately, LAE naturally aligns with near-field communications\nin extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field\nbeamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,\nwhile the additional distance dimension boosts overall spectrum efficiency.\nHowever, near-field communications in LAE still face several challenges, such\nas the increase in signal processing complexity and the necessity of\ndistinguishing between far and near-field users. Inspired by the large language\nmodels (LLM) with powerful ability to handle complex problems, we apply LLM to\nsolve challenges of near-field communications in LAE. The objective of this\narticle is to provide a comprehensive analysis and discussion on LLM-empowered\nnear-field communications in LAE. Specifically, we first introduce fundamentals\nof LLM and near-field communications, including the key advantages of LLM and\nkey characteristics of near-field communications. Then, we reveal the\nopportunities and challenges of near-field communications in LAE. To address\nthese challenges, we present a LLM-based scheme for near-field communications\nin LAE, and provide a case study which jointly distinguishes far and near-field\nusers and designs multi-user precoding matrix. Finally, we outline and\nhighlight several future research directions and open issues.", "AI": {"tldr": "本文探讨了低空经济（LAE）与超大规模MIMO（XL-MIMO）系统中的近场通信的结合，提出利用大语言模型（LLM）解决近场通信中的挑战，并提供了一个案例研究。", "motivation": "低空经济（LAE）与近场通信的结合面临信号处理复杂度增加和用户区分等挑战，LLM因其处理复杂问题的能力被引入以解决这些问题。", "method": "介绍了LLM和近场通信的基础知识，提出了一种基于LLM的近场通信方案，并通过案例研究展示了多用户预编码矩阵的设计。", "result": "LLM为解决近场通信中的挑战提供了新思路，案例研究验证了其可行性。", "conclusion": "LLM在LAE的近场通信中具有潜力，未来研究应进一步探索其应用和开放性问题。"}}
{"id": "2506.17093", "categories": ["cs.LG", "cs.AI", "math.AG", "stat.ML", "68T07, 62R01, 15A69, 14M99"], "pdf": "https://arxiv.org/pdf/2506.17093", "abs": "https://arxiv.org/abs/2506.17093", "authors": ["Konstantin Usevich", "Clara Dérand", "Ricardo Borsoi", "Marianne Clausel"], "title": "Identifiability of Deep Polynomial Neural Networks", "comment": "1 figure", "summary": "Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric\nstructure. However, their identifiability -- a key property for ensuring\ninterpretability -- remains poorly understood. In this work, we present a\ncomprehensive analysis of the identifiability of deep PNNs, including\narchitectures with and without bias terms. Our results reveal an intricate\ninterplay between activation degrees and layer widths in achieving\nidentifiability. As special cases, we show that architectures with\nnon-increasing layer widths are generically identifiable under mild conditions,\nwhile encoder-decoder networks are identifiable when the decoder widths do not\ngrow too rapidly. Our proofs are constructive and center on a connection\nbetween deep PNNs and low-rank tensor decompositions, and Kruskal-type\nuniqueness theorems. This yields both generic conditions determined by the\narchitecture, and effective conditions that depend on the network's parameters.\nWe also settle an open conjecture on the expected dimension of PNN's\nneurovarieties, and provide new bounds on the activation degrees required for\nit to reach its maximum.", "AI": {"tldr": "本文全面分析了深度多项式神经网络（PNNs）的可识别性，揭示了激活度与层宽度之间的复杂关系，并解决了关于PNNs神经变种预期维度的开放猜想。", "motivation": "多项式神经网络（PNNs）具有丰富的代数和几何结构，但其可识别性（确保可解释性的关键属性）仍未被充分理解。", "method": "通过将深度PNNs与低秩张量分解及Kruskal型唯一性定理联系起来，提出了构造性证明。", "result": "结果显示，在温和条件下，层宽度非递增的架构通常可识别，而解码器宽度增长不过快的编码器-解码器网络也可识别。", "conclusion": "研究不仅提供了由架构决定的通用条件，还给出了依赖于网络参数的有效条件，并解决了关于PNNs神经变种维度的开放问题。"}}
{"id": "2506.17103", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17103", "abs": "https://arxiv.org/abs/2506.17103", "authors": ["Shruti Sadanand Dongare", "Amun Kharel", "Jonathan Samuel", "Xiaona Zhou"], "title": "TransDreamerV3: Implanting Transformer In DreamerV3", "comment": null, "summary": "This paper introduces TransDreamerV3, a reinforcement learning model that\nenhances the DreamerV3 architecture by integrating a transformer encoder. The\nmodel is designed to improve memory and decision-making capabilities in complex\nenvironments. We conducted experiments on Atari-Boxing, Atari-Freeway,\nAtari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved\nperformance over DreamerV3, particularly in the Atari-Freeway and Crafter\ntasks. While issues in the Minecraft task and limited training across all tasks\nwere noted, TransDreamerV3 displays advancement in world model-based\nreinforcement learning, leveraging transformer architectures.", "AI": {"tldr": "TransDreamerV3通过集成Transformer编码器改进了DreamerV3架构，提升了复杂环境中的记忆和决策能力。", "motivation": "增强DreamerV3在复杂环境中的性能，特别是在记忆和决策方面。", "method": "在DreamerV3架构中集成Transformer编码器，并在多个任务（如Atari和Crafter）上进行实验。", "result": "在Atari-Freeway和Crafter任务中表现优于DreamerV3，但在Minecraft任务中存在不足。", "conclusion": "TransDreamerV3展示了基于世界模型的强化学习结合Transformer架构的进步。"}}
{"id": "2506.17128", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17128", "abs": "https://arxiv.org/abs/2506.17128", "authors": ["Botao Zhu", "Xianbin Wang"], "title": "Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model", "comment": null, "summary": "Trust is emerging as an effective tool to ensure the successful completion of\ncollaborative tasks within collaborative systems. However, rapidly and\ncontinuously evaluating the trustworthiness of collaborators during task\nexecution is a significant challenge due to distributed devices, complex\noperational environments, and dynamically changing resources. To tackle this\nchallenge, this paper proposes a Siamese-enabled rapid and continuous trust\nevaluation framework (SRCTE) to facilitate effective task collaboration. First,\nthe communication and computing resource attributes of the collaborator in a\ntrusted state, along with historical collaboration data, are collected and\nrepresented using an attributed control flow graph (ACFG) that captures\ntrust-related semantic information and serves as a reference for comparison\nwith data collected during task execution. At each time slot of task execution,\nthe collaborator's communication and computing resource attributes, as well as\ntask completion effectiveness, are collected in real time and represented with\nan ACFG to convey their trust-related semantic information. A Siamese model,\nconsisting of two shared-parameter Structure2vec networks, is then employed to\nlearn the deep semantics of each pair of ACFGs and generate their embeddings.\nFinally, the similarity between the embeddings of each pair of ACFGs is\ncalculated to determine the collaborator's trust value at each time slot. A\nreal system is built using two Dell EMC 5200 servers and a Google Pixel 8 to\ntest the effectiveness of the proposed SRCTE framework. Experimental results\ndemonstrate that SRCTE converges rapidly with only a small amount of data and\nachieves a high anomaly trust detection rate compared to the baseline\nalgorithm.", "AI": {"tldr": "论文提出了一种基于Siamese网络的快速连续信任评估框架（SRCTE），用于协作系统中动态评估合作者的可信度。", "motivation": "协作系统中，快速连续评估合作者的可信度面临分布式设备、复杂环境和动态资源的挑战。", "method": "使用属性控制流图（ACFG）表示合作者的资源属性和历史数据，通过Siamese模型学习ACFG的语义并生成嵌入，计算相似度以评估信任值。", "result": "实验表明，SRCTE能快速收敛且异常信任检测率高。", "conclusion": "SRCTE框架有效解决了协作系统中的动态信任评估问题。"}}
{"id": "2506.17139", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17139", "abs": "https://arxiv.org/abs/2506.17139", "authors": ["Michael Plainer", "Hao Wu", "Leon Klein", "Stephan Günnemann", "Frank Noé"], "title": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models", "comment": null, "summary": "Diffusion models have recently gained significant attention due to their\neffectiveness in various scientific domains, including biochemistry. When\ntrained on equilibrium molecular distributions, diffusion models provide both:\na generative procedure to sample equilibrium conformations and associated\nforces derived from the model's scores. However, using the forces for\ncoarse-grained molecular dynamics simulations uncovers inconsistencies in the\nsamples generated via classical diffusion inference and simulation, despite\nboth originating from the same model. Particularly at the small diffusion\ntimesteps required for simulations, diffusion models fail to satisfy the\nFokker-Planck equation, which governs how the score should evolve over time. We\ninterpret this deviation as an indication of the observed inconsistencies and\npropose an energy-based diffusion model with a Fokker-Planck-derived\nregularization term enforcing consistency. We demonstrate the effectiveness of\nour approach on toy systems, alanine dipeptide, and introduce a\nstate-of-the-art transferable Boltzmann emulator for dipeptides that supports\nsimulation and demonstrates enhanced consistency and efficient sampling.", "AI": {"tldr": "扩散模型在生物化学等领域表现优异，但在分子动力学模拟中存在不一致性。作者提出了一种基于能量的扩散模型，通过Fokker-Planck正则化提高一致性，并在多个系统中验证了其有效性。", "motivation": "扩散模型在生成平衡分子构象和相关力方面表现出色，但在模拟中与经典扩散推理结果不一致，尤其是在小时间步长下不满足Fokker-Planck方程。", "method": "提出了一种基于能量的扩散模型，引入Fokker-Planck正则化项以确保一致性。", "result": "在玩具系统、丙氨酸二肽等实验中验证了方法的有效性，并开发了一种支持模拟的先进Boltzmann模拟器。", "conclusion": "通过Fokker-Planck正则化，扩散模型在模拟中的一致性和采样效率得到显著提升。"}}
{"id": "2506.17155", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17155", "abs": "https://arxiv.org/abs/2506.17155", "authors": ["Samin Yeasar Arnob", "Scott Fujimoto", "Doina Precup"], "title": "Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity", "comment": null, "summary": "In this paper, we investigate the use of small datasets in the context of\noffline reinforcement learning (RL). While many common offline RL benchmarks\nemploy datasets with over a million data points, many offline RL applications\nrely on considerably smaller datasets. We show that offline RL algorithms can\noverfit on small datasets, resulting in poor performance. To address this\nchallenge, we introduce \"Sparse-Reg\": a regularization technique based on\nsparsity to mitigate overfitting in offline reinforcement learning, enabling\neffective learning in limited data settings and outperforming state-of-the-art\nbaselines in continuous control.", "AI": {"tldr": "论文研究了小数据集在离线强化学习中的应用，提出了一种基于稀疏性的正则化方法（Sparse-Reg）以解决过拟合问题。", "motivation": "许多离线强化学习应用依赖小数据集，但现有算法在小数据集上容易过拟合，导致性能下降。", "method": "引入Sparse-Reg，一种基于稀疏性的正则化技术，用于减少离线强化学习中的过拟合。", "result": "Sparse-Reg在有限数据设置下表现优异，优于现有基线方法。", "conclusion": "Sparse-Reg能有效解决小数据集中的过拟合问题，提升离线强化学习性能。"}}
{"id": "2506.17171", "categories": ["cs.LG", "68T07"], "pdf": "https://arxiv.org/pdf/2506.17171", "abs": "https://arxiv.org/abs/2506.17171", "authors": ["Vitalii Bondar", "Vira Babenko", "Roman Trembovetskyi", "Yurii Korobeinyk", "Viktoriya Dzyuba"], "title": "Deep generative models as the probability transformation functions", "comment": "12 pages, 6 figures, accepted for publication in \"ICIST 2025 Springer\n  Proceedings\"", "summary": "This paper introduces a unified theoretical perspective that views deep\ngenerative models as probability transformation functions. Despite the apparent\ndifferences in architecture and training methodologies among various types of\ngenerative models - autoencoders, autoregressive models, generative adversarial\nnetworks, normalizing flows, diffusion models, and flow matching - we\ndemonstrate that they all fundamentally operate by transforming simple\npredefined distributions into complex target data distributions. This unifying\nperspective facilitates the transfer of methodological improvements between\nmodel architectures and provides a foundation for developing universal\ntheoretical approaches, potentially leading to more efficient and effective\ngenerative modeling techniques.", "AI": {"tldr": "论文提出了一种统一的理论视角，将深度生成模型视为概率变换函数，揭示了不同生成模型的共性。", "motivation": "尽管生成模型在架构和训练方法上存在差异，但论文旨在揭示它们的共同本质，以促进方法改进和理论发展。", "method": "通过将生成模型视为从简单分布到复杂数据分布的概率变换函数，统一分析了多种模型（如自编码器、GAN、扩散模型等）。", "result": "证明了不同生成模型在本质上具有相同的操作方式，为方法迁移和理论统一提供了基础。", "conclusion": "这一统一视角有助于改进生成模型技术，并可能推动更高效、更有效的建模方法的发展。"}}
{"id": "2506.17182", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17182", "abs": "https://arxiv.org/abs/2506.17182", "authors": ["Yuli Slavutsky", "Ozgur Beker", "David Blei", "Bianca Dumitrascu"], "title": "Variational Learning of Disentangled Representations", "comment": null, "summary": "Disentangled representations enable models to separate factors of variation\nthat are shared across experimental conditions from those that are\ncondition-specific. This separation is essential in domains such as biomedical\ndata analysis, where generalization to new treatments, patients, or species\ndepends on isolating stable biological signals from context-dependent effects.\nWhile extensions of the variational autoencoder (VAE) framework have been\nproposed to address this problem, they frequently suffer from leakage between\nlatent representations, limiting their ability to generalize to unseen\nconditions. Here, we introduce DISCoVeR, a new variational framework that\nexplicitly separates condition-invariant and condition-specific factors.\nDISCoVeR integrates three key components: (i) a dual-latent architecture that\nmodels shared and specific factors separately; (ii) two parallel\nreconstructions that ensure both representations remain informative; and (iii)\na novel max-min objective that encourages clean separation without relying on\nhandcrafted priors, while making only minimal assumptions. Theoretically, we\nshow that this objective maximizes data likelihood while promoting\ndisentanglement, and that it admits a unique equilibrium. Empirically, we\ndemonstrate that DISCoVeR achieves improved disentanglement on synthetic\ndatasets, natural images, and single-cell RNA-seq data. Together, these results\nestablish DISCoVeR as a principled approach for learning disentangled\nrepresentations in multi-condition settings.", "AI": {"tldr": "DISCoVeR是一种新的变分框架，通过双潜在架构和最大-最小目标，有效分离条件不变和条件特定的因素，提升了多条件下解耦表示的学习能力。", "motivation": "在生物医学数据分析等领域，解耦表示对于泛化到新条件至关重要，但现有方法存在潜在表示泄漏问题。", "method": "DISCoVeR采用双潜在架构、并行重建和最大-最小目标，无需手工先验即可实现干净分离。", "result": "在合成数据集、自然图像和单细胞RNA-seq数据上，DISCoVeR表现出更好的解耦性能。", "conclusion": "DISCoVeR为多条件下学习解耦表示提供了一种理论严谨且实用的方法。"}}
{"id": "2506.17187", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.17187", "abs": "https://arxiv.org/abs/2506.17187", "authors": ["Kanumuri Nithin Varma", "Babak Hassibi"], "title": "Optimal Implicit Bias in Linear Regression", "comment": null, "summary": "Most modern learning problems are over-parameterized, where the number of\nlearnable parameters is much greater than the number of training data points.\nIn this over-parameterized regime, the training loss typically has infinitely\nmany global optima that completely interpolate the data with varying\ngeneralization performance. The particular global optimum we converge to\ndepends on the implicit bias of the optimization algorithm. The question we\naddress in this paper is, ``What is the implicit bias that leads to the best\ngeneralization performance?\". To find the optimal implicit bias, we provide a\nprecise asymptotic analysis of the generalization performance of interpolators\nobtained from the minimization of convex functions/potentials for\nover-parameterized linear regression with non-isotropic Gaussian data. In\nparticular, we obtain a tight lower bound on the best generalization error\npossible among this class of interpolators in terms of the\nover-parameterization ratio, the variance of the noise in the labels, the\neigenspectrum of the data covariance, and the underlying distribution of the\nparameter to be estimated. Finally, we find the optimal convex implicit bias\nthat achieves this lower bound under certain sufficient conditions involving\nthe log-concavity of the distribution of a Gaussian convolved with the prior of\nthe true underlying parameter.", "AI": {"tldr": "论文研究了过参数化学习问题中优化算法的隐式偏差对泛化性能的影响，并通过凸函数最小化分析了线性回归中最佳泛化误差的下界。", "motivation": "探讨在过参数化情况下，哪种隐式偏差能带来最佳泛化性能。", "method": "通过凸函数/势能的最小化，对非各向同性高斯数据的过参数化线性回归进行渐近分析。", "result": "得出了最佳泛化误差的紧下界，并找到了在某些条件下能实现该下界的最优凸隐式偏差。", "conclusion": "在特定条件下，最优凸隐式偏差能够实现最佳泛化性能。"}}
{"id": "2506.17204", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17204", "abs": "https://arxiv.org/abs/2506.17204", "authors": ["Guozheng Ma", "Lu Li", "Zilin Wang", "Li Shen", "Pierre-Luc Bacon", "Dacheng Tao"], "title": "Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning", "comment": "Accepted to ICML 2025", "summary": "Effectively scaling up deep reinforcement learning models has proven\nnotoriously difficult due to network pathologies during training, motivating\nvarious targeted interventions such as periodic reset and architectural\nadvances such as layer normalization. Instead of pursuing more complex\nmodifications, we show that introducing static network sparsity alone can\nunlock further scaling potential beyond their dense counterparts with\nstate-of-the-art architectures. This is achieved through simple one-shot random\npruning, where a predetermined percentage of network weights are randomly\nremoved once before training. Our analysis reveals that, in contrast to naively\nscaling up dense DRL networks, such sparse networks achieve both higher\nparameter efficiency for network expressivity and stronger resistance to\noptimization challenges like plasticity loss and gradient interference. We\nfurther extend our evaluation to visual and streaming RL scenarios,\ndemonstrating the consistent benefits of network sparsity.", "AI": {"tldr": "通过一次性随机稀疏化（随机剪枝）提升深度强化学习模型的扩展性，优于密集网络。", "motivation": "深度强化学习模型在扩展时易出现网络训练问题，现有方法如周期性重置和层归一化复杂且效果有限。", "method": "采用一次性随机剪枝，训练前随机移除部分网络权重。", "result": "稀疏网络在参数效率和优化稳定性上优于密集网络，适用于视觉和流式强化学习场景。", "conclusion": "静态网络稀疏化是一种简单有效的方法，可显著提升深度强化学习的扩展潜力。"}}
{"id": "2506.17211", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17211", "abs": "https://arxiv.org/abs/2506.17211", "authors": ["Xuechen Zhang", "Zijian Huang", "Yingcong Li", "Chenshun Ni", "Jiasi Chen", "Samet Oymak"], "title": "BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning", "comment": null, "summary": "Small language models (SLMs) struggle to learn complex reasoning behaviors,\nespecially when high-quality traces are scarce or difficult to learn from. The\nstandard training approach combines a supervised fine-tuning (SFT) stage, often\nto distill capabilities of a larger model, followed by a reinforcement learning\n(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we\ninvestigate the fundamental limitations of this SFT + RL paradigm and propose\nmethods to overcome them. Under a suitable theoretical model, we demonstrate\nthat the SFT + RL strategy can fail completely when (1) the expert's traces are\ntoo difficult for the small model to express, or (2) the small model's\ninitialization has exponentially small likelihood of success. To address these,\nwe introduce BREAD: a GRPO variant that unifies the SFT and RL stages via\npartial expert guidance and branched rollouts. When self-generated traces fail,\nBREAD adaptively inserts short expert prefixes/hints, allowing the small model\nto complete the rest of the reasoning path, and ensuring that each update\nincludes at least one successful trace. This mechanism both densifies the\nreward signal and induces a natural learning curriculum. BREAD requires fewer\nthan 40% of ground-truth traces, consistently outperforming standard GRPO while\nspeeding up the training by about 3 times. Importantly, we demonstrate that\nBREAD helps the model solve problems that are otherwise unsolvable by the SFT +\nRL strategy, highlighting how branched rollouts and expert guidance can\nsubstantially boost SLM reasoning.", "AI": {"tldr": "论文提出BREAD方法，通过结合部分专家指导和分支展开，解决了小语言模型在SFT + RL训练范式中的局限性，显著提升了推理能力和训练效率。", "motivation": "小语言模型（SLM）在复杂推理任务中表现不佳，尤其是在高质量训练数据稀缺时。传统的SFT + RL方法在专家轨迹难以表达或模型初始化成功率极低时会完全失败。", "method": "提出BREAD方法，通过部分专家指导和分支展开统一SFT和RL阶段。当模型生成失败时，BREAD插入专家提示，确保每次更新至少包含一条成功轨迹。", "result": "BREAD仅需不到40%的真实轨迹，性能优于标准GRPO，训练速度提升约3倍，并能解决SFT + RL无法解决的问题。", "conclusion": "BREAD通过分支展开和专家指导显著提升了小语言模型的推理能力，为SLM训练提供了新思路。"}}
{"id": "2506.17219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17219", "abs": "https://arxiv.org/abs/2506.17219", "authors": ["Yanzhi Zhang", "Zhaoxi Zhang", "Haoxiang Guan", "Yilin Cheng", "Yitong Duan", "Chen Wang", "Yue Wang", "Shuxin Zheng", "Jiyan He"], "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning", "comment": null, "summary": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training.", "AI": {"tldr": "论文提出了一种基于内部反馈的强化学习方法（RLIF），用于提升大语言模型的推理能力，无需外部监督。实验表明RLIF在训练初期效果显著，但后期性能下降，且对指令调优模型效果有限。", "motivation": "探索一种无需外部监督的强化学习方法，利用模型内部信号提升语言模型的推理能力。", "method": "采用RLIF方法，利用无监督奖励代理（如词级熵、轨迹级熵和自确定性）进行训练。", "result": "RLIF在训练初期能显著提升推理性能，但后期性能下降，且对指令调优模型效果有限。", "conclusion": "内部反馈在训练初期有效，但需结合其他方法以克服后期性能下降问题。"}}
