<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.LG](#cs.LG) [Total: 82]
- [cs.RO](#cs.RO) [Total: 34]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.GT](#cs.GT) [Total: 6]
- [eess.SY](#eess.SY) [Total: 9]
- [cs.NE](#cs.NE) [Total: 6]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.SD](#cs.SD) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet利用小波变换和因果推理改进低光图像增强，通过全局度量学习和局部语义损失保持因果一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽视实例级语义信息和特征特性，CWNet旨在通过因果推理和小波变换解决这一问题。

Method: 1) 全局度量学习确保因果嵌入遵循因果原则；2) 局部实例级CLIP语义损失保持因果一致性；3) 基于小波变换的主干网络优化频率信息恢复。

Result: CWNet在多个数据集上显著优于现有方法，表现鲁棒。

Conclusion: CWNet通过因果推理和小波变换实现了更精确的低光图像增强，代码已开源。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 提出了一种结合外部生物知识的新框架，用于增强显微镜图像分析模型，以解决细胞系异质性对药物筛选的挑战。


<details>
  <summary>Details</summary>
Motivation: 细胞系间的形态和生物学异质性使得药物筛选中的扰动检测具有挑战性，需要一种能够泛化到新细胞线的方法。

Method: 通过整合蛋白质相互作用数据和转录组特征，解耦扰动特异性和细胞系特异性表征，利用知识图谱指导模型训练。

Result: 在RxRx数据库上的实验表明，该方法显著提高了显微镜图像分析对新细胞线的泛化能力。

Conclusion: 该框架为基于表型的药物发现提供了有效的解决方案，具有实际应用潜力。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [3] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该研究审核了两个先进的FER数据集，发现其中许多图像是摆拍而非自然表情，且模型对非白人或深肤色人群存在负面情绪预测偏差。


<details>
  <summary>Details</summary>
Motivation: 解决FER算法在自然表情检测和跨种族/肤色表现上的性能下降问题。

Method: 随机抽样审核数据集，提出区分自然与摆拍图像的方法，并测试模型对不同肤色人群的表现。

Result: 发现数据集存在摆拍图像，且模型对非白人或深肤色人群有负面情绪预测偏差。

Conclusion: 数据集和模型需改进以避免在真实应用中造成伤害。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [4] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 提出一种无需描述符的兴趣点匹配方法，显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖描述符进行兴趣点匹配，计算和存储开销大。

Method: 在检测时直接关联兴趣点，无需描述符。

Result: 匹配精度略低于传统方法，但内存使用大幅减少。

Conclusion: 该方法为定位系统提供了高效的内存优化方案。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [5] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一种新的航天器图像分割数据集，用于开发自主检测系统，并在YOLOv8和YOLOv11模型上进行了性能测试。


<details>
  <summary>Details</summary>
Motivation: 航天器在太空环境中易受损伤，人工或机器人维修成本高，需开发可靠的自主检测系统。

Method: 创建了包含64k标注图像的航天器数据集，结合真实与合成背景，并添加噪声和失真。使用YOLOv8和YOLOv11模型进行微调。

Result: 模型在模拟真实约束下，Dice得分为0.92，Hausdorff距离为0.69，推理时间为0.5秒。

Conclusion: 提出的数据集和模型为航天器实时图像分割提供了有效解决方案。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [6] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种数据高效的方法，通过LLM代理系统增强空间推理能力，解决复杂室内仓库场景中的空间问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间理解任务上表现不足，需要更高效的方法提升其能力。

Method: 构建了一个LLM代理系统，集成多种工具进行空间推理和API交互。

Result: 在2025 AI City Challenge数据集上表现出高准确性和效率。

Conclusion: 该系统在复杂空间任务中表现优异，代码已开源。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [7] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT是一种嵌套ViT架构，通过动态调整计算资源提升效率，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决固定计算预算和输入复杂度不匹配导致的效率问题。

Method: 采用渐进式思维阶段和Token Recycling机制动态调整计算。

Result: 在ImageNet-1K上，相同吞吐量下准确率提升2.0 p.p.，相同GMACs下提升2.9 p.p.。

Conclusion: ThinkingViT通过动态计算分配显著提升效率，可作为插件升级现有ViT。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [8] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）的自主目标检测框架（LAOD），通过动态生成场景特定对象名称，实现无需标签的零样本检测。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖固定类别集，灵活性不足；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。

Method: 利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，提出CAAP和SNAP两个新指标分别评估定位和命名。

Result: 在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，展示了检测和命名新对象的强大性能。

Conclusion: LAOD框架提升了开放世界理解的自主性和适应性。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [9] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM是Grad-CAM的扩展，通过跨层聚合信息和Winsorization技术生成更鲁棒、可解释的热力图。


<details>
  <summary>Details</summary>
Motivation: 解释CNN决策过程对高风险领域部署至关重要，Grad-CAM的局限性促使改进。

Method: 提出Winsor-CAM，利用Winsorization技术抑制噪声，允许用户调整阈值以探索模型行为。

Result: 在PASCAL VOC 2012数据集上，Winsor-CAM在定位指标上优于Grad-CAM和均匀层平均基线。

Conclusion: Winsor-CAM通过多层解释和人工控制，推动了可信AI的发展。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [10] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来改进模型适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法难以解释参数更新的贡献，稀疏编码框架旨在提高模型适应性和可解释性。

Method: 采用稀疏编码框架，将微调特征表示为特征字典原子的稀疏组合，稀疏系数指示原子重要性。

Result: 方法在图像编辑和文本到图像概念定制任务中表现优异，优于基线微调方法。

Conclusion: 稀疏编码框架显著提升模型适应性和可解释性，适用于多种下游任务。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [11] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 该研究提出了一种结合局部离群因子(LOF)和YOLO-v11n的轻量级框架，用于结直肠息肉检测，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要致死原因之一，及时准确的息肉检测对诊断和预防至关重要。

Method: 使用LOF算法过滤噪声数据，结合YOLO-v11n模型，在多个公开数据集上进行5折交叉验证和增强训练。

Result: 模型精度达95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%，性能优于现有方法。

Conclusion: 该方法适合临床实时结肠镜检查，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [12] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super是一种改进的3D医学图像中心线跟踪模型，解决了原模型预测重复分支和提前终止的问题，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确跟踪管状树结构（如血管和气道）对医学任务至关重要，但现有模型存在预测重复分支和提前终止的问题。

Method: 提出Trexplorer Super模型，通过新技术改进原模型，并开发了三个难度递增的中心线数据集（一个合成，两个真实）用于评估。

Result: Trexplorer Super在所有数据集上优于现有SOTA模型，且发现合成数据上的强表现不一定适用于真实数据。

Conclusion: Trexplorer Super显著提升了中心线跟踪性能，同时公开了代码和数据集以促进进一步研究。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [13] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于CNN的轻量级全球天气预报模型KAI-a，在保持高精度的同时显著降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 当前AI天气预报模型多基于Transformer架构，计算复杂度高，资源需求大。研究旨在通过现代化CNN架构解决这一问题。

Method: 采用尺度不变架构和InceptionNeXt模块，结合地球系统数据特点设计，训练于ERA5数据集。

Result: KAI-a在中等范围天气预报中表现与最先进模型相当，计算资源需求显著降低，并能捕捉极端天气事件。

Conclusion: KAI-a展示了轻量级CNN模型在天气预报中的潜力，为未来研究提供了高效替代方案。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [14] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文提出两种正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度标签不一致问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度标签不一致（TsDLI）的问题，提升模型的泛化能力和可解释性。

Method: 提出局部变异损失（LVL）和局部-全局一致性损失（LGCL），结合数学原理和图论框架。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL表现优于现有方法，LVL综合排名最佳。

Conclusion: 提出的方法在标签不一致情况下平衡了可解释性和预测能力，效果显著。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [15] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill提出了一种几何引导的弱监督自蒸馏框架，通过教师-学生学习和基于视场的掩码增强跨视图定位的局部特征学习。


<details>
  <summary>Details</summary>
Motivation: 现有跨视图定位方法依赖昂贵的全监督学习，GeoDistill旨在通过弱监督减少对地面真实姿态标注的需求。

Method: 使用教师模型定位全景图像，学生模型从有限视场图像预测位置，通过对齐预测结果优化特征学习。

Result: 实验表明GeoDistill显著提升了定位性能，并减少了不确定性。

Conclusion: GeoDistill为跨视图定位提供了可扩展且高效的解决方案。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [16] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为GAPL-SCD的图聚合原型学习方法，用于解决语义变化检测中的多任务优化问题，通过自适应权重分配和梯度旋转提升性能。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）需要同时优化多个任务，容易因任务间冲突导致负迁移，因此需要一种方法来解决这一问题。

Method: 设计了多任务联合优化框架，结合语义分割、变化检测和图聚合原型学习，采用自适应权重分配和梯度旋转方法。

Result: 在SECOND和Landsat-SCD数据集上实现了最先进的性能，显著提升了SCD任务的准确性和鲁棒性。

Conclusion: GAPL-SCD通过多任务优化和特征交互模块，有效解决了语义变化检测中的任务冲突问题，提升了模型性能。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [17] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的新型ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，解决了身份模糊和随机生成过程带来的不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复技术虽在视觉质量上有显著提升，但身份模糊输入和随机生成过程导致身份不确定性仍未解决。

Method: RIDFR利用预训练扩散模型，结合内容注入模块和身份注入模块，并通过对齐学习抑制ID无关的面部语义干扰。

Result: 实验表明，RIDFR优于现有方法，能重建高质量且身份保真的结果，并表现出强鲁棒性。

Conclusion: RIDFR通过创新框架解决了身份不确定性问题，为人脸修复提供了更可靠的解决方案。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [18] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 本文提出了一个名为WomenSports的新数据集，用于女性运动分类，并设计了一种结合通道注意机制的CNN方法，取得了89.15%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有女性运动动作的数据集缺乏足够的类内和类间变化，限制了相关研究的发展。

Method: 提出了一种基于CNN的深度特征提取方法，并应用通道注意机制优化特征表示。

Result: 在WomenSports数据集上，使用ResNet-50实现了89.15%的Top-1分类准确率。

Conclusion: 该研究填补了女性运动数据集的空白，提出的方法在多个数据集上表现优异。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [19] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种基于小波注意力的主干网络和射线编码器架构，用于高效且可靠的人-物交互检测。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互检测方法在效率和可靠性上存在不足，需要资源密集型训练和低效架构。

Method: 设计了小波注意力主干网络和射线编码器，分别用于提取多阶交互特征和优化多尺度注意力。

Result: 在ImageNet和HICO-DET等基准数据集上表现优异。

Conclusion: 提出的架构显著提升了人-物交互检测的效率和准确性。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [20] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 论文提出RG-Gait方法，通过残差学习解决步态识别中的遮挡问题，同时保持对完整步态的识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前步态识别研究大多未解决遮挡问题，且现有方法在遮挡和完整步态识别之间难以兼顾。

Method: 将遮挡步态建模为完整步态表示的残差偏差，提出自适应残差校正网络。

Result: 在Gait3D、GREW和BRIAR数据集上验证，显著提升遮挡步态识别性能且不影响完整步态识别。

Conclusion: 残差学习是解决遮挡步态识别并保持完整步态识别能力的有效方法。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [21] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN是一种轻量级架构设计，通过多阶空间特征和通道聚合模块提升性能，显著减少参数冗余。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer在视觉任务中偏好简单特征和通道冗余的问题。

Method: 采用可变感受野的核和波基通道聚合模块，动态捕捉多阶空间特征并减少冗余。

Result: 在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数）。

Conclusion: SpaRTAN通过高效设计实现高性能，参数效率显著。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [22] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP利用训练免费的CLIP模型，结合特征匹配和跨模态对齐，提出了一种零样本异常检测方法，通过批次内图像互作参考点，并利用文本信息过滤噪声特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测（ZSAD）在工业应用中至关重要，但现有方法在批次测试和噪声过滤方面存在不足。FiSeCLIP旨在解决这些问题。

Method: FiSeCLIP结合特征匹配与跨模态对齐，利用批次内其他图像作为参考，并通过文本信息过滤噪声特征，同时恢复CLIP的局部语义相关性以提升细粒度检测能力。

Result: 在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，AU-ROC和F1-max分别提升4.6%和5.7%。

Conclusion: FiSeCLIP为零样本异常检测提供了更强基线，展示了CLIP模型在工业应用中的潜力。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [23] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于语义显著区域的SISRNet方法，用于生成更准确的放射学报告，解决了现有方法因数据偏差导致的医学不准确问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法生成的放射学报告虽然流畅，但因数据偏差导致医学不准确性，限制了临床应用。

Method: SISRNet通过细粒度跨模态语义识别医学关键显著区域，并在图像建模和报告生成中系统关注这些区域。

Result: 在IU-Xray和MIMIC-CXR数据集上，SISRNet表现优于现有方法。

Conclusion: SISRNet能有效捕捉细微异常，减少数据偏差影响，生成临床准确的报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [24] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge的CBCT-to-MDCT翻译框架，结合GAN先验和人类引导的条件扩散，确保解剖保真度和感知可控性。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN或扩散模型在CBCT-to-MDCT翻译中边界一致性和临床偏好对齐的问题。

Method: 结合GAN先验和人类反馈的条件扩散，通过迭代优化和锦标赛选择实现人类偏好内化。

Result: 在临床数据集上，RMSE、SSIM、LPIPS和Dice指标表现优异，仅需10步采样。

Conclusion: 该框架在实时医学图像翻译中高效且符合临床偏好。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [25] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种个性化开放词汇语义分割任务，通过文本提示调优和负掩码提案解决现有方法无法识别用户特定文本（如“我的杯子”）的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割（OVSS）无法处理用户个性化文本（如“我的杯子”），需要一种新方法识别特定兴趣区域。

Method: 提出基于文本提示调优的插件方法，结合负掩码提案和视觉嵌入增强文本提示表示。

Result: 在FSS$^\text{per}$、CUB$^\text{per}$和ADE$^\text{per}$基准上验证了方法的优越性。

Conclusion: 该方法在提升个性化OVSS的同时，保持了原始OVSS性能。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [26] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: DGFDNet提出了一种双域去雾网络，结合空间和频率域特征，通过物理引导的退化对齐提升去雾性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型计算成本高，且空间域特征在复杂雾况下表现不足，频率域与空间域耦合弱。

Method: DGFDNet包含HAFM模块（基于暗通道先验生成雾置信图）和MGAM模块（多尺度特征融合），并引入PCGB分支进行迭代优化。

Result: 在四个基准数据集上，DGFDNet实现了最优性能，兼具鲁棒性和实时性。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [27] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D是一个新型多视角高分辨率踝足表面点云数据集，专注于步态中的踝足区域建模，用于3D点云补全方法的评估。


<details>
  <summary>Details</summary>
Motivation: 步态中踝足复合体的运动学分析对生物力学研究和临床评估至关重要，但动态步态条件下获取精确表面几何数据具有挑战性。

Method: 使用自定义五相机深度传感系统从46名受试者收集8,403帧点云数据，包含完整5视角重建及部分视角点云。

Result: 数据集支持单模态和多模态补全网络的基准测试，为生物力学、临床步态分析和机器人应用提供详细3D模型。

Conclusion: FootGait3D有望推动生物力学和多段足建模研究，数据集已公开。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [28] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，采用Swin Transformer和新型模块，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决卫星图像中目标检测的挑战，如高分辨率和多尺度对象，同时保持计算效率。

Method: 使用Swin Transformer替代CNN主干，结合UpConvMixer块和Fusion Blocks进行特征提取与融合，采用CBAM注意力和多路径头设计。

Result: 在xView数据集上达到32.95%的准确率，比现有方法提升11.46%。

Conclusion: GLOD通过创新的架构设计，显著提升了卫星图像目标检测的性能和效率。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [29] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn框架通过原型驱动的学习减少对文本输入的依赖，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学语言引导的分割方法依赖成对的图像-文本输入，限制了数据利用和临床应用。

Method: 提出ProLearn框架，引入原型驱动的语义近似模块（PSA），从文本中提取语义并近似生成无文本图像的语义指导。

Result: 在多个数据集上，ProLearn在文本有限的情况下优于现有方法。

Conclusion: ProLearn显著减少了对文本的依赖，扩展了语言引导分割的适用性。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [30] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP框架通过3D-GALP和SLaMP方法，解决了高斯溅射中局部3D编辑的挑战，实现了精确和大幅度的部分级别修改。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经表示和实例级编辑模型在实现精确局部3D编辑时面临挑战，尤其是高斯溅射中的多视角2D分割不一致性和SDS损失的模糊性。

Method: 提出RoMaP框架，包括3D-GALP模块（利用球谐系数建模视角依赖标签）和SLaMP编辑方法（结合L1锚定损失和额外正则化器）。

Result: 实验表明，RoMaP在重建和生成的高斯场景中实现了最先进的局部3D编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更鲁棒和灵活的部分级别解决方案。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [31] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于关节角度的新方法，用于改进无标记人体姿态估计（HPE），通过高质量数据集和双向循环网络提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有HPE方法在关键点识别和轨迹平滑性上存在误差，且训练数据集标注不准确限制了深度学习模型的性能。

Method: 1. 基于关节角度的人体姿态建模；2. 使用高阶傅里叶级数近似关节角度的时间变化；3. 设计双向循环网络作为后处理模块。

Result: JAR方法在挑战性场景（如花样滑冰和霹雳舞）中优于现有HPE改进网络。

Conclusion: 基于关节角度的方法显著提升了HPE的准确性和轨迹平滑性。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [32] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 论文提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了结构对称性和部分遮挡问题，并发布了一个新的数据集SKD用于验证。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器的单目姿态估计对在轨服务任务（如卫星维护、空间碎片清除等）至关重要，但现有关键点检测方法在结构对称性和部分遮挡下表现不佳。

Method: 提出GKNet，利用关键点图的几何约束进行姿态估计，并创建SKD数据集（包含3种航天器目标和9万张模拟图像）用于验证。

Result: 实验表明，GKNet在精度和有效性上优于现有方法。

Conclusion: GKNet为航天器姿态估计提供了高精度解决方案，并公开了代码和数据集以促进研究。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [33] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于交叉验证策略的深度学习方法，用于从GPR图像中自动识别道路地下病害（RSD），显著提高了识别准确率并减少了人工工作量。


<details>
  <summary>Details</summary>
Motivation: GPR图像中的RSD识别依赖人工且效率低，现有深度学习方法受限于数据稀缺和网络区分能力不足。

Method: 构建了高质量的3D GPR数据集，并提出基于YOLO模型的交叉验证策略。

Result: 在实地测试中，召回率超过98.6%，检测系统可减少约90%的人工工作量。

Conclusion: 该方法为RSD自动识别提供了高效解决方案，具有实际应用潜力。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [34] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 论文提出Atmos-Bench，首个3D大气基准数据集，并开发FourCastX网络，通过物理约束提升大气结构恢复精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助输入和简化物理模型，缺乏标准化3D基准，可能引入不确定性和不准确的辐射传输效果。

Method: 结合WRF和增强的COSP模拟器生成3D散射数据，构建Atmos-Bench数据集；设计FourCastX网络，嵌入物理约束，实现高质量恢复。

Result: 在355 nm和532 nm波段上均优于基线模型，无需辅助输入。

Conclusion: Atmos-Bench为卫星3D大气结构恢复设定了新标准，推动气候研究深入发展。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [35] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统综述了视觉识别模型的可解释性研究，提出了一种以人为中心的分类法，并探讨了评估指标和新技术的应用。


<details>
  <summary>Details</summary>
Motivation: 随着视觉识别模型在关键领域的应用增加，理解其机制和失败原因的需求推动了可解释性研究的发展。

Method: 提出了一种基于意图、对象、呈现和方法的分类法，系统整理了可解释性方法。

Result: 建立了系统的分类标准，总结了评估指标需求，并探索了新技术（如多模态大模型）带来的机会。

Conclusion: 本文旨在组织现有研究并启发未来对视觉识别模型可解释性的探索。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [36] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++是一种新型多模态大语言模型，专注于通过用户指令引导的多模态输入实现通用关键点理解，显著提升了细粒度图像分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在捕捉细粒度语义信息（如对象关键点）方面表现不足，而关键点对细粒度图像分析、对象检索和行为识别等应用至关重要。

Method: 提出KptLLM++模型，采用“识别-检测”范式，通过结构化思维链推理机制先解释关键点语义再定位其精确位置，并扩展训练数据集至50万样本。

Result: 在多个关键点检测基准测试中表现优异，展示了卓越的准确性和泛化能力。

Conclusion: KptLLM++为细粒度图像理解提供了统一解决方案，并推动了人机交互的变革。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [37] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于水母物种检测与分类，结合多种特征提取技术和分类器，最高准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 水母在海洋生态系统中扮演重要角色，但其快速繁殖和生态影响对生物多样性和保护构成挑战，准确识别物种对生态监测和管理至关重要。

Method: 整合MobileNetV3、ResNet50等特征提取技术，结合传统机器学习分类器和前馈神经网络分类器，并使用softmax函数直接分类。

Result: MobileNetV3与人工神经网络的组合表现最佳，准确率达98%。

Conclusion: 深度学习与混合框架能有效解决生物多样性挑战，推动海洋环境中的物种检测。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [38] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 论文提出了一种任务导向的人体抓取合成方法，通过任务感知接触图结合场景和任务信息，显著提升了抓取质量和任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统接触图仅考虑手与物体的关系，缺乏任务和场景信息，导致抓取姿势与任务不匹配。

Method: 采用两阶段流程：首先生成任务感知接触图，随后基于该图合成任务导向的人体抓取姿势。

Result: 实验验证了结合场景和任务信息的重要性，方法在抓取质量和任务性能上优于现有方法。

Conclusion: 任务感知接触图能有效提升任务导向抓取的准确性和实用性。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [39] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出了一种多模态引导的硬样本生成与学习框架（HSGL），用于解决服装变化行人重识别（CC-ReID）中的硬样本问题，通过文本和视觉模态统一生成和优化硬样本。


<details>
  <summary>Details</summary>
Motivation: 硬样本在CC-ReID任务中由于模糊性或相似性难以定义，限制了学习策略的设计和模型鲁棒性。

Method: HSGL包含双粒度硬样本生成（DGHSG）和硬样本自适应学习（HSAL），前者生成多样化的硬样本，后者基于文本语义标签优化特征距离。

Result: 在多个CC-ReID基准测试中表现优异，显著加速学习收敛，并在PRCC和LTCC数据集上达到最优性能。

Conclusion: 多模态引导的硬样本生成与学习能有效提升CC-ReID的鲁棒性和判别能力。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [40] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: MMOne框架通过模态建模模块和多模态分解机制解决模态冲突，提升多模态场景表示能力。


<details>
  <summary>Details</summary>
Motivation: 多模态感知增强对物理世界的理解，但模态冲突（属性差异和粒度差异）是主要挑战。

Method: 提出MMOne框架，包括模态建模模块（带模态指示器）和多模态分解机制，分离共享和模态特定组件。

Result: 实验表明，MMOne能有效提升各模态表示能力，并支持扩展新模态。

Conclusion: MMOne为多模态场景表示提供了一种紧凑高效的解决方案。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [41] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，在检测和分割任务中取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 滑坡灾害频发，传统观测方法难以覆盖大范围复杂地形，因此需要自动化解决方案。

Method: 设计了一种新型神经网络架构，用于滑坡检测和分割任务，输入为遥感图像。

Result: 在三个基准数据集上测试，检测任务F1分数达98.23和93.83，分割任务mIoU分数为63.74和76.88。

Conclusion: 模型具有实际应用潜力，可集成到滑坡观测系统中。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [42] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型的色彩视觉能力，提出了一个测试任务和数据集，并分析了错误类型和改进策略。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的色彩视觉能力尚未被充分探索，需要填补这一研究空白。

Method: 定义了色彩视觉测试任务，构建了多类别、多难度的数据集，并分析了模型的错误类型，提出了微调策略。

Result: 通过数据集和错误分析，提出了改进模型色彩视觉性能的策略。

Conclusion: 研究为提升大型视觉语言模型的色彩视觉能力提供了方法和数据支持。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [43] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 提出了一种新型的自监督对比表示学习算法CMCRL，用于柑橘病害检测与分类，显著减少了对标注数据的依赖，并在公开数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 柑橘病害严重影响产量，传统深度学习方法依赖大量标注数据，而CMCRL旨在通过自监督学习减少标注需求并提升性能。

Method: 采用聚类引导的自监督多层对比表示学习（CMCRL），结合簇中心对比和多层对比训练范式。

Result: 在CDD数据集上，CMCRL的准确率比现有方法高4.5%-30.1%，且在其他评估指标（F1分数、精确率、召回率）上表现优异。

Conclusion: CMCRL在减少标注依赖的同时，实现了高性能的柑橘病害分类，为实际应用提供了高效解决方案。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [44] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 该论文评估了开源通用和医学专用视觉语言模型（VLMs）在多个医疗基准测试中的表现，发现通用模型在某些任务上已超越医学专用模型，但推理能力仍是瓶颈，且临床部署可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在医疗任务中的能力，填补现有研究空白。

Method: 对3B至72B参数的VLMs在八个医疗基准测试（如MedXpert、OmniMedVQA等）进行全面评估，分为理解和推理两部分。

Result: 1. 通用模型在部分任务上优于医学专用模型；2. 推理能力普遍较弱；3. 不同基准测试表现差异显著。

Conclusion: 当前模型尚未达到临床部署标准，需加强多模态对齐和更严格的评估协议。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [45] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA是一种基于模态组合的单模态解耦动态低秩适应方法，用于高效训练不完整多模态学习模型，显著提升了任务准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感识别中因传感器故障或隐私保护导致的多模态不完整问题，避免现有方法中不同模态组合训练梯度的冲突。

Method: 提出MCULoRA框架，包含模态组合感知低秩适应（MCLA）和动态参数微调（DPFT）模块，分别解耦共享信息和优化学习效率。

Result: 在多个基准数据集上的实验表明，MCULoRA显著优于现有不完整多模态学习方法。

Conclusion: MCULoRA通过解耦和动态调整，有效解决了多模态不完整问题，提升了模型性能。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [46] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 论文提出了首个专门评估长视频生成模型叙事表达能力的基准NarrLV，通过引入时间叙事原子（TNA）和基于MLLM的评估框架，揭示了当前模型在叙事内容表达上的能力边界。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成模型的评估缺乏针对叙事表达能力的专门基准，现有基准（如VBench）仅支持简单叙事提示，无法全面评估模型的叙事能力。

Method: 1. 引入时间叙事原子（TNA）作为基本叙事单元；2. 设计自动提示生成管道；3. 基于MLLM的问答框架设计评估指标。

Result: 实验结果表明，NarrLV的评估指标与人类判断高度一致，揭示了当前视频生成模型在叙事内容表达上的详细能力边界。

Conclusion: NarrLV是首个全面评估长视频生成模型叙事表达能力的基准，为未来研究提供了重要工具。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [47] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出了一种基于公平性的分组方法，用于连续敏感属性，通过最大化组间差异的新标准识别关键子群体，并在实验中验证了其有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理连续敏感属性（如肤色）时，通过预设分组可能忽略少数群体的歧视问题，因此需要更精细的分组方法。

Method: 提出基于歧视水平的公平性分组方法，最大化组间差异的新标准，识别关键子群体，并通过实验验证其鲁棒性。

Result: 在合成数据集和真实数据集（CelebA、FFHQ）上验证了方法的有效性，揭示了更细致的歧视模式，且结果稳定。

Conclusion: 该方法在提高公平性的同时保持准确性，适用于工业部署，为连续敏感属性的公平性评估提供了新思路。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [48] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的综合框架，通过改进修复模型和引入新的损失函数，提升了烟雾图像的生成质量，并用于增强烟雾检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 森林火灾烟雾图像数据稀缺，现有修复模型生成烟雾图像时存在与背景不一致的问题。

Method: 使用预训练分割模型和多模态模型获取烟雾掩码和图像描述，引入掩码和掩码图像特征引导的网络架构，提出掩码随机差异损失函数，结合烟雾特性和多模态大语言模型筛选图像。

Result: 生成的烟雾图像真实多样，有效提升了烟雾检测模型的性能。

Conclusion: 提出的框架解决了烟雾图像生成中的一致性问题，为烟雾检测任务提供了高质量的数据集。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [49] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD框架通过结构化多视角分解解决3D视觉定位中的复杂查询和视角不一致问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂多锚点查询和视角变化导致的空间描述不一致问题。

Method: 提出ViewSRD框架，包括Simple Relation Decoupling (SRD)模块、Multi-view Textual-Scene Interaction (Multi-TSI)模块和Textual-Scene Reasoning模块。

Result: 在3D视觉定位数据集上表现显著优于现有方法，尤其在复杂查询中。

Conclusion: ViewSRD通过结构化多视角分解有效解决了3D视觉定位中的挑战。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [50] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种改进的YOLOv5s模型YOLOatr，用于热红外图像中的目标检测与识别（ATR），在复杂环境下表现优异，准确率达99.6%。


<details>
  <summary>Details</summary>
Motivation: 热红外图像中的ATR任务因数据集有限、硬件限制、天气影响等因素具有挑战性，现有深度学习模型表现不佳。

Method: 改进YOLOv5s，优化检测头、特征融合和增强策略，提出YOLOatr模型。

Result: 在DSIAC MWIR数据集上测试，YOLOatr达到99.6%的准确率，优于现有方法。

Conclusion: YOLOatr在热红外ATR任务中表现出色，为复杂环境下的实时检测提供了有效解决方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [51] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP是一个基于IoT的番茄数据集，包含64,464张RGB图像和精细标注，用于解决传统植物表型分析的偏差问题。通过深度学习框架验证，其准确性与专家相当。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法存在观察者偏差和不一致性，限制了精细分析的准确性和可重复性。

Method: 开发了TomatoMAP数据集，包含多角度图像和精细标注，使用MobileNetv3、YOLOv11和MaskRCNN进行验证。

Result: 模型在准确性和速度上与专家相当，Cohen's Kappa和热图验证了方法的可靠性。

Conclusion: TomatoMAP为自动化精细植物表型分析提供了可靠的数据和方法。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [52] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 论文提出了一种基于人工智能的方法，利用YOLOv11模型自动识别侵蚀区域并估算其面积，开发了交互式网页应用ERO SCAN。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要专业知识和大量人工处理，效率低，因此研究旨在通过AI技术优化侵蚀检测和量化。

Method: 使用YOLOv11模型，结合照片和LiDAR图像进行微调和训练，数据集通过Roboflow平台标注和分割。

Result: 实验结果显示模型能高效检测侵蚀模式（准确率70%），并可靠计算侵蚀面积（像素和平方米）。

Conclusion: 开发的ERO SCAN系统为风险管理和领土规划提供了便捷工具，优化了侵蚀检测和量化流程。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [53] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新的高斯泼溅框架，首次支持在表面重建过程中使用多种几何基元，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一类型的基元（椭圆或椭球）表示物体表面，难以满足复杂形状的高质量重建需求。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的泼溅与渲染。

Result: 实验证明该框架能有效提升表面重建的准确性。

Conclusion: 通过引入多种基元，显著提升了高斯泼溅在复杂表面重建中的表现。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [54] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet结合单目深度估计与多视角立体视觉，通过注意力机制和动态深度候选更新，提升在纹理缺失和反射区域的深度预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MVS方法在纹理缺失和反射区域表现不佳，而单目深度估计在这些区域具有优势。

Method: 集成单目特征和深度信息，设计跨视角位置编码和动态深度候选更新，并使用相对一致性损失监督深度预测。

Result: 在DTU和Tanks-and-Temples数据集上达到SOTA性能，排名第一。

Conclusion: MonoMVSNet通过融合单目和多视角信息，显著提升了MVS在挑战性区域的性能。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [55] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 论文提出UGC-VideoCap，一个专注于音频和视觉平衡整合的新基准和模型框架，用于短格式用户生成视频的详细多模态字幕生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕基准和模型主要依赖视觉，忽略了音频在传达场景动态、说话者意图和叙事背景中的关键作用，缺乏全面的数据集和轻量级模型。

Method: 引入UGC-VideoCap基准，包含1000个TikTok视频和4000个QA对，提出UGC-VideoCaptioner(3B)模型，采用两阶段训练策略（监督微调+GRPO）。

Result: 新基准和模型为无约束真实世界UGC环境中的多模态视频字幕提供了高质量基础和数据高效解决方案。

Conclusion: UGC-VideoCap填补了多模态视频字幕领域的空白，为未来研究提供了新方向。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [56] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 论文提出了一种几何方法，用于分析人脸识别模型对不同面部和图像属性的依赖性或不变性，并引入了一种物理启发的对齐度量。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在人脸识别任务中取得了显著进展，但现有方法主要关注身份信息，忽略了嵌入空间中多尺度几何结构的影响。

Method: 提出了一种几何方法，结合物理启发的对齐度量，评估模型对可控属性和合成数据的依赖性。

Result: 研究发现模型对不同属性表现出不同程度的依赖性，揭示了其优势和局限性。

Conclusion: 该方法为模型提供了更深的可解释性，有助于理解其行为。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [57] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR模型在非差分隐私（DP）适应中优于扩散模型（DMs），但在DP适应中表现不佳，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探索VAR模型在特定下游任务（如医疗数据生成）中的适应方法，填补其在DP适应方面的研究空白。

Method: 实现并对比多种VAR适应策略，与最先进的DM适应策略进行基准测试。

Result: VAR在非DP适应中表现优于DMs，但在DP适应中性能下降。

Conclusion: VAR在非DP任务中具有优势，但DP适应仍需改进，未来需深入研究。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [58] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI是一种基于神经表示的新型图像压缩框架，通过加速训练和超压缩技术，显著提高了压缩效率和速度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化性差，INR虽具潜力但压缩速度慢且压缩比不足。

Method: COLI利用NeRV加速INR训练（预训练-微调、混合精度训练、并行化目标）并引入超压缩技术提升压缩比。

Result: 在医学影像数据集上，COLI在PSNR和SSIM指标上表现优异，压缩速度提升4倍。

Conclusion: COLI为大规模图像压缩提供了高效解决方案，兼具速度和性能优势。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [59] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和分层扩散模型的血管几何生成方法，用于合成高保真度的主动脉几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法在复杂血管拓扑结构中表现受限，需要更灵活且可扩展的模型。

Method: 结合NURBS参数化和分层扩散模型，生成中心线和径向轮廓，支持零样本条件生成。

Result: 生成的主动脉几何结构在生物标志物分布上与原始数据集高度匹配。

Conclusion: HUG-VAS首次通过NURBS和分层扩散模型将图像先验与生成形状建模统一，具有广泛应用潜力。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [60] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI算法通过结合组合边缘像素采样和卷积参数空间密度估计，在模糊图像中实现高精度实时圆检测与拟合。


<details>
  <summary>Details</summary>
Motivation: 解决在退化成像条件下鲁棒圆检测与拟合的计算机视觉挑战。

Method: 结合组合边缘像素采样和卷积参数空间密度估计的3C-FBI算法。

Result: 在真实医疗数据、合成数据及不同分辨率下表现优异，Jaccard指数达0.896，实时性能40.3 fps，优于传统方法。

Conclusion: 3C-FBI在精度、速度和鲁棒性上表现卓越，适用于医疗影像、机器人和工业检测。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [61] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类感知的模糊颜色模型COLIBRI，通过模糊集和逻辑构建颜色分类框架，实验验证其优于传统颜色模型。


<details>
  <summary>Details</summary>
Motivation: 解决计算机难以模仿人类颜色感知的问题，缩小计算颜色表示与人类视觉感知之间的差距。

Method: 采用三阶段实验方法，包括初步实验确定颜色刺激、大规模人类分类调查（1000+受试者）提取模糊分区和生成隶属函数，并引入适应机制。

Result: 模型在人类感知对齐上优于RGB、HSV和LAB等传统模型，且样本规模（n=2496）前所未有。

Conclusion: COLIBRI模型在设计与AI等领域具有重要意义，为颜色表示提供了更符合人类感知的解决方案。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [62] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出了一种5阶段框架，用于从EEG信号解码视觉表示，通过跨模态对齐和重新排序实现上下文感知的EEG到图像生成。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码视觉表示具有挑战性，因其复杂且噪声多。

Method: 5阶段框架：EEG编码器、跨模态对齐、标题重新排序、加权插值和图像生成。

Result: 生成高质量图像，分类准确率提升13.43%，生成准确率提升15.21%，FID降低36.61%。

Conclusion: 方法在语义对齐和图像质量上优于现有技术。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [63] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出CharaConsist方法，通过点跟踪注意力和自适应令牌合并，解决文本到图像生成中身份一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在背景细节一致性和大动作变化时身份一致性上表现不足。

Method: 使用点跟踪注意力、自适应令牌合并，并解耦前景和背景控制。

Result: CharaConsist在连续或离散场景中均能保持细粒度一致性，适用于DiT模型。

Conclusion: CharaConsist扩展了文本到图像生成的应用范围，提升了生成质量。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [64] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种流式4D视觉几何变换器，用于实时感知和重建4D时空几何，采用因果变换器架构和高效注意力机制，支持在线场景下的高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决从视频中实时感知和重建4D时空几何的挑战，支持交互式应用。

Method: 采用因果变换器架构，利用时间因果注意力和历史键值缓存作为隐式记忆，实现流式长期4D重建。通过知识蒸馏从双向视觉几何变换器（VGGT）中学习。

Result: 在多个4D几何感知基准测试中表现优异，推理速度显著提升，同时保持高质量空间一致性。

Conclusion: 该模型为可扩展和交互式4D视觉系统奠定了基础，代码已开源。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [65] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 该论文综述了深度估计领域的发展，探讨了基于视觉的方法的潜力，并提出了构建强大深度基础模型的路径。


<details>
  <summary>Details</summary>
Motivation: 传统深度估计方法（如LiDAR）成本高、分辨率低且对环境敏感，而现有视觉方法在泛化和稳定性上存在挑战。

Method: 综述了单目、立体、多视图和单目视频设置下的深度学习架构和范式，并探讨了大规模数据集的作用。

Result: 提出了深度基础模型的概念，强调其零样本泛化能力，并总结了关键架构和训练策略。

Conclusion: 论文为未来深度基础模型的研究和应用提供了方向，强调了其在解决现有挑战中的潜力。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [66] [Developing and evaluating quilts for the depiction of large layered graphs](https://arxiv.org/abs/2507.10883)
*Juhee Bae,Benjamin Watson*

Main category: cs.GR

TL;DR: 研究改进了Quilts（一种基于矩阵的分层图表示方法），并比较了其与传统节点链接和矩阵表示的性能，发现Quilts在路径查找上更快更准确。


<details>
  <summary>Details</summary>
Motivation: 传统分层图表示（如流程图）在复杂图中难以理解，Quilts旨在解决这一问题。

Method: 1. 开发三种Quilts设计变体，比较其性能；2. 将最佳变体与节点链接和矩阵表示进行比较。

Result: Quilts在路径查找上显著快于节点链接和矩阵表示（46.6秒 vs. 58.3秒和71.2秒），且在大型图中优势更明显。

Conclusion: Quilts是一种更高效的分层图表示方法，尤其在处理复杂和大规模图时表现优异。

Abstract: Traditional layered graph depictions such as flow charts are in wide use. Yet
as graphs grow more complex, these depictions can become difficult to
understand. Quilts are matrix-based depictions for layered graphs designed to
address this problem. In this research, we first improve Quilts by developing
three design alternatives, and then compare the best of these alternatives to
better-known node-link and matrix depictions. A primary weakness in Quilts is
their depiction of skip links, links that do not simply connect to a succeeding
layer. Therefore in our first study, we compare Quilts using color-only,
text-only, and mixed (color and text) skip link depictions, finding that path
finding with the color-only depiction is significantly slower and less
accurate, and that in certain cases, the mixed depiction offers an advantage
over the text-only depiction. In our second study, we compare Quilts using the
mixed depiction to node-link diagrams and centered matrices. Overall results
show that users can find paths through graphs significantly faster with Quilts
(46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams.
This speed advantage is still greater in large graphs (e.g. in 200 node graphs,
55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions).

</details>


### [67] [OffsetCrust: Variable-Radius Offset Approximation with Power Diagrams](https://arxiv.org/abs/2507.10924)
*Zihan Zhao,Pengfei Wang,Minfeng Xu,Shuangmin Chen,Shiqing Xin,Changhe Tu,Wenping Wang*

Main category: cs.GR

TL;DR: OffsetCrust提出了一种计算变半径偏移曲面的新框架，通过构建幂图解决传统方法的挑战。


<details>
  <summary>Details</summary>
Motivation: 变半径偏移曲面在几何处理中应用广泛，但计算难度大，现有方法存在对齐问题。

Method: 通过采样基曲面点及其对应的离曲面点构建幂图，并采用轻量级微调解决对齐问题。

Result: 实验验证了OffsetCrust的准确性和高效性，并展示了其在MAT重建等应用中的实用性。

Conclusion: OffsetCrust为变半径偏移曲面计算提供了高效解决方案，解决了传统方法的局限性。

Abstract: Offset surfaces, defined as the Minkowski sum of a base surface and a rolling
ball, play a crucial role in geometry processing, with applications ranging
from coverage motion planning to brush modeling. While considerable progress
has been made in computing constant-radius offset surfaces, computing
variable-radius offset surfaces remains a challenging problem. In this paper,
we present OffsetCrust, a novel framework that efficiently addresses the
variable-radius offsetting problem by computing a power diagram. Let $R$ denote
the radius function defined on the base surface $S$. The power diagram is
constructed from contributing sites, consisting of carefully sampled base
points on $S$ and their corresponding off-surface points, displaced along
$R$-dependent directions. In the constant-radius case only, these displacement
directions align exactly with the surface normals of $S$. Moreover, our method
mitigates the misalignment issues commonly seen in crust-based approaches
through a lightweight fine-tuning procedure. We validate the accuracy and
efficiency of OffsetCrust through extensive experiments, and demonstrate its
practical utility in applications such as reconstructing original boundary
surfaces from medial axis transform (MAT) representations.

</details>


### [68] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D是一个将低质量3D资产提升为高质量的新框架，核心是HFS-SDEdit纹理增强方法，结合几何优化，显著提升3D模型质量。


<details>
  <summary>Details</summary>
Motivation: 高质量3D资产稀缺且获取成本高，Elevate3D旨在解决这一问题。

Method: 采用HFS-SDEdit纹理增强和单目几何预测器，交替优化纹理和几何。

Result: 在3D模型细化中达到最先进质量，优于现有方法。

Conclusion: Elevate3D有效解决了高质量开源3D资产的稀缺问题。

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [69] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: 论文探讨了如何通过社交线索和表达不确定性来促进用户对对话式用户界面（CUI）的自我披露。


<details>
  <summary>Details</summary>
Motivation: 自我披露对心理健康有益，但用户常因担心他人反应而难以实现。研究旨在探索如何通过CUI的设计改善这一问题。

Method: 分析了社交线索的作用，并讨论了通过表达不确定性和展示CUI的推理来增强用户信任的方法。

Result: 研究表明，透明化CUI的“心智理论”可以鼓励用户更愿意进行自我披露。

Conclusion: 通过改进CUI的设计，使其更透明和人性化，可以有效促进用户的自我披露行为。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


### [70] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Main category: cs.HC

TL;DR: 提出了一种测试具身AI代理互动意识和可信度的方法，重点关注人类极限情境下的表现。


<details>
  <summary>Details</summary>
Motivation: 探索机器是否能像人类一样反应，扩展了图灵测试的范围，引入非语言行为的测试。

Method: 提出了“React to This”（RTT）测试，用于评估AI代理的非语言行为反应能力。

Result: 通过初步实验展示了RTT测试的有效性。

Conclusion: RTT测试为评估AI代理的非语言互动能力提供了新方法。

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [71] [Static or Temporal? Semantic Scene Simplification to Aid Wayfinding in Immersive Simulations of Bionic Vision](https://arxiv.org/abs/2507.10813)
*Justin M. Kasowski,Apurv Varshney,Michael Beyeler*

Main category: cs.HC

TL;DR: 论文比较了两种语义预处理方法（SemanticEdges和SemanticRaster）在视觉神经假体（仿生眼）中的应用，发现两者均能提升性能和用户体验，但各有优劣。


<details>
  <summary>Details</summary>
Motivation: 在极端分辨率和带宽限制下，如何通过语义预处理改善场景理解，避免信息过载。

Method: 在虚拟现实中对比两种语义预处理方法（SemanticEdges和SemanticRaster）与基线（Control），通过18名参与者完成寻路任务。

Result: 两种方法均优于基线，SemanticEdges提高成功率，SemanticRaster减少碰撞。

Conclusion: 自适应语义预处理对视觉假体有价值，并可指导低带宽视觉界面的设计。

Abstract: Visual neuroprostheses (bionic eye) aim to restore a rudimentary form of
vision by translating camera input into patterns of electrical stimulation. To
improve scene understanding under extreme resolution and bandwidth constraints,
prior work has explored computer vision techniques such as semantic
segmentation and depth estimation. However, presenting all task-relevant
information simultaneously can overwhelm users in cluttered environments. We
compare two complementary approaches to semantic preprocessing in immersive
virtual reality: SemanticEdges, which highlights all relevant objects at once,
and SemanticRaster, which staggers object categories over time to reduce visual
clutter. Using a biologically grounded simulation of prosthetic vision, 18
sighted participants performed a wayfinding task in a dynamic urban environment
across three conditions: edge-based baseline (Control), SemanticEdges, and
SemanticRaster. Both semantic strategies improved performance and user
experience relative to the baseline, with each offering distinct trade-offs:
SemanticEdges increased the odds of success, while SemanticRaster boosted the
likelihood of collision-free completions. These findings underscore the value
of adaptive semantic preprocessing for prosthetic vision and, more broadly, may
inform the design of low-bandwidth visual interfaces in XR that must balance
information density, task relevance, and perceptual clarity.

</details>


### [72] [AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos](https://arxiv.org/abs/2507.10963)
*Zheng Ning,Leyang Li,Daniel Killough,JooYoung Seo,Patrick Carrington,Yapeng Tian,Yuhang Zhao,Franklin Mingzhe Li,Toby Jia-Jun Li*

Main category: cs.HC

TL;DR: AROMA是一个AI系统，通过整合非视觉线索、可穿戴摄像头和视频食谱内容，为盲人或低视力（BLV）人群提供实时、情境感知的烹饪辅助。


<details>
  <summary>Details</summary>
Motivation: 视频食谱主要依赖视听信息，对BLV人群不友好，他们依赖触觉、味觉等非视觉线索，因此需要一种能结合这些线索的辅助系统。

Method: AROMA采用混合主动方法，响应用户请求的同时主动监控视频流，提供及时提醒和指导，结合用户和AI的互补优势。

Result: 通过8名BLV参与者的研究评估，AROMA展示了其有效性，并为支持BLV人群的交互式AI系统设计提供了见解。

Conclusion: AROMA通过情境感知和协作设计，帮助BLV人群更好地理解和执行视频食谱中的步骤，提升了烹饪体验。

Abstract: Videos offer rich audiovisual information that can support people in
performing activities of daily living (ADLs), but they remain largely
inaccessible to blind or low-vision (BLV) individuals. In cooking, BLV people
often rely on non-visual cues, such as touch, taste, and smell, to navigate
their environment, making it difficult to follow the predominantly audiovisual
instructions found in video recipes. To address this problem, we introduce
AROMA, an AI system that provides timely responses to the user based on
real-time, context-aware assistance by integrating non-visual cues perceived by
the user, a wearable camera feed, and video recipe content. AROMA uses a
mixed-initiative approach: it responds to user requests while also proactively
monitoring the video stream to offer timely alerts and guidance. This
collaborative design leverages the complementary strengths of the user and AI
system to align the physical environment with the video recipe, helping the
user interpret their current cooking state and make sense of the steps. We
evaluated AROMA through a study with eight BLV participants and offered
insights for designing interactive AI systems to support BLV individuals in
performing ADLs.

</details>


### [73] [Self++: Merging Human and AI for Co-Determined XR Living in the Metaverse](https://arxiv.org/abs/2507.10967)
*Thammathip Piumsomboon*

Main category: cs.HC

TL;DR: Self++是一个基于自我决定理论的九层框架，旨在通过人机协作促进人类在元宇宙中的繁荣。


<details>
  <summary>Details</summary>
Motivation: 强调人类在元宇宙中的自主性和幸福感，避免技术决定论的局限性。

Method: 通过动态人机协作和扩展现实（XR）技术，逐步培养用户的能力、自主性和关联性。

Result: 提出了用户定义AI自主性、XR中社交连接设计及伦理保障等研究方向。

Conclusion: Self++为构建以人为中心、AI增强的元宇宙提供了路线图，技术应放大而非削弱人类潜力。

Abstract: This position paper introduces Self++, a novel nine-level framework for
co-determined living in the Metaverse, grounded in Self-Determination Theory.
Self++ prioritises human flourishing by progressively cultivating competence,
autonomy, and relatedness through dynamic human-AI collaboration in extended
reality (XR). Unlike technologically deterministic approaches, Self++
emphasises user empowerment by enhancing competency, mitigating cognitive
biases and leveraging XR's immersive capabilities. Key research directions
proposed include exploring the boundaries of user-defined AI autonomy,
designing for meaningful social connection in XR, and establishing proactive
ethical safeguards. Ultimately, Self++ offers a roadmap for creating a
human-centred, AI-enhanced Metaverse where technology amplifies, rather than
diminishes, human potential.

</details>


### [74] [Terms and Conditions (Do Not) Apply: Understanding Exploitation Disparities in Design of Mobile-Based Financial Services](https://arxiv.org/abs/2507.10970)
*Lindah Kotut*

Main category: cs.HC

TL;DR: 论文探讨了移动金融服务为传统无银行账户人群带来的机会与挑战，分析了设计模式中的风险，并提出了用户赋权的设计指南。


<details>
  <summary>Details</summary>
Motivation: 研究移动金融服务如何为无银行账户人群提供机会，同时揭示设计模式中潜在的风险和不道德金融行为。

Method: 通过用户访谈，详细分析移动金融交易的用户体验，并探讨金融服务提供的基础和指南。

Result: 研究发现移动金融服务存在高利率、政策文件缺失和用户保护不足等问题，同时提出了风险缓解和恢复策略。

Conclusion: 建议通过设计指南增强用户信任、技术理解能力和风险判断能力，以支持用户赋权。

Abstract: Mobile-based financial services have made it possible for the traditionally
unbanked to access infrastructure that have been routinely unattainable.
Researchers have explored how these systems have made for safer environments to
send and receive money and have expanded financial opportunities such as
increased borrowing. With this expansion, challenges such as detrimental
interest rates, lack of access to policy documents, and inadequate user
protective guardrails emerge, amplifying the risks due to technology-aided
unethical financial practices that are aided by design patterns. Supported by
user interviews, we detail user experiences of mobile-based financial
transactions and explore the foundations and guidelines that undergird the
financial service provisions: highlighting both affordances and harms enabled
in the design of such systems. We discuss the findings by highlighting
financial exploitation disparities, deliberating strategies for mitigation of
risks and enabling recovery from harms caused by the technology use. We then
recommend guidelines for empowering design approaches that support users'
mechanisms of trust, their understanding of technological processes, and
determination of risks.

</details>


### [75] [An Exploratory Study on AI-driven Visualisation Techniques on Decision Making in Extended Reality](https://arxiv.org/abs/2507.10981)
*Ze Dong,Binyang Han,Jingjing Zhang,Ruoyu Wen,Barrett Ens,Adrian Clark,Tham Piumsomboon*

Main category: cs.HC

TL;DR: 研究探讨了AI驱动的四种可视化技术（Inform、Nudge、Recommend、Instruct）在XR中对用户决策的影响，强调用户自主权、AI透明度和情境设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索AI与XR结合如何通过不同可视化技术影响用户决策，并研究用户自主权的作用。

Method: 使用Meta Quest Pro和预录制的360度超市视频，叠加四种AI驱动的可视化技术，通过半结构化访谈收集反馈。

Result: 研究发现保持用户自主权、增强AI透明度和考虑情境设计对决策至关重要。

Conclusion: 研究为XR中AI驱动的可视化设计提供了实践建议，强调用户信任和情境适应性。

Abstract: The integration of extended reality (XR) with artificial intelligence (AI)
introduces a new paradigm for user interaction, enabling AI to perceive user
intent, stimulate the senses, and influence decision-making. We explored the
impact of four AI-driven visualisation techniques -- `Inform,' `Nudge,'
`Recommend,' and `Instruct' -- on user decision-making in XR using the Meta
Quest Pro. To test these techniques, we used a pre-recorded 360-degree video of
a supermarket, overlaying each technique through a virtual interface. We aimed
to investigate how these different visualisation techniques with different
levels of user autonomy impact preferences and decision-making. An exploratory
study with semi-structured interviews provided feedback and design
recommendations. Our findings emphasise the importance of maintaining user
autonomy, enhancing AI transparency to build trust, and considering context in
visualisation design.

</details>


### [76] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Main category: cs.HC

TL;DR: 研究探索了基于大语言模型（LLM）的家庭心理支持框架，用于检测和改善亲子沟通中的潜在心理问题，如理想父母偏见和情绪压抑。


<details>
  <summary>Details</summary>
Motivation: 常规指标常忽视家庭心理动态，特别是理想父母偏见导致的情绪压抑问题，亟需外部支持。

Method: 构建日语亲子对话语料库，开发基于LLM的多智能体对话支持框架，检测情绪压抑和理想父母偏见，生成反馈。

Result: 系统能适度准确地检测情绪压抑，生成高共情和实用性的反馈，模拟对话显示改善迹象。

Conclusion: 该框架有望支持家庭互动的积极转变，提升情感表达和相互理解。

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>


### [77] [REVA: Supporting LLM-Generated Programming Feedback Validation at Scale Through User Attention-based Adaptation](https://arxiv.org/abs/2507.11470)
*Xiaohang Tang,Sam Wong,Zicheng He,Yalong Yang,Yan Chen*

Main category: cs.HC

TL;DR: REVA是一个人类-AI系统，通过优化提交顺序和传播教师驱动的修订，加速对大量AI生成编程反馈的教师评审。


<details>
  <summary>Details</summary>
Motivation: 解决教师在评审大量AI生成反馈时的认知负担和效率问题。

Method: 通过自适应学习教师在评审和修订过程中的注意力，持续改进反馈验证流程。

Result: 在12名参与者的实验室研究中，REVA提高了反馈质量和评审效率。

Conclusion: REVA在人类-AI协作教育反馈中表现出实用性和有效性。

Abstract: This paper introduces REVA, a human-AI system that expedites instructor
review of voluminous AI-generated programming feedback by sequencing
submissions to minimize cognitive context shifts and propagating
instructor-driven revisions across semantically similar instances. REVA
introduces a novel approach to human-AI collaboration in educational feedback
by adaptively learning from instructors' attention in the review and revision
process to continuously improve the feedback validation process. REVA's
usefulness and effectiveness in improving feedback quality and the overall
feedback review process were evaluated through a within-subjects lab study with
12 participants.

</details>


### [78] [Towards Creating Infrastructures for Values and Ethics Work in the Production of Software Technologies](https://arxiv.org/abs/2507.11490)
*Richmond Y. Wong*

Main category: cs.HC

TL;DR: 论文提出通过构建基础设施而非工具来支持设计中的价值观和伦理工作，以更全面地考虑社会系统与治理结构的影响。


<details>
  <summary>Details</summary>
Motivation: 现有HCI研究通常通过工具帮助技术工作者整合社会价值观，但忽视了影响其行动的更广泛政治和社会系统。

Method: 借鉴科技研究和社会科学中的基础设施理论，提出新的设计策略。

Result: 基础设施方法为HCI研究者和设计师提供了支持价值观和伦理工作的新视角。

Conclusion: 基础设施方法能更全面地解决设计中的价值观和伦理问题，推动系统性变革。

Abstract: Recognizing how technical systems can embody social values or cause harms,
human-computer interaction (HCI) research often approaches addressing values
and ethics in design by creating tools to help tech workers integrate social
values into the design of products. While useful, these approaches usually do
not consider the politics embedded in the broader processes, organizations,
social systems, and governance structures that affect the types of actions that
tech workers can take to address values and ethics. This paper argues that
creating infrastructures to support values and ethics work, rather than tools,
is an approach that takes these broader processes into account and opens them
up for (re)design. Drawing on prior research conceptualizing infrastructures
from science \& technology studies and media studies, this paper outlines
conceptual insights from infrastructures studies that open up new tactics for
HCI researchers and designers seeking to support values and ethics in design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK是一个强化学习框架，通过双目标奖励系统优化答案质量和工具多样性，训练语言模型探索多样工具使用模式。


<details>
  <summary>Details</summary>
Motivation: 传统的高温采样方法限制了工具使用的多样性，SPaRK旨在通过强化学习鼓励系统性的工具探索。

Method: 采用离线PPO训练Llama-3.1 8B模型，引入双目标奖励系统和稀有工具优先策略，由GPT-4o评分候选动作。

Result: 在14个MMLU-Pro类别中表现优异，工具选择熵显著高于基线方法，且未牺牲准确性。

Conclusion: 通过显式工具多样性探索，SPaRK提升了推理能力，证明了算法探索的有效性。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [80] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的线性自适应交叉熵损失函数，通过增加一个依赖于真实类别预测概率的项，优化分类任务性能。


<details>
  <summary>Details</summary>
Motivation: 标准交叉熵损失函数在分类任务中可能不够高效，因此提出一种改进版本以增强优化过程。

Method: 设计了一个新的损失函数，包含一个额外的项，该项与真实类别的预测概率相关，并在ResNet模型和CIFAR-100数据集上进行了评估。

Result: 实验结果表明，新损失函数在分类准确率上优于标准交叉熵损失，同时保持了相似的效率。

Conclusion: 该方法为损失函数设计提供了新的研究方向，具有潜在的应用价值。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [81] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet是一种基于Hopfield增强的稀疏空间注意力网络，用于解决无人机动态选址问题，通过四项创新技术显著提升了计算效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 城市低空无人机经济的快速发展对无人机着陆点和补给站的动态选址提出了新挑战，传统深度强化学习方法在处理大规模问题时面临计算复杂度瓶颈。

Method: 提出GeoHopNet，包括距离偏置多头注意力机制、K近邻稀疏注意力、现代Hopfield外部记忆模块和记忆正则化策略。

Result: GeoHopNet将可解决问题的规模边界扩展，对于1000个节点的大规模实例，能在0.1秒内找到高质量解（0.22%最优性差距），优于现有方法。

Conclusion: GeoHopNet在解决无人机动态选址问题上表现出色，显著提升了计算效率和解决方案质量。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [82] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: 论文探讨了神经网络如何通过对称性学习实现激进泛化，以基加法为例，分析了不同进位函数对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 设计能够高效学习支持激进泛化功能的系统是神经网络在认知建模和人工智能中的主要挑战。

Method: 通过群论分析基加法，提出不同进位函数，并训练神经网络比较学习效果。

Result: 发现简单神经网络在合适的输入格式和进位函数下可实现激进泛化，学习速度与进位函数结构密切相关。

Conclusion: 研究对认知科学和机器学习有重要意义，揭示了对称性学习的关键因素。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [83] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wendland径向基函数的新型参数化激活函数，结合线性与指数项，优化了梯度传播与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统激活函数（如ReLU、sigmoid和tanh）的局限性，利用Wendland RBF的紧支撑性、平滑性和正定性。

Method: 将标准Wendland组件与线性和指数项结合，设计出具有可调局部性和稳定性的激活函数。

Result: 在合成任务和基准数据集上表现优异，尤其在回归任务中精度更高，同时保持计算效率。

Conclusion: Wendland激活函数通过局部平滑变换缓解过拟合，提升泛化能力，未来可探索混合架构和领域适配。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [84] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: 阿联酋依赖海水淡化满足90%以上饮用水需求，但该过程能耗高且面临气候不确定性挑战。研究提出两阶段预测模型，预测气溶胶光学厚度（AOD）及淡化效率损失，并开发基于AOD的控制逻辑和交互式仪表盘。


<details>
  <summary>Details</summary>
Motivation: 海水淡化是阿联酋主要水源，但能耗高且受气候因素（如AOD）影响显著，亟需优化其可持续性。

Method: 提出两阶段预测模型：第一阶段预测AOD，第二阶段预测淡化效率损失；开发基于AOD的规则控制逻辑和交互式仪表盘。

Result: 模型准确率达98%，SHAP分析揭示系统退化关键因素；控制逻辑优化了运行和维护。

Conclusion: 研究为气候适应性规划提供决策支持，提升海水淡化系统的可持续性和效率。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [85] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: 论文提出了一种新的工具间匹配（TTTM）方法，解决了传统方法依赖静态配置或黄金参考的问题，并在异构设备环境中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统TTTM方法依赖静态配置或黄金参考，难以在商业生产线中实现，且不适用于异构设备环境。

Method: 提出基于数据方差和模式数量的新分析流程，包括单变量和多变量方法。

Result: 最佳单变量方法的相关系数>0.95（方差）和>0.5（模式数量），多变量方法与单变量方法的相关系数>0.75。

Conclusion: 新方法在TTTM中表现有效，且对算法超参数敏感。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [86] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched是一种新型自适应学习率调度器，通过动态调整学习率提升深度神经网络训练效果，显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 预定义和自适应学习率调度器可能导致次优泛化性能，需要更有效的动态调整方法。

Method: 基于随机过程中的波动性概念（如几何布朗运动），通过长短周期准确率波动比动态调整学习率。

Result: 在CIFAR-100数据集上，VolSched显著提升ResNet-18和ResNet-34的准确率，并找到更平坦的解。

Conclusion: VolSched通过动态调整学习率，有效提升模型泛化性能，优于现有基线方法。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [87] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: 论文探讨了深度学习和Transformer的数学基础，提出了一个通用的近似定理，证明单层Transformer可以任意精度逼近连续序列到序列的映射。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和Transformer在多个领域取得了成功，但其理论理解仍然有限。本文旨在填补这一空白。

Method: 回顾了线性代数、概率和优化的关键概念，详细分析了多头自注意力机制和反向传播算法，并提出了一个通用的近似定理。

Result: 证明了单层Transformer可以逼近任何连续序列到序列的映射，并提供了形式化证明和案例研究。

Conclusion: 研究结果推动了Transformer的理论理解，缩小了理论与实践之间的差距。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [88] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: MH-FSF框架是一个模块化、可扩展的平台，用于实现和比较17种特征选择方法，解决了现有研究中基准测试不足和数据集依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 当前特征选择研究存在基准测试不足和依赖专有数据集的问题，影响可重复性和性能。

Method: 开发了MH-FSF框架，包含17种特征选择方法（11种经典，6种领域特定），并在10个公开Android恶意软件数据集上评估。

Result: 结果显示性能在不同平衡和不平衡数据集上有差异，强调了数据预处理和选择标准的重要性。

Conclusion: MH-FSF框架为特征选择方法提供了统一比较平台，推动了方法学一致性和严谨性，并为Android恶意软件检测开辟了新研究方向。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [89] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: 论文提出OL-MDISF方法，解决在线学习中混合类型、漂移和不完整特征流的挑战，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线学习中，特征空间随时间变化，但面临混合类型数据、分布漂移和标注不足的挑战。

Method: OL-MDISF构建潜在copula表示，通过集成熵和潜在不匹配检测漂移，并进行结构感知伪标注。

Result: 在14个真实数据集和两种漂移场景下进行实验，包括CER趋势、消融研究和敏感性分析。

Conclusion: 论文为复杂、弱监督流数据的在线学习提供了可复现的基准。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [90] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为DTRGC的新方法，用于解决属性缺失图的深度图聚类问题，通过分层处理和聚类信息修正，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 属性缺失图的深度图聚类在实际应用中至关重要，但现有方法未能充分利用节点邻域信息，导致结果不可靠。

Method: DTRGC方法包括动态聚类感知特征传播（DCFP）、分层邻域感知填补（HNAI）和跳数表示增强（HRE），通过分层处理和聚类信息修正填补缺失属性。

Result: 在六个广泛使用的图数据集上，DTRGC显著提升了属性缺失图下各种深度图聚类方法的性能。

Conclusion: DTRGC通过分层处理和聚类信息修正，有效解决了属性缺失图的聚类问题，为实际应用提供了可靠解决方案。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [91] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne是一个针对社交网络服务（SNS）的领域特定大语言模型，通过三阶段训练策略显著提升多任务性能，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在SNS领域单任务性能瓶颈和适应性不足的问题。

Method: 采用三阶段训练策略：持续预训练、监督微调和偏好优化，使用大规模真实数据集。

Result: 在8个主要SNS任务中平均提升14.02%，有害内容检测曝光率降低11.23%，帖子搜索点击率提升14.95%。

Conclusion: RedOne是一个强大的SNS领域特定模型，具有广泛的任务适应性和实际应用潜力。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [92] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD是一个可扩展的框架，用于生成合成布局热图，以加速物理设计中的机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 解决高质量、大规模训练数据集稀缺的问题，这些数据集通常计算成本高且受知识产权限制。

Method: 使用扩散模型快速生成多样化的布局热图，包括功率、IR压降、拥塞、宏布局和单元密度图。

Result: 生成了包含20,000多种布局配置的数据集，这些热图与真实布局相似，并提高了下游机器学习任务的准确性。

Conclusion: DALI-PD为物理设计中的机器学习研究提供了高效的数据生成解决方案。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [93] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA是一个针对医疗联邦学习中标签噪声问题的新框架，通过全局样本选择器和客户端自适应调整机制，显著提升了模型在噪声环境下的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 医疗联邦学习中的标签噪声和数据不平衡问题导致模型性能下降，现有方法难以应对噪声异质性和数据分布不均。

Method: 提出FedGSCA框架，包括全局样本选择器和客户端自适应调整机制，结合伪标签生成和鲁棒损失函数。

Result: 在真实和合成医疗数据集上，FedGSCA在极端和异质噪声场景下优于现有方法，提升了模型稳定性和泛化能力。

Conclusion: FedGSCA有效解决了医疗联邦学习中的噪声问题，适用于实际应用场景。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [94] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: 研究发现，传统扩展法则在大语言模型中存在性能提升减速现象（子扩展），数据质量和训练策略是关键因素。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型中性能提升减速的原因，特别是数据质量和训练策略的影响。

Method: 通过分析400多个模型，研究数据密度和资源分配对性能的影响。

Result: 高数据密度和资源分配不当是子扩展的主要原因，提出新的扩展法则。

Conclusion: 数据质量和多样性对模型性能至关重要，需优化资源分配以应对子扩展。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [95] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了为算法设计定制LLMs的必要性，并提出了DAR采样策略和偏好优化方法，实验表明微调后的LLMs表现优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖通用LLMs，但算法设计是否需要定制LLMs及其效果尚不明确。

Method: 引入DAR采样策略平衡数据多样性和质量，结合偏好优化微调LLMs。

Result: 微调后的LLMs在多个任务中表现优于通用模型，并展现出泛化能力。

Conclusion: 任务特定的LLMs微调在算法设计中具有价值，为未来研究开辟了新方向。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [96] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 比较了强化学习（RL）和监督微调（SFT）在数学问题上的表现，发现RL在数学领域略有提升，而SFT在知识密集型任务上表现更差。


<details>
  <summary>Details</summary>
Motivation: 理解RL和SFT在LLM后训练中的动态差异及其对模型性能的影响。

Method: 在同一模型和相似超参数下，对比RL和SFT在数学问题上的表现，并分析模型参数变化。

Result: RL在数学领域有轻微提升，SFT在知识密集型任务上表现更差；SFT对模型参数影响更大。

Conclusion: RL可能增强现有能力，而SFT可能替换旧技能；冻结部分模型的效果尚不明确。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [97] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型预训练中算法创新的计算需求，发现资源密集的创新需求每年翻倍，但计算限制对算法进步的减缓作用有限。


<details>
  <summary>Details</summary>
Motivation: 探讨算法创新在预训练大型语言模型中的计算需求，以及计算限制对创新的潜在影响。

Method: 分析了36项Llama 3和DeepSeek-V3中的预训练算法创新，估算其开发所需的总FLOP和硬件FLOP/s。

Result: 资源密集的创新需求每年翻倍，但即使严格的计算限制（如GPT-2的训练计算量或8个H100 GPU的硬件限制）仍能支持半数创新。

Conclusion: 计算限制不太可能显著减缓AI算法进步。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [98] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的框架，用于5G/6G网络中动态频谱分配，解决了传统深度强化学习（DRL）样本复杂度高和安全风险的问题。


<details>
  <summary>Details</summary>
Motivation: 传统DRL在动态频谱分配中因样本复杂度和安全风险（如网络干扰）难以应用，需要一种更高效、更安全的方法。

Method: 提出元学习框架，包括三种架构：MAML、RNN和注意力增强RNN，并在模拟动态IAB环境中与非元学习DRL算法PPO进行对比。

Result: 注意力元学习代理峰值吞吐量达48 Mbps，远超PPO的10 Mbps；SINR和延迟违规减少50%以上，公平指数0.7。

Conclusion: 元学习是复杂无线系统中智能控制的高效且安全的选择。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [99] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: 该教程概述了基于大语言模型（LLMs）的跨模态时间序列分析，分类了现有方法，并讨论了应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs处理时间序列数据的潜力，但存在跨模态差异，需探索优化方法。

Method: 提出分类法，将方法分为转换、对齐和融合三类，并讨论其在下游任务中的应用。

Result: 总结了当前进展、方法和未来研究方向，旨在平衡效果与效率。

Conclusion: 教程旨在扩展LLMs在跨模态时间序列分析中的实际应用，同时为参与者提供全面理解。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [100] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 该论文将扩散和基于流的生成模型扩展到权重空间学习，通过梯度流匹配框架统一轨迹推断技术，优化权重生成和初始化。


<details>
  <summary>Details</summary>
Motivation: 扩散和基于流的生成模型在图像合成等领域表现优异，但尚未广泛应用于权重空间学习。论文旨在利用优化动态的结构先验，提升权重生成和下游任务性能。

Method: 提出梯度流匹配框架，统一轨迹推断技术，结合奖励微调、自动编码器、任务上下文数据等，优化权重生成。

Result: 实验表明，该方法在生成权重、下游训练初始化和性能微调方面优于基线，并在安全关键系统中检测有害协变量偏移时表现更优。

Conclusion: 论文通过梯度流匹配框架为权重空间学习提供了新方法，展示了其在生成、初始化和安全应用中的潜力。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [101] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer是一种基于图增强Transformer的深度学习模型，用于预测足球比赛结果，通过多级交互框架捕捉球员和团队的动态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 足球比赛结果预测具有挑战性，现有方法常忽略球员和团队间的异质性交互，影响模型准确性。

Method: HIGFormer包含三个模块：Player Interaction Network（捕捉球员动态）、Team Interaction Network（建模团队历史关系）和Match Comparison Transformer（综合分析预测结果）。

Result: 在WyScout Open Access数据集上，HIGFormer显著提升了预测准确性，并为球员表现评估提供了新视角。

Conclusion: HIGFormer通过多级交互建模，有效提升了足球比赛预测的准确性，并扩展了球员和团队分析的应用场景。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [102] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: 论文提出了一种名为GHPO的新框架，通过动态调整任务难度和结合模仿学习与强化学习，解决了RLVR在LLMs训练中的不稳定和低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在训练大型语言模型时存在训练不稳定和低效的问题，尤其是对于资源有限的小型模型。

Method: 提出GHPO框架，通过自适应提示调整动态校准任务难度，结合模仿学习和强化学习。

Result: 在六个数学基准测试中平均性能提升约5%，显著优于现有方法。

Conclusion: GHPO提供了一种可扩展且高效的解决方案，显著提升了训练稳定性和最终推理性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [103] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: 提出RFF-GP-HSMM方法，通过随机傅里叶特征降低GP-HSMM的计算成本，实现快速无监督时间序列分割。


<details>
  <summary>Details</summary>
Motivation: GP-HSMM在训练时需要计算大规模核矩阵的逆，计算成本高，难以处理大规模数据。

Method: 利用随机傅里叶特征近似高斯过程，将其转化为线性回归问题，避免核矩阵求逆。

Result: 在CMU运动捕捉数据集上，分割性能与传统方法相当，速度提升约278倍。

Conclusion: RFF-GP-HSMM在保持性能的同时显著降低了计算成本，适用于大规模时间序列分割。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [104] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP是一种低开销的持续学习方法，结合ReLUDown和Decreasing Backpropagation机制，在Continual ImageNet上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中模型在适应新任务时遗忘旧知识的问题，平衡可塑性和稳定性。

Method: 结合ReLUDown（轻量级激活修改）和Decreasing Backpropagation（梯度调度方案）。

Result: 在Continual ImageNet上表现优于或匹配现有方法，同时降低计算成本。

Conclusion: RDBP为持续学习提供了实用解决方案和清晰的基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [105] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein距离的离线强化学习方法，通过输入凸神经网络（ICNNs）建模最优传输映射，避免了对抗训练，提升了稳定性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在数据收集成本高的场景（如机器人）中很有价值，但分布偏移问题导致策略不可靠。现有方法多基于密度比度量，本文提出更鲁棒的Wasserstein距离。

Method: 使用Wasserstein距离和ICNNs建模最优传输映射，无需对抗训练，计算更稳定。

Result: 在D4RL基准数据集上表现优于或与现有方法相当。

Conclusion: 提出的方法有效解决了分布偏移问题，性能优越且稳定。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [106] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier是一种新的分类框架，用高斯分布替代传统确定性逻辑，通过最小化KL散度统一不确定性校准和潜在控制。


<details>
  <summary>Details</summary>
Motivation: 解决温度缩放和流形近似问题，提供对类别置信度和几何一致性的自然解释。

Method: 用对角高斯分布逻辑替代确定性逻辑，最小化预测高斯分布与单位各向同性高斯之间的KL散度。

Result: 在CIFAR-10和CIFAR-100上表现优于softmax分类器，提升鲁棒性、校准性和潜在分离性。

Conclusion: ZClassifier在分类和生成任务中均表现出色，提供了一种概率化的统一框架。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [107] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: 提出了一种基于Hopfield神经网络的轻量级AI模型，用于生物声学分析，具有快速训练、低计算需求和透明性。


<details>
  <summary>Details</summary>
Motivation: 解决生物声学分析中数据量大、训练数据有限、环境负担和硬件需求高的问题。

Method: 使用Hopfield神经网络存储信号并检测相似信号，仅需每个目标声音的一个代表性信号进行训练。

Result: 模型在蝙蝠回声定位数据集上达到86%的准确率，训练速度快（3ms），内存占用低（144.09MB）。

Conclusion: 该模型为快速、轻量、可持续且透明的生物声学分析提供了新方案。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [108] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: 该论文提出了一种基于神经网络的框架，用于估计随机Petri网（SPN）中依赖于外部协变量的转移速率参数，解决了传统方法在部分观测和噪声数据下的挑战。


<details>
  <summary>Details</summary>
Motivation: 随机Petri网在流行病学和系统生物学等领域广泛应用，但其参数估计在协变量依赖和部分观测数据下仍具挑战性。

Method: 采用轻量级1D卷积残差网络，通过端到端训练学习从噪声和部分观测数据中预测速率函数系数，并利用蒙特卡洛dropout提供不确定性估计。

Result: 在20%事件缺失的合成数据上，该方法恢复速率函数系数的RMSE为0.108，且速度显著快于传统贝叶斯方法。

Conclusion: 研究表明，数据驱动的无似然替代方法能够在复杂、部分观测的离散事件系统中实现准确、鲁棒和实时的参数恢复。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [109] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种同时应对训练数据异常值和分布不确定性的方法，通过Wasserstein-1 DRO优化广义线性模型，并证明了在数据污染下的估计误差为O(√ε)。


<details>
  <summary>Details</summary>
Motivation: 解决分布鲁棒优化（DRO）在训练数据存在异常值时的有效性下降问题，同时应对分布不确定性。

Method: 提出了一种结合数据污染鲁棒性和分布鲁棒性的建模框架，并设计了一种高效算法，基于稳健统计方法解决优化问题。

Result: 在数据污染情况下，方法实现了对真实DRO目标值的O(√ε)估计误差。

Conclusion: 该研究首次为数据污染和分布偏移双重挑战下的学习提供了严格的理论保证和高效计算方法。

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [110] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: 提出了一种神经符号框架Ground-Compose-Reinforce，用于从数据中学习语言基础，并通过语言直接指导RL代理行为，避免了手动设计领域特定元素。


<details>
  <summary>Details</summary>
Motivation: 解决语言在复杂感知和动作中的基础问题，避免传统方法中手动设计或大规模数据标注的需求。

Method: 采用神经符号框架，结合数据驱动学习和形式语言的组合语义，实现高效的语言基础和行为指导。

Result: 在图像网格世界和MuJoCo机器人领域的实验中，该方法在有限数据下成功将形式语言指令映射到行为，而端到端数据驱动方法失败。

Conclusion: 该框架通过数据驱动和形式语言的组合语义，实现了高效的语言基础和行为指导，适用于复杂环境中的代理任务。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [111] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 介绍了一个用于评估AI模型在汽车空气动力学预测中性能的开源基准框架，旨在提升透明度和一致性。


<details>
  <summary>Details</summary>
Motivation: 为CAE社区提供一个标准化的方法，以比较AI模型的准确性、性能和可扩展性，加速研究和创新。

Method: 在NVIDIA PhysicsNeMo-CFD框架中开发了一个可扩展的基准框架，评估了三种AI模型（DoMINO、X-MeshGraphNet、FIGConvNet）在DrivAerML数据集上的表现。

Result: 框架展示了AI模型在表面和体积流场预测中的性能，并提供了集成新模型和数据集的指南。

Conclusion: 该框架有助于研究人员和行业专业人士选择和优化AI驱动的空气动力学建模方法，推动更高效、准确和可解释的解决方案的发展。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [112] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个软件框架，用于通过生成去噪模型对连续变量进行空间推理。


<details>
  <summary>Details</summary>
Motivation: 去噪生成模型已成为图像生成的标准方法，但在多连续变量推理中的应用尚需探索，且相关基础设施开发成本高。

Method: 提供易用接口，支持变量映射、生成模型范式和推理策略的灵活控制。

Result: 框架开源，旨在促进该领域研究。

Conclusion: Spatial Reasoners为生成推理研究提供了高效工具。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [113] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: 提出Phy-SSM方法，结合部分物理知识与状态空间模型，提升复杂环境下长期动态预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中噪声和不规则采样数据的长期动态预测问题，结合物理知识提升模型泛化能力。

Method: 将部分已知物理知识分解为已知和未知状态矩阵，融入Phy-SSM单元，并引入物理状态正则化项。

Result: 在车辆运动、无人机状态和COVID-19预测等实验中，Phy-SSM在长期内插和外推任务中表现优于基线。

Conclusion: Phy-SSM通过结合物理知识和状态空间模型，显著提升了复杂环境下的长期预测能力。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [114] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出了多臂采样框架，作为多臂老虎机优化问题的采样对应。研究发现采样无需探索，并提出了一个简单算法实现最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究采样中的探索-利用权衡，并与多臂老虎机问题建立联系。

Method: 系统定义采样框架的遗憾概念，提出简单算法并证明其最优性。

Result: 理论结果表明采样无需探索，算法达到最优遗憾界。

Conclusion: 多臂采样框架在采样研究中具有基础性作用，尤其对熵正则化强化学习、预训练模型微调和RLHF有启示。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [115] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: 提出了一种新的因果框架，用于在时域序列中推断事件对的因果关系，特别关注外域干预的影响，并通过Transformer模型提升ATE估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要关注域内事件类型，忽略了外域干预对因果动态的影响，而现实中这些干预可能显著改变因果关系。

Method: 设计了一个新的因果框架来定义ATE，超越经典的Rubin框架，并提出了一种基于Transformer的神经网络模型，以处理长程时间依赖性和局部模式，同时整合外域干预信息。

Result: 在模拟和真实数据集上的实验表明，该方法在外域干预增强的点过程中优于基线模型，ATE估计和拟合优度表现更佳。

Conclusion: 该研究填补了外域干预对因果动态影响的空白，提出的框架和模型在实际应用中具有显著优势。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [116] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: 论文提出语义上下文（SC）是工具编排的基础，通过理论、实证和实际应用验证其重要性。


<details>
  <summary>Details</summary>
Motivation: 研究语义上下文在工具编排中的作用，以提高学习效率和适应性。

Method: 提出SC-LinUCB算法，结合大语言模型进行实证验证，并设计FiReAct流程。

Result: SC显著提升工具编排的样本效率、适应性和扩展性。

Conclusion: SC为构建高效、自适应和可扩展的编排代理提供了全面指导。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [117] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: 利用图卷积网络（GCN）解决受限品种优化问题，通过小规模训练实现大规模高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统品种优化问题因组合和非线性特性通常为NP难问题，需要高效解决方案。

Method: 构建品种问题的图表示，训练GCN学习最优品种模式，提出两种基于GCN输出的推断策略。

Result: 实验表明，GCN在小规模训练后能高效解决大规模问题（90%+最优性），优于现有启发式策略。

Conclusion: GCN框架在品种优化中表现出高效性和可扩展性，适用于模型未知但数据可用的场景。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [118] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: 论文提出了一种基于视觉的认知重评增强方法，利用文本到图像扩散模型生成支持性视觉反馈，显著降低了负面情绪。


<details>
  <summary>Details</summary>
Motivation: 传统的认知重评依赖高阶认知和语言能力，对创伤或抑郁患者效果有限，因此需要一种更直观、视觉化的方法。

Method: 通过稳定扩散模型和微调的IP适配器，将用户的口头重评转化为情感一致的视觉反馈，保持与原刺激的结构相似性。

Result: 实验表明，AI辅助的重评显著降低了负面情绪，且情感一致性越强，情绪调节效果越好。

Conclusion: 生成式视觉输入能有效支持认知重评，为生成式AI、情感计算和治疗技术的结合开辟了新方向。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [119] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于图自动编码器的潜在动力学替代模型（GALDS），用于高效模拟神经树中的物质运输，显著提升了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经元的复杂几何结构对物质运输模拟提出了计算挑战，传统方法耗时且资源密集，需要优化。

Method: GALDS结合图自动编码器和神经ODE，通过潜在空间表示和全局图预测系统动力学。

Result: 在未见几何和异常运输案例中，GALDS平均相对误差3%，最大误差<8%，速度提升10倍。

Conclusion: GALDS为神经树物质运输模拟提供了高效且准确的解决方案。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [120] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 提出了一种基于编码器-解码器架构的小型语言模型（SLM），用于提升产品和服务税码的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 跨国企业每天处理大量交易，需遵守不同司法管辖区的复杂税务规定，准确预测税码（如HSN或SAC）对合规至关重要。

Method: 采用编码器-解码器架构的SLM，利用非结构化数据预测层次化税码序列，捕捉税码间的依赖关系。

Result: 实验表明，该模型在结构化税码序列预测任务中优于扁平分类器及仅编码器或仅解码器架构。

Conclusion: 该方法可扩展至其他政府规定的税码体系，如UNSPSC或NCM，展示了SLM在税务领域的潜力。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [121] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种基于模拟的生成模型（SiGMoID），用于从噪声、稀疏或不完整数据中精确推断非线性动态系统的ODE参数和未观测部分。


<details>
  <summary>Details</summary>
Motivation: 非线性动态模型的系统推断在数据噪声大、稀疏或不完整时具有挑战性，需要一种鲁棒且精确的方法。

Method: 结合物理信息神经网络与超网络构建ODE求解器，并使用Wasserstein生成对抗网络估计参数。

Result: SiGMoID能量化数据噪声、估计系统参数并推断未观测部分，实验验证了其广泛适用性。

Conclusion: SiGMoID为科学研究和工程系统中的动态系统推断提供了有效工具。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [122] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 论文研究了对抗性遗忘问题，提出了一种保护模型性能的新方法，以防止因遗忘请求导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: AI模型需要遗忘以满足法规要求（如AI法案或GDPR），或移除有毒内容、减少偏见等。但遗忘可能导致性能下降，恶意方可能利用此点发起对抗性遗忘攻击。

Method: 研究了对抗性遗忘现象，分析了影响因素（如模型结构和数据选择策略），并提出了一种保护模型性能的新方法。

Result: 展示了对抗性遗忘对模型性能的影响，并验证了新方法的有效性。

Conclusion: 提出的方法能有效保护模型性能，无论是自然遗忘还是恶意攻击导致的性能下降。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [123] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: 论文研究了预测库存仓库中满足客户需求的单位数量及其相关运输成本的问题，提出了一个概率预测模型，用于RL环境中的模拟。


<details>
  <summary>Details</summary>
Motivation: 准确建模库存和运输成本对区域库存规划至关重要，尤其是在使用强化学习（RL）开发控制策略时。

Method: 将问题框架为概率预测问题，建模所有仓库在每个时间段的出库量和运输成本的联合分布，考虑库存位置和外部客户需求。

Result: 初步结果显示模型在分布内设置下的准确性。

Conclusion: 提出的验证方案利用生产系统评估模型在RL策略引起的反事实库存状态下的表现，确保模型在RL环境中的鲁棒性。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [124] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: 论文提出了一种考虑类别间数据难度差异的核心集选择方法，通过引入类别难度可分性系数（CDSC）和改进的采样策略，显著提升了数据修剪的效果。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法假设数据难度在类别间均匀分布，忽略了实际中数据难度可能按类别聚集的现象，导致性能下降。

Method: 提出类别难度可分性系数（CDSC）量化类别间数据难度差异，并设计类别比例调整的采样策略（如CCS-CP）。

Result: 在多个数据集上，新方法在极端修剪率下表现稳定，性能下降显著低于传统方法。

Conclusion: 显式建模类别难度可分性能提升数据修剪的有效性和鲁棒性，尤其在高风险场景中。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [125] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: 本文研究了在肽段从头测序中使用扩散解码器的方法，以解决传统自回归解码器的级联错误问题。实验表明，扩散解码器结合DINOISER损失函数在氨基酸召回率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码器在肽段测序中存在级联错误和未能有效利用高置信度区域的问题，因此探索扩散解码器的应用。

Method: 研究采用三种扩散解码器设计，结合背包束搜索和多种损失函数进行实验。

Result: 最佳扩散解码器设计结合DINOISER损失函数，氨基酸召回率显著提高0.373，但肽段精度和召回率仍为0。

Conclusion: 扩散解码器在提高模型敏感性和肽段从头测序方面具有潜力。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [126] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 综述了机器学习在半导体薄膜沉积中的应用，重点分析了物理信息神经网络（PINNs）的潜力与局限，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 半导体薄膜沉积过程复杂且需要精确控制，传统方法存在局限性，机器学习尤其是PINNs为解决这些问题提供了新思路。

Method: 通过主题分析，总结了ML在薄膜沉积中的应用趋势、局限性和研究空白，并探讨了PINNs的物理知识嵌入策略。

Result: 研究发现PINNs在提升薄膜沉积过程的解释性、准确性和鲁棒性方面具有潜力，但仍需解决现有方法论的不足。

Conclusion: 提出了整合PINNs优势的未来研究方向，旨在提升半导体制造的精度、可扩展性和操作效率。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [127] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: 提出了一种名为StellarF的新模型，用于恒星耀斑预测，结合了LoRA和Adapter技术，实现了参数高效学习。


<details>
  <summary>Details</summary>
Motivation: 恒星耀斑预测是天文学的重要课题，但现有方法受限于数据稀疏和缺乏领域专用的大规模预测模型。

Method: StellarF整合了耀斑统计信息模块和历史耀斑记录模块，支持多尺度模式识别。

Result: 在自建数据集（来自Kepler和TESS光变曲线）上，StellarF表现优于现有方法。

Conclusion: StellarF为天体物理研究和跨学科应用提供了新的方法论框架。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [128] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv是一个轻量级、与学习器无关的分布式环境执行接口，采用DETACH模式解耦模拟与训练，并通过AAPS机制减少同步开销。


<details>
  <summary>Details</summary>
Motivation: 现有框架将模拟、学习逻辑和编排耦合为单一系统，限制了模块化和可重用性。

Method: 提出ClusterEnv接口和DETACH模式，将reset()和step()操作卸载到远程工作节点；引入AAPS机制以减少策略过时问题。

Result: 实验表明，AAPS在离散控制任务中实现了高样本效率，且权重更新次数显著减少。

Conclusion: ClusterEnv无缝集成到现有RL流程中，支持多种学习方法，代码改动少。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [129] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了奖励函数中终端目标和工具目标的混淆问题，指出这种混淆会导致强化学习中的严重错位。


<details>
  <summary>Details</summary>
Motivation: 奖励函数通常无法完美表达人类目标，而是混合了终端目标和工具目标，导致优化时性能下降。

Method: 通过一个简单例子展示终端目标和工具目标的轻微混淆如何导致严重错位，并分析了环境特性对强化学习敏感性的影响。

Result: 研究表明，奖励函数中的目标混淆会导致优化结果与真实目标严重偏离。

Conclusion: 论文强调了奖励函数设计中对终端目标和工具目标区分的重要性，以避免强化学习中的错位问题。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [130] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: 提出了一种基于混合输入变分自编码器（VAE）的潜在空间扰动框架，用于生成不易察觉的对抗样本，适用于表格数据。


<details>
  <summary>Details</summary>
Motivation: 表格数据的异构性（混合分类和数值特征）使得对抗攻击面临独特挑战，传统梯度方法生成的对抗样本易偏离原始数据分布。

Method: 使用混合输入VAE将分类嵌入和数值特征统一到潜在流形中，生成统计一致的对抗样本，并引入In-Distribution Success Rate (IDSR) 衡量统计不可区分性。

Result: 在六个公开数据集和三种模型架构上验证，该方法比传统输入空间攻击和其他VAE方法表现更优，生成样本的离群率更低。

Conclusion: 研究表明，基于VAE的对抗攻击依赖于重建质量，但在足够训练数据下具有实际优势，强调了在流形上扰动对表格数据攻击的重要性。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [131] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon是一个基于Muon优化器的自适应学习率框架，通过两个模块增强Muon，提升了训练效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模模型训练中Muon优化器的效率问题，进一步提升收敛速度和训练稳定性。

Method: AdaMuon引入两个模块：(1) 参数级第二矩调制，捕捉正交梯度更新；(2) RMS对齐的重新缩放，调整更新幅度。

Result: 实验表明AdaMuon在多种模型规模和学习率下均优于Muon，收敛更快且稳定。

Conclusion: AdaMuon无需额外调参，可直接集成到现有Muon训练流程中，显著提升性能。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [132] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: 利用温度数据预测湍流动能（TKE），通过机器学习模型在实验燃烧中取得显著成功。


<details>
  <summary>Details</summary>
Motivation: 探索温度数据与TKE之间的关系，以改进火灾环境中的燃烧过程理解和模型预测。

Method: 使用多种机器学习模型（如深度神经网络、随机森林回归等）分析温度扰动与TKE的时空动态相关性。

Result: 尽管预测变量与目标变量相关性较弱，机器学习模型仍能准确预测TKE，回归模型表现尤为突出。

Conclusion: 研究展示了机器学习在火灾环境数据分析中的潜力，为火灾研究和管理的进步提供了新方法。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [133] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的后训练量化方法FOEM，通过显式结合一阶梯度项改进量化误差补偿，显著提升了大型语言模型的量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于补偿的权重校准方法假设一阶项在训练良好的全精度模型中可忽略，但研究发现渐进补偿过程会引入累积的一阶偏差，导致这一假设不成立。

Method: FOEM通过直接计算潜在权重与全精度权重的差异来近似梯度，避免高成本的反向传播计算，并利用预计算的Cholesky因子实时恢复Hessian子矩阵的逆。

Result: 在3位权重量化中，FOEM将Llama3-8B的困惑度降低了89.6%，并将Llama3-70B的5-shot MMLU准确率从51.7%提升至74.9%，接近全精度性能。

Conclusion: FOEM不仅显著优于现有方法，还能无缝集成其他先进技术，进一步缩小与全精度基线的差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [134] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 论文提出了一种基于价值梯度的策略优化方法（REPPO），结合了路径策略梯度的样本效率与标准策略学习的简单性，降低了训练方差和资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统策略梯度方法的高方差问题以及路径策略梯度对准确动作条件价值函数的依赖，提出一种适用于在线学习的稳定算法。

Method: 通过平衡随机策略探索和约束策略更新，设计了一种基于相对熵的路径策略优化（REPPO），并优化了价值函数学习架构。

Result: 实验表明，REPPO在样本需求、训练时间、内存占用和超参数鲁棒性方面表现优异。

Conclusion: REPPO成功结合了路径策略梯度的效率与在线学习的简单性，为策略优化提供了高效稳定的解决方案。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [135] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: GATE框架通过自适应图表示和新型注意力机制，显著提升了Wi-Fi指纹室内定位的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习和图神经网络在室内定位中因忽略非欧几里得噪声和设备异构性而表现不佳。

Method: 提出GATE框架，包括注意力超空间向量（AHV）、多维超空间向量（MDHV）和实时边构建（RTEC）方法。

Result: GATE在多个真实场景中实现了1.6x至4.72x的更低平均定位误差和1.85x至4.57x的更低最坏情况误差。

Conclusion: GATE通过建模非欧几里得噪声和动态图适应，显著提升了室内定位性能。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [136] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: 本文提出了一种基于数学公式的MILP实例距离度量方法，通过离散化处理实现高效实例比较，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MILP缺乏有效的实例相似性度量方法，现有方法精度不足或依赖标签数据，限制了其应用和泛化能力。

Method: 提出一种数学距离度量方法，通过离散化右端项、权重和变量，并借鉴Earth mover's distance量化约束比较中的分布差异。

Result: 贪婪版本在保持高精度的同时速度提升近200倍，优于现有非学习方法，与监督分类器性能相当。

Conclusion: 该方法为MILP实例比较提供了高效且无监督的解决方案，具有广泛的应用潜力。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [137] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LoRA和适配器的高效参数微调方法，用于检测大规模日志数据中的异常序列，相比传统方法性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或深度学习的日志异常检测方法在处理大规模复杂日志序列时效果不佳，因此需要更高效的方法。

Method: 采用低秩适应（LoRA）和适配器微调方法，对比不同小型大语言模型在Thunderbird数据集上的表现。

Result: LoRA微调方法比LogBert全微调性能提升18-19%，准确率达到97.76%-98.83%，而后者为79.37%。

Conclusion: LoRA微调方法在大规模日志异常检测中表现优异，显著优于传统方法。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [138] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯在线变化点检测（BOCPD）的方法，通过监测强化学习评论网络的时间价值变化，检测无人机导航中的微小行为偏差，以应对隐蔽的漂移欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖GNSS伪距测量进行定位和导航，但易受漂移欺骗攻击，传统检测方法因延迟问题无法及时响应，因此需要更鲁棒的时间尺度检测方法。

Method: 采用贝叶斯在线变化点检测（BOCPD）方法，结合强化学习评论网络的时间价值变化监测，以识别攻击起始点。

Result: 实验表明，该方法在检测漂移欺骗攻击时，比传统GNSS欺骗检测器、半监督学习框架和Page-Hinkley测试具有更高的准确性和更低的误报率。

Conclusion: 提出的时间价值框架显著提升了无人机对隐蔽欺骗攻击的检测能力，增强了导航系统的鲁棒性。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [139] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于梯度正则化的神经格兰杰因果模型（GRNGC），解决了现有方法计算成本高和捕捉复杂交互能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经格兰杰因果模型需要为每个时间序列构建单独模型，计算成本高；且稀疏性惩罚削弱了捕捉复杂交互的能力。

Method: GRNGC仅需一个时间序列预测模型，并对输入输出梯度施加L1正则化以推断因果关系，支持多种架构（如KAN、MLP、LSTM）。

Result: 在模拟和真实数据集（如DREAM、Lorenz-96、基因数据）上表现优于基线，显著降低计算开销。

Conclusion: GRNGC灵活高效，适用于多种时间序列预测模型，在基因调控网络重建中表现优异。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [140] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 本文综述了混合专家（MoE）架构在大语言模型中的应用，分析了其性能优势、核心设计及挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨MoE架构如何提升模型性能并减少计算开销，为相关研究提供参考。

Method: 系统分析MoE的理论基础、架构设计、专家门控与路由机制、多任务学习等。

Result: MoE在模型容量、任务性能及扩展性方面表现优越，但需关注专家多样性和推理聚合。

Conclusion: MoE架构潜力巨大，但仍需解决多样性和校准问题，未来研究方向明确。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [141] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: 提出一种通信高效的联邦学习方案，通过低秩近似和量化减少网络负载，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中频繁交换模型更新导致通信开销大，需解决此问题。

Method: 采用低秩近似神经网络梯度和量化技术。

Result: 显著降低网络负载，对模型精度影响极小。

Conclusion: 该方案有效解决了联邦学习中的通信效率问题。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [142] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: 本文提出了一种结合分类和回归模型的机器学习框架，用于心脏疾病的检测和风险预测，通过SMOTE技术解决数据不平衡问题，并在真实和合成数据上取得了高准确性和低误差。


<details>
  <summary>Details</summary>
Motivation: 心脏疾病是全球健康的主要问题，传统诊断方法准确性不足，机器学习可提升诊断效率和准确性。

Method: 使用Heart Disease数据集（1,035例），应用SMOTE生成100,000个合成数据点，结合分类和回归模型进行检测和预测。

Result: 随机森林分类模型在真实和合成数据上分别达到97.2%和97.6%的准确率；线性回归模型在真实和合成数据上的R2值分别为0.992和0.984。

Conclusion: 机器学习可显著改善心脏疾病诊断和风险预测，支持早期干预和临床决策。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [143] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种隐私保护的分布式学习框架，用于在线协作医疗预测平台，以解决隐私和预测质量问题。


<details>
  <summary>Details</summary>
Motivation: 在线协作医疗预测平台虽便利，但隐私问题和预测质量低可能阻碍患者和医生的参与。

Method: 提出隐私保护机制，并整合到一次性分布式学习框架中，满足隐私和性能需求。

Result: 理论证明该框架在特定隐私要求下可实现最优预测性能，并通过实验验证。

Conclusion: 该隐私保护协作医疗预测平台有效解决了隐私和性能的双重挑战。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [144] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: 论文探讨了梯度下降在逻辑回归中的行为，特别是在数据具有相同幅度的条件下是否能保证全局收敛。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降在逻辑回归中的收敛行为，特别是在非可分数据集上的循环行为。

Method: 通过理论分析，探讨数据幅度相等时梯度下降的收敛性，并在一维和高维空间中进行验证。

Result: 在一维空间中，数据幅度相等时全局收敛成立；但在高维空间中仍可能出现循环行为。

Conclusion: 希望进一步研究循环行为的普遍性，并寻找保证全局收敛的充分条件。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [145] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 论文提出了一种结合生成模型和判别模型的两阶段训练方法，以提高点击率（CTR）预测的精度。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GPT）在表达能力上超越判别模型，但两者数据需求不同，如何结合二者优势提升CTR预测精度是研究动机。

Method: 设计两阶段训练：1）生成预训练，基于用户行为序列预测下一项；2）在判别式CTR框架中微调生成模型。

Result: 实验和在线A/B测试验证了方法的有效性，模型已部署于全球最大电商平台之一。

Conclusion: 生成模型能显著提升CTR预测精度，未来将公开代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [146] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm是一种新型优化器，结合Adam的自适应矩估计和李雅普诺夫稳定性机制，通过动态调整学习率提升收敛鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中的噪声梯度和不稳定收敛问题。

Method: 将Adam与李雅普诺夫稳定性理论结合，动态调整学习率。

Result: 在CIFAR-10和CIFAR-100上表现优于现有优化器，准确率、收敛速度和稳定性均有提升。

Conclusion: LyAm是深度学习优化的强有力候选方法。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [147] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: 论文提出了一种基于Neyman-Rubin潜在结果框架的深度强化学习方法，通过存储历史价值网络输出优化数据利用，显著提升样本效率并减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习通常需要大量训练步骤和经验回放缓冲区，导致高计算和资源成本。本文旨在通过因果理论改进数据利用效率。

Method: 利用Neyman-Rubin潜在结果框架，建立事实损失的因果边界，存储历史价值网络输出以优化经验回放缓冲区。

Result: 在Atari 2600和MuJoCo实验中，奖励比率提升高达2,427%，经验回放缓冲区大小减少96%，样本效率显著提高。

Conclusion: 该方法通过因果理论优化数据利用，显著提升深度强化学习的效率和性能，且成本可忽略。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [148] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: 研究了在插值条件下SGD的收敛性，特别是在大学习率下最后迭代的表现，提出了新的收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究SGD在插值条件下的行为，特别是对过参数化模型训练、持续学习中的遗忘以及随机Kaczmarz方法收敛的影响。

Method: 在β-平滑凸损失函数上运行T步SGD，学习率η≤1/β，分析最后迭代的期望超额风险。

Result: 得到最后迭代的期望超额风险为O~(1/(ηT^{1−βη/2}) + ηT^{βη/2}σ⋆^2)，在特定条件下达到最优收敛率。

Conclusion: 扩展了Varre等人的结果，改进了Evron等人的收敛率，为SGD在插值条件下的应用提供了理论支持。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [149] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: 提出了一种公平奖励模型（FRM）框架，用于减少大语言模型在高风险决策中的偏见，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在高风险决策中可能放大的不公平偏见问题，确保其可信赖性。

Method: 训练一个通用的公平奖励模型（FRM），为LLM的推理分配公平分数，减少偏见路径的权重。

Result: FRM在无需额外微调的情况下跨任务、领域和模型家族迁移，并在实际任务中提高公平性和准确性。

Conclusion: FRM框架能有效提升大语言模型在高风险决策中的公平性，同时保持或超越基线准确性。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [150] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文指出神经符号（NeSy）预测器中符号概念独立性假设的局限性，证明其无法表示某些概念组合的不确定性，导致模型无法识别推理捷径。


<details>
  <summary>Details</summary>
Motivation: 探讨独立性假设是否限制神经符号系统的学习能力和不确定性建模能力。

Method: 通过形式化分析，证明独立性假设导致模型无法表示某些概念组合的不确定性。

Result: 独立性假设使模型无法识别推理捷径，即预测正确但理由错误的行为。

Conclusion: 独立性假设在神经符号预测器中存在根本性限制，需重新审视其适用性。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [151] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: 提出了一种无需反向传播的神经网络训练方法，通过局部信号和前向传播训练每一层，避免存储激活值和梯度问题，在强化学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播需要存储激活值且存在梯度消失或爆炸问题，影响学习性能和稳定性。

Method: 采用局部层间损失和多维尺度匹配原则，结合奖励驱动指导，在前向传播中训练每一层。

Result: 实验表明，该方法在强化学习基准测试中表现与反向传播方法相当，且更稳定、一致，在复杂环境中表现更优。

Conclusion: 该方法提供了一种高效、稳定的替代方案，特别适用于强化学习中的神经网络训练。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [152] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: 论文提出了一种结合变分自编码器（VAE）和现代Hopfield网络（MHN）的持续学习模型，显著减少了灾难性遗忘问题，并在Split-MNIST任务上取得了接近90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中面临的灾难性遗忘问题，借鉴人类大脑的互补学习系统（CLS）理论。

Method: 结合VAE的模式完成能力和MHN的模式分离能力，构建神经合理的持续学习模型。

Result: 在Split-MNIST任务上达到接近90%的准确率，显著减少遗忘，并通过表征分析验证了功能分离。

Conclusion: 该模型为生物和人工系统中的记忆巩固、泛化和持续学习提供了功能模板。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [153] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出了一种名为R-MTGB的新型多任务梯度提升框架，通过分块学习、任务分区和微调，有效处理任务异质性和对抗性任务，提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）假设任务间存在共享信息以提升泛化能力，但现实中存在对抗性任务会降低模型性能，需解决这一问题。

Method: R-MTGB框架分为三部分：学习共享模式、通过正则化参数分区任务、微调任务特定预测器。

Result: 实验表明R-MTGB能有效隔离对抗性任务，促进知识迁移，减少预测误差，提升整体性能。

Conclusion: R-MTGB在多任务学习中表现出鲁棒性、适应性和可靠收敛性。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [154] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: 该研究评估了多种激活函数在fNIRS分类任务中的表现，发现对称性激活函数（如Tanh和Abs(x）在某些架构中优于ReLU，并强调了激活函数选择对fNIRS数据信号特性的重要性。


<details>
  <summary>Details</summary>
Motivation: fNIRS领域中激活函数对深度学习性能的影响尚未系统研究，本研究旨在填补这一空白。

Method: 使用多种深度学习架构（如fNIRSNet、AbsoluteNet等）和标准化预处理，评估传统和领域特定激活函数在单一数据集上的表现。

Result: 对称性激活函数（如Tanh和Abs(x）在某些架构中表现优于ReLU，且对称性分析进一步支持其性能优势。

Conclusion: 选择与fNIRS信号特性匹配的激活函数对模型性能至关重要。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [155] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: iTransformer是当前多变量时间序列预测的高效模型，但其倒置框架存在局限性。作者提出了一种新的数据增强方法DAIF，通过频率过滤和跨变量修补策略改进模型性能。


<details>
  <summary>Details</summary>
Motivation: iTransformer的倒置框架虽能有效捕捉多变量相关性，但会削弱时间依赖性信息并引入噪声。为解决这些问题，作者提出了DAIF方法。

Method: 提出DAIF方法，包括频率过滤和跨变量修补两种策略，专门针对倒置框架设计。

Result: 实验表明，DAIF在多数据集和不同倒置模型中均有效。

Conclusion: DAIF成功解决了iTransformer的局限性，提升了多变量时间序列预测的性能。

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [156] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR框架通过两阶段LLM方法提升直肠癌淋巴结转移评估的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI评估和现有AI模型在淋巴结转移诊断中存在性能不足和缺乏可解释性的问题。

Method: LRMR框架分两阶段：多模态LLM生成结构化报告，文本LLM进行患者间特征比较和风险排名。

Result: 在117例患者中，LRMR的AUC为0.7917，F1-score为0.7200，优于ResNet50等基线模型。

Conclusion: 两阶段LLM框架为淋巴结转移评估提供了高效、可解释的新范式。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [157] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 论文研究了非线性和非平稳时间序列数据对联邦学习（FL）性能的影响，探讨了去趋势技术的作用，发现FL在非线性数据分布下表现较差，但适当的去趋势技术能提升FL性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）的发展，设备产生的非线性和非平稳时间序列数据对预测准确性提出挑战。传统集中式分析存在延迟和通信成本问题，而FL作为一种分布式学习方法，需解决数据分布不均的影响。

Method: 通过生成合成时间序列数据集（如广义极值分布和对数正态分布），使用LSTM模型在集中式和FL框架下进行训练，并评估不同去趋势技术对模型性能的影响。

Result: 实验表明，FL在非线性数据分布下表现不如集中式方法，但合适的去趋势技术能显著降低损失，提升FL性能。

Conclusion: 研究强调了在FL中处理非线性时间序列数据时去趋势技术的重要性，为实际应用（如能源预测）提供了改进方向。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [158] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: 论文探讨了基于强化学习的TractOracle-RL框架的四种扩展方法，提出了一种新的训练方案IRT，实验表明这些方法在准确性和解剖有效性上优于传统技术。


<details>
  <summary>Details</summary>
Motivation: 传统纤维追踪方法存在局限性，强化学习（RL）框架在纤维追踪中表现出潜力，但仍有改进空间。

Method: 扩展了TractOracle-RL框架，引入IRT训练方案，利用束过滤方法迭代优化奖励机制。

Result: 实验结果显示，结合RL框架的方法在多种数据集上表现稳健，准确性更高。

Conclusion: 强化学习结合解剖先验信息能显著提升纤维追踪的准确性和可靠性。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [159] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM提出了一种分层扩散策略框架，用于通信感知的LoRA适应，通过强化学习和扩散模型优化秩配置，显著降低了传输成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行大型语言模型（LLM）面临通信带宽和计算资源限制，现有LoRA方法的固定或启发式秩配置及参数传输效率低下。

Method: AirLLM将秩配置建模为结构化动作向量，使用PPO生成粗粒度决策，并通过DDIM细化生成高分辨率、任务和信道自适应的秩向量。

Result: 实验表明，AirLLM在不同信噪比下均能提升微调性能并显著降低传输成本。

Conclusion: AirLLM通过强化驱动和扩散优化的秩适应，实现了高效且可扩展的远程微调。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [160] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow是一种基于物理先验的变分自编码器，用于建模神经群体的动态结构和外部未观测影响。


<details>
  <summary>Details</summary>
Motivation: 神经群体的潜在动态结构和时间演化活动需要模型捕捉内在网络动态和外部未观测影响。

Method: 采用Langevin方程驱动的变分自编码器，结合物理先验（如惯性、阻尼、势函数和随机力）和局部耦合振荡器网络。

Result: 在合成数据和NLB基准测试中表现优异，优于现有方法，并匹配或超越行为指标解码。

Conclusion: LangevinFlow为复杂神经动态建模提供了灵活、高性能的物理启发框架。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [161] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: 论文提出了一种名为OSMPs的新框架，通过结合学习的微分同胚编码器和超临界Hopf分岔，解决了动态运动基元在捕捉复杂周期性行为和任务间插值方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 动态运动基元（DMPs）在稳定性和抗干扰性方面表现良好，但难以捕捉复杂周期性行为且任务间插值能力有限，限制了其实际应用范围。

Method: 引入OSMPs框架，结合学习的微分同胚编码器和超临界Hopf分岔，确保周期性运动的准确学习和轨道稳定性。通过任务条件化编码器，实现多运动目标的统一表示。

Result: 在多种机器人平台上验证了OSMPs的有效性，包括协作臂、软体机械手和仿生海龟机器人，性能优于现有基线方法。

Conclusion: OSMPs提供了一种高效且稳定的方法，能够广泛适用于周期性运动任务，并具有零样本泛化能力。

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [162] [Vision Language Action Models in Robotic Manipulation: A Systematic Review](https://arxiv.org/abs/2507.10672)
*Muhayy Ud Din,Waseem Akram,Lyes Saad Saoud,Jan Rosell,Irfan Hussain*

Main category: cs.RO

TL;DR: 综述了102个VLA模型、26个基础数据集和12个仿真平台，提出了分类框架和未来方向。


<details>
  <summary>Details</summary>
Motivation: 统一视觉感知、自然语言理解和机器人控制，推动通用机器人代理的发展。

Method: 分析模型架构、数据集和仿真平台，提出分类框架和评价标准。

Result: 揭示了当前数据集的不足，提出了未来研究方向如可扩展预训练和模块化设计。

Conclusion: 为通用机器人代理的发展提供了技术参考和概念路线图。

Abstract: Vision Language Action (VLA) models represent a transformative shift in
robotics, with the aim of unifying visual perception, natural language
understanding, and embodied control within a single learning framework. This
review presents a comprehensive and forward-looking synthesis of the VLA
paradigm, with a particular emphasis on robotic manipulation and
instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26
foundational datasets, and 12 simulation platforms that collectively shape the
development and evaluation of VLAs models. These models are categorized into
key architectural paradigms, each reflecting distinct strategies for
integrating vision, language, and control in robotic systems. Foundational
datasets are evaluated using a novel criterion based on task complexity,
variety of modalities, and dataset scale, allowing a comparative analysis of
their suitability for generalist policy learning. We introduce a
two-dimensional characterization framework that organizes these datasets based
on semantic richness and multimodal alignment, showing underexplored regions in
the current data landscape. Simulation environments are evaluated for their
effectiveness in generating large-scale data, as well as their ability to
facilitate transfer from simulation to real-world settings and the variety of
supported tasks. Using both academic and industrial contributions, we recognize
ongoing challenges and outline strategic directions such as scalable
pretraining protocols, modular architectural design, and robust multimodal
alignment strategies. This review serves as both a technical reference and a
conceptual roadmap for advancing embodiment and robotic control, providing
insights that span from dataset generation to real world deployment of
generalist robotic agents.

</details>


### [163] [Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots](https://arxiv.org/abs/2507.10694)
*Francesco Fuentes,Serigne Diagne,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 利用软体生长机器人作为环境探索和地图构建工具，通过碰撞行为建模和几何模拟器验证其在非结构化环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因其被动变形能力在非结构化环境中表现出色，但需要更好地理解碰撞和变形行为以利用其进行环境结构感知。

Method: 首先分析离散转向中的碰撞行为，开发几何模拟器模拟2D环境中的机器人轨迹，并通过蒙特卡洛采样验证模型。

Result: 在均匀和非均匀环境中，该方法能快速接近理想动作，展示了软体生长机器人在环境探索中的潜力。

Conclusion: 软体生长机器人可作为有效的环境探索工具，其碰撞行为建模和模拟器验证了其在非结构化环境中的应用前景。

Abstract: Passive deformation due to compliance is a commonly used benefit of soft
robots, providing opportunities to achieve robust actuation with few active
degrees of freedom. Soft growing robots in particular have shown promise in
navigation of unstructured environments due to their passive deformation. If
their collisions and subsequent deformations can be better understood, soft
robots could be used to understand the structure of the environment from direct
tactile measurements. In this work, we propose the use of soft growing robots
as mapping and exploration tools. We do this by first characterizing collision
behavior during discrete turns, then leveraging this model to develop a
geometry-based simulator that models robot trajectories in 2D environments.
Finally, we demonstrate the model and simulator validity by mapping unknown
environments using Monte Carlo sampling to estimate the optimal next deployment
given current knowledge. Over both uniform and non-uniform environments, this
selection method rapidly approaches ideal actions, showing the potential for
soft growing robots in unstructured environment exploration and mapping.

</details>


### [164] [LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control](https://arxiv.org/abs/2507.11464)
*Ajay Shankar,Keisuke Okumura,Amanda Prorok*

Main category: cs.RO

TL;DR: 提出了一种多机器人控制框架，结合离散规划和连续控制，实现高效的点对点导航。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在全环境信息下的点对点导航问题，确保无碰撞和死锁。

Method: 采用分层方法：1) 集中式离散规划器（MAPF）快速生成路径；2) 机器人独立的连续轨迹控制器。

Result: 框架LF结合LaCAM和Freyja，支持动态环境下的多机器人导航，并成功部署15架多旋翼无人机。

Conclusion: LF框架具有鲁棒性和可扩展性，适用于动态环境下的多机器人导航。

Abstract: We propose a multi-robot control paradigm to solve point-to-point navigation
tasks for a team of holonomic robots with access to the full environment
information. The framework invokes two processes asynchronously at high
frequency: (i) a centralized, discrete, and full-horizon planner for computing
collision- and deadlock-free paths rapidly, leveraging recent advances in
multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal
trajectory controllers that ensure all robots independently follow their
assigned paths reliably. This hierarchical shift in planning representation
from (i) discrete and coupled to (ii) continuous and decoupled domains enables
the framework to maintain long-term scalable motion synthesis. As an
instantiation of this idea, we present LF, which combines a fast
state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack
(Freyja) for executing agile robot maneuvers. LF provides a robust and
versatile mechanism for lifelong multi-robot navigation even under asynchronous
and partial goal updates, and adapts to dynamic workspaces simply by quick
replanning. We present various multirotor and ground robot demonstrations,
including the deployment of 15 real multirotors with random, consecutive target
updates while a person walks through the operational workspace.

</details>


### [165] [RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding](https://arxiv.org/abs/2507.10749)
*Benjamin Stoler,Juliet Yang,Jonathan Francis,Jean Oh*

Main category: cs.RO

TL;DR: 论文提出了一种名为RCG的场景生成框架，通过将碰撞语义融入对抗扰动流程，生成更真实的高风险驾驶场景，提升自动驾驶系统的训练和测试效果。


<details>
  <summary>Details</summary>
Motivation: 现实驾驶数据中安全关键场景稀缺，限制了自动驾驶系统的训练和评估效果。

Method: 通过对比预训练和微调构建安全感知行为表示，嵌入碰撞语义，并基于此改进现有场景生成流程。

Result: 实验显示，使用生成场景训练的自动驾驶系统下游成功率平均提升9.2%，且生成的对抗行为更真实。

Conclusion: RCG框架能生成更真实的高风险场景，有效提升自动驾驶系统的压力测试效果。

Abstract: Safety-critical scenarios are essential for training and evaluating
autonomous driving (AD) systems, yet remain extremely rare in real-world
driving datasets. To address this, we propose Real-world Crash Grounding (RCG),
a scenario generation framework that integrates crash-informed semantics into
adversarial perturbation pipelines. We construct a safety-aware behavior
representation through contrastive pre-training on large-scale driving logs,
followed by fine-tuning on a small, crash-rich dataset with approximate
trajectory annotations extracted from video. This embedding captures semantic
structure aligned with real-world accident behaviors and supports selection of
adversary trajectories that are both high-risk and behaviorally realistic. We
incorporate the resulting selection mechanism into two prior scenario
generation pipelines, replacing their handcrafted scoring objectives with an
embedding-based criterion. Experimental results show that ego agents trained
against these generated scenarios achieve consistently higher downstream
success rates, with an average improvement of 9.2% across seven evaluation
settings. Qualitative and quantitative analyses further demonstrate that our
approach produces more plausible and nuanced adversary behaviors, enabling more
effective and realistic stress testing of AD systems. Code and tools will be
released publicly.

</details>


### [166] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: 提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）连续分割未见物体，无需学习分割模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有未见物体实例分割（UOIS）方法因依赖静态视觉特征而泛化性能差的问题。

Method: 基于交互视觉原则，利用机器人交互产生的相对旋转和线性速度识别物体，实时生成和更新分割掩码。

Result: 平均分割准确率比现有UOIS方法高27.5%，并可作为视觉基础模型的提示提升性能。

Conclusion: rt-RISeg通过交互感知显著提升了未见物体的分割性能，且具有独立性和扩展性。

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [167] [Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection](https://arxiv.org/abs/2507.10814)
*Huiyi Wang,Fahim Shahriar,Alireza Azimi,Gautham Vasan,Rupam Mahmood,Colin Bellinger*

Main category: cs.RO

TL;DR: 该研究提出了一种将预训练模型（如大型语言模型和物体检测器）集成到目标条件强化学习中，以实现通用机器人抓取能力的方法。通过基于掩码的目标条件，提高了特征共享和泛化能力，实验显示在模拟任务中成功率约90%。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作（如抓取）在家庭和工作环境中至关重要，但传统方法学习物体交互资源消耗大。预训练模型能高效处理文本提示和识别物体，为解决这一问题提供了可能。

Method: 使用预训练的物体检测模型，通过文本提示识别物体并生成掩码，用于目标条件强化学习。掩码提供物体无关的线索，优化特征共享和泛化。

Result: 在模拟抓取任务中，基于掩码的目标条件方法成功率约90%，且收敛更快、回报更高，适用于分布内外的物体。

Conclusion: 该方法通过结合预训练模型和目标条件强化学习，显著提升了机器人抓取的通用性和效率。

Abstract: General-purpose robotic manipulation, including reach and grasp, is essential
for deployment into households and workspaces involving diverse and evolving
tasks. Recent advances propose using large pre-trained models, such as Large
Language Models and object detectors, to boost robotic perception in
reinforcement learning. These models, trained on large datasets via
self-supervised learning, can process text prompts and identify diverse objects
in scenes, an invaluable skill in RL where learning object interaction is
resource-intensive. This study demonstrates how to integrate such models into
Goal-Conditioned Reinforcement Learning to enable general and versatile robotic
reach and grasp capabilities. We use a pre-trained object detection model to
enable the agent to identify the object from a text prompt and generate a mask
for goal conditioning. Mask-based goal conditioning provides object-agnostic
cues, improving feature sharing and generalization. The effectiveness of the
proposed framework is demonstrated in a simulated reach-and-grasp task, where
the mask-based goal conditioning consistently maintains a $\sim$90\% success
rate in grasping both in and out-of-distribution objects, while also ensuring
faster convergence to higher returns.

</details>


### [168] [Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets](https://arxiv.org/abs/2507.10878)
*Savva Morozov,Tobia Marcucci,Bernhard Paus Graesdal,Alexandre Amice,Pablo A. Parrilo,Russ Tedrake*

Main category: cs.RO

TL;DR: 研究在凸集图（GCS）中的最短路径问题（SWP），提出了一种基于半定规划和增量搜索的近似解法，并展示了其在机器人混合离散-连续规划问题中的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 凸集图（GCS）为混合离散-连续规划问题提供了一种统一的建模语言，但缺乏高效的求解方法。

Method: 通过半定规划合成问题的成本函数下界，并利用增量搜索算法近似求解最短路径。

Result: 该方法在碰撞避免运动规划、技能链和混合系统最优控制等实验中表现出高性能和计算效率。

Conclusion: GCS中的SWP为多种机器人规划问题提供了统一的解决方案，兼具高效性和通用性。

Abstract: We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A
GCS is a graph where each vertex is paired with a convex program, and each edge
couples adjacent programs via additional costs and constraints. A walk in a GCS
is a sequence of vertices connected by edges, where vertices may be repeated.
The length of a walk is given by the cumulative optimal value of the
corresponding convex programs. To solve the SWP in GCS, we first synthesize a
piecewise-quadratic lower bound on the problem's cost-to-go function using
semidefinite programming. Then we use this lower bound to guide an
incremental-search algorithm that yields an approximate shortest walk. We show
that the SWP in GCS is a natural language for many mixed discrete-continuous
planning problems in robotics, unifying problems that typically require
specialized solutions while delivering high performance and computational
efficiency. We demonstrate this through experiments in collision-free motion
planning, skill chaining, and optimal control of hybrid systems.

</details>


### [169] [Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning](https://arxiv.org/abs/2507.10899)
*Wang Zhicheng,Satoshi Yagi,Satoshi Yamamori,Jun Morimoto*

Main category: cs.RO

TL;DR: 提出了一种基于SAM2的对象中心方法，用于提升移动操作机器人在不同方向下执行任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动操作框架通常将导航和操作解耦，导致导航不精确时性能下降，尤其是在角度偏差时。

Method: 利用SAM2基础模型，将操作方向信息融入模型，实现从不同方向对任务的一致理解。

Result: 在自定义移动操作机器人上测试，模型在多种角度下表现优于Action Chunking Transformer。

Conclusion: 该方法显著提升了基于模仿学习的移动操作系统的泛化性和鲁棒性。

Abstract: Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.

</details>


### [170] [Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization](https://arxiv.org/abs/2507.10914)
*James A. Preiss,Fengze Xie,Yiheng Lin,Adam Wierman,Yisong Yue*

Main category: cs.RO

TL;DR: 提出了一种名为M-GAPS的单轨迹在线策略优化算法，用于调整机器人控制器参数，适用于动态、策略类和目标随时间变化的情况。实验表明，M-GAPS在硬件测试中表现优于基于模型和无模型的基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究在动态、策略类和目标随时间变化的情况下，如何在线调整机器人控制器参数，以提供比经典自适应控制更灵活、比无模型强化学习更稳定和数据高效的方法。

Method: 提出M-GAPS算法，重新参数化四旋翼状态空间和策略类以优化搜索空间，并在硬件实验中与基于模型和无模型的基线方法进行比较。

Result: M-GAPS在硬件测试中更快找到接近最优参数，尤其在不利的片段长度下表现更好，并能快速适应未建模的风和负载干扰。

Conclusion: M-GAPS展示了在线策略优化在硬件实践中的实用性，提供了比经典自适应控制更大的灵活性，同时比无模型强化学习更稳定和数据高效。

Abstract: We study online algorithms to tune the parameters of a robot controller in a
setting where the dynamics, policy class, and optimality objective are all
time-varying. The system follows a single trajectory without episodes or state
resets, and the time-varying information is not known in advance. Focusing on
nonlinear geometric quadrotor controllers as a test case, we propose a
practical implementation of a single-trajectory model-based online policy
optimization algorithm, M-GAPS,along with reparameterizations of the quadrotor
state space and policy class to improve the optimization landscape. In hardware
experiments,we compare to model-based and model-free baselines that impose
artificial episodes. We show that M-GAPS finds near-optimal parameters more
quickly, especially when the episode length is not favorable. We also show that
M-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and
achieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our
results demonstrate the hardware practicality of this emerging class of online
policy optimization that offers significantly more flexibility than classic
adaptive control, while being more stable and data-efficient than model-free
reinforcement learning.

</details>


### [171] [Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances](https://arxiv.org/abs/2507.10950)
*Zhiwei Wu,Jiahao Luo,Siyi Wei,Jinhui Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种统一的建模和优化框架，用于提升多磁体嵌入式软连续体机器人（MeSCRs）的运动性能。通过建立基于扩展伪刚体模型的可微系统公式，分析了磁驱动下的平衡适定性和诱导构型的几何特性。


<details>
  <summary>Details</summary>
Motivation: 提升多磁体嵌入式软连续体机器人的运动性能，解决其运动控制自由度不足的问题。

Method: 建立基于扩展伪刚体模型的可微系统公式，开发基于微分几何的结构优化框架，将经典运动学指标与磁体配置关联。

Result: 最大可控自由度等于嵌入磁体数量的两倍；提出了优化条件和梯度数值方法，仿真验证了框架的有效性。

Conclusion: 该框架通过优化磁体配置显著提升了MeSCRs的运动性能，为软体机器人设计提供了新思路。

Abstract: This paper presents a unified modeling and optimization framework to enhance
the kinematic performance of multi-magnet embedded soft continuum robots
(MeSCRs). To this end, we establish a differentiable system formulation based
on an extended pseudo-rigid-body model. This formulation enables analysis of
the equilibrium well-posedness and the geometry of the induced configuration
under magnetic actuation. In particular, we show that the maximum controllable
degrees of freedom of a MeSCR equal twice the number of embedded magnets. We
subsequently develop a structural optimization framework based on differential
geometry that links classical kinematic measures (e.g., manipulability and
dexterity) to the configuration of embedded magnets. The resulting optimization
condition reveals that improving local performance requires structurally
modulating the spectrum of the configuration space metric to counteract its
distortion. Closed-form solutions for optimal magnet configurations are derived
under representative conditions, and a gradient-based numerical method is
proposed for general design scenarios. Simulation studies validate the
effectiveness of the proposed framework.

</details>


### [172] [SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging](https://arxiv.org/abs/2507.10968)
*Toktam Mohammadnejad,Jovin D'sa,Behdad Chalaki,Hossein Nourkhiz Mahjoub,Ehsan Moradi-Pari*

Main category: cs.RO

TL;DR: 本文提出了一种基于网格的运动规划器SMART-Merge，用于安全舒适的强制合并任务，通过优化成本项和引入速度启发式，实现了100%的成功率和最短合并时间。


<details>
  <summary>Details</summary>
Motivation: 高速公路合并是一项复杂的驾驶任务，需要识别安全间隙、调整速度并完成合并，同时保证安全和舒适。

Method: 采用基于网格的运动规划器SMART-Merge，通过优化成本项和引入速度启发式，实现高效合并。

Result: 在数百个高速公路合并场景的高保真模拟中，SMART-Merge实现了100%的成功率，且合并时间最短。

Conclusion: SMART-Merge能够可靠地处理复杂的强制合并任务，为自动驾驶高速公路合并提供了稳健的解决方案。

Abstract: Merging onto a highway is a complex driving task that requires identifying a
safe gap, adjusting speed, often interactions to create a merging gap, and
completing the merge maneuver within a limited time window while maintaining
safety and driving comfort. In this paper, we introduce a Safe Merging and
Real-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed
to facilitate safe and comfortable forced merging. By deliberately adapting
cost terms to the unique challenges of forced merging and introducing a desired
speed heuristic, SMART-Merge planner enables the ego vehicle to merge
successfully while minimizing the merge time. We verify the efficiency and
effectiveness of the proposed merge planner through high-fidelity CarMaker
simulations on hundreds of highway merge scenarios. Our proposed planner
achieves the success rate of 100% as well as completes the merge maneuver in
the shortest amount of time compared with the baselines, demonstrating our
planner's capability to handle complex forced merge tasks and provide a
reliable and robust solution for autonomous highway merge. The simulation
result videos are available at
https://sites.google.com/view/smart-merge-planner/home.

</details>


### [173] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer的多任务学习框架，用于提升社交机器人在多用户环境中的决策能力，并通过新损失函数和数据集验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多用户环境中，社交机器人需要理解上下文并决定何时及向谁回应，而现有研究主要关注单用户交互。

Method: 采用Transformer多任务学习框架，引入两种新损失函数以优化场景建模和回应选择，并构建了多用户HRI数据集。

Result: 实验表明，该模型在回应决策上优于现有启发式和单任务方法，达到最先进性能。

Conclusion: 研究推动了社交机器人在自然、上下文感知的多方交互中的发展。

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [174] [MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments](https://arxiv.org/abs/2507.11211)
*Chen Cai,Ernesto Dickel Saraiva,Ya-jun Pan,Steven Liu*

Main category: cs.RO

TL;DR: 提出了一种从粗到细的运动规划框架，用于机器人在杂乱、未建模环境中的操作，结合双摄像头感知和基于B样条的模型预测控制。


<details>
  <summary>Details</summary>
Motivation: 解决在杂乱、未建模环境中机器人操作的实时运动规划问题，适应动态变化和不确定性。

Method: 结合双摄像头感知和B样条MPC，逐步优化环境模型和运动规划，使用视觉成本函数和目标驱动探索，以及高效的碰撞检测。

Result: 实验验证了框架在不确定性和杂乱环境中的鲁棒性和适应性。

Conclusion: 该框架支持动态重新规划和闭环运动学，适用于复杂环境中的机器人操作。

Abstract: This letter presents a novel coarse-to-fine motion planning framework for
robotic manipulation in cluttered, unmodeled environments. The system
integrates a dual-camera perception setup with a B-spline-based model
predictive control (MPC) scheme. Initially, the planner generates feasible
global trajectories from partial and uncertain observations. As new visual data
are incrementally fused, both the environment model and motion planning are
progressively refined. A vision-based cost function promotes target-driven
exploration, while a refined kernel-perceptron collision detector enables
efficient constraint updates for real-time planning. The framework accommodates
closed-chain kinematics and supports dynamic replanning. Experiments on a
multi-arm platform validate its robustness and adaptability under uncertainties
and clutter.

</details>


### [175] [EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks](https://arxiv.org/abs/2507.10961)
*Joohwan Seo,Arvind Kruthiventy,Soomi Lee,Megan Teng,Xiang Zhang,Seoyeon Choi,Jongeun Choi,Roberto Horowitz*

Main category: cs.RO

TL;DR: 提出了一种名为EquiContact的分层框架，用于学习视觉驱动的机器人策略，实现接触丰富任务的空间泛化。


<details>
  <summary>Details</summary>
Motivation: 解决接触丰富任务（如peg-in-hole）中策略的空间泛化问题，从小样本演示中训练出鲁棒的策略。

Method: 采用分层策略：高层视觉规划器（Diff-EDF）和低层顺应性视觉运动策略（G-CompACT），利用局部观测和SE(3)-等变性设计。

Result: 在真实世界的peg-in-hole任务中，实现了接近完美的成功率和未见空间配置的鲁棒泛化。

Conclusion: EquiContact框架及其设计原则（顺应性、局部化和等变性）有效提升了接触丰富任务的空间泛化能力。

Abstract: This paper presents a framework for learning vision-based robotic policies
for contact-rich manipulation tasks that generalize spatially across task
configurations. We focus on achieving robust spatial generalization of the
policy for the peg-in-hole (PiH) task trained from a small number of
demonstrations. We propose EquiContact, a hierarchical policy composed of a
high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)
and a novel low-level compliant visuomotor policy (Geometric Compliant ACT,
G-CompACT). G-CompACT operates using only localized observations (geometrically
consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB
images) and produces actions defined in the end-effector frame. Through these
design choices, we show that the entire EquiContact pipeline is
SE(3)-equivariant, from perception to force control. We also outline three key
components for spatially generalizable contact-rich policies: compliance,
localized policies, and induced equivariance. Real-world experiments on PiH
tasks demonstrate a near-perfect success rate and robust generalization to
unseen spatial configurations, validating the proposed framework and
principles. The experimental videos can be found on the project website:
https://sites.google.com/berkeley.edu/equicontact

</details>


### [176] [Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks](https://arxiv.org/abs/2507.11283)
*Weiyi Liu,Jingzehua Xu,Guanwen Xie,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种扩散增强的强化学习方法，用于自主水下车辆（AUV）的鲁棒控制，解决了水下轨迹规划和动态环境适应的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂多变，传统控制方法在动态条件下表现不佳，需要一种更鲁棒和高效的解决方案。

Method: 结合扩散模型生成多步轨迹和强化学习优化策略，通过扩散U-Net架构编码状态，实现高效探索和稳定策略。

Result: 仿真实验表明，该方法在复杂海洋条件下优于传统控制方法，具有更高的适应性和可靠性。

Conclusion: 扩散增强的强化学习方法为AUV控制提供了更鲁棒和高效的解决方案，适用于动态水下任务。

Abstract: This paper presents a diffusion-augmented reinforcement learning (RL)
approach for robust autonomous underwater vehicle (AUV) control, addressing key
challenges in underwater trajectory planning and dynamic environment
adaptation. The proposed method integrates three core innovations: (1) A
diffusion-based trajectory generation framework that produces physically
feasible multi-step trajectories, enhanced by a high-dimensional state encoding
mechanism combining current observations with historical states and actions
through a novel diffusion U-Net architecture, significantly improving
long-horizon planning. (2) A sample-efficient hybrid learning architecture that
synergizes diffusion-guided exploration with RL policy optimization, where the
diffusion model generates diverse candidate actions and the RL critic selects
optimal actions, achieving higher exploration efficiency and policy stability
in dynamic underwater environments. Extensive simulation experiments validating
the method's superior robustness and flexibility, outperforms conventional
control methods in challenging marine conditions, offering enhanced
adaptability and reliability for AUV operations in the underwater tasks.

</details>


### [177] [Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants](https://arxiv.org/abs/2507.11460)
*Jacinto Colan,Ana Davila,Yutaro Yamada,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文系统综述了自主手术机器人助手（ASARs）的研究进展与挑战，重点关注其在手术中为外科医生提供主动支持的应用。


<details>
  <summary>Details</summary>
Motivation: 推动人机协作在手术中的应用，提升自主机器人系统的能力以辅助复杂手术。

Method: 遵循PRISMA指南，对IEEE Xplore、Scopus和Web of Science数据库中的32项研究进行详细分析。

Result: 研究发现ASARs主要应用于内窥镜引导，并在自主工具操作方面取得进展，但仍面临动作对齐、程序意识、信息交换和技能获取等挑战。

Conclusion: 综述总结了当前趋势，指出了关键限制，并提出了未来研究方向，以提高手术中人机协作的可靠性、安全性和有效性。

Abstract: Human-robot collaboration in surgery represents a significant area of
research, driven by the increasing capability of autonomous robotic systems to
assist surgeons in complex procedures. This systematic review examines the
advancements and persistent challenges in the development of autonomous
surgical robotic assistants (ASARs), focusing specifically on scenarios where
robots provide meaningful and active support to human surgeons. Adhering to the
PRISMA guidelines, a comprehensive literature search was conducted across the
IEEE Xplore, Scopus, and Web of Science databases, resulting in the selection
of 32 studies for detailed analysis. Two primary collaborative setups were
identified: teleoperation-based assistance and direct hands-on interaction. The
findings reveal a growing research emphasis on ASARs, with predominant
applications currently in endoscope guidance, alongside emerging progress in
autonomous tool manipulation. Several key challenges hinder wider adoption,
including the alignment of robotic actions with human surgeon preferences, the
necessity for procedural awareness within autonomous systems, the establishment
of seamless human-robot information exchange, and the complexities of skill
acquisition in shared workspaces. This review synthesizes current trends,
identifies critical limitations, and outlines future research directions
essential to improve the reliability, safety, and effectiveness of human-robot
collaboration in surgical environments.

</details>


### [178] [Multi-IMU Sensor Fusion for Legged Robots](https://arxiv.org/abs/2507.11447)
*Shuo Yang,John Z. Zhang,Ibrahima Sory Sow,Zachary Manchester*

Main category: cs.RO

TL;DR: 提出一种基于多惯性测量单元的低成本、轻量化状态估计方法，用于腿式机器人在复杂运动条件下的低漂移位姿和速度估计。


<details>
  <summary>Details</summary>
Motivation: 解决标准本体感知里程计在复杂运动条件下的主要误差源问题。

Method: 利用多个惯性测量单元和关节编码器数据，通过扩展卡尔曼滤波融合，再结合相机数据在因子图滑动窗口估计器中形成视觉-惯性-腿里程计方法。

Result: 在包含强烈地面冲击、足部滑动和突然身体旋转的复杂任务中，算法始终实现最小位置偏差。

Conclusion: 该方法在复杂运动条件下表现优异，且开源了C++实现和大规模数据集。

Abstract: This paper presents a state-estimation solution for legged robots that uses a
set of low-cost, compact, and lightweight sensors to achieve low-drift pose and
velocity estimation under challenging locomotion conditions. The key idea is to
leverage multiple inertial measurement units on different links of the robot to
correct a major error source in standard proprioceptive odometry. We fuse the
inertial sensor information and joint encoder measurements in an extended
Kalman filter, then combine the velocity estimate from this filter with camera
data in a factor-graph-based sliding-window estimator to form a
visual-inertial-leg odometry method. We validate our state estimator through
comprehensive theoretical analysis and hardware experiments performed using
real-world robot data collected during a variety of challenging locomotion
tasks. Our algorithm consistently achieves minimal position deviation, even in
scenarios involving substantial ground impact, foot slippage, and sudden body
rotations. A C++ implementation, along with a large-scale dataset, is available
at https://github.com/ShuoYangRobotics/Cerberus2.0.

</details>


### [179] [LLM-based ambiguity detection in natural language instructions for collaborative surgical robots](https://arxiv.org/abs/2507.11525)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型（LLMs）的框架，用于检测手术场景中自然语言指令的模糊性，以提高人机协作的安全性。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令的模糊性在安全关键的人机交互（如手术）中带来风险，需要一种机制来识别和解决这些模糊性。

Method: 使用多种提示技术配置的LLM评估器集合，结合链式思维评估器和共形预测，检测语言、上下文、程序和关键模糊性。

Result: 在Llama 3.2 11B和Gemma 3 12B上测试，分类准确率超过60%，能有效区分模糊和非模糊手术指令。

Conclusion: 该方法通过提前识别模糊指令，提高了手术中人机协作的安全性和可靠性。

Abstract: Ambiguity in natural language instructions poses significant risks in
safety-critical human-robot interaction, particularly in domains such as
surgery. To address this, we propose a framework that uses Large Language
Models (LLMs) for ambiguity detection specifically designed for collaborative
surgical scenarios. Our method employs an ensemble of LLM evaluators, each
configured with distinct prompting techniques to identify linguistic,
contextual, procedural, and critical ambiguities. A chain-of-thought evaluator
is included to systematically analyze instruction structure for potential
issues. Individual evaluator assessments are synthesized through conformal
prediction, which yields non-conformity scores based on comparison to a labeled
calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed
classification accuracy exceeding 60% in differentiating ambiguous from
unambiguous surgical instructions. Our approach improves the safety and
reliability of human-robot collaboration in surgery by offering a mechanism to
identify potentially ambiguous instructions before robot action.

</details>


### [180] [Uncertainty Aware Mapping for Vision-Based Underwater Robots](https://arxiv.org/abs/2507.10991)
*Abhimanyu Bhowmik,Mohit Singh,Madhushree Sannigrahi,Martin Ludvigsen,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文探讨了如何在基于视觉的水下机器人中表示地图不一致性，并将深度估计置信度融入体素地图框架Voxblox中，改进了权重计算和更新机制。


<details>
  <summary>Details</summary>
Motivation: 传统传感器和预规划路径在受限空间内无法适用，视觉传感器噪声和环境变化导致环境表示不确定性高。

Method: 使用RAFT-Stereo模型估计场景深度和置信度，并集成到Voxblox框架中，改进了权重计算和更新机制。

Result: 在受限水池和Trondheim峡湾的码头进行了实验，水下机器人展示了不确定性变化的可视化效果。

Conclusion: 提出的方法能有效表示地图不一致性，并提升视觉水下机器人在受限空间中的环境感知能力。

Abstract: Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.

</details>


### [181] [ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations](https://arxiv.org/abs/2507.11000)
*Minwoo Cho,Jaehwi Jang,Daehyung Park*

Main category: cs.RO

TL;DR: 提出了一种名为ILCL的新方法，通过遗传算法和强化学习结合，学习时间约束逻辑。


<details>
  <summary>Details</summary>
Motivation: 解决从演示中学习时间约束逻辑的问题，以复现类似演示的逻辑约束行为。

Method: 采用两玩家零和游戏框架，结合遗传算法（GA-TL-Mining）和逻辑约束强化学习（Logic-CRL）。

Result: 在四个时间约束任务中优于现有方法，并成功应用于现实任务。

Conclusion: ILCL方法在学习和转移时间约束逻辑方面表现出色。

Abstract: We aim to solve the problem of temporal-constraint learning from
demonstrations to reproduce demonstration-like logic-constrained behaviors.
Learning logic constraints is challenging due to the combinatorially large
space of possible specifications and the ill-posed nature of non-Markovian
constraints. To figure it out, we introduce a novel temporal-constraint
learning method, which we call inverse logic-constraint learning (ILCL). Our
method frames ICL as a two-player zero-sum game between 1) a genetic
algorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained
reinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax
trees for parameterized truncated linear temporal logic (TLTL) without
predefined templates. Subsequently, Logic-CRL finds a policy that maximizes
task rewards under the constructed TLTL constraints via a novel constraint
redistribution scheme. Our evaluations show ILCL outperforms state-of-the-art
baselines in learning and transferring TL constraints on four temporally
constrained tasks. We also demonstrate successful transfer to real-world
peg-in-shallow-hole tasks.

</details>


### [182] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav是一个基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，实现了零样本场景理解和专家级调优。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在动态和非结构化环境中表现不佳，强化学习方法因泛化能力差和仿真多样性不足而难以实际应用。

Method: 利用多模态大语言模型推理和条件变分自编码器，结合单样本示例和思维链提示策略，实现超参数自适应调整。

Result: LE-Nav在多样场景中生成超参数达到人类调优水平，实际导航试验和用户研究表明其在成功率、效率、安全性和舒适性上优于现有方法。

Conclusion: LE-Nav通过场景感知和自适应调优，显著提升了导航性能和社会接受度，适用于动态和非结构化环境。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [183] [Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments](https://arxiv.org/abs/2507.11006)
*Ashutosh Mishra,Shreya Santra,Hazal Gozbasi,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 研究提出了一种结合自主机器人功能与人类干预控制（HITL）的先进方法，以提高在月球任务中不确定和挑战性环境下的机器人操作能力。


<details>
  <summary>Details</summary>
Motivation: 解决在极端环境下（如月球任务）机器人操作的可靠性问题，通过结合人类决策与自主功能，提升任务效率和适应性。

Method: 采用可扩展的梯状结构和机器人操纵器，结合实时反馈进行精确部署；利用数字孪生仿真增强系统鲁棒性，并通过人类干预处理模糊场景。

Result: 系统在模拟月球条件下测试，验证了其在极端光照、多变地形、变化载荷和传感器限制下的可靠性。

Conclusion: 该方法显著提高了机器人操作在复杂环境中的可靠性和效率，为未来空间任务提供了可行的解决方案。

Abstract: This study presents an advanced approach to enhance robotic manipulation in
uncertain and challenging environments, with a focus on autonomous operations
augmented by human-in-the-loop (HITL) control for lunar missions. By
integrating human decision-making with autonomous robotic functions, the
research improves task reliability and efficiency for space applications. The
key task addressed is the autonomous deployment of flexible solar panels using
an extendable ladder-like structure and a robotic manipulator with real-time
feedback for precision. The manipulator relays position and force-torque data,
enabling dynamic error detection and adaptive control during deployment. To
mitigate the effects of sinkage, variable payload, and low-lighting conditions,
efficient motion planning strategies are employed, supplemented by human
control that allows operators to intervene in ambiguous scenarios. Digital twin
simulation enhances system robustness by enabling continuous feedback,
iterative task refinement, and seamless integration with the deployment
pipeline. The system has been tested to validate its performance in simulated
lunar conditions and ensure reliability in extreme lighting, variable terrain,
changing payloads, and sensor limitations.

</details>


### [184] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景并优化高斯分布，显著提升了稀疏视角和动态环境下的3D几何重建效果。


<details>
  <summary>Details</summary>
Motivation: 透明物体的3D几何重建因反射和折射等物理特性而具有挑战性，尤其是在稀疏视角和动态环境中。

Method: TRAN-D通过分离透明物体与背景，优化对应高斯分布，并使用物体感知损失和物理模拟减少伪影和过拟合。

Result: 在合成和真实场景中，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度提升1.5倍。

Conclusion: TRAN-D在透明物体重建中表现出色，为稀疏视角和动态环境提供了高效解决方案。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [185] [Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems](https://arxiv.org/abs/2507.11076)
*Andreas Mueller,Shivesh Kumar*

Main category: cs.RO

TL;DR: 论文提出了二阶闭式时间导数的运动方程（EOM），替代现有递归算法，为机器人控制提供更直观的结构理解。


<details>
  <summary>Details</summary>
Motivation: 机器人控制需要平滑轨迹及控制力/力矩的时间导数，现有递归算法缺乏直观性。

Method: 采用李群理论，推导出紧凑且易参数化的二阶闭式EOM时间导数。

Result: 提出的方法提供了EOM导数的直接结构解析，适用于弹性多体系统。

Conclusion: 闭式EOM导数方法为机器人设计和控制提供了更高效且直观的解决方案。

Abstract: Derivatives of equations of motion(EOM) describing the dynamics of rigid body
systems are becoming increasingly relevant for the robotics community and find
many applications in design and control of robotic systems. Controlling robots,
and multibody systems comprising elastic components in particular, not only
requires smooth trajectories but also the time derivatives of the control
forces/torques, hence of the EOM. This paper presents the time derivatives of
the EOM in closed form up to second-order as an alternative formulation to the
existing recursive algorithms for this purpose, which provides a direct insight
into the structure of the derivatives. The Lie group formulation for rigid body
systems is used giving rise to very compact and easily parameterized equations.

</details>


### [186] [Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm](https://arxiv.org/abs/2507.11133)
*Luca Beber,Edoardo Lamon,Giacomo Moretti,Matteo Saveriano,Luca Fambri,Luigi Palopoli,Daniele Fontanelli*

Main category: cs.RO

TL;DR: 论文探讨了机器人系统在估计材料粘弹性参数中的准确性，并初步验证了其在生物样本中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 诊断活动（如超声扫描和触诊）虽低成本但易出错，机器人解决方案可减少主观性并缩短等待时间。

Method: 通过机器人系统测量各种材料的粘弹性参数，并与高精度仪器测得的基准数据对比。

Result: 实验结果显示机器人系统的准确性与基准数据高度匹配。

Conclusion: 机器人系统在临床应用中具有潜在价值。

Abstract: Diagnostic activities, such as ultrasound scans and palpation, are relatively
low-cost. They play a crucial role in the early detection of health problems
and in assessing their progression. However, they are also error-prone
activities, which require highly skilled medical staff. The use of robotic
solutions can be key to decreasing the inherent subjectivity of the results and
reducing the waiting list. For a robot to perform palpation or ultrasound
scans, it must effectively manage physical interactions with the human body,
which greatly benefits from precise estimation of the patient's tissue
biomechanical properties. This paper assesses the accuracy and precision of a
robotic system in estimating the viscoelastic parameters of various materials,
including some tests on ex vivo tissues as a preliminary proof-of-concept
demonstration of the method's applicability to biological samples. The
measurements are compared against a ground truth derived from silicone
specimens with different viscoelastic properties, characterised using a
high-precision instrument. Experimental results show that the robotic system's
accuracy closely matches the ground truth, increasing confidence in the
potential use of robots for such clinical applications.

</details>


### [187] [A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty](https://arxiv.org/abs/2507.11170)
*Giulio Giacomuzzo,Mohamed Abdelwahab,Marco Calì,Alberto Dalla Libera,Ruggero Carli*

Main category: cs.RO

TL;DR: 提出一种基于学习的鲁棒反馈线性化策略，用于拉格朗日系统的精确轨迹跟踪，结合高斯过程回归（GPR）估计模型失配，并通过鲁棒化控制器保证渐近跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决拉格朗日系统在模型失配无先验界情况下的精确轨迹跟踪问题。

Method: 采用GPR估计模型失配，结合经典反馈线性化外环，并通过鲁棒化控制器补偿剩余不确定性。

Result: 数值实验验证了策略在2自由度平面机器人上的有效性，证明了高概率下的渐近跟踪性能。

Conclusion: 提出的方法在模型失配未知情况下实现了高精度的轨迹跟踪。

Abstract: In this paper, we propose a novel learning-based robust feedback
linearization strategy to ensure precise trajectory tracking for an important
family of Lagrangian systems. We assume a nominal knowledge of the dynamics is
given but no a-priori bounds on the model mismatch are available. In our
approach, the key ingredient is the adoption of a regression framework based on
Gaussian Processes (GPR) to estimate the model mismatch. This estimate is added
to the outer loop of a classical feedback linearization scheme based on the
nominal knowledge available. Then, to compensate for the residual uncertainty,
we robustify the controller including an additional term whose size is designed
based on the variance provided by the GPR framework. We proved that, with high
probability, the proposed scheme is able to guarantee asymptotic tracking of a
desired trajectory. We tested numerically our strategy on a 2 degrees of
freedom planar robot.

</details>


### [188] [Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors](https://arxiv.org/abs/2507.11241)
*Tobias Kern,Leon Tolksdorf,Christian Birkner*

Main category: cs.RO

TL;DR: 研究通过缩小比例的车辆测试视觉和视觉-惯性算法在自定位精度上的表现，发现OpenVINS表现最佳，且缩小比例车辆在旋转运动估计上与真实尺寸车辆无显著差异。


<details>
  <summary>Details</summary>
Motivation: 加速自动驾驶功能开发，探索缩小比例车辆在自定位算法测试中的可行性。

Method: 选择ROS2兼容的视觉和视觉-惯性算法（OpenVINS、VINS-Fusion、RTAB-Map），记录缩小比例车辆数据并与真实尺寸车辆数据对比。

Result: OpenVINS平均定位误差最低，缩小比例车辆在平移运动估计上有微小差异，但旋转运动估计无显著差异。

Conclusion: 缩小比例车辆可作为自定位算法的测试平台，OpenVINS表现最优。

Abstract: Physically reduced-scale vehicles are emerging to accelerate the development
of advanced automated driving functions. In this paper, we investigate the
effects of scaling on self-localization accuracy with visual and
visual-inertial algorithms using cameras and an inertial measurement unit
(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms
are selected, and datasets are chosen as a baseline for real-sized vehicles. A
test drive is conducted to record data of reduced-scale vehicles. We compare
the selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in
terms of their pose accuracy against the ground-truth and against data from
real-sized vehicles. When comparing the implementation of the selected
localization algorithms to real-sized vehicles, OpenVINS has the lowest average
localization error. Although all selected localization algorithms have
overlapping error ranges, OpenVINS also performs best when applied to a
reduced-scale vehicle. When reduced-scale vehicles were compared to real-sized
vehicles, minor differences were found in translational vehicle motion
estimation accuracy. However, no significant differences were found when
comparing the estimation accuracy of rotational vehicle motion, allowing RSVRs
to be used as testing platforms for self-localization algorithms.

</details>


### [189] [Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection](https://arxiv.org/abs/2507.11270)
*Ting-Wei Ou,Jia-Hao Jiang,Guan-Lin Huang,Kuu-Young Young*

Main category: cs.RO

TL;DR: 提出了一种针对病毒热点区域的移动机器人紫外线消毒系统，优化了紫外线剂量分配，显著减少消毒时间。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了医院自动化消毒的紧迫性，现有研究忽视了人类活动对病毒分布的影响。

Method: 设计了一种移动机器人系统，优先消毒高风险区域，优化紫外线剂量分配。

Result: 在两种医院场景中，消毒时间分别减少了30.7%和31.9%，同时保持相同的消毒效果。

Conclusion: 该系统提高了消毒效率，减少了低风险区域的不必要暴露。

Abstract: The COVID-19 pandemic has severely affected public health, healthcare
systems, and daily life, especially amid resource shortages and limited
workers. This crisis has underscored the urgent need for automation in hospital
environments, particularly disinfection, which is crucial to controlling virus
transmission and improving the safety of healthcare personnel and patients.
Ultraviolet (UV) light disinfection, known for its high efficiency, has been
widely adopted in hospital settings. However, most existing research focuses on
maximizing UV coverage while paying little attention to the impact of human
activity on virus distribution. To address this issue, we propose a mobile
robotic system for UV disinfection focusing on the virus hotspot. The system
prioritizes disinfection in high-risk areas and employs an approach for
optimized UV dosage to ensure that all surfaces receive an adequate level of UV
exposure while significantly reducing disinfection time. It not only improves
disinfection efficiency but also minimizes unnecessary exposure in low-risk
areas. In two representative hospital scenarios, our method achieves the same
disinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,
respectively. The video of the experiment is available at:
https://youtu.be/wHcWzOcoMPM.

</details>


### [190] [Diffusion-Based Imaginative Coordination for Bimanual Manipulation](https://arxiv.org/abs/2507.11296)
*Huilin Xu,Jian Ding,Jiakun Xu,Ruixiang Wang,Jun Chen,Jinjie Mai,Yanwei Fu,Bernard Ghanem,Feng Xu,Mohamed Elhoseiny*

Main category: cs.RO

TL;DR: 提出了一种基于扩散的统一框架，联合优化视频和动作预测，显著提升双手机器人协调任务的性能。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作在工业自动化和家庭服务中至关重要，但高维动作空间和复杂协调需求带来挑战。视频预测的潜力在此领域尚未充分探索。

Method: 采用多帧潜在预测策略压缩未来状态，并提出单向注意力机制，视频预测依赖于动作预测，但动作预测独立于视频预测。

Result: 在模拟和真实实验中，方法显著优于基线ACT，ALOHA提升24.9%，RoboTwin提升11.1%，真实实验提升32.5%。

Conclusion: 提出的框架有效解决了双手机器人协调问题，代码和模型已开源。

Abstract: Bimanual manipulation is crucial in robotics, enabling complex tasks in
industrial automation and household services. However, it poses significant
challenges due to the high-dimensional action space and intricate coordination
requirements. While video prediction has been recently studied for
representation learning and control, leveraging its ability to capture rich
dynamic and behavioral information, its potential for enhancing bimanual
coordination remains underexplored. To bridge this gap, we propose a unified
diffusion-based framework for the joint optimization of video and action
prediction. Specifically, we propose a multi-frame latent prediction strategy
that encodes future states in a compressed latent space, preserving
task-relevant features. Furthermore, we introduce a unidirectional attention
mechanism where video prediction is conditioned on the action, while action
prediction remains independent of video prediction. This design allows us to
omit video prediction during inference, significantly enhancing efficiency.
Experiments on two simulated benchmarks and a real-world setting demonstrate a
significant improvement in the success rate over the strong baseline ACT using
our method, achieving a \textbf{24.9\%} increase on ALOHA, an \textbf{11.1\%}
increase on RoboTwin, and a \textbf{32.5\%} increase in real-world experiments.
Our models and code are publicly available at
https://github.com/return-sleep/Diffusion_based_imaginative_Coordination.

</details>


### [191] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: 该论文提出了一种仅依赖视觉的飞行控制方法，使用事件相机和神经网络替代传统惯性传感器，实现了无人机的稳定飞行。


<details>
  <summary>Details</summary>
Motivation: 许多飞行生物依赖视觉进行姿态控制，而飞行机器人通常依赖惯性传感器。研究旨在探索仅依赖视觉的飞行控制方法，以支持更小、更自主的飞行机器人。

Method: 使用向下事件相机和递归卷积神经网络，通过监督学习训练，从事件流中估计姿态和旋转速率。

Result: 实验表明，该方法可替代传统惯性测量单元，实现稳定飞行。网络在跨环境泛化中表现良好，窄视野变体泛化能力更强。

Conclusion: 视觉飞行控制是昆虫级飞行机器人自主化的有前景方案。

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


### [192] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: 论文提出了一种结合层次化操作模型的演员-规划器系统（RAE+UPOM），在机器人上实现了鲁棒的任务执行。


<details>
  <summary>Details</summary>
Motivation: 解决符号规划器模型与实际机器人控制结构不一致的问题。

Method: 结合反应式执行引擎（RAE）和蒙特卡洛规划器（UPOM），共享层次化操作模型。

Result: 在真实环境中实现了鲁棒的任务执行，能够应对动作失败和传感器噪声。

Conclusion: RAE+UPOM系统有效解决了规划与执行的不一致问题，并提供了决策过程的实证分析。

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


### [193] [From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League](https://arxiv.org/abs/2507.11402)
*Supun Dissanayaka,Alexander Ferrein,Till Hofmann,Kosuke Nakajima,Mario Sanz-Lopez,Jesus Savage,Daniel Swoboda,Matteo Tschesche,Wataru Uemura,Tarik Viehmann,Shohei Yasuda*

Main category: cs.RO

TL;DR: 本文提出了RoboCup智能制造联盟的愿景，旨在通过多赛道设计反映现代工厂的各个方面，提升竞赛的吸引力和相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的RoboCup物流联盟专注于生产物流，但未涵盖智能制造的最新发展，导致其相关性下降。

Method: 设计一个新的竞赛——RoboCup智能制造联盟，包含多个独立但逐步结合的赛道，涵盖工业机器人挑战和生产物流。

Result: 预期新竞赛将更吸引新老团队，并聚焦于工业机器人的当前和未来挑战。

Conclusion: 新竞赛有望通过更全面的智能制造场景提升竞赛的吸引力和技术相关性。

Abstract: The RoboCup Logistics League is a RoboCup competition in a smart factory
scenario that has focused on task planning, job scheduling, and multi-agent
coordination. The focus on production logistics allowed teams to develop highly
competitive strategies, but also meant that some recent developments in the
context of smart manufacturing are not reflected in the competition, weakening
its relevance over the years. In this paper, we describe the vision for the
RoboCup Smart Manufacturing League, a new competition designed as a larger
smart manufacturing scenario, reflecting all the major aspects of a modern
factory. It will consist of several tracks that are initially independent but
gradually combined into one smart manufacturing scenario. The new tracks will
cover industrial robotics challenges such as assembly, human-robot
collaboration, and humanoid robotics, but also retain a focus on production
logistics. We expect the reenvisioned competition to be more attractive to
newcomers and well-tried teams, while also shifting the focus to current and
future challenges of industrial robotics.

</details>


### [194] [Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming](https://arxiv.org/abs/2507.11498)
*Asad Ali Shahid,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 本文介绍了Robot Drummer，一种能够进行高精度、富有表现力鼓乐演奏的人形机器人系统，通过强化学习实现多段音乐的长时程演奏。


<details>
  <summary>Details</summary>
Motivation: 探索人形机器人在音乐表演等表达性领域的潜力，解决鼓乐演奏中的快速时间控制、多肢协调等挑战。

Method: 将鼓乐谱转化为节奏接触链，将乐曲分解为固定长度段，并行训练单一强化学习策略。

Result: 在三十多首流行摇滚、金属和爵士乐曲中，Robot Drummer表现出高F1分数，并涌现出类似人类的鼓乐策略。

Conclusion: 强化学习能有效推动人形机器人进入创意音乐表演领域，展示了其潜力。

Abstract: Humanoid robots have seen remarkable advances in dexterity, balance, and
locomotion, yet their role in expressive domains, such as music performance,
remains largely unexplored. Musical tasks, like drumming, present unique
challenges, including split-second timing, rapid contacts, and multi-limb
coordination over pieces lasting minutes. In this paper, we introduce Robot
Drummer, a humanoid system capable of expressive, high-precision drumming
across a diverse repertoire of songs. We formulate humanoid drumming as
sequential fulfillment of timed-contacts and transform drum scores in to a
Rhythmic Contact Chain. To handle the long-horizon nature of musical
performance, we decompose each piece into fixed-length segments and train a
single policy across all segments in parallel using reinforcement learning.
Through extensive experiments on over thirty popular rock, metal, and jazz
tracks, our results demonstrate that Robot Drummer consistently achieves high
F1 scores. The learned behaviors exhibit emergent human-like drumming
strategies, such as cross-arm strikes, and adaptive sticks assignments,
demonstrating the potential of reinforcement learning to bring humanoid robots
into the domain of creative musical performance. Project page:
\href{https://robot-drummer.github.io}{robot-drummer.github.io}

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [195] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Main category: cs.MA

TL;DR: 提出了一种基于多智能体强化学习（MARL）的无人机群协作避障框架，利用图像处理领域的知识驱动奖励。


<details>
  <summary>Details</summary>
Motivation: 解决无人机群在复杂环境中协作避障的问题，避免传统MARL方法中复杂的信用分配和观测共享机制。

Method: 通过将障碍物建模为二维场中的最大值，利用轮廓近似避免碰撞，同时减少智能体间的交互。

Result: 实验表明，该框架在大型无人机群中表现优异，适应复杂环境且能量高效。

Conclusion: 该框架为无人机群协作避障提供了一种高效且适应性强的解决方案。

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [196] [Protocols for Verifying Smooth Strategies in Bandits and Games](https://arxiv.org/abs/2507.10567)
*Miranda Christ,Daniel Reichman,Jonathan Shafer*

Main category: cs.GT

TL;DR: 研究在多臂老虎机和正规形式游戏中验证策略近似最优性的协议，提出查询次数低于动作数量的验证方法。


<details>
  <summary>Details</summary>
Motivation: 由于玩家可选动作数量庞大，需要开发查询次数低于动作数量的验证协议。

Method: 针对足够平滑的策略，设计验证协议，确保策略的ε-最优性，并证明其查询复杂度低于学习过程。

Result: 验证协议在多臂老虎机中有效，且查询复杂度接近理论下限。应用至正规形式游戏，验证近似强平滑纳什均衡。

Conclusion: 验证协议在动作数量庞大时仍高效，为策略验证提供实用工具。

Abstract: We study protocols for verifying approximate optimality of strategies in
multi-armed bandits and normal-form games. As the number of actions available
to each player is often large, we seek protocols where the number of queries to
the utility oracle is sublinear in the number of actions. We prove that such
verification is possible for sufficiently smooth strategies that do not put too
much probability mass on any specific action. We provide protocols for
verifying that a smooth policy for a multi-armed bandit is
$\varepsilon$-optimal. Our verification protocols require provably fewer arm
queries than learning. Furthermore, we establish a nearly-tight lower bound on
the query complexity of verification in our settings. As an application, we
show how to use verification for bandits to achieve verification in normal-form
games. This gives a protocol for verifying whether a given strategy profile is
an approximate strong smooth Nash equilibrium, with a query complexity that is
sublinear in the number of actions.

</details>


### [197] [Pricing with Tips in Three-Sided Delivery Platforms](https://arxiv.org/abs/2507.10872)
*Yannai A. Gonczarowski,Gary Qiurui Ma,David C. Parkes*

Main category: cs.GT

TL;DR: 论文研究了配送平台中买家、商店和骑手三方交易的均衡问题，重点探讨了小费对市场均衡和效率的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解小费在配送平台定价中的作用，以及其对市场均衡和效率的影响。

Method: 通过建模三方交易市场，分析有无小费时的均衡存在性和效率，并探讨计算最优均衡的复杂性。

Result: 结果表明，小费能确保均衡存在，且带小费的最优均衡福利更高，但高效均衡可能不存在且计算最优均衡是NP难问题。

Conclusion: 结论指出，在特定市场结构下，高效带小费均衡存在且可多项式时间计算。

Abstract: We model a delivery platform facilitating transactions among three sides:
buyers, stores, and couriers. In addition to buyers paying store-specific
purchase prices and couriers receiving store--buyer-specific delivery
compensation from the platform, each buyer has the option to directly tip for
delivery from a specific store. An equilibrium consists of prices,
compensations, tips, and transactions that clear the market, such that buyers
receive deliveries from preferred stores considering the prices and tips they
pay, and couriers deliver preferred orders considering the compensations and
tips they receive.
  We illustrate the role of tips in pricing: Without tips, an equilibrium is
only guaranteed to exist when there are at least as many couriers as buyers or
stores. In contrast, with tips an equilibrium always exists. From an efficiency
perspective, the optimal with-tip equilibrium welfare is always weakly larger
than the optimal without-tip equilibrium welfare. However, we show that even
with tips, efficient equilibria may not exist, and calculating the optimal
equilibrium welfare is NP-hard. To address these challenges, we identify
natural conditions on market structure that ensure the existence of efficient
with-tip equilibria and allow these efficient equilibria to be computed in
polynomial time.

</details>


### [198] [Fair Contracts](https://arxiv.org/abs/2507.11214)
*Matteo Castiglioni,Junjie Chen,Yingkai Li*

Main category: cs.GT

TL;DR: 研究在公平约束下设计最优合同的问题，探讨了EF、ε-EF和EF1等公平概念，证明了EF合同的存在性，但计算其近似解是NP难的。针对常数代理或任务的情况，提出了FPTAS和多项式时间算法，并分析了公平性的代价。


<details>
  <summary>Details</summary>
Motivation: 研究如何在任务分配和补偿中引入公平约束，确保合同设计既高效又公平。

Method: 采用EF及其松弛概念（ε-EF和EF1），分析其计算复杂性，并提出针对常数代理或任务的高效算法。

Result: 证明了EF合同的存在性，但近似解计算是NP难的；针对特定情况提出了高效算法，并分析了公平性的代价。

Conclusion: 公平性在合同设计中具有挑战性，但在特定约束下可实现高效解决方案，EF1合同的公平性代价是可接受的。

Abstract: We introduce and study the problem of designing optimal contracts under
fairness constraints on the task assignments and compensations. We adopt the
notion of envy-free (EF) and its relaxations, $\epsilon$-EF and envy-free up to
one item (EF1), in contract design settings. Unlike fair allocations, EF
contracts are guaranteed to exist. However, computing any constant-factor
approximation to the optimal EF contract is NP-hard in general, even using
$\epsilon$-EF contracts. For this reason, we consider settings in which the
number of agents or tasks is constant. Notably, while even with three agents,
finding an EF contract better than $2/5$ approximation of the optimal is
NP-hard, we are able to design an FPTAS when the number of agents is constant,
under relaxed notions of $\epsilon$-EF and EF1. Moreover, we present a
polynomial-time algorithm for computing the optimal EF contract when the number
of tasks is constant. Finally, we analyze the price of fairness in contract
design. We show that the price of fairness for exact EF contracts can be
unbounded, even with a single task and two agents. In contrast, for EF1
contracts, the price of fairness is bounded between $\Omega(\sqrt{n})$ and
$O(n^2)$, where $n$ is the number of agents.

</details>


### [199] [A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent](https://arxiv.org/abs/2507.11366)
*Taemin Kim,James P. Bailey*

Main category: cs.GT

TL;DR: 提出了一种基于哈密顿动力学的新方法，用于在线优化中的零和博弈，能够在有限迭代次数内收敛到纳什均衡，且支持并行化和任意学习率。


<details>
  <summary>Details</summary>
Motivation: 研究零和博弈的在线优化方法，传统方法存在收敛速度或并行化限制，需要更高效的解决方案。

Method: 基于哈密顿动力学的新方法，结合交替梯度下降，在无界设置下实现有限迭代收敛。

Result: 实验表明，新方法在性能和效率上显著优于传统方法。

Conclusion: 该方法在理论和实验上均表现出色，为在线优化和博弈论提供了新工具。

Abstract: We study online optimization methods for zero-sum games, a fundamental
problem in adversarial learning in machine learning, economics, and many other
domains. Traditional methods approximate Nash equilibria (NE) using either
regret-based methods (time-average convergence) or contraction-map-based
methods (last-iterate convergence). We propose a new method based on
Hamiltonian dynamics in physics and prove that it can characterize the set of
NE in a finite (linear) number of iterations of alternating gradient descent in
the unbounded setting, modulo degeneracy, a first in online optimization.
Unlike standard methods for computing NE, our proposed approach can be
parallelized and works with arbitrary learning rates, both firsts in
algorithmic game theory. Experimentally, we support our results by showing our
approach drastically outperforms standard methods.

</details>


### [200] [Better Regret Rates in Bilateral Trade via Sublinear Budget Violation](https://arxiv.org/abs/2507.11419)
*Anna Lunghi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: 本文研究了双边贸易中预算平衡约束与遗憾率之间的权衡，提出了一种算法，通过允许预算平衡的违反程度，实现了最优遗憾率，并给出了匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 双边贸易是算法经济学中的核心问题，现有研究表明在每一步都强制预算平衡时无法实现无遗憾学习。本文旨在探索如何在放松预算平衡约束的情况下优化遗憾率。

Method: 设计了一种算法，允许预算平衡约束在一定范围内违反（$T^{\beta}$），并实现了$\tilde O(T^{1 - \beta/3})$的遗憾率。

Result: 通过理论分析，证明了算法的遗憾率与预算平衡违反程度之间的权衡关系，并给出了匹配的下界。

Conclusion: 本文完全刻画了遗憾率与预算平衡违反程度之间的权衡关系，验证了现有研究中的上界和下界的紧性。

Abstract: Bilateral trade is a central problem in algorithmic economics, and recent
work has explored how to design trading mechanisms using no-regret learning
algorithms. However, no-regret learning is impossible when budget balance has
to be enforced at each time step. Bernasconi et al. [Ber+24] show how this
impossibility can be circumvented by relaxing the budget balance constraint to
hold only globally over all time steps. In particular, they design an algorithm
achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound
of $\Omega(T^{5/7})$.
  In this work, we interpolate between these two extremes by studying how the
optimal regret rate varies with the allowed violation of the global budget
balance constraint. Specifically, we design an algorithm that, by violating the
constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4},
\frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this
result with a matching lower bound, thus fully characterizing the trade-off
between regret and budget violation. Our results show that both the $\tilde
O(T^{3/4})$ upper bound in the global budget balance case and the
$\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation
obtained by Bernasconi et al. [Ber+24] are tight.

</details>


### [201] [On the Complexity of the Optimal Correlated Equilibria in Extensive-Form Games](https://arxiv.org/abs/2507.11509)
*Vincent Cheval,Florian Horn,Soumyajit Paul,Mahsa Shirmohammadi*

Main category: cs.GT

TL;DR: 本文研究了在多人完美记忆扩展形式游戏中，计算相关均衡（NFCE）的阈值问题的复杂性，证明其为PSPACE-hard，并揭示了与纳什均衡的复杂性反转现象。同时，解决了AFCE的阈值问题的NP-hard性质，并对多种相关均衡概念提供了完整的复杂性分类。


<details>
  <summary>Details</summary>
Motivation: 解决算法博弈论中关于在扩展形式游戏中高效计算相关均衡（NFCE）的开放性问题，并探讨其复杂性。

Method: 通过证明NFCE的阈值问题在多人完美记忆扩展形式游戏中的PSPACE-hard性质，以及纳什均衡的ER-complete性质，揭示复杂性反转现象。同时，分析AFCE的NP-hard性质，并对多种均衡概念进行分类。

Result: NFCE的阈值问题为PSPACE-hard，纳什均衡为ER-complete；AFCE的阈值问题为NP-hard。对多种均衡概念提供了NP-complete的复杂性分类。

Conclusion: 本文为扩展形式游戏中最优均衡计算的复杂性提供了最完整的分类，揭示了相关均衡与纳什均衡的复杂性反转现象。

Abstract: A major open question in algorithmic game theory is whether normal-form
correlated equilibria (NFCE) can be computed efficiently in succinct games such
as extensive-form games [DFF+25,6PR24,FP23,HvS08,VSF08,PR08]. Motivated by this
question, we study the associated Threshold problem: deciding whether there
exists a correlated equilibrium whose value exceeds a given threshold. We prove
that this problem is PSPACE-hard for NFCE in multiplayer extensive-form games
with perfect recall, even for fixed thresholds. To contextualize this result,
we also establish the complexity of the Threshold problem for Nash equilibria
in this setting, showing it is ER-complete. These results uncover a surprising
complexity reversal: while optimal correlated equilibria are computationally
simpler than optimal Nash in normal-form games, the opposite holds in
extensive-form games, where computing optimal correlated equilibria is provably
harder. Building on this line of inquiry, we also address a related question by
[VSF08], who introduced the notions of extensive-form correlated equilibrium
(EFCE) and agent-form correlated equilibrium (AFCE). They asked how difficult
the Threshold problem is for AFCE; we answer this question by proving that it
is NP-hard, even in two-player games without chance nodes. Complementing our
hardness results, we establish tight complexity classifications for the
Threshold problem across several correlated equilibrium concepts - including
EFCE, AFCE, normal-form coarse, extensive-form coarse, and agent-form coarse
correlated equilibria. For each of these solution concepts in multiplayer
stochastic extensive-form games with perfect recall, we prove NP-completeness
by providing matching NP upper bounds to the previously known hardness results.
Together, our results provide the most complete landscape to date for the
complexity of optimal equilibrium computation in extensive-form games.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [202] [Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach](https://arxiv.org/abs/2507.10634)
*Thomas Feys,Liesbet Van der Perre,François Rottenberg*

Main category: eess.SY

TL;DR: 论文研究了大规模MIMO系统中粗量化下行链路的非线性预编码问题，提出了一种基于图神经网络（GNN）的自监督学习方法，显著提高了可实现的总速率，并降低了DAC的功耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模MIMO系统的发展，DAC在硬件复杂性和功耗方面成为瓶颈，尤其是在高频和宽带场景下。

Method: 提出了一种基于GNN的非线性预编码方法，通过自监督学习直接最大化可实现速率，并使用Gumbel-softmax梯度估计解决目标函数的不可微问题。

Result: 在粗量化条件下，该方法显著提高了总速率。例如，单用户情况下，使用1位DAC即可达到与3位DAC的MRT相同的速率，功耗降低了4-7倍（基带DAC）和3倍（射频DAC）。

Conclusion: 该方法在降低DAC功耗方面效果显著，但增加了数字信号处理的功耗。综合考虑后，基带DAC在3.5 MHz带宽内仍能降低总功耗，而射频DAC在高带宽下功耗降低2.9倍。

Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency
chains, higher carrier frequencies and larger bandwidths. As such,
digital-to-analog converters (DACs) are becoming a bottleneck in terms of
hardware complexity and power consumption. In this work, non-linear precoding
for coarsely quantized downlink massive MIMO is studied. Given the NP-hard
nature of this problem, a graph neural network (GNN) is proposed that directly
outputs the precoded quantized vector based on the channel matrix and the
intended transmit symbols. The model is trained in a self-supervised manner, by
directly maximizing the achievable rate. To overcome the non-differentiability
of the objective function, introduced due to the non-differentiable DAC
functions, a straight-through Gumbel-softmax estimation of the gradient is
proposed. The proposed method achieves a significant increase in achievable sum
rate under coarse quantization. For instance, in the single-user case, the
proposed method can achieve the same sum rate as maximum ratio transmission
(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the
DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs
respectively. This, however, comes at the cost of increased digital signal
processing power consumption. When accounting for this, the reduction in
overall power consumption holds for a system bandwidth up to 3.5 MHz for
baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for
higher bandwidths. Notably, indirect effects, which further reduce the power
consumption, such as a reduced fronthaul consumption and reduction in other
components, are not considered in this analysis.

</details>


### [203] [Data-Driven Safety Certificates of Infinite Networks with Unknown Models and Interconnection Topologies](https://arxiv.org/abs/2507.10979)
*Mahdieh Zaker,Amy Nejati,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 本文提出了一种基于数据驱动的组合框架方法，用于安全认证具有未知模型和拓扑结构的无限网络，显著降低了计算复杂性和样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 由于无限网络的复杂性和现有有限网络分析框架的不适用性，需要一种新方法来处理未知模型和拓扑结构的无限网络的安全认证问题。

Method: 利用子系统的联合耗散性特性和存储证书，提出组合数据驱动条件构建无限网络的障碍证书，无需传统耗散性条件。

Result: 该方法将样本复杂度从指数级降低到线性级，并在两个物理无限网络上验证了其有效性。

Conclusion: 组合数据驱动方法为无限网络的安全认证提供了一种高效且可靠的解决方案，无需精确的拓扑知识。

Abstract: Infinite networks are complex interconnected systems comprising a countably
infinite number of subsystems, where counting them precisely poses a
significant challenge due to the seemingly endless interconnected nature of the
network (e.g., counting vehicles on the road). In such scenarios, the presence
of infinitely many subsystems within the network renders the existing analysis
frameworks tailored for finite networks inapplicable to infinite ones. This
paper is concerned with offering a data-driven approach, within a compositional
framework, for the safety certification of infinite networks with both unknown
mathematical models and interconnection topologies. Given the immense
computational complexity stemming from the extensive dimension of infinite
networks, our approach capitalizes on the joint dissipativity-type properties
of subsystems, characterized by storage certificates. We introduce innovative
compositional data-driven conditions to construct a barrier certificate for the
infinite network leveraging storage certificates of its unknown subsystems
derived from data, while offering correctness guarantees across the network
safety. We demonstrate that our compositional data-driven reasoning eliminates
the requirement for checking the traditional dissipativity condition, which
typically mandates precise knowledge of the interconnection topology. In
addition, while existing data-driven literature demonstrates an exponential
trend in sample complexity with respect to network size, we showcase that our
compositional strategy notably reduces it to a linear scale in terms of the
number of subsystems. We illustrate our data-driven results on two physical
infinite networks with unknown models and interconnection topologies.

</details>


### [204] [Approximate solutions to games of ordered preference](https://arxiv.org/abs/2507.11021)
*Pau de las Heras Molins,Eric Roy-Almonacid,Dong Ho Lee,Lasse Peters,David Fridovich-Keil,Georgios Bakirtzis*

Main category: eess.SY

TL;DR: 论文提出了一种名为“lexicographic IBR over time”的方法，用于高效计算具有排序偏好的博弈问题的近似最优解。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要平衡多个排序目标（如最短时间、安全性、交通协调），但现有方法在时间范围、玩家数量或偏好级别增加时计算复杂度高。

Method: 采用字典序迭代最佳响应（IBR）在滚动时域中近似求解，利用过去信息加速收敛。

Result: 模拟交通场景表明，该方法能高效计算近似最优解，并趋向广义纳什均衡。

Conclusion: lexicographic IBR over time方法有效解决了排序偏好博弈的复杂度问题，适用于自动驾驶车辆的多目标优化。

Abstract: Autonomous vehicles must balance ranked objectives, such as minimizing travel
time, ensuring safety, and coordinating with traffic. Games of ordered
preference effectively model these interactions but become computationally
intractable as the time horizon, number of players, or number of preference
levels increase. While receding horizon frameworks mitigate long-horizon
intractability by solving sequential shorter games, often warm-started, they do
not resolve the complexity growth inherent in existing methods for solving
games of ordered preference. This paper introduces a solution strategy that
avoids excessive complexity growth by approximating solutions using
lexicographic iterated best response (IBR) in receding horizon, termed
"lexicographic IBR over time." Lexicographic IBR over time uses past
information to accelerate convergence. We demonstrate through simulated traffic
scenarios that lexicographic IBR over time efficiently computes
approximate-optimal solutions for receding horizon games of ordered preference,
converging towards generalized Nash equilibria.

</details>


### [205] [Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems](https://arxiv.org/abs/2507.11064)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: 论文提出了一种基于信道预测的参考信号分配方法（CPRS），通过联合优化信道预测和DM-RS分配，在不依赖CSI反馈的情况下提升数据吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决5G网络中由于大规模MIMO系统天线数量增加导致的CSI反馈开销问题，填补了参考信号分配在CSI受限条件下研究的空白。

Method: 提出CPRS概念，采用ViViT/CNN架构将CSI矩阵视为序列图像数据，联合优化信道预测和DM-RS分配。

Result: 在NVIDIA Sionna生成的射线追踪信道数据上验证，吞吐量比基准策略提升高达36.60%。

Conclusion: CPRS方法有效提升了动态环境中的传输效率，为5G-Advanced标准提供了潜在的技术支持。

Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as
the growing number of antennas in modern massive MIMO systems substantially
increases the channel state information (CSI) feedback demand in frequency
division duplex (FDD) systems. To address this, extensive research has focused
on CSI compression and prediction, with neural network-based approaches gaining
momentum and being considered for integration into the 3GPP 5G-Advanced
standards. While deep learning has been effectively applied to CSI-limited
beamforming and handover optimization, reference signal allocation under such
constraints remains surprisingly underexplored. To fill this gap, we introduce
the concept of channel prediction-based reference signal allocation (CPRS),
which jointly optimizes channel prediction and DM-RS allocation to improve data
throughput without requiring CSI feedback. We further propose a
standards-compliant ViViT/CNN-based architecture that implements CPRS by
treating evolving CSI matrices as sequential image-like data, enabling
efficient and adaptive transmission in dynamic environments. Simulation results
using ray-tracing channel data generated in NVIDIA Sionna validate the proposed
method, showing up to 36.60% throughput improvement over benchmark strategies.

</details>


### [206] [Optimal Honeypot Ratio and Convergent Fictitious-Play Learning in Signaling Games for CPS Defense](https://arxiv.org/abs/2507.11113)
*Yueyue Xu,Yuewei Chen,Lin Wang,Zhaoyang Cheng,Xiaoming Hu*

Main category: eess.SY

TL;DR: 本文通过建模蜜罐部署为γ固定信号博弈，提出γ-完美贝叶斯纳什均衡，并分析其三种均衡状态。通过离散时间虚拟博弈算法，证明了在最优蜜罐比例附近扰动时收敛于防御最优均衡。数值结果验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对网络物理系统（CPS）面临的攻击增长问题，研究如何通过蜜罐部署实现主动防御。

Method: 将蜜罐部署建模为γ固定信号博弈，定义γ-完美贝叶斯纳什均衡，并通过离散时间虚拟博弈算法分析动态策略交互。

Result: 获得所有γ-PBNE的解析表达式，揭示三种均衡状态，并找到最大化网络平均效用的最优蜜罐比例和信号策略。

Conclusion: 提出的方法在CPS防御中有效，数值结果验证了其适用性和收敛性。

Abstract: Cyber-Physical Systems (CPSs) are facing a fast-growing wave of attacks. To
achieve effective proactive defense, this paper models honeypot deployment as a
gamma-fixed signaling game in which node liveness serves as the only signal and
normal-node signal gamma is exogenously fixed. We define the gamma-perfect
Bayesian-Nash equilibrium (gamma-PBNE). Analytical expressions are obtained for
all gamma-PBNEs, revealing three distinct equilibrium regimes that depend on
the priori honeypot ratio. Furthermore, the optimal honeypot ratio and
signaling strategy that jointly maximize the network average utility are
obtained. To capture strategic interaction over time, we develop a
discrete-time fictitious-play algorithm that couples Bayesian belief updates
with empirical best responses. We prove that, as long as the honeypot ratio is
perturbed within a non-degenerate neighbourhood of the optimum, every
fictitious-play path converges to the defender-optimal gamma-PBNE. Numerical
results confirm the effectiveness of the proposed method and demonstrate its
applicability to CPS defense.

</details>


### [207] [Optimal Sensor Scheduling and Selection for Continuous-Discrete Kalman Filtering with Auxiliary Dynamics](https://arxiv.org/abs/2507.11240)
*Mohamad Al Ahdab,John Leth,Zheng-Hua Tan*

Main category: eess.SY

TL;DR: 论文研究了连续-离散卡尔曼滤波（CD-KF）在多传感器、不规则时间测量下的状态空间模型（SSM）中的应用，提出了一种优化测量速率和辅助状态动态的框架。


<details>
  <summary>Details</summary>
Motivation: 解决多传感器测量过程中因测量速率和辅助状态动态带来的成本和约束问题，优化资源使用和估计精度。

Method: 通过独立泊松过程建模测量事件，推导后验协方差矩阵的上界，并利用梯度优化联合优化测量速率和辅助状态动态。

Result: 提出的方法在状态空间滤波和动态时间高斯过程回归中实现了资源使用和估计精度的更好权衡。

Conclusion: 通过优化测量速率和辅助状态动态，论文方法显著提升了资源使用效率和估计精度。

Abstract: We study the Continuous-Discrete Kalman Filter (CD-KF) for State-Space Models
(SSMs) where continuous-time dynamics are observed via multiple sensors with
discrete, irregularly timed measurements. Our focus extends to scenarios in
which the measurement process is coupled with the states of an auxiliary SSM.
For instance, higher measurement rates may increase energy consumption or heat
generation, while a sensor's accuracy can depend on its own spatial trajectory
or that of the measured target. Each sensor thus carries distinct costs and
constraints associated with its measurement rate and additional constraints and
costs on the auxiliary state. We model measurement occurrences as independent
Poisson processes with sensor-specific rates and derive an upper bound on the
mean posterior covariance matrix of the CD-KF along the mean auxiliary state.
The bound is continuously differentiable with respect to the measurement rates,
which enables efficient gradient-based optimization. Exploiting this bound, we
propose a finite-horizon optimal control framework to optimize measurement
rates and auxiliary-state dynamics jointly. We further introduce a
deterministic method for scheduling measurement times from the optimized rates.
Empirical results in state-space filtering and dynamic temporal Gaussian
process regression demonstrate that our approach achieves improved trade-offs
between resource usage and estimation accuracy.

</details>


### [208] [Moving Beyond Marginal Carbon Intensity: A Poor Metric for Both Carbon Accounting and Grid Flexibility](https://arxiv.org/abs/2507.11377)
*Philipp Wiesner,Odej Kao*

Main category: eess.SY

TL;DR: 本文认为边际碳强度（MCI）既不可靠也不实用，呼吁研究更可行的指标。


<details>
  <summary>Details</summary>
Motivation: 探讨MCI在碳感知计算中的局限性，提出其不可靠性和不可操作性。

Method: 分析MCI的基本限制，包括不可观测性、依赖不透明预测模型和缺乏可验证性。

Result: MCI无法反映高碳源导致的削减，也无法提供过剩电力的数量信息。

Conclusion: 建议研究更实用的指标，如直接报告过剩电力、明确建模能源存储和电网稳定性，并与新兴的可再生能源证书市场结合。

Abstract: Marginal Carbon Intensity (MCI) has been promoted as an effective metric for
carbon-aware computing. Although it is already considered as impractical for
carbon accounting purposes, many still view it as valuable when optimizing for
grid flexibility by incentivizing electricity usage during curtailment periods.
In this statement paper, we argue that MCI is neither reliable nor actionable
for either purpose. We outline its fundamental limitations, including
non-observability, reliance on opaque predictive models, and the lack of
verifiability. Moreover, MCI fails to reflect curtailment caused by high-carbon
sources and offers no insight into the quantity of available excess power. We
advocate moving beyond MCI and instead call for research on more actionable
metrics, such as direct reporting of excess power, explicit modeling of energy
storage and grid stability, and integration with emerging granular renewable
energy certificate markets.

</details>


### [209] [Inverse Optimal Control with Constraint Relaxation](https://arxiv.org/abs/2507.11392)
*Rahel Rickenbach,Amon Lahr,Melanie N. Zeilinger*

Main category: eess.SY

TL;DR: 论文提出了一种基于精确惩罚函数的逆最优控制方法，用于处理带噪声演示中的不等式约束问题，优于传统松弛方法。


<details>
  <summary>Details</summary>
Motivation: 在带噪声的演示中，传统逆最优控制方法依赖约束的正确激活和满足，限制了其适用性。

Method: 利用精确惩罚函数处理不等式约束，减少未知变量数量，并通过近似增强对错误约束激活的鲁棒性。

Result: 在三个仿真系统中验证，新方法在噪声环境下表现优于传统松弛方法。

Conclusion: 精确惩罚函数方法提高了逆最优控制在噪声演示中的估计准确性。

Abstract: Inverse optimal control (IOC) is a promising paradigm for learning and
mimicking optimal control strategies from capable demonstrators, or gaining a
deeper understanding of their intentions, by estimating an unknown objective
function from one or more corresponding optimal control sequences. When
computing estimates from demonstrations in environments with safety-preserving
inequality constraints, acknowledging their presence in the chosen IOC method
is crucial given their strong influence on the final control strategy. However,
solution strategies capable of considering inequality constraints, such as the
inverse Karush-Kuhn-Tucker approach, rely on their correct activation and
fulfillment; a restrictive assumption when dealing with noisy demonstrations.
To overcome this problem, we leverage the concept of exact penalty functions
for IOC and show preservation of estimation accuracy. Considering noisy
demonstrations, we then illustrate how the usage of penalty functions reduces
the number of unknown variables and how their approximations enhance the
estimation method's capacity to account for wrong constraint activations within
a polytopic-constrained environment. The proposed method is evaluated for three
systems in simulation, outperforming traditional relaxation approaches for
noisy demonstrations.

</details>


### [210] [A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification](https://arxiv.org/abs/2507.11420)
*Mingcong Li*

Main category: eess.SY

TL;DR: 提出了一种风险感知自适应鲁棒MPC框架（RAAR-MPC），通过分层架构结合主动学习和自适应安全边际，解决非平稳不确定性下的机会约束最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒MPC过于保守，而随机MPC在不确定性分布未知时表现不佳，需要一种新方法以平衡保守性和风险控制。

Method: 采用高斯过程回归和主动学习的风险评估引擎，结合自适应安全边际的双时间尺度调节机制。

Result: 在非平稳参数不确定性下，RAAR-MPC能精确满足用户定义的风险水平，且平均成本显著低于现有方法。

Conclusion: RAAR-MPC通过自适应组件保证递归可行性，并在高概率下满足机会约束，优于传统方法。

Abstract: Solving chance-constrained optimal control problems for systems subject to
non-stationary uncertainties is a significant challenge.Conventional robust
model predictive control (MPC) often yields excessive conservatism by relying
on static worst-case assumptions, while standard stochastic MPC methods
struggle when underlying uncertainty distributions are unknown a priori.This
article presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a
hierarchical architecture that systematically orchestrates a novel synthesis of
proactive, learning-based risk assessment and reactive risk regulation. The
framework employs a medium-frequency risk assessment engine, which leverages
Gaussian process regression and active learning, to construct a tight,
data-driven characterization of the prediction error set from operational
data.Concurrently, a low-timescale outer loop implements a self-correcting
update law for an adaptive safety margin to precisely regulate the empirical
risk and compensate for unmodeled dynamics.This dual-timescale adaptation
enables the system to rigorously satisfy chance constraints with a user-defined
probability, while minimizing the conservatism inherent in traditional
approaches.We formally establish that the interplay between these adaptive
components guarantees recursive feasibility and ensures the closed-loop system
satisfies the chance constraints up to a user-defined risk level with high
probability.Numerical experiments on a benchmark DC-DC converter under
non-stationary parametric uncertainties demonstrate that our framework
precisely achieves the target risk level, resulting in a significantly lower
average cost compared to state-of-the-art robust and stochastic MPC strategies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [211] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Main category: cs.NE

TL;DR: Tangma是一种新的激活函数，结合了双曲正切的平滑形状和两个可学习参数，在MNIST和CIFAR-10上表现优于ReLU、Swish和GELU。


<details>
  <summary>Details</summary>
Motivation: 改进激活函数以增强深度神经网络的表达能力和训练稳定性。

Method: 提出Tangma激活函数，引入可学习参数α和γ调整激活行为，并在MNIST和CIFAR-10上测试其性能。

Result: Tangma在MNIST上达到99.09%的验证准确率，在CIFAR-10上达到78.15%，且训练效率更高。

Conclusion: Tangma在标准视觉任务中表现优异，其可学习设计为更大模型提供了潜在优势。

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [212] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 论文探讨了使用Spiker+框架为MNIST数据集生成优化的SNN加速器，以支持低功耗FPGA部署。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器对边缘应用的低延迟、高能效推理至关重要，而SNN因其事件驱动和稀疏特性适合低功耗FPGA部署。

Method: 利用开源的Spiker+框架，生成优化的SNN加速器，支持高级网络拓扑、神经元模型和量化规范，自动生成可部署的HDL。

Result: 评估了多种配置，分析了边缘计算约束下的权衡。

Conclusion: Spiker+框架为SNN在FPGA上的高效部署提供了可行方案。

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


### [213] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Main category: cs.NE

TL;DR: 提出一种受珊瑚礁启发的群体交互网络，用于碳中性废水处理，具有高效率和低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着废水处理需求增加，实现能源中性处理具有挑战性，需要创新解决方案。

Method: 结合形态发生抽象和多任务碳意识，利用线性令牌复杂性实现可扩展性。

Result: 在七种基线方法中表现最佳，去除效率达96.7%，能耗0.31 kWh/m³，CO₂排放14.2 g/m³，且在传感器漂移下表现稳健。

Conclusion: 方法在多种场景中潜力显著，但数据科学人员配备是瓶颈，未来将集成AutoML，但需解决可解释性问题。

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [214] [An Exact Gradient Framework for Training Spiking Neural Networks](https://arxiv.org/abs/2507.10568)
*Arman Ferdowsi,Atakan Aral*

Main category: cs.NE

TL;DR: 提出了一种基于事件驱动的学习框架，用于精确计算损失梯度，提升SNN的准确性、时序精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖离散时间模拟或内部状态变量，限制了训练精度和效率，且对硬件实现不友好。

Method: 开发了一种分析性事件驱动学习框架，可精确计算权重、传输延迟和自适应神经元阈值的梯度。

Result: 在多个基准测试中，准确性提升高达7%，时序精度和鲁棒性显著优于现有方法。

Conclusion: 该框架为SNN的训练提供了更高效、精确的解决方案，适用于神经形态硬件实现。

Abstract: Spiking neural networks inherently rely on the precise timing of discrete
spike events for information processing. Incorporating additional bio-inspired
degrees of freedom, such as trainable synaptic transmission delays and adaptive
firing thresholds, is essential for fully leveraging the temporal dynamics of
SNNs. Although recent methods have demonstrated the benefits of training
synaptic weights and delays, both in terms of accuracy and temporal
representation, these techniques typically rely on discrete-time simulations,
surrogate gradient approximations, or full access to internal state variables
such as membrane potentials. Such requirements limit training precision and
efficiency and pose challenges for neuromorphic hardware implementation due to
increased memory and I/O bandwidth demands. To overcome these challenges, we
propose an analytical event-driven learning framework that computes exact loss
gradients not only with respect to synaptic weights and transmission delays but
also to adaptive neuronal firing thresholds. Experiments on multiple benchmarks
demonstrate significant gains in accuracy (up to 7%), timing precision, and
robustness compared to existing methods.

</details>


### [215] [Grammatical Structure and Grammatical Variations in Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10708)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.NE

TL;DR: 该研究介绍了非节拍伊朗古典音乐的符号数据集及结构解析与变体生成算法，验证了其方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解析非节拍伊朗古典音乐的结构，并生成可接受的变体，为教育和民族音乐学提供工具。

Method: 使用MIDI文件和数据表构建数据集，应用解析算法识别乐句和动机，通过语法变异生成新变体。

Result: 系统成功生成可接受的音乐变体，统计分析了不同表示设置对变异的影响。

Conclusion: 方法适用于伊朗古典音乐，并可扩展至阿拉伯或土耳其古典音乐。

Abstract: In this study we introduce a symbolic dataset composed of non-metric Iranian
classical music, and algorithms for structural parsing of this music, and
generation of variations. The corpus comprises MIDI files and data sheets of
Dastgah Shour from Radif Mirza Abdollah, the foundational repertoire of Iranian
classical music. Furthermore, we apply our previously-introduced algorithm for
parsing melodic structure (Kanani et al., 2023b)to the dataset. Unlike much
Western music, this type of non-metric music does not follow bar-centric
organisation. The non-metric organisation can be captured well by our parsing
algorithm. We parse each tune (Gusheh) into a grammar to identify motifs and
phrases. These grammar representations can be useful for educational and
ethnomusicological purposes. We also further develop a previously-introduced
method of creating melodic variations (Kanani et al., 2023b). After parsing an
existing tune to produce a grammar, by applying mutations to this grammar, we
generate a new grammar. Expanding this new version yields a variation of the
original tune. Variations are assessed by a domain-expert listener.
Additionally, we conduct a statistical analysis of mutation with different
representation setups for our parsing and generation algorithms. The
overarching conclusion is that the system successfully produces acceptable
variations post-mutation. While our case study focuses on Iranian classical
music, the methodology can be adapted for Arabic or Turkish classical music.

</details>


### [216] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Main category: cs.NE

TL;DR: 果蝇幼虫大脑的全连接组被转化为生物处理单元（BPU），在MNIST和CIFAR-10等任务上表现优异，甚至超越传统MLP和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 探索生物进化电路是否支持人工智能，并验证生物启发的神经网络架构在复杂认知任务中的潜力。

Method: 将果蝇幼虫大脑的连接组转化为固定循环网络（BPU），并通过结构扩展和模态特异性消融实验优化性能。

Result: BPU在MNIST上达到98%准确率，CIFAR-10上58%；轻量级GNN-BPU在ChessBench上表现优异，CNN-BPU超越参数匹配的Transformer。

Conclusion: 生物启发的神经网络架构在复杂任务中表现优异，未来可扩展至更大规模的连接组。

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [217] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 论文提出了一种名为CLS-DM的模型，通过跨模态特征对比学习，从2D X射线图像中提取3D信息并实现模态间的潜在空间对齐，显著提升了稀疏视图CT重建的效果。


<details>
  <summary>Details</summary>
Motivation: 传统CT重建方法存在时间消耗大和辐射暴露高的问题，稀疏视图X射线图像重建成为研究热点，但现有方法在2D与3D模态间的潜在空间对齐上表现不佳。

Method: 提出CLS-DM模型，利用跨模态特征对比学习，从2D X射线图像中提取3D信息并实现潜在空间对齐。

Result: 实验表明，CLS-DM在LIDC-IDRI和CTSpine1K数据集上，PSNR和SSIM指标优于经典和最新生成模型。

Conclusion: CLS-DM不仅提升了稀疏视图CT重建的效果，还可推广至其他跨模态转换任务，如文本到图像合成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [218] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 论文提出了一种名为3D MIR的新方法，结合深度学习与物理约束优化，用于半导体封装中的3D电流参数恢复。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，精确恢复3D信息对无损检测和电路缺陷定位至关重要。

Method: 方法分为三阶段：1) CNN处理磁图像预测参数；2) 物理约束提供初始估计；3) 优化器调整参数以最小化误差。

Result: 3D MIR方法能高精度恢复3D信息，为磁图像重建设定了新标准。

Conclusion: 结合深度学习与物理驱动优化在实际应用中具有潜力。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [219] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net是一种新型肝脏和肿瘤分割框架，结合多种技术提升分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 腹部CT图像中肝脏和肿瘤的准确分割对诊断和治疗至关重要，但复杂解剖结构、肿瘤外观多变和标注数据有限带来挑战。

Method: HANS-Net结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性量化和时序注意力。

Result: 在LiTS数据集上，HANS-Net平均Dice分数为93.26%，IoU为88.09%；在3D-IRCADb-01数据集上Dice为87.45%，IoU为80.30%。

Conclusion: HANS-Net在肝脏和肿瘤分割中表现出高效性、鲁棒性和泛化能力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [220] [Supporting SENĆOTEN Language Documentation Efforts with Automatic Speech Recognition](https://arxiv.org/abs/2507.10827)
*Mengzhe Geng,Patrick Littell,Aidan Pine,PENÁĆ,Marc Tessier,Roland Kuhn*

Main category: cs.SD

TL;DR: 论文提出了一种结合TTS生成数据和跨语言迁移学习的ASR驱动文档化流程，用于支持SENĆOŦEN语言的复兴，实验显示WER和CER显著降低。


<details>
  <summary>Details</summary>
Motivation: 支持SENĆOŦEN语言的复兴，解决因数据有限和词汇变化大导致的ASR开发挑战。

Method: 利用TTS生成增强数据，结合跨语言迁移学习和n-gram语言模型，优化ASR系统。

Result: 测试集上WER为19.34%，CER为5.09%；过滤小错误后WER降至14.32%，CER降至3.45%。

Conclusion: ASR驱动流程在支持SENĆOŦEN语言文档化方面具有潜力。

Abstract: The SEN\'{C}OTEN language, spoken on the Saanich peninsula of southern
Vancouver Island, is in the midst of vigorous language revitalization efforts
to turn the tide of language loss as a result of colonial language policies. To
support these on-the-ground efforts, the community is turning to digital
technology. Automatic Speech Recognition (ASR) technology holds great promise
for accelerating language documentation and the creation of educational
resources. However, developing ASR systems for SEN\'{C}OTEN is challenging due
to limited data and significant vocabulary variation from its polysynthetic
structure and stress-driven metathesis. To address these challenges, we propose
an ASR-driven documentation pipeline that leverages augmented speech data from
a text-to-speech (TTS) system and cross-lingual transfer learning with Speech
Foundation Models (SFMs). An n-gram language model is also incorporated via
shallow fusion or n-best restoring to maximize the use of available data.
Experiments on the SEN\'{C}OTEN dataset show a word error rate (WER) of 19.34%
and a character error rate (CER) of 5.09% on the test set with a 57.02%
out-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER
improves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the
potential of our ASR-driven pipeline to support SEN\'{C}OTEN language
documentation.

</details>


### [221] [Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison](https://arxiv.org/abs/2507.10985)
*Andrew Valdivia,Yueming Zhang,Hailu Xu,Amir Ghasemkhani,Xin Qin*

Main category: cs.SD

TL;DR: 提出了一种通过比较用户原始语音与发音纠正后的语音克隆版本来检测发音错误的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统发音检测方法依赖预定义的语音规则或大量训练数据，而本研究旨在提供一种更灵活、无需语言特定资源的方法。

Method: 利用语音克隆技术生成发音正确的用户语音克隆版本，并通过帧级比较原始与克隆语音的声学差异来定位发音错误。

Result: 实验证明该方法能有效识别特定发音错误，无需依赖语言特定规则或大量训练数据。

Conclusion: 该方法为发音检测提供了一种高效且通用的解决方案，适用于多种语言。

Abstract: This paper presents a novel approach for detecting mispronunciations by
analyzing deviations between a user's original speech and their voice-cloned
counterpart with corrected pronunciation. We hypothesize that regions with
maximal acoustic deviation between the original and cloned utterances indicate
potential mispronunciations. Our method leverages recent advances in voice
cloning to generate a synthetic version of the user's voice with proper
pronunciation, then performs frame-by-frame comparisons to identify problematic
segments. Experimental results demonstrate the effectiveness of this approach
in pinpointing specific pronunciation errors without requiring predefined
phonetic rules or extensive training data for each target language.

</details>


### [222] [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](https://arxiv.org/abs/2507.11096)
*Vassilis Sioros,Alexandros Potamianos,Giorgos Paraskevopoulos*

Main category: cs.SD

TL;DR: 研究提出了一种基于交叉注意力控制的自回归模型音频编辑方法，结合扩散策略和预训练模型MUSICGEN，通过三种编辑机制提升音频质量。


<details>
  <summary>Details</summary>
Motivation: 受图像编辑方法启发，探索如何通过交叉和自注意力机制实现高效的音频编辑。

Method: 开发了类似Prompt-to-Prompt的方法，结合扩散策略和MUSICGEN模型，提出替换、重加权和细化三种注意力编辑机制。

Result: 自动和人工评估显示，该方法在旋律、动态和节奏方面显著优于基于扩散的基线。

Conclusion: 结合Prompt-to-Prompt引导与自回归生成模型的方法为音频编辑提供了高效且高质量的解决方案。

Abstract: In this study, we investigate leveraging cross-attention control for
efficient audio editing within auto-regressive models. Inspired by image
editing methodologies, we develop a Prompt-to-Prompt-like approach that guides
edits through cross and self-attention mechanisms. Integrating a
diffusion-based strategy, influenced by Auffusion, we extend the model's
functionality to support refinement edits, establishing a baseline for
prompt-guided audio editing. Additionally, we introduce an alternative approach
by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and
propose three editing mechanisms, based on Replacement, Reweighting, and
Refinement of the attention scores. We employ commonly-used music-specific
evaluation metrics and a human study, to gauge time-varying controllability,
adherence to global text cues, and overall audio realism. The automatic and
human evaluations indicate that the proposed combination of prompt-to-prompt
guidance with autoregressive generation models significantly outperforms the
diffusion-based baseline in terms of melody, dynamics, and tempo of the
generated audio. Our code is available at https://github.com/billsioros/EditGen

</details>


### [223] [Improving Neural Pitch Estimation with SWIPE Kernels](https://arxiv.org/abs/2507.11233)
*David Marttila,Joshua D. Reiss*

Main category: cs.SD

TL;DR: 论文研究了使用SWIPE核作为音频前端，提升神经网络的音高估计性能，使其更准确、抗噪且参数高效。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在音高和周期性估计中占主导地位，但现有方法多基于原始音频或通用时频表示，缺乏任务特定特征。

Method: 采用SWIPE核作为音频前端，结合监督和自监督的先进架构进行实验。

Result: SWIPE前端可大幅减少网络规模而不损失性能，且SWIPE算法本身优于自监督神经音高估计器。

Conclusion: 任务特定特征（如SWIPE）能显著提升神经音高估计器的效率和性能。

Abstract: Neural networks have become the dominant technique for accurate pitch and
periodicity estimation. Although a lot of research has gone into improving
network architectures and training paradigms, most approaches operate directly
on the raw audio waveform or on general-purpose time-frequency representations.
We investigate the use of Sawtooth-Inspired Pitch Estimation (SWIPE) kernels as
an audio frontend and find that these hand-crafted, task-specific features can
make neural pitch estimators more accurate, robust to noise, and more
parameter-efficient. We evaluate supervised and self-supervised
state-of-the-art architectures on common datasets and show that the SWIPE audio
frontend allows for reducing the network size by an order of magnitude without
performance degradation. Additionally, we show that the SWIPE algorithm on its
own is much more accurate than commonly reported, outperforming
state-of-the-art self-supervised neural pitch estimators.

</details>


### [224] [FasTUSS: Faster Task-Aware Unified Source Separation](https://arxiv.org/abs/2507.11435)
*Francesco Paissan,Gordon Wichern,Yoshiki Masuyama,Ryo Aihara,François G. Germain,Kohei Saijo,Jonathan Le Roux*

Main category: cs.SD

TL;DR: 论文分析了TUSS模型的设计选择，提出了两种更高效的模型FasTUSS-8.3G和FasTUSS-11.7G，显著减少了计算量，同时性能下降较小。


<details>
  <summary>Details</summary>
Motivation: 解决TF双路径模型计算量大、执行时间长的问题，优化性能与复杂度的权衡。

Method: 通过分析TUSS模型的设计选择，提出两种更高效的模型，并研究提示条件对因果TUSS模型的影响。

Result: FasTUSS-8.3G和FasTUSS-11.7G分别减少了81%和73%的计算量，性能仅下降1.2dB和0.4dB。

Conclusion: 优化后的模型在保持性能的同时显著降低了计算复杂度，为音频源分离任务提供了更高效的解决方案。

Abstract: Time-Frequency (TF) dual-path models are currently among the best performing
audio source separation network architectures, achieving state-of-the-art
performance in speech enhancement, music source separation, and cinematic audio
source separation. While they are characterized by a relatively low parameter
count, they still require a considerable number of operations, implying a
higher execution time. This problem is exacerbated by the trend towards bigger
models trained on large amounts of data to solve more general tasks, such as
the recently introduced task-aware unified source separation (TUSS) model.
TUSS, which aims to solve audio source separation tasks using a single,
conditional model, is built upon TF-Locoformer, a TF dual-path model combining
convolution and attention layers. The task definition comes in the form of a
sequence of prompts that specify the number and type of sources to be
extracted. In this paper, we analyze the design choices of TUSS with the goal
of optimizing its performance-complexity trade-off. We derive two more
efficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original
model's operations by 81\% and 73\% with minor performance drops of 1.2~dB and
0.4~dB averaged over all benchmarks, respectively. Additionally, we investigate
the impact of prompt conditioning to derive a causal TUSS model.

</details>
