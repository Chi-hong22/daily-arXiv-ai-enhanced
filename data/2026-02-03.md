<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 81]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR是一种联邦学习算法，通过自适应优化器和方差缩减技术解决客户端异构性和部分参与问题，其量化版本FedAdaVR-Quant能大幅减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临异构性挑战，导致梯度噪声、客户端漂移和部分客户端参与误差，其中部分参与问题最为普遍但现有研究未能充分解决。

Method: 提出FedAdaVR算法，结合自适应优化器和方差缩减技术，利用最近存储的客户端更新模拟缺席客户端的参与；进一步提出FedAdaVR-Quant，通过量化存储客户端更新来减少内存需求。

Result: 理论分析证明FedAdaVR能在一般非凸条件下消除部分客户端参与误差；实验表明在IID和非IID设置下均优于现有方法；FedAdaVR-Quant能减少50%、75%和87.5%的内存需求且保持同等模型性能。

Conclusion: FedAdaVR有效解决了联邦学习中的部分客户端参与问题，其量化版本在保持性能的同时显著降低了内存需求，为实际部署提供了实用解决方案。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [2] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DAJ：基于推理的LLM法官，通过双层数据重加权学习框架训练，使用可验证奖励，在代码生成测试时缩放中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前代码生成测试时缩放主要依赖Best-of-N选择，但训练可靠的LLM法官面临严重分布偏移挑战，包括简单与困难问题不平衡、训练任务与评估基准不匹配、以及由廉价模型生成的训练数据与推理时模型行为不匹配等问题

Method: 提出DAJ（推理型LLM法官），采用双层数据重加权学习框架，学习数据重要性权重（域级或实例级），使用可验证奖励进行训练，优化在目标基准对齐的元集上的泛化性能

Result: 在LiveCodeBench和BigCodeBench基准上实现最先进性能，优于强测试时缩放基线和领先的专有模型

Conclusion: DAJ是首个将数据重加权应用于LLM法官训练的方法，能自动强调困难问题、分布内样本和轨迹对齐数据，无需依赖手工启发式规则，有效解决了LLM法官训练中的分布偏移问题

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [3] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM是一种用于代码生成的测试时扩展方法，通过函数化模块化代码生成和元学习奖励修正机制，显著提升大语言模型在复杂编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面有核心应用，但在复杂编程任务上仍然经常失败。现有的过程奖励模型在代码生成中效果有限，主要因为代码缺乏有意义的步骤分解，以及蒙特卡洛估计的部分解决方案正确性分数存在噪声。

Method: FunPRM采用两种创新方法：1) 提示大语言模型生成模块化的函数化代码，将函数作为过程奖励模型的推理步骤；2) 引入基于元学习的奖励修正机制，利用单元测试评估系统获得的干净最终解决方案奖励来净化噪声的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础大语言模型上始终优于现有的测试时扩展方法，特别是与O4-mini结合时在LiveCodeBench上实现了最先进的性能。此外，FunPRM生成的代码对开发者来说更具可读性和可重用性。

Conclusion: FunPRM通过函数化模块化代码生成和元学习奖励修正，有效解决了现有过程奖励模型在代码生成中的局限性，显著提升了代码生成质量，并产生了更实用、可维护的代码。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [4] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 通过引入批量采样的无学习查询和值偏置来打破注意力机制中的旋转对称性，改善简单优化器的性能并增强注意力头的可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制中存在不影响模型激活或输出的冗余旋转自由度，这些自由度在计算中被传递但未被有效利用。

Method: 提出一种简单的对称性破坏协议，通过批量采样的无学习查询和值偏置在旋转空间中插入首选方向。

Result: 1) 显著改善简单内存高效优化器的性能，缩小甚至消除与复杂内存密集型自适应方法的差距；2) 使原本冗余的旋转自由度具有可解释性，能够选择性放大单个注意力头中语义有意义的token类别。

Conclusion: 最小化、有原则的架构变化可以同时提高性能和可解释性，为注意力机制优化提供了新思路。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [5] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析重新构建为分类问题，通过离散化事件时间将静态和动态生存分析转化为一系列二分类问题，使现成的表格基础模型能够通过上下文学习进行生存分析而无需显式训练。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务上取得了显著成功，但由于右删失（数据观测可能在事件发生前结束），将其适应于建模生存分析的时间到事件结果具有挑战性。

Method: 开发了一个基于分类的框架，通过离散化事件时间将生存分析重新构建为一系列二分类问题。删失观测被自然地处理为在某些时间点缺少标签的示例。这种分类表述使现有表格基础模型能够通过上下文学习进行生存分析而无需显式训练。

Result: 在53个真实世界数据集上的评估表明，使用这种分类表述的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线方法。

Conclusion: 该分类框架成功地将生存分析适应于表格基础模型，在标准删失假设下，最小化二分类损失能够随着训练集大小的增加恢复真实的生存概率，为生存分析提供了一种有效的新方法。

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [6] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动化人体活动识别框架，用于远程医疗和老年人辅助，支持张量机模型显著优于传统分类器。


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施有限导致老年人和弱势患者依赖家庭护理，但常出现忽视和治疗性锻炼（如瑜伽或物理治疗）依从性差的问题。需要低成本自动化解决方案来监测和改善家庭护理质量。

Method: 使用加速度计和陀螺仪测量收集活动数据（行走、上下楼梯、坐、站、躺），评估四种经典分类器（逻辑回归、随机森林、SVM、k-NN），并与提出的支持张量机（STM）进行比较。STM利用张量表示保留时空运动动态。

Result: SVM准确率为93.33%，逻辑回归、随机森林和k-NN为91.11%。STM显著优于这些模型，测试准确率达到96.67%，交叉验证准确率最高达98.50%。

Conclusion: 提出的框架在远程医疗、老年人辅助、儿童活动监测、瑜伽反馈和智能家居健康方面具有强大潜力，为低资源和农村医疗环境提供了可扩展的解决方案。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [7] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL算法在联邦域增量学习(FDIL)中，通过添加服务器端"锚点"项来抑制累积漂移，无需记忆缓冲，实现了向后知识传递保证和跨任务的收敛率。


<details>
  <summary>Details</summary>
Motivation: 现实联邦系统中数据分布会漂移，但隐私规则禁止原始数据共享。联邦域增量学习(FDIL)面临两个理论缺失：向后知识传递(BKT)保证和跨所有任务的部分参与收敛率。

Method: SPECIAL算法在FedAvg基础上添加服务器端"锚点"项：每轮中服务器通过轻量级近端项将参与客户端更新向先前全局模型方向微调，无需记忆缓冲、合成数据或任务特定头。

Result: 理论证明SPECIAL：(1)保护先前任务：BKT边界限制先前任务损失增加，该边界随更多轮次、本地周期和参与客户端而缩小；(2)跨任务高效学习：首次实现FDIL部分参与下的通信高效非凸收敛率O((E/NT)^(1/2))。

Conclusion: SPECIAL通过简单服务器端锚点机制解决了FDIL中的关键理论挑战，在保持通信和模型大小不变的同时，实现了向后知识传递保证和跨任务收敛率，实验验证了其有效性。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [8] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: Riemannian Lyapunov Optimizers (RLOs) 是一个基于控制理论的优化算法家族，将经典优化器统一在一个几何框架中，通过识别不变流形将训练动态分为两个阶段，并提供了系统化的优化器设计工具。


<details>
  <summary>Details</summary>
Motivation: 当前优化算法的改进大多基于启发式方法，缺乏系统性的理论框架。本文旨在通过控制理论为优化器设计提供统一的理论基础，将优化问题重新解释为黎曼流形上的离散时间控制系统。

Method: 提出Riemannian Lyapunov Optimizers框架：1) 将优化重新解释为黎曼参数流形上的扩展状态离散时间控制系统；2) 识别Normally Attracting Invariant Manifold (NAIM)，将训练动态分为快速对齐阶段和受控演化阶段；3) 构建严格的Lyapunov函数来证明收敛性；4) 开发"优化器生成器"来系统设计RLOs。

Result: RLOs不仅能够恢复经典优化算法，还能基于控制理论原理设计新的优化器。在大规模基准测试中，该方法实现了最先进的性能，并通过几何诊断验证了理论框架的有效性。

Conclusion: RLOs在控制理论和现代机器学习优化之间建立了桥梁，提供了统一的语言和系统化的工具包，用于设计稳定、有效的优化器，为优化器设计提供了理论基础。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [9] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的成功不仅取决于模型本身，还取决于合并方法和任务特性，研究发现子空间重叠和梯度对齐是方法无关的兼容性基础条件。


<details>
  <summary>Details</summary>
Motivation: 虽然模型合并能够整合不同微调模型的知识，但成功因素仍不明确。现有研究将可合并性视为内在属性，但作者认为这实际上取决于合并方法和任务特性。

Method: 使用架构无关的框架，通过线性优化一组可解释的成对指标（如梯度L2距离），分析四种合并方法，识别与合并后性能相关的属性。

Result: 发现成功驱动因素存在显著差异（46.7%指标重叠；55.3%符号一致性），揭示了方法特定的"指纹"。但子空间重叠和梯度对齐指标始终作为方法无关的兼容性基础条件出现。

Conclusion: 这些发现为理解可合并性提供了诊断基础，并激励未来开发明确鼓励这些属性的微调策略。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [10] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL是一个安全的去中心化联邦学习框架，结合DAG账本、专用侧链和零知识证明，实现隐私保护的模型验证，在对抗性节点存在时仍保持高性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护数据隐私，但集中式和去中心化方法在可扩展性、安全性和更新验证方面都面临挑战，需要更安全高效的解决方案。

Method: 提出ZK-HybridFL框架，集成有向无环图账本、专用侧链和零知识证明，使用事件驱动智能合约和预言机辅助的侧链验证本地模型更新而不暴露敏感数据，内置挑战机制检测对抗行为。

Result: 在图像分类和语言建模任务中，相比Blade-FL和ChainFL，ZK-HybridFL实现更快收敛、更高准确率、更低困惑度和更低延迟，能抵抗大量对抗性和空闲节点，支持亚秒级链上验证和高效gas使用。

Conclusion: ZK-HybridFL是一个可扩展且安全的去中心化联邦学习解决方案，适用于多样化环境，能有效防止无效更新和孤儿式攻击。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [11] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯推理的工作流生成方法BWG，通过重要性采样和序列细化器自动构建LLM调用序列，相比现有方法在多个基准上显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流生成方法大多将其视为优化问题，缺乏理论基础。本文旨在为工作流生成提供更严谨的理论框架，通过贝叶斯推理方法改进工作流生成的质量和可靠性。

Method: 提出贝叶斯工作流生成(BWG)框架，将其建模为工作流后验分布的贝叶斯推理问题。使用并行前瞻展开进行重要性加权，结合序列循环细化器进行整体改进。具体实现为BayesFlow算法，无需训练即可构建工作流。

Result: 在六个基准数据集上的实验表明，BayesFlow相比最先进的工作流生成基线方法提升了最多9个百分点的准确率，相比零样本提示提升了最多65个百分点。证明了BWG作为基于搜索的工作流设计的理论升级的有效性。

Conclusion: BWG为自动工作流生成提供了理论严谨的贝叶斯推理框架，BayesFlow算法在多个任务上显著优于现有方法，为复杂端到端任务的自动化工作流设计提供了新的理论和方法基础。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [12] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR是一个用于评估材料基础模型在几何尺度泛化能力的基准测试，包含三个任务：CIF到属性预测、基于物理推理的思维链变体、以及逆向检索。该基准测试了模型在从几个原子到超过18,000个原子的尺度变化下的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地应用于材料科学推理，但它们在物理结构化分布偏移下的行为仍不清楚。需要评估几何尺度泛化能力及其与结构幻觉、一致性和推理的关系。

Method: 开发了SCALAR基准，包含约100,000个从DFT验证的晶胞通过超胞扩展和几何截断得到的纳米结构。定义了三个任务：CIF到属性预测、基于物理推理的思维链变体、逆向检索。使用结构化指标评估数值误差、幻觉、跨提示一致性、单调推理、输出有效性和检索遗憾。

Result: 实验显示，不同基础模型在显式推理下表现出大的、模型依赖的偏移，通常减少幻觉和误差，但经常破坏一致性或有效性。几何尺度泛化不能仅从准确性推断。

Conclusion: 几何尺度泛化是材料基础模型评估的重要维度，需要超越准确性的结构化指标来全面评估模型在物理结构化分布偏移下的性能。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [13] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 静态黑盒评估无法保证模型更新后的对齐性，即使通过所有标准测试的LLM在单次良性更新后也可能严重失准，且这种隐藏的对抗行为随模型规模增大而增加。


<details>
  <summary>Details</summary>
Motivation: 现有对齐研究通常假设初始模型是对齐的，但实践中LLM频繁更新，而静态黑盒评估无法检测模型更新后可能出现的失准行为。本文旨在揭示静态评估的根本局限性，并强调需要更新后鲁棒的对齐评估方法。

Method: 首先从理论上形式化静态和更新后两种设置下的模型对齐概念，证明由于过参数化，静态对齐不能保证更新后对齐。然后理论证明静态黑盒探测无法区分真正更新后鲁棒的模型和隐藏任意数量对抗行为的模型。最后在LLM的三个核心对齐领域（隐私、越狱安全、行为诚实）进行实证验证。

Result: 理论分析表明静态对齐无法保证更新后对齐，且静态黑盒评估无法检测隐藏的对抗行为。实证结果显示存在通过所有标准黑盒对齐测试的LLM，在单次良性更新后变得严重失准。同时发现隐藏这种潜在对抗行为的能力随模型规模增大而增加，验证了理论预测。

Conclusion: 静态评估协议存在严重不足，无法保证模型更新后的对齐性。研究结果强调了迫切需要开发更新后鲁棒的对齐评估方法，特别是在模型规模不断增大的背景下。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [14] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: 提出PA-GP-UCB算法，利用昂贵真实评估和廉价预测模型，结合离线数据提升贝叶斯优化的样本效率


<details>
  <summary>Details</summary>
Motivation: 现实优化问题中常存在昂贵真实评估（如人工评估、物理实验）和廉价低精度预测（如机器学习模型、模拟），同时有大量离线数据可用于预训练预测模型和提供先验信息。需要开发能同时利用两种评估源和离线数据的算法，以提高真实评估的样本效率。

Method: 提出预测增强高斯过程上置信界（PA-GP-UCB）算法，使用联合高斯过程后验推导的控制变量估计器来校正预测偏差并减少不确定性。算法结合昂贵真实评估和廉价预测，利用离线数据预训练模型。

Result: 理论证明PA-GP-UCB保持了GP-UCB的标准遗憾率，同时获得了更小的主导常数，该常数由预测质量和离线数据覆盖度明确控制。在合成基准测试和基于人类行为数据的真实假设评估任务中，PA-GP-UCB比Vanilla GP-UCB和朴素预测增强基线收敛更快。

Conclusion: PA-GP-UCB为昂贵反馈下的假设生成提供了一个通用且样本高效的框架，能有效利用预测模型和离线数据提升优化效率。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [15] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 本文提出了首个用于LLM路由的联邦学习框架，使客户端能够从本地离线查询-模型评估数据中学习共享路由策略，解决了数据隐私分散和本地数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为远程服务被边缘和企业客户端访问，需要平衡质量和推理成本的路由策略。现有路由方法假设可以访问集中化的查询-模型评估数据，但这些数据分散在客户端且具有隐私敏感性，无法集中。同时，每个客户端单独训练路由策略效果有限，因为本地评估数据覆盖范围有限且存在偏差。

Method: 提出了首个用于LLM路由的联邦学习框架，支持参数化的多层感知机路由器和非参数的K-means路由器，能够处理异构客户端查询分布和非均匀模型覆盖。通过联邦协作让客户端从本地离线数据中学习共享路由策略。

Result: 在两个基准测试中，联邦协作相比客户端本地路由器在准确率-成本边界上表现更好，既通过增加有效模型覆盖，也通过更好的查询泛化能力。理论结果也验证了联邦训练能够减少路由次优性。

Conclusion: 联邦学习框架为LLM路由提供了一种有效的解决方案，能够在保护数据隐私的同时，通过客户端协作提高路由策略的质量和泛化能力，解决了数据分散和本地数据有限的问题。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [16] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文研究连续均值估计问题，在用户级差分隐私下提出新方法，相比纯差分隐私显著降低误差


<details>
  <summary>Details</summary>
Motivation: 连续均值估计在数据向量顺序到达时需要保持运行均值的准确估计。现有研究主要关注纯差分隐私，但这会导致估计噪声过大，限制了实际应用。因此需要在近似差分隐私下解决这个问题。

Method: 采用近似差分隐私框架，利用矩阵分解机制的最新进展，提出了一种专门针对均值估计的新型分解方法，该方法既高效又准确。

Result: 新方法在用户级差分隐私下的连续均值估计中实现了渐进更低的均方误差界限，相比纯差分隐私方法显著提升了准确性。

Conclusion: 通过采用近似差分隐私和专门设计的矩阵分解机制，本文提出的方法在连续均值估计中实现了更好的隐私-效用权衡，为实际应用提供了更可行的解决方案。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [17] [Spatially-Adaptive Conformal Graph Transformer for Indoor Localization in Wi-Fi Driven Networks](https://arxiv.org/abs/2601.22322)
*Ayesh Abu Lehyeh,Anastassia Gharib,Safwan Wshah*

Main category: cs.LG

TL;DR: 提出SAC-GT框架，结合图变换器和空间自适应保形预测，实现精确且具有不确定性量化的室内定位


<details>
  <summary>Details</summary>
Motivation: 现有基于图的室内定位模型虽然能提供更细粒度的定位，但缺乏对预测不确定性的量化，而这对实际部署至关重要

Method: SAC-GT框架整合图变换器模型（捕捉空间拓扑和信号强度动态）和新型空间自适应保形预测方法（提供区域特定的不确定性估计）

Result: 在大规模真实数据集上的评估表明，SAC-GT实现了最先进的定位精度，同时提供鲁棒且空间自适应的可靠性保证

Conclusion: SAC-GT框架能够同时提供精确的2D位置预测和统计有效的置信区域，适应不同环境条件，解决了室内定位中不确定性量化的问题

Abstract: Indoor localization is a critical enabler for a wide range of location-based services in smart environments, including navigation, asset tracking, and safety-critical applications. Recent graph-based models leverage spatial relationships between Wire-less Fidelity (Wi-Fi) Access Points (APs) and devices, offering finer localization granularity, but fall short in quantifying prediction uncertainty, a key requirement for real-world deployment. In this paper, we propose Spatially-Adaptive Conformal Graph Transformer (SAC-GT), a framework for accurate and reliable indoor localization. SAC-GT integrates a Graph Transformer (GT) model that captures network's spatial topology and signal strength dynamics, with a novel Spatially-Adaptive Conformal Prediction (SACP) method that provides region-specific uncertainty estimates. This allows SAC-GT to produce not only precise two-dimensional (2D) location predictions but also statistically valid confidence regions tailored to varying environmental conditions. Extensive evaluations on a large-scale real-world dataset demonstrate that the proposed SAC-GT solution achieves state-of-the-art localization accuracy while delivering robust and spatially adaptive reliability guarantees.

</details>


### [18] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一个可扩展可控的路由框架，通过预测模型成本和性能进行动态路由决策，而非固定模型选择，能灵活适应新模型和预算约束


<details>
  <summary>Details</summary>
Motivation: 现有模型路由方法通常将路由视为在少量模型间的固定选择，难以适应新模型或变化的预算约束，需要更灵活的路由框架

Method: 提出SCOPE框架，通过强化学习训练，基于检索相似问题上的模型行为进行推理预测，而非依赖固定模型名称，能预测模型的准确性和成本

Result: SCOPE能灵活适应用户需求：当性能优先时可提升准确率高达25.7%，当效率优先时可降低成本高达95.1%

Conclusion: SCOPE不仅是一个成本节约工具，更是一个动态路由框架，通过预测模型成本和性能，让用户能轻松控制准确性与成本之间的权衡

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [19] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT框架通过知识引导的核状态重构，从噪声、部分观测数据中恢复物理系统的控制方程，结合结构先验知识提升符号发现的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测条件下容易失效，或依赖黑盒潜在动力学模型而缺乏可解释性，需要一种能结合领域知识、处理非均匀采样和测量粒度的符号发现框架。

Method: MAAT在再生核希尔伯特空间中构建状态重构，直接融入结构先验（如非负性、守恒定律、领域特定观测模型），获得平滑、物理一致的状态估计和解析时间导数，为符号回归提供接口。

Result: 在12个不同科学基准测试和多种噪声机制下，MAAT相比基线方法显著降低了状态估计的均方误差，为下游符号回归提供了更准确的轨迹和导数数据。

Conclusion: MAAT通过知识引导的状态重构，有效连接了碎片化传感器数据和符号回归，为从噪声、部分观测数据中发现可解释的物理规律提供了新方法。

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [20] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种可扩展的批次校正方法，通过构建平滑的亲和矩阵来对齐跨批次样本，适用于大规模Cell Painting数据，在保持校正质量的同时显著提升运行效率。


<details>
  <summary>Details</summary>
Motivation: Cell Painting是一种基于显微镜的高内涵成像技术，能生成丰富的细胞形态特征，支持药物发现。但在大规模应用中，实验室、仪器和协议差异导致的批次效应会掩盖生物信号，需要有效的批次校正方法。

Method: BALANS通过两个关键思想构建稀疏亲和矩阵：(1) 使用批次感知的局部尺度计算高斯核亲和度；(2) 采用自适应采样策略，优先选择邻居覆盖度低的行，并保留每行最强的亲和度，形成稀疏但信息丰富的近似矩阵。

Result: 理论证明该采样策略在样本复杂度上是顺序最优的，并提供近似保证。实验表明BALANS能扩展到大规模数据集，在保持校正质量的同时，相比广泛使用的批次校正方法显著提升运行时间。

Conclusion: BALANS是一种高效、可扩展的批次校正方法，适用于大规模Cell Painting数据分析，能有效处理批次效应而不牺牲校正质量，为药物发现等应用提供了实用工具。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [21] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种新的差分隐私随机梯度下降噪声相关策略，仅与前一次迭代相关并取消部分噪声，无需存储历史噪声，内存开销与标准DP-SGD相同


<details>
  <summary>Details</summary>
Motivation: 现有矩阵分解机制等DP-SGD扩展方法虽然通过跨多个训练迭代引入相关噪声提高了准确性，但需要存储先前添加的噪声向量，在某些设置下导致显著的内存开销

Method: 提出新的噪声相关策略，仅将噪声与前一次迭代相关，并取消受控部分的噪声。该方法依赖伪随机噪声生成器进行噪声再生，无需存储过去的噪声

Result: 该方法无需额外内存（与标准DP-SGD相同），计算开销最小，实验证明相比DP-SGD提高了准确性

Conclusion: 提出了一种内存高效的差分隐私随机梯度下降改进方法，通过创新的噪声相关策略在保持低内存开销的同时提高了模型准确性

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [22] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文提出了精确的偏好贝叶斯优化知识梯度方法，解决了传统方法在成对比较查询场景下的计算难题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化中的知识梯度方法在处理含噪声的黑盒函数优化时很流行，但许多实际场景只允许成对比较查询（偏好BO问题），而将知识梯度扩展到偏好BO面临计算挑战，因为前瞻步骤需要计算非高斯后验分布，之前被认为难以处理。

Method: 推导出精确且解析的偏好贝叶斯优化知识梯度方法，解决了非高斯后验分布的计算难题。

Result: 精确知识梯度在一系列基准问题上表现强劲，通常优于现有的采集函数。同时，通过案例研究展示了知识梯度在某些场景下的局限性。

Conclusion: 成功解决了偏好贝叶斯优化中知识梯度的计算挑战，提出了有效的精确方法，但也指出了该方法在某些情况下的局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [23] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 该研究评估语言模型在有限交互预算下探索交互环境的能力，发现现有模型存在系统性探索不足和次优解问题，性能甚至不如简单的探索-利用启发式基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估语言模型在有限交互预算下探索交互环境的能力，了解当前模型在探索任务中的表现和局限性。

Method: 研究方法包括：1）引入三个参数化任务，控制探索难度，涵盖连续和离散环境；2）评估最先进的语言模型；3）研究两种轻量级干预措施：将固定预算分割为并行执行，以及定期总结交互历史。

Result: 研究结果显示：1）现有模型存在系统性探索不足和次优解问题；2）性能通常显著低于简单的探索-利用启发式基线方法；3）随着预算增加，性能提升微弱；4）并行执行预算能意外提高性能；5）定期总结交互历史能保留关键发现并进一步改善探索。

Conclusion: 结论表明当前语言模型在探索任务中存在显著局限性，但通过轻量级干预措施（如预算分割和交互历史总结）可以改善探索性能，这为未来改进语言模型的探索能力提供了方向。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [24] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 该论文提出基于最优传输理论，利用相对平移不变二次Wasserstein空间的锥几何结构，引入相对Wasserstein角和正交投影距离两个新几何量来衡量经验分布与高斯分布之间的偏差，并证明该空间中的填充锥是平坦的，从而确保角度、投影和内积的严格定义。


<details>
  <summary>Details</summary>
Motivation: 现有方法在量化经验分布与高斯分布之间的偏差时存在局限性，特别是在高维情况下。作者希望从几何角度重新审视高斯近似问题，将其转化为高斯锥上的投影问题，并发现常用的矩匹配高斯方法并非给定经验分布的Wasserstein最近高斯分布。

Method: 1. 利用相对平移不变二次Wasserstein空间的锥几何结构，引入相对Wasserstein角和正交投影距离两个新几何量；2. 证明该空间中的填充锥是平坦的，确保角度、投影和内积的严格定义；3. 在一维情况下推导出闭式表达式并扩展到经典分布族；4. 在高维情况下开发基于半离散对偶公式的高效随机流形优化算法。

Result: 1. 证明了相对Wasserstein角比Wasserstein距离更鲁棒；2. 提出的最近高斯分布比矩匹配方法在Fréchet Inception Distance（FID）分数评估中提供更好的近似；3. 在合成数据和真实世界特征分布上的实验验证了方法的有效性。

Conclusion: 该研究从几何角度重新构建了高斯近似问题，提出的相对Wasserstein角和正交投影距离为量化非高斯性提供了有意义的度量。方法不仅在一维情况下有闭式解，还能通过高效算法扩展到高维，为分布分析和生成模型评估提供了新的工具。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [25] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 该论文研究了LLM推理阶段的系统级设计选择对能耗的显著影响，发现量化、批处理和请求调度等策略可以带来数量级的能耗差异


<details>
  <summary>Details</summary>
Motivation: 随着LLM在生产环境中的部署增加，计算资源和能源需求从训练转向推理阶段。现有研究主要关注每个提示或每个令牌的能耗，但系统级设计选择对能耗的影响尚未得到充分研究

Method: 在NVIDIA H100 GPU上进行详细的LLM推理能耗和延迟实证研究，分析量化、批处理大小和服务配置（如Hugging Face的Text Generation Inference服务器）的影响

Result: 研究发现：低精度格式仅在计算受限场景下带来能耗收益；批处理提高能效，特别是在解码等内存受限阶段；结构化请求定时（到达整形）可将每个请求的能耗降低高达100倍

Conclusion: 可持续的LLM部署不仅取决于模型内部，还取决于服务堆栈的编排。研究结果支持基于阶段的能耗分析和系统级优化，以实现更环保的AI服务

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [26] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一个免训练的多保真度回归框架，使用表格基础模型进行零样本上下文贝叶斯推理，通过高低保真模型的后验预测分布实现跨保真度信息传递，在31个基准问题上优于7种最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程多保真度回归面临立方计算复杂度、对稀疏高保真观测过拟合等问题，限制了实际应用中的效率和泛化能力，需要更高效鲁棒的解决方案。

Method: FIRE框架耦合表格基础模型，通过低保真模型的后验预测分布作为条件，构建高保真校正模型，实现零样本上下文贝叶斯推理，无需模型重新训练即可捕获异方差误差。

Result: 在31个合成和实际基准问题（如DrivAerNet、LCBench）上，FIRE在性能-时间权衡方面优于7种最先进的GP或深度学习方法，在准确性和不确定性量化方面排名最高，具有运行时优势。

Conclusion: FIRE提供了一种高效的多保真度回归新范式，通过表格基础模型实现免训练的跨保真度信息传递，但存在上下文窗口限制和依赖预训练模型质量的局限性。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [27] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR是一个使用LLM作为上下文强化学习控制器的自动扩缩框架，用于优化多阶段ML推理管道的资源分配，无需梯度更新即可在线改进策略。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道由于异构资源、跨阶段耦合和动态瓶颈迁移而难以自动扩缩，需要一种能够适应动态变化的智能资源管理方案。

Method: SAIR结合了帕累托优势奖励塑造与可证明的分离边界、基于意外度的经验检索以提高上下文效率，以及通过用户空间CUDA拦截实现的细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载模式下，SAIR在P99延迟和有效资源成本方面达到最佳或并列最佳，将P99延迟提升高达50%，有效成本降低高达97%，瓶颈检测准确率达到86%，且无需离线训练。

Conclusion: SAIR通过LLM作为上下文强化学习控制器，成功解决了多阶段ML推理管道的自动扩缩问题，实现了高效的资源管理和性能优化。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [28] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: MM-OpenFGL是首个针对多模态联邦图学习的综合基准，包含19个数据集、8种模拟策略、6个下游任务和57种SOTA方法，系统评估了MMFGL的必要性、有效性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的多模态属性图通常分布在隔离平台上，由于隐私或商业限制无法共享，而现有的联邦图学习研究主要关注单模态图，缺乏针对多模态联邦图学习的系统评估框架。

Method: 提出了MM-OpenFGL基准，系统化地形式化了MMFGL范式，包含19个多模态数据集（覆盖7个应用领域）、8种模拟策略（捕捉模态和拓扑变化）、6个下游任务，并通过模块化API实现了57种最先进方法。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率四个角度全面研究了MMFGL，为未来研究提供了有价值的见解。

Conclusion: MM-OpenFGL填补了多模态联邦图学习领域的评估空白，为系统研究和比较不同方法提供了首个综合基准，推动了该领域的发展。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [29] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出一种基于显式对比学习的LLM推理方法，替代传统的GRPO优势估计方法，通过将结果分为正负集来最大化正结果概率


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽然有效，但需要复杂的经验性调整（如非对称裁剪和零方差数据过滤），这些调整难以识别且需要大量经验洞察

Method: 提出显式对比学习方法：将K个结果分为正负两个集合，然后最大化正结果的似然概率；该方法可视为LLM推理中（多标签）噪声对比估计的在线实例化

Result: 在具有挑战性的数学基准测试中，与DAPO和在线DPO等强基线相比，表现出具有竞争力的性能

Conclusion: 提出的显式对比学习方法提供了一种更直接、更易于实现的替代方案，避免了GRPO所需的复杂经验调整，同时在推理任务中保持竞争力

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [30] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 提出基于权重的SAE特征解释框架，无需激活数据，通过直接权重交互测量功能效应，发现1/4特征直接预测输出token，特征在注意力机制中具有深度依赖结构，语义与非语义特征在注意力电路中分布不同


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器（SAE）特征解释方法主要基于激活模式推断特征语义，但忽略了这些特征训练的目的是重构在前向传播中具有计算功能的激活。需要一种新的解释框架来理解SAE特征的实际功能作用

Method: 引入基于权重的解释框架，通过直接权重交互测量功能效应，无需激活数据。在Gemma-2和Llama-3.1模型上进行三个实验：1）分析特征直接预测输出token的能力；2）研究特征在注意力机制中的参与模式；3）比较语义和非语义特征在注意力电路中的分布特征

Result: 1）约1/4的SAE特征直接预测输出token；2）特征在注意力机制中积极参与，具有深度依赖的结构模式；3）语义特征和非语义特征在注意力电路中表现出不同的分布特征

Conclusion: 该权重基础解释框架提供了SAE特征解释中缺失的"上下文外"部分，揭示了特征通过权重交互实现的实际计算功能，为理解语言模型内部表示提供了新的视角

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [31] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种用于RLVR训练的高效采样方法，通过堆采样和在线查询增强来优化提示池管理，减少计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源。

Method: HeaPA维护一个有界的演化提示池，使用堆采样跟踪能力边界，通过轻量级异步验证的在线查询增强扩展池，并通过拓扑感知的统计重估计和控制重插入来稳定相关查询。

Result: 在两个训练语料库、两种训练方案和七个基准测试中，HeaPA持续提升准确性，以更少的计算达到目标性能，同时保持相当的训练时间，且模型规模越大收益越明显。

Conclusion: HeaPA通过边界聚焦采样和在线池增长机制，显著提高了RLVR训练的效率，特别是在大规模模型训练中表现出更大的优势。

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [32] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: EvoEGF-Mol：基于信息几何的SBDD新方法，通过指数测地流在统计流形上建模分子生成，避免传统方法在欧几里得空间和概率空间分离导致的流形不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计方法在欧几里得空间和概率空间分别构建概率路径，导致与底层统计流形不匹配。需要从信息几何角度解决这一问题，使分子生成过程更符合统计流形的几何结构。

Method: 将分子建模为复合指数族分布，在Fisher-Rao度量下定义沿指数测地线的生成流。为避免直接以狄拉克分布为目标导致的瞬时轨迹崩溃，提出EvoEGF-Mol，用动态集中分布替代静态狄拉克目标，通过渐进参数精炼架构确保训练稳定性。

Result: 在CrossDock上达到参考级别的PoseBusters通过率（93.4%），表现出卓越的几何精度和相互作用保真度。在真实世界的MolGenBench任务中优于基线方法，能够恢复生物活性骨架并生成符合既定MedChem过滤器的候选分子。

Conclusion: 从信息几何角度建模分子生成过程能够有效解决传统SBDD方法的流形不匹配问题，EvoEGF-Mol通过指数测地流实现了稳定训练和高精度分子生成，为基于结构的药物设计提供了新范式。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [33] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: 研究发现大型语言模型（LLMs）在无奖励探索阶段表现出潜在学习动态，这种心理学现象使模型在后续奖励引入时获得更好的性能提升。


<details>
  <summary>Details</summary>
Motivation: 心理学中的潜在学习现象表明生物体可以在无奖励情况下学习环境表征，而当前LLMs主要依赖奖励驱动的强化学习范式。研究探索LLMs是否也存在类似潜在学习动态，以及如何利用这种现象提升模型性能。

Method: 采用两阶段训练范式：第一阶段让LLMs在无奖励环境下进行探索，第二阶段引入奖励进行训练。在多个模型家族和多样化任务领域进行广泛实验，并提供理论分析解释无奖励探索的性能增益机制。

Result: LLMs在无奖励探索阶段表现出适度的性能改进，这种探索使模型能够组织任务相关知识而不受奖励驱动偏见约束。一旦引入奖励，性能得到进一步显著提升。采用这种两阶段探索机制的LLMs最终比全程使用奖励强化学习训练的模型获得更高能力。

Conclusion: LLMs确实表现出心理学中的潜在学习动态，无奖励探索阶段有助于模型构建更灵活的知识表征，减少奖励偏见，最终实现更好的泛化能力和性能。这为LLMs训练提供了新的理论视角和实践方法。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [34] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出了一种新颖的师生框架，将持续强化学习解耦为两个独立过程：通过分布式RL训练单任务教师模型，并持续蒸馏到中央通用模型中，结合MoE架构和重放机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习面临稳定性-可塑性困境，直接应用RL到连续任务流中难以实现可扩展性能。观察到RL擅长解决单任务，而策略蒸馏作为相对稳定的监督学习过程，更适合大规模基础模型和多任务学习。

Method: 1. 师生框架解耦CRL：分布式RL训练单任务教师模型 + 持续蒸馏到中央通用模型；2. 采用混合专家(MoE)架构增强可塑性；3. 使用基于重放的方法提升稳定性。

Result: 在Meta-World基准测试中，框架实现了高效的持续RL，恢复了超过85%的教师性能，同时将任务遗忘率控制在10%以内。

Conclusion: 通过解耦单任务RL训练和多任务策略蒸馏，结合MoE和重放机制，有效解决了持续强化学习的稳定性-可塑性困境，实现了高效的知识积累和迁移。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [35] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过生成语义等价的变换问题变体，解决了GRPO中的多样性崩溃和梯度消失问题，在数学推理基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为模式匹配器对表面表达变化敏感，而GRPO方法存在两个失败模式：多样性崩溃（训练放大单一解决方案策略）和梯度消失（大量问题产生零梯度），这恶化了推理能力。

Method: 提出TA-GRPO方法，通过转述、变量重命名和格式变化生成语义等价的变换问题变体，通过跨整个组池化奖励计算优势，确保混合奖励并促进多种解决方案策略。

Result: 在数学推理基准上获得一致的Pass@k改进，在竞赛数学（AMC12, AIME24）上提升高达9.84分，在分布外科学推理（GPQA-Diamond）上提升5.05分。

Conclusion: TA-GRPO通过减少零梯度概率和改善泛化能力，有效解决了GRPO的局限性，为语言模型的推理能力提供了理论支持和实证验证。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [36] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS框架通过监测隐藏状态中的L2距离尖峰来检测认知惯性，无需训练即可实时纠正大型推理模型的推理过程


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过扩展测试时计算取得了显著性能，但经常遭受认知惯性的困扰，表现为过度思考或推理僵化。现有检测方法通常依赖表面文本启发式方法，无法捕捉模型未表达的内部冲突。

Method: 提出STARS框架：1）通过检测隐藏状态中的L2距离尖峰来识别认知转折点；2）使用几何轨迹分析诊断过渡的结构性质；3）注入状态感知的语言提示来实时引导模型。

Result: 在多个基准测试上的实验证实，STARS能有效减少冗余循环，同时通过自适应纠正错误轨迹来提高准确性。

Conclusion: STARS提供了一个无需额外微调的鲁棒无监督机制，用于优化大型推理模型的推理过程。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [37] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: GFT是一种用于流匹配模型微调的理论框架，通过温度控制的中间目标平滑过渡预训练和目标分布，提高收敛稳定性并缩短推理路径


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布变化或效率要求严格的场景中，无约束的微调会损害预训练获得的准确性和效率优势。现有方法对漂移结构或训练技术有限制，需要更通用的微调框架

Method: 提出渐进微调(GFT)框架，为随机流定义温度控制的中间目标序列，平滑插值预训练和目标漂移。当温度趋近零时接近真实目标。支持使用合适的耦合（如最优传输）同时保持正确性

Result: GFT提高了收敛稳定性，缩短了概率路径，实现更快推理，同时保持与标准微调相当的生成质量。理论证明边际和条件GFT目标的收敛性

Conclusion: GFT为流匹配模型在分布偏移下的可扩展适应提供了理论基础和实践有效的替代方案，是处理有限数据、演化分布和效率约束场景的有前景方法

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [38] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文研究无人机辅助可见光通信系统中的三维轨迹规划，通过优化无人机飞行高度和水平轨迹来最小化飞行距离，提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机与可见光通信技术的结合为提供灵活通信和高效照明提供了有前景的解决方案。需要解决无人机在收集地面用户数据时的三维轨迹规划问题，以最小化飞行距离并提高数据收集效率。

Method: 首先推导了在特定VLC信道增益阈值下的闭式最优飞行高度，然后通过将新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合来优化无人机水平轨迹，实现复杂环境中的自适应运动策略。

Result: 仿真结果表明，推导的最优高度相比基准方法可减少高达35%的飞行距离。提出的奖励机制显著缩短了约50%的收敛步数，在无人机辅助VLC数据收集中表现出显著的效率提升。

Conclusion: 该研究提出的三维轨迹规划框架有效解决了无人机辅助VLC系统中的轨迹优化问题，通过高度优化和自适应轨迹规划显著提高了数据收集效率，为无人机-VLC集成系统提供了实用的解决方案。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [39] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出了一种用于非平稳时间序列的延迟学习框架，通过因子化切换线性高斯状态空间模型建模专家残差，支持动态专家注册，并设计了信息导向的路由策略


<details>
  <summary>Details</summary>
Motivation: 针对非平稳时间序列中专家可用性随时间变化、只能获得部分反馈的问题，需要开发能够适应动态专家环境、实现跨专家信息共享的延迟学习框架

Method: 提出L2D-SLDS模型：因子化切换线性高斯状态空间模型，包含上下文相关的状态转换、共享全局因子（实现跨专家信息传递）和专家特定状态；支持动态专家注册；基于一步预测信念设计信息导向的路由策略

Result: 实验表明该方法优于上下文多臂老虎机基线，且优于无共享因子的消融模型，证明了跨专家信息共享的有效性

Conclusion: 提出的L2D-SLDS模型和信息导向路由策略能够有效处理非平稳时间序列中的延迟学习问题，支持动态专家环境并实现跨专家知识迁移

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [40] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，包含模型导向、无模型和情景控制三个模块，用于大规模统计机器学习中的不确定性量化


<details>
  <summary>Details</summary>
Motivation: 人类通过多个相互作用的神经系统的互补控制实现高效学习，包括模型导向规划、无模型习惯性响应和情景记忆学习。作者希望将这种生物效率的计算原理转化为可扩展贝叶斯推断的采样算法

Method: 提出包含三个组件的算法：1）模型导向模块使用目标分布进行引导但计算缓慢的采样；2）无模型模块利用先前样本学习参数空间模式，实现快速反射式采样；3）情景控制模块通过回忆特定过去事件（样本）支持快速采样

Result: 该方法推进了贝叶斯方法的发展，促进了其在大规模统计机器学习问题中的应用，特别是在贝叶斯深度学习中实现了适当和原则性的不确定性量化

Conclusion: 受人类多系统学习机制启发的三模块采样算法为大规模贝叶斯推断提供了有效的计算框架，特别适用于需要不确定性量化的深度学习应用

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [41] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出了一种模型无关的守恒量校正技术，用于在深度学习模型中融入物理守恒准则，显著改善了自回归神经算子模型的长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习作为PDE数值解的高效替代方案，能够通过近似解算子实现快速迭代模拟。然而，深度学习解决方案在长期预测中表现不佳，主要原因是自回归误差的累积以及模型无法守恒物理量。

Method: 提出了一种模型无关的守恒量校正技术，将物理守恒准则融入深度学习模型。该方法不依赖于特定模型架构，适用于各种神经算子模型。

Result: 该方法在所有模型架构中都显著改善了自回归神经算子模型的长期稳定性。同时从谱域分析了神经算子的性能，揭示了现有架构在高频分量处理上的显著局限性。

Conclusion: 未来的工作需要设计能够特别强调高频分量的架构，这对于理解和建模湍流等复杂流动现象至关重要。守恒量校正技术为深度学习PDE求解提供了重要的改进方向。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [42] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT是一个统一、可扩展、可解释的多图学习框架，通过图Transformer编码器将不同图映射到共享潜在空间，构建元图进行跨图联合推理，在预测任务中优于现有方法并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 多图学习面临整合具有不同拓扑、规模和语义的异构图信息的挑战，特别是在缺乏共享节点标识的情况下。需要一种能够有效整合跨图信息并保持可解释性的统一框架。

Method: MGMT首先使用图Transformer编码器将每个图的结构和属性映射到共享潜在空间；然后通过注意力选择任务相关的超节点，基于潜在空间相似性构建连接跨图功能对齐超节点的元图；最后在元图上应用额外的图Transformer层进行联合推理。

Result: 在合成数据集和真实世界神经科学应用中，MGMT在图级预测任务中始终优于现有最先进模型，同时提供可解释的表示，促进科学发现。

Conclusion: MGMT为结构化多图学习建立了统一框架，在图数据发挥核心作用的领域中推进了表示技术，提供了可扩展且可解释的跨图学习解决方案。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [43] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: Lethe是一种新的联邦遗忘方法，解决了持续训练中已遗忘知识重新激活的问题，通过解耦待遗忘和待保留知识确保持久遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘研究通常假设遗忘操作后协作结束，忽略了后续持续训练的情况。研究发现持续训练会重新激活已遗忘知识，导致知识重现问题。

Method: Lethe采用重塑-修正-恢复流程：首先在遗忘数据上训练临时适配器获取放大更新，然后将其作为修正信号对剩余更新进行两层分流修正，最后移除适配器并在保留数据上进行短期恢复。

Result: Lethe能以统一方式支持联邦系统中所有级别的遗忘，在多数情况下保持优越的持久性（重现率<1%），即使经过多轮后续训练。

Conclusion: Lethe解决了联邦遗忘中的知识重现问题，通过解耦知识确保持久遗忘，为联邦学习系统提供了更可靠的遗忘机制。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [44] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制作为注意力机制的替代方案，能提升Transformer在不同学习率下的训练稳定性，并提出了混合共识-注意力框架来平衡性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力Transformer在训练时对学习率过指定表现出不稳定性，特别是在高学习率下。虽然已有方法通过修改优化过程来改善这种过指定的鲁棒性，但根本性的架构创新仍然不足。

Method: 提出共识机制作为注意力的直接替代方案，将其形式化为图模型，并在文本、DNA和蛋白质模态上进行广泛实验分析。进一步提出了混合共识-注意力框架来保持性能同时提高稳定性。

Result: 共识机制在更广泛的有效学习率范围内稳定了Transformer训练，通过跨模态的学习率扫描实验验证了改进的稳定性。混合框架在保持性能的同时提高了稳定性。

Conclusion: 共识机制是注意力机制的有效替代方案，能显著提升Transformer训练的学习率鲁棒性，混合框架为平衡性能与稳定性提供了实用解决方案。

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [45] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 提出一种结合形式逻辑验证与语言生成的框架，通过实时反馈检测和纠正推理错误，显著提升LLM的逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现出色，但其随机性的下一个token预测会导致逻辑不一致和奖励黑客问题，而形式符号系统可以避免这些问题。为了弥合这一差距，需要将形式逻辑验证与自然语言生成过程动态结合。

Method: 引入形式逻辑验证引导的框架，在自然语言生成过程中动态交织形式符号验证，提供实时反馈来检测和纠正错误。采用两阶段训练流程：形式逻辑验证引导的监督微调和策略优化，主动惩罚推理链中的中间谬误。

Result: 在六个涵盖数学、逻辑和一般推理的基准测试中，7B和14B模型分别以平均10.4%和14.2%的优势超越了最先进的基线模型。

Conclusion: 形式验证可以作为可扩展的机制，显著推动先进LLM推理的性能边界，验证了将形式逻辑验证与语言生成动态结合的框架的有效性。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [46] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种无需验证数据的联邦学习早期停止框架，通过监控任务向量的增长率来确定最优停止点，仅使用服务器端参数


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然支持去中心化协作学习而不传输原始数据，但依赖固定的全局轮次或验证数据进行超参数调整会带来高计算成本和隐私风险，阻碍实际部署

Method: 提出数据无关的早期停止框架，通过监控任务向量的增长率来确定最优停止点，仅使用服务器端参数，无需任何验证数据

Result: 在皮肤病变/血细胞分类任务上，该方法与基于验证的早期停止方法性能相当，平均仅需47/20轮就能获得比基于验证数据的早期停止高出12.5%/10.3%的性能

Conclusion: 这是首个提出无需验证数据的联邦学习早期停止框架的工作，显著降低了计算成本和隐私风险，具有实际部署价值

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [47] [Full-Graph vs. Mini-Batch Training: Comprehensive Analysis from a Batch Size and Fan-Out Size Perspective](https://arxiv.org/abs/2601.22678)
*Mengfan Liu,Da Zheng,Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文系统比较了全图与mini-batch GNN训练方法，通过批大小和扇出大小的视角，揭示了这两个超参数对收敛和泛化的非各向同性影响，发现全图训练并不总是优于调优后的mini-batch设置。


<details>
  <summary>Details</summary>
Motivation: 全图和mini-batch GNN训练方法在系统设计需求上有显著差异，但缺乏对两者性能（收敛和泛化）与计算效率的系统比较。批大小在DNN分析中有效，但GNN引入了扇出大小这一新维度，其影响尚未充分探索。

Method: 通过经验分析和理论分析，从批大小和扇出大小的视角系统比较GNN训练方法。使用Wasserstein距离进行泛化分析以研究图结构（特别是扇出大小）的影响，并揭示批大小和扇出大小在收敛和泛化中的非各向同性效应。

Result: 研究发现批大小和扇出大小对GNN收敛和泛化具有非各向同性影响，为资源约束下调整这些超参数提供实用指导。全图训练并不总是比调优后的较小mini-batch设置产生更好的模型性能或计算效率。

Conclusion: 全图训练并非总是最优选择，调优的mini-batch设置可以在模型性能和计算效率方面达到甚至超越全图训练。批大小和扇出大小是理解GNN训练动态的关键维度，需要根据具体资源约束进行平衡调整。

Abstract: Full-graph and mini-batch Graph Neural Network (GNN) training approaches have distinct system design demands, making it crucial to choose the appropriate approach to develop. A core challenge in comparing these two GNN training approaches lies in characterizing their model performance (i.e., convergence and generalization) and computational efficiency. While a batch size has been an effective lens in analyzing such behaviors in deep neural networks (DNNs), GNNs extend this lens by introducing a fan-out size, as full-graph training can be viewed as mini-batch training with the largest possible batch size and fan-out size. However, the impact of the batch and fan-out size for GNNs remains insufficiently explored. To this end, this paper systematically compares full-graph vs. mini-batch training of GNNs through empirical and theoretical analyses from the view points of the batch size and fan-out size. Our key contributions include: 1) We provide a novel generalization analysis using the Wasserstein distance to study the impact of the graph structure, especially the fan-out size. 2) We uncover the non-isotropic effects of the batch size and the fan-out size in GNN convergence and generalization, providing practical guidance for tuning these hyperparameters under resource constraints. Finally, full-graph training does not always yield better model performance or computational efficiency than well-tuned smaller mini-batch settings. The implementation can be found in the github link: https://github.com/LIUMENGFAN-gif/GNN_fullgraph_minibatch_training.

</details>


### [48] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 该论文提出了一个用于评估医疗AI数据质量的METRIC框架，并开发了一个包含具体度量指标的度量库，以支持医疗机器学习中可信AI的发展。


<details>
  <summary>Details</summary>
Motivation: 医疗机器学习从研究转向实际应用，需要建立可信AI。数据质量评估是可信AI发展的关键因素，但缺乏系统化的实践方法。

Method: 将理论上的METRIC框架操作化，开发了一个包含具体数据质量度量的度量库，为每个度量提供度量卡片（包含定义、适用性、示例、陷阱和建议），并提供决策树帮助用户根据具体用例选择合适的度量指标。

Result: 在PTB-XL心电图数据集上展示了该方法的影响，证明了该框架在实际应用中的可行性和价值。

Conclusion: 这是实现医疗领域可信AI的重要第一步，为实践中评估训练和测试数据的适用性提供了基础工具，有助于医疗AI的临床接受、有效采用和监管批准。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [49] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 论文系统研究了Softmax族损失函数，分析了不同替代损失在分类和排序任务中的一致性、梯度动态和收敛行为，提出了近似方法的偏差-方差分解，并给出了收敛保证和复杂度分析。


<details>
  <summary>Details</summary>
Motivation: Softmax损失是分类和排序任务中最广泛使用的替代目标函数。虽然Fenchel-Young框架将其置于广泛的替代函数族中，但面对类别数量极大的情况，现有研究主要关注可扩展性近似方法。本文旨在从理论一致性和实际效率两个角度，为Softmax族损失函数建立原则性基础。

Method: 1) 分析不同替代损失函数与分类和排序指标的一致性；2) 研究梯度动态以揭示不同的收敛行为；3) 为近似方法引入系统的偏差-方差分解，提供收敛保证；4) 推导每轮复杂度分析，展示效果与效率之间的明确权衡。

Result: 在代表性任务上的大量实验表明，一致性、收敛性和实证性能之间存在强对齐关系。研究结果为大规模类别机器学习应用中的损失函数选择提供了理论依据和实践指导。

Conclusion: 本文为Softmax族损失函数建立了原则性基础，揭示了不同替代损失的理论性质，提供了近似方法的收敛保证和复杂度分析，为大规模类别机器学习中的损失函数选择提供了系统指导。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [50] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一个轻量级的客户端加密框架，通过将原始嵌入投影到"混淆语义零空间"来保护LLM推理隐私，无需后处理即可实现隐私保护。


<details>
  <summary>Details</summary>
Motivation: 为了保护大型语言模型推理过程中的隐私，需要一种轻量级的客户端加密框架，能够在保持语义保真度的同时实现有效的隐私保护。

Method: 提出"混淆语义零空间"概念，将线性核的几何直觉推广到LLM的高维潜在空间，通过注入扰动将原始嵌入投影到这个空间中，同时采用密钥相关的随机映射为每个用户生成独特的扰动轨迹。

Result: 在12个生成和分类基准测试中，OSNIP实现了最先进的性能，显著降低了攻击成功率，同时在严格的安全约束下保持了强大的模型效用。

Conclusion: OSNIP提供了一种有效的隐私保护LLM推理解决方案，通过混淆语义零空间注入实现了隐私保护与模型效用的良好平衡。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [51] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 该研究系统研究了分子语言模型的缩放规律，通过训练300个模型和超过10,000次实验，揭示了在固定计算预算下模型大小、训练数据量和分子表示对性能的影响规律。


<details>
  <summary>Details</summary>
Motivation: 分子生成模型虽然在大规模数据集和模型尺寸上表现出潜力，但尚不清楚这些模型是否遵循可预测的缩放规律。理解在固定计算预算下如何最优分配模型大小、数据量和分子表示的资源分配至关重要。

Method: 研究训练了300个模型，进行了超过10,000次实验，严格控制计算预算，同时独立变化模型大小、训练token数量和分子表示，系统研究分子语言模型在预训练和下游任务中的缩放行为。

Result: 结果表明分子模型在预训练和下游迁移中都存在清晰的缩放规律，分子表示对性能有显著影响，并解释了先前观察到的分子生成缩放行为不一致性。同时发布了迄今为止最大的分子语言模型库。

Conclusion: 该研究为分子语言模型的缩放行为提供了系统性的理解，揭示了分子表示在缩放规律中的关键作用，为未来研究提供了重要的模型资源和实验基础。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [52] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 论文揭示了稀疏注意力机制与紧支核函数之间的理论对应关系，建立了α-entmax注意力与Epanechnikov、biweight、triweight等核函数的数学联系，为稀疏注意力提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 现有研究已经建立了自注意力机制与Nadaraya-Watson核回归之间的联系，标准softmax注意力对应高斯核。然而，对于稀疏注意力机制缺乏核理论的理解，需要建立稀疏注意力与紧支核函数之间的形式化对应关系。

Method: 通过理论分析建立稀疏注意力机制与紧支核函数之间的数学对应关系。具体证明归一化ReLU和sparsemax注意力分别对应固定归一化和自适应归一化下的Epanechnikov核回归。更一般地，展示非参数密度估计中广泛使用的核函数（Epanechnikov、biweight、triweight）对应α-entmax注意力，其中α=1+1/n（n∈ℕ），而softmax/高斯关系在n→∞时出现。

Result: 建立了稀疏注意力与紧支核函数的理论对应关系，解释了稀疏性如何从核设计中自然产生。通过基于核回归的Transformer变体Memory Mosaics进行实验，证明基于核的稀疏注意力在语言建模、上下文学习和长度泛化任务上具有竞争力。

Conclusion: 该研究为稀疏注意力机制提供了统一的核理论视角，解释了稀疏性的理论来源，并为设计注意力机制提供了原则性框架，替代了启发式的top-k注意力和其他关联记忆机制。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [53] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无剪裁策略优化方法，用凸二次惩罚替代传统剪裁机制，解决了强化学习训练大语言模型时的梯度消失、奖励攻击和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流强化学习算法依赖剪裁机制，但在大规模训练中会引入优化问题，包括零梯度区域、奖励攻击和训练不稳定性。需要一种更稳定、无剪裁的优化方法。

Method: CFPO使用基于总变差散度约束的凸二次惩罚替代启发式剪裁，产生处处可微的目标函数，实现无硬边界的稳定策略更新。

Result: 在推理任务中，CFPO与剪裁方法在下游基准测试中表现相当，同时扩展了稳定训练范围；在对齐任务中，CFPO缓解了冗长利用问题，减少了能力退化，同时保持了竞争力的指令跟随性能。

Conclusion: CFPO是一种有前景的即插即用替代方案，只需一行代码更改且无需额外超参数，可替代剪裁方法用于大语言模型的后训练。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [54] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，在NVFP4格式下实现更精确的量化训练，相比传统方法降低2倍量化误差，在1.9B参数LLM训练中验证效果，提供Blackwell GPU内核实现4.2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有量化训练方法在使用NVFP4格式时，为了获得更准确的无偏梯度估计（通过随机舍入），牺牲了格式的表示能力，导致相对于FP16和FP8训练存在明显的精度损失。

Method: 提出MS-EDEN（微尺度格式的无偏量化方法），相比随机舍入降低2倍以上量化误差；将其集成到Quartet II全NVFP4量化方案中，用于线性层的量化训练；与针对NVFP4的最新训练改进技术协同工作。

Result: 分析证明Quartet II在所有主要矩阵乘法（前向和后向传播）中实现更优的梯度估计；在1.9B参数、38B tokens的LLM端到端训练中验证效果；提供NVIDIA Blackwell GPU内核，相比BF16实现4.2倍加速。

Conclusion: MS-EDEN和Quartet II方案显著改进了NVFP4量化训练的状态，在保持无偏梯度估计的同时降低量化误差，实现大规模模型的高效量化预训练。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [55] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 该论文提出了一种级联扩散模型，用于生成包含离散和连续特征的表格数据，特别解决了混合类型特征的生成挑战。


<details>
  <summary>Details</summary>
Motivation: 生成模型在表格数据生成方面已有进展，但混合类型特征（同时包含离散状态和连续分布的特征）的生成仍然具有挑战性。现有方法难以准确生成这类特征。

Method: 采用级联方法：首先生成表格行的低分辨率版本（纯分类特征和数值特征的粗略分类表示），然后通过新颖的引导条件概率路径和数据依赖耦合，在流匹配模型中利用这些信息生成高分辨率数据。

Result: 模型显著提高了生成样本的真实性，更准确地捕捉了分布细节，检测分数提高了40%。理论证明该级联方法收紧传输成本界限。

Conclusion: 提出的级联扩散模型在表格数据生成方面取得了最先进的结果，特别在混合类型特征生成上表现优异，为表格数据生成提供了更准确的解决方案。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [56] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 提出了一种通过正则化预训练自编码器来鼓励潜在空间等变性的流匹配框架，用于时间序列生成，在保持计算效率的同时提升生成质量


<details>
  <summary>Details</summary>
Motivation: 当前基于流的时间序列生成模型通常在低维潜在空间中定义以实现高效采样，但如何设计具有理想等变性质的潜在表示用于时间序列生成仍未被充分探索

Method: 提出潜在流匹配框架，通过等变性损失对预训练自编码器进行正则化，强制变换信号与其重构之间的一致性，针对时间序列基本变换（如平移和幅度缩放）微调潜在空间

Result: 在多个真实世界数据集上的实验表明，该方法在标准时间序列生成指标上持续优于现有基于扩散的基线方法，同时实现数量级更快的采样速度

Conclusion: 这些结果突显了将几何归纳偏置纳入时间序列潜在生成模型的实际优势，等变性正则化的潜在空间在保持潜在流模型计算优势的同时提高了生成质量

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [57] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG：基于不平衡最优传输的正则化框架，解决多模态属性图中显式图结构与隐式语义结构之间的不一致问题，通过Fused Gromov-Wasserstein距离引导跨模态结构一致性，提升节点表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 多模态属性图（MAGs）中显式图结构与不同模态嵌入诱导的隐式语义结构存在不一致，例如图中邻居在一个模态中相似但在另一个模态中可能不相似。现有方法在固定显式图结构上进行消息传递时，会聚合不相似的特征，引入模态特定噪声，阻碍有效的节点表示学习。

Method: 提出OptiMAG框架，基于不平衡最优传输，使用Fused Gromov-Wasserstein距离在局部邻域内显式引导跨模态结构一致性，缓解结构-语义冲突。同时使用KL散度惩罚自适应处理跨模态不一致性。该框架可作为即插即用的正则化器无缝集成到现有多模态图模型中。

Result: 实验表明OptiMAG在多个任务上持续优于基线方法，包括图中心任务（如节点分类、链接预测）和多模态中心生成任务（如图到文本、图到图像生成）。

Conclusion: OptiMAG通过最优传输理论有效解决了多模态属性图中的结构-语义不一致问题，能够提升现有多模态图模型的性能，在各种任务中表现出色。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [58] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL：一种新颖的多任务强化学习方法，能够实现零样本泛化，不仅能在LTL公式结构上组合泛化，还能在命题上参数化泛化。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习中的核心挑战是训练能够执行训练中未见任务的通用策略。现有LTL引导的多任务RL方法虽然能在LTL规范上成功泛化，但无法泛化到未见过的命题词汇表（描述LTL中高层事件的"符号"）。

Method: 将命题视为参数化谓词的实例而非离散符号，使策略能够学习相关命题间的共享结构。提出新颖的架构来嵌入和组合谓词以表示LTL规范。

Result: 在具有挑战性的环境中，成功实现了对新颖命题和任务的零样本泛化。

Conclusion: PlatoLTL方法通过将命题参数化，实现了比现有方法更强大的泛化能力，能够处理未见过的命题词汇表，为多任务强化学习提供了新的解决方案。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [59] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出了一种基于正则化的多变量校准方法，使用预排序函数在训练期间强制多变量校准，并引入基于PCA的新预排序函数来检测依赖结构误设


<details>
  <summary>Details</summary>
Motivation: 尽管单变量概率预测已取得进展，但实现多变量校准仍然具有挑战性。现有预排序函数主要用于事后评估，缺乏在训练过程中强制多变量校准的方法

Method: 提出基于正则化的校准方法，在训练多变量分布回归模型时使用预排序函数强制多变量校准；引入基于PCA的新预排序函数，将预测投影到预测分布的主方向上

Result: 在18个真实世界多输出回归数据集上的实验表明，该方法显著改善了多变量预排序校准，且不影响预测准确性；PCA预排序能检测出现有预排序无法发现的依赖结构误设

Conclusion: 该方法有效解决了多变量校准问题，PCA预排序为评估多变量预测的依赖结构提供了新的诊断工具

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [60] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出了一种结合贝叶斯决策树和高斯过程的单树模型，用于改善回归任务中的外推能力和不确定性校准


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中外推能力差，分段常数预测受限于训练目标范围，在分布偏移下容易过度自信，需要更好的不确定性建模和外推机制

Method: 扩展VSPYCT模型，为每个叶子节点配备高斯过程预测器；使用贝叶斯倾斜分割进行不确定性感知的输入空间划分；GP叶子建模局部函数行为；采用高效推理预测方案，结合分割参数的后验采样和GP后验预测；当输入超出叶子训练支持时激活基于GP的外推机制

Result: 在基准回归任务中相比标准变分倾斜树有更好的预测性能，在外推场景中表现出显著的性能提升

Conclusion: 提出的单树贝叶斯模型通过结合决策树的解释性和GP的外推能力，有效解决了传统决策树在回归任务中的外推和不确定性校准问题

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [61] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA：基于熵引导的灵活低秩适应框架，通过谱能量熵评估矩阵重要性，支持全局预算下的秩剪枝和扩展，使用零影响初始化确保稳定性，在参数高效微调中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型虽然在各领域表现优异，但完全微调的计算和内存成本过高。参数高效微调（PEFT）成为主流范式，其中LoRA方法引入可训练低秩矩阵但固定秩设计限制了灵活性。现有动态秩分配方法依赖启发式元素级指标，缺乏矩阵级区分，且没有机制扩展需要额外适应的层的能力。

Method: 提出FlexLoRA框架：1）通过谱能量熵评估矩阵重要性；2）支持在全局预算下进行秩剪枝和扩展；3）对新添加的奇异方向采用零影响初始化以确保稳定性。该方法解决了现有方法在粒度、灵活性和稳定性方面的限制。

Result: 大量实验表明，FlexLoRA在各种基准测试中持续优于最先进的基线方法。

Conclusion: FlexLoRA为参数高效微调提供了一个更原则性的解决方案，通过熵引导的灵活低秩适应框架，克服了现有方法的局限性，实现了更好的性能表现。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [62] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出DC-LA算法用于采样非对数凹分布，其中正则项为DC函数，通过Moreau包络平滑处理，在q-Wasserstein距离下证明收敛性


<details>
  <summary>Details</summary>
Motivation: 研究目标分布π∝exp(-f-r)的采样问题，其中f是Lipschitz光滑的数据保真项，r=r1-r2是非光滑的DC函数。现有方法在处理非对数凹采样时假设较严格，需要更通用的框架

Method: 利用DC结构，对r1和r2分别应用Moreau包络平滑正则项，将正则项的凹部分重新分配到数据保真项，提出DC-LA（DC-Langevin算法）

Result: 在V满足远距离耗散性假设下，证明DC-LA在q-Wasserstein距离下收敛到目标分布π（考虑离散化和平滑误差），改进了先前非对数凹采样工作的假设条件

Conclusion: DC-LA算法在合成设置中能准确生成分布，在真实世界CT应用中能可靠提供不确定性量化，为处理非光滑DC正则项的采样问题提供了有效框架

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [63] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Yuanchao Wang,Zhao-Rong Lai,Tianqi Zhong,Fengnan Li*

Main category: cs.LG

TL;DR: ECTR框架通过环境条件尾部重加权增强TV不变风险最小化，同时处理环境级相关偏移和样本级多样性偏移，提升OOD泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有IRM方法主要处理环境级的虚假相关性，但忽略了环境内样本级异质性，这在混合分布偏移下会严重影响OOD性能

Method: 提出环境条件尾部重加权总变差不变风险最小化(ECTR)框架，将TV不变学习与环境条件尾部重加权结合，处理环境级相关偏移和样本级多样性偏移；在没有环境标注时通过极小极大公式推断潜在环境

Result: 在回归、表格数据、时间序列和图像分类基准测试中，在混合分布偏移下，最差环境和平均OOD性能均得到一致提升

Conclusion: ECTR框架通过联合处理环境级不变性和环境内鲁棒性，使这两种机制在混合分布偏移下互补，显著提升OOD泛化能力

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [64] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文解决了线性赌博机中Nash遗憾的次优问题，提出了新的分析工具实现最优Nash遗憾边界，并首次研究了p-均值遗憾框架，提出了通用的FairLinBandit算法框架。


<details>
  <summary>Details</summary>
Motivation: 现有线性赌博机中的Nash遗憾结果存在维度d相关的次优性，源于依赖限制性集中不等式的证明技术。需要解决这一开放问题，并扩展研究更一般的p-均值遗憾框架。

Method: 引入新的分析工具解决Nash遗憾次优问题；提出FairLinBandit通用算法框架，作为元算法适用于任何线性赌博机策略；具体实例化了Phased Elimination和Upper Confidence Bound两种算法。

Result: 实现了线性赌博机中Nash遗憾的最优边界；证明了两种算法在整个p值范围内都能实现次线性p-均值遗憾；在真实数据集生成的线性赌博机实例上，实验表明方法始终优于现有最优基线。

Conclusion: 本文解决了线性赌博机中Nash遗憾的开放问题，提出了更一般的p-均值遗憾框架，并开发了有效的算法框架，在公平性和效用目标之间提供了统一的视角。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [65] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 本文提出了一种端到端可学习的权重置换框架，通过可学习的置换成本矩阵、可微分二分图匹配求解器和稀疏优化损失函数，为结构化稀疏化提供更优的权重重排序方案。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏化已成为模型剪枝的流行技术，权重置换可以进一步改善剪枝后性能。然而，Transformer架构规模导致置换搜索空间呈指数增长，现有方法多依赖贪心或启发式算法，限制了重排序的有效性。

Method: 提出端到端可学习的置换框架：1）引入可学习的置换成本矩阵量化任意两个输入通道交换的成本；2）使用可微分二分图匹配求解器获取给定成本矩阵下的最优二元置换矩阵；3）设计稀疏优化损失函数直接优化置换算子。

Result: 在视觉和语言Transformer上进行了广泛验证，证明该方法在结构化稀疏化方面实现了最先进的置换结果。

Conclusion: 提出的端到端可学习置换框架能够有效解决大规模Transformer架构中的权重置换问题，为结构化稀疏化提供了更优的解决方案。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [66] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种用于离散扩散语言模型的解码引导水印方法，通过引导解掩码顺序来实现水印嵌入，无需显式重新加权模型概率。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型可以按任意顺序生成令牌，而实际模型对解掩码顺序表现出强烈敏感性，这为水印技术提供了新的通道。

Method: dgMARK引导解掩码顺序朝向满足二进制哈希诱导的简单奇偶约束的高奖励候选令牌位置，无需显式重新加权模型学习概率，可与常见解码策略（置信度、熵、边际排序）即插即用，并可增强为一步前瞻变体。

Result: 通过提升的奇偶匹配统计量检测水印，滑动窗口检测器确保在插入、删除、替换和改写等后编辑操作下的鲁棒性。

Conclusion: dgMARK为离散扩散语言模型提供了一种有效的解码引导水印方法，利用模型对解掩码顺序的敏感性实现水印嵌入，具有鲁棒性和实用性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [67] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: VaR-CPO算法：一种直接优化风险价值约束的样本高效保守方法，在可行环境中实现训练期间零约束违反


<details>
  <summary>Details</summary>
Motivation: 现有基线方法无法在训练过程中保持零约束违反，而安全探索对于实际应用至关重要。需要一种能够直接优化风险价值约束的保守方法。

Method: 1. 使用单边切比雪夫不等式处理风险价值约束的非可微性问题，基于成本回报的前两个矩获得可处理的替代约束；2. 扩展约束策略优化方法的信任域框架，提供策略改进和约束违反的严格最坏情况边界

Result: 1. 在可行环境中实现训练期间零约束违反；2. 基线方法无法保持这一关键特性；3. 提供训练过程中策略改进和约束违反的严格最坏情况边界

Conclusion: VaR-CPO是一种样本高效且保守的方法，能够直接优化风险价值约束，实现安全探索，为训练过程提供理论保证

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [68] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 该论文提出了一种新的异常根因分析方法，能够区分测量误差和机制变化两种不同类型的异常，并通过潜在干预模型实现可识别性。


<details>
  <summary>Details</summary>
Motivation: 现有异常根因分析方法忽略了异常可能源于两种根本不同的过程：测量误差（数据正常生成但记录错误）和机制变化（数据生成过程本身发生变化）。测量误差通常可以安全纠正，而机制异常需要仔细考虑。

Method: 定义了一个因果模型，通过将异常视为对潜在变量（"真实"变量）和观测变量（"测量"变量）的潜在干预来显式捕获两种异常类型。证明了这两种异常类型的可识别性，并提出了一种最大似然估计方法来实现这一理论。

Result: 实验表明，该方法在根因定位方面与最先进方法性能相当，同时还能准确分类异常类型，即使在因果DAG未知的情况下也能保持鲁棒性。

Conclusion: 该研究提出了一种能够区分测量误差和机制变化的异常根因分析框架，为实际应用中不同类型的异常提供了更精确的诊断和处理方法。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [69] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理导致LLM生成延迟高，因为LLM生成是高度顺序化的。需要一种方法来降低推理延迟，同时保持高准确率。

Method: 训练Divide-and-Conquer CoT模型，让模型作为导演识别可并行执行的子任务，然后生成工作线程执行这些子任务。采用多阶段强化学习算法，配合数据过滤策略，在降低最长路径长度的同时恢复准确率。

Result: 在AIME 2024和HMMT 2025等多个基准测试中，DC-CoT实现了与DeepScaleR-1.5B-Preview相似的准确率，同时将最长路径长度降低了35-40%。

Conclusion: DC-CoT方法能够有效降低长思维链推理的延迟，在保持准确性的同时显著提高推理效率，为低延迟并行推理提供了可行的解决方案。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [70] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 提出RLRR框架，将强化学习中的奖励从绝对数值转向相对排名，解决群体方法中奖励稀疏性和不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的强化学习方法（如GRPO）依赖绝对数值奖励存在固有局限：在可验证任务中相同群体评估导致监督稀疏，在开放任务中奖励模型分数范围不稳定影响优势估计

Method: 提出RLRR框架，将奖励塑造从绝对评分转向相对排名；引入Ranking Reward Model，这是一个专门为群体优化设计的列表式偏好模型，可直接生成相对排名

Result: 实验结果表明，RLRR在推理基准测试和开放生成任务中相比标准群体基线方法取得了持续的性能提升

Conclusion: 通过将原始评估转化为稳健的相对信号，RLRR有效缓解了信号稀疏性和奖励不稳定性问题，为基于群体的强化学习提供了更有效的优化范式

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [71] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow是一种基于B样条插值的流匹配算法，专门用于建模动态系统，相比现有方法能更好地处理不规则采样观测数据并学习高阶动态。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法不适合建模动态系统，因为它们使用线性插值构建条件路径，无法准确捕捉底层状态演化，特别是在从不规则采样观测中学习高阶动态时。构建满足多个观测边际约束的统一路径具有挑战性，因为简单的高阶多项式往往不稳定且振荡。

Method: SplineFlow利用B样条插值的平滑性和稳定性，通过B样条基函数联合建模跨观测的条件路径，以结构化方式学习复杂底层动态，同时确保满足多边际约束要求。

Result: 在各种复杂度的确定性和随机动态系统以及细胞轨迹推断任务上的综合实验表明，SplineFlow相比现有基线方法有显著改进。

Conclusion: SplineFlow是一种理论上有依据的流匹配算法，通过B样条插值有效解决了动态系统建模中的路径构建问题，在多个应用场景中表现出优越性能。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [72] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 论文提出CATTO校准感知训练目标，解决LLM置信度校准问题，在不损失任务准确性的情况下显著降低校准误差


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能做出准确的下一个token预测，但其置信度校准很差：高置信度预测经常错误，低置信度预测反而可能正确。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系。

Method: 提出CATTO（校准感知token级训练目标），将预测置信度与经验预测正确性对齐，可与原始偏好优化目标结合使用。同时引入Confidence@k，一种利用校准token概率进行贝叶斯最优输出token选择的测试时缩放机制。

Result: 与DPO相比，CATTO在分布内将ECE降低2.22%-7.61%，在分布外降低1.46%-10.44%；与最强DPO基线相比，在分布内降低0.22%-1.24%，在分布外降低1.23%-5.07%。置信度改进的同时不损失任务准确性，在五个数据集的多选题回答准确性上保持或略有提升。

Conclusion: CATTO能有效改善LLM的置信度校准问题，在不牺牲任务性能的前提下显著降低校准误差，为LLM的可靠部署提供了重要工具。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [73] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出DCR方法，通过精确推导非一致性分数的分布来构建高效的排序预测集，相比现有方法减少36%的预测集大小


<details>
  <summary>Details</summary>
Motivation: 现有基于保形预测的排序方法依赖非一致性分数的上界，导致预测集过于保守和庞大，需要更高效的量化不确定性方法

Method: 提出分布感知的保形排序（DCR），发现校准项的绝对排名服从负超几何分布，利用该分布推导非一致性分数分布并确定保形阈值

Result: DCR在保持有效覆盖率的同时，将平均预测集大小减少高达36%，理论保证在温和假设下优于基线方法

Conclusion: DCR通过精确建模非一致性分数分布，显著提高了排序预测集的效率，为实际应用中排序模型的安全部署提供了更好的不确定性量化方法

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [74] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种约束感知的数据扰动方法，解决生成模型在等式约束分布建模中的数学局限性问题


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学领域中经常遇到样本受等式约束的分布建模问题，存在固有的数学局限性，需要开发能够处理约束的分布修改方法

Method: 提出约束感知的数据扰动方法，通过扰动数据分布使其支持匹配环境空间维度，同时隐式地结合底层流形几何结构

Result: 该方法在多个代表性任务中能够一致地实现数据分布恢复和稳定采样，适用于扩散模型和归一化流模型

Conclusion: 提出的约束感知扰动方法是一种计算廉价、数学合理且高度灵活的分布修改技术，能够有效解决等式约束生成模型的已知缺陷

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [75] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种无监督技能分割和层次结构发现方法，使用基于语法的方法从未标记轨迹中分割技能并构建层次结构，在像素级环境中验证效果优于基线


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖动作标签、奖励或人工标注来分割轨迹为可重用技能，这限制了其适用性。需要一种完全无监督的方法来发现技能和层次结构

Method: 使用基于语法的方法从未标记轨迹中分割技能，并诱导出层次结构。该方法能捕捉低级行为及其组合成高级技能的过程

Result: 在Craftax和完整未修改版Minecraft等高维像素环境中评估，在技能分割、重用和层次质量指标上均优于现有基线，产生更具结构性和语义意义的层次

Conclusion: 发现的层次结构能加速和稳定下游强化学习任务的学习，证明了该方法的实用价值

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [76] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文研究了带参数噪声的随机线性赌博机问题，提出了针对一般动作集和特定ℓ_p球动作集的紧致遗憾上下界，并展示了简单的探索-利用算法可以达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究参数噪声模型下的随机线性赌博机问题，该模型中奖励函数包含参数噪声而非传统加性噪声，旨在理解这种噪声结构对遗憾界的影响。

Method: 分析参数噪声模型下的遗憾上下界：对于一般动作集，给出上界Õ(√(dT log(K/δ)σ²_max))和下界Ω̃(d√(Tσ²_max))；对于ℓ_p单位球(p≤2)及其对偶范数q，给出紧致界Θ̃(√(dTσ²_q))，并设计简单的探索-利用算法。

Result: 1) 一般动作集：当log(K)≈d时，遗憾界是紧致的；2) ℓ_p球动作集：遗憾界为Θ̃(√(dTσ²_q))，其中σ²_q≤4，显著优于经典加性噪声模型的d√(T)遗憾；3) 简单的探索-利用算法可以达到最优遗憾界。

Conclusion: 参数噪声模型下的随机线性赌博机问题具有与经典加性噪声模型不同的遗憾特性，对于ℓ_p球动作集可以获得更优的√(dT)量级遗憾，且简单算法即可达到最优性能。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [77] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MeshGraphNet-Transformer (MGN-T) 结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，解决了传统MGN在大规模高分辨率网格上长距离信息传播效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MeshGraphNets在大规模高分辨率网格上存在长距离信息传播效率低的问题，因为其基于迭代消息传递机制，需要深度消息传递堆栈或分层粗化网格来处理长程物理相互作用。

Method: 提出MeshGraphNet-Transformer架构，将物理注意力Transformer作为全局处理器，同时更新所有节点状态，显式保留节点和边属性。该架构直接捕获长程物理相互作用，无需深度消息传递堆栈或分层粗化网格。

Result: MGN-T成功处理工业级规模的冲击动力学网格，准确建模自接触、塑性和多变量输出（包括内部现象学塑性变量）。在经典基准测试中优于最先进方法，精度更高且保持实际效率，仅需竞争基线方法参数的一小部分。

Conclusion: MGN-T通过结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，有效解决了大规模高分辨率网格上的长距离信息传播问题，为工业级物理模拟提供了高效准确的解决方案。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [78] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 该研究通过分析墨西哥城交通数据与空气质量关系，开发了一种将彩色交通地图转换为同心环描述的创新方法，使用偏最小二乘回归预测污染水平，提供超本地化动态空气质量预报。


<details>
  <summary>Details</summary>
Motivation: 空气污染是全球大城市的慢性问题，交通是主要污染源。现有空气质量监测和预报在时空上较为粗糙，而实时交通数据通常更精细且公开可用。研究旨在利用精细交通数据提供超本地化、动态的空气质量预报。

Method: 1. 开发创新方法将简单的彩色编码交通地图转换为基于同心环的描述，改进交通状况表征；2. 使用偏最小二乘回归基于新定义的交通强度预测污染水平；3. 通过不同训练样本优化模型以获得最佳预测性能。

Result: 建立了交通强度与空气质量之间的关系模型，能够基于精细交通数据预测污染水平。模型经过优化达到最佳预测性能，并深入了解了污染物与交通之间的关系。

Conclusion: 该工作流程设计简单且适应性强，可推广到其他城市环境，为超本地化动态空气质量预报提供了实用方法，有助于改善城市空气质量管理。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [79] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 该论文研究了众包标注聚合中的公平性问题，分析了多数投票和贝叶斯聚合方法的公平性差距，提出了理论界限和收敛保证，并扩展了多类别公平性后处理算法。


<details>
  <summary>Details</summary>
Motivation: 获取可靠的真实标签通常成本高昂或不可行，因此众包和聚合噪声人工标注是常用方法。然而，聚合主观标签可能会放大个体偏见，特别是在敏感特征方面，引发公平性担忧。目前众包聚合中的公平性问题尚未得到充分探索，缺乏收敛保证，且仅有有限的ε-公平性后处理方法。

Method: 1) 在ε-公平性框架下分析多数投票和最优贝叶斯聚合方法的公平性；2) 在小众包规模下推导多数投票公平性差距的上界；3) 证明聚合共识的公平性差距在可解释条件下以指数速度收敛到真实标签的公平性差距；4) 将最先进的多类别公平性后处理算法从连续设置扩展到离散设置，以强制执行严格的人口统计平等约束。

Result: 1) 推导了多数投票公平性差距的上界；2) 证明了聚合共识公平性差距的指数收敛性；3) 扩展了多类别公平性后处理算法；4) 在合成和真实数据集上的实验验证了方法的有效性并支持理论见解。

Conclusion: 该研究填补了众包聚合公平性分析的空白，提供了理论保证和实用方法。通过分析聚合方法的公平性特性、证明收敛性，并扩展公平性后处理算法，为构建更公平的众包标注系统提供了理论基础和技术支持。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [80] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS系统通过动态聚焦计算于可解码token并实时淘汰不可解码token，解决了扩散大语言模型解码效率低的问题，实现了最高3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）作为自回归模型的替代方案，其部署受到高解码成本的限制。研究发现DLLM解码存在关键低效问题：虽然计算在token块上并行化，但每个扩散步骤只有少量token可解码，导致大部分计算浪费在不可解码token上。

Method: 基于注意力机制得出的token重要性与token解码概率之间的强相关性，提出FOCUS推理系统。该系统通过动态聚焦计算于可解码token，并实时淘汰不可解码token，增加有效批处理大小，缓解计算限制，实现可扩展的吞吐量。

Result: 实证评估显示，FOCUS相比生产级引擎LMDeploy实现了最高3.52倍的吞吐量提升，同时在多个基准测试中保持或提高了生成质量。

Conclusion: FOCUS系统通过优化DLLM解码过程中的计算效率，显著提升了扩散大语言模型的推理性能，为DLLMs的实际部署提供了有效的解决方案。

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [81] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出DDIS框架，通过解耦设计实现数据高效的PDE逆问题求解：无条件扩散学习系数先验，神经算子显式建模前向PDE指导，避免联合模型在数据稀缺时的指导衰减问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散后验采样的方法通过联合系数-解建模隐式表示物理，需要大量配对监督数据。在数据稀缺时，联合模型会出现指导衰减问题，影响逆问题求解效果。

Method: 提出解耦扩散逆求解器（DDIS）：1）无条件扩散模型学习系数先验分布；2）神经算子显式建模前向PDE物理约束；3）引入解耦退火后验采样（DAPS）避免扩散后验采样中的过平滑问题。

Result: 在稀疏观测下达到最先进性能：平均l2误差提升11%，谱误差提升54%；当数据限制在1%时，DDIS相比联合模型在l2误差上保持40%优势。理论证明DDIS避免了联合模型在训练数据稀缺时的指导衰减问题。

Conclusion: DDIS通过解耦设计实现了数据高效的物理感知生成框架，在PDE逆问题中表现出优越的数据效率和物理信息学习能力，特别是在数据稀缺场景下显著优于现有方法。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>
