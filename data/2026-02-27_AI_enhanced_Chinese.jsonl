{"id": "2602.22243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22243", "abs": "https://arxiv.org/abs/2602.22243", "authors": ["Jan Nausner", "Kilian Wohlleben", "Michael Hubner"], "title": "SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online", "comment": "8 pages, 5 figures; Submitted to the 2026 International Conference on Information Fusion (FUSION 2026). Under review", "summary": "The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.", "AI": {"tldr": "\u63d0\u51faSODA-CitrON\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u805a\u7c7b\u591a\u6a21\u6001\u4f20\u611f\u5668\u68c0\u6d4b\u6765\u89e3\u51b3\u9759\u6001\u7269\u4f53\u6570\u636e\u5173\u8054\u95ee\u9898\uff0c\u540c\u65f6\u4f30\u8ba1\u4f4d\u7f6e\u5e76\u7ef4\u62a4\u672a\u77e5\u6570\u91cf\u7269\u4f53\u7684\u6301\u7eed\u8ddf\u8e2a\u3002", "motivation": "\u9759\u6001\u7269\u4f53\u4ece\u5f02\u6784\u4f20\u611f\u5668\u68c0\u6d4b\u4e2d\u7684\u5728\u7ebf\u878d\u5408\u548c\u8ddf\u8e2a\u662f\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u7cfb\u7edf\u548c\u73af\u5883\u6620\u5c04\u4e2d\u7684\u57fa\u672c\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u5982JPDA\u9002\u5408\u52a8\u6001\u76ee\u6807\uff0c\u4f46\u5bf9\u95f4\u6b47\u89c2\u6d4b\u3001\u5177\u6709\u5f02\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u9759\u6001\u7269\u4f53\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd0\u52a8\u6a21\u578b\u5bf9\u6742\u6ce2\u7684\u533a\u5206\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faSODA-CitrON\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u805a\u7c7b\u591a\u6a21\u6001\u4f20\u611f\u5668\u68c0\u6d4b\u6765\u5904\u7406\u9759\u6001\u7269\u4f53\u6570\u636e\u5173\u8054\u3002\u65b9\u6cd5\u5b8c\u5168\u5728\u7ebf\u8fd0\u884c\uff0c\u5904\u7406\u65f6\u95f4\u4e0d\u76f8\u5173\u548c\u591a\u4f20\u611f\u5668\u6d4b\u91cf\uff0c\u5177\u6709\u6700\u574f\u60c5\u51b5\u5bf9\u6570\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002", "result": "\u5728\u4e0d\u540c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u4e0e\u8d1d\u53f6\u65af\u6ee4\u6ce2\u3001DBSTREAM\u805a\u7c7b\u548cJPDA\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793aSODA-CitrON\u5728\u7814\u7a76\u7684\u9759\u6001\u7269\u4f53\u6620\u5c04\u573a\u666f\u4e2d\uff0c\u5728F1\u5206\u6570\u3001\u4f4d\u7f6eRMSE\u3001MOTP\u548cMOTA\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "SODA-CitrON\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9759\u6001\u7269\u4f53\u6570\u636e\u5173\u8054\u95ee\u9898\uff0c\u5728\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5728\u7ebf\u8fd0\u884c\u3001\u5904\u7406\u5f02\u6784\u4f20\u611f\u5668\u6570\u636e\u3001\u53ef\u89e3\u91ca\u6027\u5f3a\u7b49\u4f18\u52bf\u3002"}}
{"id": "2602.22459", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22459", "abs": "https://arxiv.org/abs/2602.22459", "authors": ["Yicheng Chen", "Jinjie Li", "Haokun Liu", "Zicheng Luo", "Kotaro Kaneko", "Moju Zhao"], "title": "Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments", "comment": "Accepted to IEEE T-ASE; DOI pending", "summary": "Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u7684\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u5f15\u5bfc\u548c\u914d\u7f6e\u611f\u77e5\u7684\u5c40\u90e8\u4f18\u5316\uff0c\u76f4\u63a5\u4ece\u70b9\u4e91\u6570\u636e\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "motivation": "\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u5728\u98de\u884c\u4e2d\u53ef\u4ee5\u6539\u53d8\u5f62\u6001\uff0c\u9002\u5408\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5e94\u7528\uff0c\u4f46\u8f68\u8ff9\u89c4\u5212\u9762\u4e34\u9ad8\u7ef4\u3001\u7ea6\u675f\u4e30\u5bcc\u7684\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u907f\u969c\u3001\u8fd0\u52a8\u5b66\u9650\u5236\u548c\u52a8\u6001\u53ef\u884c\u6027\u3002", "method": "\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff1a1) \u5229\u7528\u673a\u5668\u4eba\u7684\u53cc\u91cd\u7279\u6027\uff08\u6839\u8fde\u6746\u4f5c\u4e3a\u521a\u4f53\u5f15\u5bfc\uff0c\u5173\u8282\u63d0\u4f9b\u7075\u6d3b\u6027\uff09\u751f\u6210\u5168\u5c40\u951a\u70b9\u72b6\u6001\uff0c\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u5904\u7406\u6bb5\uff1b2) \u8bbe\u8ba1\u5c40\u90e8\u8f68\u8ff9\u89c4\u5212\u5668\uff0c\u5e76\u884c\u4f18\u5316\u5404\u6bb5\uff0c\u4f7f\u7528\u53ef\u5fae\u76ee\u6807\u51fd\u6570\u548c\u7ea6\u675f\uff0c\u7cfb\u7edf\u786e\u4fdd\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u5e76\u907f\u514d\u63a7\u5236\u5947\u70b9\uff1b3) \u5b9e\u73b0\u76f4\u63a5\u5904\u7406\u70b9\u4e91\u6570\u636e\u7684\u5b8c\u6574\u7cfb\u7edf\uff0c\u65e0\u9700\u624b\u5de5\u969c\u788d\u7269\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u4f7f\u5173\u8282\u5f0f\u7a7a\u4e2d\u673a\u5668\u4eba\u80fd\u591f\u5229\u7528\u5176\u5f62\u6001\u5b9e\u73b0\u521a\u4f53\u673a\u5668\u4eba\u65e0\u6cd5\u5b8c\u6210\u7684\u673a\u52a8\u3002\u8fd9\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u6f14\u793a\u7684\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u59cb\u70b9\u4e91\u8f93\u5165\u751f\u6210\u8fde\u7eed\u3001\u65e0\u78b0\u649e\u3001\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u5206\u5c42\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6d6e\u52a8\u57fa\u591a\u8fde\u6746\u673a\u5668\u4eba\u5728\u9ad8\u7ef4\u7ea6\u675f\u7a7a\u95f4\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u96be\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5f15\u5bfc\u4e0e\u5c40\u90e8\u4f18\u5316\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u70b9\u4e91\u6570\u636e\u751f\u6210\u53ef\u884c\u8f68\u8ff9\u7684\u80fd\u529b\uff0c\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22461", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22461", "abs": "https://arxiv.org/abs/2602.22461", "authors": ["Daesol Cho", "Youngseok Jang", "Danfei Xu", "Sehoon Ha"], "title": "EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow", "comment": null, "summary": "Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.", "AI": {"tldr": "EgoAVFlow\uff1a\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\uff0c\u901a\u8fc7\u5171\u4eab3D\u6d41\u8868\u793a\u5b9e\u73b0\u51e0\u4f55\u53ef\u89c1\u6027\u63a8\u7406\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u5373\u53ef\u8fc1\u79fb", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u4eba\u7c7b\u89c6\u9891\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u64cd\u4f5c\u6f14\u793a\u6765\u6e90\uff0c\u4f46\u5c06\u5176\u90e8\u7f72\u5230\u673a\u5668\u4eba\u4e0a\u9700\u8981\u4e3b\u52a8\u89c6\u89d2\u63a7\u5236\u4ee5\u7ef4\u6301\u4efb\u52a1\u5173\u952e\u53ef\u89c1\u6027\uff0c\u800c\u4eba\u7c7b\u89c6\u89d2\u6a21\u4eff\u7531\u4e8e\u4eba\u7c7b\u7279\u5b9a\u5148\u9a8c\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u8fd9\u79cd\u63a7\u5236", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u52a8\u4f5c\u3001\u672a\u67653D\u6d41\u548c\u76f8\u673a\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u9884\u6d4b\u8fd0\u52a8\u548c\u573a\u666f\u51e0\u4f55\u8ba1\u7b97\u7684\u53ef\u89c1\u6027\u611f\u77e5\u5956\u52b1\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5956\u52b1\u6700\u5927\u5316\u53bb\u566a\u6765\u4f18\u5316\u89c6\u89d2", "result": "\u5728\u4e3b\u52a8\u53d8\u5316\u89c6\u89d2\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cEgoAVFlow\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6709\u6548\u7684\u53ef\u89c1\u6027\u7ef4\u62a4\u548c\u65e0\u9700\u673a\u5668\u4eba\u6f14\u793a\u7684\u9c81\u68d2\u64cd\u4f5c", "conclusion": "EgoAVFlow\u901a\u8fc7\u5b66\u4e60\u5171\u4eab3D\u6d41\u8868\u793a\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5230\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u548c\u4e3b\u52a8\u89c6\u89c9\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u4eba\u7c7b\u89c6\u89d2\u6a21\u4eff\u4e2d\u7684\u53ef\u89c1\u6027\u7ef4\u62a4\u95ee\u9898"}}
{"id": "2602.22474", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22474", "abs": "https://arxiv.org/abs/2602.22474", "authors": ["Jessie Yuan", "Yilin Wu", "Andrea Bajcsy"], "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering", "comment": null, "summary": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/", "AI": {"tldr": "UPS\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u5668\u6765\u533a\u5206\u81ea\u4fe1\u3001\u6a21\u7cca\u548c\u65e0\u80fd\u573a\u666f\uff0c\u9009\u62e9\u6267\u884c\u9ad8\u7f6e\u4fe1\u5ea6\u52a8\u4f5c\u3001\u6f84\u6e05\u4efb\u52a1\u6a21\u7cca\u6027\u6216\u8bf7\u6c42\u5e72\u9884\uff0c\u5e76\u5229\u7528\u6b8b\u5dee\u5b66\u4e60\u6301\u7eed\u6539\u8fdb\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\u901a\u5e38\u5047\u8bbe\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6821\u51c6\u826f\u597d\uff0c\u4f46\u5b9e\u9645\u4e0aVLM\u7684\u8fc7\u5ea6\u81ea\u4fe1\u5224\u65ad\u4f1a\u964d\u4f4e\u5f15\u5bfc\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u89c4\u8303\u5b58\u5728\u9ad8\u5c42\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u5b58\u5728\u4f4e\u5c42\u52a8\u4f5c\u4e0d\u786e\u5b9a\u6027\u6216\u65e0\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7b56\u7565\u5f15\u5bfc\u6846\u67b6\uff0c\u8054\u5408\u63a8\u7406\u8bed\u4e49\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u548c\u4f4e\u5c42\u52a8\u4f5c\u53ef\u884c\u6027\uff0c\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u89e3\u51b3\u7b56\u7565\uff1b\u5229\u7528\u5171\u5f62\u9884\u6d4b\u6821\u51c6VLM\u548c\u9884\u8bad\u7ec3\u57fa\u7840\u7b56\u7565\u7684\u7ec4\u5408\uff1b\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u5728\u90e8\u7f72\u671f\u95f4\u6536\u96c6\u5e72\u9884\u6765\u6539\u8fdb\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cUPS\u80fd\u591f\u533a\u5206\u81ea\u4fe1\u3001\u6a21\u7cca\u548c\u65e0\u80fd\u573a\u666f\uff0c\u76f8\u6bd4\u672a\u6821\u51c6\u57fa\u7ebf\u548c\u5148\u524d\u7684\u4eba\u673a\u95e8\u63a7\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u4e86\u6602\u8d35\u7684\u7528\u6237\u5e72\u9884\u3002", "conclusion": "UPS\u6846\u67b6\u901a\u8fc7\u6821\u51c6\u7684\u9a8c\u8bc1\u5668\u6709\u6548\u5904\u7406\u7b56\u7565\u5f15\u5bfc\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u4f46\u53ea\u9700\u6700\u5c11\u7684\u4eba\u7c7b\u53cd\u9988\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u884c\u4e3a\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2602.22514", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22514", "abs": "https://arxiv.org/abs/2602.22514", "authors": ["Xinyu Tan", "Ningwei Bai", "Harry Gardener", "Zhengyang Zhong", "Luoyu Zhang", "Liuhaichen Yang", "Zhekai Duan", "Monkgogi Galeitsiwe", "Zezhi Tang"], "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation", "comment": "7 pages, 2 figures", "summary": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.\n  In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.\n  Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u624b\u8bed\u9a71\u52a8\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u91c7\u7528\u65e0\u6ce8\u91ca\u8bcd\u8303\u5f0f\uff0c\u76f4\u63a5\u5c06\u89c6\u89c9\u624b\u8bed\u624b\u52bf\u6620\u5c04\u4e3a\u8bed\u4e49\u6307\u4ee4\uff0c\u5b9e\u73b0\u76f4\u89c2\u5305\u5bb9\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6ce8\u91ca\u8bcd\u4f5c\u4e3a\u4e2d\u95f4\u76d1\u7763\uff0c\u5b58\u5728\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u4fe1\u606f\u635f\u5931\u95ee\u9898\u3002\u9700\u8981\u66f4\u81ea\u7136\u3001\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002", "method": "\u91c7\u7528\u65e0\u6ce8\u91ca\u8bcd\u8303\u5f0f\uff0c\u76f4\u63a5\u6620\u5c04\u89c6\u89c9\u624b\u52bf\u5230\u8bed\u4e49\u6307\u4ee4\u3002\u4e13\u6ce8\u4e8e\u5b9e\u65f6\u5b57\u6bcd\u7ea7\u6307\u62fc\u63a5\u53e3\uff0c\u901a\u8fc7\u51e0\u4f55\u5f52\u4e00\u5316\u3001\u65f6\u95f4\u5e73\u6ed1\u548c\u8bcd\u6c47\u7cbe\u70bc\u5c06\u8fde\u7eed\u624b\u52bf\u6d41\u8f6c\u6362\u4e3a\u8fde\u8d2f\u8bed\u8a00\u547d\u4ee4\u3002\u652f\u6301\u672a\u6765\u96c6\u6210\u57fa\u4e8eTransformer\u7684\u65e0\u6ce8\u91ca\u8bcd\u624b\u8bed\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u4e0d\u540c\u4ea4\u4e92\u573a\u666f\u4e0b\u80fd\u591f\u6709\u6548\u5730\u5c06\u624b\u8bed\u6307\u4ee4\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u3001\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\u901a\u9053\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u63a8\u8fdb\u53ef\u8bbf\u95ee\u3001\u53ef\u6269\u5c55\u548c\u591a\u6a21\u6001\u5177\u8eab\u667a\u80fd\u7684\u6f5c\u529b\uff0c\u4e3a\u5305\u5bb9\u6027\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2602.22579", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22579", "abs": "https://arxiv.org/abs/2602.22579", "authors": ["Pablo Valle", "Sergio Segura", "Shaukat Ali", "Aitor Arrieta"], "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots", "comment": null, "summary": "Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u673a\u5668\u4eba\u4efb\u52a1\u63a7\u5236\u5668\u4e2d\u5e94\u7528\u8715\u53d8\u6d4b\u8bd5\u6765\u7f13\u89e3\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u548c\u4e94\u79cd\u8715\u53d8\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "VLA\u6a21\u578b\u9762\u4e34\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\u7684\u6311\u6218\uff1a\u4e00\u65b9\u9762\u9700\u8981\u4e3a\u6bcf\u4e2a\u6307\u4ee4\u63d0\u793a\u5b9a\u4e49\u6d4b\u8bd5\u9884\u8a00\uff0c\u8fd9\u590d\u6742\u4e14\u96be\u4ee5\u6cdb\u5316\uff1b\u53e6\u4e00\u65b9\u9762\u73b0\u6709\u6d4b\u8bd5\u9884\u8a00\u901a\u5e38\u53ea\u8bc4\u4f30\u4efb\u52a1\u6b63\u786e\u6027\uff0c\u800c\u65e0\u6cd5\u8bc4\u4f30\u4efb\u52a1\u6267\u884c\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u8715\u53d8\u6d4b\u8bd5\u662f\u5426\u80fd\u7f13\u89e3\u8fd9\u4e00\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u8715\u53d8\u5173\u7cfb\u6a21\u5f0f\u548c\u4e94\u79cd\u8715\u53d8\u5173\u7cfb\uff0c\u7528\u4e8e\u8bc4\u4f30\u6d4b\u8bd5\u8f93\u5165\u53d8\u5316\u5bf9VLA\u673a\u5668\u4eba\u539f\u59cb\u8f68\u8ff9\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u6d89\u53ca5\u4e2aVLA\u6a21\u578b\u30012\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u548c4\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b9e\u8bc1\u7814\u7a76\u6765\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u8715\u53d8\u6d4b\u8bd5\u80fd\u6709\u6548\u7f13\u89e3\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u81ea\u52a8\u68c0\u6d4b\u591a\u79cd\u7c7b\u578b\u7684\u6545\u969c\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u672a\u5b8c\u6210\u4efb\u52a1\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6240\u63d0\u51fa\u7684\u8715\u53d8\u5173\u7cfb\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4f7f\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u7684VLA\u6a21\u578b\u3001\u673a\u5668\u4eba\u548c\u4efb\u52a1\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u6d4b\u8bd5\u9884\u8a00\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5e94\u7528\u3002", "conclusion": "\u8715\u53d8\u6d4b\u8bd5\u80fd\u591f\u6709\u6548\u7f13\u89e3VLA\u673a\u5668\u4eba\u4efb\u52a1\u63a7\u5236\u5668\u4e2d\u7684\u6d4b\u8bd5\u9884\u8a00\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u8715\u53d8\u5173\u7cfb\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e3aVLA\u7cfb\u7edf\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.22628", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22628", "abs": "https://arxiv.org/abs/2602.22628", "authors": ["Michael F. Xu", "Enhui Zhao", "Yawen Zhang", "Joseph E. Michaelis", "Sarah Sebo", "Bilge Mutlu"], "title": "Designing Robots for Families: In-Situ Prototyping for Contextual Reminders on Family Routines", "comment": "Proceedings of the 21st ACM/IEEE International Conference on Human Robot Interaction (HRI 2026)", "summary": "Robots are increasingly entering the daily lives of families, yet their successful integration into domestic life remains a challenge. We explore family routines as a critical entry point for understanding how robots might find a sustainable role in everyday family settings. Together with each of the ten families, we co-designed robot interactions and behaviors, and a plan for the robot to support their chosen routines, accounting for contextual factors such as timing, participants, locations, and the activities in the environment. We then designed, prototyped, and deployed a mobile social robot as a four-day, in-home user study. Families welcomed the robot's reminders, with parents especially appreciating the offloading of some reminding tasks. At the same time, interviews revealed tensions around timing, authority, and family dynamics, highlighting the complexity of integrating robots into households beyond the immediate task of reminders. Based on these insights, we offer design implications for robot-facilitated contextual reminders and discuss broader considerations for designing robots for family settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u652f\u6301\u5bb6\u5ead\u65e5\u5e38\u60ef\u4f8b\u6765\u4fc3\u8fdb\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u53ef\u6301\u7eed\u878d\u5165\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u3001\u539f\u578b\u5f00\u53d1\u548c\u5bb6\u5ead\u90e8\u7f72\u7814\u7a76\uff0c\u53d1\u73b0\u673a\u5668\u4eba\u63d0\u9192\u529f\u80fd\u53d7\u5230\u6b22\u8fce\u4f46\u4e5f\u5f15\u53d1\u5bb6\u5ead\u52a8\u6001\u7684\u590d\u6742\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u8fdb\u5165\u5bb6\u5ead\u751f\u6d3b\uff0c\u4f46\u6210\u529f\u878d\u5165\u5bb6\u5ead\u65e5\u5e38\u4ecd\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5bb6\u5ead\u65e5\u5e38\u60ef\u4f8b\u4f5c\u4e3a\u7406\u89e3\u673a\u5668\u4eba\u5982\u4f55\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u627e\u5230\u53ef\u6301\u7eed\u89d2\u8272\u7684\u5173\u952e\u5207\u5165\u70b9\u3002", "method": "\u4e0e10\u4e2a\u5bb6\u5ead\u534f\u540c\u8bbe\u8ba1\u673a\u5668\u4eba\u4ea4\u4e92\u548c\u884c\u4e3a\uff0c\u5236\u5b9a\u652f\u6301\u4ed6\u4eec\u9009\u62e9\u60ef\u4f8b\u7684\u8ba1\u5212\uff0c\u8003\u8651\u65f6\u95f4\u3001\u53c2\u4e0e\u8005\u3001\u4f4d\u7f6e\u548c\u73af\u5883\u6d3b\u52a8\u7b49\u60c5\u5883\u56e0\u7d20\u3002\u7136\u540e\u8bbe\u8ba1\u3001\u539f\u578b\u5316\u5e76\u90e8\u7f72\u79fb\u52a8\u793e\u4ea4\u673a\u5668\u4eba\u8fdb\u884c\u4e3a\u671f\u56db\u5929\u7684\u5bb6\u5ead\u7528\u6237\u7814\u7a76\u3002", "result": "\u5bb6\u5ead\u6b22\u8fce\u673a\u5668\u4eba\u7684\u63d0\u9192\u529f\u80fd\uff0c\u7236\u6bcd\u5c24\u5176\u6b23\u8d4f\u67d0\u4e9b\u63d0\u9192\u4efb\u52a1\u7684\u8f6c\u79fb\u3002\u540c\u65f6\uff0c\u8bbf\u8c08\u63ed\u793a\u4e86\u56f4\u7ed5\u65f6\u95f4\u5b89\u6392\u3001\u6743\u5a01\u6027\u548c\u5bb6\u5ead\u52a8\u6001\u7684\u7d27\u5f20\u5173\u7cfb\uff0c\u7a81\u663e\u4e86\u5c06\u673a\u5668\u4eba\u6574\u5408\u5230\u5bb6\u5ead\u4e2d\u8d85\u8d8a\u7b80\u5355\u63d0\u9192\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u673a\u5668\u4eba\u4fc3\u8fdb\u60c5\u5883\u63d0\u9192\u7684\u8bbe\u8ba1\u542f\u793a\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e3a\u5bb6\u5ead\u73af\u5883\u8bbe\u8ba1\u673a\u5668\u4eba\u65f6\u66f4\u5e7f\u6cdb\u7684\u8003\u8651\u56e0\u7d20\uff0c\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u529f\u80fd\u652f\u6301\u6765\u7406\u89e3\u5bb6\u5ead\u52a8\u6001\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2602.22671", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.22671", "abs": "https://arxiv.org/abs/2602.22671", "authors": ["Georgios Papaioannou", "Barys Shyrokau"], "title": "Does the testing environment matter? Carsickness across on-road, test-track, and driving simulator conditions", "comment": null, "summary": "Carsickness has gained significant attention with the rise of automated vehicles, prompting extensive research across on-road, test-track, and driving simulator environments to understand its occurrence and develop mitigation strategies. However, the lack of carsickness standardization complicates comparisons across studies and environments. Previous works demonstrate measurement validity between two setups at most (e.g., on-road vs. driving simulator), leaving gaps in multi-environment comparisons. This study investigates the recreation of an on-road motion sickness exposure - previously replicated on a test track - using a motion-based driving simulator. Twenty-eight participants performed an eyes-off-road non-driving task while reporting motion sickness using the Misery Scale during the experiment and the Motion Sickness Assessment Questionnaire afterward. Psychological factors known to influence motion sickness were also assessed. The results present subjective and objective measurements for motion sickness across the considered environments. In this paper, acceleration measurements, objective metrics and subjective motion sickness ratings across environments are compared, highlighting key differences in sickness occurrence for simulator-based research validity. Significantly lower motion sickness scores are reported in the simulator compared to on-road and test-track conditions, due to its limited working envelope to reproduce low-frequency (<0.5 Hz) motions, which are the most provocative for motion sickness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u9053\u8def\u3001\u6d4b\u8bd5\u8dd1\u9053\u548c\u9a7e\u9a76\u6a21\u62df\u5668\u4e09\u79cd\u73af\u5883\u4e2d\u6655\u8f66\u75c7\u72b6\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u6a21\u62df\u5668\u7531\u4e8e\u65e0\u6cd5\u91cd\u73b0\u4f4e\u9891\u8fd0\u52a8\uff0c\u4ea7\u751f\u7684\u6655\u8f66\u75c7\u72b6\u663e\u8457\u4f4e\u4e8e\u771f\u5b9e\u73af\u5883\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u53d1\u5c55\uff0c\u6655\u8f66\u95ee\u9898\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6655\u8f66\u6d4b\u91cf\u65b9\u6cd5\u4f7f\u5f97\u4e0d\u540c\u7814\u7a76\u73af\u5883\u95f4\u7684\u6bd4\u8f83\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u7814\u7a76\u6700\u591a\u53ea\u6bd4\u8f83\u4e24\u79cd\u73af\u5883\uff0c\u7f3a\u4e4f\u591a\u73af\u5883\u5bf9\u6bd4\u3002", "method": "28\u540d\u53c2\u4e0e\u8005\u5728\u57fa\u4e8e\u8fd0\u52a8\u7684\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u6267\u884c\u975e\u9a7e\u9a76\u4efb\u52a1\uff0c\u4f7f\u7528\u75db\u82e6\u91cf\u8868\u5b9e\u65f6\u62a5\u544a\u6655\u8f66\u75c7\u72b6\uff0c\u5b9e\u9a8c\u540e\u4f7f\u7528\u6655\u8f66\u8bc4\u4f30\u95ee\u5377\u3002\u540c\u65f6\u6d4b\u91cf\u52a0\u901f\u5ea6\u3001\u5ba2\u89c2\u6307\u6807\u548c\u4e3b\u89c2\u6655\u8f66\u8bc4\u5206\uff0c\u5e76\u4e0e\u9053\u8def\u548c\u6d4b\u8bd5\u8dd1\u9053\u73af\u5883\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6a21\u62df\u5668\u73af\u5883\u4e2d\u7684\u6655\u8f66\u8bc4\u5206\u663e\u8457\u4f4e\u4e8e\u9053\u8def\u548c\u6d4b\u8bd5\u8dd1\u9053\u6761\u4ef6\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u62df\u5668\u7684\u5de5\u4f5c\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u91cd\u73b0\u4f4e\u9891\uff08<0.5 Hz\uff09\u8fd0\u52a8\uff0c\u800c\u8fd9\u4e9b\u4f4e\u9891\u8fd0\u52a8\u662f\u8bf1\u53d1\u6655\u8f66\u7684\u6700\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "\u9a7e\u9a76\u6a21\u62df\u5668\u5728\u91cd\u73b0\u771f\u5b9e\u6655\u8f66\u4f53\u9a8c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u9891\u8fd0\u52a8\u518d\u73b0\u65b9\u9762\u3002\u8fd9\u4e3a\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u6655\u8f66\u7814\u7a76\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u8003\u8651\u73af\u5883\u5dee\u5f02\u5bf9\u6655\u8f66\u7814\u7a76\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.22707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22707", "abs": "https://arxiv.org/abs/2602.22707", "authors": ["Kai Li", "Shengtao Zheng", "Linkun Xiu", "Yuze Sheng", "Xiao-Ping Zhang", "Dongyue Huang", "Xinlei Chen"], "title": "SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration", "comment": "This paper has been accepted for publication in the IEEE ROBOTICS AND AUTOMATION LETTERS (RA-L). Please cite the paper using appropriate formats", "summary": "Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system's robustness and low latency in practical scenarios.", "AI": {"tldr": "SCOPE\u6846\u67b6\u901a\u8fc7\u589e\u91cf\u6784\u5efa\u5b9e\u65f6\u9aa8\u67b6\u56fe\u548c\u9690\u5f0f\u672a\u77e5\u533a\u57df\u5206\u6790\uff0c\u5b9e\u73b0\u9ad8\u6548\u81ea\u4e3b\u63a2\u7d22\uff0c\u5728\u4fdd\u6301\u4e0e\u5168\u5c40\u89c4\u5212\u5668\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u5e73\u5747\u964d\u4f4e86.9%", "motivation": "\u5f53\u524d\u81ea\u4e3b\u63a2\u7d22\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u7684\u5168\u5c40\u4f18\u5316\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5ef6\u8fdf\u548c\u8f68\u8ff9\u632f\u8361\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u89c4\u5212\u3002", "method": "\u63d0\u51faSCOPE\u6846\u67b6\uff1a1) \u589e\u91cf\u6784\u5efa\u5b9e\u65f6\u9aa8\u67b6\u56fe\uff1b2) \u5f15\u5165\u9690\u5f0f\u672a\u77e5\u533a\u57df\u5206\u6790\u8fdb\u884c\u9ad8\u6548\u7a7a\u95f4\u63a8\u7406\uff1b3) \u91c7\u7528\u5206\u5c42\u6309\u9700\u89c4\u5212\u7b56\u7565\uff1a\u8fd1\u7aef\u89c4\u5212\u5668\u751f\u6210\u9ad8\u9891\u5c40\u90e8\u8f68\u8ff9\uff0c\u533a\u57df\u5e8f\u5217\u89c4\u5212\u5668\u4ec5\u5728\u5fc5\u8981\u65f6\u6fc0\u6d3b\u4ee5\u4f18\u5316\u5168\u5c40\u8bbf\u95ee\u987a\u5e8f\u3002", "result": "\u4eff\u771f\u5bf9\u6bd4\u8bc4\u4f30\u663e\u793a\uff0cSCOPE\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u5168\u5c40\u89c4\u5212\u5668\u76f8\u5f53\u7684\u63a2\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u5e73\u5747\u964d\u4f4e86.9%\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "SCOPE\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u9aa8\u67b6\u56fe\u6784\u5efa\u548c\u5206\u5c42\u89c4\u5212\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u4e3b\u63a2\u7d22\u4e2d\u7684\u8ba1\u7b97\u5ef6\u8fdf\u548c\u8f68\u8ff9\u632f\u8361\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u673a\u5668\u4eba\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.22714", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22714", "abs": "https://arxiv.org/abs/2602.22714", "authors": ["Philipp Schitz", "Paolo Mercorelli", "Johann C. Dauer"], "title": "Robust Helicopter Ship Deck Landing With Guaranteed Timing Using Shrinking-Horizon Model Predictive Control", "comment": "This version was submitted to the American Control Conference 2026 and has been accepted", "summary": "We present a runtime efficient algorithm for autonomous helicopter landings on moving ship decks based on Shrinking-Horizon Model Predictive Control (SHMPC). First, a suitable planning model capturing the relevant aspects of the full nonlinear helicopter dynamics is derived. Next, we use the SHMPC together with a touchdown controller stage to ensure a pre-specified maneuver time and an associated landing time window despite the presence of disturbances. A high disturbance rejection performance is achieved by designing an ancillary controller with disturbance feedback. Thus, given a target position and time, a safe landing with suitable terminal conditions is be guaranteed if the initial optimization problem is feasible. The efficacy of our approach is shown in simulation where all maneuvers achieve a high landing precision in strong winds while satisfying timing and operational constraints with maximum computation times in the millisecond range.", "AI": {"tldr": "\u57fa\u4e8e\u6536\u7f29\u65f6\u57df\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u76f4\u5347\u673a\u5728\u79fb\u52a8\u7532\u677f\u81ea\u4e3b\u7740\u9646\u7b97\u6cd5\uff0c\u5b9e\u73b0\u6beb\u79d2\u7ea7\u8ba1\u7b97\u65f6\u95f4\u7684\u9ad8\u7cbe\u5ea6\u7740\u9646", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u5f3a\u98ce\u7b49\u5e72\u6270\u6761\u4ef6\u4e0b\uff0c\u5728\u79fb\u52a8\u8239\u8236\u7532\u677f\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u6ee1\u8db3\u65f6\u95f4\u7a97\u53e3\u7ea6\u675f\u7684\u76f4\u5347\u673a\u81ea\u4e3b\u7740\u9646\u7b97\u6cd5", "method": "\u91c7\u7528\u6536\u7f29\u65f6\u57df\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08SHMPC\uff09\u7ed3\u5408\u7740\u9646\u63a7\u5236\u5668\uff0c\u8bbe\u8ba1\u5305\u542b\u6270\u52a8\u53cd\u9988\u7684\u8f85\u52a9\u63a7\u5236\u5668\uff0c\u4ece\u5168\u975e\u7ebf\u6027\u76f4\u5347\u673a\u52a8\u529b\u5b66\u4e2d\u63d0\u53d6\u5408\u9002\u7684\u89c4\u5212\u6a21\u578b", "result": "\u4eff\u771f\u663e\u793a\u6240\u6709\u673a\u52a8\u90fd\u80fd\u5728\u5f3a\u98ce\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u7740\u9646\u7cbe\u5ea6\uff0c\u6ee1\u8db3\u65f6\u95f4\u548c\u64cd\u4f5c\u7ea6\u675f\uff0c\u6700\u5927\u8ba1\u7b97\u65f6\u95f4\u5728\u6beb\u79d2\u8303\u56f4\u5185", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4fdd\u8bc1\u5728\u521d\u59cb\u4f18\u5316\u95ee\u9898\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed9\u5b9a\u76ee\u6807\u4f4d\u7f6e\u548c\u65f6\u95f4\uff0c\u5b9e\u73b0\u5177\u6709\u5408\u9002\u7ec8\u7aef\u6761\u4ef6\u7684\u5b89\u5168\u7740\u9646\uff0c\u5177\u6709\u9ad8\u6270\u52a8\u6291\u5236\u6027\u80fd"}}
{"id": "2602.22731", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22731", "abs": "https://arxiv.org/abs/2602.22731", "authors": ["Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n", "Nived Chebrolu", "Sruthi M. Krishna Moorthy", "Yifu Tao", "Fernando Torres", "Roberto Salguero-G\u00f3mez", "Maurice Fallon"], "title": "Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring", "comment": null, "summary": "Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408NeRF\u3001LiDAR SLAM\u548cGNSS\u7684\u4e09\u7ea7\u8868\u793a\u7ba1\u9053\uff0c\u5b9e\u73b0\u5e7c\u82d7\u7684\u91cd\u590d\u6027\u5730\u7406\u5b9a\u4f4d\u751f\u6001\u76d1\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edfTLS\u80fd\u66f4\u7cbe\u786e\u5730\u6355\u83b7\u7ec6\u5c3a\u5ea6\u7ed3\u6784\u7279\u5f81\u3002", "motivation": "\u73b0\u67093D\u4f20\u611f\u65b9\u6cd5\uff08TLS\u3001MLS\u3001\u4f20\u7edf\u6444\u5f71\u6d4b\u91cf\uff09\u96be\u4ee5\u6355\u83b7\u5e7c\u82d7\u7684\u7ec6\u5c3a\u5ea6\u7ed3\u6784\u7279\u5f81\uff0c\u5982\u7ec6\u679d\u3001\u5bc6\u53f6\uff0c\u4e14\u7f3a\u4e4f\u957f\u671f\u76d1\u6d4b\u6240\u9700\u7684\u5c3a\u5ea6\u4e00\u81f4\u6027\u3002\u9690\u5f0f3D\u91cd\u5efa\u65b9\u6cd5\uff08NeRF\u30013DGS\uff09\u867d\u524d\u666f\u597d\uff0c\u4f46\u65e0\u6cd5\u6062\u590d\u771f\u5b9e\u573a\u666f\u5c3a\u5ea6\u4e14\u7f3a\u4e4f\u7cbe\u786e\u5730\u7406\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u7ea7\u8868\u793a\u7ba1\u9053\uff1a(1) GNSS\u5b9e\u73b0\u7c97\u7565\u5730\u7403\u5750\u6807\u7cfb\u5b9a\u4f4d\uff1b(2) LiDAR SLAM\u63d0\u4f9b\u5398\u7c73\u7ea7\u7cbe\u786e\u5b9a\u4f4d\u4e0e\u91cd\u5efa\uff1b(3) NeRF\u5b9e\u73b0\u5bf9\u8c61\u4e2d\u5fc3\u7684\u5e7c\u82d7\u5bc6\u96c6\u91cd\u5efa\u3002\u878d\u5408\u4e09\u79cd\u6280\u672f\u5b9e\u73b0\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u957f\u671f\u76d1\u6d4b\u3002", "result": "\u5728\u82f1\u56fdWytham Woods\u548c\u82ac\u5170Evo\u68ee\u6797\u6837\u5730\u7684\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4TLS\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u6355\u83b7\u5e7c\u82d7\u7684\u830e\u9ad8\u3001\u5206\u679d\u6a21\u5f0f\u548c\u53f6\u6728\u6bd4\u3002\u6210\u529f\u6d4b\u91cf\u4e860.5-2\u7c73\u9ad8\u5e7c\u82d7\u7684\u7cbe\u786e\u830e\u9aa8\u67b6\u548c\u53f6\u7247\u5206\u5e03\u3002", "conclusion": "\u8be5\u7ba1\u9053\u4e3a\u751f\u6001\u5b66\u5bb6\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u548c\u5b9a\u91cf\u6570\u636e\uff0c\u652f\u6301\u68ee\u6797\u52a8\u6001\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u5e7c\u82d7\u7684\u91cd\u590d\u6027\u5730\u7406\u5b9a\u4f4d\u751f\u6001\u76d1\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c3a\u5ea6\u4e00\u81f4\u6027\u548c\u5730\u7406\u5b9a\u4f4d\u96be\u9898\u3002"}}
{"id": "2602.22733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22733", "abs": "https://arxiv.org/abs/2602.22733", "authors": ["Seongyong Kim", "Junhyeon Cho", "Kang-Won Lee", "Soo-Chul Lim"], "title": "Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera", "comment": null, "summary": "To catch a thrown object, a robot must be able to perceive the object's motion and generate control actions in a timely manner. Rather than explicitly estimating the object's 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object's position and scale, allowing the policy to reason about the object's motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u5f20RGB\u56fe\u50cf\u50cf\u7d20\u7ea7\u89c6\u89c9\u4fe1\u606f\u8bc6\u522b\u7269\u4f53\u8fd0\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u914d\u5408\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u6293\u53d6\u629b\u63b7\u7269\u4f53\u7684\u80fd\u529b", "motivation": "\u673a\u5668\u4eba\u6293\u53d6\u629b\u63b7\u7269\u4f53\u9700\u8981\u5b9e\u65f6\u611f\u77e5\u7269\u4f53\u8fd0\u52a8\u5e76\u751f\u6210\u63a7\u5236\u52a8\u4f5c\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u4f30\u8ba13D\u4f4d\u7f6e\uff0c\u8ba1\u7b97\u590d\u6742\u4e14\u8017\u65f6", "method": "1. \u4ece\u5355\u5f20RGB\u56fe\u50cf\u63d0\u53d6\u50cf\u7d20\u7ea7\u89c6\u89c9\u4fe1\u606f\u8bc6\u522b\u7269\u4f53\u8fd0\u52a8\uff1b2. \u8bbe\u8ba1\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u673a\u68b0\u81c2\u548c\u591a\u6307\u624b\u5b9a\u4e49\u4e3a\u5177\u6709\u4e0d\u540c\u89d2\u8272\u7684\u72ec\u7acb\u667a\u80fd\u4f53\uff1b3. \u4f7f\u7528\u89d2\u8272\u7279\u5b9a\u7684\u89c2\u6d4b\u548c\u5956\u52b1\u8fdb\u884c\u534f\u540c\u8bad\u7ec3", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u6210\u529f\u4ece\u4eff\u771f\u73af\u5883\u8fc1\u79fb\u5230\u73b0\u5b9e\u4e16\u754c\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u6293\u53d6\u629b\u63b7\u7269\u4f53\u7684\u80fd\u529b", "conclusion": "\u57fa\u4e8e\u50cf\u7d20\u7ea7\u89c6\u89c9\u4fe1\u606f\u7684\u7269\u4f53\u8fd0\u52a8\u8bc6\u522b\u65b9\u6cd5\u7ed3\u5408\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6293\u53d6\u629b\u63b7\u4efb\u52a1\uff0c\u5e76\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb"}}
{"id": "2602.22801", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22801", "abs": "https://arxiv.org/abs/2602.22801", "authors": ["Yinan Zheng", "Tianyi Tan", "Bin Huang", "Enguang Liu", "Ruiming Liang", "Jianlin Zhang", "Jianwei Cui", "Guang Chen", "Kun Ma", "Hangjun Ye", "Long Chen", "Ya-Qin Zhang", "Xianyuan Zhan", "Jingjing Liu"], "title": "Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving", "comment": null, "summary": "Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u57fa\u4e8e\u5927\u91cf\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86Hyper Diffusion Planner\u6846\u67b6\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8610\u500d\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u51b3\u7b56\u4efb\u52a1\u4e2d\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u4ecd\u5c40\u9650\u4e8e\u4eff\u771f\u6216\u5b9e\u9a8c\u5ba4\u73af\u5883\u3002\u6269\u6563\u6a21\u578b\u5728\u5927\u89c4\u6a21\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff08\u5982\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u5927\u91cf\u771f\u5b9e\u8f66\u8f86\u6570\u636e\u548c\u9053\u8def\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u7814\u7a76\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u5168\u9762\u63a7\u5236\u7814\u7a76\uff0c\u8bc6\u522b\u5f71\u54cd\u89c4\u5212\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff08\u6269\u6563\u635f\u5931\u7a7a\u95f4\u3001\u8f68\u8ff9\u8868\u793a\u3001\u6570\u636e\u7f29\u653e\uff09\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u63d0\u51fa\u7684Hyper Diffusion Planner\u6846\u67b6\u5728\u771f\u5b9e\u8f66\u8f86\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u57286\u4e2a\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\u548c200\u516c\u91cc\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8610\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u7ecf\u8fc7\u9002\u5f53\u8bbe\u8ba1\u548c\u8bad\u7ec3\u540e\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\uff0c\u5e94\u5bf9\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u3002"}}
{"id": "2602.22818", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22818", "abs": "https://arxiv.org/abs/2602.22818", "authors": ["Remi Cadene", "Simon Aliberts", "Francesco Capuano", "Michel Aractingi", "Adil Zouitine", "Pepijn Kooijmans", "Jade Choghari", "Martino Russi", "Caroline Pascal", "Steven Palma", "Mustafa Shukor", "Jess Moss", "Alexander Soare", "Dana Aubakirova", "Quentin Lhoest", "Quentin Gallou\u00e9dec", "Thomas Wolf"], "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning", "comment": "https://github.com/huggingface/lerobot", "summary": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.", "AI": {"tldr": "lerobot\u662f\u4e00\u4e2a\u5f00\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u5e93\uff0c\u6574\u5408\u4e86\u6574\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u6808\uff0c\u4ece\u5e95\u5c42\u7535\u673a\u63a7\u5236\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u3001\u5b58\u50a8\u548c\u6d41\u5f0f\u5904\u7406\uff0c\u65e8\u5728\u964d\u4f4e\u673a\u5668\u4eba\u5b66\u4e60\u95e8\u69db\u5e76\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u867d\u7136\u56e0\u673a\u5668\u5b66\u4e60\u6280\u672f\u3001\u5ec9\u4ef7\u9065\u64cd\u4f5c\u7cfb\u7edf\u548c\u5f00\u653e\u6570\u636e\u96c6\u800c\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u5f80\u5f80\u788e\u7247\u5316\u3001\u95ed\u6e90\u4e14\u53ea\u9488\u5bf9\u7279\u5b9a\u5b50\u7ec4\u4ef6\uff0c\u963b\u788d\u4e86\u9886\u57df\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u5e93lerobot\uff0c\u6574\u5408\u6574\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u6808\uff1a\u652f\u6301\u5e95\u5c42\u4e2d\u95f4\u4ef6\u901a\u4fe1\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7ba1\u7406\u3001\u591a\u79cd\u6700\u5148\u8fdb\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4ee5\u53ca\u901a\u7528\u5f02\u6b65\u63a8\u7406\u6808\u3002\u5f3a\u8c03\u53ef\u6269\u5c55\u5b66\u4e60\u800c\u975e\u624b\u5de5\u6280\u672f\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u771f\u5b9e\u673a\u5668\u4eba\u5e94\u7528\u3001\u652f\u6301\u53ef\u8bbf\u95ee\u786c\u4ef6\u5e73\u53f0\u3001\u53ef\u6269\u5c55\u5230\u65b0\u673a\u5668\u4eba\u5f62\u6001\u7684\u5e93\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u6700\u5148\u8fdb\u7684\u673a\u5668\u4eba\u5b66\u4e60\u5e73\u53f0\u3002", "conclusion": "lerobot\u901a\u8fc7\u5176\u53ef\u8bbf\u95ee\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5f00\u653e\u6027\uff0c\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5165\u95e8\u95e8\u69db\uff0c\u4e3a\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u3002"}}
{"id": "2602.22854", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22854", "abs": "https://arxiv.org/abs/2602.22854", "authors": ["Annika Delucchi", "Vincenzo Di Paola", "Andreas M\u00fcller", "and Matteo Zoppi"], "title": "Performance and Experimental Analysis of Strain-based Models for Continuum Robots", "comment": null, "summary": "Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5e94\u53d8\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u7684\u5f62\u72b6\u91cd\u5efa\u80fd\u529b\uff0c\u5e76\u4e0e\u51e0\u4f55\u53d8\u91cf\u5e94\u53d8\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u76ee\u524d\u5e94\u53d8\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u6027\u80fd\u8bc4\u4f30\u6807\u51c6\u3002\u968f\u7740\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u539f\u578b\u5f00\u53d1\u7684\u589e\u52a0\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u9002\u7528\u6027\u5e76\u8fdb\u884c\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u7814\u7a76\u5f62\u72b6\u91cd\u5efa\u80fd\u529b\uff0c\u5206\u6790\u5176\u5bf9\u5355\u4e2a\u548c\u7ec4\u5408\u53d8\u5f62\u6548\u5e94\u7684\u6355\u6349\u80fd\u529b\u3002\u901a\u8fc7\u673a\u68b0\u81c2\u79fb\u52a8\u7ec6\u6746\u672b\u7aef\u5e76\u5229\u7528\u6444\u50cf\u5934\u8bb0\u5f55\u914d\u7f6e\uff0c\u4f7f\u7528\u53cd\u5c04\u6807\u8bb0\u63d0\u53d6\u5f62\u72b6\uff0c\u65e0\u9700\u5e94\u53d8\u8ba1\u6216\u529b\u4f20\u611f\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u9884\u6d4b\u4e0e\u89c2\u6d4b\u5f62\u72b6\u543b\u5408\u826f\u597d\uff0c\u5e73\u5747\u8bef\u5dee\u4e3a\u6746\u957f\u76840.58%\uff0c\u6bcf\u4e2a\u914d\u7f6e\u7684\u5e73\u5747\u8ba1\u7b97\u65f6\u95f4\u4e3a0.32\u79d2\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5e94\u53d8\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u8bc4\u4f30\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u4e09\u9636\u5e94\u53d8\u63d2\u503c\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.22896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22896", "abs": "https://arxiv.org/abs/2602.22896", "authors": ["Zebin Yang", "Yijiahao Qi", "Tong Xie", "Bo Yu", "Shaoshan Liu", "Meng Li"], "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation", "comment": "DAC 2026", "summary": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.", "AI": {"tldr": "DySL-VLA\uff1a\u4e00\u79cd\u52a8\u6001\u8df3\u8fc7VLA\u6a21\u578b\u5c42\u7684\u6846\u67b6\uff0c\u6839\u636e\u52a8\u4f5c\u91cd\u8981\u6027\u81ea\u9002\u5e94\u8c03\u6574\u8ba1\u7b97\u91cf\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u963b\u788d\u4e86\u5b9e\u65f6\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u52a8\u4f5c\u91cd\u8981\u6027\u4e0d\u540c\uff1a\u5173\u952e\u6b65\u9aa4\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u800c\u4e0d\u91cd\u8981\u6b65\u9aa4\u53ef\u5bb9\u5fcd\u66f4\u591a\u53d8\u5316\u3002", "method": "\u63d0\u51faDySL-VLA\u6846\u67b6\uff0c\u5c06VLA\u5c42\u5206\u4e3a\u4fe1\u606f\u5c42\uff08\u59cb\u7ec8\u6267\u884c\uff09\u548c\u589e\u91cf\u5c42\uff08\u53ef\u9009\u62e9\u6027\u8df3\u8fc7\uff09\u3002\u8bbe\u8ba1\u4e86\u5148\u9a8c-\u540e\u9a8c\u8df3\u8fc7\u6307\u5bfc\u673a\u5236\u51b3\u5b9a\u4f55\u65f6\u542f\u52a8\u5c42\u8df3\u8fc7\uff0c\u5e76\u91c7\u7528\u8df3\u8fc7\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b97\u6cd5\u8bad\u7ec3\u6807\u51c6VLA\u8f6c\u6362\u4e3aDySL-VLA\u3002", "result": "\u5728Calvin\u6570\u636e\u96c6\u4e0a\uff0cDySL-VLA\u76f8\u6bd4Deer-VLA\u5b9e\u73b0\u4e862.1%\u7684\u6210\u529f\u957f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1185.7\u500d\uff0c\u5728\u540c\u7b49\u7cbe\u5ea6\u4e0b\u76f8\u6bd4RoboFlamingo\u57fa\u7ebf\u63d0\u4f9b3.75\u500d\u52a0\u901f\u3002", "conclusion": "DySL-VLA\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7VLA\u5c42\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
