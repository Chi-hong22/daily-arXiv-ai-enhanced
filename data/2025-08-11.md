<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.LG](#cs.LG) [Total: 48]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.HC](#cs.HC) [Total: 15]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为ResPA的新型攻击方法，通过残差梯度作为扰动方向，提升对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有迁移攻击方法忽略扰动方向的影响，导致迁移性受限。

Method: ResPA利用残差梯度作为扰动方向，结合指数移动平均的历史梯度信息，引导对抗样本朝向损失函数的平坦区域。

Result: 实验表明，ResPA的迁移性优于现有典型迁移攻击方法，且与输入变换方法结合可进一步提升效果。

Conclusion: ResPA通过优化扰动方向，显著提高了对抗样本的迁移性。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出了一种广义少样本OOD检测框架（GOOD），通过引入通用知识模型（GKM）提升泛化能力，解决了现有方法因数据不足导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有少样本OOD检测方法因数据有限导致泛化能力不足，性能不稳定。

Method: 提出GOOD框架，结合GKM和知识动态嵌入（KDE）机制，动态调整输出分布以平衡通用性与特异性。

Result: 实验证明GOOD在真实OOD基准测试中表现优越。

Conclusion: GOOD通过GS平衡和KDE机制显著提升了少样本OOD检测的泛化能力。

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [3] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一个基于视觉语言模型的3D场景生成框架，支持交互式编辑，能生成多样且语义准确的场景。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景生成依赖大量人工，自动化方法难以支持开放域和灵活编辑，因此需要一种从文本直接生成3D世界的解决方案。

Method: 利用视觉语言模型解析场景需求，结合3D生成模型生成高质量资产，并通过空间约束实现语义和物理合理的布局。

Result: HOLODECK 2.0生成的场景在语义和视觉质量上优于基线，支持室内和开放域，并可通过人类反馈灵活编辑。

Conclusion: HOLODECK 2.0为3D场景生成提供了高效且灵活的解决方案，尤其在游戏建模中具有应用潜力。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [4] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide是一种新方法，通过动态推理机制UnGuidance，结合LoRA适配器，实现对扩散模型中特定知识的精确去除，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型可能被滥用生成有害内容，亟需一种有效的机器遗忘方法，去除特定知识而不影响整体性能。

Method: UnGuide引入UnGuidance机制，动态调整引导规模，结合LoRA适配器实现选择性遗忘。

Result: UnGuide在概念去除任务中表现优于现有LoRA方法，同时保持图像保真度。

Conclusion: UnGuide为扩散模型的知识去除提供了一种高效且可控的解决方案。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [5] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net是一种新框架，通过变形联合LV-海马模板网格从脑MRI生成个性化3D LV网格，解决了形状变异性和分割困难问题，并提高了形状统计准确性。


<details>
  <summary>Details</summary>
Motivation: 侧脑室（LV）形状分析作为神经系统疾病的生物标志物具有潜力，但个体间形状变异大和MRI分辨率限制导致分割困难。

Method: LV-Net通过变形联合LV-海马模板网格生成个性化3D LV网格，利用模板的解剖关系减少分割伪影并增强点对应性。

Result: LV-Net在分割不完美情况下仍实现高重建精度，并提供更可靠的形状描述符，应用于阿尔茨海默病分析发现显著相关的LV子区域。

Conclusion: LV-Net在LV形状分析中表现出色，为疾病研究提供了可靠工具。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [6] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 提出了一种基于部分卷积的风格迁移网络，专注于对图像特定区域进行风格迁移，避免了传统方法在区域选择上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统方法对整个图像进行风格迁移后通过掩码处理特定区域，但会导致风格特征捕捉不准确。

Method: 采用部分卷积网络和内部混合技术，精确应用于感兴趣区域并处理选择不完美的问题。

Result: 在SA-1B数据集上验证了视觉和量化上的改进。

Conclusion: 该方法显著提升了区域风格迁移的准确性和视觉效果。

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [7] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2是一个加速的3D医学图像合成框架，通过整合rectified flow实现快速高质量生成，并引入区域对比损失提升条件一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在医学图像合成中的泛化性差、推理速度慢和输入条件对齐弱的问题。

Method: 整合rectified flow加速生成，引入区域对比损失增强条件一致性。

Result: MAISI-v2实现33倍加速，图像质量达到SOTA，并验证了合成图像可用于数据增强。

Conclusion: MAISI-v2在速度和条件一致性上显著改进，为医学图像合成提供了高效解决方案。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [8] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于预训练MRI变换器的少样本部署框架，通过MAE策略在大规模脑MRI数据上训练，实现了高效的任务泛化。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中标注数据稀缺的问题，提升变换器模型在真实场景中的适用性。

Method: 使用MAE预训练策略在大规模脑MRI数据集上训练，结合轻量级线性头或混合架构MAE-FUnet进行高/低层次任务。

Result: 在MRI序列识别和分割任务中达到最优性能，尤其在数据有限条件下表现突出。

Conclusion: 该框架高效、稳定且可扩展，适用于低资源临床环境和神经影像应用。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [9] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 提出了一种无需重建或优化的3D高斯样条风格迁移方法，通过生成样条表示的隐式表面图结构，实现快速风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要重建或优化样条，效率低且依赖额外训练。

Method: 生成样条表示的隐式表面图结构，使用前馈表面风格化方法并插值回样条。

Result: 支持任意风格图像和3D高斯样条，速度在2分钟内完成。

Conclusion: 该方法高效且无需额外训练，适用于消费级硬件。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [10] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN是一种改进的NeRF框架，能够处理多缩放图像集，提升工业检测中的细节捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法在工业检测中无法捕捉微米级细节，且多缩放图像会破坏多视角一致性。

Method: MZEN引入可学习的缩放标量，并采用新的姿态策略：先解决广角图像，再通过缩放一致的裁剪匹配处理缩放图像。

Result: 在多个场景中，MZEN显著提升了PSNR、SSIM，并降低了LPIPS。

Conclusion: MZEN扩展了NeRF在工业检测中的应用，同时保持了全局精度和微米级细节。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [11] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2是一种改进SAM2在手术视频中可提示视频对象分割和跟踪的框架，通过多时间尺度视频采样增强和内存分割修剪机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 手术视频分析中，复杂运动动态和内存冗余限制了SAM2的应用，需要更高效的解决方案。

Method: 引入多时间尺度视频采样增强和内存分割修剪机制，优化运动动态和内存效率。

Result: 在EndoVis2017和EndoVis2018数据集上分别达到95.24和86.73的Dice分数，优于现有方法。

Conclusion: TSMS-SAM2在复杂手术场景中表现出高效、鲁棒的分割能力，具有实际应用潜力。

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [12] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 提出了一种名为TCA的轻量级、无需微调的策略，通过利用帧间时间一致性优化视频分割中的令牌聚类，显著减少计算量并保持细节。


<details>
  <summary>Details</summary>
Motivation: Swin Transformer在视频分割中计算成本高，传统令牌剪枝方法不适用，现有训练无关的令牌聚类方法未能利用时间冗余。

Method: 提出TCA策略，利用时间相关性优化令牌聚类，避免冗余令牌的随机丢弃。

Result: 在多个数据集上验证，TCA显著提升了现有聚类方法的精度与速度平衡。

Conclusion: TCA在自然和特定领域视频中均表现出色，是一种高效且通用的优化方法。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [13] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉-语言框架的方法，通过自然语言预测驾驶员视觉注意力变化，结合少样本和零样本学习，性能优于通用视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态图像的注意力分配，缺乏对动态注意力变化的建模，且缺乏可解释性。

Method: 利用BDD-A数据集的高质量标注，通过人类反馈优化，微调LLaVA模型，结合低层线索和上下文信息（如路线语义、风险预测）。

Result: 模型在注意力转移检测和可解释性上优于通用视觉语言模型，支持少样本和零样本学习。

Conclusion: 该方法为自动驾驶中的可解释AI提供了新方向，支持行为预测、人机协作等下游任务。

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [14] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出一种多视角相机方法用于视线目标估计（GTE），通过整合不同视角信息提升准确性和适用性，解决了单视角方法中的遮挡、目标模糊等问题。


<details>
  <summary>Details</summary>
Motivation: 单视角GTE方法存在遮挡、目标模糊和视野外目标等局限性，需通过多视角信息提升性能。

Method: 采用双视角输入，结合头部信息聚合（HIA）、基于不确定性的视线选择（UGS）和基于极线的场景注意力（ESA）模块。

Result: 显著优于单视角基线方法，尤其在第二视角清晰时；还能仅用第二视角估计第一视角的视线目标。

Conclusion: 多视角方法有效提升GTE性能，并提供了多视角数据集支持未来研究。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [15] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA提出了一种高效的测试时适应方法，通过递归更新模块和自适应集成模块，提升预训练视觉语言模型在分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的TTA方法仅存储高置信度样本，限制了决策边界，ETTA旨在通过动态更新和集成所有测试样本解决这一问题。

Method: ETTA引入递归更新模块动态整合所有测试样本，并采用自适应集成模块减少对提示的依赖，结合两者优势。

Result: 实验表明，ETTA在计算复杂度和准确性上均优于现有TTA模型。

Conclusion: ETTA为高效、有效的测试时适应设定了新标准。

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [16] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch是一种无监督的深度图像拼接框架，通过双分支架构和虚拟最优平面概念，显著提升了场景鲁棒性和内容自然性。


<details>
  <summary>Details</summary>
Motivation: 现有图像拼接方法在内容对齐和结构保持之间存在矛盾，且缺乏对多样化场景的泛化能力。

Method: 采用双分支架构（预训练分支和可学习分支）捕获特征，并通过虚拟最优平面估计解决对齐与结构冲突。

Result: 在多个数据集上表现优异，尤其在场景鲁棒性和内容自然性方面超越现有方法。

Conclusion: RopStitch通过创新架构和优化方案，实现了高效且自然的图像拼接。

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [17] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: 论文探讨了如何利用神经场模型在移动设备上高效处理复杂几何和光照效果，无需复杂预处理或标注数据。


<details>
  <summary>Details</summary>
Motivation: 移动设备已成为多功能计算成像平台，结合神经场模型的潜力，探索其在移动摄影中的应用。

Method: 设计自正则化的神经场模型，通过随机梯度下降直接拟合智能手机原始数据。

Result: 方法在深度估计、图层分离和图像拼接等任务中优于现有技术。

Conclusion: 神经场模型为移动计算成像提供了高效、无需复杂先验的解决方案。

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [18] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 论文探讨了计算机视觉在建筑进度监测中的应用，评估了SAM和Mask3D两种3D分割方法在复杂建筑环境中的表现，并指出当前方法在户外场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统建筑监测方法效率低且难以适应复杂多变的建筑环境，需要更高效的自动化解决方案。

Method: 通过比较SAM和Mask3D两种3D分割方法在室内和户外建筑场景中的表现，评估其适应性和性能。

Result: 研究发现当前分割方法缺乏针对户外场景的基准，SAM和Mask3D在复杂环境中表现有限。

Conclusion: 研究强调了定制化分割工作流的必要性，以提升建筑监测的自动化和精确性。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [19] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD提出了一种自监督框架，通过3D高斯扩散解决单图像法线估计中的几何不一致性和数据依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据驱动统计先验，缺乏光-表面交互的显式建模，导致多视角法线方向冲突和梯度不连续。

Method: 结合物理驱动的光交互建模和可微分渲染重投影策略，构建3D高斯重参数化模型和跨域特征融合模块。

Result: 在Google Scanned Objects数据集上定量评估显示，SINGAD在多项指标上优于现有方法。

Conclusion: SINGAD通过自监督优化和几何误差传播，解决了多视角几何不一致性和数据依赖问题。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [20] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1是一个统一框架，通过将预训练的多模态大语言模型（MLLMs）与扩散模型结合，利用CLIP图像嵌入作为潜在变量，实现高效的高保真可控图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练成本上较高，且未充分利用预训练模型的潜力。Bifrost-1旨在解决这一问题，同时保留MLLMs的多模态推理能力。

Method: 使用patch级CLIP图像嵌入作为潜在变量，通过轻量化的ControlNet适配扩散模型，并初始化MLLM的视觉生成分支。

Result: Bifrost-1在视觉保真度和多模态理解上表现优异，且训练计算成本显著降低。

Conclusion: 该框架为高效整合MLLMs和扩散模型提供了可行方案，同时支持高质量的图像生成。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [21] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG框架通过几何特征聚合和视觉语言模型动态耦合几何基元与功能可供性，解决了机器人操作中语义与几何特征脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 机器人操作中高层任务语义与低层几何特征的脱节问题限制了视觉语言模型的应用，需要动态语义-可供性关系捕捉。

Method: 提出PASG框架，包括自动基元提取、VLM驱动的语义锚定和空间语义推理基准。

Result: PASG在多样化机器人操作任务中表现优异，性能接近人工标注。

Conclusion: PASG实现了更细粒度的语义-可供性理解，为机器人操作中几何基元与任务语义的统一提供了新范式。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [22] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene提出了一种统一框架，解决了3D场景重建与4D人体动画无缝集成的挑战，包括人体位置与比例、光照风格对齐及相机轨迹重建。


<details>
  <summary>Details</summary>
Motivation: 3D场景重建与4D人体动画的集成存在人体位置与比例不匹配、光照风格不一致及相机轨迹重建困难等问题。

Method: 设计了精确放置模块、无训练风格对齐方法及联合后重建方法，分别解决人体位置、光照风格和相机轨迹问题。

Result: 实验表明，AnimateScene能生成高几何细节和时空一致性的动态场景视频。

Conclusion: AnimateScene通过统一框架有效解决了3D场景与4D人体动画的集成问题，实现了视觉上连贯的结果。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [23] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 提出了一种基于能量的测试时适应方法（ETA），用于调整预训练的深度补全模型在目标数据上的预测，以应对协变量偏移问题。


<details>
  <summary>Details</summary>
Motivation: 深度补全模型在源数据上训练后，在目标数据上可能因协变量偏移而产生错误预测。

Method: 利用对抗扰动探索数据空间，训练能量模型评估预测的分布情况，并在测试时调整模型参数以最小化能量。

Result: 在三个室内和三个室外数据集上，ETA平均优于现有方法，室内提升10.23%，室外提升6.94%。

Conclusion: ETA通过测试时适应有效提升了深度补全模型在目标数据上的性能。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [24] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 提出了一种高效视频计算机视觉系统，通过去除图像信号处理器和采用快速块匹配运动估计算法，显著减少计算开销，同时引入上下文感知块细化网络和帧选择策略以平衡精度与效率。


<details>
  <summary>Details</summary>
Motivation: 视频计算机视觉系统因时间冗余和前端计算开销而效率低下，现有方法未能充分解决这些问题。

Method: 1. 去除图像信号处理器，直接输入Bayer格式数据；2. 提出基于快速块匹配的运动估计算法，并引入MV细化模块；3. 使用上下文感知块细化网络修正误差；4. 采用帧选择策略平衡精度与效率。

Result: 在多个视频计算机视觉任务中，该方法实现了显著加速，仅带来轻微性能损失。

Conclusion: 所提系统通过优化前端计算和运动估计，有效提升了视频计算机视觉的效率，同时保持了较高的准确性。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [25] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 提出了一种多模态情感识别框架，通过预训练模型提取视觉、音频和文本特征，并采用双分支视觉编码器和上下文增强文本方法，结合自注意力机制和残差连接进行特征融合，显著提升了MER2025-SEMI数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互中的情感识别能力，解决数据稀缺问题。

Method: 利用预训练模型提取多模态特征，设计双分支视觉编码器和上下文增强文本方法，采用自注意力机制和残差连接进行特征融合，并使用多源标注策略优化训练集标签。

Result: 在MER2025-SEMI数据集上，加权F-score达到87.49%，显著优于基线模型的78.63%。

Conclusion: 所提出的框架在多模态情感识别任务中表现出色，验证了其有效性。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [26] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: 论文提出MakeupQuad数据集和EvoMakeup框架，解决现有面部化妆编辑方法在细节和身份保留上的不足，支持多任务化妆编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法因缺乏结构化配对数据，导致化妆细节粗糙且难以平衡身份和化妆保真度。

Method: 提出MakeupQuad数据集和EvoMakeup框架，通过多阶段蒸馏迭代提升数据和模型质量。

Result: EvoMakeup在真实场景中表现优异，支持高保真、可控的多任务化妆编辑。

Conclusion: 该方法在化妆保真度和身份保留上取得平衡，优于现有方法。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [27] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: MathReal是一个针对真实教育场景的多模态数学推理数据集，挑战现有MLLM模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未涵盖真实K-12教育场景中的图像输入，MathReal填补了这一空白。

Method: 构建包含2000个真实场景数学问题的数据集，分类图像问题并设计六种实验设置评估MLLM性能。

Result: 现有MLLM在真实教育场景中的表现显著受限，分析揭示了其识别、理解和推理能力的不足。

Conclusion: MathReal为MLLM在真实场景中的改进提供了方向和依据。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [28] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅（3DGS）的管道，通过生成额外训练视图和信息增益驱动的虚拟相机放置策略，提升新视角合成（NVS）的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在偏离训练轨迹的视角渲染时存在伪影和缺失区域，限制了场景的无缝探索。

Method: 采用信息增益驱动的虚拟相机放置策略最大化场景覆盖，并利用视频扩散先验优化渲染结果，最后通过增强视图微调3D高斯。

Result: 实验表明，该方法优于现有3DGS方法，实现了高质量、无伪影的任意视角渲染。

Conclusion: 提出的方法显著提升了3DGS的重建质量，适用于挑战性场景探索。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [29] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive提出了一种基于门控Mamba融合的端到端自动驾驶框架，通过几何增强的LiDAR表示和高效的空间感知状态空间模型，克服了传统Transformer的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的自动驾驶方法依赖Transformer融合，存在计算复杂度高和缺乏空间先验的问题，限制了高分辨率特征的使用和对BEV表示的有效建模。

Method: 1. 采用几何增强的LiDAR表示（形状描述符和统计特征）；2. 提出分层门控Mamba融合架构（GM-Fusion），用状态空间模型替代Transformer。

Result: 在NAVSIM基准测试中，GMF-Drive性能显著优于DiffusionDrive，达到新SOTA。

Conclusion: 任务特定的状态空间模型在自动驾驶中性能和效率上优于通用Transformer。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [30] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 利用深度学习结合流式成像显微镜分析亚可见颗粒，但数据稀缺和类别不平衡问题限制了多分类器的效果。本文提出一种扩散模型生成高质量图像以增强数据集，实验证明其有效性，并公开了模型和工具。


<details>
  <summary>Details</summary>
Motivation: 解决亚可见颗粒分析中数据稀缺和类别不平衡的问题，尤其是硅油和气泡等罕见颗粒类型。

Method: 开发扩散模型生成高保真图像，用于增强训练数据集，并通过大规模实验验证其效果。

Result: 生成的图像与真实颗粒图像相似，实验表明该方法能提升分类性能且无明显副作用。

Conclusion: 扩散模型能有效解决数据不平衡问题，公开的模型和工具有助于未来研究。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [31] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: 提出了一种基于地板特征和图卷积网络（GCN）的机器人定位框架，显著提高了定位精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统定位方法（如激光雷达或二维码）在复杂环境中存在可扩展性和适应性不足的问题。

Method: 利用图表示地板特征，并通过GCN进行定位，避免了复杂的滤波过程。

Result: 定位误差仅为0.64cm，且能实时解决机器人绑架问题。

Conclusion: 该方法为复杂环境中的机器人导航提供了新的可能性。

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [32] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum是一个统一网络，用于细粒度的人体解析（身体部位和服装），通过改进的图像到纹理扩散模型实现更精确的分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用固定掩码类别或单一人物类别，无法区分细粒度服装或详细身体部位，需要更精确的解析方法。

Method: 利用改进的图像到纹理（I2Tx）扩散模型提取特征，并通过提示引导生成语义有效的掩码。

Result: Spectrum在跨数据集实验中表现优于基线方法，特别是在提示分割任务中。

Conclusion: Spectrum通过结合扩散模型和细粒度解析，显著提升了人体和服装分割的精度。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [33] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度的数据增强方法Depth-Jitter，通过模拟自然深度变化提升模型在深度敏感环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法忽略了深度感知变换，限制了模型在真实世界深度变化中的鲁棒性。

Method: Depth-Jitter通过自适应深度偏移和深度方差阈值生成合成深度扰动，同时保持结构完整性。

Result: 在FathomNet和UTDAC2020数据集上验证了Depth-Jitter对模型稳定性的提升，尽管绝对性能不一定优于传统方法，但在深度敏感环境中表现更稳定。

Conclusion: Depth-Jitter展示了深度感知增强在真实应用中的潜力，为深度学习策略的进一步研究奠定了基础。

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [34] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit是一种基于RectifiedFlow框架的快速文本引导图像编辑方法，通过PerRFI反转策略和Inversion Latent Injection再生方法，实现高效且一致的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在速度和编辑效果上存在不足，InstantEdit旨在通过RectifiedFlow框架和新技术实现快速且高质量的编辑。

Method: 结合PerRFI反转策略、Inversion Latent Injection再生方法、Disentangled Prompt Guidance技术和Canny-conditioned ControlNet，优化编辑过程。

Result: 在PIE数据集上，InstantEdit在速度和编辑质量上均优于现有方法。

Conclusion: InstantEdit通过创新技术实现了快速且高质量的文本引导图像编辑，为相关领域提供了新思路。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [35] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种半监督学习的混合专家（MoE）情感识别系统，整合多模态输入和伪标签策略，在MER2025-SEMI挑战中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中的情感识别问题，利用多模态数据和未标记数据提升模型性能。

Method: 结合多模态输入（如视觉语言模型和动作单元信息），采用共识伪标签策略和两阶段训练，最终通过多专家投票和规则重排优化预测。

Result: 在MER2025-SEMI测试集上F1得分为0.8772，排名第二。

Conclusion: 提出的框架有效整合多模态数据和半监督学习策略，显著提升了情感识别性能。

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [36] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM通过频域压缩视觉表示，显著减少计算开销和推理延迟，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中大量视觉令牌增加了上下文长度，导致高计算开销和延迟，现有方法往往牺牲性能或引入额外成本。

Method: 利用视觉特征在低频分量集中的特性，通过二维离散余弦变换（DCT）应用低通滤波器压缩视觉表示。

Result: 在多个基准测试中表现优异，推理FLOPs减少83.8%，生成速度提升31.2%。

Conclusion: Fourier-VLM在高效性和实用性上表现突出，适用于多种架构。

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [37] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于自回归图像生成的Next Editing-token Prediction（NEP）方法，仅对需要编辑的图像区域进行重新生成，避免了不必要的计算成本和编辑偏差。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像编辑方法通常生成整个目标图像，导致计算成本高且编辑质量受限。

Method: 通过预训练任意顺序自回归文本到图像（T2I）模型，实现零样本图像编辑，并适应NEP方法。

Result: 在广泛使用的图像编辑基准测试中达到新最优性能，并支持零样本迭代优化。

Conclusion: NEP方法显著提升了图像编辑的效率和质量，支持灵活的区域编辑和零样本优化。

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [38] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker是一个基于推理的视频质量评估框架，利用大型多模态模型和强化学习解决现有模型的泛化性和可解释性问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估模型在泛化性和可解释性方面存在不足，限制了其实际应用。

Method: 提出VQAThinker框架，结合GRPO强化学习算法和三种特定奖励（回归、排序和时间一致性），模拟人类感知决策。

Result: 在域内和域外基准测试中表现优异，同时在失真归因和质量描述任务中优于现有模型。

Conclusion: 强化学习为构建仅需分数级监督的泛化性和可解释性VQA模型提供了有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [39] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 本文探讨了卫星光谱图像在AGI中的重要性，指出现有基准的不足，并提出更全面的任务集以评估地球观测模型。


<details>
  <summary>Details</summary>
Motivation: 卫星光谱图像作为AGI的新模态潜力巨大，但研究不足，需更全面的评估基准。

Method: 分析现有基准的局限性，并提出一套任务集以评估模型的泛化能力。

Result: 强调了地球观测数据对AGI的价值，并提出了改进基准的方向。

Conclusion: 需开发更全面的基准以推动地球观测模型的发展。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [40] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet是一种轻量级的两阶段网络，通过状态空间增强的交叉注意力，分别处理事件像素修复和去马赛克任务，显著提升了HybridEVS相机的图像质量。


<details>
  <summary>Details</summary>
Motivation: HybridEVS相机结合Quad Bayer CFA传感器和事件像素时，去马赛克过程会出现伪影和混叠问题，现有方法难以在资源有限的移动设备上解决这些问题。

Method: TSANet采用两阶段网络设计，分别处理事件像素修复和去马赛克，并引入轻量级的Cross-Swin State Block，利用位置先验和状态空间模型增强全局依赖。

Result: TSANet在模拟和真实HybridEVS数据上表现优异，PSNR和SSIM均优于现有方法DemosaicFormer，同时参数和计算成本分别降低1.86倍和3.29倍。

Conclusion: TSANet为移动设备上的高效图像去马赛克提供了新思路，代码已开源。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [41] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: SCJoint是一种联合学习方法，用于同时处理显著目标检测（SOD）和伪装目标检测（COD）任务，通过任务特定的参数和共享网络结构实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管SOD和COD任务具有矛盾属性，但作者认为通过正确的学习方法，网络可以同时具备检测显著和伪装目标的能力。

Method: 提出SCJoint联合学习方案，通过任务特定参数学习解码过程的分布特性，并引入SBSS采样策略平衡训练集。

Result: 实验表明JoNet网络在SOD和COD任务上均表现出色。

Conclusion: SCJoint和SBSS有效实现了联合学习，提升了网络性能。

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [42] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena 是一个通过视觉动画评估大语言模型（LLMs）和多模态大语言模型（MLLMs）的新框架，利用点光源成像放大模型间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试无法提供直观、可感知的性能差异反馈，BioMotion Arena 旨在填补这一空白。

Method: 采用成对比较评估方法，收集了超过45k票的人类投票，用于评估53个主流LLMs和MLLMs在90种生物运动变体上的表现。

Result: 数据分析显示，众包人类投票与专家评分高度一致，且90%以上的模型无法生成基本的人形点光源组或流畅的生物运动。

Conclusion: BioMotion Arena 是一个具有挑战性的性能可视化基准，无需依赖真实数据即可灵活评估模型。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [43] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出了一种基于临床MR扫描生成高分辨率3D伪健康目标形态的管道，用于治疗滑车发育不良（TD），避免了CT扫描的辐射。


<details>
  <summary>Details</summary>
Motivation: 当前TD治疗依赖低分辨率MR扫描和外科医生经验，导致结果不一致且微创技术应用有限。

Method: 1. 使用隐式神经表示（INR）生成各向同性超分辨率MR体积；2. 用多标签网络分割骨骼；3. 训练小波扩散模型（WDM）生成滑车区域的伪健康目标形态。

Result: 在25名TD患者中验证，显著改善了滑车角度（SA）和滑车沟深度（TGD）。

Conclusion: 该方法无需CT扫描，可生成亚毫米级3D形态，适用于术前和术中规划。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [44] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE是一个基于指令的图像和视频编辑统一模型，采用两阶段训练策略和多样化的数据合成方法，提升了编辑性能。


<details>
  <summary>Details</summary>
Motivation: 指令编辑潜力巨大但受限于训练数据不足，DreamVE旨在解决这一问题。

Method: 两阶段训练（先图像后视频）结合拼贴和生成模型数据合成，并设计了高效的编辑框架。

Result: DreamVE在关键编辑类型上表现优异，并通过微调提升了属性编辑能力。

Conclusion: DreamVE为指令编辑提供了高效统一的解决方案，代码和模型将开源。

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [45] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo是一个结合轨迹保持和分布匹配优势的统一蒸馏框架，用于加速视频生成模型，减少推理步骤同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散或流的视频生成模型需要多次迭代采样，计算开销大；现有蒸馏方法在少步设置下性能下降或产生更多伪影。

Method: 提出连续时间一致性蒸馏确保ODE轨迹精确保存，并引入双视角对齐（分布对齐和轨迹对齐）。

Result: 在OpenVid-1M基准测试中，SwiftVideo在少步视频生成上显著优于现有方法。

Conclusion: SwiftVideo通过统一框架解决了少步设置下的性能问题，实现了高效高质量的视频生成。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [46] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer是一种轻量级、即插即用的自适应视觉令牌剪枝框架，通过动态文本引导和跨模态注意力分析，显著降低推理成本，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法未能充分利用推理过程中生成的动态内部信号，导致视觉令牌处理效率低下。

Method: 提出动态文本引导剪枝机制和基于跨模态注意力分析的剪枝策略，实现高效令牌剪枝。

Result: 在LLaVA-1.5-7B上，CUDA延迟降低61.3%，平均准确率保持92.9%。

Conclusion: AdaptInfer在降低推理成本的同时，优于现有方法，具有广泛适用性。

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [47] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP是一种基于视觉语言模型（VLM）的视频质量评估（VQA）框架，通过共享跨模态适配器（SCMA）和可学习质量提示提升性能，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前VQA方法依赖大规模预训练数据集，存在语义知识迁移不足和计算资源需求高的问题，而视觉语言模型（VLMs）在质量评估中展现出潜力。

Method: 提出Q-CLIP框架，采用SCMA增强视觉和文本表示，引入可学习质量提示，并研究帧采样策略对性能的影响。

Result: Q-CLIP在多个VQA数据集上表现优异，且计算成本显著降低。

Conclusion: Q-CLIP为VQA提供了一种高效且性能优越的解决方案，展示了VLMs在此领域的潜力。

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [48] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于情感驱动的反应动作生成方法，通过半监督情感先验和扩散模型，提升了动作的自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成框架未考虑情感影响，导致动作不自然且应用受限，尤其是在交互任务中。

Method: 采用半监督学习训练情感先验，并结合扩散模型生成考虑空间交互和情感响应的反应动作。

Result: 实验表明，该方法在生成多样情感驱动的反应动作上优于现有方法。

Conclusion: 该方法为情感驱动的动作生成提供了有效解决方案，代码和数据将公开。

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [49] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的生成框架UGD-IML，首次将IML和CIML任务统一在一个框架中，减少了对大规模标注数据的依赖，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字时代中，高级图像编辑工具威胁视觉内容的真实性，现有IML方法依赖大规模标注数据，但数据集规模不足且多样性有限，CIML方法则因复杂流程效率低下。

Method: 基于扩散模型的生成框架UGD-IML，通过类嵌入机制和参数共享设计，实现IML和CIML任务的无缝切换，端到端设计简化了标注流程。

Result: 在多个数据集上，UGD-IML在IML和CIML任务中的F1指标分别平均提升9.66和4.36，且在不确定性估计、可视化和鲁棒性方面表现优异。

Conclusion: UGD-IML通过生成模型统一IML和CIML任务，显著减少数据依赖并提升性能，为图像篡改定位提供了高效解决方案。

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [50] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: 提出了一种名为MCA的鲁棒性2D-3D跨模态检索框架，通过多模态联合标签校正和多层次自适应对齐解决噪声标签问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理噪声标签时容易过拟合，且缺乏跨模态一致性，需要更鲁棒的解决方案。

Method: 引入多模态联合标签校正（MJC）机制和多层次自适应对齐（MAA）策略，优化标签和特征对齐。

Result: MCA在传统和噪声3D基准测试中均表现优异，达到最先进水平。

Conclusion: MCA框架在噪声标签条件下具有鲁棒性和有效性，适用于跨模态检索任务。

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [51] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 提出了一种自监督学习框架用于手写数学表达式识别，通过全局和局部对比损失预训练图像编码器，并引入渐进空间掩码策略的自监督注意力网络，无需标注数据即可学习语义焦点区域。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别因二维结构、符号尺度变化和复杂空间关系而具有挑战性，现有方法依赖昂贵标注数据。

Method: 结合全局和局部对比损失预训练图像编码器，设计渐进空间掩码策略的自监督注意力网络，最后用Transformer解码器进行监督微调。

Result: 在CROHME基准测试中优于现有自监督和全监督基线方法。

Conclusion: 渐进注意力机制显著提升了手写数学表达式识别性能，验证了自监督方法的有效性。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [52] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++是一种新的训练框架，通过结合特征图收敛评分（FMCS）和任务标签来优化模型性能，无需修改架构或额外数据。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络（DNNs）内部表示不透明的问题，并改进现有FMCE方法的实验验证和闭环集成不足。

Method: 提出FMCE-Net++框架，利用预训练的FMCE-Net作为辅助头生成FMCS预测，结合任务标签通过表示辅助损失（RAL）动态优化主干网络。

Result: 在多个数据集（如MNIST、CIFAR-10等）上实验显示，FMCE-Net++显著提升模型性能，如ResNet-50在CIFAR-10上准确率提高1.16个百分点。

Conclusion: FMCE-Net++能有效提升现有模型的性能上限，验证了其方法的有效性。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [53] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SynSeg的弱监督方法，通过多类别对比学习（MCCL）和特征协同结构（FSS）解决开放词汇语义分割的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割因语义类别广泛且粒度细而面临挑战，现有弱监督方法依赖类别特定监督和不适合对比学习的特征构建方法，导致语义不对齐和性能差。

Method: 提出SynSeg方法，采用MCCL策略结合类别内外对齐与分离，并通过FSS重构特征以避免视觉编码器引入的前景偏差。

Result: 在多个基准测试中表现优于现有方法，如VOC上准确率提升4.5%，Context上提升8.9%。

Conclusion: SynSeg在弱监督下显著提升了语义定位和区分能力，验证了其有效性。

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [54] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 该研究比较了PCA、CAE和预训练残差网络在卫星图像天气事件分类中的表现，发现CAE效果最佳，但缺乏物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索不同表示学习算法在卫星图像天气事件分类中的表现，并评估其潜在空间的有效性。

Method: 使用PCA、CAE和预训练残差网络（PT）处理卫星图像，并通过分类任务评估潜在空间。

Result: CAE在所有任务中表现最佳，PCA高命中率但高误报率，PT在热带气旋识别中表现突出。高分辨率数据对深度学习算法更有利。

Conclusion: CAE高效但缺乏物理可解释性，未来可开发物理信息增强的CAE。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [55] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner是一个强化学习框架，通过设计奖励函数提升图像描述模型的自我纠正能力，显著优于直接偏好优化策略。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述模型缺乏自我纠正能力，导致生成的描述不够准确。

Method: 通过场景图解析算法分解描述为对象、属性和关系集，计算集差异以设计奖励函数。

Result: 实验表明，SC-Captioner能生成更优的图像描述，显著优于基准方法。

Conclusion: SC-Captioner通过自我纠正机制提升了图像描述的质量和准确性。

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [56] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: VeSCA方法通过利用SAM的编码器生成可迁移的对抗样本，显著提升了性能，揭示了SAM的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 评估SAM的可迁移漏洞，以预防下游应用失败。

Method: 提出VeSCA方法，通过参数化单纯形复形和顶点细化生成对抗样本。

Result: VeSCA性能提升12.7%，优于现有方法。

Conclusion: SAM的漏洞对下游模型构成风险，需开发更鲁棒的基础模型。

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [57] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出了一种基于3D眼球结构的视线重定向框架，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式神经表示，无法显式建模眼球旋转和平移。

Method: 使用3D高斯点云（3DGS）显式建模眼球结构，并引入自适应变形模块模拟眼部肌肉运动。

Result: 在ETH-XGaze数据集上验证，生成图像质量更高，视线估计更准确。

Conclusion: 显式3D眼球结构能有效提升视线重定向的逼真度和准确性。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [58] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的方法，结合稀疏IMU和单目摄像头进行实时人体运动捕捉，通过融合两种信号模态实现高效运动估计。


<details>
  <summary>Details</summary>
Motivation: 稀疏IMU和单目摄像头结合是一种新兴的实时运动捕捉方案，但现有方法在视觉信息缺失时表现不佳，需要一种鲁棒的融合方法。

Method: 将视觉信息整体转化为条件嵌入，同时逐帧结合IMU测量与噪声姿态输入扩散模型，以充分利用两种信号的特性。

Result: 实验表明，该方法在姿态估计上优于现有工作，具有鲁棒性和高效性。

Conclusion: 提出的扩散模型框架有效融合了IMU和视觉信号，解决了视觉信息缺失问题，实现了实时高性能运动捕捉。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [59] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval是一个动态安全评估框架，通过调整安全基准的分布和复杂性来应对MLLMs的安全问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集因MLLM技术进步而过时及数据污染问题。

Method: 采用文本、图像和文本-图像动态策略生成新样本，分析其对模型安全的影响。

Result: SDEval显著影响安全评估，减轻数据污染，并暴露MLLMs的安全局限性。

Conclusion: SDEval是一个通用框架，可应用于现有安全和能力基准，提升评估效果。

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [60] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO提出了一种文本引导的视觉Prompt DINO框架，通过早期融合机制、顺序对齐查询选择和生成数据引擎，解决了开放世界分割中的特征融合和查询选择问题，并在性能和数据生成方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决多模态视觉模型中后期特征融合不足、查询选择次优以及固定词汇表限制的问题。

Method: 1. 早期融合机制统一文本/视觉提示和骨干特征；2. 顺序对齐查询选择优化文本与视觉查询的结构对齐；3. 生成数据引擎通过双路径交叉验证减少标签噪声。

Result: Prompt-DINO在开放世界检测基准上达到最先进性能，语义覆盖显著扩展，标签噪声减少80.5%。

Conclusion: Prompt-DINO为开放世界场景下的可扩展多模态检测和数据生成建立了新范式。

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [61] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: 提出了一种名为DSConv的动态卷积核分割方法，结合注意力机制，用于提升遥感图像融合的分辨率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖标准卷积，而自适应卷积在遥感图像中更有效，但研究较少。

Method: 动态分割卷积核并结合注意力机制，选择感兴趣位置，将原始卷积核分割为多个小核。

Result: DSConv能更有效提取特征，提升网络泛化、优化和特征表示能力，实验证明其先进性能。

Conclusion: DSConv在遥感图像融合中表现出优越性和高效性，为任务提供了新解决方案。

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [62] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST是一种新型无监督域自适应方法，利用语言模态的鲁棒性指导视觉模型适应，通过生成伪标签和不确定性估计提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂域偏移（如地理偏移）下现有UDA方法的不足，利用语言模态的鲁棒性改进视觉模型。

Method: 1. 从目标样本的标题生成伪标签；2. 提出基于CLIP相似性分数的伪标签不确定性估计；3. 引入多模态软对比学习损失对齐视觉和语言特征空间。

Result: 在DomainNet和GeoNet上超越现有方法，达到新SOTA。

Conclusion: TRUST通过语言模态引导和不确定性估计，显著提升了复杂域偏移下的UDA性能。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [63] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR是一个用户中心的多维度文本到图像（T2I）评估基准，通过混合方法（确定性指标和HWPQ方案）解决现有指标的局限性，显著提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估指标存在不足，VISTAR旨在提供更全面、用户导向的评估框架。

Method: 采用两阶段混合范式：确定性指标量化物理属性，HWPQ方案评估抽象语义。基于专家研究定义用户角色和评估角度。

Result: HWPQ方案在抽象语义评估中达到85.9%准确率，整体指标与人类判断对齐度>75%。不同模型无绝对优势，角色加权分数提供领域部署指导。

Conclusion: VISTAR为T2I评估提供高效、可复现的工具，公开资源促进研究发展。

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [64] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST是一种弱监督生成网络，用于通过时空融合方法估计每日10米分辨率的陆地表面温度（LST），优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 城市化、气候变化和农业压力增加了对环境监测的需求，但现有遥感系统在时空分辨率上存在权衡。

Method: 采用条件生成对抗网络架构，包括特征提取、融合、LST重建和噪声抑制四个阶段，结合弱监督训练策略。

Result: WGAST在定量和定性评估中均优于现有方法，平均降低RMSE 17.18%，提高SSIM 11.00%。

Conclusion: WGAST能有效捕捉精细热模式，对云干扰具有鲁棒性，适用于高分辨率LST估计。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [65] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为MPF-KANSC的创新框架，通过多平面融合和KANSC注意力机制提升阿尔茨海默病的诊断精度。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期诊断困难，现有深度学习方法难以捕捉脑部复杂结构变化。

Method: 结合多平面融合（MPF）和KANSC注意力机制，并行提取多平面特征并精确识别异常。

Result: 在ADNI数据集上表现优异，并发现AD进展中右脑结构不对称的新证据。

Conclusion: MPF-KANSC显著提升了AD诊断能力，并具有良好可解释性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [66] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff框架通过混合分辨率去噪和模块级缓存策略，在无需微调的情况下优化预训练扩散模型的效率与生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但计算成本高，限制了其在资源有限平台上的部署。本文旨在探索在不进行微调的情况下，如何更有效地优化扩散模型的部署效率。

Method: 提出PostDiff框架，包括混合分辨率去噪方案（早期步骤降低分辨率）和模块级缓存策略（跨步骤重用计算）。

Result: 实验表明，PostDiff显著提升了扩散模型的效率与生成质量平衡，且降低单步推理成本比减少去噪步骤更有效。

Conclusion: PostDiff为预训练扩散模型的高效部署提供了一种无需微调的可行方案，同时保持生成质量。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [67] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: 论文提出UW-3DGS框架，通过3D高斯点云和物理感知优化，解决水下3D重建中的光吸收和散射问题，显著提升几何和颜色保真度。


<details>
  <summary>Details</summary>
Motivation: 传统方法如NeRF及其扩展在水下环境中因光吸收和散射导致几何和颜色保真度下降，且效率受限。

Method: 采用3D高斯点云（3DGS）框架，结合可学习的水下成像模块和物理感知不确定性剪枝（PAUP），优化噪声高斯点云。

Result: 在SeaThru-NeRF和UWBundle数据集上表现优异，PSNR达27.604，SSIM为0.868，LPIPS为0.104，浮动伪影减少65%。

Conclusion: UW-3DGS通过物理感知优化和高效点云处理，显著提升水下3D重建质量，适用于复杂水下环境。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [68] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 研究提出了一种多方向架构框架，用于自动化结肠镜图像中的息肉检测，结合合成数据生成和检测分割算法，显著提升了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查是结直肠癌早期诊断的关键工具，但数据集有限且标注复杂，因此需要自动化解决方案以提高效率和准确性。

Method: 采用Faster R-CNN进行初始目标定位，结合Segment Anything Model（SAM）优化分割掩码，并评估了五种分割模型（U-Net、PSPNet、FPN、LinkNet、MANet）。

Result: Faster R-CNN的召回率为93.08%，精确度为88.97%，F1分数为90.98%；FPN在PSNR和SSIM上表现最佳，UNet在召回率上领先，LinkNet在IoU和Dice分数上表现均衡。

Conclusion: 该框架有效解决了数据集和标注问题，显著提升了息肉检测和分割性能，为结直肠癌早期诊断提供了可靠工具。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [69] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: 论文提出了一种基于多智能体异步协作的犯罪行为预测框架（MA-CBP），通过实时视频流分析和高低语义融合，实现潜在犯罪行为的早期预警。


<details>
  <summary>Details</summary>
Motivation: 城市化加速导致公共场景犯罪行为威胁加剧，传统方法难以捕捉高层次行为语义或满足实时需求。

Method: MA-CBP框架将视频流转化为语义描述，构建因果一致的历史摘要，并融合相邻帧进行长短上下文联合推理。

Result: 实验表明，该方法在多个数据集上表现优异，为城市公共安全风险预警提供了有效解决方案。

Conclusion: MA-CBP框架通过多尺度语言监督和语义融合，显著提升了犯罪行为预测的准确性和实时性。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [70] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: 提出了一种名为DBIF-AUNet的双分支交互融合注意力模型，用于解决胸水CT图像语义分割中的挑战，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 胸水CT图像语义分割面临灰度相似、边缘模糊和形态多变等问题，现有方法难以处理复杂图像变化和语义鸿沟。

Method: 设计了双域特征解耦模块（DDFD）和分支交互注意力融合模块（BIAF），结合嵌套深度监督机制，实现多尺度特征互补和动态融合。

Result: 在1622张胸水CT图像上验证，IoU和Dice分数分别达到80.1%和89.0%，优于现有模型。

Conclusion: DBIF-AUNet显著优化了复杂胸水CT图像的分割精度，为临床诊断提供了更准确的工具。

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [71] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: 论文提出了一种名为LiLoRA的高效架构扩展方法，用于解决多模态大语言模型在持续视觉指令调整中的灾难性遗忘问题，显著提高了参数效率。


<details>
  <summary>Details</summary>
Motivation: 持续视觉指令调整（CVIT）中，多模态大语言模型（MLLMs）在学习新任务时会出现灾难性遗忘问题，现有方法因扩展整个层导致参数冗余和可扩展性差。

Method: 提出LiLoRA方法：共享LoRA矩阵A以减少冗余，对矩阵B进行低秩分解以减少任务特定参数，并引入余弦正则化稳定性损失以保持共享表示的一致性。

Result: 在多样化CVIT基准测试中，LiLoRA在顺序任务学习中表现优异，且参数效率显著优于现有方法。

Conclusion: LiLoRA是一种高效且可扩展的架构扩展方法，有效解决了CVIT中的灾难性遗忘问题。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [72] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE是一个基于Mixture-of-Experts架构的通用异常检测框架，通过分层设计和专家模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法过于专业化，泛化能力有限，需要一种通用框架。

Method: AnomalyMoE将问题分解为三个语义层次，分别使用专家网络处理，并引入EIR和ESB模块优化专家多样性及利用率。

Result: 在8个数据集上表现优异，超越领域专用方法。

Conclusion: AnomalyMoE是一种高效且通用的异常检测解决方案。

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [73] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: PA-HOI数据集填补了现有HOI数据集中忽略物体物理属性对人类长期运动影响的空白，通过562个运动序列展示了物体尺寸、形状和重量对人类姿势、速度等动态的影响。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集多关注功能细节，忽视了物体物理属性对人类运动的影响，限制了相关领域的研究。

Method: 构建PA-HOI数据集，包含562个运动序列，涉及不同性别受试者与35种3D物体的交互，记录物体物理属性对人类运动的影响。

Result: 数据集扩展了现有研究范围，验证了物体物理属性对人类姿势、速度和交互策略的影响，并成功应用于现有运动生成方法。

Conclusion: PA-HOI数据集为理解物体物理属性对人类运动的影响提供了新工具，推动了相关领域的研究。

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [74] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: 提出了一种基于双腕X光片的可解释性图像级SvdH评分预测方法，通过注意力机制的多实例学习整合疾病相关区域，显著提升了评分效率与准确性。


<details>
  <summary>Details</summary>
Motivation: SvdH评分在RA临床研究中广泛应用，但其复杂性限制了常规临床实践中的使用，需要一种高效的自动化评分方法。

Method: 采用两阶段流程：1）提取疾病相关图像区域；2）基于注意力机制的多实例学习整合区域特征进行预测。提出了两种区域提取方案：异常区域采样和关节区域裁剪。

Result: 最佳模型PCC达0.943，RMSE为15.73；集成学习后PCC提升至0.945，RMSE降至15.57，接近放射科专家水平（PCC=0.97，RMSE=18.75）。

Conclusion: 该流程不仅高效准确，还能识别与RA进展相关的解剖结构，为临床决策提供支持。

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [75] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: 提出了一种纹理感知和边缘引导的Transformer（TEFormer），用于解决城市遥感图像语义分割中的纹理差异和边缘模糊问题。


<details>
  <summary>Details</summary>
Motivation: 城市遥感图像中地物纹理差异小、空间结构相似，易导致语义模糊和误分类，且不规则形状和模糊边界增加了分割难度。

Method: 设计了纹理感知模块（TaM）捕获纹理差异，边缘引导三分支解码器（Eg3Head）保留边缘细节，边缘引导特征融合模块（EgFFM）融合上下文与边缘信息。

Result: 在Potsdam、Vaihingen和LoveDA数据集上分别达到88.57%、81.46%和53.55%的mIoU。

Conclusion: TEFormer有效提升了城市遥感图像语义分割的准确性。

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [76] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: 提出了一种通用的图像去模糊方法，能够处理多种模糊类型，通过混合专家（MoE）解码模块动态路由特征，实现高效恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定模糊类型设计，缺乏通用性，需要多个模型覆盖不同模糊类型，不适用于实际场景。

Method: 采用混合专家（MoE）解码模块，根据识别的模糊类型动态路由图像特征，实现端到端的精确恢复。

Result: 该方法性能与专用模型相当，且在未见过的模糊场景中表现出优异的鲁棒性和泛化能力。

Conclusion: 提出的统一方法解决了现有方法的局限性，为图像去模糊提供了高效且通用的解决方案。

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [77] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: 论文提出了一种参数高效的方法LNCLIP-DF，通过微调预训练CLIP模型的层归一化参数（仅0.03%），并结合L2归一化和潜在空间增强，实现了对未见过的深度伪造技术的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测器在应对未知伪造技术时泛化能力不足，现有方法通常引入复杂架构，而本文旨在通过参数高效的方式实现鲁棒泛化。

Method: 仅微调CLIP模型的层归一化参数（0.03%），并采用L2归一化和潜在空间增强来优化特征流形。

Result: 在13个基准数据集上评估，LNCLIP-DF表现优于复杂方法，取得最高平均跨数据集AUROC。分析发现：1）成对真实-伪造数据训练对泛化至关重要；2）学术数据集的检测难度并未随时间增加。

Conclusion: 通过针对性微调预训练CLIP模型，实现了高效且可复现的深度伪造检测方法，证明了参数高效调整的潜力。

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [78] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: 提出了一种名为FedX的新策略，通过解释引导的剪枝减少联邦学习中的通信开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在遥感图像分类中面临通信开销大的问题，FedX旨在解决这一问题。

Method: 利用反向传播解释方法评估模型组件的重要性，剪枝最不相关的部分以减少传输数据量。

Result: 在BigEarthNet-S2和EuroSAT数据集上验证了FedX能显著减少参数数量并提升泛化能力。

Conclusion: FedX有效降低了通信开销，同时优于未剪枝模型和其他先进剪枝方法。

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [79] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net是一种新型2.5D U-Net架构，结合了像素级跨切片注意力和跳跃注意力门控机制，显著提升了股骨MRI分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D深度学习方法在股骨MRI分割中存在局限性，需要更高效的解决方案。

Method: 提出XAG-Net，采用像素级跨切片注意力（CSA）和跳跃注意力门控（AG）机制，优化切片间上下文建模和切片内特征细化。

Result: XAG-Net在分割精度上优于基线2D、2.5D和3D U-Net模型，同时保持计算效率。

Conclusion: XAG-Net是高效且准确的股骨MRI分割框架，CSA和AG模块是关键。

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [80] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker是一个空间感知的多模态框架，通过深度增强的边界框和自然语言交互，提升复杂视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂视觉任务（如空间理解和细粒度感知）中表现不足，缺乏利用空间线索进行注意力校正的能力。

Method: 提出SIFThinker框架，采用反向扩展前向推理策略生成图像-文本链，并引入GRPO-SIF强化训练范式，动态校正和聚焦提示相关区域。

Result: 实验表明，SIFThinker在空间理解和细粒度视觉感知任务中优于现有方法，同时保持通用能力。

Conclusion: SIFThinker通过空间感知和动态注意力校正，显著提升了复杂视觉任务的性能。

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [81] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: 论文提出了一种数据高效的无标注跨域视频时间定位方法（URPA），通过少量目标域无标注视频实现模型适应，解决了GRPO依赖标注数据和高计算成本的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GRPO）依赖标注数据且计算成本高，难以在无标注域或实时场景中应用。

Method: 提出URPA方法，利用GRPO生成多个候选预测，通过伪标签和置信度加权训练奖励，实现无标注跨域知识迁移。

Result: 在三个数据集的六种跨域设置中，URPA仅需少量无标注目标视频即可实现良好泛化。

Conclusion: URPA有效解决了无标注跨域视频时间定位问题，计算和存储开销低，适合实时部署。

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [82] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 论文提出GS-MoE框架，通过专家模型和时序高斯损失提升弱监督视频异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测模型在处理复杂异常时表现不佳，主要因缺乏类别特异性特征和弱监督信号不足。

Method: 采用高斯喷溅引导的专家混合模型（GS-MoE），每个专家专注于特定异常类型，并通过时序高斯损失增强监督。

Result: 在UCF-Crime数据集上达到91.58% AUC，在XD-Violence和MSAD数据集上表现优异。

Conclusion: GS-MoE通过类别特异性专家和时序指导，为弱监督视频异常检测设定了新基准。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [83] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的合成数据生成方法，用于解决心脏MR图像分析中的域偏移问题，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 心脏MR图像因设备和协议差异导致域偏移，限制了AI模型在实际场景中的应用。传统方法如数据增强或迁移学习存在局限性。

Method: 使用扩散模型生成合成心脏MR图像，保持结构和空间一致性，用于域泛化和域适应策略。

Result: 在未见目标域数据上，分割性能显著提升（p < 0.01），优于仅使用真实数据训练的方法。

Conclusion: 该方法有效解决了域偏移问题，减少了对迁移学习的需求，特别适用于数据稀缺场景。

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [84] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: 改进ViPro模型，使其能从未知初始状态推断视频帧，并在无监督下学习，扩展了3D数据集以接近现实场景。


<details>
  <summary>Details</summary>
Motivation: 解决ViPro模型因依赖初始符号状态而无法处理噪声观测的问题。

Method: 在ViPro基础上改进，使其能从未知初始状态推断状态，并在无监督下学习，扩展了3D数据集。

Result: 模型能从未知初始状态正确推断状态，并在3D数据上验证。

Conclusion: 改进后的ViPro模型更适用于现实场景，无需依赖初始状态。

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [85] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: 利用街景图像和大型语言模型分析城市社交性，验证了城市规划理论中环境与社交互动的关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注行人数量而非社交互动质量，探索街景图像中潜在的社交信息。

Method: 分析15个城市的2,998张街景图像，结合Mehta的社交分类理论，使用线性回归模型控制变量。

Result: 天空视野指数与所有社交类型相关，绿视野指数预测持久社交性，地方依恋与短暂社交性正相关。

Conclusion: 街景图像可作为研究城市社交性的工具，支持跨文化理论验证和城市规划设计。

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [86] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT是一种新型多模态大语言模型，专注于视频中异常事件的总结和定位，通过空间和时间有效令牌选择模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型在处理异常事件时因空间和时间稀疏性表现不佳，需改进。

Method: 提出VA-GPT模型，包含SETS和TETG模块，优化视觉与语言模型的令牌对齐，并构建专用数据集和评测基准。

Result: VA-GPT在多个基准测试中优于现有方法。

Conclusion: VA-GPT通过高效令牌选择和跨域评测，显著提升了异常事件分析的准确性。

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [87] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: 本文实现了一种基于Goldstein等人的两阶段图像分割算法，通过改进Chan-Vese能量模型，利用split Bregman方法高效完成图像分割。


<details>
  <summary>Details</summary>
Motivation: 解决传统图像分割算法效率低下的问题，提出一种更高效的两阶段分割方法。

Method: 改进Chan-Vese能量模型，引入split Bregman方法进行优化，实现高效的两阶段图像分割。

Result: 通过多组图像和参数测试，验证了该方法的有效性和性能。

Conclusion: 该方法在保持分割质量的同时显著提高了效率，适用于多种图像分割场景。

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [88] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 论文提出了一种新方法，利用排名第一身份的额外注册图像来预测其是否属于图库内或图库外，以减少误识别和调查时间浪费。


<details>
  <summary>Details</summary>
Motivation: 解决一对面多识别中图库外身份误判的问题，减少误识别和错误逮捕。

Method: 通过提取排名第一身份的额外注册图像排名生成训练数据，训练分类器预测图库内/外状态。

Result: 实验证明该方法适用于多种图像质量，且在人口统计组间表现一致。

Conclusion: 该方法能有效减少误识别，且仅在使用高级损失函数训练的匹配器时有效。

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [89] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT是一种通过一致提示调优构建通用表示模型的方法，仅需文本数据即可扩展到无限模态。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量模态特定标注数据或仅适用于单一模态，限制了泛化能力。

Method: TaAM-CPT结合模态提示池、文本构建和预训练模型的模态对齐文本编码器，设计了跨模态学习目标。

Result: 无需模态特定标注数据，TaAM-CPT在视频、图像和音频分类任务中取得领先结果。

Conclusion: TaAM-CPT展示了通过文本数据扩展模态的潜力，为多模态学习提供了高效解决方案。

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [90] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen框架通过蒸馏视频扩散模型，实现快速新视角合成，采样步骤减少90%以上，同时保持或提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图在3D重建中易产生伪影，现有方法采样速度慢，限制了效率。

Method: 提出FVGen框架，使用GAN和软化反向KL散度最小化，将多步去噪教师模型蒸馏为少步去噪学生模型。

Result: 实验表明，FVGen在相同视图数量下，视觉质量相似或更好，采样时间减少90%以上。

Conclusion: FVGen显著提升稀疏输入视图下的时间效率，适用于下游3D重建任务。

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [91] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 论文探讨了将分类目标直接融入超分辨率过程是否能提升分类准确性，并提出了一种新颖的方法来优化图像质量和分类性能。


<details>
  <summary>Details</summary>
Motivation: 低分辨率图像限制了自动化分析的准确性，传统超分辨率方法仅关注像素级指标，未充分探索超分辨率图像保真度与下游分类性能的关系。

Method: 提出了一种新颖的算法策略，通过优化同时考虑图像质量和分类性能的损失函数，提升合成孔径雷达图像的分辨率。

Result: 该方法不仅提高了图像质量（通过科学指标衡量），还增强了分类准确性。

Conclusion: 研究表明，将分类目标融入超分辨率过程可以同时提升图像质量和分类性能。

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [92] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 论文研究了在SAR船只分类中特征空间过采样的效果，提出了两种新算法M2m_f和M2m_u，并在两个公开数据集上验证了其优于原始方法和基线。


<details>
  <summary>Details</summary>
Motivation: 解决SAR船只分类中长尾数据集导致的类别不平衡问题，特别是少数类别的分类困难。

Method: 提出了两种基于Major-to-minor方法的算法M2m_f和M2m_u，并在OpenSARShip和FuSARShip数据集上测试，使用了ViT、VGG16和ResNet50作为特征提取器。

Result: 新方法在FuSARShip和OpenSARShip上的平均F1分数分别提高了8.82%和4.44%。

Conclusion: 特征空间过采样方法能有效提升SAR船只分类性能，特别是在长尾数据集中。

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [93] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的半监督学习框架，用于解决医学影像中标记数据不足的问题，在极端低标记数据（5-50样本/类）下表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中效果受限于标记数据不足，需要一种能在低标记数据下稳健分类的方法。

Method: 结合生成器、判别器和分类器的三阶段训练框架，通过图像翻译和集成伪标记技术利用未标记数据。

Result: 在11个MedMNIST数据集上显著优于6种现有方法，尤其在5样本/类的极端情况下表现突出。

Conclusion: 该框架为高标注成本的医学影像应用提供了实用解决方案，代码已开源。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [94] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: 本文提出了一种改进的SimSwap框架，通过引入自注意力与交叉注意力机制、动态损失权重和余弦退火学习率调度，显著提升了人脸交换的保真度、身份保留和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 人脸交换技术在学术和商业应用中备受关注，但现有方法在身份保留和视觉质量上仍有提升空间。本文旨在通过改进SimSwap框架解决这些问题。

Method: 在生成器架构中集成自注意力和交叉注意力机制，采用动态损失权重和余弦退火学习率调度。

Result: 实验结果表明，改进后的模型在身份相似性、FID分数和视觉质量上均优于基线模型。消融研究验证了各改进的重要性。

Conclusion: 未来研究方向包括整合StyleGAN3、改进唇部同步、引入3D面部建模和视频应用中的时间一致性。

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [95] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin是一种非对比性插件，可无缝集成到CLIP架构中，提升多模态语义对齐能力，增强监督和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模自然图像-文本数据集语义对齐松散和医学数据集内容多样性低的问题，以提升CLIP模型的泛化能力。

Method: 设计CLIPin插件，引入两个共享预投影器，结合对比和非对比学习，优化参数。

Result: 在多样化下游任务中验证了CLIPin的有效性和通用性。

Conclusion: CLIPin作为即插即用组件，兼容多种对比框架，显著提升性能。

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [96] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: 论文研究了将大语言模型（LLMs）与Swin-UMamba架构结合，用于CT图像病灶分割的可行性，并展示了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病（如淋巴瘤）的临床评估需要自动测量病灶，结合影像特征与放射报告中的文本描述可能提升分割效果。

Method: 采用Swin-UMamba架构，结合文本描述（来自放射报告）进行病灶分割，使用ULS23 DeepLesion数据集进行验证。

Result: 测试集上Dice Score达82%，Hausdorff距离为6.58像素，优于LLM驱动的LanGuideMedSeg（提升37%）及纯图像方法xLSTM-UNet和nnUNet。

Conclusion: Text-Swin-UMamba模型在病灶分割任务中表现优异，证明了文本与影像结合的有效性。

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [97] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 通过模块化和多样化图表生成方法，提出了一种改进多模态大语言模型（MLLM）图表理解能力的数据合成流程，并发布了包含10k+图表和300k+问答对的有效图表数据集（ECD）。


<details>
  <summary>Details</summary>
Motivation: 现有开源MLLM在图表理解任务上的成功率较低（30%-50%），且合成图表与真实图表相似度不足，影响了模型性能。

Method: 设计了五步数据合成流程：分离数据和函数生成单图、条件生成多子图、视觉多样化、过滤低质量数据、用GPT-4生成问答对。

Result: 生成的ECD数据集显著提升了多种MLLM在真实和合成测试集上的表现。

Conclusion: 模块化和多样化图表生成方法有效提升了MLLM的图表理解能力，ECD数据集为相关研究提供了高质量资源。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: 论文提出LightSwitch，一种基于材料重光照的扩散框架，通过多视图和材料信息高效重光照多视图数据。


<details>
  <summary>Details</summary>
Motivation: 现有2D重光照生成先验未充分利用目标的内在属性或大规模多视图数据，导致效果不佳。

Method: 提出LightSwitch框架，结合多视图和材料信息，采用可扩展去噪方案。

Result: LightSwitch在2D重光照预测质量上超越现有方法，并在合成和真实物体重光照中表现优异。

Conclusion: LightSwitch高效且一致地处理多视图数据，重光照效果优于现有方法。

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [99] [A United Framework for Planning Electric Vehicle Charging Accessibility](https://arxiv.org/abs/2508.05827)
*Tony Kinchen,Panagiotis Typaldos,Andreas A. Malikopoulos*

Main category: eess.SY

TL;DR: 本文提出了一种优化电动汽车充电站位置的框架，结合交通模拟、能耗模型和公平性指标，以纽约市为例，显著提升了充电站的可达性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车的普及需要合理的充电基础设施布局，尤其是在密集城市环境中平衡效率和空间可达性。

Method: 提出一个优化框架，整合交通模拟、能耗建模和公平性指标，评估充电站的社会覆盖范围。

Result: 在纽约市的案例中，充电站的可达性提升了15-20%，减少了出行时间的不确定性。

Conclusion: 该框架为电动汽车基础设施规划提供了可扩展的方法，但经济因素和电网整合仍需进一步研究。

Abstract: The shift towards electric vehicles (EVs) is crucial for establishing
sustainable and low-emission urban transportation systems. However, the success
of this transition depends on the strategic placement of the charging
infrastructure. This paper addresses the challenge of optimizing charging
station locations in dense urban environments while balancing efficiency with
spatial accessibility. We propose an optimization framework that integrates
traffic simulation, energy consumption modeling, and a mobility equity measure
to evaluate the social reach of each potential charging station. Using New York
City as a case study, we demonstrate consistent improvements in accessibility
(15-20% reduction in travel time variability). Our results provide a scalable
methodology for incorporating equity considerations into EV infrastructure
planning, although economic factors and grid integration remain important areas
for future development.

</details>


### [100] [Distributed Optimization and Learning for Automated Stepsize Selection with Finite Time Coordination](https://arxiv.org/abs/2508.05887)
*Apostolos I. Rikos,Nicola Bastianello,Themistoklis Charalambous,Karl H. Johansson*

Main category: eess.SY

TL;DR: 提出了一种分布式学习算法，通过消除节点间的步长异质性，提升收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式梯度方法中步长异质性导致的学习过程不稳定和潜在发散问题。

Method: 设计了一种分布式学习算法，结合有限时间协调机制自动选择步长，消除节点间异质性。

Result: 算法收敛到最优解，数值模拟显示其在线性回归问题中优于现有方法。

Conclusion: 消除步长异质性显著提升分布式学习的收敛性能。

Abstract: Distributed optimization and learning algorithms are designed to operate over
large scale networks enabling processing of vast amounts of data effectively
and efficiently. One of the main challenges for ensuring a smooth learning
process in gradient-based methods is the appropriate selection of a learning
stepsize. Most current distributed approaches let individual nodes adapt their
stepsizes locally. However, this may introduce stepsize heterogeneity in the
network, thus disrupting the learning process and potentially leading to
divergence. In this paper, we propose a distributed learning algorithm that
incorporates a novel mechanism for automating stepsize selection among nodes.
Our main idea relies on implementing a finite time coordination algorithm for
eliminating stepsize heterogeneity among nodes. We analyze the operation of our
algorithm and we establish its convergence to the optimal solution. We conclude
our paper with numerical simulations for a linear regression problem,
showcasing that eliminating stepsize heterogeneity enhances convergence speed
and accuracy against current approaches.

</details>


### [101] [Distributed Quantized Average Consensus in Open Multi-Agent Systems with Dynamic Communication Links](https://arxiv.org/abs/2508.05895)
*Jiaqi Hu,Karl H. Johansson,Apostolos I. Rikos*

Main category: eess.SY

TL;DR: 本文提出了一种分布式算法，用于解决开放动态多智能体系统中的量化平均共识问题，具有高效通信和有限时间收敛的特点。


<details>
  <summary>Details</summary>
Motivation: 研究开放动态多智能体系统中量化平均共识问题，以应对通信链路动态变化的挑战。

Method: 提出一种分布式算法，支持节点交换量化值消息，并确保有限时间内收敛到解。

Result: 算法正确性得到验证，并提供了成功解决问题的拓扑条件。数值模拟展示了算法性能。

Conclusion: 该算法在开放动态多智能体系统中有效解决了量化平均共识问题，具有实际应用潜力。

Abstract: In this paper, we focus on the distributed quantized average consensus
problem in open multi-agent systems consisting of communication links that
change dynamically over time. Open multi-agent systems exhibiting the
aforementioned characteristic are referred to as \textit{open dynamic
multi-agent systems} in this work. We present a distributed algorithm that
enables active nodes in the open dynamic multi-agent system to calculate the
quantized average of their initial states. Our algorithm consists of the
following advantages: (i) ensures efficient communication by enabling nodes to
exchange quantized valued messages, and (ii) exhibits finite time convergence
to the desired solution. We establish the correctness of our algorithm and we
present necessary and sufficient topological conditions for it to successfully
solve the quantized average consensus problem in an open dynamic multi-agent
system. Finally, we illustrate the performance of our algorithm with numerical
simulations.

</details>


### [102] [Panel-Scale Reconfigurable Photonic Interconnects for Scalable AI Computation](https://arxiv.org/abs/2508.06079)
*Tzu-Chien Hsueh,Bill Lin,Zijun Chen,Yeshaiahu Fainman*

Main category: eess.SY

TL;DR: 提出了一种新型光子交换结构，支持大尺寸玻璃基板上的可重构光子互连，实现高带宽、低能耗和异构集成。


<details>
  <summary>Details</summary>
Motivation: 解决大尺寸玻璃基板上光子互连的高带宽、低能耗和可重构性问题，支持异构计算系统的集成。

Method: 采用可重构光子互连技术，结合多层波导、高速光调制器、宽带光电探测器和光交叉开关，实现面板级互连。

Result: 实现了500mm x 500mm或更大尺寸的玻璃基板上的高带宽、低能耗光子互连，支持异构集成。

Conclusion: 该技术为大规模异构计算系统提供了高效的光子互连解决方案。

Abstract: Panel-scale reconfigurable photonic interconnects on a glass substrate up to
500-mm x 500-mm or larger are envisioned by proposing a novel photonic switch
fabric that enables all directional panel-edge-to-panel-edge reach without the
need for active repeaters while offering high communication bandwidth,
planar-direction reconfigurability, low energy consumption, and compelling data
bandwidth density for heterogeneous integration of an in-package AI computing
system on a single glass-substrate photonic interposer exceeding thousands of
centimeters square. The proposed approach focuses on reconfigurable photonic
interconnects, which are integration-compatible with commercial processor
chiplets and 3D high-bandwidth memory (HBM) stacks on a large-area glass
substrate, to create a novel panel-scale heterogeneously integrated interposer
or package enabling low-energy and high-capacity
wavelength-division-multiplexing (WDM) optical data links using advanced
high-speed optical modulators, broadband photodetectors, novel optical crossbar
switches with multi-layer waveguides, and in-package frequency comb sources.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [103] [DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models](https://arxiv.org/abs/2508.05685)
*Yara Bahram,Mohammadhadi Shateri,Eric Granger*

Main category: cs.GR

TL;DR: DogFit方法通过领域感知的微调机制，在扩散模型迁移学习中实现了高效的可控性，减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在小目标领域迁移学习中泛化性差和计算成本高的问题。

Method: 提出Domain-guided Fine-tuning (DogFit)，通过领域感知的指导偏移和轻量级条件机制，优化训练和推理过程。

Result: 在多个目标领域和骨干模型上，DogFit在FID和FDDINOV2指标上优于现有方法，且计算效率更高。

Conclusion: DogFit为扩散模型迁移学习提供了一种高效且可控的解决方案。

Abstract: Transfer learning of diffusion models to smaller target domains is
challenging, as naively fine-tuning the model often results in poor
generalization. Test-time guidance methods help mitigate this by offering
controllable improvements in image fidelity through a trade-off with sample
diversity. However, this benefit comes at a high computational cost, typically
requiring dual forward passes during sampling. We propose the Domain-guided
Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion
transfer learning that maintains controllability without incurring additional
computational overhead. DogFit injects a domain-aware guidance offset into the
training loss, effectively internalizing the guided behavior during the
fine-tuning process. The domain-aware design is motivated by our observation
that during fine-tuning, the unconditional source model offers a stronger
marginal estimate than the target model. To support efficient controllable
fidelity-diversity trade-offs at inference, we encode the guidance strength
value as an additional model input through a lightweight conditioning
mechanism. We further investigate the optimal placement and timing of the
guidance offset during training and propose two simple scheduling strategies,
i.e., late-start and cut-off, which improve generation quality and training
stability. Experiments on DiT and SiT backbones across six diverse target
domains show that DogFit can outperform prior guidance methods in transfer
learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling
TFLOPS.

</details>


### [104] [Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions](https://arxiv.org/abs/2508.06086)
*Kojiro Tanaka,Keiichi Sato,Masahiko Mikawa,Makoto Fujisawa*

Main category: cs.GR

TL;DR: 本文提出了一种基于虚拟环境的草显示颜色变化特性交互模拟方法，解决了传统方法耗时耗成本的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要在实际设备上进行实验，每次光照或视角变化都需重新实验，效率低且成本高。

Method: 通过虚拟环境模拟草的颜色变化特性，并在多视角和多环境下评估其准确性。

Result: 模拟结果与实际特性相似，且速度更快，准确性接近先前研究。

Conclusion: 该方法为草显示的颜色特性模拟提供了一种高效且准确的解决方案。

Abstract: Recent research has focused on incorporating media into living environments
via color-controlled materials and image display. In particular, grass-based
displays have drawn attention as landscape-friendly interactive interfaces. To
develop the grass display, it is important to obtain the grass color change
characteristics that depend on the real environment. However, conventional
methods require experiments on actual equipment every time the lighting or
viewpoint changes, which is time-consuming and costly. Although research has
begun on simulating grass colors, this approach still faces significant issues
as it takes many hours for a single measurement. In this paper, we explore an
interactive simulation of a grass display color change characteristic based on
real-world conditions in a virtual environment. We evaluated our method's
accuracy by simulating grass color characteristics across multiple viewpoints
and environments, and then compared the results against prior work. The results
indicated that our method tended to simulate the grass color characteristics
similar to the actual characteristics and showed the potential to do so more
quickly and with comparable accuracy to the previous study.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [105] [GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems](https://arxiv.org/abs/2508.05773)
*Keyvan Majd,Hardik Parwana,Bardh Hoxha,Steven Hong,Hideki Okamoto,Georgios Fainekos*

Main category: cs.RO

TL;DR: BR-MPPI方法通过嵌入CBF约束改进MPPI控制，提高了在复杂环境中导航的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决铰接式车辆在拥挤空间中的反向和机动问题，确保行人安全和动态可行性。

Method: 将CBF约束直接嵌入路径积分更新，引导重要性采样分布朝向无碰撞、动态可行的轨迹。

Result: 在CarMaker模拟器中，BR-MPPI在12米拖拉机拖车上的反向和正向停车任务中表现优于标准MPPI和带碰撞成本的MPPI基线。

Conclusion: BR-MPPI在复杂环境中导航时具有更高的鲁棒性和实时性，适用于实际应用。

Abstract: Articulated vehicles such as tractor-trailers, yard trucks, and similar
platforms must often reverse and maneuver in cluttered spaces where pedestrians
are present. We present how Barrier-Rate guided Model Predictive Path Integral
(BR-MPPI) control can solve navigation in such challenging environments.
BR-MPPI embeds Control Barrier Function (CBF) constraints directly into the
path-integral update. By steering the importance-sampling distribution toward
collision-free, dynamically feasible trajectories, BR-MPPI enhances the
exploration strength of MPPI and improves robustness of resulting trajectories.
The method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]
tractor-trailer tasked with reverse and forward parking in a parking lot.
BR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for
scenarios with eight obstacles) and maintains better parking clearance than a
standard MPPI baseline and an MPPI with collision cost baseline.

</details>


### [106] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 论文提出了一种结合视觉基础模型与强化学习的新方法，通过整合SAM和YOLOv5以及PPO算法，提升了模拟环境中对象的交互能力。实验显示，该方法显著提高了交互成功率和导航效率。


<details>
  <summary>Details</summary>
Motivation: 提升模拟环境中智能体与对象的交互能力，探索视觉基础模型与强化学习的结合潜力。

Method: 整合Segment Anything Model (SAM)和YOLOv5作为视觉基础模型，结合PPO算法在AI2-THOR环境中训练智能体。

Result: 实验结果显示，平均累积奖励提升68%，对象交互成功率提高52.5%，导航效率增加33%。

Conclusion: 结合视觉基础模型与强化学习能显著提升复杂机器人任务的性能，为更先进的自主智能体铺平道路。

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [107] [Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration](https://arxiv.org/abs/2508.05936)
*Haohui Pan,Takuya Kiyokawa,Tomoki Ishikura,Shingo Hamada,Genichiro Matsuda,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于模块化真空夹具的系统，利用软气球夹持器适应复杂几何形状，提高螺丝拆卸任务的稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统刚性夹具无法适应小型家电的复杂曲面几何形状，导致拆卸任务困难。

Method: 开发了稳定性感知规划框架，通过采样目标物体底面、筛选接触点并评估支撑配置，使用气球夹持器实现稳定支撑。

Result: 与传统刚性夹具相比，新系统在螺丝拆卸任务中表现出更高的成功率和稳定性。

Conclusion: 模块化真空夹具系统显著提升了复杂几何形状物体的拆卸效率和稳定性。

Abstract: The disassembly of small household appliances poses significant challenges
due to their complex and curved geometries, which render traditional rigid
fixtures inadequate. In this paper, we propose a modular vacuum-based fixturing
system that leverages commercially available balloon-type soft grippers to
conform to arbitrarily shaped surfaces and provide stable support during
screw-removal tasks. To enable a reliable deployment of the system, we develop
a stability-aware planning framework that samples the bottom surface of the
target object, filters candidate contact points based on geometric continuity,
and evaluates support configurations using convex hull-based static stability
criteria. We compare the quality of object placement under different numbers
and configurations of balloon hands. In addition, real-world experiments were
conducted to compare the success rates of traditional rigid fixtures with our
proposed system. The results demonstrate that our method consistently achieves
higher success rates and superior placement stability during screw removal
tasks.

</details>


### [108] [Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts](https://arxiv.org/abs/2508.05937)
*Gen Sako,Takuya Kiyokawa,Kensuke Harada,Tomoki Ishikura,Naoya Miyaji,Genichiro Matsuda*

Main category: cs.RO

TL;DR: 提出了一种基于视觉引导的远程操作系统，用于解决机器人拆卸配对零件时的灵活操作和内部结构不可见问题。


<details>
  <summary>Details</summary>
Motivation: 机器人拆卸配对零件时面临灵活操作和内部结构不可见的挑战，需要一种直观的解决方案。

Method: 采用视觉引导的远程操作系统，结合虚拟环境中的可行抓取位姿和拆卸方向，并集成混合控制器（位置和阻抗控制）。

Result: 实验验证了系统的有效性，提高了任务成功率并减少了物体位姿偏差。

Conclusion: 该系统为配对零件的拆卸提供了一种直观且高效的解决方案。

Abstract: Robotic non-destructive disassembly of mating parts remains challenging due
to the need for flexible manipulation and the limited visibility of internal
structures. This study presents an affordance-guided teleoperation system that
enables intuitive human demonstrations for dual-arm fix-and-disassemble tasks
for mating parts. The system visualizes feasible grasp poses and disassembly
directions in a virtual environment, both derived from the object's geometry,
to address occlusions and structural complexity. To prevent excessive position
tracking under load when following the affordance, we integrate a hybrid
controller that combines position and impedance control into the teleoperated
disassembly arm. Real-world experiments validate the effectiveness of the
proposed system, showing improved task success rates and reduced object pose
deviation.

</details>


### [109] [Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution](https://arxiv.org/abs/2508.05941)
*Zhanyi Sun,Shuran Song*

Main category: cs.RO

TL;DR: Latent Policy Barrier (LPB) 是一种通过隐式屏障分离专家分布内外的状态，提升视觉运动策略鲁棒性和数据效率的框架。


<details>
  <summary>Details</summary>
Motivation: 行为克隆训练的视觉运动策略容易受到协变量偏移的影响，传统方法如人工校正或数据增强成本高或效果有限。

Method: LPB 将专家演示的隐式嵌入作为屏障，分离安全与不安全状态，并分为基础扩散策略和动态模型两部分。

Result: 实验表明，LPB 提升了策略鲁棒性和数据效率，无需额外人工干预。

Conclusion: LPB 通过隐式屏障和模块化设计，有效解决了协变量偏移问题。

Abstract: Visuomotor policies trained via behavior cloning are vulnerable to covariate
shift, where small deviations from expert trajectories can compound into
failure. Common strategies to mitigate this issue involve expanding the
training distribution through human-in-the-loop corrections or synthetic data
augmentation. However, these approaches are often labor-intensive, rely on
strong task assumptions, or compromise the quality of imitation. We introduce
Latent Policy Barrier, a framework for robust visuomotor policy learning.
Inspired by Control Barrier Functions, LPB treats the latent embeddings of
expert demonstrations as an implicit barrier separating safe, in-distribution
states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the
role of precise expert imitation and OOD recovery into two separate modules: a
base diffusion policy solely on expert data, and a dynamics model trained on
both expert and suboptimal policy rollout data. At inference time, the dynamics
model predicts future latent states and optimizes them to stay within the
expert distribution. Both simulated and real-world experiments show that LPB
improves both policy robustness and data efficiency, enabling reliable
manipulation from limited expert data and without additional human correction
or annotation.

</details>


### [110] [Social and Telepresence Robots for Accessibility and Inclusion in Small Museums](https://arxiv.org/abs/2508.05946)
*Nello Balossino,Rossana Damiano,Cristina Gena,Alberto Lillo,Anna Maria Marras,Claudio Mattutino,Antonio Pizzo,Alessia Prin,Fabiana Vernero*

Main category: cs.RO

TL;DR: ROBSO-PM项目旨在通过社交机器人和远程社交机器人提升小型博物馆的可访问性，重点关注感知、文化和认知障碍。


<details>
  <summary>Details</summary>
Motivation: 许多博物馆存在可访问性障碍，尤其是在人口稀少地区，项目希望通过技术手段改善这一问题。

Method: 项目以三个博物馆为案例研究，探索机器人作为导览工具和远程访问工具的应用，研究内容包括故事讲述、机器人个性化和协作。

Result: 项目探讨了机器人在提升博物馆可访问性方面的潜力，特别是在支持包容性参观和远程访问方面。

Conclusion: 社交机器人和远程社交机器人有望成为改善小型博物馆可访问性的有效工具。

Abstract: There are still many museums that present accessibility barriers,
particularly regarding perceptual, cultural, and cognitive aspects. This is
especially evident in low-density population areas. The aim of the ROBSO-PM
project is to improve the accessibility of small museums through the use of
social robots and social telepresence robots, focusing on three museums as case
studies: the Museum of the Holy Shroud in Turin, a small but globally known
institution, and two lesser known mountain museums: the Museum of the Champlas
du Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and
Traditions. The project explores two main applications for robots: as guides
supporting inclusive visits for foreign or disabled visitors, and as
telepresence tools allowing people with limited mobility to access museums
remotely. From a research perspective, key topics include storytelling, robot
personality, empathy, personalization, and, in the case of telepresence,
collaboration between the robot and the person, with clearly defined roles and
autonomy.

</details>


### [111] [Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles](https://arxiv.org/abs/2508.05972)
*Shaoting Liu,Zhou Liu*

Main category: cs.RO

TL;DR: 本文提出了一种扰动感知的规划框架，用于增强双模态车辆在复杂环境中的轨迹规划鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 双模态车辆结合了空中和地面移动的优势，但在复杂环境中易受扰动影响，需要更鲁棒的规划方法。

Method: 框架结合实时扰动估计，动态调整安全边界，并利用车辆动力学模型进行自适应轨迹优化。

Result: 实验验证了方法在跟踪精度、任务效率和能源性能上的提升，尤其在扰动环境下表现优异。

Conclusion: 该方法显著提高了双模态车辆在复杂环境中的规划鲁棒性和适应性。

Abstract: Air-land bimodal vehicles provide a promising solution for navigating complex
environments by combining the flexibility of aerial locomotion with the energy
efficiency of ground mobility. To enhance the robustness of trajectory planning
under environmental disturbances, this paper presents a disturbance-aware
planning framework that incorporates real-time disturbance estimation into both
path searching and trajectory optimization. A key component of the framework is
a disturbance-adaptive safety boundary adjustment mechanism, which dynamically
modifies the vehicle's feasible dynamic boundaries based on estimated
disturbances to ensure trajectory feasibility. Leveraging the dynamics model of
the bimodal vehicle, the proposed approach achieves adaptive and reliable
motion planning across different terrains and operating conditions. A series of
real-world experiments and benchmark comparisons on a custom-built platform
validate the effectiveness and robustness of the method, demonstrating
improvements in tracking accuracy, task efficiency, and energy performance
under both ground and aerial disturbances.

</details>


### [112] [ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference](https://arxiv.org/abs/2508.06053)
*Kaixuan Wu,Yuanzhuo Xu,Zejun Zhang,Weiping Zhu,Steve Drew,Xiaoguang Niu*

Main category: cs.RO

TL;DR: ReNiL是一个基于贝叶斯深度学习的框架，用于高效、准确且具有不确定性感知的行人惯性定位，通过引入IPDPs和ASLE技术，显著提升了定位精度和不确定性一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖于固定滑动窗口积分，难以适应多样化的运动尺度和节奏，且不确定性不一致，限制了实际应用。

Method: ReNiL结合了运动感知方向滤波器和Any-Scale Laplace Estimator（ASLE），通过IPDPs在上下文有意义的路点上估计运动，支持任意尺度的IMU序列推理。

Result: 在RoNIN-ds和WUDataset上，ReNiL在位移精度和不确定性一致性上达到最先进水平，计算量更低。

Conclusion: ReNiL为移动和IoT定位提供了可扩展且不确定性感知的基础，具有实际应用的鲁棒性和实用性。

Abstract: Pedestrian inertial localization is key for mobile and IoT services because
it provides infrastructure-free positioning. Yet most learning-based methods
depend on fixed sliding-window integration, struggle to adapt to diverse motion
scales and cadences, and yield inconsistent uncertainty, limiting real-world
use. We present ReNiL, a Bayesian deep-learning framework for accurate,
efficient, and uncertainty-aware pedestrian localization. ReNiL introduces
Inertial Positioning Demand Points (IPDPs) to estimate motion at contextually
meaningful waypoints instead of dense tracking, and supports inference on IMU
sequences at any scale so cadence can match application needs. It couples a
motion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a
dual-task network that blends patch-based self-supervision with Bayesian
regression. By modeling displacements with a Laplace distribution, ReNiL
provides homogeneous Euclidean uncertainty that integrates cleanly with other
sensors. A Bayesian inference chain links successive IPDPs into consistent
trajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor
motion from 28 participants, ReNiL achieves state-of-the-art displacement
accuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN
variants while reducing computation. Application studies further show
robustness and practicality for mobile and IoT localization, making ReNiL a
scalable, uncertainty-aware foundation for next-generation positioning.

</details>


### [113] [Incremental Language Understanding for Online Motion Planning of Robot Manipulators](https://arxiv.org/abs/2508.06095)
*Mitchell Abrams,Thies Oelerich,Christian Hartl-Nesic,Andreas Kugi,Matthias Scheutz*

Main category: cs.RO

TL;DR: 提出了一种基于推理的增量解析器，将在线运动规划算法集成到认知架构中，实现实时语言输入下的连续适应。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设指令完全指定，导致修正或澄清时需停止并重新规划，效率低下。

Method: 结合符号推理与在线运动规划，维护多个候选解析，动态调整运动计划。

Result: 在真实人机交互场景中验证了系统对动态语言输入的适应性。

Conclusion: 增量语言理解与实时运动规划的结合提升了人机协作的流畅性。

Abstract: Human-robot interaction requires robots to process language incrementally,
adapting their actions in real-time based on evolving speech input. Existing
approaches to language-guided robot motion planning typically assume fully
specified instructions, resulting in inefficient stop-and-replan behavior when
corrections or clarifications occur. In this paper, we introduce a novel
reasoning-based incremental parser which integrates an online motion planning
algorithm within the cognitive architecture. Our approach enables continuous
adaptation to dynamic linguistic input, allowing robots to update motion plans
without restarting execution. The incremental parser maintains multiple
candidate parses, leveraging reasoning mechanisms to resolve ambiguities and
revise interpretations when needed. By combining symbolic reasoning with online
motion planning, our system achieves greater flexibility in handling speech
corrections and dynamically changing constraints. We evaluate our framework in
real-world human-robot interaction scenarios, demonstrating online adaptions of
goal poses, constraints, or task objectives. Our results highlight the
advantages of integrating incremental language understanding with real-time
motion planning for natural and fluid human-robot collaboration. The
experiments are demonstrated in the accompanying video at
www.acin.tuwien.ac.at/42d5.

</details>


### [114] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: 提出一种基于变分自编码器的新颖性检测方法，提升视觉世界模型的鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉世界模型对训练质量敏感，需覆盖完整动作和状态空间以避免推理时发散。

Method: 使用变分自编码器作为新颖性检测器，确保规划中的动作轨迹不偏离训练数据分布。

Result: 在模拟机器人环境中实验，证明该方法提高了数据效率。

Conclusion: 该方法优于现有解决方案，增强了模型预测控制的鲁棒性。

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [115] [Beyond Constant Parameters: Hyper Prediction Models and HyperMPC](https://arxiv.org/abs/2508.06181)
*Jan Węgrzynowski,Piotr Kicki,Grzegorz Czechmanowski,Maciej Krupka,Krzysztof Walas*

Main category: cs.RO

TL;DR: 提出了一种基于时间依赖动态模型的Hyper Prediction Model（HyperPM），通过神经网络学习时间变化参数，显著降低了长时预测误差，并在MPC框架中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的MPC动态模型受限于计算复杂性和状态表示，需要一种更高效且能预测未建模现象的方法。

Method: 提出HyperPM，将未建模动态投影到时间依赖的动态模型中，通过神经网络学习时间变化参数。

Result: 在F1TENTH自动驾驶赛车等系统中验证，显著减少长时预测误差，MPC框架中表现优于现有技术。

Conclusion: HyperPM在保持计算效率的同时提升了动态模型的预测能力，为机器人控制提供了更可靠的解决方案。

Abstract: Model Predictive Control (MPC) is among the most widely adopted and reliable
methods for robot control, relying critically on an accurate dynamics model.
However, existing dynamics models used in the gradient-based MPC are limited by
computational complexity and state representation. To address this limitation,
we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we
project the unmodeled dynamics onto a time-dependent dynamics model. This
time-dependency is captured through time-varying model parameters, whose
evolution over the MPC prediction horizon is learned using a neural network.
Such formulation preserves the computational efficiency and robustness of the
base model while equipping it with the capacity to anticipate previously
unmodeled phenomena. We evaluated the proposed approach on several challenging
systems, including real-world F1TENTH autonomous racing, and demonstrated that
it significantly reduces long-horizon prediction errors. Moreover, when
integrated within the MPC framework (HyperMPC), our method consistently
outperforms existing state-of-the-art techniques.

</details>


### [116] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: Affordance-R1提出了一种统一的affordance grounding框架，结合认知CoT和GRPO强化学习，解决了现有模型缺乏推理能力和OOD泛化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型因缺乏Chain-of-Thought推理能力，难以共享不同对象的affordance，限制了OOD泛化和显式推理能力。

Method: 提出Affordance-R1框架，整合认知CoT和GRPO强化学习，设计包含格式、感知和认知奖励的affordance函数，并构建ReasonAff数据集支持训练。

Result: Affordance-R1在零样本泛化和推理能力上表现优异，实验证明其优于现有方法并具有开放世界泛化能力。

Conclusion: Affordance-R1首次将GRPO强化学习与推理结合，为affordance reasoning领域提供了新思路，代码和数据集已开源。

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [117] [Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization](https://arxiv.org/abs/2508.06207)
*Andrea Dal Prete,Seyram Ofori,Chan Yon Sin,Ashwin Narayan,Francesco Braghin,Marta Gandolla,Haoyong Yu*

Main category: cs.RO

TL;DR: 研究提出了一种基于肌肉活动减少、不适感和用户偏好的优化空间，开发了实时负载估计的自适应控制方法，显著降低了背部肌肉激活峰值。


<details>
  <summary>Details</summary>
Motivation: 解决背部外骨骼支持策略优化和自适应控制的挑战，以提升其工业应用效果。

Method: 构建优化空间，开发基于视觉的自适应控制管道，实时估计负载并动态调整支持。

Result: 实验显示自适应控制将背部肌肉激活峰值降低23%，准确率超80%，且用户偏好和不适感均改善。

Conclusion: 验证了智能、上下文感知控制在工业外骨骼中的潜力。

Abstract: Back exoskeletons can reduce musculoskeletal strain, but their effectiveness
depends on support modulation and adaptive control. This study addresses two
challenges: defining optimal support strategies and developing adaptive control
based on payload estimation. We introduce an optimization space based on muscle
activity reduction, perceived discomfort, and user preference, constructing
functions to identify optimal strategies. Experiments with 12 subjects revealed
optimal operating regions, highlighting the need for dynamic modulation. Based
on these insights, we developed a vision-based adaptive control pipeline that
estimates payloads in real-time by enhancing exoskeleton contextual
understanding, minimising latency and enabling support adaptation within the
defined optimisation space. Validation with 12 more subjects showed over 80%
accuracy and improvements across all metrics. Compared to static control,
adaptive modulation reduced peak back muscle activation by up to 23% while
preserving user preference and minimising discomfort. These findings validate
the proposed framework and highlight the potential of intelligent,
context-aware control in industrial exoskeletons.

</details>


### [118] [REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance](https://arxiv.org/abs/2508.06229)
*Zihao Xu,Ce Hao,Chunzheng Wang,Kuankuan Sima,Fan Shi,Jin Song Dong*

Main category: cs.RO

TL;DR: 本文提出了一种名为REBot的控制框架，用于四足机器人在动态障碍物环境中的实时反射性避障。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖导航轨迹重规划，反应时间不足，无法应对快速接近的障碍物，因此需要一种低延迟的反射性避障能力。

Method: REBot结合避障策略和恢复策略，通过有限状态机实现，并采用精心设计的学习课程、正则化和自适应奖励。

Result: 实验表明，REBot在避障成功率、能效和应对快速移动障碍物的鲁棒性方面均有显著提升。

Conclusion: REBot为四足机器人提供了高效的实时反射性避障解决方案。

Abstract: Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating
in environments with moving obstacles or humans. Existing approaches typically
rely on navigation-based trajectory replanning, which assumes sufficient
reaction time and leading to fails when obstacles approach rapidly. In such
scenarios, quadrupedal robots require reflexive evasion capabilities to perform
instantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion
Robot (REBot), a control framework that enables quadrupedal robots to achieve
real-time reflexive obstacle avoidance. REBot integrates an avoidance policy
and a recovery policy within a finite-state machine. With carefully designed
learning curricula and by incorporating regularization and adaptive rewards,
REBot achieves robust evasion and rapid stabilization in instantaneous DOA
tasks. We validate REBot through extensive simulations and real-world
experiments, demonstrating notable improvements in avoidance success rates,
energy efficiency, and robustness to fast-moving obstacles. Videos and appendix
are available on https://rebot-2025.github.io/.

</details>


### [119] [ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints](https://arxiv.org/abs/2508.06266)
*Zezeng Li,Rui Yang,Ruochen Chen,ZhongXuan Luo,Liming Chen*

Main category: cs.RO

TL;DR: 提出了一种自适应扩散策略（ADP），通过引入几何流形约束和解析引导初始化，优化了扩散策略在机器人操作中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在动作生成中忽略了几何和控制结构的先验知识，导致效率不足。

Method: ADP引入几何流形约束和解析引导初始化，优化扩散过程。

Result: 实验表明，ADP显著提高了成功率、泛化能力和采样效率，执行速度提升25%，成功率提高9%。

Conclusion: ADP无需重新训练即可适配新任务，提升了扩散策略的实用性和性能。

Abstract: Diffusion policies have recently emerged as a powerful class of visuomotor
controllers for robot manipulation, offering stable training and expressive
multi-modal action modeling. However, existing approaches typically treat
action generation as an unconstrained denoising process, ignoring valuable a
priori knowledge about geometry and control structure. In this work, we propose
the Adaptive Diffusion Policy (ADP), a test-time adaptation method that
introduces two key inductive biases into the diffusion. First, we embed a
geometric manifold constraint that aligns denoising updates with task-relevant
subspaces, leveraging the fact that the relative pose between the end-effector
and target scene provides a natural gradient direction, and guiding denoising
along the geodesic path of the manipulation manifold. Then, to reduce
unnecessary exploration and accelerate convergence, we propose an analytically
guided initialization: rather than sampling from an uninformative prior, we
compute a rough registration between the gripper and target scenes to propose a
structured initial noisy action. ADP is compatible with pre-trained diffusion
policies and requires no retraining, enabling test-time adaptation that tailors
the policy to specific tasks, thereby enhancing generalization across novel
tasks and environments. Experiments on RLBench, CALVIN, and real-world dataset
show that ADPro, an implementation of ADP, improves success rates,
generalization, and sampling efficiency, achieving up to 25% faster execution
and 9% points over strong diffusion baselines.

</details>


### [120] [EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators](https://arxiv.org/abs/2508.06276)
*Juan Heredia,Christian Schlette,Mikkel Baun Kjærgaard*

Main category: cs.RO

TL;DR: 提出了一种基于Matlab的开源库，用于自动生成机械臂的能耗模型，解决了现有模型局限于传统工业机器人和精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机械臂能耗模型主要针对传统工业机器人且精度不足，需改进。

Method: 使用Denavit-Hartenberg参数、质量、质心等输入，结合实时操作数据（如关节位置、速度、加速度、电功率等），开发数据驱动的Matlab库。

Result: 在四种轻量级机器人上验证，训练集RMSE为1.42-2.80 W，测试集为1.45-5.25 W。

Conclusion: 该方法有效提升了机械臂能耗模型的通用性和精度。

Abstract: Existing literature proposes models for estimating the electrical power of
manipulators, yet two primary limitations prevail. First, most models are
predominantly tested using traditional industrial robots. Second, these models
often lack accuracy. To address these issues, we introduce an open source
Matlab-based library designed to automatically generate \ac{ec} models for
manipulators. The necessary inputs for the library are Denavit-Hartenberg
parameters, link masses, and centers of mass. Additionally, our model is
data-driven and requires real operational data, including joint positions,
velocities, accelerations, electrical power, and corresponding timestamps. We
validated our methodology by testing on four lightweight robots sourced from
three distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The
model underwent testing, and the results demonstrated an RMSE ranging from 1.42
W to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing
dataset.

</details>


### [121] [Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs](https://arxiv.org/abs/2508.06278)
*Petr Novak,Stefan Biffl,Marek Obitko,Petr Kadera*

Main category: cs.RO

TL;DR: 提出了一种名为PPR-AKG的语义模型，用于分析和缓解工业4.0中灵活CPPS的不良条件，结合LLM提供自然语言交互。


<details>
  <summary>Details</summary>
Motivation: 工业4.0的灵活性破坏了传统质量保证机制，需要新的方法来分析不良条件。

Method: 基于PPR模型构建OWL本体，结合语义技术和LLM，提供自然语言接口。

Result: 在电动汽车电池再制造案例中，PPR-AKG有效支持资源分配和不良条件识别与缓解。

Conclusion: PPR-AKG模型和LLM的结合为CPPS提供了多维知识表示和直观交互方式。

Abstract: Contemporary industrial cyber-physical production systems (CPPS) composed of
robotic workcells face significant challenges in the analysis of undesired
conditions due to the flexibility of Industry 4.0 that disrupts traditional
quality assurance mechanisms. This paper presents a novel industry-oriented
semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),
which is designed to analyze and mitigate undesired conditions in flexible
CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model
originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses
shortcomings of conventional model-driven engineering for CPPS, particularly
inadequate undesired condition and error handling representation. The
integration of semantic technologies with large language models (LLMs) provides
intuitive interfaces for factory operators, production planners, and engineers
to interact with the entire model using natural language. Evaluation with the
use case addressing electric vehicle battery remanufacturing demonstrates that
the PPR-AKG approach efficiently supports resource allocation based on
explicitly represented capabilities as well as identification and mitigation of
undesired conditions in production. The key contributions include (1) a
holistic PPR-AKG model capturing multi-dimensional production knowledge, and
(2) the useful combination of the PPR-AKG with LLM-based chatbots for human
interaction.

</details>


### [122] [Situationally-aware Path Planning Exploiting 3D Scene Graphs](https://arxiv.org/abs/2508.06283)
*Saad Ejaz,Marco Giberna,Muhammad Shaheer,Jose Andres Millan-Romera,Ali Tourani,Paul Kremer,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: S-Path利用3D场景图的结构提升路径规划效率和可解释性，通过两阶段规划和子问题分解实现高效并行求解。


<details>
  <summary>Details</summary>
Motivation: 3D场景图的结构未被充分利用于提升路径规划效率和可解释性，S-Path旨在解决这一问题。

Method: 采用两阶段规划：先在语义图上搜索生成高层路径，再分解为独立子问题并行求解，并引入重规划机制。

Result: 实验显示，S-Path平均减少5.7倍规划时间，路径最优性与传统方法相当，复杂场景表现更优。

Conclusion: S-Path是一种高效且可解释的路径规划方法，适用于室内3D场景图环境。

Abstract: 3D Scene Graphs integrate both metric and semantic information, yet their
structure remains underutilized for improving path planning efficiency and
interpretability. In this work, we present S-Path, a situationally-aware path
planner that leverages the metric-semantic structure of indoor 3D Scene Graphs
to significantly enhance planning efficiency. S-Path follows a two-stage
process: it first performs a search over a semantic graph derived from the
scene graph to yield a human-understandable high-level path. This also
identifies relevant regions for planning, which later allows the decomposition
of the problem into smaller, independent subproblems that can be solved in
parallel. We also introduce a replanning mechanism that, in the event of an
infeasible path, reuses information from previously solved subproblems to
update semantic heuristics and prioritize reuse to further improve the
efficiency of future planning attempts. Extensive experiments on both
real-world and simulated environments show that S-Path achieves average
reductions of 5.7x in planning time while maintaining comparable path
optimality to classical sampling-based planners and surpassing them in complex
scenarios, making it an efficient and interpretable path planner for
environments represented by indoor 3D Scene Graphs.

</details>


### [123] [Real-Time 3D Vision-Language Embedding Mapping](https://arxiv.org/abs/2508.06291)
*Christian Rauch,Björn Ellensohn,Linus Nwankwo,Vedant Dave,Elmar Rueckert*

Main category: cs.RO

TL;DR: 提出了一种将2D视觉语言模型嵌入到3D表示中的实时方法，结合局部嵌入掩码策略和置信度加权3D集成，实现了高精度语义3D表示。


<details>
  <summary>Details</summary>
Motivation: 为机器人任务提供高精度的语义3D表示，支持自然语言交互。

Method: 采用局部嵌入掩码策略和置信度加权3D集成，优化嵌入分布和可靠性。

Result: 在真实场景中验证了方法的准确性，提升了目标定位性能并满足实时性要求。

Conclusion: 该方法适用于多种交互式机器人任务，仅需原始图像数据。

Abstract: A metric-accurate semantic 3D representation is essential for many robotic
tasks. This work proposes a simple, yet powerful, way to integrate the 2D
embeddings of a Vision-Language Model in a metric-accurate 3D representation at
real-time. We combine a local embedding masking strategy, for a more distinct
embedding distribution, with a confidence-weighted 3D integration for more
reliable 3D embeddings. The resulting metric-accurate embedding representation
is task-agnostic and can represent semantic concepts on a global multi-room, as
well as on a local object-level. This enables a variety of interactive robotic
applications that require the localisation of objects-of-interest via natural
language. We evaluate our approach on a variety of real-world sequences and
demonstrate that these strategies achieve a more accurate object-of-interest
localisation while improving the runtime performance in order to meet our
real-time constraints. We further demonstrate the versatility of our approach
in a variety of interactive handheld, mobile robotics and manipulation tasks,
requiring only raw image data.

</details>


### [124] [Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots](https://arxiv.org/abs/2508.06295)
*Juan Heredia,Emil Stubbe Kolvig-Raun,Sune Lundo Sorensen,Mikkel Baun Kjaergaard*

Main category: cs.RO

TL;DR: 该研究提出了一种基于机器人电力功率分析的新框架，用于从体现角度评估机器人程序性能，替代传统的CPU指标。


<details>
  <summary>Details</summary>
Motivation: 传统CPU指标忽略了代码对机器人行为的物理影响，研究旨在通过电力功率分析更全面地评估程序性能。

Method: 采用归一化指标（能量利用系数、能量转换指标和可靠性系数）及机器人磨损指标，分析任务执行中的能量效率和可靠性。

Result: 通过UR5e机器人实验比较四种程序策略，揭示了各策略的优缺点，为优化编程实践提供了依据。

Conclusion: 基于体现的电力功率分析方法不仅提升机器人性能，还支持可持续制造和成本降低等工业目标。

Abstract: The code performance of industrial robots is typically analyzed through CPU
metrics, which overlook the physical impact of code on robot behavior. This
study introduces a novel framework for assessing robot program performance from
an embodiment perspective by analyzing the robot's electrical power profile.
Our approach diverges from conventional CPU based evaluations and instead
leverages a suite of normalized metrics, namely, the energy utilization
coefficient, the energy conversion metric, and the reliability coefficient, to
capture how efficiently and reliably energy is used during task execution.
Complementing these metrics, the established robot wear metric provides further
insight into long term reliability. Our approach is demonstrated through an
experimental case study in machine tending, comparing four programs with
diverse strategies using a UR5e robot. The proposed metrics directly compare
and categorize different robot programs, regardless of the specific task, by
linking code performance to its physical manifestation through power
consumption patterns. Our results reveal the strengths and weaknesses of each
strategy, offering actionable insights for optimizing robot programming
practices. Enhancing energy efficiency and reliability through this embodiment
centric approach not only improves individual robot performance but also
supports broader industrial objectives such as sustainable manufacturing and
cost reduction.

</details>


### [125] [Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators](https://arxiv.org/abs/2508.06313)
*Amir Hossein Barjini,Mohammad Bahari,Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出了一种用于全电动重型机械臂的统一系统级建模与控制框架，结合了增强的代理模型和虚拟分解控制架构，实现了高精度跟踪和实时控制。


<details>
  <summary>Details</summary>
Motivation: 为全电动重型机械臂（HDRM）开发一种模块化、实时控制的框架，以支持其在下一代移动工作机器中的应用。

Method: 结合电机械线性执行器（EMLA）的代理增强模型与虚拟分解控制（VDC）架构，并引入自然适应律，构建分层控制结构。

Result: 在多域仿真和实验验证中，控制器实现了亚厘米级的笛卡尔跟踪精度。

Conclusion: 代理增强的EMLA模型与VDC方法结合，能够实现全电动HDRM的模块化实时控制，具有实际应用潜力。

Abstract: This paper presents a unified system-level modeling and control framework for
an all-electric heavy-duty robotic manipulator (HDRM) driven by
electromechanical linear actuators (EMLAs). A surrogate-enhanced actuator
model, combining integrated electromechanical dynamics with a neural network
trained on a dedicated testbed, is integrated into an extended virtual
decomposition control (VDC) architecture augmented by a natural adaptation law.
The derived analytical HDRM model supports a hierarchical control structure
that seamlessly maps high-level force and velocity objectives to real-time
actuator commands, accompanied by a Lyapunov-based stability proof. In
multi-domain simulations of both cubic and a custom planar triangular
trajectory, the proposed adaptive modular controller achieves sub-centimeter
Cartesian tracking accuracy. Experimental validation of the same 1-DoF platform
under realistic load emulation confirms the efficacy of the proposed control
strategy. These findings demonstrate that a surrogate-enhanced EMLA model
embedded in the VDC approach can enable modular, real-time control of an
all-electric HDRM, supporting its deployment in next-generation mobile working
machines.

</details>


### [126] [Towards Balanced Behavior Cloning from Imbalanced Datasets](https://arxiv.org/abs/2508.06319)
*Sagar Parekh,Heramb Nemlekar,Dylan P. Losey*

Main category: cs.RO

TL;DR: 论文分析了模仿学习中数据集不平衡的问题，提出自动调整数据权重的方法，并引入一种新的元梯度重平衡算法。


<details>
  <summary>Details</summary>
Motivation: 人类演示的数据集通常不平衡，导致现有方法过分关注高频行为，而忽视低频但重要的行为。

Method: 提出算法重新平衡离线数据集，分析不同方法的优缺点，并引入元梯度重平衡算法。

Result: 实验证明数据集重平衡能提升模仿学习算法的性能，无需额外数据收集。

Conclusion: 数据集重平衡是提升模仿学习性能的有效方法，新算法解决了现有方法的局限性。

Abstract: Robots should be able to learn complex behaviors from human demonstrations.
In practice, these human-provided datasets are inevitably imbalanced: i.e., the
human demonstrates some subtasks more frequently than others. State-of-the-art
methods default to treating each element of the human's dataset as equally
important. So if -- for instance -- the majority of the human's data focuses on
reaching a goal, and only a few state-action pairs move to avoid an obstacle,
the learning algorithm will place greater emphasis on goal reaching. More
generally, misalignment between the relative amounts of data and the importance
of that data causes fundamental problems for imitation learning approaches. In
this paper we analyze and develop learning methods that automatically account
for mixed datasets. We formally prove that imbalanced data leads to imbalanced
policies when each state-action pair is weighted equally; these policies
emulate the most represented behaviors, and not the human's complex, multi-task
demonstrations. We next explore algorithms that rebalance offline datasets
(i.e., reweight the importance of different state-action pairs) without human
oversight. Reweighting the dataset can enhance the overall policy performance.
However, there is no free lunch: each method for autonomously rebalancing
brings its own pros and cons. We formulate these advantages and disadvantages,
helping other researchers identify when each type of approach is most
appropriate. We conclude by introducing a novel meta-gradient rebalancing
algorithm that addresses the primary limitations behind existing approaches.
Our experiments show that dataset rebalancing leads to better downstream
learning, improving the performance of general imitation learning algorithms
without requiring additional data collection. See our project website:
https://collab.me.vt.edu/data_curation/.

</details>


### [127] [L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience](https://arxiv.org/abs/2508.06330)
*Baorun Li,Chengrui Zhu,Siyi Du,Bingran Chen,Jie Ren,Wenfei Wang,Yong Liu,Jiajun Lv*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的外参标定框架，无需结构化目标或强激励数据，通过直接优化SE(3)外参提升里程计精度。


<details>
  <summary>Details</summary>
Motivation: 现有外参标定方法依赖结构化目标或强激励数据，限制了实际应用；在线标定因弱激励导致估计不可靠。

Method: 将外参标定建模为决策问题，利用Bingham分布建模3D旋转，采用轨迹对齐奖励机制和无信息样本过滤模块。

Result: 在无人机、无人车和手持设备上实验表明，该方法优于传统优化方法，在弱激励条件下仍能实现高精度标定。

Conclusion: 该框架简化了多机器人平台部署，无需高质量初始外参，仅需常规操作数据即可完成标定。

Abstract: Extrinsic calibration is essential for multi-sensor fusion, existing methods
rely on structured targets or fully-excited data, limiting real-world
applicability. Online calibration further suffers from weak excitation, leading
to unreliable estimates. To address these limitations, we propose a
reinforcement learning (RL)-based extrinsic calibration framework that
formulates extrinsic calibration as a decision-making problem, directly
optimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach
leverages a probabilistic Bingham distribution to model 3D rotations, ensuring
stable optimization while inherently retaining quaternion symmetry. A
trajectory alignment reward mechanism enables robust calibration without
structured targets by quantitatively evaluating estimated tightly-coupled
trajectory against a reference trajectory. Additionally, an automated data
selection module filters uninformative samples, significantly improving
efficiency and scalability for large-scale datasets. Extensive experiments on
UAVs, UGVs, and handheld platforms demonstrate that our method outperforms
traditional optimization-based approaches, achieving high-precision calibration
even under weak excitation conditions. Our framework simplifies deployment on
diverse robotic platforms by eliminating the need for high-quality initial
extrinsics and enabling calibration from routine operating data. The code is
available at https://github.com/APRIL-ZJU/learn-to-calibrate.

</details>


### [128] [V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles](https://arxiv.org/abs/2508.06404)
*Abdullah Zareh Andaryan,Michael G. H. Bell,Mohsen Ramezani,Glenn Geers*

Main category: cs.RO

TL;DR: V*是一种基于图的运动规划器，通过时空速度网格直接整合速度和方向状态变量，实现动态可行的高效轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 解决结构化环境中自动驾驶车辆需要生成时间最优、无碰撞且满足动态和运动学约束的轨迹问题。

Method: 采用六边形离散化策略和动态图生成技术，结合数学建模和几何剪枝策略，确保动态可行性。

Result: 在模拟实验中，V*能够有效避免冲突、主动让行，并生成安全高效的轨迹。

Conclusion: V*通过直接整合运动维度，提供了一种无需后处理的动态可行轨迹规划方法。

Abstract: Autonomous vehicle navigation in structured environments requires planners
capable of generating time-optimal, collision-free trajectories that satisfy
dynamic and kinematic constraints. We introduce V*, a graph-based motion
planner that represents speed and direction as explicit state variables within
a discretised space-time-velocity lattice. Unlike traditional methods that
decouple spatial search from dynamic feasibility or rely on post-hoc smoothing,
V* integrates both motion dimensions directly into graph construction through
dynamic graph generation during search expansion. To manage the complexity of
high-dimensional search, we employ a hexagonal discretisation strategy and
provide formal mathematical proofs establishing optimal waypoint spacing and
minimal node redundancy under constrained heading transitions for
velocity-aware motion planning. We develop a mathematical formulation for
transient steering dynamics in the kinematic bicycle model, modelling steering
angle convergence with exponential behaviour, and deriving the relationship for
convergence rate parameters. This theoretical foundation, combined with
geometric pruning strategies that eliminate expansions leading to infeasible
steering configurations, enables V* to evaluate dynamically admissible
manoeuvres, ensuring each trajectory is physically realisable without further
refinement. We further demonstrate V*'s performance in simulation studies with
cluttered and dynamic environments involving moving obstacles, showing its
ability to avoid conflicts, yield proactively, and generate safe, efficient
trajectories with temporal reasoning capabilities for waiting behaviours and
dynamic coordination.

</details>


### [129] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: 论文研究了通用机器人策略在训练数据分布外泛化能力不足的原因，发现‘捷径学习’是主要障碍，并提出数据集收集和数据增强策略以改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略在大规模数据集（如OXE）上训练后表现良好，但在训练数据分布外泛化能力有限，需要探究其根本原因。

Method: 通过理论和实证分析，识别出‘捷径学习’及其两大成因：子数据集内部多样性不足和子数据集间分布差异导致的碎片化。

Result: 发现数据集结构和收集方式是泛化能力受限的关键，并提出数据收集和增强策略以减少捷径学习。

Conclusion: 优化数据集结构和采用数据增强策略可显著提升通用机器人策略的泛化能力，尤其在无法获取新数据时效果显著。

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [130] [Training chord recognition models on artificially generated audio](https://arxiv.org/abs/2508.05878)
*Martyna Majchrzak,Jacek Mańdziuk*

Main category: cs.SD

TL;DR: 研究比较了两种基于Transformer的神经网络模型在音频和弦序列识别中的表现，探讨了使用人工生成数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决音乐信息检索中非版权音频数据不足的问题。

Method: 使用人工生成音频（AAM）、Schubert数据集和McGill Billboard数据集训练模型，并通过Root、MajMin和CCM指标评估。

Result: 实验证明人工生成音乐在特定场景下可作为有效训练数据。

Conclusion: AAM可补充或替代人类创作音乐数据集，用于流行音乐和弦序列预测。

Abstract: One of the challenging problems in Music Information Retrieval is the
acquisition of enough non-copyrighted audio recordings for model training and
evaluation. This study compares two Transformer-based neural network models for
chord sequence recognition in audio recordings and examines the effectiveness
of using an artificially generated dataset for this purpose. The models are
trained on various combinations of Artificial Audio Multitracks (AAM),
Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated
with three metrics: Root, MajMin and Chord Content Metric (CCM). The
experiments prove that even though there are certainly differences in
complexity and structure between artificially generated and human-composed
music, the former can be useful in certain scenarios. Specifically, AAM can
enrich a smaller training dataset of music composed by a human or can even be
used as a standalone training set for a model that predicts chord sequences in
pop music, if no other data is available.

</details>


### [131] [DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching](https://arxiv.org/abs/2508.05978)
*Wei Chen,Binzhu Sha,Dan Luo,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu*

Main category: cs.SD

TL;DR: DAFMSVC是一种新的歌唱声音转换方法，通过替换自监督学习特征和双交叉注意力机制，显著提升了音色相似性和音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有歌唱声音转换方法中音色泄漏和音色相似性不足的问题。

Method: 使用自监督学习特征替换和双交叉注意力机制，结合流匹配模块生成高质量音频。

Result: 实验表明DAFMSVC在音色相似性和自然度上优于现有方法。

Conclusion: DAFMSVC有效解决了音色泄漏问题，提升了歌唱声音转换的性能。

Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target
while keeping melody and lyrics. The key challenge in any-to-any SVC is
adapting unseen speaker timbres to source audio without quality degradation.
Existing methods either face timbre leakage or fail to achieve satisfactory
timbre similarity and quality in the generated audio. To address these
challenges, we propose DAFMSVC, where the self-supervised learning (SSL)
features from the source audio are replaced with the most similar SSL features
from the target audio to prevent timbre leakage. It also incorporates a dual
cross-attention mechanism for the adaptive fusion of speaker embeddings,
melody, and linguistic content. Additionally, we introduce a flow matching
module for high quality audio generation from the fused features. Experimental
results show that DAFMSVC significantly enhances timbre similarity and
naturalness, outperforming state-of-the-art methods in both subjective and
objective evaluations.

</details>


### [132] [MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows](https://arxiv.org/abs/2508.06098)
*Xiquan Li,Junxi Liu,Yuzhe Liang,Zhikang Niu,Wenxi Chen,Xie Chen*

Main category: cs.SD

TL;DR: MeanAudio是一种基于MeanFlow的快速文本到音频生成模型，通过回归平均速度场实现高效生成，并在单步和多步生成中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音频生成系统虽然合成质量和可控性高，但推理速度慢，限制了实际应用。

Method: 采用Flux风格潜在变换器，结合无分类器指导和瞬时到平均课程学习策略，优化训练效率和生成质量。

Result: MeanAudio在单步生成中实现0.013的实时因子，比现有扩散模型快100倍，并在多步生成中表现优异。

Conclusion: MeanAudio在速度和生成质量上均达到先进水平，为文本到音频生成提供了高效解决方案。

Abstract: Recent developments in diffusion- and flow- based models have significantly
advanced Text-to-Audio Generation (TTA). While achieving great synthesis
quality and controllability, current TTA systems still suffer from slow
inference speed, which significantly limits their practical applicability. This
paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and
faithful text-to-audio generation. Built on a Flux-style latent transformer,
MeanAudio regresses the average velocity field during training, enabling fast
generation by mapping directly from the start to the endpoint of the flow
trajectory. By incorporating classifier-free guidance (CFG) into the training
target, MeanAudio incurs no additional cost in the guided sampling process. To
further stabilize training, we propose an instantaneous-to-mean curriculum with
flow field mix-up, which encourages the model to first learn the foundational
instantaneous dynamics, and then gradually adapt to mean flows. This strategy
proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art
performance in single-step audio generation. Specifically, it achieves a real
time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup
over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates
strong performance in multi-step generation, enabling smooth and coherent
transitions across successive synthesis steps.

</details>


### [133] [Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis](https://arxiv.org/abs/2508.06262)
*Wenjie Tian,Xinfa Zhu,Hanke Xie,Zhen Ye,Wei Xue,Lei Xie*

Main category: cs.SD

TL;DR: Llasa+是一种基于Llasa的加速流式TTS模型，通过多令牌预测模块和验证算法实现1.48倍加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有自回归结构和大型模型（如Llasa）在推理延迟和流式合成方面存在挑战。

Method: 引入两个多令牌预测模块和验证算法，设计因果解码器实现流式语音重建。

Result: Llasa+在LibriTTS上训练，实现1.48倍加速且不牺牲生成质量。

Conclusion: MTP和验证框架可加速任何基于LLM的模型，代码和模型已开源。

Abstract: Recent progress in text-to-speech (TTS) has achieved impressive naturalness
and flexibility, especially with the development of large language model
(LLM)-based approaches. However, existing autoregressive (AR) structures and
large-scale models, such as Llasa, still face significant challenges in
inference latency and streaming synthesis. To deal with the limitations, we
introduce Llasa+, an accelerated and streaming TTS model built on Llasa.
Specifically, to accelerate the generation process, we introduce two
plug-and-play Multi-Token Prediction (MTP) modules following the frozen
backbone. These modules allow the model to predict multiple tokens in one AR
step. Additionally, to mitigate potential error propagation caused by
inaccurate MTP, we design a novel verification algorithm that leverages the
frozen backbone to validate the generated tokens, thus allowing Llasa+ to
achieve speedup without sacrificing generation quality. Furthermore, we design
a causal decoder that enables streaming speech reconstruction from tokens.
Extensive experiments show that Llasa+ achieves a 1.48X speedup without
sacrificing generation quality, despite being trained only on LibriTTS.
Moreover, the MTP-and-verification framework can be applied to accelerate any
LLM-based model. All codes and models are publicly available at
https://github.com/ASLP-lab/LLaSA_Plus.

</details>


### [134] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: EmoAugNet是一种结合LSTM和1D-CNN的混合深度学习框架，用于语音情感识别（SER）。通过数据增强和特征提取，模型在IEMOCAP和RAVDESS数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互（HCI）效果，通过可靠的语音情感识别（SER）系统改进情感信号识别。

Method: 使用LSTM和1D-CNN的混合模型，结合传统和创新的数据增强策略，提取RMSE、MFCC和ZCR特征。

Result: 在IEMOCAP数据集上，ReLU激活的加权准确率为95.78%，ELU激活为96.75%；在RAVDESS数据集上，ReLU激活的加权准确率为94.53%，ELU激活为93.72%。

Conclusion: EmoAugNet通过数据增强和混合建模显著提升了SER系统的鲁棒性和性能。

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


### [135] [SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models](https://arxiv.org/abs/2508.06372)
*Han Yin,Yafeng Chen,Chong Deng,Luyao Cheng,Hui Wang,Chao-Hong Tan,Qian Chen,Wen Wang,Xiangang Li*

Main category: cs.SD

TL;DR: SpeakerLM是一个统一的多模态大型语言模型，用于端到端的说话人分离与识别（SDR），解决了传统级联系统的局限性，如错误传播和重叠语音处理困难。


<details>
  <summary>Details</summary>
Motivation: 现有SDR系统采用级联框架，存在错误传播、难以处理重叠语音及缺乏联合优化等问题，需要一种更高效的解决方案。

Method: 提出SpeakerLM，通过多阶段训练策略和灵活的说话人注册机制，联合优化SD和ASR任务。

Result: 实验表明SpeakerLM在数据扩展和泛化能力上优于现有级联系统，并在不同说话人注册条件下表现稳健。

Conclusion: SpeakerLM为SDR任务提供了一种高效、灵活的端到端解决方案，显著提升了性能。

Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke
when and what" within an audio clip, which is a crucial task in various
real-world multi-speaker scenarios such as meeting transcription and dialogue
systems. Existing SDR systems typically adopt a cascaded framework, combining
multiple modules such as speaker diarization (SD) and automatic speech
recognition (ASR). The cascaded systems suffer from several limitations, such
as error propagation, difficulty in handling overlapping speech, and lack of
joint optimization for exploring the synergy between SD and ASR tasks. To
address these limitations, we introduce SpeakerLM, a unified multimodal large
language model for SDR that jointly performs SD and ASR in an end-to-end
manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a
flexible speaker registration mechanism into SpeakerLM, enabling SDR under
different speaker registration settings. SpeakerLM is progressively developed
with a multi-stage training strategy on large-scale real data. Extensive
experiments show that SpeakerLM demonstrates strong data scaling capability and
generalizability, outperforming state-of-the-art cascaded baselines on both
in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental
results show that the proposed speaker registration mechanism effectively
ensures robust SDR performance of SpeakerLM across diverse speaker registration
conditions and varying numbers of registered speakers.

</details>


### [136] [Improved Dysarthric Speech to Text Conversion via TTS Personalization](https://arxiv.org/abs/2508.06391)
*Péter Mihajlik,Éva Székely,Piroska Barta,Máté Soma Kádár,Gergely Dobsinszki,László Tóth*

Main category: cs.SD

TL;DR: 研究通过合成语音和真实数据微调ASR模型，显著降低了匈牙利语重度构音障碍患者的语音识别错误率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在零样本情况下对构音障碍语音识别效果差，需改进以提升可访问性。

Method: 利用个性化TTS系统生成合成语音，结合真实数据微调ASR模型，控制构音障碍严重程度。

Result: 字符错误率从36-51%降至7.3%，合成语音贡献了18%的相对CER降低。

Conclusion: 个性化ASR系统可显著改善重度构音障碍患者的语音识别效果。

Abstract: We present a case study on developing a customized speech-to-text system for
a Hungarian speaker with severe dysarthria. State-of-the-art automatic speech
recognition (ASR) models struggle with zero-shot transcription of dysarthric
speech, yielding high error rates. To improve performance with limited real
dysarthric data, we fine-tune an ASR model using synthetic speech generated via
a personalized text-to-speech (TTS) system. We introduce a method for
generating synthetic dysarthric speech with controlled severity by leveraging
premorbidity recordings of the given speaker and speaker embedding
interpolation, enabling ASR fine-tuning on a continuum of impairments.
Fine-tuning on both real and synthetic dysarthric speech reduces the character
error rate (CER) from 36-51% (zero-shot) to 7.3%. Our monolingual
FastConformer_Hu ASR model significantly outperforms Whisper-turbo when
fine-tuned on the same data, and the inclusion of synthetic speech contributes
to an 18% relative CER reduction. These results highlight the potential of
personalized ASR systems for improving accessibility for individuals with
severe speech impairments.

</details>


### [137] [Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling](https://arxiv.org/abs/2508.06393)
*Md Asif Jalal,Luca Remaggi,Vasileios Moschopoulos,Thanasis Kotsiopoulos,Vandana Rajan,Karthikeyan Saravanan,Anastasis Drosou,Junho Heo,Hyuk Oh,Seokyeong Jeong*

Main category: cs.SD

TL;DR: 提出一种无需预先标注说话者的语音分离和说话人日志方法，通过双重训练管道和重叠频谱损失函数显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖目标说话者的先验知识或固定参与者数量，限制了灵活性。

Method: 采用双重训练管道学习抗噪声干扰的说话人表征，并设计重叠频谱损失函数优化重叠语音帧的日志准确性。

Result: 实验显示，相对当前SOTA基线，DER提升71%，cpWER提升69%。

Conclusion: 该方法显著提高了语音分离和说话人日志的性能，无需预先标注说话者。

Abstract: Traditional speech separation and speaker diarization approaches rely on
prior knowledge of target speakers or a predetermined number of participants in
audio signals. To address these limitations, recent advances focus on
developing enrollment-free methods capable of identifying targets without
explicit speaker labeling. This work introduces a new approach to train
simultaneous speech separation and diarization using automatic identification
of target speaker embeddings, within mixtures. Our proposed model employs a
dual-stage training pipeline designed to learn robust speaker representation
features that are resilient to background noise interference. Furthermore, we
present an overlapping spectral loss function specifically tailored for
enhancing diarization accuracy during overlapped speech frames. Experimental
results show significant performance gains compared to the current SOTA
baseline, achieving 71% relative improvement in DER and 69% in cpWER.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR是一个基于边缘AI的资源管理框架，通过压缩CQI数据并利用强化学习优化6G车载娱乐网络的调度和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统RRM技术难以处理6G车载网络中复杂且大量的CQI数据，因此需要一种高效的方法来优化资源管理。

Method: SCAR采用ML压缩技术（如聚类和RBF网络）减少CQI数据量，同时保留关键特征，并结合强化学习策略优化调度。

Result: SCAR将可行调度区域时间提升14%，减少不公平调度时间15%，并通过SAST聚类降低CQI聚类失真10%。

Conclusion: SCAR在动态车载网络中展现出良好的可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [139] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE是一种新型变分自编码器，通过结构方程建模设计，直接嵌入测量结构，实现可解释的潜在表示学习。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据中潜在表示学习的可解释性问题，特别是在科学和社会领域中理论驱动的潜在构造和测量有效性至关重要的情况下。

Method: SE-VAE结合结构方程建模，将潜在子空间与已知指标分组对齐，并引入全局干扰潜在变量以隔离构造特异性干扰变异。

Result: SE-VAE在模拟表格数据集上表现优异，在因子恢复、可解释性和对干扰变异的鲁棒性方面均优于基线方法。

Conclusion: SE-VAE通过设计而非统计正则化实现解耦，为科学和社会领域的白盒生成建模提供了原则性框架。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [140] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: D2D方法将因果循环图（CLD）转换为系统动力学模型（SDM），支持动态分析和干预策略探索，优于传统网络中心性分析。


<details>
  <summary>Details</summary>
Motivation: CLD作为静态定性工具，无法支持动态分析或干预策略，且传统定量方法易产生错误推断。

Method: D2D利用CLD中的结构信息（链接存在性和极性），通过用户简单标注变量类型（存量、流量/辅助变量或常量），生成SDM进行模拟。

Result: D2D能区分高低优先级杠杆点，与数据驱动模型一致性优于网络中心性分析，并提供不确定性估计和数据收集指导。

Conclusion: D2D方法通过开源工具实现，降低了动态建模门槛，未来验证有望扩展其应用范围。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [141] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 论文提出了一种基于加权知识图的物理定律表示与分析框架，通过构建包含659个物理方程的数据库，并优化为400个高级方程，利用图注意力网络（GAT）进行链接预测，性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决物理方程表示中的语义模糊问题，并探索跨领域物理定律的潜在联系。

Method: 构建物理方程数据库，设计加权知识图表示，训练GAT模型进行链接预测。

Result: GAT模型在测试中AUC达到0.9742，显著优于基线方法，并揭示了物理学的宏观结构和跨领域关系。

Conclusion: 该框架不仅能重新发现已知物理结构，还能生成新的跨领域关系假设，为物理学研究提供新工具。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [142] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的数据驱动方法，用于学习非线性状态空间模型中的nudging项，并在三个混沌基准问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统中设计有效的nudging项具有挑战性，因此需要一种数据驱动的方法来解决这一问题。

Method: 采用神经网络学习nudging项，基于Kazantzis-Kravaris-Luenberger观测器理论，并在Lorenz 96模型、Kuramoto-Sivashinsky方程和Kolmogorov流等混沌系统中进行测试。

Result: 理论证明了nudging项的存在性，并在实验中验证了方法的有效性。

Conclusion: 神经网络nudging方法为非线性系统中的数据同化提供了一种可行的解决方案。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [143] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 提出了一种可扩展的框架，通过整合异构数据重建可信的配电网拓扑，结合空间布局和动态行为，并引入置信感知机制和物理约束，验证了高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 配电网拓扑的准确性对现代电网运行至关重要，但实际数据来源多样且质量不均，需要一种可靠的重建方法。

Method: 结合空间布局（如GIS）和动态行为（如电压时间序列），引入置信感知推理机制，并嵌入物理约束（如变压器容量限制）。

Result: 在Oncor的8000多个电表数据上验证，拓扑重建准确率超过95%，置信校准和计算效率显著提升。

Conclusion: 该框架在现实条件下能快速收敛到可信拓扑，为电网操作提供了实用工具。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [144] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯风险最小化的统一理论框架，用于分析线性编码器-解码器架构，以解决数据驱动的科学机器学习问题。


<details>
  <summary>Details</summary>
Motivation: 尽管非线性神经网络在科学领域取得了成功，但其理论不透明性限制了其在需要可解释性场景中的应用。线性神经网络则提供了简单且有效的替代方案。

Method: 通过贝叶斯风险最小化，推导了闭式、秩约束的线性和仿射线性最优映射，用于正向建模和逆恢复任务。

Result: 理论结果在生物医学成像、金融因子分析和非线性流体动力学模拟等数据集上进行了验证。

Conclusion: 该研究为理解和基准测试科学机器学习中的神经网络模型提供了坚实基础。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [145] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种无监督、多智能体强化学习框架，通过动态生成多样化的训练伙伴，无需预训练伙伴或手动调参，显著提升了协作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要预训练伙伴或手动调参的问题，实现完全无监督的协作训练。

Method: 通过随机混合自我策略与偏置随机行为生成多样化伙伴，并使用基于方差的易学性指标评分。

Result: 在Overcooked-AI等任务中，UPD优于基线方法，用户研究显示其更具适应性和人性化。

Conclusion: UPD为无监督协作训练提供了高效解决方案，显著提升了性能和用户体验。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [146] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出了一种结合TAPE与Graphormer的新框架，利用ChatGPT生成语义丰富的节点表示，显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合文本语义与图结构信息时存在困难，尤其是在处理领域术语、长程依赖、时间演化和大规模数据时表现不佳。

Method: 结合TAPE框架与Graphormer，利用ChatGPT生成语义解释，并通过注意力机制融合结构与语义特征。

Result: 在ogbn-arxiv数据集上达到0.772的分类准确率，显著优于基线方法，并在其他指标上表现优异。

Conclusion: 该框架为动态TAGs中的节点分类提供了可扩展且鲁棒的解决方案，为未来研究指明了方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [147] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 提出了一种基于MDP和RL-PG的自主避碰策略，旨在平衡碰撞风险和燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统避碰策略中燃料消耗高且碰撞风险控制不足的问题。

Method: 使用MDP建模避碰决策，结合RL-PG算法训练策略，优化燃料消耗和碰撞风险。

Result: 在合成和历史数据上，策略显著降低燃料消耗，同时保持或提高碰撞风险控制。

Conclusion: 该方法有效平衡了燃料消耗和碰撞风险，优于传统策略。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [148] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT是一种2位量化方法，通过固定资源预算视角，可能提高信息密度。


<details>
  <summary>Details</summary>
Motivation: 传统量化被视为性能与计算资源的折中，但SZT从固定资源预算角度出发，探索量化可能带来的优势。

Method: 提出Signed-Zero Ternary (SZT)，一种2位量化方法，确定性提供梯度信息且无前向路径损失。

Result: 分析表明，SZT可能比非量化方法具有更高的信息密度。

Conclusion: SZT为量化提供了新的视角，可能在某些场景下优于非量化方法。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [149] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 论文提出了一种将随机时间序列分解为均值、离散度和噪声三部分的方法，通过机器学习拟合双信号并优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分解问题，提取均值与离散度信号，同时隔离噪声，为后续分析和预测提供基础。

Method: 采用机器学习方法，通过优化损失函数（结合原始序列拟合和双信号正则化）分解时间序列，支持顺序或联合学习。

Result: 实现了时间序列的平滑和去噪，并能揭示异方差性时间序列的复杂关系。

Conclusion: 该方法适用于多种应用场景，如结构学习、均值与离散度预测以及多时间序列交叉效应分析。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [150] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 论文提出了一种名为Shifted Gaussian Encoding的方法，通过优化激活矩阵的条件数，显著提升了神经PDE求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 神经PDE求解器的精度问题通常源于优化困难，尤其是在多保真度和刚性问题上，激活矩阵的严重病态性限制了收敛性。

Method: 引入Shifted Gaussian Encoding，一种简单的激活过滤步骤，提高矩阵秩和表达能力，同时保持凸性。

Result: 方法在稳态对流-扩散方程中将Peclet数的可解范围扩展了两个数量级，在多频函数学习上误差降低了六个数量级，且在高保真图像向量拟合上比百万参数深度网络更快更准确。

Conclusion: 研究表明，条件数而非网络深度是科学神经求解器的瓶颈，简单的架构改进可带来显著性能提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [151] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO是一种改进的GRPO方法，通过噪声感知优势权重稳定训练，解决了Think-Answer Mismatch问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在训练大型推理模型时存在Think-Answer Mismatch问题，噪声奖励信号会干扰学习过程，尤其在响应组不平衡时更为严重。

Method: 提出S-GRPO，通过噪声感知优势权重优化训练过程。

Result: 在多个数学推理基准测试中，S-GRPO显著优于GRPO，性能提升达2.5%，且在20%合成噪声下仍能稳定学习。

Conclusion: S-GRPO为大规模推理模型的训练提供了更稳健和有效的方法。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [152] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 提出了一种基于多臂老虎机（MAB）的决策树剪枝方法，通过强化学习动态优化剪枝过程，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如CCP和REP）基于贪心策略，可能导致泛化能力不足，特别是在小规模复杂数据集上。

Method: 将剪枝过程建模为探索-利用问题，利用MAB算法动态选择最优剪枝节点。

Result: 实验表明，该方法在多个基准数据集上优于传统剪枝方法。

Conclusion: MAB剪枝方法为决策树优化提供了一种动态且概率化的新思路。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [153] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MCRE的框架和MCRQ算法，通过平衡保守性和性能提升，解决了离线强化学习中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，学习策略与行为策略之间的分布偏移会导致OOD动作和过高估计，需要一种方法既能防止过高估计，又不过度保守。

Method: 提出了MCRE框架，结合TD误差和行为克隆项，开发了MCRQ算法，将其整合到离线策略的actor-critic框架中。

Result: 实验表明，MCRQ在基准数据集上优于现有基线方法和最先进的离线RL算法。

Conclusion: MCRE和MCRQ有效平衡了保守性与性能，为离线强化学习提供了一种高效解决方案。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [154] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义对齐的强化学习方法，通过SBERT计算奖励，避免手动设计奖励函数，实验表明该方法在多种环境中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，设计有效的奖励函数是一个挑战，尤其是在任务目标难以数值化的环境中。现有方法多依赖启发式或手动调整。

Method: 使用SBERT计算当前状态与目标语义指令的余弦相似度作为奖励，替代手动设计的奖励函数。

Result: 实验证明，语义奖励能引导学习达到竞争性控制行为，无需手动设计奖励函数。

Conclusion: 该方法为自然语言目标与智能体行为的对齐提供了新思路，并为大语言模型与控制应用的整合奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [155] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: 论文提出了一种解决非线性固定点方程的新方法，通过结合半范数收缩和诱导范数的单调性，首次实现了参数无关的最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决非线性固定点方程（如平均奖励Q学习和TD学习）中半范数非单调性导致的参数无关最优收敛速率难以实现的问题。

Method: 将平均误差重新表述为涉及非线性扰动的线性递归，并通过半范数收缩与适当诱导范数的单调性结合来控制非线性。

Result: 首次实现了参数无关的$\tilde{O}(1/\sqrt{t})$最优收敛速率，适用于平均奖励和指数折扣设置的Q学习。

Conclusion: 该方法在同步/异步更新、单智能体/分布式部署以及模拟或马尔可夫轨迹数据流等多种场景中均适用。

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [156] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP是一种新颖的CoT压缩框架，通过锚点引导和基于惊讶度的剪枝，显著减少推理延迟和训练成本，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长推理链带来的训练成本高、推理延迟大和部署困难的问题。

Method: 提出ASAP框架，包括锚点引导剪枝和基于第一标记惊讶度的逻辑感知剪枝，并训练模型自主生成简洁CoT。

Result: 在代码生成任务中，ASAP显著减少23.5%的标记生成和43.5%的推理延迟，同时保持高准确性。

Conclusion: ASAP为构建高效强大的LRMs提供了有前景的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [157] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: MCTS-OPS结合蒙特卡洛树搜索（MCTS）与大型语言模型（LLM），通过多步提示序列优化代码生成质量，显著提升复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成和结构化推理中表现出色，但在需要多步规划的复杂任务中性能下降。现有方法多关注启发式代码生成或简单任务，缺乏对复杂优化的支持。

Method: 提出MCTS-OPS框架，将提示选择建模为MCTS引导的序列决策过程，探索并优化多步提示序列。

Result: 在网络优化实验中，MCTS-OPS在代码执行成功率、优化结果（奖励提升2~4倍，标准差降低3倍）及最优解获取率（提升约10%）上显著优于基线。

Conclusion: 结合符号规划与LLM的方法在复杂领域展现出高质量代码生成的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [158] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 论文提出了一种新颖的分阶段动态竞争风险模型，用于预测心脏骤停后昏迷患者的神经功能恢复情况。该模型分两阶段利用时间不变和时间变化的特征，并通过神经网络捕捉复杂非线性关系。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停后昏迷患者的预后预测对ICU临床决策至关重要，但目前的方法未能充分利用随时间变化的动态数据。

Method: 扩展了Fine and Gray模型，分两阶段（时间不变和时间变化特征）进行预测，并引入神经网络处理非线性关系。

Result: 在2,278名患者的回顾性队列中，模型对觉醒、撤除生命支持治疗和死亡等竞争结局表现出稳健的判别性能。

Conclusion: 该模型可推广至多阶段特征收集的动态预测任务，适用于需要动态评估新特征预测价值的场景。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [159] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 论文提出了一种自适应异构图神经网络（AHGNN），用于解决异构图中的异质性分布和语义多样性问题，实验证明其在高异质性场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有研究多孤立关注异质性或异质性，忽略了实际应用中异质性图的普遍性，导致性能下降。

Method: AHGNN采用异质性感知卷积和粗到细的注意力机制，分别处理跳数和元路径的异质性分布及语义多样性。

Result: 在七个真实世界图和二十个基线上的实验显示，AHGNN在高异质性情况下表现优越。

Conclusion: AHGNN有效解决了异质性图中的关键挑战，提升了模型性能。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [160] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM是一种动态分配精度的机制，通过轻量级误差估计器和微调学习的阈值，为LLM的每一层在运行时选择位宽，优化性能与延迟的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在设备上运行的大型语言模型（LLM）如何根据不同的运行时约束（如延迟和准确性）有效处理查询的问题。

Method: 提出DP-LLM机制，动态为每一层分配精度，利用轻量级误差估计器和微调学习的阈值在运行时选择位宽。

Result: 实验结果表明，DP-LLM在多个模型和基准测试中实现了优于先前方法的性能与延迟权衡。

Conclusion: DP-LLM通过动态精度分配，显著提升了LLM的运行时适应能力，为性能与延迟的优化提供了新思路。

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [161] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 论文提出了针对深度时序模型（如TCNs）的非空泛、架构感知的泛化边界，并引入了一种公平比较方法，揭示了时序依赖性在固定信息预算下可能增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 尽管深度时序架构在序列数据上表现出色，但其泛化能力的理论理解有限。本文旨在填补这一空白。

Method: 通过延迟反馈阻塞机制将依赖样本转化为近似独立样本，推导出泛化边界，并设计公平比较方法固定有效样本量。

Result: 强依赖性序列的泛化差距比弱依赖性序列小76%，但收敛速度与理论预测存在差异。

Conclusion: 时序依赖性在固定信息预算下可能有益，但理论与实践之间的差距仍需进一步研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [162] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文首次实现了循环深度可微分逻辑门网络（RDDLGN），将布尔运算与循环架构结合用于序列到序列学习，在WMT'14英德翻译任务中表现接近GRU。


<details>
  <summary>Details</summary>
Motivation: 探索可微分逻辑门在序列建模中的应用，填补了现有研究的空白。

Method: 提出RDDLGN，结合布尔运算与循环架构，用于序列到序列学习。

Result: 在WMT'14英德翻译任务中，RDDLGN达到5.00 BLEU和30.9%的训练准确率，接近GRU性能（5.41 BLEU）。

Conclusion: 证明了基于循环逻辑的神经计算的可行性，为FPGA加速等研究方向开辟了新途径。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [163] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hindsight Goal-conditioned Regularization (HGR)的技术，结合Hindsight Self-Imitation Regularization (HSR)，显著提高了目标导向强化学习（GCRL）的样本效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励下的目标导向强化学习（GCRL）仍是一个挑战，现有方法如HER未能充分利用经验。

Method: 提出HGR技术，基于后见目标生成动作正则化先验，结合HSR最大化经验利用率。

Result: 在导航和操作任务中，HGR和HSR显著提高了样本重用效率和性能。

Conclusion: HGR和HSR的组合为GCRL提供了一种更高效的样本利用方法。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [164] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散模型的图像修复技术，用于合成逼真的口腔癌病变图像，以解决训练数据不足的问题，显著提升了诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 口腔癌诊断中，标注数据集的稀缺和训练数据的不足限制了诊断模型的性能。

Method: 使用微调的扩散模型和图像修复技术合成逼真的口腔癌病变图像，并结合多源数据集。

Result: 分类模型诊断准确率达0.97，检测模型定位准确率为0.85。

Conclusion: 合成图像生成在医学诊断中具有潜力，可推广至其他癌症诊断研究。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [165] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster是一种轻量级技术，通过随机重新平衡聚类分配，减少隐私噪声，提升联邦聚类中的隐私/效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类通过为每个聚类训练一个模型提升性能，但隐私泄露风险更高。直接应用差分隐私机制会显著降低效用。

Method: 提出RR-Cluster技术，通过随机重新平衡聚类分配，确保每个聚类有最少客户端数量，减少隐私噪声。

Result: 理论分析表明RR-Cluster降低了隐私噪声方差，实验证明其在合成和真实数据集上显著提升了隐私/效用权衡。

Conclusion: RR-Cluster是一种简单有效的技术，可显著改善联邦聚类中的隐私保护与模型性能。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [166] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 预训练神经网络在化学和小分子药物设计中备受关注，但研究发现大多数模型性能与基线ECFP指纹相比无显著提升，仅CLAMP模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 评估预训练神经网络在分子化学任务中的实际效果，揭示现有研究的评估严谨性问题。

Method: 对25种模型在25个数据集上进行公平比较，采用分层贝叶斯统计测试模型进行分析。

Result: 几乎所有神经网络模型表现与基线ECFP指纹相当，仅CLAMP模型显著优于其他模型。

Conclusion: 研究揭示了现有评估的不足，提出了改进建议，并强调需更严谨的模型验证。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [167] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: 本文提出了一种新型的图联邦学习方法（GFed-PP），用于个性化隐私推荐，通过结合公开用户数据提升推荐性能，同时适应不同隐私需求。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户对隐私保护需求一致，忽略了利用公开数据提升服务的潜力。本文旨在解决这一问题。

Method: GFed-PP利用公开用户数据构建用户-项目交互图和用户关系图，采用轻量级GCN学习个性化项目嵌入，并在客户端本地学习用户嵌入和评分函数。

Result: 实验表明，GFed-PP在五个数据集上显著优于现有方法，推荐准确性更高且不损害隐私。

Conclusion: GFed-PP为联邦推荐系统中适应不同隐私偏好提供了实用解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [168] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 论文提出了一种稳定的重参数化策略梯度方法（RPO），通过结合PPO的代理目标和KL正则化，解决了RPG训练不稳定的问题，并在实验中表现出优越的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 重参数化策略梯度（RPG）虽然能提高样本效率，但训练不稳定，高方差梯度会破坏学习过程。

Method: 结合PPO的代理目标，提出RPO方法，通过时间反向传播高效计算梯度，并引入KL正则化和裁剪代理目标。

Result: 在运动和操作任务中，RPO表现出更高的样本效率和稳定性能。

Conclusion: RPO通过结合PPO的稳定性和RPG的效率，提供了一种稳定且高效的策略优化方法。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [169] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究了在部分特征信息下进行成员推断攻击的问题，提出了MRAD框架，通过两阶段方法有效解决了这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断方法通常假设攻击者能获取目标样本的全部特征，但现实中往往只能获取部分特征，限制了方法的适用性。

Method: 提出MRAD框架，第一阶段优化未知特征值以最小化样本损失，第二阶段通过异常检测衡量重构样本与训练分布的偏差。

Result: 实验表明MRAD在多种数据集上有效，例如在STL-10上，即使缺失40%特征，AUC仍能达到约0.6。

Conclusion: MRAD在部分特征场景下具有实用性，且兼容多种现成的异常检测技术。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [170] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: CMOSS是一种高效的组合多臂老虎机算法，解决了UCB和对抗方法在长期性能与计算开销上的权衡问题，实现了与理论下限匹配的遗憾。


<details>
  <summary>Details</summary>
Motivation: UCB方法（如CUCB）存在长期性能问题，而对抗方法（如EXP3.M和HYBRID）计算开销大，需要一种兼顾性能与效率的算法。

Method: 提出CMOSS算法，在半强盗反馈下实现实例无关的遗憾$O\big( (\log k)^2\sqrt{kmT}\big )$，并扩展到级联反馈。

Result: CMOSS在合成和真实数据集上均优于基准算法，遗憾和运行效率表现一致。

Conclusion: CMOSS解决了现有方法的局限性，实现了理论最优性能，适用于多种反馈场景。

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [171] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型（LLM）微调中出现的突发性失调（EMA）问题，并提出了四种训练正则化干预方法以减少EMA。


<details>
  <summary>Details</summary>
Motivation: 微调LLM可能导致模型在目标领域外产生有害行为，即使微调数据本身无害。研究旨在为API提供者提供实用的防护措施。

Method: 研究了四种干预方法：KL散度正则化、特征空间L2距离、安全子空间投影（SafeLoRA）和混合安全训练数据。

Result: 评估了这些方法在四个恶意任务中的EMA抑制效果，并分析了其对良性任务的影响。

Conclusion: 讨论了EMA研究中的开放性问题，为未来研究提供了方向。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [172] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 提出了一种基于张量网络（MPS）的隐私保护高质量合成表格数据生成方法，在数据保真度和隐私保护能力上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私约束及多样化数据集需求，为训练鲁棒模型提供支持。

Method: 使用矩阵乘积态（MPS）生成合成数据，结合噪声注入和梯度裁剪实现差分隐私。

Result: MPS在严格隐私约束下表现优于CTGAN、VAE和PrivBayes等模型。

Conclusion: MPS是隐私感知合成数据生成的有前景工具，兼具数据质量和隐私保护能力。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [173] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 提出了一种名为GTMancer的框架，利用图神经网络和对比学习整合多组学数据，显著提升了癌症亚型分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多组学数据整合中忽略了异构组学间的复杂耦合关系，限制了其在精准肿瘤学中对细微癌症亚型异质性的解析能力。

Method: GTMancer结合图神经网络和对比学习，将多组学数据嵌入统一语义空间，并引入双重注意力系数捕捉组学内和组学间的结构先验。

Result: 在七个真实癌症数据集上的实验表明，GTMancer优于现有最先进算法。

Conclusion: GTMancer通过全局组学信息优化个体组学表征，为癌症亚型分类提供了更有效的解决方案。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [174] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: OM2P是一种新型离线多智能体强化学习算法，通过一步动作采样和奖励感知优化，解决了生成模型采样效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散和流模型）在离线多智能体强化学习中表现优异，但采样效率低且与奖励最大化目标不一致。

Method: 提出OM2P算法，结合均值流匹配损失和Q函数监督，设计广义时间步分布和无导数估计策略。

Result: 在Multi-Agent Particle和MuJoCo基准测试中，OM2P性能优越，GPU内存使用减少3.8倍，训练速度提升10.8倍。

Conclusion: OM2P首次成功将均值流模型整合到离线MARL中，为多智能体合作场景提供了实用且可扩展的生成策略。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [175] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 本文研究了在印度语言中应用持续学习（CL）于自动语音识别（ASR）系统，以解决数据顺序到达和隐私限制的问题。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性对开发包容性ASR系统提出了挑战，传统多语言模型因数据顺序到达和隐私限制不适用。

Method: 使用基于Conformer的混合RNN-T/CTC模型，从印地语预训练开始，逐步学习八种印度语言，评估了三种CL策略（EWC、MAS、LwF）。

Result: 结果表明，CL能有效减少遗忘，相比简单微调表现更优，适用于现实约束下的印度语言ASR扩展。

Conclusion: 持续学习是解决印度语言ASR系统扩展问题的有效方法，尤其在数据顺序和隐私限制下。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [176] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出了一种新型多输出脉冲神经元模型，结合了线性状态转移和非线性反馈机制，性能与现有SNN模型相当。


<details>
  <summary>Details</summary>
Motivation: 桥接SNN和深度SSM模型的优势，解决SSM模型高精度激活函数和无复位机制的问题。

Method: 设计了一种结合线性状态转移和非线性反馈机制的神经元模型，明确了脉冲功能、复位条件和复位动作的区别。

Result: 在多个任务（如关键词识别、事件视觉任务和序列模式识别）中表现与现有SNN基准相当，复位机制克服了线性动态不稳定性。

Conclusion: 提出的复位机制扩展了深度SSM模型的应用范围，解决了稳定性问题，为神经形态计算提供了新工具。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [177] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新颖的联邦元学习方法，通过隐私保护损失函数解决传统FML的隐私泄露问题，实现高效优化和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需要大量数据和计算资源，传统FML方法存在隐私泄露问题，FedMeNF旨在解决这些限制。

Method: 提出FedMeNF方法，采用隐私保护损失函数，优化本地元学习过程，避免保留客户端私有数据。

Result: 实验表明，FedMeNF在少样本或非独立同分布数据下仍能快速优化并保持鲁棒的重建性能。

Conclusion: FedMeNF在保护隐私的同时，实现了高效的神经场学习，适用于资源受限的边缘设备。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [178] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: 提出了一种自适应鲁棒损失函数FCL，通过分数阶导数动态调整对标签噪声的鲁棒性，无需手动调参即可实现高性能分类。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒损失函数需要大量数据集特定的超参数调优，限制了其实际应用。

Method: FCL结合了交叉熵损失的分数阶导数（主动成分）和平均绝对误差（被动成分），通过可学习参数μ动态平衡鲁棒性和收敛速度。

Result: 实验表明，FCL在基准数据集上实现了最先进的分类性能，无需手动调参。

Conclusion: FCL通过动态调整损失函数，有效解决了标签噪声下的分类问题，具有广泛的应用潜力。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [179] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means是一种基于几何原理（标量投影）的新型k-means算法，通过高效利用高表达数据（HE）并忽略低表达数据（LE），显著提升了计算效率和能源经济性。


<details>
  <summary>Details</summary>
Motivation: 传统k-means算法计算效率低且能耗高，Gk-means旨在通过几何优化解决这一问题。

Method: 利用标量投影技术，专注于高表达数据（HE），忽略低表达数据（LE），减少计算开销。

Result: 实验表明，Gk-means在运行时间、距离计算和能源效率上优于传统及SOTA的k-means变体。

Conclusion: Gk-means是一种高效、节能且可持续的k-means改进方案。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [180] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLMs）在无明确诱导情况下自发性欺骗行为的可能性，并提出了一种基于心理学的量化框架。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理、规划和决策任务中的广泛应用使其可信度成为关键问题，但现有研究多依赖人为诱导欺骗，未能充分反映真实场景。

Method: 提出使用‘接触搜索问题’框架，结合心理学原理设计两种统计指标（欺骗意图分和欺骗行为分）量化LLM的欺骗倾向。

Result: 评估14种主流LLMs发现，任务难度增加时，欺骗倾向和行为不一致性均显著上升。

Conclusion: 研究揭示了LLMs在复杂任务中自发性欺骗的倾向，对其在关键领域的部署提出了警示。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [181] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff是一种基于扩散模型的生成方法，通过分类器引导技术实现对分子生物活性的精确控制，包括增强目标活性和减少脱靶效应。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要关注单一活性分子，缺乏同时管理多目标和非目标交互的机制，难以满足药物设计需求。

Method: 采用扩散模型和分类器引导技术，分别训练药物-靶点分类器进行正负引导，优化分子设计。

Result: 实验表明，ActivityDiff能有效处理单/双靶点生成、片段约束双靶点设计、增强靶点特异性及减少脱靶效应等任务。

Conclusion: ActivityDiff为分子设计提供了一种平衡效果和安全性的新范式，是一个多功能且可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [182] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 本文提出了一种三阶段端到端文本到SQL框架，通过识别用户目标数据库再生成SQL查询，解决了多数据库场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统文本到SQL方法假设目标数据库已知，但在多数据库场景中识别正确数据库是关键却被忽视的步骤。

Method: 结合LLM和提示工程提取自然语言查询的隐含规则，训练RoBERTa微调编码器预测数据库标识符，最后通过批评代理修正SQL。

Result: 实验表明，该框架在数据库意图预测和SQL生成准确性上优于当前最先进模型。

Conclusion: 提出的三阶段框架有效解决了多数据库环境下的文本到SQL问题，提升了性能。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [183] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 利用公开众包数据（如311服务电话和街景图像）预测旧金山无家可归者帐篷趋势，提供更及时、细粒度的监测方法。


<details>
  <summary>Details</summary>
Motivation: 现有监测方法（如PIT统计）在频率、一致性和空间细节上存在不足，需更高效的工具指导政策。

Method: 结合311服务电话和街景图像数据，构建预测模型捕捉每日和社区级变化。

Result: 模型揭示了传统方法忽略的快速波动（如疫情期间）和空间变化，提供更精准的监测。

Conclusion: 该方法为政策制定和干预评估提供了更及时、低成本的信息支持。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [184] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 论文提出了一种名为LoRR的插件方法，通过高重放训练和周期性重置策略，提升基于偏好的LLM优化框架的样本效率，同时避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习和偏好优化的LLM训练方法存在样本效率低和初始偏好过拟合（primacy bias）的问题，限制了模型性能的提升。

Method: 提出LoRR方法，结合高重放训练、周期性重置策略和混合优化目标（SFT与偏好损失），提升数据利用效率。

Result: 实验表明，LoRR显著提升了多种偏好优化方法的性能，在数学和通用推理任务上表现优异，甚至媲美复杂RL算法。

Conclusion: LoRR为LLM微调提供了一种高效、实用的范式，能够在有限数据下实现更高性能。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [185] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN是一个针对大语言模型（LLM）的模块化、目标明确的遗忘框架，通过梯度比指标定位关键参数，选择性注入噪声以提升遗忘效果，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于法律和伦理问题，需要有效遗忘LLM中的敏感或未授权数据，现有方法存在遗忘不彻底或影响无关知识的问题。

Method: GRIN提出基于梯度比的指标定位关键参数，选择性注入噪声后进行微调。

Result: 在TOFU、WMDP和SafePKU等标准基准上验证了GRIN的有效性。

Conclusion: GRIN提供了一种高效且精准的LLM遗忘方法，解决了现有方法的局限性。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [186] [Functional Connectivity Graph Neural Networks](https://arxiv.org/abs/2508.05786)
*Yang Li,Luopeiwen Yi,Tananun Songdechakraiwut*

Main category: cs.NE

TL;DR: 提出了一种结合结构信息和功能连通性的图神经网络框架，用于提升图分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实网络需要同时捕捉局部和全局交互，受脑成像中多模态分析的启发，提出了一种新的方法。

Method: 引入基于持久图同调的功能连通性块，结合结构信息构建多模态架构。

Result: 实验表明，该方法在多种网络上优于现有方法。

Conclusion: 脑启发的表示方法在图分类任务中具有显著价值。

Abstract: Real-world networks often benefit from capturing both local and global
interactions. Inspired by multi-modal analysis in brain imaging, where
structural and functional connectivity offer complementary views of network
organization, we propose a graph neural network framework that generalizes this
approach to other domains. Our method introduces a functional connectivity
block based on persistent graph homology to capture global topological
features. Combined with structural information, this forms a multi-modal
architecture called Functional Connectivity Graph Neural Networks. Experiments
show consistent performance gains over existing methods, demonstrating the
value of brain-inspired representations for graph-level classification across
diverse networks.

</details>


### [187] [Identity Increases Stability in Neural Cellular Automata](https://arxiv.org/abs/2508.06389)
*James Stovold*

Main category: cs.NE

TL;DR: 论文提出了一种通过引入带简单约束的'identity'层来提升神经细胞自动机（NCA）生成的人工生物稳定性的方法。


<details>
  <summary>Details</summary>
Motivation: NCA生成的人工生物在生长过程中存在稳定性问题，如边界崩溃或形状失控，需要改进。

Method: 在训练过程中引入带简单约束的'identity'层。

Result: 改进后的NCA模型在近距离生长时更稳定，且仅需单一'identity'值即可实现稳定性提升。此外，稳定生物表现出涌现运动行为。

Conclusion: 该研究为未来探索NCA生成生物间的互动奠定了基础，为研究人工生物的细胞层面社会行为提供了可能。

Abstract: Neural Cellular Automata (NCAs) offer a way to study the growth of
two-dimensional artificial organisms from a single seed cell. From the outset,
NCA-grown organisms have had issues with stability, their natural boundary
often breaking down and exhibiting tumour-like growth or failing to maintain
the expected shape. In this paper, we present a method for improving the
stability of NCA-grown organisms by introducing an 'identity' layer with simple
constraints during training.
  Results show that NCAs grown in close proximity are more stable compared with
the original NCA model. Moreover, only a single identity value is required to
achieve this increase in stability. We observe emergent movement from the
stable organisms, with increasing prevalence for models with multiple identity
values.
  This work lays the foundation for further study of the interaction between
NCA-grown organisms, paving the way for studying social interaction at a
cellular level in artificial organisms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [188] [Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.05687)
*Alistair Reid,Simon O'Callaghan,Liam Carroll,Tiberio Caetano*

Main category: cs.MA

TL;DR: 论文探讨了多智能体AI系统的风险识别与分析，提出了六种关键故障模式，并为实践者提供了工具包。


<details>
  <summary>Details</summary>
Motivation: 随着组织从单智能体转向多智能体网络，其交互可能引发新的故障模式，需要不同于单智能体的风险分析方法。

Method: 通过分阶段测试（模拟、观察分析、基准测试和红队演练）逐步提高分析有效性。

Result: 提出了六种关键故障模式及相应的工具包，为组织风险管理奠定了基础。

Conclusion: 多智能体系统需要分阶段测试和证据收集，以确保风险管理的有效性。

Abstract: Organisations are starting to adopt LLM-based AI agents, with their
deployments naturally evolving from single agents towards interconnected,
multi-agent networks. Yet a collection of safe agents does not guarantee a safe
collection of agents, as interactions between agents over time create emergent
behaviours and induce novel failure modes. This means multi-agent systems
require a fundamentally different risk analysis approach than that used for a
single agent.
  This report addresses the early stages of risk identification and analysis
for multi-agent AI systems operating within governed environments where
organisations control their agent configurations and deployment. In this
setting, we examine six critical failure modes: cascading reliability failures,
inter-agent communication failures, monoculture collapse, conformity bias,
deficient theory of mind, and mixed motive dynamics. For each, we provide a
toolkit for practitioners to extend or integrate into their existing frameworks
to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our
approach centres on analysis validity, and advocates for progressively
increasing validity through staged testing across stages of abstraction and
deployment that gradually increases exposure to potential negative impacts,
while collecting convergent evidence through simulation, observational
analysis, benchmarking, and red teaming. This methodology establishes the
groundwork for robust organisational risk management as these LLM-based
multi-agent systems are deployed and operated.

</details>


### [189] [Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control](https://arxiv.org/abs/2508.05702)
*Yan Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种名为Grid-Agent的AI驱动框架，结合大型语言模型和多智能体强化学习，实时检测和修复电网违规问题。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源、电动汽车的普及以及极端天气事件的增加，电网规划和管理变得更为复杂，传统方法难以应对现代电网的需求。

Method: Grid-Agent采用模块化智能体架构，结合语义推理和数值计算，通过规划智能体和验证智能体协同工作，并采用自适应多尺度网络表示实现可扩展性。

Result: 在IEEE和CIGRE测试系统中，Grid-Agent表现出卓越的违规缓解性能，并能持续学习和适应不同网络拓扑。

Conclusion: Grid-Agent的自适应性和实时响应能力使其特别适合现代智能电网的动态需求。

Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread
adoption of Electric Vehicles (EVs), and the growing frequency of extreme
weather events have significantly increased the complexity of power grid
planning, operation, and management. Traditional rule-based systems and
numerical optimization approaches often struggle with the scale, dynamics, and
adaptability required by modern power networks. This paper introduces
Grid-Agent, an autonomous, AI-driven framework that combines Large Language
Models (LLMs) with multi-agent reinforcement learning to detect and remediate
grid violations in real time. Grid-Agent integrates semantic reasoning with
numerical precision through a modular agent architecture: a planning agent
generates coordinated action sequences using numerical power flow solvers,
while a validation agent evaluates system stability and action effectiveness
via sandboxed execution with safety rollbacks. To ensure scalability,
Grid-Agent incorporates an adaptive multiscale network representation that
dynamically selects optimal encoding schemes based on network size and
complexity. The framework enables coordinated violation resolution through
optimizing switch configurations, battery deployment, and load curtailment
strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE
69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation
performance. Additionally, the framework's built-in data collection and
learning capabilities enable continuous learning and adaptation to diverse
network topologies. The autonomous nature of the framework makes it
particularly suitable for modern smart grid applications requiring rapid
response to dynamic operating conditions.

</details>


### [190] [Flow-Based Task Assignment for Large-Scale Online Multi-Agent Pickup and Delivery](https://arxiv.org/abs/2508.05890)
*Yue Zhang,Zhe Chen,Daniel Harabor,Pierre Le Bodic,Peter J. Stuckey*

Main category: cs.MA

TL;DR: 本文提出了一种基于最小成本流的在线多智能体拾取与交付（MAPD）任务分配方法，解决了现有方法在实时性和可扩展性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有在线方法要么依赖简单启发式导致决策质量低，要么因复杂推理而难以在实时约束下扩展。

Method: 将任务分配问题建模为环境图上的最小成本流，避免成对距离计算，并引入两种拥塞感知边成本模型以提高质量。

Result: 该方法支持实时执行，可扩展到20000个智能体和30000个任务，1秒内完成规划，优于现有基线。

Conclusion: 最小成本流方法在计算效率和分配质量上均表现优异，适用于大规模实时MAPD问题。

Abstract: We study the problem of online Multi-Agent Pickup and Delivery (MAPD), where
a team of agents must repeatedly serve dynamically appearing tasks on a shared
map. Existing online methods either rely on simple heuristics, which result in
poor decisions, or employ complex reasoning, which suffers from limited
scalability under real-time constraints. In this work, we focus on the task
assignment subproblem and formulate it as a minimum-cost flow over the
environment graph. This eliminates the need for pairwise distance computations
and allows agents to be simultaneously assigned to tasks and routed toward
them. The resulting flow network also supports efficient guide path extraction
to integrate with the planner and accelerates planning under real-time
constraints. To improve solution quality, we introduce two congestion-aware
edge cost models that incorporate real-time traffic estimates. This approach
supports real-time execution and scales to over 20000 agents and 30000 tasks
within 1-second planning time, outperforming existing baselines in both
computational efficiency and assignment quality.

</details>


### [191] [Policy Optimization in Multi-Agent Settings under Partially Observable Environments](https://arxiv.org/abs/2508.06061)
*Ainur Zhaikhan,Malek Khammassi,Ali H. Sayed*

Main category: cs.MA

TL;DR: 提出了一种结合自适应社会学习和多智能体强化学习（MARL）的方法，用于估计部分可观测的全局状态，避免了传统的双时间尺度学习框架的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要时间密集和计算密集的双时间尺度学习框架，限制了效率。本文旨在通过交替进行社会学习和MARL步骤，提高效率。

Method: 交替执行单步社会学习和单步MARL，避免双时间尺度学习框架。

Result: 理论分析和仿真结果表明，该方法性能接近已知真实状态时的强化学习效果。

Conclusion: 该方法在保持性能的同时显著降低了计算和时间成本。

Abstract: This work leverages adaptive social learning to estimate partially observable
global states in multi-agent reinforcement learning (MARL) problems. Unlike
existing methods, the proposed approach enables the concurrent operation of
social learning and reinforcement learning. Specifically, it alternates between
a single step of social learning and a single step of MARL, eliminating the
need for the time- and computation-intensive two-timescale learning frameworks.
Theoretical guarantees are provided to support the effectiveness of the
proposed method. Simulation results verify that the performance of the proposed
methodology can approach that of reinforcement learning when the true state is
known.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [192] [Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding](https://arxiv.org/abs/2508.05844)
*François Bachoc,Nicolò Cesa-Bianchi,Tommaso Cesari,Roberto Colomboni*

Main category: cs.GT

TL;DR: 论文提出了一种随机多臂老虎机模型，用于解决预算分配问题，设计了算法并证明了其遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于众包和自动竞价中的预算分配问题，需要将固定预算分配给多个任务或拍卖。

Method: 定义了一个基于概率单纯形的多臂老虎机模型，设计了一种算法，其遗憾界为$K\sqrt{T}$，并在特定条件下优化为$K(\log T)^2$。

Result: 算法在$T$步后的期望遗憾为$K\sqrt{T}$，且在满足递减回报条件下可优化至$K(\log T)^2$。

Conclusion: 该模型和算法为预算分配问题提供了理论支持，并在特定条件下进一步优化了性能。

Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is
split among $K$ workers, and autobidding, where a fixed budget is used to bid
in $K$ simultaneous auctions, we define a stochastic bandit model where arms
belong to the $K$-dimensional probability simplex and represent the fraction of
budget allocated to each task/auction. The reward in each round is the sum of
$K$ stochastic rewards, where each of these rewards is unlocked with a
probability that varies with the fraction of the budget allocated to that
task/auction. We design an algorithm whose expected regret after $T$ steps is
of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound.
Improved bounds of order $K (\log T)^2$ are shown when the function mapping
budget to probability of unlocking the reward (i.e., terminating the task or
winning the auction) satisfies additional diminishing-returns conditions.

</details>


### [193] [An Overlapping Coalition Game Approach for Collaborative Block Mining and Edge Task Offloading in MEC-assisted Blockchain Networks](https://arxiv.org/abs/2508.06031)
*Licheng Ye,Zehui Xiong,Lin Gao,Dusit Niyato*

Main category: cs.GT

TL;DR: 该论文研究了移动边缘计算（MEC）辅助的多联盟协作区块链网络，提出了一种两阶段Stackelberg博弈模型，以分析矿工和边缘计算服务提供商的行为，并设计了算法优化资源定价和联盟形成。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单联盟协作模式，而多联盟协作模式能进一步提升区块链网络的效率、安全性和可扩展性。

Method: 提出两阶段Stackelberg博弈模型：第一阶段由边缘计算服务提供商定价，第二阶段矿工形成重叠联盟并竞争边缘资源。设计了OCF交替算法和近最优定价策略。

Result: 推导了边缘资源竞争游戏的纳什均衡解，并通过算法实现了稳定的联盟结构和优化的资源定价。

Conclusion: 多联盟协作模式结合MEC技术能显著提升区块链网络的性能，提出的博弈模型和算法为实际应用提供了理论支持。

Abstract: Mobile edge computing (MEC) is a promising technology that enhances the
efficiency of mobile blockchain networks, by enabling miners, often acted by
mobile users (MUs) with limited computing resources, to offload
resource-intensive mining tasks to nearby edge computing servers. Collaborative
block mining can further boost mining efficiency by allowing multiple miners to
form coalitions, pooling their computing resources and transaction data
together to mine new blocks collaboratively. Therefore, an MEC-assisted
collaborative blockchain network can leverage the strengths of both
technologies, offering improved efficiency, security, and scalability for
blockchain systems. While existing research in this area has mainly focused on
the single-coalition collaboration mode, where each miner can only join one
coalition, this work explores a more comprehensive multi-coalition
collaboration mode, which allows each miner to join multiple coalitions. To
analyze the behavior of miners and the edge computing service provider (ECP) in
this scenario, we propose a novel two-stage Stackelberg game. In Stage I, the
ECP, as the leader, determines the prices of computing resources for all MUs.
In Stage II, each MU decides the coalitions to join, resulting in an
overlapping coalition formation (OCF) game; Subsequently, each coalition
decides how many edge computing resources to purchase from the ECP, leading to
an edge resource competition (ERC) game. We derive the closed-form Nash
equilibrium for the ERC game, based on which we further propose an OCF-based
alternating algorithm to achieve a stable coalition structure for the OCF game
and develop a near-optimal pricing strategy for the ECP's resource pricing
problem.

</details>


### [194] [Social Welfare in Battery Charging Games](https://arxiv.org/abs/2508.06320)
*Simon Krogmann,Pascal Lenzner,Alexander Skopalik,Tobias Sträubig*

Main category: cs.GT

TL;DR: 论文探讨了分散式可再生能源市场中电池所有者的策略行为，提出了一种基于博弈论的Stackelberg市场模型，分析了不同定价策略下的均衡存在性和社会福利。


<details>
  <summary>Details</summary>
Motivation: 随着分散式可再生能源和家用电池的普及，电网供需平衡面临时空挑战，而电池所有者的自私行为可能偏离电网目标，因此需要从博弈论角度分析。

Method: 采用Stackelberg博弈模型，第三方通过定价激励优化可再生能源利用并保持电网可行性，研究不同定价策略下的均衡存在性和社会福利。

Result: 研究发现均衡存在性取决于定价策略，社会福利差异显著，表明需要更复杂的市场模型和定价机制。

Conclusion: 论文为可再生能源网络中的激励机制研究开辟了新的方向，未来可在算法博弈论领域进一步探索。

Abstract: The recent rise of renewable energy produced by many decentralized sources
yields interesting market design challenges for electrical grids. Balancing
supply and demand in such networks is both a temporal and spatial challenge due
to capacity constraints. The recent surge in the number of household-owned
batteries, especially in regions with rooftop solar adoption, offers mitigation
potential but often acts misaligned with grid-level objectives. In fact, the
decision to charge or discharge a household-owned battery is a strategic choice
by each battery owner governed by selfish incentives. This calls for an
analysis from a game-theoretic point of view.
  We initiate this timely research direction by considering a game-theoretic
setting where selfish agents strategically charge or discharge their batteries
to increase their profit. In particular, we study a Stackelberg-like market
model where a third party introduces price incentives, aiming to optimize
renewable energy utilization while preserving grid feasibility. For this, we
study the existence and the quality of equilibria under various pricing
strategies. We find that the existence of equilibria crucially depends on the
chosen pricing and that the obtained social welfare varies widely. This calls
for more sophisticated market models and pricing mechanisms and opens up a rich
field for future research in Algorithmic Game Theory on incentives in renewable
energy networks.

</details>


### [195] [A Geometric Analysis of Gains from Trade](https://arxiv.org/abs/2508.06469)
*Jason Hartline,Kangning Wang*

Main category: cs.GT

TL;DR: 论文通过几何方法证明随机提议机制在双边交换中对最优交易收益的近似比为4，并进一步优化至3.15。


<details>
  <summary>Details</summary>
Motivation: 研究双边交换中随机提议机制的性能，探索其对最优交易收益的近似效果。

Method: 采用几何分析方法，对随机提议机制的性能进行理论推导和优化。

Result: 初始证明近似比为4，进一步优化后达到3.15。

Conclusion: 随机提议机制在双边交换中具有较高的近似性能，优化后接近最优解。

Abstract: We provide a geometric proof that the random proposer mechanism is a
$4$-approximation to the first-best gains from trade in bilateral exchange. We
then refine this geometric analysis to recover the state-of-the-art
approximation ratio of $3.15$.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [196] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)
*Siddharth Gangwar,David A. Selby,Sebastian J. Vollmer*

Main category: cs.HC

TL;DR: 论文探讨了如何利用多模态大语言模型（LLMs）改进数据可视化，通过半自动生成反馈提升图表质量。


<details>
  <summary>Details</summary>
Motivation: 数据可视化是艺术与科学的结合，但通常未纳入数据科学课程。社区通过视觉改造练习改进图表，但能否用LLMs模拟这一过程？

Method: 利用预训练模型，结合用户指定的可视化最佳实践，通过提示工程生成对现有图表的改进建议。

Result: 定量评估显示LLM代理对不同图表类型的问题具有敏感性，并开发了易用的自托管工具。

Conclusion: LLMs可有效用于改进数据可视化，提供了一种教育用户的新方法。

Abstract: Making a good graphic that accurately and efficiently conveys the desired
message to the audience is both an art and a science, typically not taught in
the data science curriculum. Visualisation makeovers are exercises where the
community exchange feedback to improve charts and data visualizations. Can
multi-modal large language models (LLMs) emulate this task? Given a plot in the
form of an image file, or the code used to generate it, an LLM, primed with a
list of visualization best practices, is employed to semi-automatically
generate constructive criticism to produce a better plot. Our system is centred
around prompt engineering of a pre-trained model, relying on a combination of
userspecified guidelines and any latent knowledge of data visualization
practices that might lie within an LLMs training corpus. Unlike other works,
the focus is not on generating valid visualization scripts from raw data or
prompts, but on educating the user how to improve their existing data
visualizations according to an interpretation of best practices. A quantitative
evaluation is performed to measure the sensitivity of the LLM agent to various
plotting issues across different chart types. We make the tool available as a
simple self-hosted applet with an accessible Web interface.

</details>


### [197] [A Humanoid Social Robot as a Teaching Assistant in the Classroom](https://arxiv.org/abs/2508.05646)
*Thomas Sievers*

Main category: cs.HC

TL;DR: 研究探讨了社交机器人Pepper结合ChatGPT在高中课堂中的应用，测试了其技术可行性和学生接受度。


<details>
  <summary>Details</summary>
Motivation: 教育系统需要创新技术支持以减轻负担，但目前社交机器人在学校中的应用较少。

Method: 使用Pepper机器人结合ChatGPT在高中课堂中教授新内容，并调查学生的接受度和感知实用性。

Result: 所有参与者认为机器人呈现学习材料的方式合适或部分合适，且其使用有意义。

Conclusion: 社交机器人在教育中具有潜力，能支持教师并丰富学习环境。

Abstract: Although innovation and the support of new technologies are much needed to
ease the burden on the education system, social robots in schools to help
teachers with educational tasks are rare. Child-Robot Interaction (CRI) could
support teachers and add an embodied social component to modern multi-modal and
multi-sensory learning environments already in use. The social robot Pepper,
connected to the Large Language Model (LLM) ChatGPT, was used in a high school
classroom to teach new learning content to groups of students. I tested the
technical possibilities with the robot on site and asked the students about
their acceptance and perceived usefulness of teaching with the help of a social
robot. All participants felt that the robot's presentation of the learning
material was appropriate or at least partially appropriate and that its use
made sense.

</details>


### [198] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)
*Jules Clerc,Domitile Lourdeaux,Mohamed Sallak,Johann Barbier,Marc Ravaine*

Main category: cs.HC

TL;DR: 本文提出了一种用于交互式叙事系统（INS）的形式化表示框架，旨在解决研究碎片化和系统多样性问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 交互式叙事系统（INS）的研究因碎片化和多样性而面临挑战，需要一种统一的表示框架以促进分析和比较。

Method: 提出了一种形式化表示框架，提供一致的词汇和建模结构，并在“小红帽”场景中进行实验验证。

Result: 实验验证表明，该框架有助于分析和比较INS特性，并提升系统评估效果。

Conclusion: 该框架为INS研究社区提供了一种统一的方法论，有望促进协作和一致性。

Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences
by empowering users to actively shape their stories, diverging from traditional
passive storytelling. However, the field faces challenges due to fragmented
research efforts and diverse system representations. This paper introduces a
formal representation framework for INS, inspired by diverse approaches from
the state of the art. By providing a consistent vocabulary and modeling
structure, the framework facilitates the analysis, the description and
comparison of INS properties. Experimental validations on the "Little Red
Riding Hood" scenario highlight the usefulness of the proposed formalism and
its impact on improving the evaluation of INS. This work aims to foster
collaboration and coherence within the INS research community by proposing a
methodology for formally representing these systems.

</details>


### [199] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: 研究探讨了AI伦理与用户满意度之间的关系，发现七项伦理维度均与用户满意度正相关，但关联强度因用户和产品类型而异。


<details>
  <summary>Details</summary>
Motivation: 尽管AI伦理原则被广泛认可，但缺乏用户视角的实证证据，研究旨在填补这一空白。

Method: 通过分析10万+条AI产品用户评论，使用基于Transformer的语言模型测量七项伦理维度的情感。

Result: 所有伦理维度均与用户满意度正相关，非技术用户和终端应用用户的关联更强。

Conclusion: 强调从用户角度设计伦理AI的重要性，并需考虑用户角色和产品类型的差异。

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [200] [REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition](https://arxiv.org/abs/2508.05933)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 提出了一种基于自适应正交非负矩阵分解的EEG特征选择方法，用于解决多维情感识别中标签缺失和噪声问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多维情感识别中，多类型EEG特征的高维性和样本稀缺性导致分类器过拟合和实时性能差，且标签缺失和个体情感感知差异进一步增加了挑战。

Method: 结合自适应正交非负矩阵分解重建标签空间，并采用最小二乘回归与图流形学习正则化，实现缺失信息下的EEG特征选择。

Result: 在DREAMER、DEAP和HDED数据集上，该方法在鲁棒性上优于13种先进特征选择方法。

Conclusion: 该方法有效解决了多维情感识别中的标签缺失和噪声问题，提升了EEG情感识别的鲁棒性。

Abstract: The affective brain-computer interface is a crucial technology for affective
interaction and emotional intelligence, emerging as a significant area of
research in the human-computer interaction. Compared to single-type features,
multi-type EEG features provide a multi-level representation for analyzing
multi-dimensional emotions. However, the high dimensionality of multi-type EEG
features, combined with the relatively small number of high-quality EEG
samples, poses challenges such as classifier overfitting and suboptimal
real-time performance in multi-dimensional emotion recognition. Moreover,
practical applications of affective brain-computer interface frequently
encounters partial absence of multi-dimensional emotional labels due to the
open nature of the acquisition environment, and ambiguity and variability in
individual emotion perception. To address these challenges, this study proposes
a novel EEG feature selection method for missing multi-dimensional emotion
recognition. The method leverages adaptive orthogonal non-negative matrix
factorization to reconstruct the multi-dimensional emotional label space
through second-order and higher-order correlations, which could reduce the
negative impact of missing values and outliers on label reconstruction.
Simultaneously, it employs least squares regression with graph-based manifold
learning regularization and global feature redundancy minimization
regularization to enable EEG feature subset selection despite missing
information, ultimately achieving robust EEG-based multi-dimensional emotion
recognition. Simulation experiments on three widely used multi-dimensional
emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method
outperforms thirteen advanced feature selection methods in terms of robustness
for EEG emotional feature selection.

</details>


### [201] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)
*Xueyuan Xu,Tianze Yu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 提出了一种名为ASLSL的新方法，用于处理不完整多模态生理信号的特征选择问题，通过共享潜在结构学习减少缺失信息的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态生理信号情感识别中，高维特征常包含无关、冗余和噪声信息，导致分类器性能下降。现有方法假设数据完整，但实际数据常不完整。

Method: ASLSL方法利用自适应共享潜在结构学习，探索不完整多模态信号和情感标签的共同潜在空间，挖掘共识信息。

Result: 在DEAP和DREAMER数据集上，ASLSL优于17种特征选择方法。

Conclusion: ASLSL能有效处理不完整多模态生理信号的特征选择问题，提升情感识别性能。

Abstract: Recently, multi-modal physiological signals based emotion recognition has
garnered increasing attention in the field of brain-computer interfaces.
Nevertheness, the associated multi-modal physiological features are often
high-dimensional and inevitably include irrelevant, redundant, and noisy
representation, which can easily lead to overfitting, poor performance, and
high computational complexity in emotion classifiers. Feature selection has
been widely applied to address these challenges. However, previous studies
generally assumed that multi-modal physiological data are complete, whereas in
reality, the data are often incomplete due to the openness of the acquisition
and operational environment. For example, a part of samples are available in
several modalities but not in others. To address this issue, we propose a novel
method for incomplete multi-modal physiological signal feature selection called
adaptive shared latent structure learning (ASLSL). Based on the property that
similar features share similar emotional labels, ASLSL employs adaptive shared
latent structure learning to explore a common latent space shared for
incomplete multi-modal physiological signals and multi-dimensional emotional
labels, thereby mitigating the impact of missing information and mining
consensus information. Two most popular multi-modal physiological emotion
datasets (DEAP and DREAMER) with multi-dimensional emotional labels were
utilized to compare the performance between compare ASLSL and seventeen feature
selection methods. Comprehensive experimental results on these datasets
demonstrate the effectiveness of ASLSL.

</details>


### [202] [It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design](https://arxiv.org/abs/2508.05940)
*Kathy Cheng,Alison Olechowski,Shurui Zhou*

Main category: cs.HC

TL;DR: 硬件开发团队面临设计变更和协作问题，研究探讨了CAD依赖管理的挑战，并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 硬件设计师缺乏对设计变更和协作行为的意识，CAD依赖管理问题影响团队协作效率。

Method: 通过分析100个在线论坛讨论和10位设计师的半结构化访谈，识别CAD依赖管理的九大挑战。

Result: 研究发现依赖管理的可追溯性、导航和一致性问题是主要障碍。

Conclusion: 提出设计目标和功能改进方案，以提升硬件设计师的依赖管理能力，优化协作流程。

Abstract: In today's landscape, hardware development teams face increasing demands for
better quality products, greater innovation, and shorter manufacturing lead
times. Despite the need for more efficient and effective processes, hardware
designers continue to struggle with a lack of awareness of design changes and
other collaborators' actions, a persistent issue in decades of CSCW research.
One significant and unaddressed challenge is understanding and managing
dependencies between 3D CAD (computer-aided design) models, especially when
products can contain thousands of interconnected components. In this two-phase
formative study, we explore designers' pain points of CAD dependency management
through a thematic analysis of 100 online forum discussions and semi-structured
interviews with 10 designers. We identify nine key challenges related to the
traceability, navigation, and consistency of CAD dependencies, that harm the
effective coordination of hardware development teams. To address these
challenges, we propose design goals and necessary features to enhance hardware
designers' awareness and management of dependencies, ultimately with the goal
of improving collaborative workflows.

</details>


### [203] [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)
*Wei Xiang,Ziyue Lei,Haoyuan Che,Fangyuan Ye,Xueting Wu,Lingyun Sun*

Main category: cs.HC

TL;DR: 该论文探讨了如何通过LLM（大语言模型）和EMS（电肌肉刺激）结合，为操作技能学习提供动觉辅助，填补了现有LLM训练助手在动觉反馈上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练助手主要提供文本反馈，忽视了动觉反馈的重要性。论文旨在探索LLM驱动的动觉辅助在操作技能学习中的潜力。

Method: 提出“对齐-分析-调整”策略，开发了FlightAxis工具，结合LLM和EMS，用于飞行技能学习。

Result: 实验显示用户对LLM驱动的身体控制接受度高，任务完成时间显著减少，且动觉辅助增强了操作缺陷意识和训练参与度。

Conclusion: 该研究展示了LLM驱动的动觉辅助在操作技能学习中的潜力，为未来相关研究提供了方向。

Abstract: Operational skill learning, inherently physical and reliant on hands-on
practice and kinesthetic feedback, has yet to be effectively replicated in
large language model (LLM)-supported training. Current LLM training assistants
primarily generate customized textual feedback, neglecting the crucial
kinesthetic modality. This gap derives from the textual and uncertain nature of
LLMs, compounded by concerns on user acceptance of LLM driven body control. To
bridge this gap and realize the potential of collaborative human-LLM action,
this work explores human experience of LLM driven kinesthetic assistance.
Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed
FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)
for flight skill acquisition, a representative operational skill domain.
FlightAxis learns flight skills from manuals and guides forearm movements
during simulated flight tasks. Our results demonstrate high user acceptance of
LLM-mediated body control and significantly reduced task completion times.
Crucially, trainees reported that this kinesthetic assistance enhanced their
awareness of operation flaws and fostered increased engagement in the training
process, rather than relieving perceived load. This work demonstrated the
potential of kinesthetic LLM training in operational skill acquisition.

</details>


### [204] [RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.06056)
*Sizhe Cheng,Jiaping Li,Huanchen Wang,Yuxin Ma*

Main category: cs.HC

TL;DR: RAGTrace是一个交互式评估系统，用于分析RAG工作流中的检索与生成动态，支持多级分析和集成探索。


<details>
  <summary>Details</summary>
Motivation: RAG系统中内部知识集成和检索-生成交互的不透明性是一个关键挑战，需要更深入的分析工具。

Method: 通过文献综述和专家访谈，开发了RAGTrace系统，支持从高层性能评估到细粒度检索相关性、生成保真度及跨组件交互的分析。

Result: RAGTrace通过案例研究和专家评估展示了其在真实RAG应用中的有效性。

Conclusion: RAGTrace为RAG工作流提供了透明度和可追溯性，有助于优化检索过程并识别潜在问题。

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a promising
solution to enhance large language models (LLMs) by integrating external
knowledge retrieval with generative capabilities. While significant
advancements have been made in improving retrieval accuracy and response
quality, a critical challenge remains that the internal knowledge integration
and retrieval-generation interactions in RAG workflows are largely opaque. This
paper introduces RAGTrace, an interactive evaluation system designed to analyze
retrieval and generation dynamics in RAG-based workflows. Informed by a
comprehensive literature review and expert interviews, the system supports a
multi-level analysis approach, ranging from high-level performance evaluation
to fine-grained examination of retrieval relevance, generation fidelity, and
cross-component interactions. Unlike conventional evaluation practices that
focus on isolated retrieval or generation quality assessments, RAGTrace enables
an integrated exploration of retrieval-generation relationships, allowing users
to trace knowledge sources and identify potential failure cases. The system's
workflow allows users to build, evaluate, and iterate on retrieval processes
tailored to their specific domains of interest. The effectiveness of the system
is demonstrated through case studies and expert evaluations on real-world RAG
applications.

</details>


### [205] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane系统通过交互式主题设计平面帮助用户操控高级语义概念（如情绪、风格或叙事基调），弥合创意意图与系统控制之间的差距，支持非专家用户更直观地进行生成式设计。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使图像创作更易获取，但非专家用户仍难以将输出与复杂创意意图对齐，现有工具限制了流畅探索。

Method: 引入ThematicPlane系统，用户可通过交互界面操控语义概念，支持发散与收敛创意模式。

Result: 探索性研究（N=6）显示用户常以熟悉主题为基础，但需更明确的控制解释。系统促进了表达性和迭代性工作流程。

Conclusion: ThematicPlane为生成式设计工具提供了语义驱动交互的新方向，强调直观性与迭代性。

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


### [206] [A Multimodal Framework for Understanding Collaborative Design Processes](https://arxiv.org/abs/2508.06117)
*Maurice Koch,Nelusa Pathmanathan,Daniel Weiskopf,Kuno Kurzhals*

Main category: cs.HC

TL;DR: 提出一个模块化框架reCAPit，用于多模态数据采集、AI提取和可视化分析，以改进协作设计工作坊的结果分析。


<details>
  <summary>Details</summary>
Motivation: 协作设计工作坊的分析通常依赖文本记录，难以整合多源异构数据，需要更高效的方法。

Method: 开发reCAPit系统，结合视频、音频、笔记等多模态数据，通过AI提取和可视化技术（如流图、主题卡片）进行分析。

Result: 在六个工作坊中验证了框架的有效性，并详细分析了两个案例。

Conclusion: 该研究通过多模态数据采集和交互式分析，扩展了协作设计工作坊的方法论。

Abstract: An essential task in analyzing collaborative design processes, such as those
that are part of workshops in design studies, is identifying design outcomes
and understanding how the collaboration between participants formed the results
and led to decision-making. However, findings are typically restricted to a
consolidated textual form based on notes from interviews or observations. A
challenge arises from integrating different sources of observations, leading to
large amounts and heterogeneity of collected data. To address this challenge we
propose a practical, modular, and adaptable framework of workshop setup,
multimodal data acquisition, AI-based artifact extraction, and visual analysis.
Our interactive visual analysis system, reCAPit, allows the flexible
combination of different modalities, including video, audio, notes, or gaze, to
analyze and communicate important workshop findings. A multimodal streamgraph
displays activity and attention in the working area, temporally aligned topic
cards summarize participants' discussions, and drill-down techniques allow
inspecting raw data of included sources. As part of our research, we conducted
six workshops across different themes ranging from social science research on
urban planning to a design study on band-practice visualization. The latter two
are examined in detail and described as case studies. Further, we present
considerations for planning workshops and challenges that we derive from our
own experience and the interviews we conducted with workshop experts. Our
research extends existing methodology of collaborative design workshops by
promoting data-rich acquisition of multimodal observations, combined AI-based
extraction and interactive visual analysis, and transparent dissemination of
results.

</details>


### [207] [Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models](https://arxiv.org/abs/2508.06300)
*Weihan Zhang,Jun Tao*

Main category: cs.HC

TL;DR: 论文提出了一种基于自然语言交互的流场可视化框架，通过将流模式表示与大型语言模型（LLM）语义空间对齐，实现无需手动标注的智能流场探索。


<details>
  <summary>Details</summary>
Motivation: 传统流场可视化界面依赖专业图形表示和交互方式，学习成本高；自然语言交互更直观，但机器识别科学概念并提取流场结构存在挑战。

Method: 采用去噪自编码器编码流线段，通过投影层将生成的流模式表示映射到LLM嵌入空间，利用注意力机制实现文本嵌入与流表示的语义匹配。

Result: 开发了交互式界面，用户可通过自然语言查询和可视化流场结构，案例研究验证了框架的有效性。

Conclusion: 该框架实现了直观且智能的流场探索，提升了可访问性。

Abstract: Explorative flow visualization allows domain experts to analyze complex flow
structures by interactively investigating flow patterns. However, traditional
visual interfaces often rely on specialized graphical representations and
interactions, which require additional effort to learn and use. Natural
language interaction offers a more intuitive alternative, but teaching machines
to recognize diverse scientific concepts and extract corresponding structures
from flow data poses a significant challenge. In this paper, we introduce an
automated framework that aligns flow pattern representations with the semantic
space of large language models (LLMs), eliminating the need for manual
labeling. Our approach encodes streamline segments using a denoising
autoencoder and maps the generated flow pattern representations to LLM
embeddings via a projector layer. This alignment empowers semantic matching
between textual embeddings and flow representations through an attention
mechanism, enabling the extraction of corresponding flow patterns based on
textual descriptions. To enhance accessibility, we develop an interactive
interface that allows users to query and visualize flow structures using
natural language. Through case studies, we demonstrate the effectiveness of our
framework in enabling intuitive and intelligent flow exploration.

</details>


### [208] [Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance](https://arxiv.org/abs/2508.06349)
*Serena Tardelli,Lorenzo Alvisi,Lorenzo Cima,Stefano Cresci,Maurizio Tesconi*

Main category: cs.HC

TL;DR: 研究发现Telegram上的表情符号反应更多体现社交认可而非情感共鸣，且正面反应占主导。


<details>
  <summary>Details</summary>
Motivation: 探讨表情符号反应在消息平台中的社交功能，而非仅作为情感或情绪指标。

Method: 收集并分析65万条Telegram消息，标注情感、情绪、修辞策略等，并推断表情反应的情感。

Result: 表情反应与消息情感不匹配，正面反应占主导，表明其可能反映社交认可。

Conclusion: 表情符号反应不宜直接作为情感反应的代理，需考虑其社交功能。

Abstract: Emoji reactions are a frequently used feature of messaging platforms. Prior
work mainly interpreted emojis as indicators of emotional resonance or user
sentiment. However, emoji reactions may instead reflect broader social
dynamics. Here, we investigate the communicative function of emoji reactions on
Telegram by analyzing the relationship between the emotional and rhetorical
content of messages and the emoji reactions they receive. We collect and
analyze over 650k Telegram messages that received at least one emoji reaction.
We annotate each message with sentiment, emotion, persuasion strategy, and
speech act labels, and infer the sentiment and emotion of emoji reactions using
both lexicons and large languages. We find a systematic mismatch between
message sentiment and reaction sentiment, with positive reactions dominating
even when the message is neutral or negative. We show that this pattern remains
consistent across rhetorical strategies and emotional tones, suggesting that
emoji reactions may signal a degree of social approval rather than reflecting
emotional resonance. Finally, we shed light on the communicative strategies
that predict greater emoji engagement. These findings have methodological
implications for sentiment analysis, as interpreting emoji reactions as direct
proxies for emotional response may be misleading.

</details>


### [209] [Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems](https://arxiv.org/abs/2508.06354)
*Clara Rigaud*

Main category: cs.HC

TL;DR: 研究探讨如何利用废弃智能手机和平板电脑构建新的交互系统，以音乐控制器为例，记录从诊断到自主电子对象的设计过程，并提出开源工具包的开发方向。


<details>
  <summary>Details</summary>
Motivation: 探索废弃设备的再利用潜力，减少电子垃圾，同时为设计师和开发者提供低成本交互系统解决方案。

Method: 通过诊断废弃设备、设计音乐控制器原型，并与专业音乐家讨论，分析软硬件方面的挑战与机遇。

Result: 提出基于高级网络技术的方法，使设计师能够更易进入设备内部，促进智能手机的可持续计算。

Conclusion: 废弃设备再利用具有潜力，开源工具包可推动更多人参与，实现可持续交互系统设计。

Abstract: This article explores the possibilities of reusing obsolete smartphones and
tablets to build new interactive systems. Taking the case of a musical
instrument, I present my research into the design of a controller made from
various of these obsolete smartphones. From the diagnostic stage to the
creation of a new autonomous electronic object, I document the process, the
barriers and the levers encountered. Based on these explorations and
discussions with two professional musicians, I provide several insights into
the software and hardware aspects, with a view to continuing this work, towards
the creation of an open-source toolkit enabling anyone to build new interactive
systems with old devices. I discuss the implication of how a high-level
web-based approach could allow designers to enter the black box and foster
permacomputing using smartphones.

</details>


### [210] [Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data](https://arxiv.org/abs/2508.06484)
*Yuvraj Virk,Dongyu Liu*

Main category: cs.HC

TL;DR: 非技术终端用户依赖AI生成代码执行技术任务（如数据分析），但大型语言模型（LLM）不可靠。研究发现，即使提示用户识别错误，营销和销售专业人员仍难以发现关键缺陷。改进展示方式后效果有限，表明业务人员无法独立可靠验证AI生成的分析。


<details>
  <summary>Details</summary>
Motivation: 研究非技术终端用户（如营销和销售专业人员）是否能有效识别LLM生成代码中的错误，尤其是在实际业务场景中。

Method: 通过调查营销和销售专业人员，展示LLM生成的市场数据分析代码及解释，提示用户识别错误。随后改进展示方式（分步呈现和提供替代方案），观察效果。

Result: 用户普遍难以发现关键错误，改进展示方式后效果有限，仍无法独立验证AI分析。

Conclusion: 业务人员无法可靠验证AI生成的数据分析，需改进设计以减少因AI不可靠和人为监督不足导致的风险。

Abstract: Non-technical end-users increasingly rely on AI code generation to perform
technical tasks like data analysis. However, large language models (LLMs)
remain unreliable, and it is unclear whether end-users can effectively identify
model errors $\unicode{x2014}$ especially in realistic and domain-specific
scenarios. We surveyed marketing and sales professionals to assess their
ability to critically evaluate LLM-generated analyses of marketing data.
Participants were shown natural language explanations of the AI's code,
repeatedly informed the AI often makes mistakes, and explicitly prompted to
identify them. Yet, participants frequently failed to detect critical flaws
that could compromise decision-making, many of which required no technical
knowledge to recognize. To investigate why, we reformatted AI responses into
clearly delineated steps and provided alternative approaches for each decision
to support critical evaluation. While these changes had a positive effect,
participants often struggled to reason through the AI's steps and alternatives.
Our findings suggest that business professionals cannot reliably verify
AI-generated data analyses on their own and explore reasons why to inform
future designs. As non-programmers adopt code-generating AI for technical
tasks, unreliable AI and insufficient human oversight poses risks of unsafe or
low-quality decisions.

</details>
