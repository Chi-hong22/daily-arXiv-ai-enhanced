<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.RO](#cs.RO) [Total: 43]
- [eess.SY](#eess.SY) [Total: 31]
- [cs.HC](#cs.HC) [Total: 35]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.GT](#cs.GT) [Total: 6]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 128]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 比较分析了多种算法策略用于拟合3D图像数据的镶嵌模型，评估了不同优化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，准确拟合3D图像数据（如多晶体和泡沫）的镶嵌模型是一个不断发展的领域，需要评估不同优化方法的适用性。

Method: 采用线性/非线性规划、随机优化（交叉熵法）和梯度下降等方法，生成Voronoi、Laguerre和广义平衡功率图（GBPDs）来近似体素基的晶粒结构。

Result: 通过实际数据集评估拟合质量，发现模型复杂度、优化方法复杂度和近似质量之间存在权衡。

Conclusion: 研究结果为根据数据特征和应用需求选择合适方法提供了指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 论文研究了深度学习在语义分割中的应用，提出多种高效模型，并通过BDD100k数据集验证，结果表明选择合适的骨干网络对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 人工智能和深度学习在复杂任务中表现出色，尤其在自动驾驶领域。本文旨在通过语义分割提升场景理解能力。

Method: 提出多种高效模型，使用BDD100k数据集进行验证，并尝试不同骨干网络作为编码器。

Result: 实验结果显示，选择合适的骨干网络显著提升了语义分割的性能，具体表现为准确率、平均IoU和损失函数的改善。

Conclusion: 论文证明了骨干网络选择对语义分割模型的重要性，为自动驾驶等领域的场景理解提供了有效方法。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种基于梯度的测试时适应方法，针对视觉语言模型（VLMs）设计，通过软对比损失与CLIP的预训练目标对齐，解决了熵最小化目标的不匹配问题，并在开放集设置中进一步优化了OOD检测。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在分布偏移下泛化能力不足，而传统的测试时适应方法（如熵最小化）与VLMs的对比训练目标不匹配，导致性能受限和伪标签漂移等问题。

Method: 提出CLIPTTA方法，采用软对比损失与CLIP预训练目标对齐，并通过理论分析其梯度设计以避免崩溃。在开放集设置中，引入Outlier Contrastive Exposure (OCE)损失改进OOD检测。

Result: 在75个数据集上的实验表明，CLIPTTA性能优于基于熵的目标，并在多种分布偏移下表现稳定，优于现有TTA方法。

Conclusion: CLIPTTA通过对齐预训练目标的梯度设计，显著提升了VLMs在测试时的适应能力，尤其在开放集设置中表现优异。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 论文提出了一种名为注意力聚焦（AF）的机制，通过修剪非信息性标记来优化广义类别发现（GCD）中的特征提取。AF包含两个组件：标记重要性测量（TIME）和标记自适应修剪（TAP），显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时容易受到背景区域的干扰，导致特征提取不理想。

Method: 提出AF机制，包括TIME和TAP两个组件，通过多尺度重要性评分修剪非信息性标记。

Result: AF在SimGCD方法中实现了15.4%的性能提升，且计算开销极小。

Conclusion: AF是一种轻量级、即插即用的模块，能显著提升GCD方法的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 生成式超分辨率（GSR）在感知图像质量上领先，但仍存在与低分辨率图像（LRI）或真实图像（GTI）不匹配的伪影（幻觉）。本文提出了一种基于多模态大语言模型（MLLM）的“幻觉评分”（HS）方法，并结合深度特征距离优化GSR模型。


<details>
  <summary>Details</summary>
Motivation: 现有GSR模型在感知质量上表现优异，但生成的细节与输入图像或真实图像不匹配的问题（幻觉）未被充分研究，限制了实际应用。

Method: 利用MLLM构建提示词评估幻觉元素，生成HS评分；发现某些深度特征距离与HS强相关，并以此作为可微分奖励函数优化GSR模型。

Result: HS与人类评估高度一致，且为现有超分辨率指标提供补充；深度特征距离与HS强相关，可用于模型优化。

Conclusion: HS是衡量和缓解GSR幻觉的有效工具，结合深度特征距离优化模型可显著减少幻觉问题。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [6] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一种结合深度学习和光流的半自动化工具包，用于在B型超声视频中跟踪任意点，解决了噪声和运动模糊等问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: B型超声中组织运动的准确跟踪因噪声和低对比度而困难，需要一种通用且鲁棒的解决方案。

Method: 结合深度学习和光流技术，提供图形界面支持训练数据生成和模型优化，并引入光流滤波减少噪声。

Result: DUSTrack在多种应用场景中表现优异，准确度高于零样本跟踪器，与专用方法相当。

Conclusion: DUSTrack是一种开源工具，为临床和生物力学研究提供了强大的通用框架。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [7] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，用于可解释的affordance grounding，通过结合ConceptNet和语言模型的结构化常识先验与CLIP的视觉证据，迭代优化预测。


<details>
  <summary>Details</summary>
Motivation: 解决场景理解中affordance grounding的可解释性和鲁棒性问题。

Method: 整合ConceptNet和语言模型的常识先验与CLIP的视觉证据，通过基于能量的推理循环迭代优化预测。

Result: 在多对象、无标签设置中，CRAFT提高了准确性并增强了可解释性。

Conclusion: CRAFT为鲁棒且可信的场景理解提供了重要一步。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [8] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯变形场的3DGS视频流框架，通过混合显著性分块和差异化质量建模，实现了高效压缩和带宽适应，提升了传输质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯喷溅（3DGS）视频的数据量大且传输复杂，传统方法难以满足需求，因此需要新的流媒体框架。

Method: 设计基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模，实现高效压缩和带宽适应。

Result: 实验表明，该方法在视频质量、压缩效率和传输速率上优于现有方法。

Conclusion: 提出的框架有效解决了3DGS视频流媒体的挑战，为高质量传输提供了可行方案。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [9] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个针对真实红外图像的多模态大语言模型，基于26万真实红外-文本对数据集IR-TD，通过双向跨模态课程迁移学习策略，在9项任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖合成红外图像的局限性，捕捉红外模态的独特特性。

Method: 构建IR-TD数据集，结合LLM生成和规则描述；提出双向跨模态课程迁移学习策略。

Result: 在9项任务中性能最优，超越更大规模模型。

Conclusion: IRGPT通过真实数据和迁移学习策略，显著提升红外图像的多模态理解能力。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [10] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），用于点云配准中的高质量对应关系识别，通过正交几何一致性减少冗余信息并提升性能。


<details>
  <summary>Details</summary>
Motivation: 点云配准中高质量对应关系的识别因局部和全局特征融合的复杂性及冗余问题而极具挑战性。

Method: 利用Gestalt原则设计GPI-Net，引入正交集成策略和Gestalt特征注意力块（GFA），并开发双路径多粒度并行交互聚合块（DMG）。

Result: 在多个挑战性任务上的实验表明，GPI-Net优于现有方法。

Conclusion: GPI-Net通过Gestalt原则和并行交互设计，有效提升了点云配准中对应关系的质量。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [11] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种自适应3D高斯溅射视频（3DGS）流媒体解决方案，包括基于显著性分析的3DGS分块技术、质量评估框架和基于元学习的自适应比特率算法。


<details>
  <summary>Details</summary>
Motivation: 3DGS流媒体在提供沉浸式3D视频体验方面具有潜力，但仍面临分块、质量评估和比特率适应等挑战。

Method: 提出自适应3DGS分块技术（结合时空特征）、新型质量评估框架（评估3DGS表示和2D渲染质量）和基于元学习的自适应比特率算法。

Result: 实验表明，所提方法显著优于现有技术。

Conclusion: 本文为3DGS流媒体提供了一套全面的解决方案，解决了关键挑战。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [12] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一种混合专家端到端自动驾驶框架，通过全局专家和场景自适应专家组的动态路由，实现多样场景下的自适应和鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 解决单模式规划方法难以处理多样化驾驶场景的问题。

Method: 提出GEMINUS框架，包含全局专家、场景自适应专家组和双感知路由器，动态激活专家模块。

Result: 在Bench2Drive基准测试中表现优异，驾驶评分和成功率均达到SOTA，且仅需单目视觉输入。

Conclusion: GEMINUS通过专家混合和动态路由显著提升了自动驾驶的适应性和鲁棒性。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [13] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一种抗篡改的可视化图像数据检索框架，通过嵌入元数据链接解决现有方法易受图像篡改影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可视化图像中嵌入元数据时易受篡改（如裁剪和编辑），导致信息丢失，缺乏实用性。

Method: VisGuard采用重复数据平铺、可逆信息广播和基于锚点的裁剪定位技术，增强嵌入数据的鲁棒性。

Result: 实验表明VisGuard在数据检索准确性、嵌入容量和抗篡改安全性方面表现优异。

Conclusion: VisGuard能有效保护和促进可视化传播及信息传递。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [14] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet提出了一种新的序列建模框架，结合空间特征提取和时间差分，通过端到端训练提升动态环境中的视觉地点识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态和感知混淆环境中视觉地点识别的挑战，现有方法忽略了图像序列的时间连贯性。

Method: 采用轻量级1D卷积编码器和可学习的差分时间算子（DSD），结合LSTM和四元组损失，直接学习序列级嵌入。

Result: 在多个公开基准测试中，OptiCorNet在季节和视角变化下优于现有方法。

Conclusion: OptiCorNet通过端到端学习序列级嵌入，显著提升了动态环境中的地点识别效果。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [15] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT提出了一种无需数据的ViT量化方法，通过合成样本和激活校正矩阵提升量化模型性能，性能接近真实数据量化模型。


<details>
  <summary>Details</summary>
Motivation: 现有DFQ方法未能充分平衡全局和局部特征，且量化模型与全精度模型的中间层激活分布差异大，导致性能下降。

Method: 按难度递增顺序合成样本，并引入激活校正矩阵对齐量化与全精度模型的中间层激活。

Result: DFQ-ViT性能显著优于现有DFQ方法，3位量化DeiT-T性能提升4.29%，且无需微调。

Conclusion: DFQ-ViT降低了计算开销和部署门槛，符合绿色学习原则，适用于资源受限环境。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [16] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强的点云补全框架，通过跨模态检索学习结构先验信息，生成细粒度点云。


<details>
  <summary>Details</summary>
Motivation: 解决不完整点云补全任务中缺乏典型结构特征的问题，并提升生成能力和泛化能力。

Method: 设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），通过双通道控制门和分层特征融合机制整合参考样本的先验信息。

Result: 在多个数据集和真实场景中验证了方法的有效性，能够生成细粒度点云，并处理稀疏数据和未见类别。

Conclusion: 该方法在点云补全任务中表现出色，具有较高的生成能力和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [17] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种新型MLLM架构，通过令牌压缩解决WSI视觉问答中的高计算资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 病理学中的全切片图像（WSI）尺寸巨大，现有方法在视觉问答（VQA）中计算资源消耗过高且缺乏生成能力。

Method: 提出TCP-LLaVA，利用可训练的压缩令牌通过模态压缩模块聚合视觉和文本信息，仅将压缩后的令牌输入LLM生成答案。

Result: 在10种TCGA肿瘤亚型上的实验表明，TCP-LLaVA在VQA准确率上优于现有MLLM基线，同时大幅减少训练资源消耗。

Conclusion: TCP-LLaVA通过令牌压缩有效解决了WSI VQA中的计算资源问题，提升了性能。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [18] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 提出了一种基于事件流和几何约束的运动分割与自运动估计框架，适用于神经形态视觉传感器，无需完整光流计算。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而神经形态传感器的事件数据稀疏且高时间分辨率，需新方法处理。

Method: 利用事件数据，结合几何约束，通过过分割、残差分析和层次聚类优化运动分割与估计。

Result: 在EVIMO2v2数据集上验证了准确的分割和平移运动估计，尤其在物体边界表现优异。

Conclusion: 该方法在实时机器人和导航应用中具有潜力，尤其在处理复杂场景时表现突出。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [19] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 综述了基于前馈方法的3D重建与视图合成技术，分类讨论了不同表示架构，并探讨了其应用、数据集及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算复杂度高，限制了实际应用，而深度学习驱动的前馈方法提供了快速且通用的解决方案。

Method: 分类讨论了点云、3D高斯泼溅、神经辐射场等表示架构，并分析了关键任务如无姿态重建、动态3D重建等。

Result: 总结了前馈方法在3D视觉中的广泛应用，包括数字人、SLAM和机器人等领域。

Conclusion: 前馈方法有望推动3D视觉领域的进步，但仍需解决开放研究挑战。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [20] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为DCHM的框架，通过深度一致性建模和多视角融合，在稀疏视角、大规模和拥挤场景中实现精确的行人检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在行人建模中引入噪声且精度低，依赖昂贵的多视角3D标注，难以泛化到多样场景。

Method: 采用深度一致性行人建模（DCHM）框架，结合超像素高斯泼溅技术，实现多视角深度一致性和全局坐标融合。

Result: 显著减少噪声，优于现有方法，首次在挑战性场景中重建行人并完成多视角分割。

Conclusion: DCHM无需依赖人工标注，提升了行人建模的精度和泛化能力。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [21] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: ArtiMuse是一个基于多模态大语言模型的图像美学评估模型，结合评分与专家级理解能力，并发布了首个专家标注的数据集ArtiMuse-10K。


<details>
  <summary>Details</summary>
Motivation: 随着教育应用、艺术创作和AI生成内容的发展，对图像美学评估的需求增加，现有方法存在模态偏差和缺乏细粒度分析的问题。

Method: 提出ArtiMuse模型，具备联合评分和专家级理解能力，并构建ArtiMuse-10K数据集，包含10,000张图像，标注了8维属性和整体评分。

Result: ArtiMuse展示了更强的感知和泛化能力，解决了传统方法的局限性。

Conclusion: 模型和数据集将公开，以推动图像美学评估领域的发展。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [22] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 提出一种浏览器扩展，通过计算机视觉识别手语并将其翻译为字幕，以消除听障人士与普通人之间的沟通障碍。


<details>
  <summary>Details</summary>
Motivation: 听障人士与普通人沟通困难，尤其在疫情期间视频会议成为主要交流方式时，手语识别技术显得尤为重要。

Method: 使用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展实现手语到字幕的自动翻译。

Result: 通过浏览器扩展实现手语到字幕的实时翻译，帮助听障人士在视频会议中更便捷地交流。

Conclusion: 该技术有望显著改善听障人士的沟通体验，尤其在远程交流场景中。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [23] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍了用于ImageCLEFmed MEDVQA 2025挑战赛子任务1的视觉问答（VQA）方法，采用Florence模型作为主干，通过领域特定增强提升泛化能力，在KASVIR数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决胃肠道内窥镜视觉问答任务，探索大型多模态模型在医学VQA中的潜力。

Method: 使用Florence模型作为VQA管道主干，结合视觉和文本编码器，并应用领域特定增强技术。

Result: 在KASVIR数据集上微调Florence模型，取得官方挑战指标的准确回答。

Conclusion: 大型多模态模型在医学VQA中具有潜力，为未来可解释性、鲁棒性和临床集成研究提供了基线。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [24] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 研究探讨了人工神经网络（ANN）在情感认知科学中的应用，发现ANN分类模糊的刺激在人类观察者中也会引发感知差异，揭示了情感感知的共同计算原则。


<details>
  <summary>Details</summary>
Motivation: 解决情感认知科学中外部情感刺激与人类内部体验关系的建模问题，尤其是探索ANN在捕捉个体感知差异方面的潜力。

Method: 提出了一种新颖的感知边界采样方法，生成位于ANN决策边界的面部表情刺激，构建了varEmotion数据集，并通过大规模人类行为实验验证假设。

Result: 发现ANN分类模糊的刺激在人类中也引发了更高的感知不确定性，通过行为数据微调ANN表征，实现了ANN预测与人类感知模式的对齐。

Conclusion: 研究建立了ANN决策边界与人类感知变异性之间的系统性联系，为情感解释的个性化建模提供了新见解。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [25] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 论文提出了一种相机引导系统，帮助用户识别和去除照片中的杂乱内容，提升照片美学质量。


<details>
  <summary>Details</summary>
Motivation: 摄影爱好者常因疏忽或经验不足在照片中留下杂乱内容，影响情感或故事的传达。

Method: 系统通过算法评估对象对照片美学的贡献，提供交互式杂乱识别工具，并结合生成对抗网络进行图像修复。

Result: 用户研究表明，系统能帮助用户更高效地识别杂乱并提升照片质量。

Conclusion: 该系统通过灵活界面和准确算法，显著改善了用户的摄影体验和成果。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [26] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D通过自然语言描述增强3D场景理解，显著提升了关系推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景语言模型在对象关系理解上表现不足，尤其是仅依赖视觉嵌入时。

Method: Descrip3D为每个对象添加文本描述，并通过嵌入融合和提示级注入双重集成关系线索。

Result: 在五个基准数据集上，Descrip3D均优于基线模型。

Conclusion: 语言引导的关系表示能有效提升复杂室内场景的理解能力。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [27] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 论文提出LEAD方法，通过建模优化过程的非线性动态，有效预测预训练模型在下游任务中的迁移性能，避免冗长的微调过程。


<details>
  <summary>Details</summary>
Motivation: 预训练模型数量激增，如何高效选择最适合下游任务的模型成为挑战，现有方法未能准确捕捉微调动态的非线性特性。

Method: LEAD基于网络输出的logits，提出理论框架建模优化过程，并用ODE描述非线性演化，设计类感知分解方法。

Result: 在24个预训练模型和10个下游数据集上的实验显示优异性能，尤其在低数据场景下表现突出。

Conclusion: LEAD通过优化对齐和非线性建模，提供了一种高效预测模型迁移性能的解决方案。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 该论文通过对比GAN、扩散模型和流匹配技术，评估了它们在T1w到T2w MRI图像转换中的表现，发现GAN-based Pix2Pix在结构保真度、图像质量和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的模态图像。

Method: 使用GANs、扩散模型和流匹配技术进行T1w到T2w的2D MRI图像转换，并在三个公开数据集上评估。

Result: GAN-based Pix2Pix在结构保真度、图像质量和计算效率上优于其他方法。

Conclusion: GAN更适合小数据集和简单任务，流匹配模型可能需要更多数据才能达到类似性能。研究为实际MRI工作流程提供了指导。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了TensorFlow、PyTorch和JAX三种深度学习框架在血液细胞图像分类中的性能，重点关注推理时间和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在血液图像分析中表现出潜力，但对具体框架的性能分析不足，因此需要比较不同框架的表现。

Method: 使用BloodMNIST数据集，比较三种框架（TensorFlow、PyTorch、JAX）在分类任务中的推理时间和准确性。

Result: 结果显示不同框架性能存在差异，JAX和PyTorch的分类准确性接近当前基准。

Conclusion: JAX和PyTorch在医学图像分类中表现高效，适合相关应用。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [30] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是一种结合无监督分割和开放词汇引导的方法，首次解决了3D开放词汇子概念发现的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务目标或场景内容，DiSCO-3D旨在提供同时适应场景和用户查询的3D语义分割。

Method: 基于神经场表示，结合无监督分割和弱开放词汇引导。

Result: 在开放词汇子概念发现中表现优异，并在开放词汇和无监督分割的边缘案例中达到最先进水平。

Conclusion: DiSCO-3D为3D语义分割提供了更灵活的解决方案，适应性强且性能优越。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出了一种名为Exp-Graph的新框架，通过图建模结合视觉变换器和图卷积网络，提升面部表情识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互等领域至关重要，但面部属性的结构变化需要被有效建模。

Method: 使用面部关键点作为图的顶点，基于邻近性和局部外观相似性确定边，结合视觉变换器和图卷积网络捕捉结构依赖。

Result: 在三个基准数据集上分别达到98.09%、79.01%和56.39%的识别准确率。

Conclusion: Exp-Graph在实验室和真实环境中均表现出强大的泛化能力，适用于实际应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2是一种高效适配框架，通过Depthwise-Dilated Adapter增强多尺度特征提取，适用于医学视频分割与跟踪，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态特定设计，适应性差，且SAM2等模型在医学视频场景中需大规模数据重训练，计算成本高。

Method: 提出DD-SAM2框架，结合Depthwise-Dilated Adapter，以最小参数量增强多尺度特征提取，支持小数据量微调。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上表现优异，Dice分数分别达0.93和0.97。

Conclusion: DD-SAM2为医学视频分割与跟踪提供了一种高效适配方案，代码与模型将开源。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [33] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++是一个新型跨模态检测框架，用于识别和解释合成媒体，通过强化学习后训练策略和多阶段训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有单模态检测方法无法应对多模态合成内容。

Method: 采用强化学习后训练策略，结合多阶段训练、思维奖励和混合推理。

Result: BusterX++在性能上取得显著提升，并通过GenBuster++基准验证了其有效性。

Conclusion: BusterX++为跨模态合成媒体检测提供了高效解决方案，具有广泛适用性。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [34] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: 提出了一种基于状态空间模型的多光谱特征融合框架MS2Fusion，通过双路径参数交互机制解决现有方法在局部互补特征和跨模态共享语义上的不足，显著提升了多光谱目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱特征融合方法过度关注局部互补特征而忽视跨模态共享语义，且难以平衡感受野大小与计算复杂度，限制了性能提升。

Method: MS2Fusion采用双路径参数交互机制：一条路径通过跨模态隐藏状态解码挖掘互补信息，另一条路径通过参数共享探索跨模态对齐的相似语义特征。两者在状态空间模型下联合优化。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他先进方法，并在RGB-T语义分割和RGBT显著目标检测任务中表现出泛化能力。

Conclusion: MS2Fusion通过统一框架实现了功能互补和共享语义空间，不仅提升了多光谱目标检测性能，还展示了在其他多光谱感知任务中的潜力。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [35] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个基于AI的框架，用于提升体育裁判的效率和公平性，特别是在跆拳道中实时头部踢击检测和评分。


<details>
  <summary>Details</summary>
Motivation: 传统裁判系统存在延迟、主观性和不一致的问题，影响公平性和运动员信任。

Method: 结合计算机视觉、深度学习和边缘推理，实现动作的自动识别与分类。

Result: 决策时间从分钟级缩短至秒级，提高一致性和透明度。

Conclusion: FST.ai框架具有鲁棒性、可扩展性和跨体育项目的潜力。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [36] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 研究提出了一种基于计算机视觉的框架，通过语义分割RGB图像来量化餐盘级食物浪费，适用于五种伊朗菜肴，模型表现良好，部分达到90%以上的像素比例一致性。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮环境中的食物浪费，支持数据驱动的可持续发展策略。

Method: 使用四种全监督模型（U-Net、U-Net++及其轻量版），采用动态逆频率损失和AdamW优化器，通过多种指标评估性能。

Result: 模型表现良好，部分食物类型的像素比例一致性达90%以上，轻量模型实现实时推理。干燥、刚性食物分割效果更佳。

Conclusion: 该框架为大规模餐饮环境中的实时浪费监测提供了可扩展的无接触解决方案，并为减少食物浪费提供了可行方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [37] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通过双路径多级判别框架，增强组织病理学图像与基因表达谱之间的跨模态表示对齐，提升基因表达预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用组织病理学图像与基因表达谱的多层次跨模态对齐，限制了预测性能。

Method: 提出Gene-DML框架，通过双路径多级判别（多尺度实例级判别和跨级实例-组判别）增强形态学与转录模态的对应关系。

Result: 在公共空间转录组数据集上，Gene-DML实现了基因表达预测的最先进性能。

Conclusion: Gene-DML通过学习鲁棒的跨模态表示，提升了预测准确性和泛化能力。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [38] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 论文提出了Doc-750K数据集和Docopilot模型，解决了多模态大语言模型在复杂文档理解中的不足，无需依赖检索增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂多页文档理解上表现不足，且检索增强生成方法存在上下文碎片化、多阶段误差累积等问题。

Method: 构建了Doc-750K数据集，包含多样文档结构和跨页依赖关系，并开发了原生多模态模型Docopilot。

Result: Docopilot在文档理解任务中表现出更高的连贯性、准确性和效率。

Conclusion: Docopilot为文档级多模态理解设立了新基准，数据集和模型已开源。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [39] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一种新型协作多代理系统，用于多模态WSI分析，通过任务分配、验证机制和总结模块提升准确性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在WSI分析中表现不如任务特定模型，协作多代理系统在病理学领域潜力未充分挖掘。

Method: WSI-Agents包括任务分配模块（使用模型库分配任务）、验证机制（内部一致性和外部验证）和总结模块（生成最终摘要和视觉解释图）。

Result: 实验表明WSI-Agents在多模态WSI基准测试中优于现有WSI MLLMs和医疗代理框架。

Conclusion: WSI-Agents通过协作多代理系统平衡了多功能性和准确性，为病理学领域提供了高效解决方案。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [40] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 论文提出了一种名为MIPD的新框架，通过从教师MLLM中蒸馏多模态知识，提升小规模GSR模型的泛化和零样本能力，解决了复杂场景识别和边缘设备部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在复杂GSR任务中表现不佳且资源消耗大，而传统GSR模型泛化能力不足。论文旨在通过知识迁移提升小模型的性能。

Method: 提出MIPD框架，包括LLM生成的理性判断、场景感知和实例感知提示，以及负向引导的多模态提示对齐模块，将知识蒸馏到学生模型中。

Result: 在Ov-SWiG和HICO-DET数据集上，MIPD在已知、罕见和未知场景中表现优异，并提升了未知检测能力。

Conclusion: MIPD通过知识蒸馏有效提升了小模型的泛化和零样本能力，为复杂场景识别提供了新思路。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [41] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 论文提出了首个全球梯田地块精细数据集GTPBD，覆盖复杂梯田地形的200,000多个地块，支持多种任务如语义分割和边缘检测。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究缺乏对复杂梯田地形的精细表示，无法满足精准农业需求。

Method: 构建GTPBD数据集，包含47,537张高分辨率图像和三级标注（边界、掩码、地块标签），覆盖全球多地。

Result: GTPBD在多种任务（如语义分割、边缘检测）上进行了基准测试，填补了梯田遥感研究的空白。

Conclusion: GTPBD为精细农业地形分析和跨场景知识迁移提供了基础设施。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [42] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet结合视网膜成像、社会经济因素和共病资料，通过多模态融合和临床延迟系统，提高糖尿病视网膜病变（DR）分期的准确性，特别关注低收入人群的早期检测。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球可预防性失明的主要原因，低收入人群因筛查机会有限，更容易进展到晚期。共病条件进一步加速疾病发展。

Method: 提出MultiRetNet，整合视网膜成像、社会经济因素和共病资料，采用三种多模态融合方法，最优为全连接层融合。通过合成对抗性低质量图像和对比学习训练延迟系统，识别需临床复查的异常样本。

Result: 系统在低质量图像上保持诊断准确性，整合关键健康数据，提高早期检测率，尤其对服务不足人群有效。

Conclusion: MultiRetNet可降低医疗成本，提高早期检测率，减少医疗资源分配不均，促进医疗公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [43] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: 论文提出了InterAct VideoQA数据集，用于提升视频问答模型在复杂交通场景中的表现，包含8小时真实交通视频和25,000个QA对。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答模型难以处理真实交通场景中的复杂时空动态和多事件并发问题。

Method: 构建InterAct VideoQA数据集，包含多样化的交通视频片段和QA对，并评估和微调现有模型。

Result: 模型在InterAct VideoQA上表现不佳，但微调后性能显著提升。

Conclusion: 领域特定数据集对提升视频问答模型在智能交通系统中的实用性至关重要。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [44] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA提出了一种结合因果感知查询优化和细粒度视觉定位的方法，显著提升了视频问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答方法存在任务无关采样和启发式检索的局限性，无法有效捕捉关键事件和因果时序结构。

Method: 利用LLM优化问题-选项对，通过时间定位模型精确检索关键片段，并结合自适应融合机制和MLLM生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA数据集上实现了SOTA性能，同时保持计算效率。

Conclusion: LeAdQA通过因果感知和视觉定位的协同作用，显著提升了复杂推理任务的性能。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [45] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS框架首次实现了对冻结Vision Transformers（ViTs）的高效空间-光谱可解释性，解决了现有方法在HSI数据中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉有意义的光谱线索且计算成本高，阻碍了ViTs在HSI数据中的可解释性应用。

Method: FOCUS引入类特定光谱提示和可学习的[SINK]令牌，通过单次前向生成稳定的3D显著性图和光谱重要性曲线。

Result: FOCUS将波段级IoU提高15%，减少注意力崩溃40%以上，且结果与专家标注高度一致。

Conclusion: FOCUS以不到1%的参数开销，为高分辨率ViT可解释性提供了实用解决方案，填补了黑盒建模与可信HSI决策之间的空白。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [46] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 本文提出了一种轻量化的机器人动作预测方法，利用InstructPix2Pix模型实现多模态未来帧预测，显著降低了计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹在机器人、自主系统和人类活动预测等领域至关重要，但传统视频预测模型计算成本高且推理慢。

Method: 通过改进InstructPix2Pix模型，结合视觉和文本输入，实现单图像和文本提示的多模态未来帧预测。

Result: 在RoboTWin数据集上，该方法在SSIM和PSNR指标上优于现有基线，且计算效率更高。

Conclusion: 该方法为机器人动作预测提供了一种高效、轻量化的解决方案，特别适用于需要快速推理和多模态控制的应用场景。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [47] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于信息互补的下采样方法HPD，通过MinMaxPooling替代传统方法，在语义分割任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法可能导致关键空间信息丢失，影响像素级预测精度，因此需要一种能保留图像细节特征的新方法。

Method: 提出Hybrid Pooling Downsampling (HPD)，利用MinMaxPooling提取局部区域的最大值信息，保留图像明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上的实验表明，HPD在分割性能上优于传统方法，DSC系数平均提高0.5%。

Conclusion: HPD模块为语义分割任务提供了一种高效解决方案。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [48] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EPD的新ODE求解器，通过并行梯度评估减少截断误差，实现高质量低延迟采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因顺序去噪导致采样延迟高，现有加速方法在低延迟预算下图像质量下降。

Method: EPD利用多个并行梯度评估优化ODE步长，通过蒸馏学习少量参数，可作为插件提升现有ODE采样器。

Result: 在5 NFE延迟下，EPD在多个数据集上FID显著优于现有学习型求解器。

Conclusion: EPD在保持低延迟的同时显著提升了图像生成质量。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [49] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI是一个基于伦理和偏见的AI框架，用于水下图像增强，解决了数据集偏见、高计算成本和透明度问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对海洋保护（如珊瑚礁监测）至关重要，但现有AI模型存在偏见、高成本和透明度不足的问题。

Method: EBA-AI利用CLIP嵌入检测和缓解数据集偏见，结合自适应处理优化能源效率，并通过不确定性估计和可解释性技术增强信任。

Result: 实验显示PSNR下降1.0 dB，但计算效率显著提升，支持实时大规模监测。与其他方法相比，EBA-AI在效率、公平性和可解释性上表现优异。

Conclusion: EBA-AI为可持续、偏见感知且高效的水下图像处理提供了解决方案，推动了海洋保护的发展。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [50] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 论文评估了DUSt3R、MASt3R和VGGT等3D重建模型在稀疏航拍图像上的表现，发现它们能从极少图像中重建密集点云，但高分辨率和大规模图像集仍有局限。


<details>
  <summary>Details</summary>
Motivation: 探索这些模型在航拍图像块上的潜力，填补现有研究空白。

Method: 在UseGeo数据集的航拍图像块上评估预训练模型，进行姿态估计和密集3D重建。

Result: 模型能从少于10张图像（分辨率最高518像素）重建密集点云，完整性比COLMAP高50%，VGGT计算效率更高。

Conclusion: Transformer方法无法完全取代传统SfM和MVS，但在低分辨率、稀疏场景下可作为补充。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [51] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 论文提出LASED数据集和可转向CNN，解决无人机视觉定位中的数据集不足和旋转模糊问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉定位面临大规模高空数据集稀缺和图像旋转模糊的挑战，限制了模型的泛化能力。

Method: 引入LASED大规模数据集，并提出使用可转向CNN处理旋转模糊。

Result: LASED数据集和可转向CNN显著提升召回率，后者平均比传统网络高12%。

Conclusion: 结合大规模数据集和旋转等变网络，显著增强了无人机视觉定位的鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [52] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一种基于视觉提示的低级视觉任务统一框架VPIP，通过输入-目标图像对指导模型完成多种任务，验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决低级视觉任务多样性带来的统一建模挑战。

Method: 设计了包含图像处理主干、提示编码器和提示交互模块的VPIP框架，并开发了统一模型GenLV。

Result: 在多任务基准测试中表现优异，增加训练任务数量提升了泛化能力，尤其在数据有限的任务中。

Conclusion: VPIP框架展示了作为低级视觉统一建模基础的潜力，具有强适应性和可扩展性。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [53] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种新颖的LiDAR-视觉里程计框架，结合LiDAR点云和图像，通过深度补全和多尺度特征提取实现高精度姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决自主系统中自定位和导航的关键问题，提升里程计的准确性和鲁棒性。

Method: 利用深度补全生成稠密深度图，结合多尺度特征提取网络和注意力机制，通过层次化姿态优化模块逐步优化运动估计。

Result: 在KITTI里程计基准测试中表现优异，准确性和鲁棒性达到或超越现有方法。

Conclusion: 提出的LiDAR-视觉融合框架在动态环境和尺度模糊场景下表现出色，为自主系统提供了可靠的里程计解决方案。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [54] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 提出了一种基于人类认知的多脸深度伪造视频检测方法HICOM，通过分析人类依赖的四种关键线索，显著提升了检测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法在单脸场景表现良好，但在多脸场景中因缺乏上下文线索而表现不佳。

Method: 通过人类研究识别四种关键线索（场景-运动一致性、脸间外观兼容性、人际注视对齐和脸体一致性），并开发了HICOM框架。

Result: HICOM在基准数据集上平均准确率提升3.3%，在真实扰动下提升2.8%，在未见数据集上优于现有方法5.8%。

Conclusion: 结合人类认知线索可有效提升深度伪造检测性能，HICOM框架具有高泛化性和可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [55] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一种基于人类视频训练的灵巧视觉-语言-动作模型（VLA），通过物理指令调优和部分级运动标记化方法，解决了现有VLA在复杂操作任务中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLA依赖合成数据或有限规模的遥操作演示，难以处理高灵巧性任务或适应新场景。

Method: 结合大规模VLA预训练、物理空间对齐和机器人任务后适应，提出物理指令调优和部分级运动标记化方法。

Result: Being-H0在手部运动生成和指令跟随方面表现优异，并能扩展到模型和数据规模。

Conclusion: 物理指令调优显著提升了Being-H0在真实机器人操作中的性能。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [56] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant是一种统一的量化框架，通过自适应结合互补技术提升扩散模型的跨模型通用性，解决了现有后训练量化方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成能力上表现出色，但计算成本高，难以在资源受限或延迟敏感的环境中部署。现有后训练量化方法依赖特定架构启发式，通用性差。

Method: 提出SegQuant框架，包含SegLinear（分段感知的图量化策略）和DualScale（双尺度量化方案），以捕捉结构语义和空间异质性，同时保持视觉保真度。

Result: SegQuant在多种扩散模型上表现优异，且与主流部署工具兼容。

Conclusion: SegQuant为扩散模型提供了一种高效、通用的量化解决方案，适用于工业部署。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [57] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench是首个专注于真实世界金融图表的基准测试，包含1200张图表和7016个问题。评估了25种LVLM，揭示了性能差距缩小、模型升级后性能下降、指令遵循困难、空间推理能力有限以及LVLM不可靠等问题。


<details>
  <summary>Details</summary>
Motivation: 金融图表因复杂的时序结构和领域术语而未被充分研究，需要专门的基准测试。

Method: 构建FinChart-Bench数据集，包含1200张金融图表和7016个问题，评估25种LVLM。

Result: 发现开源与闭源模型性能差距缩小，模型升级后性能下降，指令遵循和空间推理能力不足，LVLM不可靠。

Conclusion: 当前LVLM在金融图表理解方面存在显著局限性，FinChart-Bench为未来研究提供了基准。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [58] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 论文提出了一种物理引导的雾霾转移网络（PHATNet），通过将未见目标域的雾霾模式转移到源域无雾图像上，生成特定域的微调数据集，以提升去雾模型的域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在未见真实雾霾图像上性能显著下降，因训练数据有限。

Method: 提出PHATNet，利用雾霾模式预测比恢复干净内容更容易的观察，通过雾霾转移生成域特定微调集，并引入雾霾转移一致性和内容泄漏损失。

Result: 实验表明PHATNet显著提升了最先进去雾模型在真实图像数据集上的性能。

Conclusion: PHATNet通过灵活域适应方法有效提升了去雾模型的泛化能力。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [59] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种无需外部条件的配对图像生成方法，用于解决DBT图像中肿块分割的数据标注不足问题。


<details>
  <summary>Details</summary>
Motivation: 高密度乳腺组织导致肿块隐蔽，手动标注困难且耗时，现有扩散模型生成图像质量低且无法生成标注。

Method: 训练额外的扩散引导器，实现条件扩散模型的配对图像生成，生成DBT切片和肿块掩码。

Result: 实验表明，该方法提高了生成质量，缓解了标注数据短缺，提升了下游任务性能。

Conclusion: 该方法有效解决了数据标注不足问题，为肿块分割任务提供了高质量的训练数据。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [60] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出了一种仅需稀疏深度测量和对应图像的自监督深度补全方法，无需密集标签或多帧图像。


<details>
  <summary>Details</summary>
Motivation: 密集深度标注成本高，多帧依赖限制了自监督方法在静态或单帧场景的适用性。

Method: 利用深度分布特性设计新损失函数，结合视觉基础模型生成的语义分割图增强深度估计。

Result: 实验证明该方法有效。

Conclusion: 新方法解决了现有方法的局限性，提升了深度补全的效果。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [61] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言和基础模型的全能视频修复框架，无需预知退化信息，并在推理时无额外成本。同时呼吁标准化全能视频修复的基准测试，并提出了新的多退化基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预知退化信息，限制了灵活性和可解释性。本文旨在通过自然语言和基础模型提供更灵活、可解释的修复指导。

Method: 利用基础模型通过自然语言理解视频帧的退化语义上下文，学习退化知识的近似表示，并在推理时无需基础模型参与。

Result: 在提出的多退化基准测试中，方法表现优于现有技术，达到最新性能。

Conclusion: 提出的框架在无需预知退化信息的情况下实现了高效修复，并呼吁标准化基准测试以推动领域发展。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [62] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的DETR增强框架，通过建模边界框为高斯分布并引入Gromov-Wasserstein距离损失，提升定位精度和预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器忽略预测不确定性，限制了模型鲁棒性。本文旨在改进边界框定位精度并显式建模不确定性。

Method: 将边界框建模为多元高斯分布，引入Gromov-Wasserstein距离损失，提出Bayes Risk公式过滤高风险信息，并设计算法量化定位不确定性。

Result: 在COCO基准测试中有效提升DETR变体性能，在白细胞检测任务（LISC和WBCDD数据集）上达到SOTA。

Conclusion: 框架在通用和特定领域检测任务中均具有可扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [63] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于微手势的情感识别方法，通过超图增强Transformer在混合监督框架中重构行为模式，并在两个公开数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 微手势能传达人类情感状态，但基于微手势的情感建模研究不足，因此探索其建模方法。

Method: 设计了超图增强Transformer的编码器和解码器，结合自监督和监督学习，通过重构任务和情感识别头实现端到端训练。

Result: 在iMiGUE和SMG数据集上表现优于现有方法，多项指标达到最佳。

Conclusion: 提出的方法有效建模微手势的局部运动与情感状态关系，为情感计算提供了新思路。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [64] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出了一种无需学习的方法，利用稀疏深度测量将基础模型的相对尺度深度预测转换为度量尺度深度，避免了额外的训练或微调。


<details>
  <summary>Details</summary>
Motivation: 基础模型在零样本单目深度估计中表现出色，但其输出通常是相对尺度而非度量尺度，限制了实际应用。现有方法需要额外训练且可能损害模型的泛化能力。

Method: 采用非学习的方法，利用稀疏深度测量将相对尺度深度转换为度量尺度深度，无需重新训练或微调。

Result: 实验证明该方法有效，能够在保持基础模型泛化能力的同时实现度量尺度深度预测。

Conclusion: 该方法填补了相对尺度与度量尺度深度之间的差距，无需额外计算成本或牺牲泛化能力。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [65] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer结合了深度学习和手工方法的优势，提出了一种轻量级的光谱注意力模型，用于远程光电容积描记术（rPPG）估计，并通过谱对比学习（SCL）实现无标签训练。


<details>
  <summary>Details</summary>
Motivation: 深度学习在rPPG估计中依赖大数据集，而手工方法在未见场景中泛化能力强但性能受限，因此需要结合两者优势。

Method: 提出BeatFormer模型，整合了放大的正交复数注意力和频域能量测量，并引入SCL进行无标签训练。

Result: 在PURE、UBFC-rPPG和MMPD数据集上验证了模型的鲁棒性和性能，尤其在运动场景下的跨数据集评估中表现优异。

Conclusion: BeatFormer通过结合深度学习和手工方法的优势，实现了高效且鲁棒的rPPG估计。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [66] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于2D预训练多模态网络的统一方法，用于处理RGB图像、文本和点云数据，简化了架构并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖多模态分离编码器，导致模型复杂且训练效率低。本文旨在通过统一的多模态网络解决这一问题。

Method: 利用2D CLIP双模态模型，通过适配器微调适应三模态设置，设计GARF模块融合几何多尺度特征，并引入多模态解码器。

Result: 相比基线，可训练参数减少约58%，3D检测任务性能提升6.52%，3D视觉定位任务提升6.25%。

Conclusion: 提出的方法实现了跨模态的统一特征提取与融合，显著简化了模型架构并提升了性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [67] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 提出了一种语义感知表示学习（SARL）方法，通过标签语义相关特征学习和最优传输注意力机制，提升多标签图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如注意力机制或GCN）的表示可能包含噪声且定位不精确，需要更精确的语义对齐表示。

Method: 使用标签语义相关特征学习模块提取特征，设计最优传输注意力机制实现语义对齐，结合区域分数聚合策略进行预测。

Result: 在PASCAL VOC 2007和MS-COCO数据集上，SARL优于现有方法。

Conclusion: SARL通过语义对齐和噪声抑制，显著提升了多标签图像分类的准确性。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [68] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 提出了一种解耦框架\method，用于高效预测3D高斯分布，通过立体视觉提取特征并融合，实现无姿态依赖的高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯几何和外观预测上耦合性强，依赖数据驱动先验且回归速度慢，需解决资源消耗大和实用性不足的问题。

Method: 使用立体视觉骨干网络提取局部图像对特征，通过全局注意力块融合，生成几何和外观的高斯特征，结合为GS-maps，并通过细化网络提升重建质量。

Result: 实现了无姿态依赖的3D重建，提高了鲁棒性和实用性，同时减少了资源需求。

Conclusion: \method为现实世界的3D内容生成提供了高效、可扩展的解决方案。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [69] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 提出了一种基于多维尺度分析（MDS）和鲁棒优化的冷冻电镜姿态估计方法，显著提升了低信噪比条件下的重建精度。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜中姿态估计和位移校正是关键挑战，低信噪比直接影响3D重建的准确性。现有方法对噪声敏感且约束条件处理不足。

Method: 结合MDS技术和鲁棒优化框架（如ℓ₁范数目标），同时估计旋转轴和平面内向量，并通过迭代位移校正算法优化全局平移。

Result: 在欧拉角精度和重建保真度（FSC）上均优于现有方法。

Conclusion: 该方法通过鲁棒优化和严格约束，显著提升了低信噪比条件下的姿态估计和重建质量。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [70] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种新的概率框架，用于多实例学习（MIL）中的注意力机制，通过估计注意力值的概率分布，捕捉全局和局部交互，并在医学影像分类中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法将注意力值视为确定性，可能忽略个体实例贡献的不确定性。

Method: 提出概率框架，估计注意力值的概率分布，同时考虑全局和局部交互。

Result: 在三个医学数据集和十一个基线模型的评估中，该方法在预测性能和不确定性解释方面表现优异。

Conclusion: 该方法不仅提升了预测性能，还提供了可解释的不确定性映射，有助于疾病定位。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [71] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 论文提出了开放集跨模态泛化（OSCMG）任务，并提出了MICU方法，包含FCMI和CUJP两个组件，以解决开放集环境下的多模态统一表示问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态泛化方法缺乏对开放集环境的考虑，而实际应用中常遇到未见类别和新模态的情况。

Method: 提出MICU方法，包括FCMI（细粒度与粗粒度掩码对比学习）和CUJP（跨模态统一拼图），以增强多模态对齐和特征多样性。

Result: 在CMG和新提出的OSCMG任务上验证了方法的有效性。

Conclusion: MICU方法在开放集跨模态泛化任务中表现出色，解决了现有方法的局限性。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [72] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一种上下文感知框架，通过动态激活轻量级适配器（LoRA）实现嵌入式设备上的实时多标签视频分类，显著降低能耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备在实时多标签视频分类中面临计算和能源预算的限制，而视频流的结构特性（如标签稀疏性、时间连续性和标签共现）可被利用以提高效率。

Method: Polymorph框架动态激活每帧所需的轻量级LoRA适配器，这些适配器专用于共现模式中的子类，避免了全模型切换和权重合并。

Result: 在TAO数据集上，Polymorph能耗降低40%，mAP提升9个百分点。

Conclusion: Polymorph通过模块化策略显著提升了嵌入式设备上视频分类的效率和性能，且已开源。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [73] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的低重叠点云配准评估方法，显著提升了现有配准算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标在极低内点比例下失效，需重新审视配准结果评估问题。

Method: 构建数据集并训练深度学习分类器，评估配准质量，并将其集成到标准配准流程中。

Result: 与GeoTransformer结合，在3DLoMatch基准上达到86.97%的配准召回率，并在ETH数据集上表现出强泛化能力。

Conclusion: 深度学习框架首次全面解决了低重叠点云配准评估问题，显著提升了配准性能。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [74] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL提出了一种分层跨模态提示学习框架，解决了预训练视觉语言模型在适应下游任务时的模态隔离和语义衰减问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型（如CLIP）在下游任务中难以保持泛化能力，现有提示学习方法存在模态隔离和分层语义衰减的瓶颈。

Method: HiCroPL通过双向知识流和分层知识映射器，实现文本和视觉模态的语义相互增强，并引入轻量级知识代理提升效率。

Result: 在四个任务的11个基准测试中，HiCroPL取得了最先进的性能，显著优于现有方法。

Conclusion: HiCroPL通过跨模态交互和分层语义保留，有效提升了模型的泛化能力，为视觉语言模型的适应提供了新思路。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [75] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 论文提出RvTC方法，通过灵活的基于分箱的分类替代预设词汇分类，结合数据特定提示提升多模态大语言模型在图像回归任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在图像回归任务中依赖预设词汇和通用提示，未能充分利用文本输入的语义信息，性能与仅图像模型相当。

Method: 提出Regression via Transformer-Based Classification (RvTC)，采用分箱方法替代词汇约束分类，并通过数据特定提示增强语义理解。

Result: 在四个图像评估数据集上达到最优性能，AVA数据集中提示加入挑战标题将相关性从0.83提升至0.90。

Conclusion: 语义提示信息对多模态回归任务至关重要，RvTC方法显著提升了模型性能。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [76] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于轴对齐几何约束的文档去扭曲方法，通过利用物理文档的几何特性，显著提升了去扭曲效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法主要依赖标注数据进行监督回归，而未充分利用物理文档的固有几何特性。

Method: 在训练阶段引入轴对齐几何约束，推理阶段采用轴对齐预处理策略，并提出了新的评估指标AAD。

Result: 在多个基准测试中达到SOTA结果，AAD指标提升18.2%~34.5%。

Conclusion: 通过几何约束和预处理策略，显著提升了文档去扭曲的性能和鲁棒性。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [77] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于B样条曲线拟合的新方法，用于改进FastSAM的边缘质量，解决了其生成的锯齿状边缘问题，同时保持了实时处理能力。


<details>
  <summary>Details</summary>
Motivation: FastSAM虽然实现了实时分割，但其生成的边缘存在锯齿状问题，影响了分割的视觉质量和分析准确性。

Method: 采用四阶段细化过程，包括两轮B样条曲线拟合，以平滑锯齿状边缘。

Result: 显著提升了边缘的视觉质量和分析准确性，同时保持了实时处理能力。

Conclusion: 该方法增强了FastSAM的实用性，使其在工业自动化、医疗成像和自主系统等需要精确边缘识别的场景中更具潜力。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [78] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了Video Thinking Test（Video-TT），用于评估视频大语言模型（video LLMs）在真实视频理解中的正确性和鲁棒性，发现其与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在视频理解的正确性和鲁棒性方面与人类智能存在差距，缺乏合适的评估基准。

Method: 设计了Video-TT测试，包含1,000个YouTube Shorts视频，每个视频配有一个开放式问题和四个对抗性问题，以评估模型对复杂视觉叙事的理解和鲁棒性。

Result: 评估结果显示，视频大语言模型在正确性和鲁棒性方面与人类表现存在显著差距。

Conclusion: Video-TT揭示了视频大语言模型在真实视频理解中的不足，为未来研究提供了改进方向。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [79] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波动方程数据集，旨在解决传统数值求解器和现有神经算子在真实成像应用中的局限性，为医学成像提供高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统波动方程数值求解器计算量大且不稳定，而现有神经算子数据集过于简化，无法满足真实成像需求。OpenBreastUS填补了这一空白。

Method: OpenBreastUS包含8,000个解剖学真实的人体乳腺模型和超过1,600万次频域波动模拟，用于全面评估神经算子的性能。

Result: 该数据集首次实现了神经算子求解器在人体乳腺活体成像中的高效应用。

Conclusion: OpenBreastUS为开发创新神经PDE求解器提供了平台，并推动了其在真实医学成像中的部署。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [80] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON是一个无需训练的通用虚拟试穿框架，通过分离服装和姿势条件，实现跨场景的高保真纹理和姿势一致性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术要么依赖监督学习（限制跨域泛化），要么是无监督方法（受数据偏差限制），缺乏统一的解决方案。

Method: 提出服装先验生成机制和连续边界缝合技术以保留细节，利用DDIM反演实现精确姿势对齐。

Result: 实验表明OmniVTON在多样数据集和场景中表现优异，首次实现多人物虚拟试穿。

Conclusion: OmniVTON通过解耦服装和姿势约束，解决了扩散模型在多条件处理中的偏差问题，成为首个通用且无需训练的虚拟试穿框架。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [81] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: 提出PanTiny，一种轻量级单步全色锐化框架，通过多数据集联合训练和复合损失函数，实现高效且泛化性强的性能。


<details>
  <summary>Details</summary>
Motivation: 当前全色锐化模型趋向复杂且针对单一数据集，导致计算开销大且泛化性差，PanTiny旨在解决这一问题。

Method: 设计轻量级单步框架PanTiny，采用多数据集（WV2、WV3、GF2）联合训练和复合损失函数。

Result: PanTiny在性能和效率上优于大型专用模型，显著提升全分辨率数据的泛化能力。

Conclusion: 通过模型设计、训练范式和损失函数的优化，PanTiny展示了高效、泛化性强且数据敏感的模型设计方向。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [82] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一个基于视频扩散模型的框架，通过可学习的姿态对齐和身份保持技术，生成高质量的人体动画视频。


<details>
  <summary>Details</summary>
Motivation: 现有的人体图像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性。

Method: 结合可学习的姿态对齐模块、全局内容感知的面部编码器和分布感知的身份适配器，优化扩散轨迹。

Result: 实验证明 StableAnimator++ 在质量和数量上均优于现有方法。

Conclusion: StableAnimator++ 通过创新的模块设计，显著提升了身份一致性和生成质量。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [83] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 评估当前最先进的生成模型在文本图像生成和编辑中的能力，提出将真实感文本图像生成作为通用生成模型的基础技能。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型是否能掌握文本图像生成和编辑的复杂性，并评估其在OCR任务中的表现。

Method: 选择33个代表性任务，分为五类，评估六种模型在闭源和开源领域的表现。

Result: 发现当前生成模型在OCR任务中的弱点，提出改进方向。

Conclusion: 应将真实感文本图像生成作为通用生成模型的基础技能，而非依赖专用解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [84] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 论文提出了一种新的数据集BleedOrigin-Bench和框架BleedOrigin-Net，用于内镜下黏膜剥离术（ESD）中出血源的实时检测与追踪，解决了现有AI方法的不足。


<details>
  <summary>Details</summary>
Motivation: ESD术中出血源的实时定位与追踪对止血干预至关重要，但现有AI方法仅关注出血区域分割，缺乏针对出血源检测和动态追踪的解决方案，且缺乏专用数据集。

Method: 提出了BleedOrigin-Bench数据集（44个手术的106,222帧数据）和BleedOrigin-Net双阶段检测-追踪框架，结合了出血起始检测和空间追踪。

Result: BleedOrigin-Net在出血起始检测（帧级准确率96.85%）、初始源检测（像素级准确率70.24%）和点追踪（像素级准确率96.11%）上表现优异。

Conclusion: BleedOrigin-Bench和BleedOrigin-Net填补了ESD术中出血源检测与追踪的空白，为AI辅助系统提供了有效工具。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [85] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet方法通过多任务ResNet架构改进SLAM闭环检测精度，并优化嵌入式设备实时计算，结合DISK描述符提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM闭环检测精度低和嵌入式设备实时计算受限的问题。

Method: 采用多任务ResNet架构，支持在线动态视觉数据集训练，结合DISK描述符。

Result: LoopNet在多变条件下表现优于传统方法，并提供了新的LoopDB数据集。

Conclusion: LoopNet为SLAM闭环问题提供了高效解决方案，并开源了代码和数据集。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [86] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VPA通过视频预测用户行为序列，提出Auxiliary Task Augmentation和Multi-token Prediction解决数据稀缺和结构化动作空间问题，VideoPlan在COIN和CrossTask上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决长时程视觉规划中数据稀缺和结构化动作空间建模的挑战。

Method: 引入Auxiliary Task Augmentation增强规划能力，采用Multi-token Prediction建模结构化动作空间。

Result: VideoPlan在COIN和CrossTask数据集上分别提升7.3%和3.4%，在Ego4D任务中表现优异。

Conclusion: VideoPlan通过辅助任务和多令牌预测，显著提升了视觉规划任务的性能。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [87] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空多图表示方法，通过解耦空间图和时间图，提升了事件传感器数据的建模能力，显著提高了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器数据稀疏且异步，传统方法将其转换为密集张量会丧失优势，而现有图方法对时空动态建模不足，限制了性能。

Method: 构建解耦的空间图（使用B样条基函数建模全局结构）和时间图（基于运动向量的注意力机制建模局部动态），替代计算昂贵的3D核。

Result: 在Gen1和eTraM数据集上，检测精度提升6%，速度提升5倍，参数减少且计算成本不变。

Conclusion: 结构化图建模能有效提升异步视觉任务的性能，证明了方法的优越性。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [88] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba利用Mamba-SSMs高效学习3D关节网格模型，支持超过10,000顶点的生成与重建，并开发了MambaDiff3D和Mamba-HMR两个应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理大规模3D网格（如衣物和手部几何）时的效率与扩展性问题。

Method: 通过序列化技术将网格顶点排序，利用Mamba-SSMs处理，设计了MambaDiff3D（生成模型）和Mamba-HMR（重建模型）。

Result: MambaDiff3D在生成任务中表现优异；Mamba-HMR扩展了非参数化方法的能力，支持全身重建。

Conclusion: MeshMamba在3D网格生成与重建任务中高效且可扩展，优于现有方法。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [89] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 论文提出N-JEPA方法，将扩散噪声引入掩码图像建模（MIM），结合自监督学习（SSL）与生成模型的优势，提升表示能力。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在判别任务中表现优异，但生成模型在图像生成和细节增强方面更优。结合两者可增强SSL的表示能力。

Method: 提出N-JEPA方法，通过掩码标记的位置嵌入引入扩散噪声，并采用多级噪声调度增强模型鲁棒性。

Result: 在分类任务中验证了方法的有效性。

Conclusion: N-JEPA成功结合SSL与生成模型，为下游任务提供了更强的表示能力。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [90] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种基于分层的部分框架方法，用于3D血管生成，通过分离全局拓扑和局部几何细节，显著提升了复杂血管网络的建模效果。


<details>
  <summary>Details</summary>
Motivation: 由于血管的复杂分支、弯曲和不规则形状，准确建模其几何和拓扑结构仍具挑战性。

Method: 采用三阶段方法：生成关键图建模全局层次结构，基于几何属性生成血管段，最后整合局部段到全局图中。

Result: 在真实数据集上验证，性能优于现有方法，首次成功应用部分生成方法于3D血管建模。

Conclusion: 该框架为血管数据生成设定了新基准，代码已开源。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [91] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的解释方法，用于分析乳腺影像中的基础模型Mammo-CLIP，揭示了模型决策的临床相关特征和潜在干扰因素。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像等高风险领域，模型决策的可解释性对临床应用至关重要。

Method: 通过训练一个基于Mammo-CLIP的SAE模型，识别和探测与乳腺临床概念相关的潜在特征。

Result: 研究发现SAE的潜在神经元与真实区域对齐，并揭示了影响模型决策的干扰因素。

Conclusion: SAE的潜在表示为理解乳腺影像基础模型的内部机制提供了新视角。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [92] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 论文提出了一种名为Coalescent Projection（CP）的新方法，结合伪类生成和自监督变换（SSTs），在极端领域偏移的CD-FSL任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有CD-FSL方法因更新过多参数导致过拟合的问题。

Method: 提出CP作为soft prompts的替代，结合伪类生成和SSTs，仅依赖基础领域数据。

Result: 在BSCD-FSL基准测试中表现优于现有SOTA方法。

Conclusion: CP和SSTs的组合有效提升了模型在极端领域偏移下的性能。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [93] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus是一个无需训练即可激活扩散变换器（DiT）能力的框架，通过注意力共享机制、动态移位分析和多模态大语言模型（MLLMs）实现高质量主题驱动合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练过程，限制了实际应用，且未能充分利用扩散变换器的零样本潜力。FreeCus旨在填补这一空白。

Method: 1）注意力共享机制；2）动态移位分析改进特征提取；3）集成MLLMs增强跨模态语义表示。

Result: 实验表明，FreeCus在无需额外训练的情况下，实现了与需训练方法相当或更优的主题合成效果。

Conclusion: FreeCus成功解锁了DiT的零样本能力，兼容现有流程，为设计工作流和娱乐应用提供了新可能。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [94] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 论文提出了一种基于近似盲PnP的对应学习方法（MinCD-PnP），通过最小化学习到的2D和3D关键点之间的Chamfer距离，解决了传统差分PnP对噪声和异常值敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统差分PnP在图像到点云（I2P）配准中对噪声和异常值敏感，影响了对应学习的有效性。受盲PnP对噪声和异常值的鲁棒性启发，作者提出了近似盲PnP方法。

Method: 提出MinCD-PnP方法，简化盲PnP为最小化2D和3D关键点之间的Chamfer距离。设计轻量级多任务学习模块MinCD-Net，可集成到现有I2P配准架构中。

Result: 在多个数据集（7-Scenes、RGBD-V2、ScanNet等）上实验表明，MinCD-Net在跨场景和跨数据集设置中表现优于现有方法，具有更高的内点率（IR）和配准召回率（RR）。

Conclusion: MinCD-PnP和MinCD-Net有效解决了传统差分PnP的敏感性问题，提升了I2P配准的鲁棒性和性能。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [95] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过多粒度条件、紧凑表示和多条件训练模块，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐重建中的优势，优化视频压缩的感知质量。

Method: 将视频压缩重构为条件生成任务，引入多粒度条件、紧凑表示和多条件训练模块。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器，尤其在高压缩比下表现突出。

Conclusion: 条件扩散模型在视频压缩中具有显著优势，能够实现感知优化的高质量重建。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [96] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的上下文学习框架，用于检测生物识别系统中的物理呈现攻击和数字变形攻击，其性能优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 随着生物识别系统检测方法的改进，攻击技术也日益复杂。传统深度学习模型在适应不同类型攻击或环境条件变化时表现不佳，且需要大量训练数据。本研究旨在探索VLM在安全关键场景中的应用。

Method: 提出了一个基于开源模型的上下文学习框架，首次系统性地定量评估VLM在安全关键场景中的表现。

Result: 实验结果表明，该框架在物理和数字攻击检测中具有竞争力，且无需资源密集型训练。

Conclusion: 该框架为提升攻击检测的泛化能力提供了有前景的工具。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [97] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为DMD的局部密集表示方法，用于在多样捕获条件下实现鲁棒且准确的指纹匹配。


<details>
  <summary>Details</summary>
Motivation: 指纹匹配在多样捕获条件下仍是一个基础挑战，需要一种能够同时捕捉细粒度纹理和判别性细节特征的方法。

Method: 通过以每个检测到的细节为中心和方向的局部块提取描述符，形成三维张量，结合空间结构和语义特征。

Result: 在多种指纹数据集上验证了方法的有效性和泛化性，达到了最先进的准确性和计算效率。

Conclusion: DMD方法在指纹识别中表现出强大的潜力，适用于大规模应用。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [98] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 论文提出了一种基于通道间相关性的空间-通道状态建模模块（SCSM），用于提升少样本目标检测（FSOD）中特征通道的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前少样本目标检测方法在有限训练样本下难以准确提取有效特征通道，高权重通道未必有效，低权重通道可能仍有价值。

Method: 设计了SCSM模块，包含空间特征建模（SFM）和基于Mamba的通道状态建模（CSM），以平衡空间与通道关系学习。

Result: 在VOC和COCO数据集上的实验表明，SCSM模块提升了特征通道的聚焦质量，实现了最先进的性能。

Conclusion: SCSM模块通过建模通道相关性，有效提升了少样本目标检测的性能。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [99] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出BenchDepth，一种通过五个下游代理任务评估深度基础模型（DFMs）的新基准，避免传统对齐指标带来的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有深度评估协议存在不一致性，传统基准依赖对齐指标，导致偏见和不公平比较。

Method: 设计BenchDepth，通过深度补全、立体匹配、单目3D场景重建、SLAM和视觉语言空间理解五个任务评估DFMs。

Result: 对八种先进DFMs进行基准测试，提供关键发现和分析。

Conclusion: BenchDepth为深度模型评估提供新方法，促进未来研究和深度估计的进步。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [100] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过显式建模双特征分布和改进数据稀缺问题，显著提升工业缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统单类异常检测假设异常分布均匀且难以应对真实制造环境中的数据稀缺问题。

Method: ExDD利用并行记忆库捕获正常和异常模式的特征分布，结合潜在扩散模型生成合成缺陷数据，并通过邻域感知评分机制融合距离度量。

Result: 在KSDD2数据集上，ExDD表现优异（I-AUROC 94.2%，P-AUROC 97.7%），最佳增强效果为100个合成样本。

Conclusion: ExDD通过显式双分布建模和合成数据生成，有效解决了工业缺陷检测中的关键限制。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [101] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion利用合成异常生成和双路径特征适应解决路面缺陷检测中的数据稀缺、领域偏移和缺陷多样性问题。


<details>
  <summary>Details</summary>
Motivation: 解决路面缺陷检测中标注数据不足、训练与部署环境间的领域偏移以及缺陷外观的高变异性问题。

Method: 通过潜在扩散模型生成多样化的合成缺陷，使用双路径特征适应器分别处理正常和异常输入，并采用轻量级判别器进行细粒度缺陷模式识别。

Result: 在六个基准数据集上，RoadFusion在分类和定位任务中表现优异，创下多项实际道路检测相关指标的新纪录。

Conclusion: RoadFusion通过合成数据和特征适应技术显著提升了路面缺陷检测的性能和鲁棒性。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [102] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 论文提出了一种使用高保真合成数据集训练模型的方法，无需牺牲准确性且效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的人类中心计算机视觉模型需要大量参数、数据集和计算资源，而合成数据集可以降低成本并提供更好的数据控制。

Method: 通过程序化合成数据集训练模型，确保数据多样性、细节和标签准确性。

Result: 在深度估计、表面法线估计和软前景分割三个任务上，模型表现与大型模型相当，但成本显著降低。

Conclusion: 合成数据集为高效、低成本且公平的模型训练提供了可行方案。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [103] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet通过多模态语义引导、多尺度交互模块和动态对抗排斥增强损失，提升了面部遮挡下的表情识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有FER模型在面部部分遮挡时难以提取有效特征，导致分类不准确。

Method: 引入多模态语义引导（语义分割图和面部关键点）、多尺度交互模块（MCM）和动态对抗排斥增强损失（DARELoss）。

Result: 在公开基准和自建Occlu-FER数据集上达到SOTA性能。

Conclusion: ORSANet有效解决了遮挡和数据集偏差问题，提升了FER的鲁棒性。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [104] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX是一个新颖的概念解释框架，旨在提高手术阶段识别模型的可解释性，通过将神经元与相关概念关联。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在手术阶段识别中缺乏可解释性，阻碍了信任和调试。

Method: 选择代表性示例序列，构建手术视频数据集的概念集，关联神经元与概念，并识别关键神经元。

Result: 在两个手术阶段识别模型上的实验验证了方法的有效性。

Conclusion: SurgX展示了在解释手术阶段识别方面的潜力。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [105] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune是一种无需训练的令牌修剪方法，专为自我运动视频推理设计，通过关键帧选择、冗余过滤和多样性考虑显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 自我运动视频是具身AI的主要视觉输入，现有方法无法有效处理其时空连续性和运动约束，计算成本高。

Method: EgoPrune包括关键帧选择、视角感知冗余过滤（PARF）和基于MMR的令牌选择器，结合视觉-文本相关性和帧内多样性。

Result: 在两个基准测试中，EgoPrune在多种修剪比例下优于现有方法，显著降低计算资源消耗，并在边缘设备上验证了实用性。

Conclusion: EgoPrune为自我运动视频推理提供了一种高效、实用的解决方案，适用于实际部署。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [106] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda是一种简单有效的视觉语言模型微调方法，通过动态校准融合表示来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了融合表示在决策中的关键作用，RAda旨在填补这一空白。

Method: RAda使用轻量级注意力层生成学习掩码，动态调整融合表示中各元素的贡献。

Result: 在不同设置下，RAda均表现优异，性能接近当前最佳方法。

Conclusion: RAda是一种通用且高效的微调技术，适用于多种场景。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [107] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 论文提出了一种基于众包标注的异常检测数据集，用于改进复杂森林环境中的目标搜索方法。


<details>
  <summary>Details</summary>
Motivation: 在德国农村一起家庭谋杀案中，传统搜索方法因植被遮挡而失效，亟需新方法支持。

Method: 通过研究飞机采集高分辨率航拍图像，结合众包标注生成数据集，并开发交互式网络界面。

Result: 现有方法在初始测试中表现不佳，表明需要上下文感知的改进方法。

Conclusion: 该数据集为复杂环境中的异常检测提供了基准，支持搜索和救援任务，并开放访问以促进研究。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [108] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR是一个交互式文本到视频检索框架，通过量化文本模糊性、映射不确定性和帧不确定性，生成针对性澄清问题，显著减少检索模糊性。


<details>
  <summary>Details</summary>
Motivation: 当前交互式文本到视频检索方法依赖启发式策略，未显式量化不确定性，限制了其效果。

Method: UMIVR通过语义熵、Jensen-Shannon散度和时序质量采样器量化三种不确定性，并生成澄清问题。

Result: 在MSR-VTT-1k数据集上，UMIVR在10轮交互后Recall@1达到69.2%。

Conclusion: UMIVR为交互式文本到视频检索建立了不确定性最小化的基础。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [109] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer是一种基于Transformer的低光增强方法，通过动态积分图像表示和光照引导的自注意力机制，有效解决了非均匀光照场景下的亮度恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在非均匀光照场景（如背光和阴影）中表现不佳，导致过曝或亮度恢复不足。

Method: 提出动态积分图像表示建模空间变化的光照，构建SAI²E估计器，并引入光照引导的多头自注意力机制（IG-MSA）。

Result: 在五个标准低光数据集和跨域基准（LOL-Blur）上，SAIGFormer在定量和定性指标上均显著优于现有方法。

Conclusion: SAIGFormer在非均匀光照增强中表现优异，并展现出强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [110] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 本文提出了一种自监督程序学习框架，通过融合Gromov-Wasserstein最优传输和对比正则化，解决了现有方法在顺序变化、冗余帧和重复动作中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有程序学习方法在未标记视频中学习关键步骤及其顺序时，常因顺序变化、冗余帧和重复动作而性能受限。

Method: 提出了一种融合Gromov-Wasserstein最优传输和结构先验的自监督框架，并引入对比正则化以避免嵌入空间退化。

Result: 在EgoProceL、ProceL和CrossTask等基准测试中表现优于传统方法（如OPEL）。

Conclusion: 该方法通过优化时间对齐和对比正则化，显著提升了程序学习的性能。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [111] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 论文提出了一种新的图表示方法SSG-Com和数据集Endoscapes-SG201，用于更全面地建模手术场景中的工具-动作-目标组合和操作手身份，提升了手术场景理解的效果。


<details>
  <summary>Details</summary>
Motivation: 手术场景理解需要建模复杂的工具、解剖结构及其交互，现有图表示方法未能充分探索工具-动作-目标组合和操作手身份的重要性。

Method: 提出了Endoscapes-SG201数据集和SSG-Com方法，通过图结构建模工具-动作-目标组合和操作手身份。

Result: 实验表明，这些关键元素的整合显著提升了手术场景理解任务（如安全性评估和动作三元组识别）的性能。

Conclusion: 论文强调了工具-动作-目标组合和操作手身份在手术场景理解中的重要性，并提出了一种有效的图表示方法。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [112] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa提出了一种零样本人-物交互检测方法，通过低秩分解VLM特征适应，提升对未见类别的泛化能力和动作区分能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在零样本HOI检测中难以区分相同对象的不同动作或泛化到未见类别的问题。

Method: 使用低秩分解VLM文本特征，生成类共享基特征和可调权重，结合LLM动作正则化优化权重适应。

Result: 在HICO-DET数据集上，未见动词设置下达到27.91的mAP，创下新纪录。

Conclusion: HOLa通过低秩分解和权重适应，显著提升了零样本HOI检测的性能和泛化能力。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [113] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Dynamic-Image（DynImg）的视频表示方法，通过引入非关键帧作为时间提示，提升对快速移动物体空间特征的关注，从而改善视频理解任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法将空间和时间信息分开处理，导致快速移动物体的空间信息难以准确表示，影响时空交互和视频理解。

Method: 提出DynImg方法，利用非关键帧作为时间提示，引导模型关注快速移动物体的细粒度空间特征，并采用4D视频旋转位置嵌入保持时空顺序。

Result: 实验表明，DynImg在多个视频理解基准上优于现有方法约2%。

Conclusion: DynImg通过时间提示有效提升了视频理解的准确性，证明了其方法的有效性。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [114] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一种基于类条件GAN的两阶段图像增强框架，通过生成语义一致的图像替代传统mixup的像素级插值，显著提升了医学图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统mixup的像素级插值在医学图像中可能生成不真实图像，影响学习效果，特别是在高风险应用中。

Method: 使用StyleGAN2-ADA生成器，通过Dirichlet先验采样标签向量，结合Beta分布系数生成连续类流形上的图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据显著提升了所有骨干网络的macro-F1，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种无需改动现有训练流程的增强方法，提供更强的正则化和语义保真度，代码已开源。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [115] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 论文提出了一种卫星上实时变化检测的新框架，通过端到端深度神经网络解决数据存储、图像配准和变化检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像变化检测因数据传输和处理延迟无法满足实时需求，需将整个工作流程移至卫星上。

Method: 框架包含三个子模块：图像压缩、轻量级配准和高效变化检测模型，集成在端到端网络中。

Result: 实验表明，在低功耗硬件上实现了0.7 Mpixel/s的吞吐量，F1分数随压缩率保持竞争力。

Conclusion: 该框架首次在卫星上实现了端到端变化检测，为实时应用提供了可行方案。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [116] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT是一种基于扩散变换器（DiT）的新型皮肤病变分割模型，适用于低成本硬件，并通过Rectified Flow提高生成质量，减少推理步骤。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变分割对皮肤癌诊断和患者监测至关重要，但现有方法在性能和效率上仍有提升空间。

Method: 提出SegDT模型，结合扩散变换器和Rectified Flow技术，优化生成质量和推理速度。

Result: 在三个基准数据集上测试，SegDT达到最先进性能，同时保持快速推理。

Conclusion: SegDT为医学图像分析提供了高效、准确的工具，适用于实际医疗应用。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种结合SDF和3DGS的混合方法，用于稀疏视图图像的重建和新视角渲染，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决SDF方法在细节捕捉和3DGS方法在全局几何一致性上的不足。

Method: 结合SDF的粗几何捕捉和3DGS的细节渲染，相互优化。

Result: 在DTU和MobileBrick数据集上，表面重建和新视角合成效果优于现有方法。

Conclusion: 混合方法有效结合了两者的优势，提升了性能。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于圆柱坐标系的CylinderPlane表示方法，解决了Tri-plane表示中的多视角一致性问题，实现了高质量、无伪影的360°图像合成。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示在3D感知图像生成中存在多视角特征模糊问题，限制了360°视图图像的生成能力。

Method: 提出CylinderPlane，基于圆柱坐标系，通过嵌套圆柱表示捕捉多尺度特征，提升细节学习和分辨率适应性。

Result: 实验表明，该方法在合成数据集和真实图像上均优于现有方法。

Conclusion: CylinderPlane是一种通用且高效的表示方法，适用于多种神经渲染流程。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了深度神经网络（DNN）在视频分析中效率优化的技术，填补了现有研究主要关注准确性优化的空白。


<details>
  <summary>Details</summary>
Motivation: 视频数据的爆炸性增长对视频分析的准确性和效率提出了更高要求，而现有研究多关注准确性优化，效率优化仍是一个开放挑战。

Method: 采用自下而上的方式组织现有方法，涵盖硬件支持、数据处理、操作部署等多视角。

Result: 提出了一个优化框架，并基于现有工作分析了DNN在视频分析中性能优化的问题与挑战。

Conclusion: 本文为DNN在视频分析中的效率优化提供了全面综述，并指出了未来研究的挑战与方向。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 该论文研究了在光学音乐识别（OMR）中应用主动学习（AL）和顺序学习（SL）的方法，以解决标注数据稀缺和历史手稿复杂性的问题。实验表明，通过选择不确定性高的样本进行迭代标注和训练，可以在减少标注量的情况下达到接近全监督训练的精度。


<details>
  <summary>Details</summary>
Motivation: 解决光学音乐识别中标注数据稀缺和历史手稿复杂性的问题，探索在数据稀缺场景下更高效的学习方法。

Method: 使用YOLOv8模型，结合主动学习和顺序学习，选择预测置信度最低的样本进行迭代标注和训练。

Result: 实验结果表明，该方法在减少标注量的情况下，能够达到与全监督训练相当的精度。但在特定手稿中，基于不确定性的主动学习效果不佳。

Conclusion: 研究提出了一种在数据稀缺场景下高效学习的方法，但指出基于不确定性的主动学习在某些情况下可能不适用，需要探索更实用的方法。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究探讨了彩票假设（LTH）在深度伪造检测中的应用，发现关键子网络在高度稀疏情况下仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对信息完整性和社会信任构成挑战，现有检测方法模型庞大且机制不明确，难以在资源有限环境中部署。

Method: 通过彩票假设和迭代幅度剪枝方法，对MesoNet、CNN-5和ResNet-18架构进行实验，分析关键特征和子网络性能。

Result: MesoNet在80%稀疏度下仍保持56.2%准确率（基线为62.6%），且LTH方法优于一次性剪枝。

Conclusion: LTH方法可高效剪枝网络，保持检测性能，并具有跨数据集的可迁移性，适用于可部署的深度伪造检测系统。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: 提出了一种名为EVA的训练无关方法，通过动态选择中间层提取视觉事实信息，显著减少多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉识别和语言理解方面取得进展，但仍存在对象幻觉问题，即模型生成看似合理但实际错误的输出。研究发现先验知识在深层抑制视觉信息，但中间层的抑制机制尚不明确。

Method: 提出EVA方法，通过对比原始输入和纯文本输入的中间层输出分布，动态选择最具视觉事实信息的层，并将其融入最终层以修正输出。

Result: 在广泛使用的基准测试中，EVA显著降低了幻觉率，优于基线方法。

Conclusion: EVA是一种模型无关、无需训练的方法，能有效减少MLLMs中的幻觉问题，适用于多种解码策略和模型。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个新的多语言手写文档视觉问答基准，旨在解决现有模型在手写文档理解上的不足，包含1600页手写文档和2400个问答对。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言视觉问答模型在处理多样化手写文档时表现不佳，缺乏真实的多语言手写文档理解基准。

Method: HW-MLVQA提供了一个包含文本、图像及图文结合模态的评估框架，并测试了专有和开源OCR模型。

Result: 基准包含1600页手写文档和2400个问答对，支持多模态评估。

Conclusion: HW-MLVQA有望推动多语言手写文档理解领域的创新和研究。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 提出一种基于视觉语言模型知识蒸馏的方法，优化CLIP在图像质量评估任务中的性能，降低模型复杂度并提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在图像质量评估任务中参数过多和局部失真特征识别能力不足的问题。

Method: 设计质量分级提示模板，微调CLIP，提出模态自适应知识蒸馏策略。

Result: 在多个数据集上验证，显著降低复杂度且性能优于现有方法。

Conclusion: 该方法具有实际部署潜力，为图像质量评估任务提供了高效解决方案。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯抛光的视觉重定位方法（Hi²-GSLoc），通过双层次框架解决了现有方法在精度和计算复杂度上的权衡问题，适用于大规模遥感场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重定位方法在精度和计算复杂度之间存在固有权衡，尤其在遥感场景中因大规模、高海拔变化和领域差距而表现不佳。

Method: 采用3D高斯抛光作为场景表示，提出双层次框架（稀疏到密集、粗到细），结合分区高斯训练、GPU加速并行匹配和动态内存管理。

Result: 在仿真数据、公开数据集和实际飞行实验中，方法表现出竞争力的定位精度、召回率和计算效率，并能有效过滤不可靠位姿估计。

Conclusion: Hi²-GSLoc方法为实际遥感应用提供了一种高效且可靠的视觉重定位解决方案。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 提出了一种基于INR的无损点云几何压缩方法LINR-PCGC，通过分组编码框架和轻量级网络设计，显著提升了编码速度和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI点云压缩方法依赖特定训练数据分布，限制了实际应用；INR方法虽解决了分布问题，但仅支持有损压缩。

Method: 设计了分组编码框架和基于多尺度SparseConv的轻量级网络，包括尺度上下文提取、子节点预测和模型压缩模块。

Result: 在MVUB数据集中，比特流比G-PCC TMC13v23减少21.21%，比SparsePCGC减少21.95%，编码时间减少60%。

Conclusion: LINR-PCGC首次实现了基于INR的无损点云压缩，显著提升了压缩效率和速度，适用于实际应用。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS提出了一种基于小波变换的频率正则化方法，通过监督低频子带和稀疏高频子带，改善了稀疏视图3D高斯泼溅的重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅在重建高质量新视图时容易过拟合高频细节，现有频率正则化方法依赖傅里叶变换，存在参数调优困难和偏向高频学习的缺陷。

Method: DWTGS利用小波空间损失提供额外的空间监督，仅监督低频LL子带，并以自监督方式稀疏高频HH子带。

Result: 实验表明，DWTGS在多个基准测试中优于基于傅里叶变换的方法，低频策略提升了泛化能力并减少了高频幻觉。

Conclusion: DWTGS通过小波变换的频率正则化方法，有效解决了稀疏视图3D高斯泼溅的高频过拟合问题。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种高效的人脸图像质量评估方法，通过教师-学生模型蒸馏实现低计算开销的高性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有FIQA方法计算复杂度高、难以实际部署的问题。

Method: 采用两阶段方法：训练强大的教师模型，并通过自训练策略增强其能力；然后蒸馏出轻量级学生模型。

Result: 学生模型性能接近教师模型，计算开销极低，并在ICCV 2025 VQualA FIQA挑战赛中夺冠。

Conclusion: 该方法在保持高性能的同时显著降低了计算复杂度，适合实际应用部署。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 论文聚焦于空间控制图像生成模型的研究，通过实验比较不同生成范式，提出了一种简单高效的基线方法，并探索了采样时间增强技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究因快速模型改进而缺乏详细科学比较的问题，澄清文献中的知识空白，为开发基于Transformer的空间控制生成系统提供清晰指导。

Method: 在ImageNet上对扩散/流模型和自回归模型进行控制实验，提出控制令牌预填充基线方法，并研究采样时间增强技术（如分类器自由引导和softmax截断）。

Result: 控制令牌预填充方法表现优异；采样时间增强技术显著提升控制与生成一致性；适配器方法在有限数据下保持生成质量但一致性较差。

Conclusion: 研究为空间控制生成提供了实用见解，强调了采样时间增强的重要性，并澄清了适配器方法的适用场景。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen是一个两阶段框架，通过压缩令牌解决长视频生成中的内存和一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成短视频时表现良好，但扩展到长视频时面临内存瓶颈和长期一致性问题。

Method: 分两阶段：1) 训练To2V模型生成短视频；2) 引入T2To模型生成全局一致的令牌，并使用FIFO-Diffusion策略平滑过渡。

Result: 实验表明，该方法显著提升了长期时间和内容一致性，且计算开销可控。

Conclusion: TokensGen为长视频生成提供了可扩展的模块化解决方案，适用于影视制作和沉浸式模拟。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出一种基于Transformer的方法，通过预测空间自适应双边网格来校正多视角下的光度变化，无需场景特定重训练，提高了3D高斯溅射管线的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现代相机管线处理导致多视角间的光度不一致，破坏多视角一致性并降低新视角合成的质量。现有方法通过联合优化场景表示和每图像外观嵌入解决此问题，但增加了计算复杂性和训练时间。

Method: 提出基于Transformer的方法，预测空间自适应双边网格以校正光度变化，并将其集成到3D高斯溅射管线中。

Result: 实验表明，该方法在重建质量和收敛速度上优于或匹配现有场景特定优化方法。

Conclusion: 该方法在多视角一致性和训练效率上表现优异，具有跨场景泛化能力。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为HDF的新框架，通过两个模块增强动态面部表情识别的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多源数据和个体表达差异导致的样本异质性对性能的影响。

Method: 设计了Time-Frequency Distributional Attention Module（DAM）和Distribution-aware Scaling Module（DSM），分别用于时间-频率建模和优化不平衡。

Result: 在DFEW和FERV39k数据集上显著提高了识别准确性和鲁棒性。

Conclusion: HDF框架在多样化和不平衡场景中表现出色，具有强泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出两种基于树结构的语义损失函数，利用标签的层次结构改进医学图像分割任务，在稀疏标注和全监督场景下均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法对所有错误同等惩罚，未能利用标签空间的语义层次结构，尤其是当标签类别丰富且差异细微时效果不佳。

Method: 提出两种树状语义损失函数，结合稀疏标注训练方法，适用于层次化标签空间。

Result: 在全监督的脑部MRI分割和稀疏标注的神经外科高光谱图像分割任务中均取得最优性能。

Conclusion: 树状语义损失函数能有效利用标签层次结构，提升医学图像分割的准确性和丰富性。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 论文提出了一种动态调整低秩适应（LoRA）秩的方法，用于医学图像分割，通过引入L1稀疏正则化器自动优化秩选择，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法在医学图像任务中需要固定秩，难以适应不同任务的复杂性，因此需要一种动态调整秩的方法。

Method: 通过将低秩权重矩阵视为奇异值分解，引入L1稀疏正则化器，并使用近端优化器自动优化秩选择。

Result: 在少量样本微调实验中，该方法显著优于标准LoRA和其他PEFT方法，表现出高效性和鲁棒性。

Conclusion: 动态调整秩的方法在医学图像分割中具有显著优势，代码已开源。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 论文研究了低参数深度神经网络在计算机视觉中的性能，重点关注瓶颈架构及其使用超线性激活函数的行为，提出减少干扰可提升小规模网络的准确性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究低参数深度神经网络中特征图的干扰现象，以提升小规模网络的性能和扩展性。

Method: 通过分析不同瓶颈架构，识别减少干扰的关键设计元素，并提出名为NoDepth Bottleneck的概念验证架构。

Result: 实验表明，减少干扰可显著提升低参数网络（少于150万参数）在ImageNet数据集上的准确性和扩展性。

Conclusion: 研究为低参数范围的神经网络提供了更高效和可扩展的设计思路，并深化了对计算机视觉中瓶颈架构的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型SEEM生成未标注数据的预测掩码，并通过不确定性校准和自依赖训练策略提升半监督语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务中标注数据稀缺的问题，探索基础分割模型作为标注工具的潜力。

Method: 提出ConformalSAM框架，结合不确定性校准和自依赖训练，利用SEEM生成的掩码进行半监督学习。

Result: 在三个标准基准测试中，ConformalSAM性能优于现有方法，并可作为插件提升其他方法的性能。

Conclusion: ConformalSAM通过可靠利用基础模型和动态调整训练策略，有效解决了半监督语义分割中的标注稀缺问题。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 当前的多模态大语言模型（MLLMs）在多模态上下文学习（MICL）中过度依赖文本信息，忽视视觉线索。本文提出动态注意力重分配（DARA）和专用数据集TrueMICL，以提升模型对视觉信息的利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多模态上下文学习中未能有效利用视觉信息，导致性能受限。本文旨在解决这一问题，提升模型的真正多模态适应能力。

Method: 提出动态注意力重分配（DARA）策略，通过调整视觉和文本令牌的注意力权重，增强模型对视觉信息的关注。同时，构建TrueMICL数据集，明确要求模型整合多模态信息完成任务。

Result: 实验表明，DARA和TrueMICL显著提升了模型的多模态上下文学习能力，特别是在需要视觉理解的场景中。

Conclusion: 本文通过DARA和TrueMICL解决了MLLMs在多模态上下文学习中的视觉信息利用不足问题，为未来的多模态学习研究提供了新方向。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中表现出色，优于变分自编码器和生成对抗网络，并改进了扩散后验采样方法。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在多元地下建模和概率反演中的应用，以提升建模能力和计算效率。

Method: 提出对扩散后验采样方法的修正，包括噪声污染的似然近似，并测试其在多元地质场景中的性能。

Result: 相比原始方法，新方法显著提高了统计鲁棒性、后验概率密度采样的准确性，并降低了计算成本。

Conclusion: 该方法适用于硬数据和间接数据，且反演速度快于需要外循环的其他方法。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个评估文本到视频生成模型物理常识能力的基准，包含383个精心设计的提示，通过三阶段评估流程间接测试模型的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在物理常识方面表现不足，导致输出违反直觉的因果关系和对象行为，需要一种评估方法来填补这一空白。

Method: 提出PhysVidBench基准，通过生成视频、视觉语言模型标注和语言模型回答问题三阶段流程间接评估物理合理性。

Result: PhysVidBench提供了一个结构化、可解释的框架，用于评估生成视频模型的物理常识能力，重点关注工具使用和材料属性。

Conclusion: 该基准填补了当前评估方法的不足，为提升文本到视频生成模型的物理合理性提供了重要工具。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种概念驱动的视频对象分割框架（SeC），通过结合视觉语言模型（LVLMs）构建高层次对象表示，显著提升了在复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频对象分割方法依赖外观匹配，缺乏人类对对象的概念理解，导致在视觉变化、遮挡和复杂场景中表现不佳。

Method: SeC框架利用LVLMs整合多帧视觉线索，构建对象的概念先验，并在推理时动态平衡语义推理与特征匹配。

Result: 在提出的SeCVOS基准测试中，SeC比SAM 2.1提升了11.8个百分点，达到了新的最优性能。

Conclusion: SeC通过概念驱动的分割方法，显著提升了复杂场景下的视频对象分割能力，为未来研究提供了新方向。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Latent Denoising Tokenizer (l-DeTok)的视觉分词器，通过直接与下游去噪目标对齐，提升生成模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型的训练目标（如从高斯噪声或掩码中重建干净信号）与分词器的设计目标存在潜在联系，作者希望通过对齐这两者来提升分词器的有效性。

Method: 提出了l-DeTok，通过在潜在嵌入中引入插值噪声和随机掩码，训练分词器重建干净图像。

Result: 在ImageNet 256x256上，l-DeTok在六种代表性生成模型中均优于标准分词器。

Conclusion: 去噪应作为分词器设计的基本原则，为未来分词器设计提供新视角。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [142] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: APTx Neuron是一种新型神经计算单元，将非线性和线性变换整合为单一可训练表达式，提升计算效率和架构简洁性。


<details>
  <summary>Details</summary>
Motivation: 传统神经元需要分开处理激活和线性变换，APTx Neuron旨在统一这两者，简化架构并提高效率。

Method: APTx Neuron基于APTx激活函数，形式为$y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$，所有参数可训练。

Result: 在MNIST数据集上验证，仅用20个周期和约332K参数即达到96.69%测试准确率。

Conclusion: APTx Neuron表现出卓越的表达能力和计算效率，为神经元设计提供了新范式。

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [143] [Training oscillator Ising machines to assign the dynamic stability of their equilibrium points](https://arxiv.org/abs/2507.14386)
*Yi Cheng,Zongli Lin*

Main category: cs.NE

TL;DR: 提出了一种神经网络模型，通过调整平衡点的稳定性实现类似Hopfield的联想记忆，并基于Hamiltonian能量设计了权重训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield模型需同时考虑平衡点的存在和动态稳定性，而OIM模型因其结构稳定性简化了权重设计，仅需关注动态稳定性。

Method: 提出了Hamiltonian正则化特征值对比方法（HRECM），用于训练OIM的耦合权重，以分配适当的稳定性。

Result: 数值实验验证了HRECM方法的有效性。

Conclusion: OIM模型通过HRECM方法简化了权重设计，实现了高效的联想记忆。

Abstract: We propose a neural network model, which, with appropriate assignment of the
stability of its equilibrium points (EPs), achieves Hopfield-like associative
memory. The oscillator Ising machine (OIM) is an ideal candidates for such a
model, as all its $0/\pi$ binary EPs are structurally stable with their dynamic
stability tunable by the coupling weights. Traditional Hopfield-based models
store the desired patterns by designing the coupling weights between neurons.
The design of coupling weights should simultaneously take into account both the
existence and the dynamic stability of the EPs for the storage of the desired
patterns. For OIMs, since all $0/\pi$ binary EPs are structurally stable, the
design of the coupling weights needs only to focus on assigning appropriate
stability for the $0/\pi$ binary EPs according to the desired patterns. In this
paper, we establish a connection between the stability and the Hamiltonian
energy of EPs for OIMs, and, based on this connection, provide a
Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to train the
coupling weights of OIMs for assigning appropriate stability to their EPs.
Finally, numerical experiments are performed to validate the effectiveness of
the proposed method.

</details>


### [144] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: 本文研究了脉冲神经网络（SNNs）中神经元模型参数的优化空间，发现了一个操作空间（由膜时间常数和电压阈值定义），在该区域内网络表现最佳。


<details>
  <summary>Details</summary>
Motivation: SNNs因其高能效和生物合理性成为传统神经网络的替代方案，但其性能依赖于神经元参数的精细调节。

Method: 通过系统探索不同数据集和架构，识别并量化了神经元超参数的操作空间，评估了对抗噪声的鲁棒性。

Result: 在操作空间内，SNNs实现了分类准确性和脉冲活动的最佳平衡；超出该区域会导致性能下降或能量浪费。

Conclusion: 研究强调了超参数调优的重要性，为部署高效稳健的SNNs提供了实用指南。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


### [145] [DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving](https://arxiv.org/abs/2507.15615)
*Zhihao Zhang,Siyuan Li,Chenxi Li,Feifan Liu,Mengjing Chen,Kai Li,Tao Zhong,Bo An,Peng Liu*

Main category: cs.NE

TL;DR: 论文提出了一种数据-算法协同进化框架（DHEvo），通过迭代选择代表性实例并进化启发式方法，解决了当前基于LLM的方法在混合整数规划（MILP）中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的MILP启发式方法在问题类内泛化能力有限，无法捕捉实例特征，导致性能不佳。

Method: 提出DHEvo框架，利用LLM多智能体系统迭代生成和优化数据-代码对，基于适应度评分选择最优启发式。

Result: 实验表明，DHEvo显著优于人工设计的启发式和现有LLM方法。

Conclusion: DHEvo通过协同进化提升了MILP启发式的泛化能力和性能。

Abstract: Primal heuristics play a critical role in improving the efficiency of mixed
integer programming (MILP) solvers. As large language models (LLMs) have
demonstrated superior code generation abilities, recent MILP works are devoted
to leveraging the evolutionary computation approaches with LLMs to generate
effective primal heuristics. Although the generated heuristics have achieved
better solving performance than the hand-crafted ones with little adaptability,
the advantage of current LLM-based methods is limited to few MILP instances in
one problem class, as they fail to capture the instance characteristics in the
problem class (the MILP instances generated from the same mathematical model
are defined as a problem class). Since MILP instances often differ
significantly in structure and feature distribution, the neglect of their
characteristics in the evolution process results in poor generalization within
the same problem class. To overcome this challenge, we propose a data-algorithm
co-evolution framework (DHEvo) that iteratively selects representative
instances and evolves corresponding heuristics. With the initial instance
distribution, we develop an LLM-based multi-agent system to generate data-code
pairs simultaneously. These data-code pairs are iteratively refined based on
their fitness scores, leading to the identification of the most effective
heuristic over the entire problem class. Extensive experiments across diverse
MILP benchmarks demonstrate that our approach significantly outperforms both
human-designed heuristics and existing LLM-based methods.

</details>


### [146] [TONUS: Neuromorphic human pose estimation for artistic sound co-creation](https://arxiv.org/abs/2507.15734)
*Jules Lecomte,Konrad Zinner,Michael Neumeier,Axel von Arnim*

Main category: cs.NE

TL;DR: 论文提出了一种基于神经形态身体传感的艺术声音装置，旨在通过无缝互动支持人与机器的诗意对话。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互过于技术化和机器驱动，艺术领域对新技术的探索不足，未能充分发挥其潜力。

Method: 设计了一种神经形态多头人体姿态估计神经传感器，利用尖峰神经网络提取特征，生成声音景观和视觉输出。

Result: 通过精细的身体动作控制，访客可以与机器共同创造声音景观，体验神经化的对话。

Conclusion: 该装置展示了神经形态技术在艺术中的应用潜力，为人机交互提供了更自然和诗意的界面。

Abstract: Human machine interaction is a huge source of inspiration in today's media
art and digital design, as machines and humans merge together more and more.
Its place in art reflects its growing applications in industry, such as
robotics. However, those interactions often remains too technical and
machine-driven for people to really engage into. On the artistic side, new
technologies are often not explored in their full potential and lag a bit
behind, so that state-of-the-art research does not make its way up to museums
and exhibitions. Machines should support people's imagination and poetry in a
seamless interface to their body or soul. We propose an artistic sound
installation featuring neuromorphic body sensing to support a direct yet non
intrusive interaction with the visitor with the purpose of creating sound
scapes together with the machine. We design a neuromorphic multihead human pose
estimation neural sensor that shapes sound scapes and visual output with fine
body movement control. In particular, the feature extractor is a spiking neural
network tailored for a dedicated neuromorphic chip. The visitor, immersed in a
sound atmosphere and a neurally processed representation of themselves that
they control, experience the dialogue with a machine that thinks neurally,
similarly to them.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [147] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: 论文提出了一种基于无线电地图和多源混合注意力强化学习（MSHA-RL）的UAM路径规划方法，以解决通信质量和动态乘客需求问题。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通（UAM）系统需要解决通信质量保障和动态乘客需求响应的双重挑战，传统路径规划方法缺乏灵活性。

Method: 构建无线电地图评估通信质量，提出MSHA-RL框架，通过数据对齐和混合注意力机制实现实时路径规划。

Result: 实验表明该方法能有效规划符合通信要求的轨迹，减少旅行时间并提升运营效率。

Conclusion: MSHA-RL框架为UAM系统提供了兼顾通信质量和动态需求的实时路径规划解决方案。

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [148] [A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators](https://arxiv.org/abs/2507.14274)
*Andreas Mueller,Shivesh Kumar,Thomas Kordik*

Main category: cs.RO

TL;DR: 本文首次提出了一种计算高效的并行运动学机械臂（PKM）配备串联弹性执行器（SEA）的逆动力学解二阶时间导数的算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究未涉及PKM配备SEA的轨迹控制，关键在于高效计算逆动力学解的二阶时间导数。

Method: 利用PKM的特殊拓扑结构，复用串联机器人的递归算法，并采用李群框架推导所有关系。

Result: 数值结果展示了6自由度Gough-Stewart平台和平面PKM在平坦性控制方案下的表现。

Conclusion: 本文填补了PKM配备SEA轨迹控制的空白，为相关应用提供了理论基础和实用方法。

Abstract: Series elastic actuators (SEA) were introduced for serial robotic arms. Their
model-based trajectory tracking control requires the second time derivatives of
the inverse dynamics solution, for which algorithms were proposed. Trajectory
control of parallel kinematics manipulators (PKM) equipped with SEAs has not
yet been pursued. Key element for this is the computationally efficient
evaluation of the second time derivative of the inverse dynamics solution. This
has not been presented in the literature, and is addressed in the present paper
for the first time. The special topology of PKM is exploited reusing the
recursive algorithms for evaluating the inverse dynamics of serial robots. A
Lie group formulation is used and all relations are derived within this
framework. Numerical results are presented for a 6-DOF Gough-Stewart platform
(as part of an exoskeleton), and for a planar PKM when a flatness-based control
scheme is applied.

</details>


### [149] [Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support](https://arxiv.org/abs/2507.14412)
*Mengxue Fu,Zhonghao Shi,Minyu Huang,Siqi Liu,Mina Kian,Yirui Song,Maja J. Matarić*

Main category: cs.RO

TL;DR: 论文提出使用端到端语音语言模型（SLM）改进社交辅助机器人（SAR）的对话系统，并通过用户研究验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有SAR对话系统在实时延迟、反馈和个性化对话方面存在不足，需改进。

Method: 集成SLM到SAR中，并通过小规模用户研究（N=11）评估系统效果。

Result: 用户认为SLM-SAR系统能提供共情反馈和自然对话，但非语言行为和反馈仍需改进。

Conclusion: 需优化机器人动作同步、反馈个性化和语音生成，以提升SAR系统效果。

Abstract: Socially assistive robots (SARs) have shown great potential for supplementing
well-being support. However, prior studies have found that existing dialogue
pipelines for SARs remain limited in real-time latency, back-channeling, and
personalized speech dialogue. Toward addressing these limitations, we propose
using integrated end-to-end speech-language models (SLMs) with SARs. This work
1) evaluated the usability of an SLM-enabled SAR dialogue system through a
small user study, and 2) identified remaining limitations through study user
feedback to inform future improvements. We conducted a small within-participant
user study with university students (N = 11) whose results showed that
participants perceived an SLM-enabled SAR system as capable of providing
empathetic feedback, natural turn-taking, back-channeling, and adaptive
responses. We also found that participants reported the robot's nonverbal
behaviors as lacking variability and synchronization with conversation, and the
SLM's verbal feedback as generic and repetitive. These findings highlighted the
need for real-time robot movement synchronized with conversation, improved
prompting or fine-tuning to generate outputs better aligned with mental health
practices, and more expressive, adaptive vocal generation.

</details>


### [150] [Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking](https://arxiv.org/abs/2507.14455)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 时间延迟嵌入技术用于构建非线性光滑系统的线性状态空间模型，本文将其扩展到周期性非光滑或混合系统，并提出一种新型状态历史增强线性二次调节器（LQR）。


<details>
  <summary>Details</summary>
Motivation: 探索时间延迟嵌入技术是否适用于周期性非光滑或混合系统，并开发相应的控制方法。

Method: 扩展时间延迟嵌入技术，应用于两个周期性混合系统（弹跳摆和简化步行器），并设计状态历史增强LQR。

Result: 成功构建了周期性混合系统的线性模型，并验证了状态历史增强LQR的有效性。

Conclusion: 时间延迟嵌入技术可推广到周期性非光滑系统，状态历史增强LQR为控制此类系统提供了新方法。

Abstract: Time-delay embedding is a technique that uses snapshots of state history over
time to build a linear state space model of a nonlinear smooth system. We
demonstrate that periodic non-smooth or hybrid system can also be modeled as a
linear state space system using this approach as long as its behavior is
consistent in modes and timings. We extended time-delay embeddings to generate
a linear model of two periodic hybrid systems: the bouncing pendulum and the
simplest walker with control inputs. This leads to a novel state history
augmented linear quadratic regulator (LQR) which uses current and past state
history for feedback control.

</details>


### [151] [A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0](https://arxiv.org/abs/2507.14538)
*Jin Chai,Xiang Yao,Mengfan Hou,Yanghong Li,Erbao Dong*

Main category: cs.RO

TL;DR: CYJ Hand-0是一种21自由度仿人灵巧手，采用混合肌腱驱动系统（SMAs和DC电机），通过3D打印金属框架和人工肌腱实现仿生设计。


<details>
  <summary>Details</summary>
Motivation: 设计一种仿人灵巧手，结合SMAs和DC电机的优势，实现高仿生性和灵活性。

Method: 使用3D打印金属框架和人工肌腱，结合线性电机和SMA模块控制手指运动。

Result: 机械和运动学实验验证了设计的有效性，展示了仿生灵巧性。

Conclusion: CYJ Hand-0的设计成功实现了仿生灵巧手的性能目标。

Abstract: CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid
tendon-driven actuation system that combines shape memory alloys (SMAs) and DC
motors. The hand employs high-strength fishing line as artificial tendons and
uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal
and tendon-muscle structure of the human hand. A linear motor-driven module
controls finger flexion, while an SMA-based module enables finger extension and
lateral abduction. These modules are integrated into a compact hybrid actuation
unit mounted on a custom rear support structure. Mechanical and kinematic
experiments, conducted under an Arduino Mega 2560-based control system,
validate the effectiveness of the design and demonstrate its biomimetic
dexterity.

</details>


### [152] [BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives](https://arxiv.org/abs/2507.14582)
*Zezhi Liu,Shizhen Wu,Hanqian Luo,Deyun Qin,Yongchun Fang*

Main category: cs.RO

TL;DR: 论文提出了一种名为BT-TL-DMPs的分层框架，结合行为树、时序逻辑和动态运动基元，以解决机器人从演示中学习技能并适应新场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 在从演示学习（LfD）领域，机器人难以将学到的技能推广到具有不同任务和运动需求的新环境中，尤其是在复杂的长期任务中。

Method: 框架整合了行为树（BT）、时序逻辑（TL）和动态运动基元（DMPs），使用信号时序逻辑（STL）规范任务需求，并将其转化为模块化行为树，同时优化DMP以适应时空约束。

Result: 通过仿真和真实实验验证，框架能够有效桥接符号与运动之间的差距，实现更可靠和通用的自主操作。

Conclusion: BT-TL-DMPs框架在复杂机器人任务中表现出色，为长期任务的技能泛化提供了有效解决方案。

Abstract: In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.

</details>


### [153] [Search-Based Autonomous Vehicle Motion Planning Using Game Theory](https://arxiv.org/abs/2507.15088)
*Pouya Panahandeh,Mohammad Pirani,Baris Fidan,Amir Khajepour*

Main category: cs.RO

TL;DR: 提出了一种基于搜索的交互式运动规划方案，用于自动驾驶车辆，采用博弈论方法，考虑其他道路使用者为智能体而非静态障碍物。


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法将其他道路使用者视为静态障碍物，缺乏现实性，新方法旨在生成更真实的路径。

Method: 使用博弈论方法，将其他道路用户建模为智能体，实现实时计算。

Result: 方案计算时间短，适用于实时应用，实验验证性能优于现有技术。

Conclusion: 提出的运动规划方案在现实性和实时性方面表现优异，适用于自动驾驶车辆。

Abstract: In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.

</details>


### [154] [Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition](https://arxiv.org/abs/2507.14605)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: 利用Koopman算子理论和EDMD方法，构建高维空间中的线性模型，保留非线性动态，实现四足机器人的在线多步态控制。


<details>
  <summary>Details</summary>
Motivation: 解决LMPC因线性化运动方程导致的解质量差问题，提升四足机器人在复杂地形中的运动规划能力。

Method: 结合Koopman算子理论和EDMD，构建高维线性模型；分阶段建模空中和地面接触动态；应用LMPC实现多种步态及其转换。

Result: 成功演示了在平坦和崎岖地形上的跳跃、小跑及步态转换。

Conclusion: Koopman算子理论为四足机器人提供了高效的在线多步态控制方法，扩展了其运动规划的灵活性。

Abstract: Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.

</details>


### [155] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI 提出了一种基于可逆网络的方法，用于3D人体运动预测中的不确定性量化，适用于安全关键场景。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景（如人机协作）中，准确量化预测的不确定性至关重要，但现有方法难以实现。

Method: 使用可逆网络将姿态参数化解耦到潜在空间，并显式预测未来潜在分布以实现不确定性量化。

Result: 在基准测试中，ProbHMI 在确定性和多样性预测上表现优异，且验证了不确定性校准的有效性。

Conclusion: ProbHMI 为风险感知决策提供了可靠的不确定性量化方法。

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [156] [Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.14700)
*Nicholas Mohammad,Nicola Bezzo*

Main category: cs.RO

TL;DR: 提出了一种结合CLF和CBF的MPCC框架，通过动态调整CBF参数确保安全导航，并在未知杂乱环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MPCC方法缺乏形式化的安全保障，难以在未知杂乱环境中实现安全导航。

Method: 结合CLF和CBF的MPCC框架，动态调整CBF参数以增强可行性，使用SAC策略优化。

Result: 在仿真和移动机器人实验中验证了方法的有效性。

Conclusion: 该方法为未知杂乱环境中的安全导航提供了可靠解决方案。

Abstract: Safe navigation in unknown and cluttered environments remains a challenging
problem in robotics. Model Predictive Contour Control (MPCC) has shown promise
for performant obstacle avoidance by enabling precise and agile trajectory
tracking, however, existing methods lack formal safety assurances. To address
this issue, we propose a general Control Lyapunov Function (CLF) and Control
Barrier Function (CBF) enabled MPCC framework that enforces safety constraints
derived from a free-space corridor around the planned trajectory. To enhance
feasibility, we dynamically adapt the CBF parameters at runtime using a Soft
Actor-Critic (SAC) policy. The approach is validated with extensive simulations
and an experiment on mobile robot navigation in unknown cluttered environments.

</details>


### [157] [Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls](https://arxiv.org/abs/2507.14721)
*Keita Kobashi,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出了一种分层强化学习框架，结合Q学习和CVAE，解决机器人因环境遮挡导致抓取失败的问题，并通过实验验证了方法的通用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决因环境遮挡导致机器人抓取失败的问题，尤其是当可用环境特征（如墙）不满足假设条件时。

Method: 采用分层强化学习框架，高层策略通过Q学习选择动作类型，低层技能结合CVAE生成连续空间动作，并通过域随机化提升泛化能力。

Result: 在仿真中训练并部署到真实世界，实验显示方法具有通用性和鲁棒的仿真到现实迁移性能，成功率较高。

Conclusion: 提出的方法有效解决了复杂环境下的抓取问题，展示了分层强化学习和CVAE在机器人任务中的潜力。

Abstract: This study addresses the problem of occluded grasping, where primary grasp
configurations of an object are not available due to occlusion with
environment. Simple parallel grippers often struggle with such tasks due to
limited dexterity and actuation constraints. Prior works have explored object
pose reorientation such as pivoting by utilizing extrinsic contacts between an
object and an environment feature like a wall, to make the object graspable.
However, such works often assume the presence of a short wall, and this
assumption may not always hold in real-world scenarios. If the wall available
for interaction is too large or too tall, the robot may still fail to grasp the
object even after pivoting, and the robot must combine different types of
actions to grasp. To address this, we propose a hierarchical reinforcement
learning (RL) framework. We use Q-learning to train a high-level policy that
selects the type of action expected to yield the highest reward. The selected
low-level skill then samples a specific robot action in continuous space. To
guide the robot to an appropriate location for executing the selected action,
we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on
the object point cloud and the skill ID, enabling it to infer a suitable
location based on the object geometry and the selected skill. To promote
generalization, we apply domain randomization during the training of low-level
skills. The RL policy is trained entirely in simulation with a box-like object
and deployed to six objects in real world. We conduct experiments to evaluate
our method and demonstrate both its generalizability and robust sim-to-real
transfer performance with promising success rates.

</details>


### [158] [X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots](https://arxiv.org/abs/2507.14731)
*Haitong Wang,Aaron Hao Tan,Angus Fung,Goldie Nejat*

Main category: cs.RO

TL;DR: X-Nav是一个跨机器人形态的端到端导航框架，通过两阶段学习（专家策略训练与策略蒸馏）实现通用性，支持零样本迁移到新形态和环境中。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法局限于特定机器人形态，缺乏通用性。X-Nav旨在解决这一问题，实现跨形态导航。

Method: 1) 使用深度强化学习训练多个专家策略；2) 通过Nav-ACT蒸馏出通用策略，直接映射观测到控制命令。

Result: X-Nav在仿真中实现零样本迁移，性能随训练形态数量提升，并通过真实实验验证了通用性。

Conclusion: X-Nav展示了跨形态导航的潜力，为通用机器人导航提供了新思路。

Abstract: Existing navigation methods are primarily designed for specific robot
embodiments, limiting their generalizability across diverse robot platforms. In
this paper, we introduce X-Nav, a novel framework for end-to-end
cross-embodiment navigation where a single unified policy can be deployed
across various embodiments for both wheeled and quadrupedal robots. X-Nav
consists of two learning stages: 1) multiple expert policies are trained using
deep reinforcement learning with privileged observations on a wide range of
randomly generated robot embodiments; and 2) a single general policy is
distilled from the expert policies via navigation action chunking with
transformer (Nav-ACT). The general policy directly maps visual and
proprioceptive observations to low-level control commands, enabling
generalization to novel robot embodiments. Simulated experiments demonstrated
that X-Nav achieved zero-shot transfer to both unseen embodiments and
photorealistic environments. A scalability study showed that the performance of
X-Nav improves when trained with an increasing number of randomly generated
embodiments. An ablation study confirmed the design choices of X-Nav.
Furthermore, real-world experiments were conducted to validate the
generalizability of X-Nav in real-world environments.

</details>


### [159] [KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning](https://arxiv.org/abs/2507.14820)
*Bingran Chen,Baorun Li,Jian Yang,Yong Liu,Guangyao Zhai*

Main category: cs.RO

TL;DR: KGN-Pro是一种新型抓取网络，通过概率PnP层直接优化3D信息，解决了现有方法在2D监督和不可微问题上的局限性，显著提升了抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在6-DoF抓取估计中存在小物体和传感器噪声问题，或依赖昂贵的3D标注和离散化问题。KGN-Pro旨在保留2D效率的同时，充分利用3D信息。

Method: KGN-Pro通过RGB-D图像生成关键点图和2D置信图，利用概率PnP层进行3D优化，实现端到端学习。

Result: 实验表明，KGN-Pro在模拟和真实环境中均优于现有方法，抓取覆盖率和成功率更高。

Conclusion: KGN-Pro通过直接3D优化和端到端学习，显著提升了6-DoF抓取性能。

Abstract: High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation
to serve as a basic function. Previous approaches either directly generate
grasps from point-cloud data, suffering from challenges with small objects and
sensor noise, or infer 3D information from RGB images, which introduces
expensive annotation requirements and discretization issues. Recent methods
mitigate some challenges by retaining a 2D representation to estimate grasp
keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF
poses. However, these methods are limited by their non-differentiable nature
and reliance solely on 2D supervision, which hinders the full exploitation of
rich 3D information. In this work, we present KGN-Pro, a novel grasping network
that preserves the efficiency and fine-grained object grasping of previous KGNs
while integrating direct 3D optimization through probabilistic PnP layers.
KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further
outputs a 2D confidence map to weight keypoint contributions during
re-projection error minimization. By modeling the weighted sum of squared
re-projection errors probabilistically, the network effectively transmits 3D
supervision to its 2D keypoint predictions, enabling end-to-end learning.
Experiments on both simulated and real-world platforms demonstrate that KGN-Pro
outperforms existing methods in terms of grasp cover rate and success rate.

</details>


### [160] [CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning](https://arxiv.org/abs/2507.14903)
*Pan Hu*

Main category: cs.RO

TL;DR: 提出了一种名为CDGMP的框架，通过混合专家（MoE）架构和多策略强化学习，紧密集成决策和运动规划，提升自动驾驶的灵活性和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中决策（车道选择）和运动规划（控制命令生成）的灵活性与安全性挑战。

Method: 采用MoE架构和多策略强化学习，通过门控机制协调多个专用子网络，将复杂驾驶任务分解为模块化组件。

Result: 仿真结果显示CDGMP在车道选择和运动规划中表现可靠，适应性和鲁棒性增强。

Conclusion: CDGMP为自动驾驶提供了一种可扩展的解决方案，其MoE架构也为其他高维决策和控制任务提供了基础。

Abstract: Autonomous driving demands reliable and efficient solutions to closely
related problems such as decision-making and motion planning. In this work,
decision-making refers specifically to highway lane selection, while motion
planning involves generating control commands (such as speed and steering) to
reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),
achieving both flexible and safe lane selection alongside precise trajectory
execution remains a significant challenge. This paper proposes a framework
called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly
integrates decision-making and motion planning using a Mixture of Experts (MoE)
inspired architecture combined with multi-policy reinforcement learning. By
coordinating multiple specialized sub-networks through a gating mechanism, the
method decomposes the complex driving task into modular components. Each
sub-network focuses on a specific aspect of driving, improving efficiency by
activating only the most relevant modules during inference. This design also
enhances safety through modular specialization. CDGMP improves the adaptability
and robustness of CAVs across diverse traffic scenarios, offering a scalable
solution to real-world autonomy challenges. The architectural principles behind
CDGMP, especially the use of MoE, also provide a strong foundation for other
high-dimensional decision and control tasks. Simulation results (available at
https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane
selection and motion planning.

</details>


### [161] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora是一种三阶段馈通和布局感知的矩形平面规划器，通过分阶段优化馈通和组件布局，显著提升芯片性能。


<details>
  <summary>Details</summary>
Motivation: 现有平面规划方法难以与后续物理设计阶段集成，导致模块内组件布局不优和模块间馈通过多。

Method: Flora分三个阶段：粗粒度优化馈通和HPWL，固定轮廓下的细粒度优化，以及基于树搜索的组件布局和边界调整。

Result: 实验显示，Flora在HPWL、FTpin、FTmod和组件布局性能上均优于现有方法。

Conclusion: Flora通过跨阶段优化，显著提升了芯片的PPA指标。

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [162] [Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly](https://arxiv.org/abs/2507.14929)
*Tero Kaarlela,Sami Salo,Jose Outeiro*

Main category: cs.RO

TL;DR: 提出了一种用于安全拆解和分类电动汽车电池的远程操作系统，结合了人工操作和自动化，提高了安全性、适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动拆解电动汽车电池存在安全隐患，如触电和有毒化学品暴露，需要一种更安全的解决方案。

Method: 采用远程操作系统，结合RGB摄像头和ROS中间件，实现物理与数字孪生的对齐，支持人工创建和保存拆解序列。

Result: 在线试点研究表明该方法具有用户友好性，并能减少劳动力依赖、提高电池回收效率。

Conclusion: 该混合方法为电动汽车电池的可持续回收提供了一种安全、高效的解决方案。

Abstract: Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.

</details>


### [163] [Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry](https://arxiv.org/abs/2507.14931)
*Qiaoqiao Ren,Remko Proesmans,Arend Pissens,Lara Dehandschutter,William Denecker,Lotte Rouckhout,Joke Carrette,Peter Vanhopplinus,Tony Belpaeme,Francis wyffels*

Main category: cs.RO

TL;DR: 研究探讨了在法医心理健康护理中，通过共同设计开发伴侣机器人以监测和调节患者压力，同时记录互动行为。


<details>
  <summary>Details</summary>
Motivation: 法医心理健康护理环境官僚化严重，患者自主权受限，导致心理压力加剧。研究旨在通过共同设计改善患者体验。

Method: 在法医精神病诊所进行了四场共同设计工作坊，参与者包括患者、护理人员和治疗师，逐步完善机器人设计。

Result: 研究发现，让患者参与设计过程并基于其情绪状态调整方案至关重要，确保每位患者的声音被听到。

Conclusion: 共同设计伴侣机器人能有效提升患者自主权，改善心理健康护理效果。

Abstract: Forensic mental health care involves the treatment of individuals with severe
mental disorders who have committed violent offences. These settings are often
characterized by high levels of bureaucracy, risk avoidance, and restricted
autonomy. Patients frequently experience a profound loss of control over their
lives, leading to heightened psychological stress-sometimes resulting in
isolation as a safety measure. In this study, we explore how co-design can be
used to collaboratively develop a companion robot that helps monitor and
regulate stress while maintaining tracking of the patients' interaction
behaviours for long-term intervention. We conducted four co-design workshops in
a forensic psychiatric clinic with patients, caregivers, and therapists. Our
process began with the presentation of an initial speculative prototype to
therapists, enabling reflection on shared concerns, ethical risks, and
desirable features. This was followed by a creative ideation session with
patients, a third workshop focused on defining desired functions and emotional
responses, and we are planning a final prototype demo to gather direct patient
feedback. Our findings emphasize the importance of empowering patients in the
design process and adapting proposals based on their current emotional state.
The goal was to empower the patient in the design process and ensure each
patient's voice was heard.

</details>


### [164] [Heterogeneous object manipulation on nonlinear soft surface through linear controller](https://arxiv.org/abs/2507.14967)
*Pratik Ingle,Kasper Støy,Andres Faiña*

Main category: cs.RO

TL;DR: 提出了一种基于PID的线性闭环反馈控制策略，用于在低密度驱动阵列上实现异构物体的精确操控。


<details>
  <summary>Details</summary>
Motivation: 高密度驱动阵列的复杂性和控制难度限制了软体机器人在实际应用中的推广，需要一种简单、鲁棒且无需大量训练的控制方法。

Method: 采用几何变换驱动的PID控制器，将倾斜角度控制输出直接映射到驱动指令，避免复杂的黑盒训练。

Result: 通过仿真和物理实验验证，成功操控了多种几何形状、重量和质地的物体，包括易碎物品。

Conclusion: 该方法具有高度通用性，为软体机器人操控提供了实用且可靠的解决方案，适合实际应用。

Abstract: Manipulation surfaces indirectly control and reposition objects by actively
modifying their shape or properties rather than directly gripping objects.
These surfaces, equipped with dense actuator arrays, generate dynamic
deformations. However, a high-density actuator array introduces considerable
complexity due to increased degrees of freedom (DOF), complicating control
tasks. High DOF restrict the implementation and utilization of manipulation
surfaces in real-world applications as the maintenance and control of such
systems exponentially increase with array/surface size. Learning-based control
approaches may ease the control complexity, but they require extensive training
samples and struggle to generalize for heterogeneous objects. In this study, we
introduce a simple, precise and robust PID-based linear close-loop feedback
control strategy for heterogeneous object manipulation on MANTA-RAY
(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation
density). Our approach employs a geometric transformation-driven PID
controller, directly mapping tilt angle control outputs(1D/2D) to actuator
commands to eliminate the need for extensive black-box training. We validate
the proposed method through simulations and experiments on a physical system,
successfully manipulating objects with diverse geometries, weights and
textures, including fragile objects like eggs and apples. The outcomes
demonstrate that our approach is highly generalized and offers a practical and
reliable solution for object manipulation on soft robotic manipulation,
facilitating real-world implementation without prohibitive training demands.

</details>


### [165] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: 提出了FCRF框架，通过灵活的自反思机制提升家用机器人在复杂任务中的错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的自反思机制不够灵活，限制了其在任务规划错误纠正中的效果。

Method: 提出FCRF框架，采用导师-执行者架构，结合任务难度和历史经验进行灵活自反思。

Result: 在AlfWorld仿真和现实环境中验证，FCRF显著提升了性能和自反思灵活性。

Conclusion: FCRF为复杂长周期机器人任务提供了更有效的错误纠正方法。

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [166] [CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions](https://arxiv.org/abs/2507.15022)
*Sumeadh MS,Kevin Dsouza,Ravi Prakash*

Main category: cs.RO

TL;DR: 本文提出了一种基于CPED-NCBFs的分割共形预测验证策略，用于验证从专家演示中学习的神经控制屏障函数（NCBFs）的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的验证方法（如SMT求解器、MIP和区间传播方法）在验证NCBFs时往往引入宽松且保守的边界，无法确保学习到的CBFs在整个状态空间中真正强制执行安全性。

Method: 采用基于分割共形预测的CPED-NCBFs验证策略，验证从专家演示中学习的NCBFs。

Result: 在点质量系统和非完整模型上验证了所提理论的有效性。

Conclusion: CPED-NCBFs方法能够有效验证学习到的NCBFs的安全性，克服了现有方法的局限性。

Abstract: Among the promising approaches to enforce safety in control systems, learning
Control Barrier Functions (CBFs) from expert demonstrations has emerged as an
effective strategy. However, a critical challenge remains: verifying that the
learned CBFs truly enforce safety across the entire state space. This is
especially difficult when CBF is represented using neural networks (NCBFs).
Several existing verification techniques attempt to address this problem
including SMT-based solvers, mixed-integer programming (MIP), and interval or
bound-propagation methods but these approaches often introduce loose,
conservative bounds. To overcome these limitations, in this work we use
CPED-NCBFs a split-conformal prediction based verification strategy to verify
the learned NCBF from the expert demonstrations. We further validate our method
on point mass systems and unicycle models to demonstrate the effectiveness of
the proposed theory.

</details>


### [167] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: 便携式夹持器集成触觉传感器，提出跨模态表示学习框架，提升机器人操作的精确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手持夹持器缺乏触觉反馈，而触觉反馈对精确操作至关重要。

Method: 开发便携轻量夹持器，集成触觉传感器，提出跨模态表示学习框架，结合视觉与触觉信号。

Result: 在精细任务（如试管插入和移液操作）中表现出更高的准确性和鲁棒性。

Conclusion: 集成触觉反馈和跨模态学习显著提升机器人操作的效率和精确性。

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [168] [Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions](https://arxiv.org/abs/2507.15155)
*Majid Roshanfar,Alex Zhang,Changyan He,Amir Hooshiar,Dale J. Podolsky,Thomas Looi,Eric Diller*

Main category: cs.RO

TL;DR: 提出了一种基于学习的磁控软吸引装置建模框架，用于内窥镜鼻内脑肿瘤切除，通过实验数据训练模型，实现了高精度形状预测。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够精确建模和预测磁控软机器人变形的框架，以支持微创神经外科手术中的智能控制。

Method: 使用3D打印的微型装置和FBG传感器，通过神经网络和随机森林模型训练实验数据，预测变形形状。

Result: 随机森林模型表现优于神经网络，预测控制点的均方根误差为0.087 mm，形状重建误差为0.064 mm。

Conclusion: 该学习框架无需简化物理假设即可建模超弹性软机器人的复杂非线性行为，为微创神经外科手术提供了智能控制工具。

Abstract: This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.

</details>


### [169] [CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer](https://arxiv.org/abs/2507.15189)
*Kevin Christiansen Marsim,Jinwoo Jeon,Yeeun Kim,Myeongwoo Jeong,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级深度补全网络CHADET，通过RGB图像和稀疏深度点生成精确的密集深度图，解决了现有方法在计算效率和准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 深度信息对机器人任务至关重要，但现有深度补全方法在实时应用中存在计算效率和准确性的显著权衡，需要改进。

Method: 提出CHADET网络，结合深度块提取特征和轻量级基于Transformer的解码器，使用跨层次注意力模块优化图像特征。

Result: 在KITTI、NYUv2和VOID数据集上验证，提高了深度图预测质量并减少了内存使用。

Conclusion: CHADET在保持轻量级的同时，显著提升了深度补全的准确性和处理速度，适用于实时机器人任务。

Abstract: Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion transformer), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and sparse depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
transformer-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.

</details>


### [170] [VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving](https://arxiv.org/abs/2507.15266)
*Haichao Liu,Haoren Guo,Pei Liu,Benshan Ma,Yuxiang Zhang,Jun Ma,Tong Heng Lee*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型（VLM）的统一决策与运动控制框架VLM-UDMC，通过场景推理和风险感知提升自动驾驶的透明性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 模仿人类驾驶员的场景理解和风险感知能力，提升自动驾驶的安全性和决策透明度。

Method: 采用两级系统架构，上层慢系统通过RAG技术处理多模态输入并生成风险感知洞察，下层快系统实时调整运动规划。

Result: 仿真和真实实验验证了VLM-UDMC的有效性，提升了城市驾驶性能。

Conclusion: VLM-UDMC框架成功结合场景理解和注意力分解，为自动驾驶提供了更合理的决策支持。

Abstract: Scene understanding and risk-aware attentions are crucial for human drivers
to make safe and effective driving decisions. To imitate this cognitive ability
in urban autonomous driving while ensuring the transparency and
interpretability, we propose a vision-language model (VLM)-enhanced unified
decision-making and motion control framework, named VLM-UDMC. This framework
incorporates scene reasoning and risk-aware insights into an upper-level slow
system, which dynamically reconfigures the optimal motion planning for the
downstream fast system. The reconfiguration is based on real-time environmental
changes, which are encoded through context-aware potential functions. More
specifically, the upper-level slow system employs a two-step reasoning policy
with Retrieval-Augmented Generation (RAG), leveraging foundation models to
process multimodal inputs and retrieve contextual knowledge, thereby generating
risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM
provides real-time trajectory predictions for heterogeneous traffic
participants by extracting smoother trend representations for short-horizon
trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is
verified via both simulations and real-world experiments with a full-size
autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively
leverages scene understanding and attention decomposition for rational driving
decisions, thus improving the overall urban driving performance. Our
open-source project is available at https://github.com/henryhcliu/vlmudmc.git.

</details>


### [171] [RepILN: Reparameterized Inertial Localization Network](https://arxiv.org/abs/2507.15293)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: 提出了一种重参数化的惯性定位网络，通过多分支结构增强特征提取，并在推理时转换为单路径架构以提高效率，结合稀疏注意力机制和门控卷积单元，实现了精度与模型紧凑性的平衡。


<details>
  <summary>Details</summary>
Motivation: 惯性定位因其成本效益和独立性成为物联网设备的理想方案，但现有方法依赖复杂网络架构且忽略长期依赖关系，限制了性能。

Method: 采用多分支训练结构转换为单路径推理架构，引入时间尺度稀疏注意力机制和门控卷积单元以捕捉长期依赖和局部特征。

Result: 在RoNIN数据集上，绝对轨迹误差降低2.59%，参数数量减少3.86%。

Conclusion: 该方法在精度和模型效率间取得了良好平衡，适用于资源受限的物联网设备。

Abstract: Inertial localization is regarded as a promising positioning solution for
consumer-grade IoT devices due to its cost-effectiveness and independence from
external infrastructure. However, data-driven inertial localization methods
often rely on increasingly complex network architectures to improve accuracy,
which challenges the limited computational resources of IoT devices. Moreover,
these methods frequently overlook the importance of modeling long-term
dependencies in inertial measurements - a critical factor for accurate
trajectory reconstruction - thereby limiting localization performance. To
address these challenges, we propose a reparameterized inertial localization
network that uses a multi-branch structure during training to enhance feature
extraction. At inference time, this structure is transformed into an equivalent
single-path architecture to improve parameter efficiency. To further capture
long-term dependencies in motion trajectories, we introduce a temporal-scale
sparse attention mechanism that selectively emphasizes key trajectory segments
while suppressing noise. Additionally, a gated convolutional unit is
incorporated to effectively integrate long-range dependencies with local
fine-grained features. Extensive experiments on public benchmarks demonstrate
that our method achieves a favorable trade-off between accuracy and model
compactness. For example, on the RoNIN dataset, our approach reduces the
Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while
reducing the number of parameters by 3.86%.

</details>


### [172] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了一种基于实时流场测量的四旋翼无人机闭环控制系统，用于在狭窄管道中悬停，解决了气流扰动问题。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在狭窄管道中悬停时因气流扰动导致的稳定性问题。

Method: 开发了低延迟的事件式烟雾测速方法，结合基于循环卷积神经网络的扰动估计器和强化学习训练的控制器。

Result: 系统在管道横截面横向移动时表现优异，能有效抵消瞬态气动效应，避免碰撞。

Conclusion: 首次展示了基于实时流场测量的无人机闭环控制，为复杂气动环境飞行研究开辟了新方向。

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [173] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: 本文综述了路径规划领域的研究进展，包括传统方法和深度强化学习（DRL）的应用，重点分析了算法优缺点，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境中自主系统的需求推动了路径规划方法的研究，尤其是DRL在自主导航中的应用。

Method: 综述了传统图搜索、线性规划和进化计算方法，以及DRL在路径规划中的应用。

Result: 分析了各类算法的计算效率、可扩展性、适应性和鲁棒性，并提出了混合方法的潜力。

Conclusion: 未来研究应关注DRL与传统方法的结合，以实现更稳健的自主导航。

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [174] [All-UWB SLAM Using UWB Radar and UWB AOA](https://arxiv.org/abs/2507.15474)
*Charith Premachandra,Achala Athukorala,U-Xuan Tan*

Main category: cs.RO

TL;DR: 论文提出了一种结合超宽带（UWB）雷达和到达角（AOA）测量的新方法，用于在视觉受限且特征稀缺的环境中提升SLAM的精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在恶劣环境（如烟雾、灰尘）中，光学传感器（如LiDAR、相机）容易失效，而UWB雷达因其低频特性能够穿透这些环境，但现有UWB雷达SLAM方法受限于环境中可区分特征的数量。

Method: 通过动态部署UWB锚点-标签单元获取AOA测量数据，并将其整合到UWB雷达SLAM系统中，以解决特征稀缺问题。

Result: 实验结果表明，结合UWB AOA单元的UWB雷达SLAM系统能够在视觉受限且特征稀缺的环境中实现定位与建图。

Conclusion: 该方法为恶劣环境中的SLAM提供了新的解决方案，克服了传统方法的局限性。

Abstract: There has been a growing interest in autonomous systems designed to operate
in adverse conditions (e.g. smoke, dust), where the visible light spectrum
fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating
through such challenging environmental conditions due to the lower frequency
components within its broad bandwidth. Therefore, UWB radar has emerged as a
potential sensing technology for Simultaneous Localization and Mapping (SLAM)
in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are
prone to failure. Existing approaches involving UWB radar as the primary
exteroceptive sensor generally extract features in the environment, which are
later initialized as landmarks in a map. However, these methods are constrained
by the number of distinguishable features in the environment. Hence, this paper
proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements
into UWB radar-based SLAM systems to improve the accuracy and scalability of
SLAM in feature-deficient environments. The AOA measurements are obtained using
UWB anchor-tag units which are dynamically deployed by the robot in featureless
areas during mapping of the environment. This paper thoroughly discusses
prevailing constraints associated with UWB AOA measurement units and presents
solutions to overcome them. Our experimental results show that integrating UWB
AOA units with UWB radar enables SLAM in vision-denied feature-deficient
environments.

</details>


### [175] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: 论文提出了一种神经符号系统框架（CoCo），结合概率符号推理与深度学习，以提升自主代理在不确定环境中的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在不确定环境中可靠且合规行为的挑战。

Method: 引入Constitutional Controller（CoCo）框架，结合深度概率逻辑程序和自我怀疑机制。

Result: 在真实空中交通研究中，CoCo能安全合规地导航复杂不确定环境。

Conclusion: 神经符号系统为自主代理的安全性和可靠性提供了有效解决方案。

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [176] [Robots for Kiwifruit Harvesting and Pollination](https://arxiv.org/abs/2507.15484)
*Jamie Bell*

Main category: cs.RO

TL;DR: 研究开发了用于猕猴桃果园的移动机器人，实现了定向花粉喷洒和自动化采摘，改进了采摘机制和导航系统。


<details>
  <summary>Details</summary>
Motivation: 提高猕猴桃果园的自动化水平，解决传统采摘和授粉效率低的问题。

Method: 设计多种猕猴桃采摘机制，测试了3D激光雷达和计算机视觉的导航系统，并优化花粉喷洒技术。

Result: 采摘机制覆盖80%以上的果实，优于传统方法的70%；导航系统在30公里自主驾驶中表现良好。

Conclusion: 移动机器人在猕猴桃果园中具有高效性和可行性，为自动化农业提供了新方案。

Abstract: This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.

</details>


### [177] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: GR-3是一个大规模视觉-语言-动作（VLA）模型，展示了在新对象、环境和抽象指令上的泛化能力，并能通过少量人类轨迹数据高效微调。


<details>
  <summary>Details</summary>
Motivation: 目标是构建通用机器人策略，以辅助人类日常生活。

Method: 通过多方面的训练方法，包括与网络规模视觉-语言数据共同训练、基于VR设备收集的人类轨迹数据微调，以及机器人轨迹数据的模仿学习。

Result: GR-3在多种挑战性任务中超越了最先进的基线方法π₀，尤其在长时程和灵巧任务中表现优异。

Conclusion: GR-3是迈向通用机器人发展的重要一步，展示了其在现实世界中的潜力。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [178] [CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions](https://arxiv.org/abs/2507.15499)
*Jongseok Lee,Timo Birr,Rudolph Triebel,Tamim Asfour*

Main category: cs.RO

TL;DR: CLEVER是一个基于深度神经网络的主动学习系统，通过在线获取人类支持并适应指令，提升语义感知的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在流数据中语义感知的鲁棒性问题，通过人类干预和在线适应提升性能。

Method: 采用贝叶斯框架编码领域知识作为先验，设计了一个流式主动学习系统，并在真实机器人上实现。

Result: 通过用户验证实验和实验验证，证明了CLEVER在提升语义感知鲁棒性方面的有效性。

Conclusion: CLEVER是首个在真实机器人上实现流式主动学习的系统，显著提升了DNN语义感知的鲁棒性。

Abstract: We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.

</details>


### [179] [Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding](https://arxiv.org/abs/2507.15604)
*Johannes Hartwig,Philipp Lienhardt,Dominik Henrich*

Main category: cs.RO

TL;DR: 论文提出了一种无需专用负载惯性参数（PIP）校准的方法，通过非接触运动部分估计机器人工具的PIP，从而简化非专家用户编程接触运动的过程。


<details>
  <summary>Details</summary>
Motivation: 随着协作机器人（cobots）的普及，需要为非专业用户提供更简便的编程方式，尤其是涉及接触运动时。传统方法需要专门的PIP校准，限制了工具的灵活性。

Method: 利用任务中的非接触运动部分，通过已有估计技术计算PIP，避免专用校准。

Result: 实验表明，负载质量的估计准确，但质心和惯性张量受噪声和激励不足的影响。

Conclusion: 该方法可行，但需要足够的负载加速度以提高估计精度。

Abstract: As the availability of cobots increases, it is essential to address the needs
of users with little to no programming knowledge to operate such systems
efficiently. Programming concepts often use intuitive interaction modalities,
such as hand guiding, to address this. When programming in-contact motions,
such frameworks require knowledge of the robot tool's payload inertial
parameters (PIP) in addition to the demonstrated velocities and forces to
ensure effective hybrid motion-force control. This paper aims to enable
non-expert users to program in-contact motions more efficiently by eliminating
the need for a dedicated PIP calibration, thereby enabling flexible robot tool
changes. Since demonstrated tasks generally also contain motions with
non-contact, our approach uses these parts to estimate the robot's PIP using
established estimation techniques. The results show that the estimation of the
payload's mass is accurate, whereas the center of mass and the inertia tensor
are affected by noise and a lack of excitation. Overall, these findings show
the feasibility of PIP estimation during hand guiding but also highlight the
need for sufficient payload accelerations for an accurate estimation.

</details>


### [180] [A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning](https://arxiv.org/abs/2507.15607)
*Yanbo Chen,Yunzhe Tan,Yaojia Wang,Zhengzhe Xu,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: 提出了一种新型通用车辆-拖车导航系统，结合混合运动学模型和在线残差学习模块，通过模型预测控制框架实现高性能导航。


<details>
  <summary>Details</summary>
Motivation: 车辆-拖车系统在机场、超市等环境中的自主导航需求迫切，但传统建模方法难以准确描述带脚轮的拖车动力学。

Method: 采用混合运动学模型（车辆非完整约束+神经网络拖车动力学）和在线残差学习模块，结合模型预测控制框架。

Result: 通过多种拖车和载荷条件的真实实验验证，系统表现稳健，无需手动调整或拖车特定校准。

Conclusion: 该方法为复杂拖车系统提供了一种通用且高效的导航解决方案。

Abstract: Autonomous navigation of vehicle-trailer systems is crucial in environments
like airports, supermarkets, and concert venues, where various types of
trailers are needed to navigate with different payloads and conditions.
However, accurately modeling such systems remains challenging, especially for
trailers with castor wheels. In this work, we propose a novel universal
vehicle-trailer navigation system that integrates a hybrid nominal kinematic
model--combining classical nonholonomic constraints for vehicles and neural
network-based trailer kinematics--with a lightweight online residual learning
module to correct real-time modeling discrepancies and disturbances.
Additionally, we develop a model predictive control framework with a weighted
model combination strategy that improves long-horizon prediction accuracy and
ensures safer motion planning. Our approach is validated through extensive
real-world experiments involving multiple trailer types and varying payload
conditions, demonstrating robust performance without manual tuning or
trailer-specific calibration.

</details>


### [181] [Optimizing Force Signals from Human Demonstrations of In-Contact Motions](https://arxiv.org/abs/2507.15608)
*Johannes Hartwig,Fabian Viessmann,Dominik Henrich*

Main category: cs.RO

TL;DR: 论文探讨了如何优化力信号以更好地反映人类意图，比较了不同信号滤波方法，并提出了一种峰值检测方法，显著提高了运动质量。


<details>
  <summary>Details</summary>
Motivation: 非机器人编程专家通过动觉引导进行编程时，输入信号不精确且噪声多，影响运动再现和机器学习输入。

Method: 比较不同信号滤波方法，提出峰值检测方法处理首次接触偏差，并分析关键参数对滤波方法的影响。

Result: 单个运动的误差标准可提高达20%，提升了机器人编程的可用性和人机交互。

Conclusion: 优化力信号的方法能显著改善人机交互和机器人编程的实用性。

Abstract: For non-robot-programming experts, kinesthetic guiding can be an intuitive
input method, as robot programming of in-contact tasks is becoming more
prominent. However, imprecise and noisy input signals from human demonstrations
pose problems when reproducing motions directly or using the signal as input
for machine learning methods. This paper explores optimizing force signals to
correspond better to the human intention of the demonstrated signal. We compare
different signal filtering methods and propose a peak detection method for
dealing with first-contact deviations in the signal. The evaluation of these
methods considers a specialized error criterion between the input and the
human-intended signal. In addition, we analyze the critical parameters'
influence on the filtering methods. The quality for an individual motion could
be increased by up to \SI{20}{\percent} concerning the error criterion. The
proposed contribution can improve the usability of robot programming and the
interaction between humans and robots.

</details>


### [182] [EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation](https://arxiv.org/abs/2507.15649)
*Haocheng Xu,Haodong Zhang,Zhenghan Chen,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的框架，使人形机器人模仿人类上半身动作并保持整体稳定性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在站立时可控范围有限，影响全身稳定性，需研究如何在执行上半身动作时保持稳定。

Method: 设计了重定向网络生成大规模上半身动作数据集，结合强化学习策略和领域随机化提升鲁棒性，并引入可执行运动先验（EMP）模块调整目标动作以确保安全和稳定性。

Result: 通过仿真和实际测试验证了框架的实用性。

Conclusion: 该框架有效提升了人形机器人在模仿人类动作时的稳定性，具有实际应用价值。

Abstract: To support humanoid robots in performing manipulation tasks, it is essential
to study stable standing while accommodating upper-body motions. However, the
limited controllable range of humanoid robots in a standing position affects
the stability of the entire body. Thus we introduce a reinforcement learning
based framework for humanoid robots to imitate human upper-body motions while
maintaining overall stability. Our approach begins with designing a retargeting
network that generates a large-scale upper-body motion dataset for training the
reinforcement learning (RL) policy, which enables the humanoid robot to track
upper-body motion targets, employing domain randomization for enhanced
robustness. To avoid exceeding the robot's execution capability and ensure
safety and stability, we propose an Executable Motion Prior (EMP) module, which
adjusts the input target movements based on the robot's current state. This
adjustment improves standing stability while minimizing changes to motion
amplitude. We evaluate our framework through simulation and real-world tests,
demonstrating its practical applicability.

</details>


### [183] [Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms](https://arxiv.org/abs/2507.15677)
*Huayue Liang,Yanbo Chen,Hongyang Cheng,Yanzhao Yu,Shoujie Li,Junbo Tan,Xueqian Wang,Long Zeng*

Main category: cs.RO

TL;DR: 提出了一种基于输入输出数据的模型预测控制（MPC）方法，用于提高柔性电缆驱动机械臂（FCRA）的控制精度，无需物理模型。通过数据选择算法（DSA）优化计算效率，并在实际平台上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 柔性电缆驱动机械臂（FCRA）的电缆特性（如弹性、迟滞和摩擦）导致建模和控制困难，传统方法效果不佳。

Method: 1. 基于输入输出数据构建隐式模型，并集成到MPC框架中；2. 引入数据选择算法（DSA）优化计算效率；3. 通过仿真研究超参数对跟踪误差的影响。

Result: 在实际FCRA平台上验证，平均定位精度约2.070 mm；相比PID方法（平均跟踪误差1.418°），提出的方法平均跟踪误差为0.541°。

Conclusion: 提出的数据驱动MPC方法显著提高了FCRA的控制精度和计算效率，适用于复杂任务。

Abstract: Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant
motion. Still, the inherent properties of cables, such as resilience,
hysteresis, and friction, often lead to particular difficulties in modeling and
control. This paper proposes a model predictive control (MPC) method that
relies exclusively on input-output data, without a physical model, to improve
the control accuracy of FCRAs. First, we develop an implicit model based on
input-output data and integrate it into an MPC optimization framework. Second,
a data selection algorithm (DSA) is introduced to filter the data that best
characterize the system, thereby reducing the solution time per step to
approximately 4 ms, which is an improvement of nearly 80%. Lastly, the
influence of hyperparameters on tracking error is investigated through
simulation. The proposed method has been validated on a real FCRA platform,
including five-point positioning accuracy tests, a five-point response tracking
test, and trajectory tracking for letter drawing. The results demonstrate that
the average positioning accuracy is approximately 2.070 mm. Moreover, compared
to the PID method with an average tracking error of 1.418{\deg}, the proposed
method achieves an average tracking error of 0.541{\deg}.

</details>


### [184] [Strong, Accurate, and Low-Cost Robot Manipulator](https://arxiv.org/abs/2507.15693)
*Georges Chebly,Spencer Little,Nisal Perera,Aliya Abedeen,Ken Suzuki,Donghyun Kim*

Main category: cs.RO

TL;DR: Forte是一款完全3D打印的6自由度机械臂，具有接近工业级的性能（0.63 kg负载、0.467 m工作范围和亚毫米级重复精度），成本低于215美元。


<details>
  <summary>Details</summary>
Motivation: 为教育课堂和AI实验提供高性能、低成本的机器人平台，突破现有低成本教育机械臂的性能限制。

Method: 采用基于capstan的电缆驱动、同步带、简单张紧机构和轻量化3D打印结构，结合拓扑优化提高刚度，通过精心设计的传动系统减少背隙。

Result: 实验验证表明，Forte具有高重复性和负载能力。

Conclusion: Forte是一个适用于课堂教学和高级机器人研究的理想平台。

Abstract: This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed
to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,
and sub-millimeter repeatability - at a material cost under $215. As an
accessible robot for broad applications across classroom education to AI
experiments, Forte pushes forward the performance limitations of existing
low-cost educational arms. We introduce a cost-effective mechanical design that
combines capstan-based cable drives, timing belts, simple tensioning
mechanisms, and lightweight 3D-printed structures, along with topology
optimization for structural stiffness. Through careful drivetrain engineering,
we minimize backlash and maintain control fidelity without relying on
high-power electronics or expensive manufacturing processes. Experimental
validation demonstrates that Forte achieves high repeatability and load
capacity, offering a compelling robotic platform for both classroom instruction
and advanced robotics research.

</details>


### [185] [Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages](https://arxiv.org/abs/2507.15710)
*Lu Huang,Lingxiao Meng,Jiankun Wang,Xingjian Jing*

Main category: cs.RO

TL;DR: 提出了一种高效的多分辨率采样规划框架，通过动态调整采样密度，在复杂配置空间中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有采样规划算法在复杂配置空间中效率低下，启发式方法缺乏通用性或需要大量训练。

Method: 结合多分辨率采样，动态调整稀疏与密集采样，在线探索配置空间。

Result: 在多个高维空间中表现优于现有方法，并在机器人实验中验证了优越性。

Conclusion: 该方法在复杂环境中高效且通用，无需大量先验训练。

Abstract: Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.

</details>


### [186] [DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models](https://arxiv.org/abs/2507.15716)
*Ziyu Wan,Lin Zhao*

Main category: cs.RO

TL;DR: DiffPF是一种可微分粒子滤波器，利用扩散模型进行动态系统状态估计，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统可微分粒子滤波器依赖预定义或低容量提议分布，限制了性能。DiffPF旨在通过扩散模型学习灵活的后验采样器，提升复杂分布下的状态估计。

Method: DiffPF通过扩散模型在预测粒子和当前观测条件下学习后验采样器，实现高维、多模态分布下的精确采样。

Result: 在模拟和真实任务中，DiffPF表现优异，如在多模态全局定位任务中准确率提升82.8%，在KITTI视觉测距基准中提升26%。

Conclusion: DiffPF首次将条件扩散模型引入粒子滤波，显著提升了后验采样质量和状态估计性能。

Abstract: This paper proposes DiffPF, a differentiable particle filter that leverages
diffusion models for state estimation in dynamic systems. Unlike conventional
differentiable particle filters, which require importance weighting and
typically rely on predefined or low-capacity proposal distributions. DiffPF
learns a flexible posterior sampler by conditioning a diffusion model on
predicted particles and the current observation. This enables accurate,
equally-weighted sampling from complex, high-dimensional, and multimodal
filtering distributions. We evaluate DiffPF across a range of scenarios,
including both unimodal and highly multimodal distributions, and test it on
simulated as well as real-world tasks, where it consistently outperforms
existing filtering baselines. In particular, DiffPF achieves an 82.8%
improvement in estimation accuracy on a highly multimodal global localization
benchmark, and a 26% improvement on the real-world KITTI visual odometry
benchmark, compared to state-of-the-art differentiable filters. To the best of
our knowledge, DiffPF is the first method to integrate conditional diffusion
models into particle filtering, enabling high-quality posterior sampling that
produces more informative particles and significantly improves state
estimation.

</details>


### [187] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 论文提出了一种基于大语言模型（LLM）的辅助机器人交互系统，通过多模态输入（如视线和语音）支持动态用户任务，提升了适应性和用户体验，但可能产生冗余输出。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互系统在单向指令执行和任务解决方面取得进展，但在双向、多模态和上下文感知的协作任务支持方面仍存在挑战。

Method: 开发了一个模块化、可转移的系统，结合多视觉输入和实时语言交互状态表示，支持动态用户任务。

Result: 实验表明，基于LLM的系统提升了适应性和用户参与度，但可能产生冗余输出；传统脚本化系统更适合简单任务。

Conclusion: LLM方法在复杂任务中表现优越，但需优化以减少冗余；传统方法在简单任务中更高效。

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>


### [188] [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](https://arxiv.org/abs/2507.15782)
*Ruochu Yang,Yu Zhou,Fumin Zhang,Mengxue Hou*

Main category: cs.RO

TL;DR: 论文提出了一种名为Inter-LLM的新型算法，用于解决家庭机器人在多对象收集任务中的长时程规划问题，通过结合LLM和运动规划，显著提升了任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 家庭机器人在处理开放集对象和大型环境导航时缺乏人类智能，多对象收集任务在长时程规划中面临巨大挑战。

Method: 提出Inter-LLM算法，结合多模态动作成本相似性函数，优化历史与未来规划，平衡质量与效率。

Result: 仿真实验显示，相比现有方法，任务完成率提升30%，同时最大化成功率并最小化成本。

Conclusion: Inter-LLM算法在多对象收集任务中表现出色，为家庭机器人智能规划提供了有效解决方案。

Abstract: Household robots have been a longstanding research topic, but they still lack
human-like intelligence, particularly in manipulating open-set objects and
navigating large environments efficiently and accurately. To push this
boundary, we consider a generalized multi-object collection problem in large
scene graphs, where the robot needs to pick up and place multiple objects
across multiple locations in a long mission of multiple human commands. This
problem is extremely challenging since it requires long-horizon planning in a
vast action-state space under high uncertainties. To this end, we propose a
novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a
multimodal action cost similarity function, our algorithm can both reflect the
history and look into the future to optimize plans, striking a good balance of
quality and efficiency. Simulation experiments demonstrate that compared with
latest works, our algorithm improves the overall mission performance by 30% in
terms of fulfilling human commands, maximizing mission success rates, and
minimizing mission costs.

</details>


### [189] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 论文探讨了将人类主动注视机制引入机器人视觉系统，以提高效率和性能。通过结合眼动数据和机器人演示，提出了一种新的框架，并验证了其在计算效率和任务性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 人类视觉通过注视和注意力主动处理任务相关区域，而机器人系统通常被动处理图像。研究旨在探索主动注视机制如何提升机器人视觉的效率和性能。

Method: 结合眼动数据和机器人演示，提出了一种基于Vision Transformers的注视焦点化处理框架，并探索了两种注视模仿和预测方法。

Result: 该方法显著减少了计算开销，同时提高了高精度任务的性能和对抗干扰的鲁棒性。

Conclusion: 人类启发的视觉处理为机器人视觉系统提供了有效的归纳偏置，具有实际应用潜力。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [190] [Distributed consensus-based observer design for target state estimation with bearing measurements](https://arxiv.org/abs/2507.14300)
*Marcelo Jacinto,Pedro Trindade,Francisco Rego,Rita Cunha*

Main category: eess.SY

TL;DR: 提出了一种基于分布式共识的观测器设计，用于解决无向通信网络中多智能体的目标跟踪问题，目标建模为任意阶积分器链。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体在无向网络中仅通过自身位置和相对目标的方位向量进行目标跟踪的问题。

Method: 设计了一种连续时间观测器，结合创新和共识项，减少网络数据传输，并引入非线性闭环系统的稳定性结果。

Result: 提供了显式稳定性条件，依赖于目标与智能体的几何构型，并通过数值结果验证了算法性能。

Conclusion: 提出的方法有效解决了分布式目标跟踪问题，并通过理论和数值验证了其稳定性和实用性。

Abstract: This paper introduces a novel distributed consensus-based observer design
that enables a group of agents in an undirected communication network to solve
the problem of target tracking, where the target is modeled as a chain of
integrators of arbitrary order. Each agent is assumed to know its own position
and simultaneously measure bearing vectors relative to the target. We start by
introducing a general continuous time observer design tailored to systems whose
state dynamics are modeled as chains of integrators and whose measurement model
follows a particular nonlinear but observer-suited form. This design leverages
a correction term that combines innovation and consensus components, allowing
each agent to broadcast only a part of the state estimate to its neighbours,
which effectively reduces the data flowing across the network. To provide
uniform exponential stability guarantees, a novel result for a class of
nonlinear closed-loop systems in a generalized observer form is introduced and
subsequently used as the main tool to derive stability conditions on the
observer gains. Then, by exploring the properties of orthogonal projection
matrices, the proposed design is used to solve the distributed target tracking
problem and provide explicit stability conditions that depend on the
target-agents geometric formation. Practical examples are derived for a target
modeled as first-, second-, and third-order integrator dynamics, highlighting
the design procedure and the stability conditions imposed. Finally, numerical
results showcase the properties of the proposed algorithm.

</details>


### [191] [Remote Assistance or Remote Driving: The Impact of Operational Design Domains on ADS-Supporting Systems Selection](https://arxiv.org/abs/2507.14347)
*Ole Hans,Benedikt Walter*

Main category: eess.SY

TL;DR: 本文提出了一种基于ODD和用例分析的结构化方法，用于在RDS和RAS之间选择适合ADS的远程支持系统。


<details>
  <summary>Details</summary>
Motivation: 高级自动驾驶系统（ADS）在无驾驶员情况下（如Level 4）需要远程支持系统（RDS或RAS），但现有选择方法忽略了用例和系统互补性。

Method: 应用PEGASUS框架系统描述和分析ODD，并引入结构化框架以明确标准评估和选择远程支持系统。

Result: 提出了一种基于ODD和用例分析的选择方法，填补了现有方法的不足。

Conclusion: 通过结构化方法选择远程支持系统，可优化ADS与远程系统的互补性，提升系统适用性。

Abstract: High level Automated Driving Systems (ADS) can handle many situations, but
they still encounter situations where human intervention is required. In
systems where a physical driver is present in the vehicle, typically SAE Level
3 systems, this intervention is relatively straightforward and is handled by
the in-vehicle driver. However, the complexity increases for Level 4 systems,
where, in most cases, no physical driver remains in the vehicle. The two common
industry solutions for this challenge are the integration of a remote support
system, such as a Remote Driving System (RDS) or Remote Assistance System
(RAS). While it is clear that ADS will require one of these systems, it is less
clear how the suitability of either system for a particular ADS application
should be evaluated. Currently, the selection process often focuses on system
architecture as well as its design and integration challenges. Furthermore,
since many ADS developers choose to develop remote system solutions in-house,
it is advantageous to select the simpler approach to streamline development and
integration efforts. While these decision points are certainly relevant, this
approach overlooks the most critical factors: the use cases and the
complementarity of the ADS and the remote support system within the context of
the Operational Design Design Domain (ODD). This paper proposes a structured
approach for selecting between RDS and RAS as an ADS support system, based on
the defined ODD and use case analysis. To achieve this, the paper applies the
PEGASUS framework to systematically describe and analyze the ODD. A structured
framework is introduced to evaluate and select the most suitable remote support
system for an ADS based on clearly defined criteria.

</details>


### [192] [Bi-level Model Predictive Control for Energy-aware Integrated Product Pricing and Production Scheduling](https://arxiv.org/abs/2507.14385)
*Hongliang Li,Herschel C. Pangborn,Ilya Kovalenko*

Main category: eess.SY

TL;DR: 提出了一种双层模型预测控制框架，联合优化产品价格和生产调度，以提升可持续性和经济竞争力。


<details>
  <summary>Details</summary>
Motivation: 制造业面临提升可持续性同时保持经济竞争力的压力，需要整合现场可再生能源和实时电价。

Method: 采用双层模型预测控制框架，上层优化产品价格和可再生能源使用，下层实时调度生产以降低成本。

Result: 案例研究表明，该方法能降低电网能源成本并增加利润。

Conclusion: 该框架为制造业在可持续性和盈利性之间提供了平衡解决方案。

Abstract: The manufacturing industry is under growing pressure to enhance
sustainability while preserving economic competitiveness. As a result,
manufacturers have been trying to determine how to integrate onsite renewable
energy and real-time electricity pricing into manufacturing schedules without
compromising profitability. To address this challenge, we propose a bi-level
model predictive control framework that jointly optimizes product prices and
production scheduling with explicit consideration of renewable energy
availability. The higher level determines the product price to maximize revenue
and renewable energy usage. The lower level controls production scheduling in
runtime to minimize operational costs and respond to the product demand. Price
elasticity is incorporated to model market response, allowing the system to
increase demand by lowering the product price during high renewable energy
generation. Results from a lithium-ion battery pack manufacturing system case
study demonstrate that our approach enables manufacturers to reduce grid energy
costs while increasing profit.

</details>


### [193] [Collaborative Indirect Influencing and Control on Graphs using Graph Neural Networks](https://arxiv.org/abs/2507.14409)
*Max L. Gardenswartz,Brandon C. Fallin,Cristian F. Nino,Warren E. Dixon*

Main category: eess.SY

TL;DR: 提出了一种基于图神经网络（GNN）的新方法，用于解决网络系统中间接影响问题，并通过实时学习目标节点动态实现轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决网络系统中节点协作调节目标节点动态不确定性的问题。

Method: 利用GNN的消息传递结构，开发了一种基于GNN的反步控制策略，并结合Lyapunov分析提供稳定性保证。

Result: 数值模拟验证了所开发控制器的性能。

Conclusion: 该方法有效解决了间接影响问题，并具有稳定性保证。

Abstract: This paper presents a novel approach to solving the indirect influence
problem in networked systems, in which cooperative nodes must regulate a target
node with uncertain dynamics to follow a desired trajectory. We leverage the
message-passing structure of a graph neural network (GNN), allowing nodes to
collectively learn the unknown target dynamics in real time. We develop a novel
GNN-based backstepping control strategy with formal stability guarantees
derived from a Lyapunov-based analysis. Numerical simulations are included to
demonstrate the performance of the developed controller.

</details>


### [194] [A Black Start Strategy for Hydrogen-integrated Renewable Grids with Energy Storage Systems](https://arxiv.org/abs/2507.14450)
*Jin Lu,Linhan Fang,Fan Jiang,Xingpeng Li*

Main category: eess.SY

TL;DR: 本文提出了一种结合燃料电池和电池存储的新型黑启动模型，以提高现代电力系统的可靠性和韧性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源的集成增加，电力系统的可靠性和韧性变得至关重要，但大规模停电仍是一个重大威胁，需要有效的恢复策略。

Method: 研究提出了针对现代电力系统的黑启动模型，整合了燃料电池和电池存储，考虑了它们的独特特性及其对电网韧性的贡献。模型包括输电网络的通电路径和时间，并进行了黑启动模拟和敏感性分析。

Result: 在IEEE 39总线系统上进行了模拟，比较了燃料电池与电池系统的发电机启动顺序（GSUS），并分析了燃料电池容量、电池存储容量、初始SOC和资源位置对黑启动操作的影响。

Conclusion: 研究为优化黑启动操作提供了有效策略，燃料电池和电池存储的结合显著提升了电网恢复能力。

Abstract: With the increasing integration of renewable energy, the reliability and
resilience of modern power systems are of vital significance. However,
large-scale blackouts caused by natural disasters or equipment failures remain
a significant threat, necessitating effective restoration strategies. This
study proposes novel black start models for modern power systems that integrate
fuel cells and battery storage, recognizing their distinct characteristics and
contributions to grid resilience. These models specifically address the
restoration of electrical grids, including the energization paths and time of
the transmission network, while accounting for the unique power output traits
of fuel cells and the energy storage capacity of batteries as black start
resources. Black start simulations, comparing the generator startup sequence
(GSUS) with fuel cell versus battery systems, are performed on the IEEE 39-bus
system. We conduct sensitivity analyses on fuel cell capacity, battery storage
capacity, initial state of charge (SOC), and resource locations to identify
optimal scenarios for black start operations.

</details>


### [195] [Learning-Augmented Control: Adaptively Confidence Learning for Competitive MPC](https://arxiv.org/abs/2507.14595)
*Tongxin Li*

Main category: eess.SY

TL;DR: LAC（学习增强控制）通过结合机器学习预测与控制非线性动态系统，实现预测准确时的最优性能和预测错误时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决在非线性动态系统中如何安全有效地结合不可信的机器学习预测的问题。

Method: 采用延迟置信学习程序，在线优化置信参数，平衡ML预测与名义预测。

Result: 在非线性系统中建立了竞争比界限，线性二次情况下证明了界限的紧性，数值实验显示其优于标准方法。

Conclusion: LAC在保证稳定性的同时，显著提升了系统性能，尤其在对抗性预测错误时表现优异。

Abstract: We introduce Learning-Augmented Control (LAC), an approach that integrates
untrusted machine learning predictions into the control of constrained,
nonlinear dynamical systems. LAC is designed to achieve the
"best-of-both-worlds" guarantees, i.e, near-optimal performance when
predictions are accurate, and robust, safe performance when they are not. The
core of our approach is a delayed confidence learning procedure that optimizes
a confidence parameter online, adaptively balancing between ML and nominal
predictions. We establish formal competitive ratio bounds for general nonlinear
systems under standard MPC regularity assumptions. For the linear quadratic
case, we derive a competitive ratio bound that is provably tight, thereby
characterizing the fundamental limits of this learning-augmented approach. The
effectiveness of LAC is demonstrated in numerical studies, where it maintains
stability and outperforms standard methods under adversarial prediction errors.

</details>


### [196] [One-Time Programmable Passive Electromagnetic Skins](https://arxiv.org/abs/2507.14601)
*Giacomo Oliveri,Francesco Zardi,Aaron Angel Salas Sancez,Andrea Massa*

Main category: eess.SY

TL;DR: 论文提出了一种低成本、易于大规模生产的“一次性可编程”电磁皮肤（OTP-EMS），用于智能电磁环境（SEME）的实现。通过集成可消耗组件，实现了模块化制造、一次性可配置反射特性、被动静态操作和零维护。


<details>
  <summary>Details</summary>
Motivation: 为了解决智能电磁环境（SEME）实现中低成本、大规模生产的需求，提出了OTP-EMS的概念。

Method: 定义并优化了一种OTP元原子结构，用于构建具有场景依赖的电磁波操控功能的EMS。

Result: 通过分析、数值模拟和实验验证了OTP-EMS在不同孔径、照射和电磁波操控需求下的性能。

Conclusion: OTP-EMS提供了一种简单、经济且高效的解决方案，适用于智能电磁环境的实现。

Abstract: The implementation of simple, inexpensive, and mass-production-oriented
solutions for smart electromagnetic environments (SEMEs) is dealt with by
introducing the concept of "one-time programmable" electromagnetic skins
(OTP-EMSs). The simultaneous achievement of modular fabrication, (one-time)
configurable reflection properties, passive-static operation, and zero
maintenance is yielded by integrating expendable components at the atomic level
of EMSs. Towards this end, an OTP meta-atom structure is properly defined and
optimized to build EMSs featuring the desired scenario-dependent EM wave
manipulation functionalities. In order to illustrate the features as well as to
point out the potentialities of OTP-EMSs, a representative set of analytical,
numerical, and experimental results is reported by considering different
apertures, illuminations, and EM wave manipulation requirements.

</details>


### [197] [Gait Transitions in Load-Pulling Quadrupeds: Insights from Sled Dogs and a Minimal SLIP Model](https://arxiv.org/abs/2507.14727)
*Jiayu Ding,Benjamin Seleb,Saad Bhamla,Zhenyu Gan*

Main category: eess.SY

TL;DR: 研究揭示了雪橇犬在高负荷高速运动中如何通过切换步态（旋转和横向疾驰）实现多稳态运动，并通过物理模型和轨迹优化验证了摆动腿刚度调节是关键控制机制。


<details>
  <summary>Details</summary>
Motivation: 探索四足动物在高负荷高速运动中的适应性步态转换机制，填补生物力学理解的空白。

Method: 结合高速视频、力记录和基于物理的四足弹簧倒立摆模型，通过轨迹优化模拟实验观察到的步态序列。

Result: 发现雪橇犬在不改变速度或地形的情况下快速切换步态，摆动腿刚度调节是诱导转换的关键机制。

Conclusion: 研究为高速动物牵引提供了生物力学视角，并为适应性腿式系统设计提供了建模框架。

Abstract: Quadrupedal animals employ diverse galloping strategies to optimize speed,
stability, and energy efficiency. However, the biomechanical mechanisms that
enable adaptive gait transitions during high-speed locomotion under load remain
poorly understood. In this study, we present new empirical and modeling
insights into the biomechanics of load-pulling quadrupeds, using sprint sled
dogs as a model system. High-speed video and force recordings reveal that sled
dogs often switch between rotary and transverse galloping gaits within just a
few strides and without any observable changes in speed, stride duration, or
terrain, providing clear evidence of locomotor multistability during high-speed
load-pulling. To investigate the mechanical basis of these transitions, a
physics-based quadrupedal Spring-Loaded Inverted Pendulum model with hybrid
dynamics and prescribed footfall sequences to reproduce the asymmetric
galloping patterns observed in racing sled dogs. Through trajectory
optimization, we replicate experimentally observed gait sequences and identify
swing-leg stiffness modulation as a key control mechanism for inducing
transitions. This work provides a much-needed biomechanical perspective on
high-speed animal draft and establishes a modeling framework for studying
locomotion in pulling quadrupeds, with implications for both biological
understanding and the design of adaptive legged systems.

</details>


### [198] [Enhancing Sustainability in HAPS-Assisted 6G Networks: Load Estimation Aware Cell Switching](https://arxiv.org/abs/2507.14728)
*Maryam Salamatmoghadasi,Metin Ozturk,Halim Yanikomeroglu*

Main category: eess.SY

TL;DR: 论文提出了一种解决垂直异构网络中小区切换时流量负载估计问题的有效方法，通过空间和时间预测技术显著提高了估计准确性。


<details>
  <summary>Details</summary>
Motivation: 小区切换的能效受限于休眠模式下小基站（SBSs）流量负载数据的缺失，导致许多依赖负载的节能方法不切实际。

Method: 研究结合了高空平台站（HAPS）作为超级宏基站（SMBS），探索了空间插值方案（随机邻居选择、基于距离选择和多级聚类）和时间深度学习方法（LSTM网络）。

Result: 实证结果显示，空间和时间方法均显著提高了估计准确性，其中多级聚类（MLC）和LSTM方法表现尤为突出。

Conclusion: 通过消除流量负载估计的基础障碍，研究为垂直异构网络中的小区切换提供了更可靠的解决方案。

Abstract: This study introduces and addresses the critical challenge of traffic load
estimation in cell switching within vertical heterogeneous networks. The
effectiveness of cell switching is significantly limited by the lack of
accurate traffic load data for small base stations (SBSs) in sleep mode, making
many load-dependent energy-saving approaches impractical, as they assume
perfect knowledge of traffic loads, an assumption that is unrealistic when SBSs
are inactive. In other words, when SBSs are in sleep mode, their traffic loads
cannot be directly known and can only be estimated, inevitably with
corresponding errors. Rather than proposing a new switching algorithm, we focus
on eliminating this foundational barrier by exploring effective prediction
techniques. A novel vertical heterogeneous network model is considered,
integrating a high-altitude platform station (HAPS) as a super macro base
station (SMBS). We investigate both spatial and temporal load estimation
approaches, including three spatial interpolation schemes, random neighboring
selection, distance based selection, and multi level clustering (MLC),
alongside a temporal deep learning method based on long short-term memory
(LSTM) networks. Using a real world dataset for empirical validation, our
results show that both spatial and temporal methods significantly improve
estimation accuracy, with the MLC and LSTM approaches demonstrating
particularly strong performance.

</details>


### [199] [Multi Target Observability](https://arxiv.org/abs/2507.14765)
*Debadrita Banerjee,Debjani Mitra,Rajesh Dey,Mudassir Khan,Lalan Kumar*

Main category: eess.SY

TL;DR: 本文研究了多目标可观测性问题，提出了基于单观测者对多目标高阶动态的观测条件，并分析了模糊轨迹以改进目标区分和跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 解决多目标状态估计中的可观测性问题，尤其是在单观测者情况下如何利用不同测量类型（如多普勒、方位角等）提高目标区分和跟踪准确性。

Method: 推导了基于方位角的多目标高阶动态观测的必要且充分条件，并分析了模糊轨迹，提出了针对多普勒、方位角及其组合测量的可观测性条件。

Result: 建立了多目标可观测性的必要且充分条件，并展示了这些条件如何提升目标区分、轨迹重建和整体跟踪精度。

Conclusion: 通过理论分析和条件推导，本文为多目标观测提供了新的可观测性标准，有助于改进实际跟踪系统的性能。

Abstract: In this paper, we mainly focus on the problem of multi-target observability,
focusing on the unique state estimation criteria for multiple targets. We
derive the condition which is necessary as well as sufficient for observability
using bearing angles with multiple higher-order dynamics observed by a single
observer. We then establish an alternative notion of observability by analyzing
ambiguous target trajectories and deriving the condition which is NECNDSUF
(Nec. and Suff.) for multi-target observability, considering three types of
measurements: Doppler-only, bearing-only, and combined Doppler and bearing
measurements, which offers insights that can improve target distinguishability,
trajectory reconstruction, and overall tracking accuracy.

</details>


### [200] [Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control](https://arxiv.org/abs/2507.14800)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Wenchuan Wu*

Main category: eess.SY

TL;DR: 本文提出了一种基于大语言模型（LLM）的电压控制方法，通过多模块协作实现策略的自主生成与优化。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的高级推理和信息分析能力，解决电力系统中电压控制的自主调度问题。

Method: 通过经验存储、检索、生成和修改四个模块的协作，实现LLM驱动的电压控制策略自进化。

Result: 实验验证了该方法的有效性，展示了LLM在电力系统调度中的适用性。

Conclusion: LLM为电力系统调度提供了一种创新的解决方案，具有实际应用潜力。

Abstract: With the advanced reasoning and information analysis capabilities, large
language models (LLMs) can offer a novel approach for the autonomous generation
of dispatch strategies in power systems. This letter proposes an LLM-based
experience-driven voltage control solution for distribution networks, which
enables the self-evolution of LLM-based voltage control strategies through the
collaboration and interaction of multiple modules-specifically, experience
storage, experience retrieval, experience generation, and experience
modification. Comprehensive experimental results validate the effectiveness of
the proposed method and highlight the applicability of LLM in addressing power
system dispatch challenges.

</details>


### [201] [Grid Stability and Power Factor Dynamics in Solar Farms Integration](https://arxiv.org/abs/2507.14857)
*Hassan Osseily*

Main category: eess.SY

TL;DR: 研究太阳能发电波动对电网稳定性的影响，提出无功功率控制策略和先进逆变器功能，结合AI优化SVC控制，提升可再生能源并网可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决太阳能发电波动对电网稳定性的挑战，优化无功功率控制以维持最佳功率因数。

Method: 采用ETAP仿真和案例研究，分析实时电网性能；提出无功功率控制策略和逆变器功能；尝试AI控制SVC。

Result: 理论分析和仿真验证了有效的并网技术，AI在SVC控制中表现出潜力。

Conclusion: 研究为可再生能源并网提供了实用解决方案，提升了电网可靠性。

Abstract: This paper examines the impact of solar farm fluctuations on grid stability,
focusing on maintaining an optimal power factor. ETAP-based simulations and
case studies are used to analyze real-time grid performance under solar
variability. Reactive power control strategies and advanced inverter functions
are proposed for stabilization. Theoretical analysis and simulation results
highlight effective integration techniques. Artificial intelligence is trailed
for controlling the SVC in adaptive reactive power compensation. The study
provides practical solutions for improving reliability in renewable-integrated
power systems.

</details>


### [202] [Adversarial Destabilization Attacks to Direct Data-Driven Control](https://arxiv.org/abs/2507.14863)
*Hampei Sasahara*

Main category: eess.SY

TL;DR: 研究揭示了直接数据驱动控制方法在对抗性扰动下的脆弱性，并提出攻击与防御策略。


<details>
  <summary>Details</summary>
Motivation: 探讨数据驱动控制方法在面对隐蔽对抗性扰动时的稳定性问题，以提升系统安全性。

Method: 提出DGSM和I-DGSM方法生成扰动，并基于KKT条件设计高效梯度计算技术；防御策略包括正则化和鲁棒控制。

Result: 实验表明，微小对抗性扰动即可破坏稳定性，而防御策略能有效降低攻击成功率。

Conclusion: 强调数据保密的重要性，并验证了防御策略的实用性。

Abstract: This study investigates the vulnerability of direct data-driven control
methods, specifically for the linear quadratic regulator problem, to
adversarial perturbations in collected data used for controller synthesis. We
consider stealthy attacks that subtly manipulate offline-collected data to
destabilize the resulting closed-loop system while evading detection. To
generate such perturbations, we propose the Directed Gradient Sign Method
(DGSM) and its iterative variant (I-DGSM), adaptations of the fast gradient
sign method originally developed for neural networks, which align perturbations
with the gradient of the spectral radius of the closed-loop matrix to reduce
stability. A key contribution is an efficient gradient computation technique
based on implicit differentiation through the Karush-Kuhn-Tucker conditions of
the underlying semidefinite program, enabling scalable and exact gradient
evaluation without repeated optimization computations. To defend against these
attacks, we propose two defense strategies: a regularization-based approach
that enhances robustness by suppressing controller sensitivity to data
perturbations and a robust data-driven control approach that guarantees
closed-loop stability within bounded perturbation sets. Extensive numerical
experiments on benchmark systems show that adversarial perturbations with
magnitudes up to ten times smaller than random noise can destabilize
controllers trained on corrupted data and that the proposed defense strategies
effectively mitigate attack success rates while maintaining control
performance. Additionally, we evaluate attack transferability under partial
knowledge scenarios, highlighting the practical importance of protecting
training data confidentiality.

</details>


### [203] [An approach to the LQG/LTR design problem with specifications for finite-dimensional SISO control systems](https://arxiv.org/abs/2507.14952)
*Mahyar Mahinzaeim,Kamyar Mehran*

Main category: eess.SY

TL;DR: 本文提出了一种基于权重增强的LQG/LTR设计方法，用于满足SISO控制系统的低频和高频设计规范。


<details>
  <summary>Details</summary>
Motivation: 为有限维SISO控制系统设计LQG补偿器，同时满足闭环性能和鲁棒性要求，适用于实际应用场景。

Method: 利用权重增强技术将设计规范融入LTR框架，通过LQG补偿器实现敏感性和控制器噪声敏感性的优化。

Result: 通过数值示例（弹性安装转子的直流电机扭矩控制）验证了方法的有效性。

Conclusion: 该方法为非专业人士和实际应用提供了一种有效的LQG/LTR设计途径。

Abstract: This is an expository paper which discusses an approach to the LQG/LTR design
problem for finite-dimensional SISO control systems. The approach is based on
the utilisation of weighting augmentation for incorporating design
specifications into the framework of the LTR technique for LQG compensator
design. The LQG compensator is to simultaneously meet given analytical low- and
high-frequency design specifications expressed in terms of desirable
sensitivity and controller noise sensitivity functions. The paper is aimed at
nonspecialists and, in particular, practitioners in finite-dimensional LQG
theory interested in the design of feedback compensators for closed-loop
performance and robustness shaping of SISO control systems in realistic
situations. The proposed approach is illustrated by a detailed numerical
example: the torque control of a current-controlled DC motor with an
elastically mounted rotor.

</details>


### [204] [Safety Controller Synthesis for Stochastic Networked Systems under Communication Constraints](https://arxiv.org/abs/2507.15031)
*Omid Akbarzadeh,Mohammad H. Mamduhi,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 该论文提出了一种框架，用于为在通信缺陷下运行的离散时间随机线性控制系统（dt-SLS）合成安全控制器。通过建模数据丢失和延迟，并利用控制屏障证书（CBCs）确保系统安全。


<details>
  <summary>Details</summary>
Motivation: 远程控制单元通过不完美的无线网络与传感器和执行器通信，存在延迟和数据丢失问题，需要一种系统方法来保证系统安全。

Method: 通过引入增强的离散时间随机线性系统（dt-ASLS）建模延迟和数据丢失，并利用控制屏障证书（CBCs）将安全约束转化为矩阵不等式，最终通过优化问题量化安全概率。

Result: 提出的方法能够量化在通信缺陷下满足安全规范的概率，并在RLC电路中验证了有效性。

Conclusion: 该框架为存在通信缺陷的随机线性控制系统提供了一种有效的安全控制器合成方法。

Abstract: This paper develops a framework for synthesizing safety controllers for
discrete-time stochastic linear control systems (dt-SLS) operating under
communication imperfections. The control unit is remote and communicates with
the sensor and actuator through an imperfect wireless network. We consider a
constant delay in the sensor-to-controller channel (uplink), and data loss in
both sensor-to-controller and controller-to-actuator (downlink) channels. In
our proposed scheme, data loss in each channel is modeled as an independent
Bernoulli-distributed random process. To systematically handle the uplink
delay, we first introduce an augmented discrete-time stochastic linear system
(dt-ASLS) by concatenating all states and control inputs that sufficiently
represent the state-input evolution of the original dt-SLS under the delay and
packet loss constraints. We then leverage control barrier certificates (CBCs)
for dt-ASLS to synthesize a controller that guarantees dt-SLS safety in a
stochastic sense, ensuring that all trajectories of dt-SLS remain within safe
regions with a quantified probabilistic bound. Our approach translates safety
constraints into matrix inequalities, leading to an optimization problem that
eventually quantifies the probability of satisfying the safety specification in
the presence of communication imperfections. We validate our results on an RLC
circuit subject to both constant delay and probabilistic data loss.

</details>


### [205] [On an Abstraction of Lyapunov and Lagrange Stability](https://arxiv.org/abs/2507.15047)
*Michelangelo Bin,David Angeli*

Main category: eess.SY

TL;DR: 论文研究了基于集值映射的抽象系统中Lyapunov和Lagrange稳定性的集合论推广，揭示了两种稳定性之间的对偶性，并提出了全局稳定性的定义。


<details>
  <summary>Details</summary>
Motivation: 探索抽象系统中稳定性的集合论推广，以揭示Lyapunov和Lagrange稳定性之间的深层关系，并为控制理论中的其他性质（如安全性和正性）提供统一框架。

Method: 通过集值映射定义抽象系统的稳定性，将Lyapunov稳定性描述为逆映射将滤波器映射到滤波器，Lagrange稳定性描述为将理想映射到理想。

Result: 揭示了两种稳定性的对偶性，提出了全局稳定性的定义，并推广了基本串联、并联和反馈互联的稳定性定理，包括小增益定理。

Conclusion: Lagrange稳定性与控制理论中的其他性质（如安全性和正性）抽象等价，为这些性质在互联系统中的保持提供了理论支持。

Abstract: This paper studies a set-theoretic generalization of Lyapunov and Lagrange
stability for abstract systems described by set-valued maps. Lyapunov stability
is characterized as the property of inversely mapping filters to filters,
Lagrange stability as that of mapping ideals to ideals. These abstract
definitions unveil a deep duality between the two stability notions, enable a
definition of global stability for abstract systems, and yield an agile
generalization of the stability theorems for basic series, parallel, and
feedback interconnections, including a small-gain theorem. Moreover, it is
shown that Lagrange stability is abstractly identical to other properties of
interest in control theory, such as safety and positivity, whose preservation
under interconnections can be thus studied owing to the developed stability
results.

</details>


### [206] [Exact Finite Koopman Embedding of Block-Oriented Polynomial Systems](https://arxiv.org/abs/2507.15093)
*Lucian Cristian Iacob,Roland Tóth,Maarten Schoukens*

Main category: eess.SY

TL;DR: 本文提出了一种系统化的方法，为非线性系统生成有限维且精确的Koopman嵌入，避免了传统数据驱动方法的随意性和误差量化困难。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在构建Koopman嵌入时存在模型结构和维度选择的随意性，且难以量化误差。本文旨在解决这一问题。

Method: 通过将非线性系统表示为线性和非线性（多项式）块的网络，推导出具有恒定状态和输出矩阵的Koopman模型，输入影响为多项式。若线性块无直通，则简化为双线性模型。

Result: 提出的方法能够生成有限维且精确的Koopman嵌入，避免了传统方法的随意性和误差问题。

Conclusion: 本文提供了一种系统化的方法，为非线性系统生成精确的Koopman嵌入，具有理论和实际应用价值。

Abstract: The challenge of finding exact and finite-dimensional Koopman embeddings of
nonlinear systems has been largely circumvented by employing data-driven
techniques to learn models of different complexities (e.g., linear, bilinear,
input affine). Although these models may provide good accuracy, selecting the
model structure and dimension is still ad-hoc and it is difficult to quantify
the error that is introduced. In contrast to the general trend of data-driven
learning, in this paper, we develop a systematic technique for nonlinear
systems that produces a finite-dimensional and exact embedding. If the
nonlinear system is represented as a network of series and parallel linear and
nonlinear (polynomial) blocks, one can derive an associated Koopman model that
has constant state and output matrices and the input influence is polynomial.
Furthermore, if the linear blocks do not have feedthrough, the Koopman
representation simplifies to a bilinear model.

</details>


### [207] [Adaptive Network Security Policies via Belief Aggregation and Rollout](https://arxiv.org/abs/2507.15163)
*Kim Hammar,Yuchao Li,Tansu Alpcan,Emil C. Lupu,Dimitri Bertsekas*

Main category: eess.SY

TL;DR: 提出了一种基于强化学习的网络安全策略更新方法，具有可扩展性、理论保证和快速适应能力。


<details>
  <summary>Details</summary>
Motivation: 网络安全漏洞和操作条件的变化需要频繁更新安全策略，现有方法缺乏性能保证且适应速度慢。

Method: 结合粒子滤波的信念估计、基于聚合的离线策略计算和基于rollout的在线策略适应，提出了一种新的特征聚合技术。

Result: 在模拟和测试中，该方法在多个基准（如CAGE-2）上优于现有方法。

Conclusion: 该方法在网络安全策略更新中表现出高效性和适应性。

Abstract: Evolving security vulnerabilities and shifting operational conditions require
frequent updates to network security policies. These updates include
adjustments to incident response procedures and modifications to access
controls, among others. Reinforcement learning methods have been proposed for
automating such policy adaptations, but most of the methods in the research
literature lack performance guarantees and adapt slowly to changes. In this
paper, we address these limitations and present a method for computing security
policies that is scalable, offers theoretical guarantees, and adapts quickly to
changes. It assumes a model or simulator of the system and comprises three
components: belief estimation through particle filtering, offline policy
computation through aggregation, and online policy adaptation through rollout.
Central to our method is a new feature-based aggregation technique, which
improves scalability and flexibility. We analyze the approximation error of
aggregation and show that rollout efficiently adapts policies to changes under
certain conditions. Simulations and testbed results demonstrate that our method
outperforms state-of-the-art methods on several benchmarks, including CAGE-2.

</details>


### [208] [A New Ultrafast Printer for Large-Scale Assembly of Piezoelectric Biomaterials](https://arxiv.org/abs/2507.15167)
*Nan An,Mingtong Chen,Zhengbao Yang*

Main category: eess.SY

TL;DR: 提出了一种模块化、快速、大面积的生物压电薄膜制备技术，基于导电尖刺金属盘的高电压电场形成锥射流模式，结合纳米限域自组装和原位极化效应。


<details>
  <summary>Details</summary>
Motivation: 解决生物压电薄膜制备中速度慢、面积小的问题，实现高效、灵活的薄膜制备。

Method: 利用导电尖刺金属盘的高电压电场形成锥射流模式，结合纳米限域自组装和原位极化效应。

Result: 打印速度高达9.2×10⁹ μm³/s，模块化设计理论上可实现无限打印效率，灵活适应不同形状和面积的薄膜制备。

Conclusion: MLSP技术展示了生物压电材料的超快速、大规模组装能力，具有作为通用生物器件制备生物压电薄膜的良好潜力。

Abstract: We propose a modular, fast and large-area fabrication of bio-piezoelectric
films. The technique is based on the formation of cone-jet mode by applying a
high voltage electric field to conductive spiked metal disks. And the
self-assembly process of biomolecular materials through nanoconfinement with
in-situ poling effect. This job achieved print speeds of up to 9.2 109 um3/s
with a combination of only 2 printheads. At the same time, the modular design
allows the MLSP to achieve theoretically unlimited print efficiency. It also
provides flexible configuration options for different printing needs, such as
preparing films of different areas and shapes. In short, MLSP demonstrates the
ability of piezoelectric biomaterials to undergo ultra-fast, large-scale
assembly. Demonstrates good potential as a universally applicable bio-device
for the fabrication of bio-piezoelectric films

</details>


### [209] [Energy consumption optimization and self-powered environmental monitoring design for low-carbon smart buildings](https://arxiv.org/abs/2507.15169)
*Yuhan Dai,Mingtong Chen,Zhengbao Yang*

Main category: eess.SY

TL;DR: 结合BIM和太阳能技术的优化设计显著降低建筑能耗，实现自供电环境监测。


<details>
  <summary>Details</summary>
Motivation: 智能建筑在可持续城市发展中至关重要，但设计、材料选择和用户行为导致能源效率低下，BIM与太阳能技术的结合潜力尚未充分探索。

Method: 通过BIM能源模拟、材料优化和太阳能技术集成，对广东一所小学进行案例分析。

Result: 年能耗降低40.68%，照明能耗减少36.59%，屋顶光伏系统回收期为7.46年，并实现环境传感器的自供电。

Conclusion: BIM与太阳能技术的结合可将传统建筑改造为高效、自持的智能结构，未来研究可扩展至不同气候和建筑类型。

Abstract: Despite the growing emphasis on intelligent buildings as a cornerstone of
sustainable urban development, significant energy inefficiencies persist due to
suboptimal design, material choices, and user behavior. The applicability of
integrated Building Information Modeling (BIM) and solarpowered environmental
monitoring systems for energy optimization in low-carbon smart buildings
remains underexplored. Can BIM-driven design improvements, combined with
photovoltaic systems, achieve substantial energy savings while enabling
self-powered environmental monitoring? This study conducts a case analysis on a
retrofitted primary school building in Guangdong, China, utilizing BIM-based
energy simulations, material optimization, and solar technology integration.
The outcomes reveal that the proposed approach reduced annual energy
consumption by 40.68%, with lighting energy use decreasing by 36.59%. A rooftop
photovoltaic system demonstrated a payback period of 7.46 years while powering
environmental sensors autonomously. Hardware system integrates sensors and an
ARDUINO-based controller to detect environmental factors like rainfall,
temperature, and air quality. It is powered by a 6W solar panel and a 2200
mAh/7.4 V lithium battery to ensure stable operation. This study underscores
the potential of BIM and solar energy integration to transform traditional
buildings into energy-efficient, self-sustaining smart structures. Further
research can expand the scalability of these methods across diverse climates
and building typologies.

</details>


### [210] [Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies](https://arxiv.org/abs/2507.15259)
*Kyung-Bin Kwon,Sayak Mukherjee,Ramij R. Hossain,Marcelo Elizondo*

Main category: eess.SY

TL;DR: 提出了一种基于物理信息的神经ODE框架（PI-LNM），用于模拟逆变器的专有动态特性，以提高电网动态仿真的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前行业中，逆变器的内部控制和参数通常不公开，导致动态仿真和相关研究（如稳定性分析和控制增益调整）难以准确进行。

Method: 结合系统物理和神经网络学习层，提出PI-LNM模型，捕捉专有单元的未建模行为。

Result: 通过电网形成逆变器（GFM）案例验证，证明该方法比纯数据驱动学习更准确。

Conclusion: PI-LNM框架显著提高了动态仿真的准确性，为专有单元建模提供了新思路。

Abstract: This letter develops a novel physics-informed neural ordinary differential
equations-based framework to emulate the proprietary dynamics of the inverters
-- essential for improved accuracy in grid dynamic simulations. In current
industry practice, the original equipment manufacturers (OEMs) often do not
disclose the exact internal controls and parameters of the inverters, posing
significant challenges in performing accurate dynamic simulations and other
relevant studies, such as gain tunings for stability analysis and controls. To
address this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)
that integrates system physics with neural learning layers to capture the
unmodeled behaviors of proprietary units. The proposed method is validated
using a grid-forming inverter (GFM) case study, demonstrating improved dynamic
simulation accuracy over approaches that rely solely on data-driven learning
without physics-based guidance.

</details>


### [211] [Dual-Channel Adaptive NMPC for Quadrotor under Instantaneous Impact and Payload Disturbances](https://arxiv.org/abs/2507.15261)
*Xinqi Chen,Xiuxian Li,Min Meng*

Main category: eess.SY

TL;DR: 提出了一种新型控制架构DCA-NMPC，用于解决四旋翼无人机在捕获具有一定质量物体时的瞬时接触力和负载不确定性带来的控制挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注轻量物体捕获，而具有一定质量的物体捕获会带来瞬时接触力和负载不确定性，对四旋翼控制构成挑战。

Method: 提出DCA-NMPC架构，将非线性模型预测控制与两个底层模型参考自适应控制器级联，以抵抗剧烈冲击并适应不确定惯性参数。

Result: 通过数值仿真实验验证了方法的有效性。

Conclusion: DCA-NMPC能够有效应对捕获过程中的瞬时冲击和负载不确定性。

Abstract: Capturing target objects using the quadrotor has gained increasing popularity
in recent years, but most studies focus on capturing lightweight objects. The
instantaneous contact force generated when capturing objects of a certain mass,
along with the payload uncertainty after attachment, will pose significant
challenges to the quadrotor control. This paper proposes a novel control
architecture, namely Dual-Channel Adaptive Nonlinear Model Predictive Control
(DCA-NMPC), which cascades a nonlinear model predictive control with two
lower-level model reference adaptive controllers and can resist drastic impact
and adapt to uncertain inertial parameters. Numerical simulation experiments
are performed for validation.

</details>


### [212] [Joint Optimisation of Electric Vehicle Routing and Scheduling: A Deep Learning-Driven Approach for Dynamic Fleet Sizes](https://arxiv.org/abs/2507.15307)
*Jun Kang Yap,Vishnu Monn Baskaran,Wen Shan Tan,Ze Yang Ding,Hao Wang,David L. Dowe*

Main category: eess.SY

TL;DR: 论文提出了一种深度学习辅助的方法，用于优化电动汽车（EV）的联合路由和调度问题，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 电动汽车作为移动储能系统可以为电网提供支持，但现有的混合整数规划（MIP）方法在时间敏感任务中难以解决。

Method: 使用卷积神经网络预测二进制变量，减少解空间，并通过填充机制处理输入输出尺寸变化。

Result: 在IEEE 33-bus系统和Nguyen-Dupuis交通网络中，运行时间减少了97.8%，同时保持了99.5%的可行性和接近最优解的精度。

Conclusion: 深度学习辅助方法显著提高了EV调度问题的计算效率，且无需重新训练即可适应不同规模的输入。

Abstract: Electric Vehicles (EVs) are becoming increasingly prevalent nowadays, with
studies highlighting their potential as mobile energy storage systems to
provide grid support. Realising this potential requires effective charging
coordination, which are often formulated as mixed-integer programming (MIP)
problems. However, MIP problems are NP-hard and often intractable when applied
to time-sensitive tasks. To address this limitation, we propose a deep learning
assisted approach for optimising a day-ahead EV joint routing and scheduling
problem with varying number of EVs. This problem simultaneously optimises EV
routing, charging, discharging and generator scheduling within a distribution
network with renewable energy sources. A convolutional neural network is
trained to predict the binary variables, thereby reducing the solution search
space and enabling solvers to determine the remaining variables more
efficiently. Additionally, a padding mechanism is included to handle the
changes in input and output sizes caused by varying number of EVs, thus
eliminating the need for re-training. In a case study on the IEEE 33-bus system
and Nguyen-Dupuis transportation network, our approach reduced runtime by 97.8%
when compared to an unassisted MIP solver, while retaining 99.5% feasibility
and deviating less than 0.01% from the optimal solution.

</details>


### [213] [RoCoF Constrained Regional Inertia Security Region: Formulation and Application](https://arxiv.org/abs/2507.15344)
*Jiahao Liu,Cheng Wang,Tianshu Bi*

Main category: eess.SY

TL;DR: 本文研究了可再生能源渗透电力系统中区域惯性安全问题，提出了非线性非凸的区域惯性安全区域（R-ISR）定义，并通过局部线性化方法计算全局最大RoCoF，最终将凸化的R-ISR约束嵌入惯性优化调整模型。


<details>
  <summary>Details</summary>
Motivation: 在可再生能源渗透的电力系统中，区域惯性对频率变化率（RoCoF）的影响至关重要。传统线性化方法无法准确描述其非线性关系，因此需要更全面的研究。

Method: 定义了非线性非凸的R-ISR，采用局部线性化方法计算全局最大RoCoF，并通过搜索方法分解非凸边界为多个简单边界，最终形成凸化的R-ISR约束。

Result: 在3区域系统中的结果显示了一些反直觉现象，例如增加某一区域的惯性可能恶化其RoCoF。

Conclusion: 本文提出的方法能有效解决区域惯性安全问题，为电力系统优化提供了新思路。

Abstract: The regional inertia, which determines the regional rate of change of
frequency (RoCoF), should be kept in a secure status in renewable-penetrated
power systems. To break away from mapping the regional maximum RoCoF with
regional inertia in a linearized form, this paper comprehensively studies the
regional inertia security problem from formulation to applications. Firstly,
the regional inertia security region (R-ISR) is defined, whose boundary is
non-linear and non-convex. Then, a local linearized method is devised to
calculate the global maximum of regional RoCoF. The non-convex ISR boundary is
expressed by multiple simple boundaries corresponding to each local solution,
which can be obtained by a simple search-based method. Finally, the convexified
R-ISR constraint is formed by convex decomposition and embedded in an inertia
optimal adjustment model. The results on a 3-region system show some
counter-intuitive findings, such as increasing the inertia of one region may
worsen its RoCoF.

</details>


### [214] [Revisiting the Effect of Grid-Following Converter on Frequency Dynamics -- Part I: Center of Inertia](https://arxiv.org/abs/2507.15358)
*Jiahao Liu,Cheng Wang,Tianshu Bi*

Main category: eess.SY

TL;DR: 该论文研究了电网跟随（GFL）转换器对系统频率动态的影响，揭示了GFL通过虚拟连接支持中心惯性（COI）频率的机制。


<details>
  <summary>Details</summary>
Motivation: 理解GFL转换器对系统频率动态的影响，尤其是从COI和频率空间变化的角度。

Method: 开发了一个包含GFL本地动态的多发电机模型，并通过虚拟连接与同步发电机交互，最终聚合到COI框架中。

Result: 模拟验证了模型的准确性，表明GFL对COI频率的影响较弱，但其等效惯性和其他组件仍显著影响频率动态。

Conclusion: GFL的等效惯性和其他组件对COI频率动态具有时间可变和可调节的影响。

Abstract: Understanding the impact of grid-following (GFL) converters on system
frequency dynamics is crucial, from both the center of inertia (COI) and
frequency spatial variation perspectives. Part I of this series clarifies the
mechanisms by which GFLs influence COI frequency dynamics. A multi-generator
model of the power system with GFLs is developed, incorporating the local
dynamics of GFLs and their interaction with synchronous generators via virtual
tie lines. By aggregating the multi-generator model into the COI frame, the
interaction between the COI frequency and the equivalent frequency of GFLs is
revealed. The equivalent inertia and other components at the GFL side,
determined by control parameters and operating conditions, support the COI
through virtual tying power. Simulation validates the accuracy of the proposed
modeling and demonstrates that the impact of GFLs on COI frequency is
relatively weak. The equivalent inertia and other components of GFLs still
significantly influence COI frequency dynamics, with their effects being both
time-variable and adjustable.

</details>


### [215] [Revisiting the Effect of Grid-Following Converter on Frequency Dynamics -- Part II: Spatial Variation](https://arxiv.org/abs/2507.15362)
*Jiahao Liu,Cheng Wang,Tianshu Bi*

Main category: eess.SY

TL;DR: 本文研究了电网跟随（GFL）变流器对电力系统空间频率变化的影响，提出了扩展频率分配（FD）公式，并验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨GFL变流器对电力系统频率空间变化的影响，填补现有研究的空白。

Method: 利用接口状态变量和等效频率，提出扩展FD公式，建立节点频率与同步发电机转子频率及GFL等效频率的线性映射关系。

Result: 仿真验证了扩展FD公式的准确性，表明GFL对节点频率的叠加贡献较弱且系数时变，支路频率叠加模式复杂且不同。

Conclusion: 扩展FD公式有效量化了GFL对频率空间变化的影响，为电力系统频率分析提供了新工具。

Abstract: Besides the center of inertia (COI) frequency dynamics addressed in Part I,
the spatial frequency variation in power systems with grid-following (GFL)
converters is also crucial. Part II revisits the effect of GFLs on frequency
spatial variation. Leveraging the interfacing state variables and equivalent
frequency defined in Part I, an extended frequency divider (FD) formula is
proposed. The linearized mapping relationship between network node frequency
and synchronous generator (SG) rotor frequency, as well as GFL equivalent
frequency, is modeled. The superposition contribution from GFLs is determined
by the electrical distance between the generator and the frequency observation
node, as well as the system power flow conditions. Additionally, the frequency
mapping for branch currents, which is overlooked in previous research, is
addressed. Simulation results validate the accuracy of the proposed extended FD
formula. They quantitatively demonstrate that the superposition contribution of
GFLs to node frequency is relatively weak and that the superposition
coefficient is time-varying. The branch frequency superposition reveals a
complex and distinctly different pattern.

</details>


### [216] [Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers](https://arxiv.org/abs/2507.15385)
*Jun Kang Yap,Vishnu Monn Baskaran,Wen Shan Tan,Ze Yang Ding,Hao Wang,David L. Dowe*

Main category: eess.SY

TL;DR: 论文探讨了利用电动汽车（EVs）作为移动储能单元解决可再生能源间歇性问题，提出了一种基于MIP的联合路由与调度问题，并通过深度学习模型简化求解过程。


<details>
  <summary>Details</summary>
Motivation: 可再生能源的间歇性和不确定性给电力系统带来挑战，移动储能系统（如EVs）因其灵活性成为解决方案，但高投资成本限制了其广泛应用。

Method: 提出了一种基于混合整数规划（MIP）的电动汽车联合路由与调度问题，并利用基于Transformer的深度学习模型预测最优解，以简化求解过程。

Result: 在IEEE 33-bus系统和Nguyen-Dupuis交通网络上进行仿真，验证了方法的有效性。

Conclusion: 该方法能够灵活适应不同规模的电动汽车车队，且避免了频繁重新训练的计算开销，为移动储能系统的实际应用提供了可行方案。

Abstract: The growing integration of renewable energy sources in modern power systems
has introduced significant operational challenges due to their intermittent and
uncertain outputs. In recent years, mobile energy storage systems (ESSs) have
emerged as a popular flexible resource for mitigating these challenges.
Compared to stationary ESSs, mobile ESSs offer additional spatial flexibility,
enabling cost-effective energy delivery through the transportation network.
However, the widespread deployment of mobile ESSs is often hindered by the high
investment cost, which has motivated researchers to investigate utilising more
readily available alternatives, such as electric vehicles (EVs) as mobile
energy storage units instead. Hence, we explore this opportunity with a
MIP-based day-ahead electric vehicle joint routing and scheduling problem in
this work. However, solving the problem in a practical setting can often be
computationally intractable since the existence of binary variables makes it
combinatorial challenging. Therefore, we proposed to simplify the problem's
solution process for a MIP solver by pruning the solution search space with a
transformer-based deep learning (DL) model. This is done by training the model
to rapidly predict the optimal binary solutions. In addition, unlike many
existing DL approaches that assume fixed problem structures, the proposed model
is designed to accommodate problems with EV fleets of any sizes. This
flexibility is essential since frequent re-training can introduce significant
computational overhead. We evaluated the approach with simulations on the IEEE
33-bus system coupled with the Nguyen-Dupuis transportation network.

</details>


### [217] [Scaled Relative Graph Analysis of General Interconnections of SISO Nonlinear Systems](https://arxiv.org/abs/2507.15564)
*Julius P. J. Krebbekx,Roland Tóth,Amritam Das*

Main category: eess.SY

TL;DR: 论文提出了一种改进的SRG方法，解决了现有非线性系统分析的局限性，并结合Nyquist准则，扩展了其应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有SRG方法在分析实际非线性系统时存在局限性，需要改进以提升适用性。

Method: 通过重新定义线性时不变算子的SRG，并将其与Nyquist准则结合，提出新定理。

Result: 新方法可用于评估非线性动态系统的稳定性和L2增益性能，并推广了经典圆判据。

Conclusion: 改进的SRG方法在多个示例中展示了其强大的分析能力。

Abstract: Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain
method for the analysis of nonlinear systems. However, we show that the current
SRG analysis suffers from a pitfall that limits its applicability in analyzing
practical nonlinear systems. We overcome this pitfall by introducing a novel
reformulation of the SRG of a linear time-invariant operator and combining the
SRG with the Nyquist criterion. The result is a theorem that can be used to
assess stability and $L_2$-gain performance for general interconnections of
nonlinear dynamic systems. We provide practical calculation results for
canonical interconnections and apply our result to Lur'e systems to obtain a
generalization of the celebrated circle criterion, which deals with broader
class of nonlinearities, and we derive (incremental) $L_2$-gain performance
bounds. We illustrate the power of the new approach on the analysis of several
examples.

</details>


### [218] [Improving Functional Reliability of Near-Field Monitoring for Emergency Braking in Autonomous Vehicles](https://arxiv.org/abs/2507.15594)
*Junnan Pan,Prodromos Sotiriadis,Vladislav Nenchev,Ferdinand Englberger*

Main category: eess.SY

TL;DR: 论文提出三种基于动态空间属性、相关物体尺寸和运动感知预测的监测策略，显著提高了近场监测系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要可靠的危险检测，但主要传感器系统可能遗漏近场障碍物，导致安全风险。

Method: 提出三种监测策略：动态空间属性、相关物体尺寸和运动感知预测，并在验证模拟中对比初始策略。

Result: 实验结果表明，提出的策略显著提高了近场监测系统的可靠性。

Conclusion: 三种改进策略有效减少了误报，提升了近场监测系统的性能。

Abstract: Autonomous vehicles require reliable hazard detection. However, primary
sensor systems may miss near-field obstacles, resulting in safety risks.
Although a dedicated fast-reacting near-field monitoring system can mitigate
this, it typically suffers from false positives. To mitigate these, in this
paper, we introduce three monitoring strategies based on dynamic spatial
properties, relevant object sizes, and motion-aware prediction. In experiments
in a validated simulation, we compare the initial monitoring strategy against
the proposed improvements. The results demonstrate that the proposed strategies
can significantly improve the reliability of near-field monitoring systems.

</details>


### [219] [Reliability-Based Fault Analysis and Modeling of Satellite Electrical Power Subsystems Using Fault Tree and Simulation Tools](https://arxiv.org/abs/2507.15708)
*Niloofar Nobahari,Alireza Rezaee*

Main category: eess.SY

TL;DR: 论文通过MATLAB模拟卫星电力系统的健康与故障状态，并使用Windchill软件绘制故障树分析可靠性，最终实现了0.999的任务保障率。


<details>
  <summary>Details</summary>
Motivation: 卫星电力系统的故障可能导致任务失败，因此计算其可靠性对改进设计至关重要。

Method: 使用MATLAB模拟电力系统的健康与故障状态，并通过Windchill软件绘制故障树分析可靠性。

Result: 实现了0.999的高任务保障率，表明系统可靠性高。

Conclusion: 模拟和分析方法有效提升了卫星电力系统的可靠性设计。

Abstract: One of the most important satellite subsystems is its electric power
subsystem. The occurrence of a fault in the satellite power system causes the
failure of all or part of the satellite. Calculating the overall reliability of
the power system before the mission is crucial in improving the design of the
satellite power system. Each component of the power system may malfunction due
to pressure, launch pressure, and operating conditions. Accordingly, in this
paper, first, a healthy and faulty system for the components of the electrical
power system is simulated with MATLAB. Finally, by drawing a fault tree to
analyze the reliability of the power subsystem, overall mission reliability,
power system fault rate, and overall fault rate of the mission are calculated
by Windchill software. Finally, a total mission assurance of 0.999 was
achieved, indicating the high reliability of the simulated system.

</details>


### [220] [Density control of multi-agent swarms via bio-inspired leader-follower plasticity](https://arxiv.org/abs/2507.15781)
*Gian Carlo Maffettone,Alain Boldini,Mario di Bernardo,Maurizio Porfiri*

Main category: eess.SY

TL;DR: 提出了一种基于生物启发的领导者-跟随者控制方法，用于解决移动代理的空间自组织问题，适用于大规模群体，并考虑了能量约束。


<details>
  <summary>Details</summary>
Motivation: 移动代理的空间自组织控制在多个工程领域（如群体机器人和合成生物学）中是一个开放性问题。

Method: 采用生物启发的领导者-跟随者策略，通过非线性偏微分方程建模群体密度控制问题，避免了维度灾难。

Result: 推导了一维和高维问题中稳态解的存在性及局部稳定性的解析保证，并通过数值验证了方法的有效性、鲁棒性和通用性。

Conclusion: 提出的生物启发控制策略在理论和数值上均表现出色，适用于大规模群体控制。

Abstract: The design of control systems for the spatial self-organization of mobile
agents is an open challenge across several engineering domains, including swarm
robotics and synthetic biology. Here, we propose a bio-inspired leader-follower
solution, which is aware of energy constraints of mobile agents and is apt to
deal with large swarms. Akin to many natural systems, control objectives are
formulated for the entire collective, and leaders and followers are allowed to
plastically switch their role in time. We frame a density control problem,
modeling the agents' population via a system of nonlinear partial differential
equations. This approach allows for a compact description that inherently
avoids the curse of dimensionality and improves analytical tractability. We
derive analytical guarantees for the existence of desired steady-state
solutions and their local stability for one-dimensional and higher-dimensional
problems. We numerically validate our control methodology, offering support to
the effectiveness, robustness, and versatility of our proposed bio-inspired
control strategy.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [221] [Can AR-Embedded Visualizations Foster Appropriate Reliance on AI in Spatial Decision Making? A Comparative Study of AR See-Through vs. 2D Minimap](https://arxiv.org/abs/2507.14316)
*Xianhao Carton Liu,Difan Jia,Tongyu Nie,Evan Suma Rosenberg,Victoria Interrante,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: 研究比较了AR嵌入式可视化与2D小地图在AI辅助空间目标选择任务中的表现，发现AR反而导致用户对AI的不当依赖增加，但AR在空间推理方面有优势。


<details>
  <summary>Details</summary>
Motivation: 在紧急疏散等高风险、时间紧迫的场景中，决策者需快速选择空间目标，但传统2D地图会增加认知负荷，可能导致对AI建议的不当依赖。AR嵌入式可视化可能缓解这一问题。

Method: 通过实证研究（N=32），比较AR嵌入式可视化与2D小地图在AI辅助空间目标选择任务中的表现。

Result: 与预期相反，AR条件下用户对AI的不当依赖更高，主要原因是过度依赖和AR特有的视觉挑战。但AR在空间推理方面表现更优。

Conclusion: AR嵌入式可视化虽在空间推理上有优势，但需解决过度依赖问题。研究为未来AR中的人机决策协作提供了设计启示和研究方向。

Abstract: In high-stakes, time-critical scenarios-such as emergency evacuation, first
responder prioritization, and crisis management -- decision-makers must rapidly
choose among spatial targets, such as exits, individuals to assist, or areas to
secure. Advances in indoor sensing and artificial intelligence (AI) can support
these decisions by visualizing real-time situational data and AI suggestions on
2D maps. However, mentally mapping this information onto real-world spaces
imposes significant cognitive load. This load can impair users' ability to
appropriately judge AI suggestions, leading to inappropriate reliance (e.g.,
accepting wrong AI suggestions or rejecting correct ones). Embedded
visualizations in Augmented Reality (AR), by directly overlaying information
onto physical environments, may reduce this load and foster more deliberate,
appropriate reliance on AI. But is this true? In this work, we conducted an
empirical study (N = 32) comparing AR see-through (embedded visualization) and
2D Minimap in time-critical, AI-assisted spatial target selection tasks.
Contrary to our expectations, users exhibited greater inappropriate reliance on
AI in the AR condition. Our analysis further reveals that this is primarily due
to over-reliance, with factors specific to embedded visualizations, such as
perceptual challenges, visual proximity illusions, and highly realistic visual
representations. Nonetheless, embedded visualizations demonstrated notable
benefits in spatial reasoning, such as spatial mapping and egocentric spatial
imagery. We conclude by discussing the empirical insights, deriving design
implications, and outlining important directions for future research on
human-AI decision collaboration in AR.

</details>


### [222] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: 研究探讨了使用ChatGPT等大型语言模型（LLM）进行结构化演绎定性编码的潜力，测试了四种干预方法，发现逐步任务分解策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注归纳编码，而LLM在演绎分类任务中的潜力尚未充分探索。

Method: 使用CAP主代码本对最高法院案例摘要进行分类，测试了零样本、少样本、基于定义和逐步任务分解四种方法。

Result: 逐步任务分解策略表现最佳（准确率0.775，kappa 0.744），分类行为受干预策略显著影响。

Conclusion: 通过定制化干预，LLM可达到适合严格定性编码工作流程的可靠性水平。

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [223] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: 研究探讨了对话AI在技术面试中“大声思考”练习中的作用，通过实验发现参与者重视AI的模拟、反馈和学习功能，并提出了设计建议和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 技术面试中的“大声思考”过程缺乏结构化练习机会，对话AI的潜力尚未充分研究。

Method: 使用基于LLM的技术面试练习工具，对17名参与者进行实验。

Result: 参与者认可AI在模拟、反馈和学习中的作用，提出了设计建议和未来研究方向。

Conclusion: AI在技术面试练习中具有潜力，需结合人机协作，促进公平学习。

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [224] [Conch: Competitive Debate Analysis via Visualizing Clash Points and Hierarchical Strategies](https://arxiv.org/abs/2507.14482)
*Qianhe Chen,Yong Wang,Yixin Yu,Xiyuan Zhu,Xuerou Yu,Ran Wang*

Main category: cs.HC

TL;DR: Conch是一个交互式可视化系统，用于系统分析辩论内容和方式，通过并行螺旋可视化追踪辩论中的多维冲突点和参与者互动，并利用大语言模型自动识别关键辩论元素。


<details>
  <summary>Details</summary>
Motivation: 手动分析非结构化和未标记的辩论文本记录耗时且低效，难以重建上下文语义和逻辑联系。

Method: 提出Conch系统，采用并行螺旋可视化追踪辩论过程的多维冲突点和互动，并利用大语言模型自动识别关键辩论元素。

Result: 通过案例研究和用户研究验证了Conch在竞争性辩论分析中的有效性和可用性。

Conclusion: Conch为辩论分析提供了一种高效且直观的工具，帮助参与者全面理解辩论内容。

Abstract: In-depth analysis of competitive debates is essential for participants to
develop argumentative skills and refine strategies, and further improve their
debating performance. However, manual analysis of unstructured and unlabeled
textual records of debating is time-consuming and ineffective, as it is
challenging to reconstruct contextual semantics and track logical connections
from raw data. To address this, we propose Conch, an interactive visualization
system that systematically analyzes both what is debated and how it is debated.
In particular, we propose a novel parallel spiral visualization that compactly
traces the multidimensional evolution of clash points and participant
interactions throughout debate process. In addition, we leverage large language
models with well-designed prompts to automatically identify critical debate
elements such as clash points, disagreements, viewpoints, and strategies,
enabling participants to understand the debate context comprehensively.
Finally, through two case studies on real-world debates and a
carefully-designed user study, we demonstrate Conch's effectiveness and
usability for competitive debate analysis.

</details>


### [225] ["It looks sexy but it's wrong." Tensions in creativity and accuracy using genAI for biomedical visualization](https://arxiv.org/abs/2507.14494)
*Roxanne Ziman,Shehryar Saharan,Gaël McGill,Laura Garrison*

Main category: cs.HC

TL;DR: 分析了生成式AI在生物医学可视化中的使用及其引发的信任与准确性挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在生物医学可视化中的潜在问题及其对科学传播的影响。

Method: 通过17次访谈，定性分析从业者和研究人员对生成式AI的态度和使用情况。

Result: 发现专家对生成式AI的态度各异，从积极采用到谨慎避免，并强调人类干预的必要性。

Conclusion: 生成式AI在生物医学可视化中需谨慎使用，以确保科学视觉的准确性和可信度。

Abstract: We contribute an in-depth analysis of the workflows and tensions arising from
generative AI (genAI) use in biomedical visualization (BioMedVis). Although
genAI affords facile production of aesthetic visuals for biological and medical
content, the architecture of these tools fundamentally limits the accuracy and
trustworthiness of the depicted information, from imaginary (or fanciful)
molecules to alien anatomy. Through 17 interviews with a diverse group of
practitioners and researchers, we qualitatively analyze the concerns and values
driving genAI (dis)use for the visual representation of spatially-oriented
biomedical data. We find that BioMedVis experts, both in roles as developers
and designers, use genAI tools at different stages of their daily workflows and
hold attitudes ranging from enthusiastic adopters to skeptical avoiders of
genAI. In contrasting the current use and perspectives on genAI observed in our
study with predictions towards genAI in the visualization pipeline from prior
work, our refocus the discussion of genAI's effects on projects in
visualization in the here and now with its respective opportunities and
pitfalls for future visualization research. At a time when public trust in
science is in jeopardy, we are reminded to first do no harm, not just in
biomedical visualization but in science communication more broadly. Our
observations reaffirm the necessity of human intervention for empathetic design
and assessment of accurate scientific visuals.

</details>


### [226] [PaperBridge: Crafting Research Narratives through Human-AI Co-Exploration](https://arxiv.org/abs/2507.14527)
*Runhua Zhang,Yang Ouyang,Leixian Shen,Yuying Tang,Xiaojuan Ma,Huamin Qu,Xian Xu*

Main category: cs.HC

TL;DR: PaperBridge是一个人类与AI协同探索系统，帮助研究者将出版物组织成连贯的叙事，支持多样化的学术沟通需求。


<details>
  <summary>Details</summary>
Motivation: 研究者需要将跨领域、多方法论的出版物组织成连贯叙事，这在HCI等跨学科领域尤为困难。

Method: PaperBridge采用双向分析引擎（基于大语言模型），支持自上而下的用户意图和自下而上的叙事组件细化。

Result: 用户研究（N=12）验证了PaperBridge的可用性和有效性，能帮助探索替代性研究叙事。

Conclusion: 研究为交互系统如何支持学术沟通任务提供了实证见解。

Abstract: Researchers frequently need to synthesize their own publications into
coherent narratives that demonstrate their scholarly contributions. To suit
diverse communication contexts, exploring alternative ways to organize one's
work while maintaining coherence is particularly challenging, especially in
interdisciplinary fields like HCI where individual researchers' publications
may span diverse domains and methodologies. In this paper, we present
PaperBridge, a human-AI co-exploration system informed by a formative study and
content analysis. PaperBridge assists researchers in exploring diverse
perspectives for organizing their publications into coherent narratives. At its
core is a bi-directional analysis engine powered by large language models,
supporting iterative exploration through both top-down user intent (e.g.,
determining organization structure) and bottom-up refinement on narrative
components (e.g., thematic paper groupings). Our user study (N=12) demonstrated
PaperBridge's usability and effectiveness in facilitating the exploration of
alternative research narratives. Our findings also provided empirical insights
into how interactive systems can scaffold academic communication tasks.

</details>


### [227] [Uncovering the EEG Temporal Representation of Low-dimensional Object Properties](https://arxiv.org/abs/2507.14537)
*Jiahua Tang,Song Wang,Jiachen Zou,Chen Wei,Quanying Liu*

Main category: cs.HC

TL;DR: 论文提出了一种新方法，利用EEG信号研究低维物体属性的时间编码，填补了EEG在神经表征动态研究中的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管EEG具有毫秒级时间分辨率，但其在神经表征动态研究中的应用仍不足，主要受限于低信噪比和复杂的时空耦合特性。

Method: 整合先进的神经解码算法，系统研究EEG信号中低维物体属性的时间编码，首次尝试识别概念在时间分布中的特异性与原型特征。

Result: 框架不仅提升了神经表征的可解释性，还为脑机接口中的视觉解码提供了新见解。

Conclusion: 该方法为EEG在神经表征动态研究中的应用开辟了新途径，具有重要的理论和应用价值。

Abstract: Understanding how the human brain encodes and processes external visual
stimuli has been a fundamental challenge in neuroscience. With advancements in
artificial intelligence, sophisticated visual decoding architectures have
achieved remarkable success in fMRI research, enabling more precise and
fine-grained spatial concept localization. This has provided new tools for
exploring the spatial representation of concepts in the brain. However, despite
the millisecond-scale temporal resolution of EEG, which offers unparalleled
advantages in tracking the dynamic evolution of cognitive processes, the
temporal dynamics of neural representations based on EEG remain underexplored.
This is primarily due to EEG's inherently low signal-to-noise ratio and its
complex spatiotemporal coupling characteristics. To bridge this research gap,
we propose a novel approach that integrates advanced neural decoding algorithms
to systematically investigate how low-dimensional object properties are
temporally encoded in EEG signals. We are the first to attempt to identify the
specificity and prototypical temporal characteristics of concepts within
temporal distributions. Our framework not only enhances the interpretability of
neural representations but also provides new insights into visual decoding in
brain-computer interfaces (BCI).

</details>


### [228] [EventBox: A Novel Visual Encoding for Interactive Analysis of Temporal and Multivariate Attributes in Event Sequences](https://arxiv.org/abs/2507.14685)
*Luis Montana,Jessica Magallanes,Miguel Juarez,Suzanne Mason,Andrew Narracott,Lindsey van Gemeren,Steven Wood,Maria-Cruz Villa-Uriol*

Main category: cs.HC

TL;DR: EventBox是一种新颖的数据表示和视觉编码方法，用于分析事件组及其多变量属性，集成到Sequen-C系统中，通过用户驱动的转换和自动统计分析增强分析深度。


<details>
  <summary>Details</summary>
Motivation: 快速增长的事件序列数据需要有效的分析和探索方法，现有方法常忽略时间和多变量属性的交互作用。

Method: 提出EventBox数据表示和视觉编码方法，集成到Sequen-C系统中，支持用户驱动的转换（对齐、排序、替换、聚合）和自动统计分析。

Result: 通过21名参与者评估（包括专家和新手），使用ICE-T框架验证可视化价值，并通过真实医疗数据案例展示其有效性。

Conclusion: EventBox为探索事件序列中的时间和多变量属性提供了灵活解决方案，推动了视觉分析的发展。

Abstract: The rapid growth and availability of event sequence data across domains
requires effective analysis and exploration methods to facilitate
decision-making. Visual analytics combines computational techniques with
interactive visualizations, enabling the identification of patterns, anomalies,
and attribute interactions. However, existing approaches frequently overlook
the interplay between temporal and multivariate attributes. We introduce
EventBox, a novel data representation and visual encoding approach for
analyzing groups of events and their multivariate attributes. We have
integrated EventBox into Sequen-C, a visual analytics system for the analysis
of event sequences. To enable the agile creation of EventBoxes in Sequen-C, we
have added user-driven transformations, including alignment, sorting,
substitution and aggregation. To enhance analytical depth, we incorporate
automatically generated statistical analyses, providing additional insight into
the significance of attribute interactions. We evaluated our approach involving
21 participants (3 domain experts, 18 novice data analysts). We used the ICE-T
framework to assess visualization value, user performance metrics completing a
series of tasks, and interactive sessions with domain experts. We also present
three case studies with real-world healthcare data demonstrating how EventBox
and its integration into Sequen-C reveal meaningful patterns, anomalies, and
insights. These results demonstrate that our work advances visual analytics by
providing a flexible solution for exploring temporal and multivariate
attributes in event sequences.

</details>


### [229] [A Notification Based Nudge for Handling Excessive Smartphone Use](https://arxiv.org/abs/2507.14702)
*Partha Sarker,Dipto Dey,Marium-E-Jannat*

Main category: cs.HC

TL;DR: 提出了一种基于通知的干预方法，通过提高用户自我意识来减少智能手机过度使用，避免引起用户反感。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过限制用户使用智能手机来减少过度使用，但用户可能感到困扰或不愿使用这些应用。

Method: 设计了一个Android原型应用“App Usage Monitor”，结合轻推和可视化技术，通过通知提高用户自我意识。

Result: 通过为期3周的实验验证了假设，证明该方法有效。

Conclusion: 基于通知的干预方法能有效减少智能手机过度使用，且不会引起用户反感。

Abstract: Excessive use of smartphones is a worldwide known issue. In this study, we
proposed a notification-based intervention approach to reduce smartphone
overuse without making the user feel any annoyance or irritation. Most of the
work in this field tried to reduce smartphone overuse by making smartphone use
more difficult for the user. In our user study (n = 109), we found that 19.3%
of the participants are unwilling to use any usage-limiting application because
a) they do not want their smartphone activities to get restricted or b) those
applications are annoying. Following that, we devised a hypothesis to minimize
smartphone usage among undergraduates. Finally, we designed a prototype for
Android, "App Usage Monitor," and conducted a 3-week experiment through which
we found proof of concept for our hypothesis. In our prototype, we combined
techniques such as nudge and visualization to increase self-awareness among the
user by leveraging notifications.

</details>


### [230] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct是一个可视化分析框架，支持在子群体中模拟、解释和推理个体层面的干预效果，解决了现有方法在异质性系统中群体层面分析的不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要关注群体层面的效果，难以应对异质性系统中干预效果的广泛差异。

Method: 提出XplainAct框架，通过两个案例研究（阿片类药物相关死亡和总统选举投票倾向）验证其有效性。

Result: XplainAct在个体层面和子群体中有效模拟和解释了干预效果。

Conclusion: XplainAct为异质性系统中的因果推理提供了更细粒度的分析工具。

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [231] [Task Mode: Dynamic Filtering for Task-Specific Web Navigation using LLMs](https://arxiv.org/abs/2507.14769)
*Ananya Gubbi Mohanbabu,Yotam Sechayk,Amy Pavel*

Main category: cs.HC

TL;DR: Task Mode是一个动态过滤网页内容的系统，利用大语言模型根据用户目标筛选和优先显示相关内容，减少干扰。研究表明，它显著提升了屏幕阅读用户的任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现代网页界面过于复杂，尤其是对屏幕阅读用户（SRUs）造成不便，他们需要花费更多时间浏览无关内容。设计一种能同时满足视觉用户（VUs）和非视觉用户需求的交互方式，以减少可访问性差距。

Method: 提出Task Mode系统，通过大语言模型动态过滤网页内容，保留页面结构并提供多种浏览模式。

Result: 用户研究（12人，6VUs和6SRUs）显示，Task Mode显著缩短了SRUs的任务完成时间，同时保持VUs的表现，将两组完成时间差距从2倍降至1.2倍。11/12参与者希望未来继续使用。

Conclusion: Task Mode展示了如何通过同时设计视觉和非视觉交互方式，减少可访问性差距，为未来人机交互研究提供了新思路。

Abstract: Modern web interfaces are unnecessarily complex to use as they overwhelm
users with excessive text and visuals unrelated to their current goals. This
problem particularly impacts screen reader users (SRUs), who navigate content
sequentially and may spend minutes traversing irrelevant elements before
reaching desired information compared to vision users (VUs) who visually skim
in seconds. We present Task Mode, a system that dynamically filters web content
based on user-specified goals using large language models to identify and
prioritize relevant elements while minimizing distractions. Our approach
preserves page structure while offering multiple viewing modes tailored to
different access needs. Our user study with 12 participants (6 VUs, 6 SRUs)
demonstrates that our approach reduced task completion time for SRUs while
maintaining performance for VUs, decreasing the completion time gap between
groups from 2x to 1.2x. 11 of 12 participants wanted to use Task Mode in the
future, reporting that Task Mode supported completing tasks with less effort
and fewer distractions. This work demonstrates how designing new interactions
simultaneously for visual and non-visual access can reduce rather than
reinforce accessibility disparities in future technology created by
human-computer interaction researchers and practitioners.

</details>


### [232] [SenseSeek Dataset: Multimodal Sensing to Study Information Seeking Behaviors](https://arxiv.org/abs/2507.14792)
*Kaixin Ji,Danula Hettiachchi,Falk Scholer,Flora D. Salim,Damiano Spina*

Main category: cs.HC

TL;DR: SenseSeek数据集通过消费级传感器捕捉信息搜索过程中的生理和行为数据，支持多阶段搜索行为的研究。


<details>
  <summary>Details</summary>
Motivation: 理解复杂信息处理行为需要个性化的数据捕捉，尤其是用户与信息系统的交互方式。

Method: 利用消费级传感器（如可穿戴设备）收集20名参与者在235次搜索试验中的生理和行为数据，包括EDA、EEG、PUPIL等。

Result: 数据集包含258个特征，验证了不同认知意图和交互方式对传感器数据的影响，并能区分搜索阶段。

Conclusion: SenseSeek是首个多传感器生理信号数据集，为未来信息搜索行为研究提供参考。

Abstract: Information processing tasks involve complex cognitive mechanisms that are
shaped by various factors, including individual goals, prior experience, and
system environments. Understanding such behaviors requires a sophisticated and
personalized data capture of how one interacts with modern information systems
(e.g., web search engines). Passive sensors, such as wearables, capturing
physiological and behavioral data, have the potential to provide solutions in
this context. This paper presents a novel dataset, SenseSeek, designed to
evaluate the effectiveness of consumer-grade sensors in a complex information
processing scenario: searching via systems (e.g., search engines), one of the
common strategies users employ for information seeking. The SenseSeek dataset
comprises data collected from 20 participants, 235 trials of the stimulated
search process, 940 phases of stages in the search process, including the
realization of Information Need (IN), Query Formulation (QF), Query Submission
by Typing (QS-T) or Speaking (QS-S), and Relevance Judgment by Reading (RJ-R)
or Listening (RJ-L). The data includes Electrodermal Activities (EDA),
Electroencephalogram (EEG), PUPIL, GAZE, and MOTION data, which were captured
using consumer-grade sensors. It also contains 258 features extracted from the
sensor data, the gaze-annotated screen recordings, and task responses. We
validate the usefulness of the dataset by providing baseline analysis on the
impacts of different cognitive intents and interaction modalities on the sensor
data, and effectiveness of the data in discriminating the search stages. To our
knowledge, SenseSeek is the first dataset that characterizes the multiple
stages involved in information seeking with physiological signals collected
from multiple sensors. We hope this dataset can serve as a reference for future
research on information-seeking behaviors.

</details>


### [233] [Understanding How Visually Impaired Players Socialize in Mobile Games](https://arxiv.org/abs/2507.14818)
*Zihe Ran,Xiyu Li,Qing Xiao,Yanyun Wang,Franklin Mingzhe Li,Zhicong Lu*

Main category: cs.HC

TL;DR: 研究探讨了中国视障玩家如何通过手机游戏社交，发现尽管游戏满足了部分社交需求，但技术障碍和社区分裂仍带来挑战。


<details>
  <summary>Details</summary>
Motivation: 视障人士在现实生活中面临社交困难，手机游戏成为重要社交平台，但游戏设计和可访问性问题影响了他们的体验。

Method: 通过对30名视障玩家的访谈，分析他们在游戏中的社交体验和挑战。

Result: 手机游戏满足了视障玩家的部分社交需求，但技术障碍、可访问性不足和社区分裂限制了参与。

Conclusion: 研究为设计更具包容性和可访问性的手机游戏提供了见解。

Abstract: Mobile games are becoming a vital medium for social interaction, offering a
platform that transcends geographical boundaries. An increasing number of
visually impaired individuals are engaging in mobile gaming to connect,
collaborate, compete, and build friendships. In China, visually impaired
communities face significant social challenges in offline settings, making
mobile games a crucial avenue for socialization. However, the design of mobile
games and their mapping to real-world environments significantly shape their
social gaming experiences. This study explores how visually impaired players in
China navigate socialization and integrate into gaming communities. Through
interviews with 30 visually impaired players, we found that while mobile games
fulfill many of their social needs, technological barriers and insufficient
accessibility features, and internal community divisions present significant
challenges to their participation. This research sheds light on their social
experiences and offers insights for designing more inclusive and accessible
mobile games.

</details>


### [234] [Progressive Sentences: Combining the Benefits of Word and Sentence Learning](https://arxiv.org/abs/2507.14846)
*Nuwan Janaka,Shengdong Zhao,Ashwin Ram,Ruoxin Sun,Sherisse Tan Jing Wen,Danae Li,David Hsu*

Main category: cs.HC

TL;DR: 研究探讨了轻量级AR智能眼镜如何通过渐进式句子展示支持移动第二语言学习，发现该方法在移动场景中提升记忆效果。


<details>
  <summary>Details</summary>
Motivation: 利用AR智能眼镜的多模态信息传递能力，探索其在移动第二语言学习中的潜力，尤其是针对传统词汇学习方法的不足。

Method: 提出“渐进式展示”方法，逐步显示句子成分（主语、动词、宾语），并保留上下文，同时测试定时间隔对学习效果的影响。

Result: 渐进式展示在移动场景（如行走）中显著提升记忆效果，定时间隔进一步优化多任务条件下的学习效率。

Conclusion: 渐进式展示方法有效，为移动学习提供了实用指南，适用于短暂、即时的学习场景。

Abstract: The rapid evolution of lightweight consumer augmented reality (AR) smart
glasses (a.k.a. optical see-through head-mounted displays) offers novel
opportunities for learning, particularly through their unique capability to
deliver multimodal information in just-in-time, micro-learning scenarios. This
research investigates how such devices can support mobile second-language
acquisition by presenting progressive sentence structures in multimodal
formats. In contrast to the commonly used vocabulary (i.e., word) learning
approach for novice learners, we present a "progressive presentation" method
that combines both word and sentence learning by sequentially displaying
sentence components (subject, verb, object) while retaining prior context.
Pilot and formal studies revealed that progressive presentation enhances
recall, particularly in mobile scenarios such as walking. Additionally,
incorporating timed gaps between word presentations further improved learning
effectiveness under multitasking conditions. Our findings demonstrate the
utility of progressive presentation and provide usage guidelines for
educational applications-even during brief, on-the-go learning moments.

</details>


### [235] [Holistic Specification of the Human Digital Twin: Stakeholders, Users, Functionalities, and Applications](https://arxiv.org/abs/2507.14859)
*Nils Mandischer,Alexander Atanasyan,Ulrich Dahmen,Michael Schluse,Jürgen Rossmann,Lars Mikelsons*

Main category: cs.HC

TL;DR: 论文提出了一个关于人类数字孪生的整体愿景，并通过需求、利益相关者和用户定义其规范，展示了六个功能级别的应用示例。


<details>
  <summary>Details</summary>
Motivation: 目前对人类数字孪生的定义和应用缺乏清晰认识，市场潜力不明，研究旨在填补这一空白。

Method: 通过定义需求、利益相关者和用户，以及六个功能级别的应用示例，展示人类数字孪生的可行性。

Result: 提出了一个全面的需求列表，作为研究和工业界实现人类数字孪生的指南。

Conclusion: 研究为人类数字孪生的实现提供了指导，特别是在多目标应用中的可重用性。

Abstract: The digital twin of humans is a relatively new concept. While many diverse
definitions, architectures, and applications exist, a clear picture is missing
on what, in fact, makes a human digital twin. Within this context, researchers
and industrial use-case owners alike are unaware about the market potential of
the - at the moment - rather theoretical construct. In this work, we draw a
holistic vision of the human digital twin, and derive the specification of this
holistic human digital twin in form of requirements, stakeholders, and users.
For each group of users, we define exemplary applications that fall into the
six levels of functionality: store, analyze, personalize, predict, control, and
optimize. The functionality levels facilitate an abstraction of abilities of
the human digital twin. From the manifold applications, we discuss three in
detail to showcase the feasibility of the abstraction levels and the analysis
of stakeholders and users. Based on the deep discussion, we derive a
comprehensive list of requirements on the holistic human digital twin. These
considerations shall be used as a guideline for research and industries for the
implementation of human digital twins, particularly in context of reusability
in multiple target applications.

</details>


### [236] [LEKIA: A Framework for Architectural Alignment via Expert Knowledge Injection](https://arxiv.org/abs/2507.14944)
*Boning Zhao,Yutong Hu*

Main category: cs.HC

TL;DR: 论文提出了一种名为LEKIA的新框架，通过分层专家知识注入架构，统一了知识注入和价值对齐的目标，解决了LLM在高风险领域部署中的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在高风险领域部署时面临的知识注入与价值对齐之间的冲突。

Method: 提出LEKIA框架，包含理论层、实践层和评估层，通过智能中介指导LLM的推理过程。

Result: 成功实现了一个基于LEKIA的心理支持助手，验证了框架的有效性。

Conclusion: LEKIA为专家驱动的AI行为设计提供了新路径，解决了知识与对齐的冲突。

Abstract: Deploying Large Language Models (LLMs) in high-stakes domains is impeded by a
dual challenge: the need for deep, dynamic expert knowledge injection and
nuanced value alignment. Prevailing paradigms often address these challenges
separately, creating a persistent tension between knowledge and alignment;
knowledge-focused methods like Retrieval-Augmented Generation (RAG) have
limited deep alignment capabilities, while alignment-focused methods like
Reinforcement Learning from Human Feedback (RLHF) struggle with the agile
injection of expert wisdom. This paper introduces a new collaborative
philosophy, Expert-owned AI behavior design, realized through Architectural
Alignment-a paradigm that unifies these two goals within a single framework
called the Layered Expert Knowledge Injection Architecture (LEKIA). LEKIA
operates as an intelligent intermediary that guides an LLM's reasoning process
without altering its weights, utilizing a three-tiered structure: a Theoretical
Layer for core principles, a Practical Layer for exemplary cases, and an
Evaluative Layer for real-time, value-aligned self-correction. We demonstrate
the efficacy of this paradigm through the successful implementation of a
LEKIA-based psychological support assistant for the special education field.
Our work presents a path toward more responsible and expert-driven AI,
empowering domain specialists to directly architect AI behavior and resolve the
tension between knowledge and alignment.

</details>


### [237] [Echoes of the Land: An Interactive Installation Based on Physical Model of Earthquake](https://arxiv.org/abs/2507.14947)
*Ivan C. H. Liu,Chung-En Hao,Jing Xie*

Main category: cs.HC

TL;DR: 《Echoes of the Land》是一个将地震动力学转化为多感官体验的互动装置，通过科学基础的弹簧块模型模拟地震重现和自组织临界性。


<details>
  <summary>Details</summary>
Motivation: 结合科学知识与艺术实践，探索新兴复杂性、美学和互动性的交叉点。

Method: 使用运动捕捉和拼接颗粒合成技术，实时生成声音和光效，每个块作为代理产生突发的视听效果。

Result: 作品展示了破裂和阈值行为的物理学，为新型乐器和叙事媒介开辟了新途径。

Conclusion: 该作品为科学和艺术的融合提供了范例，并为进一步研究复杂性与互动性的关系提供了机会。

Abstract: Echoes of the Land is an interactive installation that transforms seismic
dynamics into a multisensory experience through a scientifically grounded
spring-block model. Simulating earthquake recurrence and self-organized
criticality, the work generates real-time sound and light via motion capture
and concatenative granular synthesis. Each block acts as an agent, producing
emergent audiovisual cascades that visualize the physics of rupture and
threshold behavior. This work exemplifies the amalgamation of scientific
knowledge and artistic practice, opening new avenues for novel forms of musical
instrument and narrative medium, while inviting further investigation into the
intersection of emergent complexity, aesthetics and interactivity.

</details>


### [238] [Emphasizing Deliberation and Critical Thinking in an AI Hype World](https://arxiv.org/abs/2507.14961)
*Katja Rogers*

Main category: cs.HC

TL;DR: 论文主张通过缓慢、深思熟虑的技术使用和批判性参与来应对AI炒作，而非禁止或放弃技术。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在后AI炒作时代中，通过更谨慎的方式使用技术，以减少对教育和研究的负面影响。

Method: 提倡缓慢、深思熟虑的技术使用，并结合批判性参与和非参与。

Result: 有助于建立坚实的知识基础，减少AI在教育与研究中的有害影响。

Conclusion: 批判性参与和谨慎使用技术是应对AI炒作的有效途径。

Abstract: AI solutionism is accelerated and substantiated by hype and HCI's elevation
of novelty. Banning or abandoning technology is unlikely to work and probably
not beneficial on the whole either -- but slow(er), deliberate use together
with conscientious, critical engagement and non-engagement may help us navigate
a post-AI hype world while contributing to a solid knowledge foundation and
reducing harmful impacts in education and research.

</details>


### [239] ['A Little Bubble of Friends': An Analysis of LGBTQ+ Pandemic Experiences Using Reddit Data](https://arxiv.org/abs/2507.15033)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 研究通过LDA主题建模和情感分析，分析了疫情期间LGBTQ+群体在Reddit上的讨论主题和态度，探讨了Reddit在该群体生活中的角色及其变化。


<details>
  <summary>Details</summary>
Motivation: 疫情期间，社交媒体成为年轻人交流的重要平台，尤其是LGBTQ+群体。研究旨在了解该群体在Reddit上的讨论主题和情感态度，以及平台在其生活中的作用。

Method: 使用LDA主题建模和情感分析技术，分析了五个LGBTQ+相关子版块的数据。

Result: 揭示了疫情期间LGBTQ+群体在Reddit上的主要讨论主题和情感倾向，并探讨了平台角色的变化。

Conclusion: Reddit在疫情期间为LGBTQ+群体提供了重要的交流空间，其角色可能与疫情前有所不同。

Abstract: Social media was one of the most popular forms of communication among young
people with digital access during the pandemic. Consequently, crucial debates
and discussions about the pandemic crisis have also developed on social media
platforms, making them a great primary source to study the experiences of
specific groups and communities during the pandemic. This study involved
research using LDA topic modeling and sentiment analysis on data obtained from
the social media platform Reddit to understand the themes and attitudes in
circulation within five subreddits devoted to LGBTQ+ experiences and issues. In
the process, we attempt to make sense of the role that Reddit may have played
in the lives of LGBTQ+ people who were online during the pandemic, and whether
this was marked by any continuities or discontinuities from before the pandemic
period.

</details>


### [240] [Visibility vs. Engagement: How Two Indian News Websites Reported on LGBTQ+ Individuals and Communities during the Pandemic](https://arxiv.org/abs/2507.15041)
*Dhruvee Birla,Nazia Akhtar*

Main category: cs.HC

TL;DR: 研究分析了印度两家英语新闻网站在疫情期间对LGBTQ+群体的报道，发现报道数量增加但质量不足，且存在过时和恐跨语言。


<details>
  <summary>Details</summary>
Motivation: 探讨印度在线新闻媒体在疫情期间如何报道LGBTQ+群体的生活现实，尤其是在法律认可有限的背景下。

Method: 使用情感分析和主题建模分析2020年3月至2021年8月的文章，并与疫情前（2019年1月至12月）对比。

Result: 报道数量增加，但质量不足，部分文章语言过时且恐跨。

Conclusion: 研究揭示了疫情期间印度新闻网站对LGBTQ+群体的可见性和代表性，但报道仍需改进。

Abstract: In India, online news media outlets were an important source of information
for people with digital access during the COVID-19 pandemic. In India, where
"transgender" was legally recognised as a category only in 2014, and same-sex
marriages are yet to be legalised, it becomes crucial to analyse whether and
how they reported the lived realities of vulnerable LGBTQ+ communities during
the pandemic. This study analysed articles from online editions of two
English-language newspaper websites, which differed vastly in their circulation
figures-The Times of India and The Indian Express. The results of our study
suggest that these newspaper websites published articles surrounding various
aspects of the lives of LGBTQ+ individuals with a greater focus on transgender
communities. However, they lacked quality and depth. Focusing on the period
spanning March 2020 to August 2021, we analysed articles using sentiment
analysis and topic modelling. We also compared our results to the period before
the pandemic (January 2019 - December 2019) to understand the shift in topics,
sentiments, and stances across the two newspaper websites. A manual analysis of
the articles indicated that the language used in certain articles by The Times
of India was transphobic and obsolete. Our study captures the visibility and
representation of the LGBTQ+ communities in Indian newspaper websites during
the pandemic.

</details>


### [241] [Beyond Visual Line of Sight: UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence](https://arxiv.org/abs/2507.15049)
*Andres Navarro,Carlos de Quinto,José Alberto Hernández*

Main category: cs.HC

TL;DR: 本文介绍了一种低成本的四旋翼无人机平台，结合5G通信、边缘计算和AI技术，解决非地面网络（NTN）中的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 无人机作为智能节点在非地面网络中具有巨大潜力，但需要低成本、高效能的解决方案来应对复杂场景。

Method: 采用配备全景摄像头、强大机载计算能力和大型语言模型（LLMs）的无人机系统，实现低延迟视觉流处理和5G通信。

Result: 实地测试证实了系统的低延迟处理能力和稳定的5G连接，LLMs进一步提升了数据分析和决策支持效率。

Conclusion: 该系统在应急响应、基础设施评估和环境监测等场景中表现出高度适应性，展示了其在复杂环境中的实用价值。

Abstract: Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as
agile, intelligent nodes capable of advanced analytics and instantaneous
situational awareness. This article introduces a budget-friendly quadcopter
platform that unites 5G communications, edge-based processing, and AI to tackle
core challenges in NTN scenarios. Outfitted with a panoramic camera, robust
onboard computation, and LLMs, the drone system delivers seamless object
recognition, contextual analysis, and immersive operator experiences through
virtual reality VR technology. Field evaluations confirm the platform's ability
to process visual streams with low latency and sustain robust 5G links. Adding
LLMs further streamlines operations by extracting actionable insights and
refining collected data for decision support. Demonstrated use cases, including
emergency response, infrastructure assessment, and environmental surveillance,
underscore the system's adaptability in demanding contexts.

</details>


### [242] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: 开发了一种多模态引导模拟器，帮助盲人和低视力（BLV）用户在工业仓库中安全遥控机器人，提供视觉、听觉和触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 工业仓库环境复杂，BLV用户遥控机器人存在高风险，现有研究缺乏针对BLV用户的多模态引导设计。

Method: 结合导航网格和实时路径规划，提供同步的视觉路径线、语音导航提示和触觉障碍反馈。

Result: 系统为BLV用户提供了可重复的测试平台和算法参考，支持快速部署到实际仓库。

Conclusion: 该模拟器设计原则易于适配真实机器人，为包容性遥控工具的研究和部署提供了可行方案。

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [243] ["If I were in Space": Understanding and Adapting to Social Isolation through Designing Collaborative Narratives](https://arxiv.org/abs/2507.15081)
*Qi Gong,Ximing Shen,Ziyou Yin,Yaning Li,Ray Lc*

Main category: cs.HC

TL;DR: 研究探讨了通过社交VR中的协作叙事体验帮助人们应对社交隔离，揭示了创意适应策略。


<details>
  <summary>Details</summary>
Motivation: 社交隔离可能导致焦虑和孤独等健康问题，现有研究多关注物理干预，忽视了叙事策略的潜力。

Method: 设计了一个协作在线叙事体验，18名隔离者参与虚拟角色扮演，设计太空旅程隐喻隔离。

Result: 定性分析显示参与者通过创意活动应对隔离，叙事体验意外影响了适应过程。

Conclusion: 研究表明，设计趣味叙事体验可作为探索人们应对社交隔离方式的工具。

Abstract: Social isolation can lead to pervasive health issues like anxiety and
loneliness. Previous work focused on physical interventions like exercise and
teleconferencing, but overlooked the narrative potential of adaptive
strategies. To address this, we designed a collaborative online storytelling
experience in social VR, enabling participants in isolation to design an
imaginary space journey as a metaphor for quarantine, in order to learn about
their isolation adaptation strategies in the process. Eighteen individuals
participated during real quarantine undertaken a virtual role-play experience,
designing their own spaceship rooms and engaging in collaborative activities
that revealed creative adaptative strategies. Qualitative analyses of
participant designs, transcripts, and interactions revealed how they coped with
isolation, and how the engagement unexpectedly influenced their adaptation
process. This study shows how designing playful narrative experiences, rather
than solution-driven approaches, can serve as probes to surface how people
navigate social isolation.

</details>


### [244] [TalkLess: Blending Extractive and Abstractive Speech Summarization for Editing Speech to Preserve Content and Style](https://arxiv.org/abs/2507.15202)
*Karim Benharrak,Puyuan Peng,Amy Pavel*

Main category: cs.HC

TL;DR: TalkLess结合提取和抽象方法，灵活编辑语音以保持内容和风格，显著降低认知负担和编辑工作量。


<details>
  <summary>Details</summary>
Motivation: 解决语音编辑繁琐耗时的问题，同时保留说话者的风格和内容。

Method: TalkLess生成可能的文本编辑，选择最优编辑以最大化压缩、覆盖率和音频质量，并通过语音编辑模型将文本编辑转化为音频编辑。

Result: TalkLess在覆盖率和去除语音错误方面优于现有提取方法，显著降低认知负担和编辑工作量。

Conclusion: TalkLess为语音编辑提供了一种高效且灵活的方法，适用于实际创作场景。

Abstract: Millions of people listen to podcasts, audio stories, and lectures, but
editing speech remains tedious and time-consuming. Creators remove unnecessary
words, cut tangential discussions, and even re-record speech to make recordings
concise and engaging. Prior work automatically summarized speech by removing
full sentences (extraction), but rigid extraction limits expressivity. AI tools
can summarize then re-synthesize speech (abstraction), but abstraction strips
the speaker's style. We present TalkLess, a system that flexibly combines
extraction and abstraction to condense speech while preserving its content and
style. To edit speech, TalkLess first generates possible transcript edits,
selects edits to maximize compression, coverage, and audio quality, then uses a
speech editing model to translate transcript edits into audio edits. TalkLess's
interface provides creators control over automated edits by separating
low-level wording edits (via the compression pane) from major content edits
(via the outline pane). TalkLess achieves higher coverage and removes more
speech errors than a state-of-the-art extractive approach. A comparison study
(N=12) showed that TalkLess significantly decreased cognitive load and editing
effort in speech editing. We further demonstrate TalkLess's potential in an
exploratory study (N=3) where creators edited their own speech.

</details>


### [245] [How Does Empirical Research Facilitate Creation Tool Design? A Data Video Perspective](https://arxiv.org/abs/2507.15244)
*Leixian Shen,Leni Yang,Haotian Li,Yun Wang,Yuyu Luo,Huamin Qu*

Main category: cs.HC

TL;DR: 本文通过数据视频案例研究，揭示了实证研究对创作工具开发的影响，并提出了加强两者整合的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨实证研究如何影响创作工具开发，以及如何加强这种整合。

Method: 分析46篇实证研究和48篇工具论文，结合11位专家访谈，进行情境感知引用分析和模式分类。

Result: 揭示了实证研究如何通过引用功能影响工具设计，并总结了影响应用性的关键因素。

Conclusion: 提出了加强实证研究与工具研究互动的建议，以增强工具的理论基础和实践影响。

Abstract: Empirical research in creative design deepens our theoretical understanding
of design principles and perceptual effects, offering valuable guidance for
innovating creation tools. However, how these empirical insights currently
influence the development of creation tools, and how their integration can be
enhanced in the future, remains insufficiently understood. In this paper, we
aim to unveil the gap through a case study on data videos, a prominent and
wide-spread medium for effective data storytelling. To achieve the goal, we
conducted a comprehensive analysis of 46 empirical research papers and 48
creation tool papers on data video, complemented by interviews with 11 experts.
Building upon a systematic collection and structured characterization of
empirical research by their methodologies (e.g., corpus analysis, comparative
evaluations) and component focus (e.g., visuals, motions, narratives, audio),
we conducted a context-aware citation analysis and revealed a taxonomy of
recurring patterns in how empirical findings inform tool design across citation
functions (e.g., problem framing, technical reference). Expert interviews
further uncovered researchers' practice patterns in applying empirical findings
(e.g., adaptation, synthesis, iteration, etc.) and identified key factors
influencing applicability, such as contextual relevance, granularity matching,
clarity, credibility, and feasibility. Finally, we derive suggestions and
discuss future opportunities to foster closer mutual engagement between
empirical and tool research, aiming to reinforce the theoretical grounding of
creation tools and enhance the practical impact of empirical research.

</details>


### [246] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO结合元学习和PBO，通过利用先验用户偏好模型，显著减少迭代次数，实现更高效的个性化视觉优化。


<details>
  <summary>Details</summary>
Motivation: 由于视觉参数调整搜索空间大且缺乏明确目标函数，用户依赖隐式偏好，传统PBO方法迭代次数多，不适合普通用户。

Method: Meta-PO整合PBO与元学习，通过先验用户偏好模型智能推荐设计候选，加速收敛。

Result: 实验显示，Meta-PO在相似目标下仅需5.86次迭代，跨目标下8次迭代即可达到满意效果。

Conclusion: Meta-PO通过高效、通用的优化方法，使个性化视觉优化更适用于终端用户。

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [247] [Designing at 1:1 Scale on Wall-Sized Displays Using Existing UI Design Tools](https://arxiv.org/abs/2507.15433)
*Lou Schwartz,Mohammad Ghoniem,Valérie Maquil,Adrien Coppens,Johannes Hermen*

Main category: cs.HC

TL;DR: 研究探讨了在墙尺寸显示器上使用1:1比例设计的可行性，通过用户研究和技术评估发现，平板交互最舒适，并提出12条设计指南。


<details>
  <summary>Details</summary>
Motivation: 墙尺寸显示器的空间特性对用户界面设计提出挑战，现有工具未完全支持此类设计。

Method: 进行了两项用户研究和一项技术评估，测试了三种交互方式（触控、键盘+触控板、平板）在两种墙尺寸显示器上的表现。

Result: 1:1比例设计受欢迎，平板交互最舒适，混合交互方式有潜力，环境因素（如家具）需考虑。

Conclusion: 现有工具需改进以支持墙尺寸显示器设计，提出了12条设计指南。

Abstract: Wall-Sized Displays have spatial characteristics that are difficult to
address during user interface design. The design at scale 1:1 could be part of
the solution. In this paper, we present the results of two user studies and one
technology review, exploring the usability of popular, desktop-optimized
prototyping tools, for designing at scale on Wall-Sized Displays. We considered
two wall-sized display setups, and three different interaction methods: touch,
a keyboard equipped with a touchpad, and a tablet. We observed that designing
at scale 1:1 was appreciated. Tablet-based interaction proved to be the most
comfortable interaction method, and a mix of interaction modalities is
promising. In addition, care must be given to the surrounding environment, such
as furniture. We propose twelve design guidelines for a design tool dedicated
to this specific context. Overall, existing user interface design tools do not
yet fully support design on and for wall-sized displays and require further
considerations in terms of placement of user interface elements and the
provision of additional features.

</details>


### [248] [Evaluating Joint Attention for Mixed-Presence Collaboration on Wall-Sized Displays](https://arxiv.org/abs/2507.15443)
*Adrien Coppens,Valérie Maquil*

Main category: cs.HC

TL;DR: 提出了一种基于头部注视数据的联合注意力测量方法，用于评估混合存在协作在墙式显示器上的质量。


<details>
  <summary>Details</summary>
Motivation: 需要一种适应房间规模体验且不显眼的评估方法，以量化混合存在协作的质量。

Method: 通过头部注视数据测量联合注意力，并在用户研究中实施，研究混合存在协作。

Result: 初步结果来自一次特定会话的数据，展示了方法的可行性和初步见解。

Conclusion: 该方法为混合存在协作的评估提供了新的视角，未来可进一步扩展研究。

Abstract: To understand and quantify the quality of mixed-presence collaboration around
wall-sized displays, robust evaluation methodologies are needed, that are
adapted for a room-sized experience and are not perceived as obtrusive. In this
paper, we propose our approach for measuring joint attention based on head gaze
data. We describe how it has been implemented for a user study on mixed
presence collaboration with two wall-sized displays and report on the insights
we gained so far from its implementation, with a preliminary focus on the data
coming from one particular session.

</details>


### [249] [Challenging Disability and Interaction Norms in XR: Cooling Down the Empathy Machine in Waiting for Hands](https://arxiv.org/abs/2507.15481)
*Yesica Duarte,Puneet Jain*

Main category: cs.HC

TL;DR: 论文通过互动XR装置《Waiting for Hands》批判VR作为‘终极共情机器’的局限性，通过创造替代控制器和荒诞表演颠覆对残疾的刻板叙事。


<details>
  <summary>Details</summary>
Motivation: 回应VR技术将残疾简化为同情或激励的表演，探讨XR如何以伦理姿态关注非规范性身体体验。

Method: 设计替代控制器并构建荒诞XR表演，通过干扰观众对纪录片的直接关注引入不确定性。

Result: XR表演通过荒诞和疏离感挑战了沉浸式媒体的传统框架，成功颠覆了残疾的共情驱动叙事。

Conclusion: XR表演可以采取伦理姿态，引导参与者关注非规范性身体体验，同时避免将其作为奇观展示。

Abstract: Virtual Reality (VR) is often described as the "ultimate empathy machine,"
framing disability as an experience to be simulated through such technologies,
which can reduce disability to a spectacle of pity or inspiration. In response,
we present Waiting for Hands (WfH), an interactive eXtended Reality (XR)
installation that critiques this logic by: (1) repurposing interaction norms in
XR through the creation of Alternative Controllers, and (2) staging an absurd
XR performance using the built controllers to disrupt sentimentalized
disability narratives. The performance involves eight people: two XR
participants on stage and six audience members watching a projected documentary
about Hema Kumari, an Indian singer living with Rheumatoid Arthritis. The XR
users partially obscure the film, drawing attention through strange mouth and
hand movements performed in XR. This creates a layered experience that disrupts
direct engagement with Hema's story and introduces uncertainty. While XR is
often seen as a fully immersive, sensory-dominant medium, this piece subverts
that framing by using XR to produce absurdity and alienation. By challenging
empathy-driven and pitiable narratives of disability, we ask what ethical
stance an XR performance can take to attune participants to non-normative
embodiment while resisting spectacle.

</details>


### [250] [FollowUpBot: An LLM-Based Conversational Robot for Automatic Postoperative Follow-up](https://arxiv.org/abs/2507.15502)
*Chen Chen,Jianing Yin,Jiannong Cao,Zhiyuan Wen,Mingjin Zhang,Weixun Gao,Xiang Wang,Haihua Shu*

Main category: cs.HC

TL;DR: 本文介绍了FollowUpBot，一种基于LLM的边缘部署机器人，用于术后护理和监测，解决了传统随访方法的效率和数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统术后随访方法耗时且劳动密集，现有数字解决方案存在交互不灵活或隐私泄露问题。

Method: 开发FollowUpBot，利用边缘部署的LLM进行自适应面对面交流，动态规划最优路径，并自动生成结构化随访报告。

Result: 实验显示机器人实现了高覆盖率和满意度，以及高精度的报告生成。

Conclusion: FollowUpBot有效提升了术后随访的效率和隐私保护。

Abstract: Postoperative follow-up plays a crucial role in monitoring recovery and
identifying complications. However, traditional approaches, typically involving
bedside interviews and manual documentation, are time-consuming and
labor-intensive. Although existing digital solutions, such as web
questionnaires and intelligent automated calls, can alleviate the workload of
nurses to a certain extent, they either deliver an inflexible scripted
interaction or face private information leakage issues. To address these
limitations, this paper introduces FollowUpBot, an LLM-powered edge-deployed
robot for postoperative care and monitoring. It allows dynamic planning of
optimal routes and uses edge-deployed LLMs to conduct adaptive and face-to-face
conversations with patients through multiple interaction modes, ensuring data
privacy. Moreover, FollowUpBot is capable of automatically generating
structured postoperative follow-up reports for healthcare institutions by
analyzing patient interactions during follow-up. Experimental results
demonstrate that our robot achieves high coverage and satisfaction in follow-up
interactions, as well as high report generation accuracy across diverse field
types. The demonstration video is available at
https://www.youtube.com/watch?v=_uFgDO7NoK0.

</details>


### [251] [Strategies to Manage Human Factors in Mixed Reality Pilot Training: A Survey](https://arxiv.org/abs/2507.15526)
*Antonio Perez,Avinash Singh,Jonathan Mitchell,Philip Swadling*

Main category: cs.HC

TL;DR: 综述探讨了混合现实（MR）头戴显示器在飞行员培训中的人因挑战及缓解策略，结合航空标准评估了硬件、软件等多方面干预措施。


<details>
  <summary>Details</summary>
Motivation: MR技术为飞行模拟训练提供了沉浸感和成本效益，但人因问题如晕动症和视觉疲劳可能影响训练效果，需解决以发挥其潜力。

Method: 系统回顾现有文献，结合航空权威标准，从硬件、软件、人机工程等多角度分析干预措施。

Result: 研究揭示了人因挑战的缓解策略，并评估了其在现有航空培训法规下的适用性。

Conclusion: MR技术需平衡技术要求与飞行员福祉，研究为制定MR航空培训指南提供了重要依据。

Abstract: Mixed Reality (MR) head mounted displays (HMDs) offer a promising alternative
to traditional Flight Simulator Training Device (FSTD) displays, providing
immersion, realism and cost efficiency. However, these technologies require
management of human factors; cybersickness, visual fatigue and ergonomic
strain. If left unmitigated, these effects can hinder pilot performance and
training outcomes. For safety critical fields like aviation, addressing human
factors challenges is crucial for MR's training potential. This survey
systematically reviews the current literature identifying key human factors
challenges in MR HMD use in pilot training and examines strategies to mitigate
these barriers. Drawing on existing industry standards set by a leading
aviation authority, the review adopts a regulatory perspective to explore
hardware, software, ergonomic, physiological and psychological interventions
improving pilot comfort, safety and training effectiveness in an MR FSTD.
Additionally, it evaluates which of these interventions are most appropriate
and viable for MR pilot training under existing aviation training regulations,
ensuring that technical requirements and pilot wellbeing remain balanced. The
findings yield significant insights for the human dimensions of aviation
simulation training, highlighting how regulatory considerations shape the
practicality of mitigation measures. These insights inform emerging MR aviation
training guidelines and best practices, supporting MR's readiness to enhance
aviation training.

</details>


### [252] [FlowForge: Guiding the Creation of Multi-agent Workflows with Design Space Visualization as a Thinking Scaffold](https://arxiv.org/abs/2507.15559)
*Pan Hao,Dongyeop Kang,Nicholas Hinds,Qianwen Wang*

Main category: cs.HC

TL;DR: FLOWFORGE是一个交互式可视化工具，通过结构化视觉探索和实时指导，帮助优化多智能体工作流设计。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体工作流设计依赖专家直觉，存在设计固定化和试错探索效率低的问题。

Method: FLOWFORGE将设计过程分为三个层次（任务规划、智能体分配、智能体优化），并提供基于设计模式的实时建议。

Result: 用户研究和案例表明，FLOWFORGE提高了工作流设计的可用性和效率。

Conclusion: FLOWFORGE通过结构化探索和实时指导，显著改进了多智能体工作流的设计过程。

Abstract: Multi-agent workflows have become an effective strategy for tackling
complicated tasks by decomposing them into multiple sub-tasks and assigning
them to specialized agents. However, designing optimal workflows remains
challenging due to the vast and intricate design space. Current practices rely
heavily on the intuition and expertise of practitioners, often resulting in
design fixation or an unstructured, time-consuming exploration of
trial-and-error. To address these challenges, this work introduces FLOWFORGE,
an interactive visualization tool to facilitate the creation of multi-agent
workflow through i) a structured visual exploration of the design space and ii)
in-situ guidance informed by established design patterns. Based on formative
studies and literature review, FLOWFORGE organizes the workflow design process
into three hierarchical levels (i.e., task planning, agent assignment, and
agent optimization), ranging from abstract to concrete. This structured visual
exploration enables users to seamlessly move from high-level planning to
detailed design decisions and implementations, while comparing alternative
solutions across multiple performance metrics. Additionally, drawing from
established workflow design patterns, FLOWFORGE provides context-aware, in-situ
suggestions at each level as users navigate the design space, enhancing the
workflow creation process with practical guidance. Use cases and user studies
demonstrate the usability and effectiveness of FLOWFORGE, while also yielding
valuable insights into how practitioners explore design spaces and leverage
guidance during workflow development.

</details>


### [253] [Chapter 11 Students' interaction with and appreciation of automated informative tutoring feedback](https://arxiv.org/abs/2507.15650)
*Gerben van der Hoek,Bastiaan Heeren,Rogier Bos,Paul Drijvers,Johan Jeuring*

Main category: cs.HC

TL;DR: 研究探讨了一种平衡探索空间与学习障碍缓解的反馈策略，发现学生喜欢这种平衡反馈，并能有效促进学习。


<details>
  <summary>Details</summary>
Motivation: 探索如何在计算机辅助形成性评估中设计反馈策略，以平衡学生的探索空间和学习障碍的缓解。

Method: 25名15-17岁学生在在线环境中完成线性与指数外推任务，通过屏幕记录和访谈收集数据。

Result: 学生能自我指导并避免脱离学习，且偏好错误特定反馈而非完整解答。

Conclusion: 平衡反馈策略能有效促进学生与环境的互动，提升学习效果。

Abstract: Computer aided formative assessment can be used to enhance a learning
process, for instance by providing feedback. There are many design choices for
delivering feedback, that lead to a feedback strategy. In an informative
feedback strategy, students do not immediately receive information about the
correct response, but are offered the opportunity to retry a task to apply
feedback information. In this small-scale qualitative study, we explore an
informative feedback strategy designed to offer a balance between room for
exploration and mitigation of learning barriers. The research questions concern
the ways in which students interact with the feedback strategy and their
appreciation of error-specific feedback as opposed to worked-out solutions. To
answer these questions, twenty-five 15-to-17-year-old senior general secondary
education students worked for approximately 20 minutes on linear and
exponential extrapolation tasks in an online environment. Data included screen
captures of students working with the environment and post-intervention
interviews. Results showed that room for exploration offered opportunities for
self-guidance while mitigation of learning barriers prevented disengagement.
Furthermore, students appreciated balanced feedback. We conclude that the
balanced feedback strategy yielded fruitful student-environment interactions.

</details>


### [254] [Surfacing Variations to Calibrate Perceived Reliability of MLLM-generated Image Descriptions](https://arxiv.org/abs/2507.15692)
*Meng Chen,Akhil Iyer,Amy Pavel*

Main category: cs.HC

TL;DR: 多模态大语言模型（MLLM）为盲人和低视力（BLV）人群提供了获取视觉信息的新途径，但其错误难以被BLV用户察觉，存在安全和社交风险。研究通过展示多个MLLM响应的差异，帮助BLV用户检测不可靠信息，显著提高了识别能力并降低了信任度。


<details>
  <summary>Details</summary>
Motivation: MLLM为BLV人群提供了便利，但其错误难以被察觉，可能导致安全和社交问题。现有解决方法（如交叉验证或咨询视力正常者）效率低下。

Method: 提出一个设计空间，用于提取和展示MLLM描述的差异，开发了一个原型系统，包含三种差异展示方式，并通过15名BLV用户的实验验证效果。

Result: 展示差异显著提高了用户识别不可靠信息的能力（提升4.9倍），并降低了用户对MLLM响应的信任度。14/15用户更喜欢差异展示，所有用户都表示对系统感兴趣。

Conclusion: 通过系统化展示MLLM响应的差异，可以有效帮助BLV用户检测不可靠信息，提升使用安全性。

Abstract: Multimodal large language models (MLLMs) provide new opportunities for blind
and low vision (BLV) people to access visual information in their daily lives.
However, these models often produce errors that are difficult to detect without
sight, posing safety and social risks in scenarios from medication
identification to outfit selection. While BLV MLLM users use creative
workarounds such as cross-checking between tools and consulting sighted
individuals, these approaches are often time-consuming and impractical. We
explore how systematically surfacing variations across multiple MLLM responses
can support BLV users to detect unreliable information without visually
inspecting the image. We contribute a design space for eliciting and presenting
variations in MLLM descriptions, a prototype system implementing three
variation presentation styles, and findings from a user study with 15 BLV
participants. Our results demonstrate that presenting variations significantly
increases users' ability to identify unreliable claims (by 4.9x using our
approach compared to single descriptions) and significantly decreases perceived
reliability of MLLM responses. 14 of 15 participants preferred seeing
variations of MLLM responses over a single description, and all expressed
interest in using our system for tasks from understanding a tornado's path to
posting an image on social media.

</details>


### [255] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: 研究分析了青少年对Character.AI聊天机器人的过度依赖现象，发现其可能导致心理困扰和现实关系问题，并提出了改进设计的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨青少年与可定制人格聊天机器人的互动模式及其对情感依赖和数字过度依赖的影响。

Method: 分析了318篇由13-17岁青少年在Character.AI子论坛发布的Reddit帖子。

Result: 青少年常因情感支持或创意表达使用聊天机器人，但易形成强烈依赖，干扰现实生活和关系。依赖通常在反思危害、回归现实社交或受平台限制时终止。

Conclusion: 建议未来聊天机器人设计应增强自我意识、支持现实互动，并让青少年参与开发更安全的数字工具。

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [256] [Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence](https://arxiv.org/abs/2507.14658)
*Faizan Contractor,Li Li,Ranwa Al Mallah*

Main category: cs.MA

TL;DR: 论文提出了一种在部分可观测环境中通过通信提升多智能体协作防御网络威胁的方法，使用DIAL算法在Cyber Operations Research Gym中训练智能体。


<details>
  <summary>Details</summary>
Motivation: 现有方法中智能体在执行时独立行动，限制了策略的协调效果，而通过通信可以提升决策效果。

Method: 使用DIAL算法在Cyber Operations Research Gym中训练智能体，学习防御战术策略和最小成本通信消息。

Result: 智能体学习到的战术策略类似于人类专家在应对网络威胁时的行为，同时学会了高效通信。

Conclusion: 通过通信和协作训练，智能体能够更有效地防御网络威胁，并减少通信成本。

Abstract: Popular methods in cooperative Multi-Agent Reinforcement Learning with
partially observable environments typically allow agents to act independently
during execution, which may limit the coordinated effect of the trained
policies. However, by sharing information such as known or suspected ongoing
threats, effective communication can lead to improved decision-making in the
cyber battle space. We propose a game design where defender agents learn to
communicate and defend against imminent cyber threats by playing training games
in the Cyber Operations Research Gym, using the Differentiable Inter Agent
Learning algorithm adapted to the cyber operational environment. The tactical
policies learned by these autonomous agents are akin to those of human experts
during incident responses to avert cyber threats. In addition, the agents
simultaneously learn minimal cost communication messages while learning their
defence tactical policies.

</details>


### [257] [LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading](https://arxiv.org/abs/2507.14995)
*Chengwei Lou,Zekai Jin,Wei Tang,Guangfei Geng,Jin Yang,Lu Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种结合大型语言模型（LLM）和多智能体强化学习（MARL）的框架，用于实时P2P电力交易，解决了用户技术能力不足、缺乏专家经验和电网安全问题。


<details>
  <summary>Details</summary>
Motivation: 解决实时P2P电力市场中大规模个性化用户面临的决策多样性和缺乏定制化建模框架的挑战。

Method: 提出LLM-MARL框架，利用LLM生成个性化策略，通过模仿学习指导MARL，并设计差分注意力评论网络提升收敛性能。

Result: 实验表明LLM策略可替代人类专家，提出的算法在经济成本和电压违规率上优于基线算法，且稳定性强。

Conclusion: 该研究通过结合专家知识与智能体学习，为实时P2P电力市场决策提供了有效解决方案。

Abstract: Real-time peer-to-peer (P2P) electricity markets dynamically adapt to
fluctuations in renewable energy and variations in demand, maximizing economic
benefits through instantaneous price responses while enhancing grid
flexibility. However, scaling expert guidance for massive personalized
prosumers poses critical challenges, including diverse decision-making demands
and lack of customized modeling frameworks. This paper proposed an integrated
large language model-multi-agent reinforcement learning (LLM-MARL) framework
for real-time P2P energy trading to address challenges such as the limited
technical capability of prosumers, the lack of expert experience, and security
issues of distribution networks. LLMs are introduced as experts to generate
personalized strategy, guiding MARL under the centralized training with
decentralized execution (CTDE) paradigm through imitation learning. A
differential attention-based critic network is designed to enhance convergence
performance. Experimental results demonstrate that LLM generated strategies
effectively substitute human experts. The proposed multi-agent imitation
learning algorithms achieve significantly lower economic costs and voltage
violation rates on test sets compared to baselines algorithms, while
maintaining robust stability. This work provides an effective solution for
real-time P2P electricity market decision-making by bridging expert knowledge
with agent learning.

</details>


### [258] [EduThink4AI: Translating Educational Critical Thinking into Multi-Agent LLM Systems](https://arxiv.org/abs/2507.15015)
*Xinmeng Hou,Zhouquan Lu,Wenli Chen,Hai Hu,Qing Guo*

Main category: cs.MA

TL;DR: EDU-Prompting是一种多智能体框架，旨在提升LLM在教育中的批判性思维和内容真实性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的教育系统在促进批判性思维和应对对抗性提示方面存在不足。

Method: 提出EDU-Prompting框架，结合教育理论和LLM设计，生成批判性、无偏见的解释。

Result: 在理论和实际场景中，EDU-Prompting显著提升了内容的真实性和逻辑性。

Conclusion: 该框架模块化设计便于集成，无需大规模修改即可提升批判性思维。

Abstract: Large language models (LLMs) have demonstrated significant potential as
educational tutoring agents, capable of tailoring hints, orchestrating lessons,
and grading with near-human finesse across various academic domains. However,
current LLM-based educational systems exhibit critical limitations in promoting
genuine critical thinking, failing on over one-third of multi-hop questions
with counterfactual premises, and remaining vulnerable to adversarial prompts
that trigger biased or factually incorrect responses. To address these gaps, we
propose EDU-Prompting, a novel multi-agent framework that bridges established
educational critical thinking theories with LLM agent design to generate
critical, bias-aware explanations while fostering diverse perspectives. Our
systematic evaluation across theoretical benchmarks and practical college-level
critical writing scenarios demonstrates that EDU-Prompting significantly
enhances both content truthfulness and logical soundness in AI-generated
educational responses. The framework's modular design enables seamless
integration into existing prompting frameworks and educational applications,
allowing practitioners to directly incorporate critical thinking catalysts that
promote analytical reasoning and introduce multiple perspectives without
requiring extensive system modifications.

</details>


### [259] [LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra](https://arxiv.org/abs/2507.15815)
*Seth Karten,Wenzhe Li,Zihan Ding,Samuel Kleiner,Yu Bai,Chi Jin*

Main category: cs.MA

TL;DR: LLM Economist是一个基于代理的框架，用于设计和评估战略环境中的经济政策，通过分层决策和自然语言机制设计实现。


<details>
  <summary>Details</summary>
Motivation: 旨在通过大型语言模型代理模拟复杂经济系统，为政策评估提供可信的实验平台。

Method: 使用分层代理模型：下层为工人代理（基于人口统计和收入数据），上层为规划代理（通过强化学习设计税收政策）。

Result: 实验显示规划代理能接近Stackelberg均衡，提高社会福利，且分散治理下的投票机制进一步优化结果。

Conclusion: LLM Economist展示了语言模型代理在模拟和治理复杂经济系统中的潜力，为大规模政策评估提供可行工具。

Abstract: We present the LLM Economist, a novel framework that uses agent-based
modeling to design and assess economic policies in strategic environments with
hierarchical decision-making. At the lower level, bounded rational worker
agents -- instantiated as persona-conditioned prompts sampled from U.S.
Census-calibrated income and demographic statistics -- choose labor supply to
maximize text-based utility functions learned in-context. At the upper level, a
planner agent employs in-context reinforcement learning to propose
piecewise-linear marginal tax schedules anchored to the current U.S. federal
brackets. This construction endows economic simulacra with three capabilities
requisite for credible fiscal experimentation: (i) optimization of
heterogeneous utilities, (ii) principled generation of large, demographically
realistic agent populations, and (iii) mechanism design -- the ultimate nudging
problem -- expressed entirely in natural language. Experiments with populations
of up to one hundred interacting agents show that the planner converges near
Stackelberg equilibria that improve aggregate social welfare relative to Saez
solutions, while a periodic, persona-level voting procedure furthers these
gains under decentralized governance. These results demonstrate that large
language model-based agents can jointly model, simulate, and govern complex
economic systems, providing a tractable test bed for policy evaluation at the
societal scale to help build better civilizations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [260] [U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model](https://arxiv.org/abs/2507.14237)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: 论文研究了从弱监督到完全无监督的训练设置对去混响模型的影响，仅依赖混响信号和声学模型进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常需要成对的干湿数据，实际中难以获取。

Method: 提出了一种基于贝叶斯公式的序列学习策略，通过深度神经网络从混响输入中估计声学参数和干信号，并使用混响匹配损失指导。

Result: 最数据高效的变体仅需100个带混响参数标签的样本即可超越无监督基线。

Conclusion: 该方法在低资源场景下具有高效性和实用性。

Abstract: This paper explores the outcome of training state-ofthe-art dereverberation
models with supervision settings ranging from weakly-supervised to fully
unsupervised, relying solely on reverberant signals and an acoustic model for
training. Most of the existing deep learning approaches typically require
paired dry and reverberant data, which are difficult to obtain in practice. We
develop instead a sequential learning strategy motivated by a bayesian
formulation of the dereverberation problem, wherein acoustic parameters and dry
signals are estimated from reverberant inputs using deep neural networks,
guided by a reverberation matching loss. Our most data-efficient variant
requires only 100 reverberation-parameter-labelled samples to outperform an
unsupervised baseline, demonstrating the effectiveness and practicality of the
proposed method in low-resource scenarios.

</details>


### [261] [The Rest is Silence: Leveraging Unseen Species Models for Computational Musicology](https://arxiv.org/abs/2507.14638)
*Fabian C. Moss,Jan Hajič jr.,Adrian Nachtwey,Laurent Pugin*

Main category: cs.SD

TL;DR: 该论文首次将生态学中的'未观测物种模型'（USMs）应用于音乐学领域，以解决音乐数据不完整的问题，并通过四个案例研究展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 音乐学研究中存在大量不完整的数据集，传统方法无法准确估计缺失数据的规模，因此需要引入新的定量方法。

Method: 采用生态学中的'未观测物种模型'（USMs），并将其应用于音乐学数据，通过四个案例研究验证其有效性。

Result: USMs能够帮助回答音乐学中的定量问题，如缺失作曲家的数量、已编目中世纪圣歌的比例等。

Conclusion: USMs为音乐学研究提供了一种新的定量工具，能够有效估计缺失数据的规模，填补了传统方法的不足。

Abstract: For many decades, musicologists have engaged in creating large databases
serving different purposes for musicological research and scholarship. With the
rise of fields like music information retrieval and digital musicology, there
is now a constant and growing influx of musicologically relevant datasets and
corpora. In historical or observational settings, however, these datasets are
necessarily incomplete, and the true extent of a collection of interest remains
unknown -- silent. Here, we apply, for the first time, so-called Unseen Species
models (USMs) from ecology to areas of musicological activity. After
introducing the models formally, we show in four case studies how USMs can be
applied to musicological data to address quantitative questions like: How many
composers are we missing in RISM? What percentage of medieval sources of
Gregorian chant have we already cataloged? How many differences in music prints
do we expect to find between editions? How large is the coverage of songs from
genres of a folk music tradition? And, finally, how close are we in estimating
the size of the harmonic vocabulary of a large number of composers?

</details>


### [262] [Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer](https://arxiv.org/abs/2507.14647)
*Go Nishikawa,Wataru Nakata,Yuki Saito,Kanami Imamura,Hiroshi Saruwatari,Tomohiko Nakamura*

Main category: cs.SD

TL;DR: 该论文提出了一种用于多采样频率语音MOS预测的模型，结合了采样频率无关的卷积层和自监督学习，通过知识蒸馏和大规模数据集预训练提升性能，在AMC 2025 Track 3中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多采样频率语音的MOS预测问题，提升模型对不同采样频率的适应性。

Method: 集成采样频率无关的卷积层到自监督学习模型中，采用知识蒸馏和大规模数据集预训练策略。

Result: 在AMC 2025 Track 3中一项指标排名第一，最终排名第四，并通过消融研究验证模型关键因素。

Conclusion: 提出的模型在多采样频率语音MOS预测中表现优异，知识蒸馏和预训练策略对性能提升至关重要。

Abstract: We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3:
mean opinion score (MOS) prediction for speech with multiple sampling
frequencies (SFs). Our submitted model integrates an SF-independent (SFI)
convolutional layer into a self-supervised learning (SSL) model to achieve SFI
speech feature extraction for MOS prediction. We present some strategies to
improve the MOS prediction performance of our model: distilling knowledge from
a pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset.
Our submission to the AMC 2025 Track 3 ranked the first in one evaluation
metric and the fourth in the final ranking. We also report the results of our
ablation study to investigate essential factors of our model.

</details>


### [263] [Frame-level Temporal Difference Learning for Partial Deepfake Speech Detection](https://arxiv.org/abs/2507.15101)
*Menglu Li,Xiao-Ping Zhang,Lian Zhao*

Main category: cs.SD

TL;DR: 论文提出了一种无需帧级标注的深度伪造语音检测方法，通过分析时间差异来识别局部伪造。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的帧级标注且难以检测平滑过渡的深度伪造语音，亟需新方法。

Method: 引入时间差异注意力模块（TDAM），通过双层次差异表征捕捉时间异常，结合自适应平均池化减少信息丢失。

Result: 在PartialSpoof和HAD数据集上分别达到0.59%和0.03%的EER，显著优于现有方法。

Conclusion: TDAM-AvgPool模型无需帧级监督即可高效检测局部深度伪造语音，为实际应用提供了可行方案。

Abstract: Detecting partial deepfake speech is essential due to its potential for
subtle misinformation. However, existing methods depend on costly frame-level
annotations during training, limiting real-world scalability. Also, they focus
on detecting transition artifacts between bonafide and deepfake segments. As
deepfake generation techniques increasingly smooth these transitions, detection
has become more challenging. To address this, our work introduces a new
perspective by analyzing frame-level temporal differences and reveals that
deepfake speech exhibits erratic directional changes and unnatural local
transitions compared to bonafide speech. Based on this finding, we propose a
Temporal Difference Attention Module (TDAM) that redefines partial deepfake
detection as identifying unnatural temporal variations, without relying on
explicit boundary annotations. A dual-level hierarchical difference
representation captures temporal irregularities at both fine and coarse scales,
while adaptive average pooling preserves essential patterns across
variable-length inputs to minimize information loss. Our TDAM-AvgPool model
achieves state-of-the-art performance, with an EER of 0.59% on the PartialSpoof
dataset and 0.03% on the HAD dataset, which significantly outperforms the
existing methods without requiring frame-level supervision.

</details>


### [264] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: 提出了一种基于语音时间动态的上下文相关时长嵌入方法，用于表征说话人特征，并开发了攻击模型，显著提升了说话人验证性能。


<details>
  <summary>Details</summary>
Motivation: 语音的时间动态（如节奏、语调和语速）包含独特的说话人身份信息，但现有方法对其表征不足。

Method: 提取上下文相关时长嵌入，开发新型攻击模型，分析说话人验证和语音匿名化系统的潜在漏洞。

Result: 实验表明，该方法在原始和匿名数据上的说话人验证性能显著优于文献中的简单表征方法。

Conclusion: 上下文相关时长嵌入能有效表征说话人特征，提升攻击模型的性能，揭示了语音系统的潜在漏洞。

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [265] [EchoVoices: Preserving Generational Voices and Memories for Seniors and Children](https://arxiv.org/abs/2507.15221)
*Haiying Xu,Haoze Liu,Mingshi Li,Siyu Cai,Guangxuan Zheng,Yuhuang Jia,Jinghua Zhao,Yong Qin*

Main category: cs.SD

TL;DR: EchoVoices是一个端到端的数字人管道，专注于为老年人和儿童创建持久的数字角色，通过改进的语音识别、合成和记忆系统保存他们的声音和记忆。


<details>
  <summary>Details</summary>
Motivation: 智能语音和数字人技术主要服务于主流成年用户，忽视了老年人和儿童独特的语音模式和交互方式。

Method: 结合k-NN增强的Whisper模型、年龄自适应的VITS模型和基于LLM的代理，实现语音识别、合成和记忆系统的优化。

Result: 在SeniorTalk和ChildMandarin数据集上，识别准确性、合成质量和说话人相似性显著提升。

Conclusion: EchoVoices为保存代际声音提供了全面框架，促进了代际联系和数字遗产的创建。

Abstract: Recent breakthroughs in intelligent speech and digital human technologies
have primarily targeted mainstream adult users, often overlooking the distinct
vocal patterns and interaction styles of seniors and children. These
demographics possess distinct vocal characteristics, linguistic styles, and
interaction patterns that challenge conventional ASR, TTS, and LLM systems. To
address this, we introduce EchoVoices, an end-to-end digital human pipeline
dedicated to creating persistent digital personas for seniors and children,
ensuring their voices and memories are preserved for future generations. Our
system integrates three core innovations: a k-NN-enhanced Whisper model for
robust speech recognition of atypical speech; an age-adaptive VITS model for
high-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that
automatically generates persona cards and leverages a RAG-based memory system
for conversational consistency. Our experiments, conducted on the SeniorTalk
and ChildMandarin datasets, demonstrate significant improvements in recognition
accuracy, synthesis quality, and speaker similarity. EchoVoices provides a
comprehensive framework for preserving generational voices, offering a new
means of intergenerational connection and the creation of lasting digital
legacies.

</details>


### [266] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: 提出了一种基于扩散模型的说话人条件文本转语音系统，支持未见过的说话人和多种印度语言，通过参考音频嵌入和交叉注意力机制提升自然度和韵律。


<details>
  <summary>Details</summary>
Motivation: 解决为未见过的说话人生成语音和支持多样印度语言的挑战。

Method: 采用扩散模型架构，结合说话人编码器和交叉注意力时长预测机制，使用分类器自由指导提升零样本生成能力。

Result: 生成的语音更接近目标说话人，提升了时长建模和整体表现力。

Conclusion: 该方法在多种印度语言上表现优异，为未见说话人生成语音提供了有效解决方案。

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>


### [267] [MeMo: Attentional Momentum for Real-time Audio-visual Speaker Extraction under Impaired Visual Conditions](https://arxiv.org/abs/2507.15294)
*Junjie Li,Wenxuan Wu,Shuai Wang,Zexu Pan,Kong Aik Lee,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 论文提出了一种名为MeMo的新框架，通过自适应记忆库存储注意力相关信息，以解决音频-视觉目标说话人提取（AV-TSE）在视觉线索缺失时的性能问题。


<details>
  <summary>Details</summary>
Motivation: 受人类在缺乏辅助信息时仍能保持对目标说话人注意力的认知能力启发，旨在提升AV-TSE系统在视觉线索缺失时的鲁棒性。

Method: MeMo框架包含两个自适应记忆库，用于存储注意力相关信息，并在实时场景中维持注意力动量。

Result: 实验表明，MeMo框架在SI-SNR指标上比基线至少提升了2 dB。

Conclusion: MeMo框架有效提升了AV-TSE系统在视觉线索缺失时的性能，具有实际应用潜力。

Abstract: Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate a target
speaker's voice from multi-speaker environments by leveraging visual cues as
guidance. However, the performance of AV-TSE systems heavily relies on the
quality of these visual cues. In extreme scenarios where visual cues are
missing or severely degraded, the system may fail to accurately extract the
target speaker. In contrast, humans can maintain attention on a target speaker
even in the absence of explicit auxiliary information. Motivated by such human
cognitive ability, we propose a novel framework called MeMo, which incorporates
two adaptive memory banks to store attention-related information. MeMo is
specifically designed for real-time scenarios: once initial attention is
established, the system maintains attentional momentum over time, even when
visual cues become unavailable. We conduct comprehensive experiments to verify
the effectiveness of MeMo. Experimental results demonstrate that our proposed
framework achieves SI-SNR improvements of at least 2 dB over the corresponding
baseline.

</details>


### [268] [Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation](https://arxiv.org/abs/2507.15396)
*Hui-Guan Yuan,Ryandhimas E. Zezario,Shafique Ahmed,Hsin-Min Wang,Kai-Lung Hua,Yu Tsao*

Main category: cs.SD

TL;DR: Neuro-MSBG是一种轻量级端到端模型，用于高效模拟听力损失，支持并行推理并显著降低计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有听力损失模拟模型计算复杂度高、延迟大，难以实时应用且缺乏与语音处理系统的直接集成。

Method: 提出Neuro-MSBG模型，结合个性化听力图编码器，实现高效的时频建模。

Result: 实验显示，Neuro-MSBG在STOI和PESQ指标上表现优异，计算时间减少46倍。

Conclusion: Neuro-MSBG高效实用，适合实时应用和集成到语音处理系统中。

Abstract: Hearing loss simulation models are essential for hearing aid deployment.
However, existing models have high computational complexity and latency, which
limits real-time applications and lack direct integration with speech
processing systems. To address these issues, we propose Neuro-MSBG, a
lightweight end-to-end model with a personalized audiogram encoder for
effective time-frequency modeling. Experiments show that Neuro-MSBG supports
parallel inference and retains the intelligibility and perceptual quality of
the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of
0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for
Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation
runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second
input), further demonstrating its efficiency and practicality.

</details>


### [269] [Multichannel Keyword Spotting for Noisy Conditions](https://arxiv.org/abs/2507.15558)
*Dzmitry Saladukha,Ivan Koriabkin,Kanstantsin Artsiom,Aliaksei Rak,Nikita Ryzhikov*

Main category: cs.SD

TL;DR: 提出一种改进关键词检测算法的方法，通过多输入通道和注意力机制优化噪声环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有波束成形和自适应噪声消除技术可能扭曲或抑制有用信号，影响激活系统性能。

Method: 使用多输入通道和注意力机制的神经网络架构，动态选择最有用的信号通道或组合。

Result: 在实验室和自然条件下的数据集上验证了算法改进，噪声减少和关键词检测指标优于基线。

Conclusion: 所提算法在噪声环境中表现优越，计算资源消耗合理。

Abstract: This article presents a method for improving a keyword spotter (KWS)
algorithm in noisy environments. Although beamforming (BF) and adaptive noise
cancellation (ANC) techniques are robust in some conditions, they may degrade
the performance of the activation system by distorting or suppressing useful
signals. The authors propose a neural network architecture that uses several
input channels and an attention mechanism that allows the network to determine
the most useful channel or their combination. The improved quality of the
algorithm was demonstrated on two datasets: from a laboratory with controlled
conditions and from smart speakers in natural conditions. The proposed
algorithm was compared against several baselines in terms of the quality of
noise reduction metrics, KWS metrics, and computing resources in comparison
with existing solutions.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [270] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: 论文研究了通用AI模型开放性的定义及其对开发者的经济激励，分析了监管政策对模型发布和下游微调的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在监管框架（如欧盟AI法案）下，如何定义开源基础模型以平衡经济激励和开放性要求。

Method: 通过建模通用模型创建者（generalist）和微调者（specialist）的策略互动，分析不同开放性标准下的市场均衡。

Result: 模型性能基线决定了监管惩罚和开源阈值对通用模型发布策略的影响，提出了有效的监管措施范围。

Conclusion: 为AI治理中的开放性决策提供了理论基础，并支持实际开源政策的评估与优化。

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [271] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: 论文探讨了网络拍卖中的策略证明性，提出了两种单调分配规则（ID-MON和IP-MON），并解决了单需求组合网络拍卖的难题。


<details>
  <summary>Details</summary>
Motivation: 现有网络拍卖缺乏通用的策略证明分配规则，导致多单位需求拍卖中策略证明性难以实现。

Method: 提出ID-MON和IP-MON两种单调分配规则，并分析其策略证明支付规则的存在性和计算可行性。

Result: 证明了在ID-MON和IP-MON规则下，存在收益最大化的策略证明支付规则。

Conclusion: 通过ID-MON和IP-MON规则，解决了单需求组合网络拍卖的策略证明性问题。

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [272] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 论文研究了不可分割物品的公平分配问题，重点探讨了EFX和PMMS问题。通过构造实例证明了PMMS分配在某些情况下不存在，从而区分了EFX和PMMS。同时，证明了在三种特殊情况下公平分配的存在性，并提供了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决不可分割物品公平分配中的核心问题EFX和PMMS，填补理论空白并提供实际应用的可能性。

Method: 通过构造实例和理论证明，分析了EFX和PMMS分配的存在性，并针对三种特殊估值情况提出了构造性证明和算法。

Result: 证明了PMMS分配在某些情况下不存在，区分了EFX和PMMS；同时证明了三种特殊情况下的公平分配存在性，并提供了多项式时间算法。

Conclusion: 论文在公平分配领域取得了重要进展，为EFX和PMMS问题提供了新的理论支持，并展示了实际应用的可行性。

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


### [273] [Strategically Robust Game Theory via Optimal Transport](https://arxiv.org/abs/2507.15325)
*Nicolas Lanzetti,Sylvain Fricker,Saverio Bolognani,Florian Dörfler,Dario Paccagnan*

Main category: cs.GT

TL;DR: 论文提出了一种名为“战略鲁棒均衡”的新均衡概念，通过模糊集和最优传输方法，在不确定环境中保护代理决策，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 在多代理博弈中，代理面临来自不完全信息、有限计算和有限理性等多源不确定性，需要一种方法保护其决策免受这些不确定性的影响。

Method: 代理在模糊集内考虑最坏情况行为，使用最优传输方法操作化，定义战略鲁棒均衡。

Result: 战略鲁棒均衡在存在性、计算成本和性能上优于纳什均衡，并在实验中表现出更高的均衡收益。

Conclusion: 战略鲁棒均衡为多源不确定性下的博弈提供了有效的解决方案，并通过鲁棒化实现协调效应。

Abstract: In many game-theoretic settings, agents are challenged with taking decisions
against the uncertain behavior exhibited by others. Often, this uncertainty
arises from multiple sources, e.g., incomplete information, limited
computation, bounded rationality. While it may be possible to guide the agents'
decisions by modeling each source, their joint presence makes this task
particularly daunting. Toward this goal, it is natural for agents to seek
protection against deviations around the emergent behavior itself, which is
ultimately impacted by all the above sources of uncertainty. To do so, we
propose that each agent takes decisions in face of the worst-case behavior
contained in an ambiguity set of tunable size, centered at the emergent
behavior so implicitly defined. This gives rise to a novel equilibrium notion,
which we call strategically robust equilibrium. Building on its definition, we
show that, when judiciously operationalized via optimal transport,
strategically robust equilibria (i) are guaranteed to exist under the same
assumptions required for Nash equilibria; (ii) interpolate between Nash and
security strategies; (iii) come at no additional computational cost compared to
Nash equilibria. Through a variety of experiments, including bi-matrix games,
congestion games, and Cournot competition, we show that strategic robustness
protects against uncertainty in the opponents' behavior and, surprisingly,
often results in higher equilibrium payoffs - an effect we refer to as
coordination via robustification.

</details>


### [274] [The Root of Revenue Continuity](https://arxiv.org/abs/2507.15735)
*Sergiu Hart,Noam Nisan*

Main category: cs.GT

TL;DR: 论文证明了买家估值分布的小变化对可提取收入的影响有限，提出了基于Wasserstein距离的简洁不等式，并展示了如何通过“统一折扣”机制实现近似最优。


<details>
  <summary>Details</summary>
Motivation: 研究买家估值分布变化对收入的影响，为机制设计提供理论支持。

Method: 使用Wasserstein距离衡量估值分布差异，推导出收入变化的不等式，并提出“统一折扣”机制。

Result: 证明了收入变化的平方根与Wasserstein距离的平方根成正比，且“统一折扣”机制对接近的分布近似最优。

Conclusion: 论文提供了一个简洁通用的理论框架，说明估值分布的小变化对收入影响有限，并提出了实用的机制调整方法。

Abstract: In the setup of selling one or more goods, various papers have shown, in
various forms and for various purposes, that a small change in the distribution
of a buyer's valuations may cause only a small change in the possible revenue
that can be extracted. We prove a simple, clean, convenient, and general
statement to this effect: let X and Y be random valuations on k additive goods,
and let W(X,Y) be the Wasserstein (or "earth mover's") distance between them;
then sqrt(Rev(X))-sqrt(Rev(Y)) <= sqrt(W(X,Y)). This further implies that a
simple explicit modification of any optimal mechanism for X, namely, "uniform
discounting", is guaranteed to be almost optimal for any Y that is close to X
in the Wasserstein distance.

</details>


### [275] [General Matching Games](https://arxiv.org/abs/2507.15737)
*Felipe Garrido-Lucero,Rida Laraki*

Main category: cs.GT

TL;DR: 扩展了Garrido-Lucero和Laraki的匹配游戏模型，涵盖了一对多匹配市场和室友模型，提出了核心稳定且抗重新谈判的结果存在并可高效计算的条件。


<details>
  <summary>Details</summary>
Motivation: 解决传统匹配模型中缺乏对重新谈判的鲁棒性问题，并扩展模型以适应更广泛的匹配市场。

Method: 通过扩展模型到一对多和室友模型，提出两种框架，确保核心稳定和抗重新谈判的结果存在，并提供高效计算方法。

Result: 证明了核心稳定且抗重新谈判的结果存在，并可通过高效算法计算。

Conclusion: 扩展后的模型为更广泛的匹配市场提供了理论支持，并解决了重新谈判的鲁棒性问题。

Abstract: Matching games is a one-to-one two sided market model introduced by
Garrido-Lucero and Laraki, in which coupled agents' utilities are endogenously
determined as the outcome of a strategic game. They refine the classical
pairwise stability by requiring robustness to renegotiation and provide general
conditions under which pairwise stable and renegotiation-proof outcomes exist
as the limit of a deferred acceptance with competitions algorithm together with
a renegotiation process. In this article, we extend their model to a general
setting encompassing most of one-to-many matching markets and roommates models
and specify two frameworks under which core stable and renegotiation-proof
outcomes exist and can be efficiently computed.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [276] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 提出一种基于探针数据结构的神经渲染方法，高效重建复杂大场景，避免显式几何依赖。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法在大规模场景中效率低、显式几何维护成本高的问题。

Method: 利用稀疏图像重建多尺度隐式几何表示，通过探针数据结构存储深度信息，实现高效渲染。

Result: 方法显著降低计算成本，渲染效率与场景复杂度无关，适用于VR/AR。

Conclusion: 结合几何重建与新颖视图合成，为大规模场景渲染提供高效解决方案。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [277] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出了一种三阶段框架，通过单图像引导生成3D场景，解决多对象场景中的生成质量和一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 当前方法在从单RGB图像生成3D场景时，难以同时保证对象生成质量和场景一致性。

Method: 采用三阶段框架：1) 图像实例分割和修复；2) 伪立体视角构建以估计相机参数和场景深度；3) 通过点云距离优化布局参数。

Result: 在几何精度和纹理保真度上优于现有方法，且在场景布局合成中表现突出。

Conclusion: 该方法显著提升了3D场景生成的完整性和一致性，适用于多对象场景。

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [278] [Time Series Information Visualization -- A Review of Approaches and Tools](https://arxiv.org/abs/2507.14920)
*Evandro S. Ortigossa,Fábio F. Dias,Diego C. Nascimento,Luis Gustavo Nonato*

Main category: cs.GR

TL;DR: 综述探讨了时间序列数据的可视化技术，旨在通过视觉分析帮助用户理解动态行为和发现模式，同时提出了设计和理论指导。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据广泛存在于多个领域，但分析这类数据需要复杂工具。可视化技术能提升数据的可解释性，帮助数据科学家从原始数据中提取有用知识。

Method: 综述了时间序列数据的可视化技术和工具，整合多种分析方法，设计丰富的可视化系统。

Result: 提出了时间序列多特征数据的可视化设计指南，并总结了当前挑战和未来研究方向。

Conclusion: 时间序列可视化是理解动态数据的关键工具，未来需进一步解决多特征数据的可视化挑战。

Abstract: Time series data are prevalent across various domains and often encompass
large datasets containing multiple time-dependent features in each sample.
Exploring time-varying data is critical for data science practitioners aiming
to understand dynamic behaviors and discover periodic patterns and trends.
However, the analysis of such data often requires sophisticated procedures and
tools. Information visualization is a communication channel that leverages
human perceptual abilities to transform abstract data into visual
representations. Visualization techniques have been successfully applied in the
context of time series to enhance interpretability by graphically representing
the temporal evolution of data. The challenge for information visualization
developers lies in integrating a wide range of analytical tools into rich
visualization systems that can summarize complex datasets while clearly
describing the impacts of the temporal component. Such systems enable data
scientists to turn raw data into understandable and potentially useful
knowledge. This review examines techniques and approaches designed for handling
time series data, guiding users through knowledge discovery processes based on
visual analysis. We also provide readers with theoretical insights and design
guidelines for considering when developing comprehensive information
visualization approaches for time series, with a particular focus on time
series with multiple features. As a result, we highlight the challenges and
future research directions to address open questions in the visualization of
time-dependent data.

</details>


### [279] [Model Simplification through refinement](https://arxiv.org/abs/2507.15186)
*Dmitry Brodsky,Benjamin Watson*

Main category: cs.GR

TL;DR: 提出了一种适用于大型模型交互式简化的快速算法，保证在给定时间内生成可显示且质量良好的结果。


<details>
  <summary>Details</summary>
Motivation: 现有多边形网格简化算法速度慢或质量差，无法满足大型模型的交互式简化需求。

Method: 受向量量化文献中的分裂算法启发，从极粗糙近似开始反向简化，利用表面曲率近似指导过程。

Result: 算法速度快，能在限定时间内生成高质量简化模型。

Conclusion: 该算法为大型模型的交互式简化提供了有效解决方案。

Abstract: As modeling and visualization applications proliferate, there arises a need
to simplify large polygonal models at interactive rates. Unfortunately existing
polygon mesh simplification algorithms are not well suited for this task
because they are either too slow (requiring the simplified model to be
pre-computed) or produce models that are too poor in quality. These
shortcomings become particularly acute when models are extremely large. We
present an algorithm suitable for simplification of large models at interactive
speeds. The algorithm is fast and can guarantee displayable results within a
given time limit. Results also have good quality. Inspired by splitting
algorithms from vector quantization literature, we simplify models in reverse,
beginning with an extremely coarse approximation and refining it.
Approximations of surface curvature guide the simplification process.
Previously produced simplifications can be further refined by using them as
input to the algorithm.

</details>


### [280] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 提出了一种基于修复的框架，用于编辑点云表示的3D形状，通过结合结构指导和坐标混合算法，实现了局部编辑的同时保持全局一致性和形状身份。


<details>
  <summary>Details</summary>
Motivation: 自然语言为3D形状的局部细粒度编辑提供了直观接口，但现有方法难以在局部修改时保持全局一致性。

Method: 采用基于3D扩散模型的修复框架，引入部分条件形状作为结构指导，并提出推理时坐标混合算法以平衡重建与修复。

Result: 实验表明，该方法在保持原始形状保真度和遵循文本描述方面优于其他技术。

Conclusion: 该方法实现了细粒度编辑，同时避免了计算昂贵且不准确的反演过程。

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [281] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS是一个结合3D场景重建与语义理解的框架，通过将场景分解为独立对象并赋予语义ID，提升了对象级感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决3D Gaussian Splatting缺乏语义理解的问题，实现对象级感知。

Method: 将场景分解为独立对象，动态调整锚点并优化特征，使用分类损失强化语义约束。

Result: 在开放词汇和全景分割任务中优于现有方法，并支持网格提取和场景编辑。

Conclusion: ObjectGS成功统一了3D重建与语义理解，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [282] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 提出了一种离散化SDF方法，通过高斯样条实现高效渲染，避免了传统方法的复杂性和高内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决高斯样条在逆渲染中难以应用几何约束的问题，同时避免引入额外内存和复杂训练。

Method: 使用离散化SDF表示，通过SDF-to-opacity转换链接高斯不透明度，并引入投影一致性损失。

Result: 实验表明，该方法在高斯基逆渲染中表现优于现有方法，且无需额外内存。

Conclusion: 离散化SDF方法在保持高效渲染的同时，提升了逆渲染质量。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [283] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种名为Catalyst正则化的新方法，用于结构化剪枝，解决了传统正则化方法（如L1或Group Lasso）的幅度偏差问题，并通过理论证明和实验验证了其公平性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法在剪枝决策中存在幅度偏差和边界不稳定的问题，影响了剪枝的公平性和鲁棒性。

Method: 通过代数条件定义了剪枝操作保留模型性能的条件，并利用辅助催化剂变量构建了一种新型正则化方法。

Result: 实验证明，Catalyst剪枝算法在多种数据集和模型上优于现有方法，并表现出公平和鲁棒的剪枝特性。

Conclusion: Catalyst正则化提供了一种理论支持且实际有效的结构化剪枝方法，解决了传统方法的局限性。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [284] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种基于投影空间的新型剪枝策略IPPRO，通过PROscore衡量滤波器剪枝可能性，挑战了传统基于幅度的剪枝方法，实现了近乎无损的剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于幅度的剪枝方法限制了剪枝决策的能力，即使冗余的滤波器也可能因幅度较大而未被剪枝。本文旨在挑战这种幅度主导效应。

Method: 将滤波器置于投影空间，观察梯度下降运动中滤波器是否向原点移动，以此衡量剪枝可能性，并构建PROscore作为重要性评分。

Result: 提出的方法在剪枝后性能下降较小，经过微调后表现优异，实现了近乎无损的剪枝效果。

Conclusion: 本文打破了剪枝中“大小决定一切”的迷思，从理论和实证上扩展了基于重要性的剪枝方法的前沿。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [285] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种结合语言模型与自改进进化循环的程序合成方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在单次尝试中难以解决复杂程序合成任务，而传统进化方法受限于生成模型的固定能力。

Method: SOAR通过交替进行进化搜索和事后学习，利用LLM采样和优化候选方案，并通过微调提升模型能力。

Result: 在ARC-AGI基准测试中，SOAR显著提升了性能，解决了52%的公共测试集问题。

Conclusion: SOAR通过自改进循环和任务间正向迁移，为程序合成提供了高效解决方案。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [286] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 论文探讨了方向性作为人工神经网络的归纳偏置的作用，通过剪枝技术诱导方向性，而非硬编码，发现方向性是可学习的优势偏置。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环电路的启发，研究方向性是否对人工神经网络有帮助。

Method: 提出一种全连接感知层（等同于权重绑定的循环神经网络），并通过剪枝技术诱导方向性。

Result: 剪枝方案成功诱导神经元间信息流的拓扑排序，且不损害性能。

Conclusion: 方向性并非学习的前提，但可能是梯度下降和稀疏化可发现的优势偏置。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [287] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 该论文研究了潜在空间融合方法在预测抑郁症状中的表现，发现其优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型无法充分捕捉精神疾病数据的多模态复杂性，需要更先进的融合技术。

Method: 使用BRIGHTEN临床试验数据，比较了随机森林（RF）和潜在空间融合（CM）方法，评估了多模态数据的整合效果。

Result: CM在所有实验中都优于RF和线性回归，表现出更低的MSE和更高的R2，且避免了过拟合问题。

Conclusion: 潜在空间融合是多模态心理健康数据预测的有效方法，未来需进一步探索模型可解释性和个体化预测。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [288] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 提出了一种遗传算法，通过优化分类和回归任务的问题复杂度指标，生成具有不同难度级别的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 研究社区需要更先进的合成数据生成器来评估机器学习方法的优缺点，本研究旨在通过生成多样化复杂度的数据集来满足这一需求。

Method: 使用遗传算法优化分类任务的10个复杂度指标和回归任务的4个指标，通过线性特征投影调整合成数据集的复杂度。

Result: 实验表明，遗传算法能生成具有目标复杂度的数据集，且生成数据的复杂度与识别质量相关。

Conclusion: 该方法能有效生成多样化复杂度的数据集，为机器学习方法评估提供了可靠工具。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [289] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文提出了一种名为‘预测代表性’（PR）的公平性审计框架，通过皮肤癌分类器的案例研究，揭示了AI模型在不同肤色人群中的性能差异，并强调了公平性审计的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在医疗决策中的应用日益增多，但算法偏见和不公平结果的问题，尤其是对历史上边缘化群体的影响，仍然存在。

Method: 通过分析HAM10000数据集和哥伦比亚的BOSQUE测试集，评估AI皮肤癌分类器在不同肤色人群中的表现，并提出PR框架和外部可迁移性标准。

Result: 研究发现，尽管数据集中肤色比例均衡，但AI模型在深色皮肤人群中的表现显著较差。

Conclusion: 论文强调公平性审计的必要性，并提出了PR框架作为诊断AI系统结构性不平等的工具，推动数据驱动的医疗公平性讨论。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [290] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 论文提出了一种多代理滑雪租赁问题，扩展了经典滑雪租赁困境，考虑了代理的个体和共享成本，并设计了三种竞争比率的优化策略。


<details>
  <summary>Details</summary>
Motivation: 研究多代理环境下的滑雪租赁问题，解决代理在动态状态下的决策优化，扩展经典问题的应用场景。

Method: 定义了三种竞争比率（整体、状态依赖和个体理性），设计了确定性和随机化策略，包括状态感知阈值函数和分布。

Result: 对称策略优于非对称策略，提供了竞争比率的上限和下限，扩展了经典滑雪租赁问题的理论。

Conclusion: 研究为多代理环境下的群体决策提供了理论和实践指导，特别是在动态状态和不确定性下的优化。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [291] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究了两层神经网络的训练解，揭示了其隐藏层平滑激活函数的机制，证明了通用逼近性，并通过实验验证了解空间的“黑箱”特性。


<details>
  <summary>Details</summary>
Motivation: 理解通过反向传播算法获得的两层神经网络的训练解，特别是隐藏层使用平滑激活函数（如Sigmoid）时的机制。

Method: 通过构建泰勒级数展开、严格的节点偏序、平滑样条实现和平滑连续性限制四种主要原理进行分析。

Result: 证明了任意输入维度的通用逼近性，并通过实验验证了解空间的特性。

Conclusion: 研究不仅揭示了解空间的“黑箱”特性，还丰富了逼近理论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [292] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种名为FBE的方法，通过约束极端特征改进OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习中数据特征的偏态分布和极端特征导致基于距离的OOD检测方法性能受限。

Method: 使用数据集的统计特性识别并约束极端特征，扩大分布内外样本的距离。

Result: 在ImageNet-1k和CIFAR-10上达到最优性能。

Conclusion: FBE方法简单有效，显著提升了OOD检测能力。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [293] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 论文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性聚类，有效预测和利用大语言模型中的激活稀疏性，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的激活稀疏性为降低计算成本提供了机会，但直接预测神经元级别的激活模式计算代价高昂。

Method: 提出聚类框架，将相似激活模式分组为少量代表性聚类，预测聚类分配而非单个神经元状态。

Result: 聚类精度达79.34%，困惑度（PPL）最低为12.49，在保持模型质量的同时显著降低计算开销。

Conclusion: 该方法为未来激活模式预测奠定了基础，有望提升大规模语言模型的高效推理能力。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [294] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的毫米波MIMO系统波束对齐引擎（BAE），通过数字孪生技术和迁移学习减少数据收集开销，并利用SHAP和DkNN提高透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在6G愿景下，毫米波系统的可解释性和鲁棒性对建立信任和可靠性能至关重要。现有深度学习方法面临数据收集开销大、硬件限制、缺乏可解释性及对抗攻击等问题。

Method: 提出一种BAE框架，利用宽波束的RSSI测量预测最佳窄波束，减少波束扫描开销；通过数字孪生生成合成数据，结合迁移学习优化模型；使用SHAP和DkNN提高透明度和鲁棒性。

Result: 实验表明，该框架减少70%真实数据需求、62%波束训练开销，异常检测鲁棒性提高8.5倍，接近最优频谱效率。

Conclusion: 该框架在减少开销的同时，实现了透明且鲁棒的决策，优于传统基于softmax的深度学习方法。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [295] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 本文提出了一种半监督联邦学习框架SSFL-DCSL，通过双重对比损失和软标签解决数据分布不均和标签稀缺问题，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据且成本高，而数据分布差异可能影响模型性能。

Method: 设计了基于拉普拉斯分布的样本加权函数、双重对比损失（局部和全局），并通过加权平均和动量更新原型共享知识。

Result: 在仅10%数据标注的情况下，SSFL-DCSL比现有方法准确率提高1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据分布不均和标签稀缺问题，提升了模型性能。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [296] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 开发了一种用于聋人或听力障碍者的深度学习系统，实时定位和识别声源，填补了当前研究的空白。


<details>
  <summary>Details</summary>
Motivation: 为弱势群体提供技术支持，利用机器学习填补现有研究的不足。

Method: 系统包括三个组件：JerryNet（CNN架构确定声源方向）、音频分类（基于CLAP模型）、多模态集成模型（结合音频、视觉和文本数据）。

Result: JerryNet精度91.1%，CLAP模型在自定义和AudioSet数据集上分别达到98.5%和95%准确率，多模态模型的cIoU为0.892。

Conclusion: 研究为新一代无障碍设备奠定了基础，具有广阔的应用前景。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [297] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 论文提出B4模型，通过结合价格序列和外部信号，建模牛熊动态中的投资者偏见与行为适应关系，提升市场趋势预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场动态复杂，受历史价格和外部叙事（如新闻、社交媒体情绪）影响，投资者偏见使建模困难。本文探索牛熊机制在投资者驱动市场中的作用。

Method: 提出B4模型，将价格序列和外部信号嵌入共享潜在空间，通过惯性配对和双竞争机制建模偏见驱动的非对称性和行为惯性。

Result: 实验显示B4在预测市场趋势上表现优异，并提供偏见、行为和市场动态关系的可解释性。

Conclusion: B4模型有效捕捉市场异质性，为理解投资者偏见与市场动态提供新视角。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [298] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 该研究通过测试时适应（TTA）技术解决音频分类中的域偏移问题，提出改进版CoNMix方法，在噪声背景下表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中的域偏移问题，特别是在音频分类中由背景噪声引起的性能下降。

Method: 采用TTT、TENT和CoNMix三种TTA方法，并在AudioMNIST和SpeechCommands V1数据集上测试其性能。

Result: 改进版CoNMix在噪声背景下分类准确率最高（如10 dB运动自行车噪声下错误率为5.31%）。

Conclusion: 该研究首次将TTA技术应用于音频分类中的域偏移问题，改进版CoNMix表现最佳。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [299] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种无需训练的KV缓存优化方法，通过梯形状KV缓存模式和迭代压缩机制，提升LLMs的长距离建模能力和连续生成效率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，LLMs中的KV对数量激增，导致效率瓶颈，需要一种方法同时解决长距离建模和内存不足问题。

Method: LaCache采用梯形状KV缓存模式（跨层存储KV对）和迭代压缩机制（动态压缩旧缓存），以固定存储预算提升性能。

Result: 实验验证LaCache在多种任务、基准和LLM模型中有效增强了长距离能力和连续生成效率。

Conclusion: LaCache为LLMs的长距离建模和连续生成提供了一种高效且无需训练的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [300] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，通过非线性效用聚合和几何感知查询选择解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，提高用户偏好建模的效率。

Method: 使用Choquet积分建模用户偏好，结合几何感知查询选择和分支定界策略。

Result: 在UCI数据集上表现优于现有方法（如ChoquetRank），以更少的用户交互实现更高的排名准确性。

Conclusion: 该方法有效解决了模式爆炸问题，提升了用户交互效率和排名准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [301] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本研究提出了一种基于AI的框架，用于计算绿色氢产量和选址适宜性指数，结合了无监督聚类、监督学习和SHAP算法，模型预测准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 在缺乏直接氢产量数据的情况下，为太阳能丰富的干旱地区提供一种客观、可重复的绿色氢选址方法。

Method: 采用多阶段AI框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，整合气象、地形和时间数据。

Result: 模型预测准确率为98%，显示水接近度、海拔和季节变化是阿曼绿色氢选址的最关键因素。

Conclusion: 该研究为数据稀缺地区提供了可扩展的工具，支持绿色氢基础设施规划和决策。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [302] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种新的POGM方法，通过梯度轨迹匹配解决领域泛化问题，避免了梯度波动和高计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在梯度匹配中存在梯度波动和高计算开销的问题，需要一种更高效且稳定的解决方案。

Method: 利用梯度轨迹作为数据，在元学习器中独立训练，最大化梯度内积同时限制梯度偏离经验风险最小化轨迹。

Result: 在DomainBed数据集上表现出竞争力，同时保持计算效率。

Conclusion: POGM方法有效解决了梯度波动和计算开销问题，实现了领域泛化的高效学习。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [303] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M是最大的纳米材料-蛋白质相互作用数据集，结合NanoProFormer模型，通过多模态学习预测亲和力，显著优于单模态方法，并减少实验依赖。


<details>
  <summary>Details</summary>
Motivation: 纳米材料在医学和环境科学中的应用潜力受限于其与蛋白质相互作用的复杂性，现有模型泛化能力不足且数据集有限。

Method: 提出NanoPro-3M数据集（320万样本，3.7万蛋白质），开发NanoProFormer模型，通过多模态表示学习预测亲和力。

Result: 模型表现优于单模态方法，能处理缺失特征和未知样本，并识别冠形成的关键因素。

Conclusion: 该研究为高性能、泛化的纳米材料-蛋白质相互作用预测奠定了基础，加速了体外应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [304] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，结合了扩散映射的几何直觉与线性嵌入的计算优势，在特定数据集上优于PCA。


<details>
  <summary>Details</summary>
Motivation: 结合非线性扩散映射的几何优势与线性方法的计算效率和可解释性。

Method: 通过线性近似扩散映射核构建LDM，并在合成和真实数据集上验证其性能。

Result: LDM在具有明确流形结构的高维数据中优于PCA，而PCA在噪声或方差主导的场景中更优。

Conclusion: LDM是一种有前景的线性降维技术，具有理论和实际应用的扩展潜力。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [305] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于单轮反馈的强化学习方法（UFO），用于提升大型推理模型在多轮问题解决中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在单轮训练中表现良好，但在多轮推理和反馈修正中效果不佳，导致模型重复回答。论文旨在探索模型是否能在多轮环境中学习和反思。

Method: 引入单轮反馈作为观察（UFO），利用简单的用户反馈（如“再试一次”）进行多轮强化学习训练，同时设计奖励结构以优化模型回答。

Result: 实验表明，UFO在保持单轮性能的同时，将多轮推理准确率提升高达14%，并增强模型对反馈的反应能力。

Conclusion: UFO是一种简单有效的多轮强化学习方法，能显著提升模型在多轮问题解决中的表现。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [306] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一个基于元学习的动态防御框架，通过实时选择最优聚合规则来应对联邦学习中的模型中毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受模型中毒攻击，现有静态防御方法在自适应攻击或异构数据环境下效果有限。

Method: 设计了一个轻量级上下文赌博机代理，动态选择防御规则，并通过实时诊断指标优化策略。

Result: 实验表明，静态规则无法普遍适用，而FedStrategist能在多样化场景中学习到更优策略，包括对抗隐蔽攻击。

Conclusion: FedStrategist提供了一种可控、实用的方法，平衡性能与安全性，为去中心化AI系统提供了新思路。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [307] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 论文提出首个可推广的框架，用于提升深度伪造检测中的个体公平性，填补了文献中的空白。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用带来了显著风险，而现有检测方法在个体公平性方面存在不足，尤其是对相似个体的预测一致性未被充分研究。

Method: 提出一个可集成到现有深度伪造检测器中的框架，以增强个体公平性和泛化能力。

Result: 在多个主流深度伪造数据集上的实验表明，该方法显著提升了个体公平性，同时保持了强大的检测性能。

Conclusion: 该研究填补了深度伪造检测中个体公平性的空白，为未来研究提供了重要方向。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [308] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 该研究开发并验证了四种机器学习模型，用于预测环形几何结构中的临界热通量（CHF），显著优于传统经验方法。


<details>
  <summary>Details</summary>
Motivation: 临界热通量（CHF）的准确预测对反应堆安全分析至关重要，传统经验方法存在误差大、缺乏可解释性和数据稀缺性问题。

Method: 研究使用CTF子通道代码，基于三种经验模型（Biasi、Bowring、Katto）开发了四种混合机器学习模型，利用577个环形实验数据点进行训练和测试。

Result: 机器学习模型的平均相对误差低于3.5%，显著优于经验模型的26%以上误差。

Conclusion: 混合机器学习方法在环形几何结构中表现出更高的准确性和可靠性，优于传统经验方法。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [309] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 论文探讨了通过影响函数近似方法过滤噪声训练数据，提升语言模型微调性能，实验显示过滤后准确率提升1.5%。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据集通常存在噪声，影响模型性能，需有效方法检测和过滤有害数据。

Method: 使用共轭梯度近似影响函数过滤数据集，并在TL;DR数据集上进行实验验证。

Result: 过滤10%训练数据后，准确率提升1.5%；梯度相似性在检测有益数据时优于影响函数。

Conclusion: 局部曲率对检测有害数据重要，但对有益数据识别作用较小。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [310] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection是一种新的参数高效微调方法，通过调整解码器块级别的表示而非单个权重矩阵，显著减少可训练参数数量，并在自然语言生成任务中优于LoRA。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对大型语言模型（如GPT2）微调时如何更高效地利用跳跃连接的重新思考，尤其是在模型层数增加的背景下。

Method: 提出Solo Connection方法，通过可训练的线性变换在零向量和任务特定表示之间平滑插值，实现解码器块级别的表示调整。

Result: Solo Connection在E2E自然语言生成任务中优于LoRA，并将可训练参数减少59%（相比LoRA）和99%以上（相比全微调）。

Conclusion: Solo Connection为大型语言模型的高效微调提供了一种新方法，尤其在模型层数增加的背景下具有潜力。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [311] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET是一种新颖的增量因果图学习框架，用于实时网络攻击检测，通过动态更新因果图来适应系统行为变化，显著提高了检测准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 网络攻击对关键基础设施的威胁日益严重，传统实时异常检测方法因高数据方差和类别不平衡导致误报率高，且现有因果图方法无法适应实时动态数据分布。

Method: INCADET包含三个模块：早期症状检测（通过边权重分布变化检测系统状态转变）、增量因果图学习（通过经验回放和边强化更新因果结构）、因果图分类（使用GCN分类系统状态）。

Result: 在真实关键基础设施数据集上的实验表明，INCADET在准确性、鲁棒性和适应性上优于静态因果和深度时序基线方法。

Conclusion: INCADET通过增量学习动态因果图，有效解决了实时网络攻击检测中的动态适应和知识保留问题。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [312] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单测试时缩放方法，发现其主要效果来自于通过限制最大长度实现的缩放，而通过追加“Wait”实现的缩放会导致不一致性。o1类模型通过强化学习自然提升性能，而简单测试时缩放则限制了性能上限。


<details>
  <summary>Details</summary>
Motivation: 研究简单测试时缩放方法的实际效果及其与o1类模型性能提升的区别。

Method: 分析简单测试时缩放方法，包括限制最大长度和追加“Wait”两种方式，并与o1类模型的自然性能提升进行对比。

Result: 限制最大长度的缩放效果显著，而追加“Wait”会导致模型输出不一致。o1类模型通过强化学习自然提升性能，超越简单测试时缩放。

Conclusion: 简单测试时缩放虽能模拟o1类模型的缩放行为，但无法实现其性能提升目标，真正的缩放应追求超越原始性能。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [313] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 本文提出了一种结合强化学习和深度学习的方法，用于解决大规模随机优化问题，特别是在供应链中的多源多周期库存管理问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模随机优化问题的复杂性，尤其是在供应链管理中，传统方法难以处理复杂的物理约束和随机过程。

Method: 通过预训练的深度学习模型模拟和组合随机过程，并结合强化学习进行优化，同时引入约束协调机制预测双成本。

Result: 该方法将复杂的供应链过程分解为可扩展和可组合的深度学习模块，提高了在真实数据集上的性能。

Conclusion: 该方法为大规模随机优化问题提供了有效的解决方案，并指出了未来研究的开放性问题。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [314] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC是一种基于重参数化掩码扩散模型的结构化节点分类方法，通过变分EM框架学习节点标签的联合分布，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法假设节点标签条件独立，忽略了图结构中标签的相关性，ReDiSC旨在解决这一问题。

Method: 使用重参数化掩码扩散模型估计节点标签的联合分布，结合变分EM框架优化。

Result: ReDiSC在多种图上表现优于现有GNN、标签传播和扩散模型，且能扩展到大规模数据集。

Conclusion: ReDiSC在结构化节点分类任务中具有显著优势，尤其适用于大规模图数据。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [315] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 提出了一种联邦强化学习框架FRL-EH，解决局部环境统计异质性问题，通过聚合经验学习全局策略并保护隐私。提出新目标函数优化全局策略，确保鲁棒性。算法FedRQ理论收敛，并扩展至连续状态空间。实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦强化学习中局部环境统计异质性及隐私保护问题，优化全局策略的鲁棒性。

Method: 提出FRL-EH框架及新目标函数，设计FedRQ算法并理论证明其收敛性，扩展至连续状态空间。

Result: FedRQ在异质环境中表现优异，优于现有方法。

Conclusion: FRL-EH框架及FedRQ算法有效解决异质性问题，具有理论和实践优势。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [316] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种称为“glitches”的新不可靠行为来源，它可能显著影响具有陡峭决策边界的AI模型的可靠性。通过形式化定义和算法搜索，证明了检测glitches的NP完全性，并提出了一种基于MILP编码的有效搜索方法。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型的决策可信、可靠且输出一致是当前关键任务，而glitches作为一种新的不可靠行为可能破坏模型的一致性。

Method: 形式化定义了glitches，并通过算法搜索（基于MILP编码）在梯度提升决策树（GBDT）模型中检测glitches。

Result: 证明了glitches在广泛使用的GBDT模型中普遍存在，且检测问题对深度为4的树集成是NP完全的。

Conclusion: glitches的存在通常表明模型在局部区域的不一致性，提出的算法为检测和解决此类问题提供了有效工具。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [317] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 论文提出了一种生成式知识蒸馏框架GenDD，通过Split Tokenization和Distribution Contraction技术解决了高维优化和缺乏标签监督的问题，在无监督和监督设置下均取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）通常面临高维优化和标签监督不足的挑战，本文旨在通过生成式方法解决这些问题。

Method: 提出GenDD框架，结合Split Tokenization实现稳定无监督KD，并通过Distribution Contraction技术整合标签监督。

Result: 在ImageNet验证集上，无监督设置下超越KL基线16.29%；监督设置下，ResNet-50达到82.28%的top-1准确率。

Conclusion: GenDD在无监督和监督知识蒸馏中均表现出色，为多任务学习提供了高效的梯度级替代方案。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [318] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了SDSC，一种用于时间序列自监督表示学习的结构感知度量函数，解决了传统距离目标（如MSE）在信号处理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法（如MSE）对幅度敏感、对波形极性不变且尺度无界，影响语义对齐和可解释性。

Method: SDSC基于Dice相似系数，通过量化信号的结构一致性来改进表示学习，并提出混合损失函数结合SDSC与MSE。

Result: 实验表明，SDSC在预测和分类任务中表现优于或与MSE相当，尤其在领域内和低资源场景。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [319] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 论文提出了一种基于正未标记（PU）学习的方法，用于在观察性研究中从未标记数据中识别控制单元，从而估计平均处理效应（ATE）。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，缺乏明确标记的控制单元是一个常见挑战，这限制了因果效应的估计。

Method: 使用PU学习框架，仅基于已处理的（正）单元从未标记数据中高置信度地识别控制单元，并通过模拟和真实数据评估方法。

Result: PU学习能成功从未标记数据中识别控制单元，并估计接近真实值的ATE。

Conclusion: 该方法为观察性因果推断提供了新工具，尤其适用于难以进行随机实验的领域。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [320] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于最大因果熵逆强化学习的方法，用于无限时域平稳平均场博弈，通过再生核希尔伯特空间建模未知奖励函数。


<details>
  <summary>Details</summary>
Motivation: 现有平均场博弈的逆强化学习方法通常限制奖励函数为固定基函数的线性组合，且多基于有限时域。本文旨在解决这些局限性。

Method: 引入拉格朗日松弛将问题转化为无约束对数似然最大化，并通过梯度上升算法求解。理论分析证明了算法的平滑性。

Result: 在平均场交通路径博弈中，方法能准确恢复专家行为。

Conclusion: 该方法在建模非线性奖励结构和无限时域问题上具有优势，实验验证了其有效性。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [321] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedClusAvg的隐私保护联邦学习框架，用于在非独立同分布（Non-IID）和资源受限的环境中提高虚假数据注入攻击（FDIAs）的检测能力。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击（FDIAs）对智能电网构成严重安全威胁，传统集中式训练方法存在隐私风险、数据共享限制和高传输成本问题。

Method: 提出FedClusAvg框架，结合基于聚类的分层采样和分层通信（客户端-子服务器-服务器）机制，实现局部训练和加权参数聚合。

Result: 在基准智能电网数据集上的实验表明，FedClusAvg提高了异构数据分布下的检测精度，并显著减少了通信轮次和带宽消耗。

Conclusion: 该研究为大规模分布式电力系统中安全高效的FDIAs检测提供了有效解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [322] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 本文追溯了自注意力机制的概念起源，将其视为基于亲和矩阵的广义计算原则的特例，并与无限特征选择（Inf-FS）方法进行了对比。


<details>
  <summary>Details</summary>
Motivation: 揭示自注意力机制与更广泛的基于亲和矩阵的计算范式之间的联系，统一不同领域的机器学习研究。

Method: 通过分析自注意力机制与Inf-FS的相似性，指出两者均依赖于亲和矩阵，但定义和应用方式不同。

Result: 自注意力是Inf-FS的单跳特例，两者共享对成对关系的推理结构。

Conclusion: 将自注意力置于基于亲和矩阵的计算范式中，为不同模型和任务提供了统一的数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [323] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN是一种高效、低成本的GNN框架，能在单GPU上处理1000亿规模的图数据，性能提升13.8%。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在效率和准确性之间难以平衡，尤其在大规模图上计算和内存需求高。

Method: 提出LPS-GNN框架，结合LPMetis图分区算法和子图增强策略，兼容多种GNN算法。

Result: 在公开和真实数据集上性能提升8.24%至13.89%，优于现有SOTA方法。

Conclusion: LPS-GNN高效且灵活，已在腾讯平台成功部署，适用于大规模图学习任务。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [324] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer-GAN和MILET的新框架，用于无人机飞行状态分类，显著提升了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类方法在动态无人机环境中缺乏鲁棒性和泛化能力，而现有SOTA模型需要大数据集和高计算成本。

Method: 集成Transformer编码器捕捉长期时间依赖，GAN模块生成合成数据增强数据集，MILET聚焦关键输入段减少噪声。

Result: 在DroneDetect和DroneRF数据集上分别达到96.5%和98.6%的准确率，优于其他SOTA方法。

Conclusion: 该框架在计算效率和泛化能力上表现优异，适合资源受限环境中的实时部署。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [325] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出了一种基于通信与计算（C²）感知的联邦学习框架，通过优化批量大小控制来降低端到端学习延迟，同时确保收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在6G网络中部署时面临的高维模型更新开销和设备异构性带来的低延迟需求挑战。

Method: 设计了一种C²感知框架，通过平衡批量大小与通信轮次的权衡，优化端到端延迟。

Result: 实验证明，该框架在慢衰落和快衰落场景下均优于传统批量大小调整方案。

Conclusion: 提出的策略有效解决了联邦学习中的延迟问题，同时适应设备异构性。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [326] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 论文提出了一种多项式时间确定性算法，用于近似计算k-子空间中位数，其近似因子为√d，且运行时间与输入规模成多项式关系。


<details>
  <summary>Details</summary>
Motivation: 传统的k-PCA通过最小化平方欧氏距离（ℓ2,2范数）来近似k-子空间均值，但对噪声和异常值敏感。而k-子空间中位数（ℓ2,1混合范数）更鲁棒，但因其非凸性难以高效近似。

Method: 提出了一种多项式时间确定性算法，避免了运行时间和近似因子随k指数增长的问题。

Result: 算法的近似因子为√d，运行时间与输入规模成多项式关系，适用于其他相关问题（如ℓ2,z范数）。

Conclusion: 该技术有望应用于更多相关问题（如处理异常值/稀疏性），并提供了开源代码和真实数据集实验结果。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [327] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 论文提出SST-CL框架，结合空间-时间变换器和课程学习，解决EEG情感识别中的非平稳时空模式整合和动态情感强度适应问题。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在脑机通信系统中至关重要，但面临非平稳时空模式整合和动态情感强度变化的挑战。

Method: SST-CL框架包含空间编码器（建模通道间关系）和时间编码器（通过窗口注意力捕获多尺度依赖），并采用强度感知课程学习策略动态调度样本。

Result: 在三个基准数据集上表现优异，消融实验验证了框架各组件和课程学习机制的必要性。

Conclusion: SST-CL框架有效整合时空特征并适应动态情感强度，为EEG情感识别提供了新方法。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [328] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为Rec-AD的高效框架，结合Tensor Train分解和深度学习推荐模型，用于智能电网中的虚假数据注入攻击检测，显著提升计算效率和实时性能。


<details>
  <summary>Details</summary>
Motivation: 智能电网中虚假数据注入攻击检测的深度学习模型面临计算和内存负担增加的问题，尤其是在大规模工业数据集中，限制了检测效率。

Method: Rec-AD框架通过嵌入压缩、索引重排序优化数据访问，以及减少内存通信开销的流水线训练机制，提升训练和推理效率。

Result: 实验结果表明，Rec-AD显著提高了计算吞吐量和实时检测性能，缩小了攻击窗口并增加了攻击者成本。

Conclusion: Rec-AD增强了边缘计算能力和可扩展性，为智能电网安全提供了强有力的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [329] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为AD-GCL的新型图对比学习框架，旨在解决现有方法在结构不平衡网络中检测尾部异常节点时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在异常检测中过度关注整体性能，而忽视了对结构不平衡（如幂律分布网络）的鲁棒性，导致尾部异常节点检测效果不佳。

Method: AD-GCL采用邻居修剪策略过滤噪声边，并通过从头部节点对齐伪造尾部节点来提升尾部异常检测。此外，通过异常引导的邻居补全扩大尾部节点的感知范围，并引入原始图与增广图的一致性损失。

Result: 在多个数据集上的实验表明，AD-GCL在整体、头部和尾部节点的异常检测中均表现出优越性能。

Conclusion: AD-GCL显著提升了图异常检测在结构不平衡网络中的鲁棒性，尤其是在尾部异常节点的检测上表现突出。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [330] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 提出了一种新的垃圾文本检测框架GCC-Spam，通过字符相似性网络、对比学习和GAN生成伪样本，解决了对抗策略和标注数据稀缺问题，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾文本的指数增长需要强大的检测机制，以应对信息泄露和社会不稳定等风险。

Method: 1. 字符相似性网络捕捉拼写和语音特征；2. 对比学习优化潜在空间距离；3. GAN生成伪样本缓解数据稀缺。

Result: 在真实数据集上实验显示，GCC-Spam优于基线方法，检测率更高且所需标注数据更少。

Conclusion: GCC-Spam框架有效解决了垃圾文本检测中的对抗策略和数据稀缺问题，提升了检测性能。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [331] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 论文提出GUI-G²，一种基于高斯分布的奖励框架，用于改进GUI定位任务中的稀疏奖励问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在GUI定位中使用二元奖励，忽略了空间交互的连续性，而人类点击行为呈现高斯分布特征。

Method: GUI-G²通过高斯点奖励和覆盖奖励建模连续分布，并引入自适应方差机制处理不同尺寸元素。

Result: 在多个基准测试中，GUI-G²显著优于UI-TARS-72B，最高提升24.7%。

Conclusion: 连续建模提高了对界面变化的鲁棒性和泛化能力，为GUI交互任务提供了新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [332] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 论文提出了一种名为CPAC的新型架构，结合VAE-GAN和原型注意力机制，用于解决信用卡欺诈检测中的类别不平衡问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测因数据类别不平衡和欺诈模式隐蔽而具有挑战性，现有生成模型（如GANs、VAEs）在仅处理少数类数据时易导致分类器过自信和潜在聚类分离不佳。

Method: 提出CPAC架构，结合原型注意力机制和VAE-GAN编码器，优化潜在空间结构，并与传统过采样方法（如SMOTE）和生成模型进行对比。

Result: CPAC在F1-score（93.14%）和召回率（90.18%）上表现优异，同时改善了潜在聚类分离。

Conclusion: CPAC通过分类器引导的潜在空间优化，显著提升了欺诈检测性能，为表示学习提供了新思路。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [333] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 论文研究了实时生成AI（RTGen）工作负载在异构SoC平台上的调度问题，分析了不同调度策略对性能的影响，并强调了动态异构调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型（如LLMs）在实时多模型应用中的集成，RTGen工作负载的调度复杂性和性能影响尚未充分探索。

Method: 在AMD的Ryzen AI异构SoC上构建多模型场景，评估五种调度策略对实时指标和LLM性能的影响。

Result: 调度决策显著影响性能（如截止时间违规率平均差异达41.7%），需动态异构调度策略。

Conclusion: 动态异构调度是实现高性能RTGen应用的关键。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [334] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 论文提出了LeanTree，一种白盒方法，通过分解复杂证明状态为简单分支，提升自动定理证明的效率。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明（ATP）因状态和动作空间庞大而具有挑战性，现有的大语言模型（LLMs）缺乏正确性保证，白盒方法相对落后。

Method: 引入LeanTree，包括基于Lean 4的工具和分解后的中间状态数据集，支持并行搜索和状态重用。

Result: 初步结果表明白盒方法在某些场景下优于黑盒方法。

Conclusion: LeanTree为ATP提供了更高效的白盒解决方案，具有简化评估、丰富训练数据等优势。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [335] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID是一个统一的持续学习框架，解决了任务无关推理中的潜在遗忘和提示内存爆炸问题，通过任务感知解码和梯度提示选择策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法假设任务感知推理并维护任务特定提示列表，限制了可扩展性并隐藏了潜在遗忘。

Method: GRID结合任务感知解码机制（利用代表性输入、自动任务识别和约束解码）和梯度提示选择策略（压缩低信息量提示为聚合表示）。

Result: 在短序列、长序列和负迁移基准测试中，GRID显著改善后向迁移，减少遗忘任务达80%，优于现有方法。

Conclusion: GRID通过统一框架有效解决了持续学习中的关键问题，提升了模型的可扩展性和记忆效率。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [336] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种基于控制屏障函数（CBFs）的安全分层多智能体强化学习（HMARL）方法，用于解决多智能体安全关键系统中的安全策略学习问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体安全关键系统中，每个智能体需要始终满足安全要求，同时与其他智能体协作完成任务。

Method: 采用分层方法，将强化学习问题分解为高层学习联合协作行为和低层学习安全个体行为，并提出了基于技能的HMARL-CBF算法。

Result: 在复杂环境中验证了方法的有效性，显著提高了安全性（接近完美的成功率/安全率）和性能。

Conclusion: 该方法在多智能体安全导航任务中表现优异，优于现有方法。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [337] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 研究可训练有理激活函数在强化学习和持续学习中的表现，发现其灵活性与稳定性之间存在权衡，并提出约束变体以提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索可训练有理激活函数在动态非平稳环境中的表现及其对训练稳定性的影响。

Method: 提出一种约束变体的有理激活函数，限制输出缩放以平衡表达能力和稳定性。

Result: 在MetaWorld和DMC环境中，约束变体提升了训练稳定性和性能；在持续学习任务中，约束影响了表达能力和长期记忆的平衡。

Conclusion: 可训练有理激活函数在连续控制任务中表现出表达性与稳定性的权衡，约束设计能有效提升性能。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [338] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA算法通过EKFAC预条件器和Neumann级数迭代，高效近似iHVP，显著提升训练数据归因性能。


<details>
  <summary>Details</summary>
Motivation: 梯度基础的TDA方法（如影响函数和展开微分）需要近似iHVP，计算效率低且难以准确实现。

Method: 提出ASTRA算法，结合EKFAC预条件器和Neumann级数迭代，高效且准确地近似iHVP。

Result: ASTRA比传统方法更易调参、迭代次数更少、精度更高，显著提升TDA性能。

Conclusion: ASTRA通过改进iHVP近似精度，为TDA提供了更高效和准确的解决方案。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [339] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 提出了一种新框架，通过整合多个近优模型的PDP来解释模型不确定性，提升可解释AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动机器学习系统通常只关注单一最优模型，忽略了解释不确定性，这在以人为中心的AI中至关重要。

Method: 提出Rashomon PDP框架，通过聚合Rashomon集中多个近优模型的PDP，捕捉解释变异性。

Result: 实验表明，Rashomon PDP在大多数情况下仅覆盖最佳模型PDP的70%以下，突显单一模型解释的局限性。

Conclusion: Rashomon PDP通过补充被忽略的信息，提升了模型解释的可靠性和可信度，尤其适用于高风险领域。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [340] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 论文提出两种高斯过程采样方法（随机傅里叶特征和路径条件采样），用于支持全局敏感性分析和优化任务，并通过数值实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 高保真模拟和物理实验成本高昂，限制了其在全局敏感性分析和优化中的应用，因此需要高效的代理模型（如高斯过程）来提供不确定性预测。

Method: 介绍了随机傅里叶特征和路径条件采样两种方法，用于生成高斯过程的后验样本，并简要描述其他替代方法。

Result: 通过数值实验验证了所提采样方法在全局敏感性分析、单目标和多目标优化中的成功应用。

Conclusion: 论文提出的采样方法为工程优化中的不确定性决策提供了有效工具，扩展了高斯过程的应用范围。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [341] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 本文研究了基于互信息技能学习（MISL）的自监督特征学习方法，特别是对比后继特征（CSF）方法，证明了其能从状态和像素中恢复环境的真实特征。


<details>
  <summary>Details</summary>
Motivation: 探索MISL中表示和互信息参数化的理论作用，填补了强化学习中表示学习可识别性保证的空白。

Method: 通过理论分析和实验验证，研究了CSF方法的可识别性，并探讨了不同互信息目标和熵正则化的影响。

Result: 证明了CSF能线性恢复环境的真实特征，并在MuJoCo和DeepMind Control中进行了实证验证。

Conclusion: CSF为强化学习中的表示学习提供了首个可识别性保证，揭示了互信息目标和熵正则化的潜在问题。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [342] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 扩散模型在数据受限场景下优于自回归模型，尤其在计算资源充足时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在数据受限情况下是否比自回归模型更具优势。

Method: 系统研究掩码扩散模型在数据受限环境下的表现，并与自回归模型对比。

Result: 扩散模型在数据稀缺时表现更好，验证损失更低，下游任务性能更优。

Conclusion: 当数据成为瓶颈时，扩散模型是自回归模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


### [343] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一种新型多模态框架，结合稀疏时间序列的胸部X光和临床数据，预测ICU患者异常X光结果，提前12小时提供预警。


<details>
  <summary>Details</summary>
Motivation: ICU患者需要及时监测和干预，但现有胸部X光分析工具无法捕捉时间动态，限制了其效用。

Method: CXR-TFT通过视觉编码器生成潜在嵌入，与高频临床数据对齐，利用Transformer模型预测未来X光结果。

Result: 在2万名ICU患者的回顾性研究中，CXR-TFT能提前12小时高精度预测异常X光结果。

Conclusion: CXR-TFT通过时间分辨率高的预测能力，有望改善急性呼吸窘迫综合征等时间敏感病症的管理和临床结果。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [344] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 本文研究了语言模型中的记忆化问题，探讨了记忆化是否可以通过最优学习避免，以及隐私威胁是否被夸大。通过重新审视现有记忆化度量方法，并引入新的上下文记忆化概念，实验表明记忆化度量存在分歧，最优学习无法完全避免记忆化，且改进学习会减少某些记忆化形式。


<details>
  <summary>Details</summary>
Motivation: 研究记忆化在语言模型中的影响，探讨其是否可以通过最优学习避免，以及隐私威胁是否被夸大。

Method: 重新审视了基于回忆和反事实记忆化的现有度量方法，并提出了新的上下文记忆化概念。实验在18个语言模型和多种形式语言上进行。

Result: 发现记忆化度量方法对字符串的记忆化顺序存在分歧；最优学习无法完全避免记忆化；改进学习会减少上下文和反事实记忆化，但增加基于回忆的记忆化。

Conclusion: 记忆化在最优学习中不可避免，但某些记忆化形式（如基于回忆的记忆化）可能被夸大，不一定构成隐私威胁。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [345] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think是一个统一的强化学习框架，通过结合规则奖励和生成偏好信号提升LLM性能，课程学习显著优于联合训练和模型合并。


<details>
  <summary>Details</summary>
Motivation: 通用人工智能需要LLM在多样化任务中表现优异，但现有方法如SFT存在泛化不足问题。

Method: 提出Omni-Think框架，结合规则奖励和LLM-as-a-Judge评估，采用课程学习策略从结构化到开放任务逐步训练。

Result: 课程学习在四个领域中性能提升5.2%（相比联合训练）和9.1%（相比模型合并）。

Conclusion: 任务感知采样和混合监督对RL后训练LLM的扩展至关重要。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [346] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 论文探讨了利用大型语言模型（LLM）在金融知识图谱的子图上进行反洗钱（AML）推理的潜力。


<details>
  <summary>Details</summary>
Motivation: 洗钱涉及的实体复杂且相互关联，需要基于图数据的推理方法。

Method: 提出轻量级流程：提取目标实体的k跳邻域，序列化为结构化文本，通过少样本上下文学习提示LLM评估可疑性并生成解释。

Result: 在合成AML场景中，LLM能模拟分析师逻辑，标记风险并提供合理解释。

Conclusion: 研究表明LLM在图推理中具有潜力，为可解释的语言驱动金融犯罪分析奠定基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [347] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 论文提出了一种将等变性网络理论扩展到时间参数化序列变换（如RNN）的方法，解决了传统RNN在动态变换中的局限性，并展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的等变性网络仅适用于静态变换和前馈网络，无法处理时间参数化的序列变换（如RNN），限制了其应用范围。

Method: 通过引入流等变性（flow equivariance），改进RNN，使其能够处理动态变换（如视觉运动）。

Result: 改进后的模型在训练速度、长度泛化和速度泛化方面显著优于非等变模型，适用于下一步预测和序列分类任务。

Conclusion: 这是构建能够处理时间参数化对称性的序列模型的第一步，为未来研究奠定了基础。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [348] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现语言模型可以通过语义无关的数据传递行为特征，称为“潜意识学习”。即使过滤掉相关特征，学生模型仍能从教师模型生成的数据中学习到这些特征。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否能够通过看似无关的数据传递行为特征，揭示AI开发中的潜在风险。

Method: 通过教师模型生成数字序列、代码或推理痕迹，训练学生模型，观察其是否学习到教师模型的行为特征。

Result: 学生模型确实能从无关数据中学习到教师模型的行为特征，但前提是教师和学生模型的基础模型相同。

Conclusion: 潜意识学习是一种普遍现象，可能成为AI开发中的潜在陷阱，即使通过数据过滤也无法完全避免。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [349] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 本文提出了一个评估基础模型在电子健康记录（EHR）中性能、公平性和可解释性的综合基准，使用MIMIC-IV数据库，并开发了标准化数据处理流程。


<details>
  <summary>Details</summary>
Motivation: 基础模型在EHR处理中表现出强大潜力，但缺乏对其性能、公平性和可解释性的系统评估。

Method: 开发标准化数据处理流程，比较八种基础模型（单模态和多模态、领域专用和通用模型）。

Result: 多模态数据整合能提升预测性能且不引入额外偏差。

Conclusion: 该基准支持开发可信赖的多模态AI系统，适用于临床实践。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [350] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 研究在对比学习中引入自适应边界（eMargin）对时间序列表示学习的影响，发现其在无监督聚类指标上表现优异，但在下游分类任务中效果不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨自适应边界是否能改善时间序列表示学习中相邻但不相似时间步的分离，并提升下游任务性能。

Method: 在对比损失函数中引入自适应边界（eMargin），基于预定义相似度阈值调整，并在三个基准数据集上评估其对聚类和分类性能的影响。

Result: eMargin在无监督聚类指标上优于基线，但在下游分类任务中表现不佳。

Conclusion: 无监督聚类指标的高分并不一定意味着学到的嵌入对下游任务有效。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [351] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR（带可验证奖励的强化学习）虽能提升AI在复杂逻辑任务中的表现，但其是否能扩展模型的推理边界尚不明确。研究发现RLVR受限于基础模型的初始支持，可能限制原创解决方案的发现，并存在熵-奖励权衡。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是否真正扩展了模型的推理能力，还是仅放大了基础模型已知的高奖励输出。

Method: 通过理论和实证研究，分析RLVR的潜在限制，包括其对基础模型支持的依赖和熵-奖励的权衡。

Result: RLVR虽能提升精确度，但会缩小探索范围，可能忽略正确但低概率的解决方案。实证显示其支持范围缩小，且答案多样性下降。

Conclusion: RLVR在扩展推理能力方面存在潜在限制，未来需结合探索机制或混合策略以突破限制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [352] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR是一个基于Transformer的框架，通过时间感知注意力机制和预训练语言模型，解决了电子健康记录（EHR）中数据异质性和复杂时间模式的问题，显著提升了疾病进展预测的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的临床信息，但数据异质性和复杂的时间模式使其建模困难，尤其是临床事件之间的不规则时间间隔。

Method: 提出TALE-EHR框架，结合时间感知注意力机制和预训练语言模型（LLM）生成的嵌入，以捕捉细粒度的时间动态和语义信息。

Result: 在MIMIC-IV和PIC数据集上的实验表明，TALE-EHR在疾病进展预测等任务上优于现有方法。

Conclusion: TALE-EHR通过整合连续时间建模和强语义表示，为EHR分析提供了有效的解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [353] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: Graph Tsetlin Machine (GraphTM) 是一种基于图结构输入的机器学习方法，通过消息传递构建深度子句，提高了可解释性和数据利用率，在多个领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统 Tsetlin Machine (TM) 在处理图结构输入时存在局限性，GraphTM 旨在扩展其能力，支持序列、网格、关系和多模态输入，同时保持可解释性和高效性。

Method: GraphTM 通过消息传递构建嵌套深度子句，识别子图模式，从而减少子句数量并提高效率。

Result: 在图像分类、动作共指跟踪、推荐系统和病毒基因组序列分析中，GraphTM 表现优于或与现有方法相当，例如在 CIFAR-10 上比卷积 TM 高 3.86% 准确率。

Conclusion: GraphTM 展示了图表示学习和深度子句为 TM 学习带来的新可能性，适用于多种领域。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [354] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 提出了一种增强的重要性度量框架，用于结构化剪枝，以在压缩模型的同时保持应用特定性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的高复杂性和计算需求限制了其广泛应用，而传统剪枝方法在保持性能方面存在不足。

Method: 采用多策略确定每组剪枝的最优幅度，平衡压缩与任务性能。

Result: 在MNIST图像重建任务中，该方法有效保留了任务相关性能，即使大幅剪枝后仍保持模型可用性。

Conclusion: 所提方法成功解决了结构化剪枝中性能保持的挑战，为模型压缩提供了实用解决方案。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [355] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习中模型透明度不足的问题，提出将经典不确定性量化方法映射到量子领域，以提高模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和量子机器学习中模型透明度的缺失导致过拟合和预测过度自信等问题，目前量子机器学习领域缺乏解决这一问题的研究。

Method: 基于经典不确定性量化和量子贝叶斯建模的初步探索，理论开发并实证评估将经典方法映射到量子机器学习的技术。

Result: 研究发现，将经典不确定性量化方法应用于量子机器学习模型设计是必要的。

Conclusion: 强调在量子机器学习模型设计中引入不确定性意识的重要性，借鉴经典方法以提高模型透明度。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [356] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM是一种动态调整动量的方法，用于解决联邦学习中长尾数据分布导致的模型偏差和收敛问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据（尤其是长尾分布）中面临模型偏差和收敛困难的问题，现有动量方法表现不佳。

Method: 通过全局和每轮数据的动态调整动量（FedWCM），纠正长尾分布引入的方向偏差。

Result: 实验表明，FedWCM解决了非收敛问题，并在处理客户端异构性和数据不平衡方面优于现有方法。

Conclusion: FedWCM显著提升了联邦学习在长尾数据场景下的效率和效果。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [357] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出了一种新任务Time-RA，将时间序列异常检测从判别式任务转变为生成式任务，并引入多模态基准数据集RATs40K。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法仅支持二元分类，缺乏详细分类和解释性推理。

Method: 利用大型语言模型（LLMs）进行生成式推理，并开发多模态数据集RATs40K，包含40,000个样本，标注了细粒度类别和解释性推理。

Result: 通过LLMs和多模态LLMs的基准测试，展示了当前模型的优势和局限性，强调了监督微调的重要性。

Conclusion: 该任务和数据集为可解释的时间序列异常检测和推理开辟了新方向。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [358] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBAD的新型Transformer模型，用于检测互联网平台上的恶意用户，并通过局部和全局信息捕获以及对抗性训练增强模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在检测恶意用户时对输入序列的微小变化敏感，缺乏对抗攻击的鲁棒性。

Method: ROBAD结合Transformer编码器和解码器块，分别捕获帖子级局部信息和序列级全局信息，并通过对比学习增强分类层来模拟攻击者行为。

Result: 在Yelp和Wikipedia数据集上的实验表明，ROBAD能有效抵御先进的对抗攻击。

Conclusion: ROBAD通过局部-全局信息捕获和对抗性训练，显著提升了恶意用户检测的鲁棒性。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [359] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 本文提出通过强化学习训练流匹配策略，超越原始演示策略性能，并引入两种方法：RWFM和GRPO，在模拟任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过强化学习训练流匹配策略，以超越由次优策略（如人类操作员）生成的演示性能。

Method: 提出两种方法：Reward-Weighted Flow Matching (RWFM) 和 Group Relative Policy Optimization (GRPO)，并在模拟任务中验证其性能。

Result: 两种方法均显著优于原始演示策略，GRPO方法在成本上比ILFM方法减少50%至85%。

Conclusion: 强化学习可以显著提升流匹配策略的性能，GRPO方法尤其有效。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [360] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 提出了一种名为iQRA的新方法，通过集成点预测生成概率预测，改进电力市场价格预测的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 电力市场价格预测对市场参与者决策至关重要，但现有机器学习模型缺乏不确定性估计，增加了决策风险。

Method: 基于Quantile Regression Averaging (QRA)框架，引入随机顺序约束，提出Isotonic Quantile Regression Averaging (iQRA)方法。

Result: 在德国日前电力市场的预测研究中，iQRA在可靠性和锐度上优于现有后处理方法，提供更优的预测区间校准。

Conclusion: iQRA通过等渗正则化简化分位数回归问题，提供无超参数变量选择方法，显著提升预测性能。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [361] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒控制理论扩展，明确处理价值函数梯度的不确定性，适用于强化学习等领域。通过零和动态博弈建模，推导出新的非线性偏微分方程（GU-HJBI），并证明其适定性。在LQ情况下，发现经典二次价值函数假设失效，提出GURAC算法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习等领域中价值函数梯度不确定性问题，提升鲁棒控制理论的应用范围。

Method: 通过零和动态博弈建模，推导GU-HJBI方程，证明其适定性，分析LQ情况并提出GURAC算法。

Result: 发现经典二次价值函数假设在梯度不确定性下失效，提出非多项式修正和GURAC算法，验证其有效性。

Conclusion: 为鲁棒控制理论提供新方向，对强化学习和计算金融等领域有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [362] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: 论文提出AnalogFed，一种联邦学习方法，用于在保护隐私的前提下实现分散式协作的模拟电路拓扑发现。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计数据具有专有性，限制了生成式AI的应用和协作创新。

Method: 提出AnalogFed，结合联邦学习技术，处理数据异构性和隐私问题。

Result: 实验表明，AnalogFed在性能和隐私保护上与集中式基线相当，生成式AI模型表现出高效和可扩展性。

Conclusion: AnalogFed为模拟电路设计提供了一种安全、高效的协作创新解决方案。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [363] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 论文提出了一种分布级遗忘方法，通过移除少量数据点使模型远离不需要的分布，同时保留目标分布的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘工具主要针对单个样本，难以有效移除整个主题域的信息，导致隐私、法律或质量问题。

Method: 提出分布级遗忘框架，利用Kullback-Leibler散度量化移除和保留效果，推导高斯情况下的帕累托边界，并提出基于距离的选择规则。

Result: 实验表明，相比随机移除，该方法减少15-72%的删除量，且对保留性能影响可忽略。

Conclusion: 分布级遗忘是一种高效、模型无关的解决方案，适用于隐私、法律和内容质量需求。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [364] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 论文提出U-Cast模型和Time-HD基准，解决高维时间序列预测（HDTSF）中的复杂通道相关性问题。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）研究未充分关注高维场景（HDTSF）中复杂的通道相关性。

Method: 提出U-Cast模型，利用查询注意力学习潜在层次通道结构，并引入全秩正则化解耦高相关通道表示。

Result: 理论证明跨通道信息降低预测风险，实验显示U-Cast在Time-HD基准上优于基线模型。

Conclusion: U-Cast和Time-HD为未来HDTSF研究提供了坚实基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [365] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究多标签分类问题，利用序列模型处理标签间的逻辑约束，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决多标签分类中标签间逻辑约束的建模问题，以提高分类准确性。

Method: 采用序列模型联合训练个体标签分类器，生成联合分布以捕捉标签相关性。

Result: 实验证明该架构能有效利用训练中的约束，并在推理时强制执行约束。

Conclusion: 该架构在多标签分类中表现出色，尤其在处理标签约束时具有优势。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [366] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于共振隧穿二极管（RTD）的神经形态计算架构，用于物理储层计算（RC），并在图像识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向实时、边缘和资源受限环境扩展，需要硬件高效的计算模型。

Method: 理论构建并数值实现基于RTD的RC系统，应用于手写数字分类和Fruit~360数据集的对象识别。

Result: 该架构在性能表现良好，同时符合下一代RC的原则，即用确定性非线性变换替代随机连接。

Conclusion: RTD为基础的RC架构在资源受限环境中具有潜力。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [367] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 论文研究了反事实解释（CFEs）在机器学习决策模型中的应用，发现现有评估指标与用户偏好不一致，提出了一种新的用户中心评估模型AWP。


<details>
  <summary>Details</summary>
Motivation: 机器学习决策模型的不透明性导致用户难以理解决策原因，现有CFE评估指标可能忽视用户偏好和约束。

Method: 通过两个用户研究（20名和41名参与者）验证现有指标与用户偏好的对齐性，并提出AWP模型。

Result: 用户偏好的CFEs与现有指标仅63.81%一致，AWP模型预测用户偏好准确率达84.37%。

Conclusion: 研究强调了用户中心评估指标的重要性，并验证了AWP模型的有效性。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [368] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 本文提出了首个在仅观察配置变化时高效学习Ising模型的算法，解决了此前需要观察所有站点更新尝试的限制问题。


<details>
  <summary>Details</summary>
Motivation: 研究在更自然的观察模型下学习Ising模型的结构和参数，以解决此前强观察模型在实际应用中的局限性。

Method: 提出一种算法，在最大度为d的Ising模型中，通过观察配置变化恢复依赖图，并在额外时间内恢复参数。

Result: 算法在时间poly(d)·n²log n内恢复依赖图，并在额外Õ(2^d n)时间内恢复参数，与i.i.d.设置下的最优结果相当。

Conclusion: 该算法在更弱的观察模型下实现了与现有最优方法相当的性能，适用于更广泛的单站点可逆马尔可夫链。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [369] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT是一种将GAT应用于多智能体强化学习（MARL）的交通信号控制方法，通过结合邻近智能体的信息，平衡了可扩展性和增强的接地能力。


<details>
  <summary>Details</summary>
Motivation: 解决MARL在真实交通网络中因环境动态变化导致的性能下降（sim-to-real gap）问题。

Method: 采用分散式GAT方法，结合邻近智能体信息，提升MARL在交通信号控制中的可扩展性和接地能力。

Result: 在模拟恶劣天气条件下的多种道路网络中，JL-GAT表现出有效性。

Conclusion: JL-GAT为MARL在真实交通网络中的应用提供了一种可扩展且高效的解决方案。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [370] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出利用平均可控性和新型排名编码方法提升GNN在社交网络分类任务中的性能，解决了节点特征缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 社交网络中节点特征常因隐私或属性缺失而不可用，影响GNN性能。

Method: 提出两种策略：1) 使用平均可控性和中心性指标（NCT-EFA）作为节点特征；2) 开发排名编码方法将图论指标转化为固定维度特征。

Result: 实验表明，平均可控性和排名编码显著提升GNN性能，排名编码在GitHub数据集上ROC AUC从68.7%提升至73.9%。

Conclusion: 平均可控性和排名编码方法有效解决了节点特征缺失问题，提升了GNN在社交网络中的表现。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [371] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多模态方法LSDGNN，通过长距离和短距离图神经网络分别捕捉远距离和近距离话语特征，并引入差分正则器和双仿射模块优化特征交互。此外，改进的课程学习（ICL）解决了数据不平衡问题。实验表明，该模型在IEMOCAP和MELD数据集上表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 情感识别在对话中（ERC）是一个实际且具有挑战性的任务，需要有效捕捉多模态特征并处理数据不平衡问题。

Method: 基于有向无环图（DAG）构建长距离和短距离图神经网络，使用差分正则器和双仿射模块优化特征交互，并提出改进的课程学习（ICL）解决数据不平衡。

Result: 在IEMOCAP和MELD数据集上的实验结果表明，该模型优于现有基准。

Conclusion: LSDGNN通过多模态特征捕捉和优化的课程学习，显著提升了情感识别在对话中的性能。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [372] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 论文提出了一种精确约束重构方法，用于解决不平衡分类中的直接指标优化问题，包括固定精度优化召回率、固定召回率优化精度和优化Fβ分数。实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有不平衡分类方法通常优化平衡准确率，但在某些场景下无法满足特定指标要求，因此需要直接优化关键分类指标（如精度和召回率）。

Method: 引入精确约束重构，将直接指标优化问题转化为可精确求解的形式，并使用精确惩罚方法求解。

Result: 在多个基准数据集上，该方法在三种直接指标优化问题上优于现有方法。

Conclusion: 提出的精确重构与优化框架不仅适用于不平衡分类，还可推广到其他直接指标优化问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [373] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的图神经网络框架，用于捕捉时空依赖性，提高食品配送平台的需求预测准确性。


<details>
  <summary>Details</summary>
Motivation: 食品配送平台的需求预测对运营效率至关重要，但时空异质性和波动性增加了预测难度。

Method: 将配送环境建模为图，节点代表配送区域，边反映空间邻近性和订单流模式；结合注意力机制动态加权邻近区域影响，并联合学习时空趋势。

Result: 在真实数据集上验证了模型的高预测准确性。

Conclusion: 该框架为城市食品配送运营提供了可扩展和自适应的解决方案，支持车队定位、资源分配和调度优化。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [374] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS是一种无需训练、模型无关的扩散采样加速框架，通过多核并行实现高效推理，显著提升采样速度且不降低质量。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型的高保真生成能力受限于计算昂贵的推理过程，现有加速方法需重新训练或牺牲质量。

Method: 将多核扩散采样视为ODE求解器管道，通过理论支持的核间通信机制，慢速但精确的求解器逐步修正快速求解器。

Result: CHORDS在多样大规模图像和视频扩散模型中显著加速采样，四核提速2.1倍，八核提速2.9倍，且无质量损失。

Conclusion: CHORDS为实时高保真扩散生成奠定了坚实基础。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [375] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间基函数模型（TBFMs）的闭环神经刺激方法，用于个性化治疗神经系统疾病，如帕金森病。该方法在样本效率、训练时间和延迟方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决闭环神经刺激在个体化治疗中的挑战，如样本效率、训练时间和延迟问题，以推动AI技术在神经疾病治疗中的应用。

Method: 使用时间基函数模型（TBFMs）进行单次试验的时空前向预测，模拟光遗传刺激对局部场电位（LFPs）的影响，并应用于闭环刺激控制。

Result: TBF模型在40次实验中表现出高效性（15-20分钟数据收集，2-4分钟训练，0.2ms延迟），预测精度与基线非线性模型相当，优于线性模型。

Conclusion: TBF模型为复杂AI方法与临床实用的闭环刺激协议之间架起了桥梁，展现了其在神经疾病治疗中的潜力。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [376] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 论文提出了一种流式遗忘学习范式，解决了现有方法在处理流式数据遗忘请求时的效率低下和数据访问问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通常批量处理遗忘数据，而实际场景中数据遗忘请求往往是流式的，导致效率和效果下降。

Method: 将遗忘问题形式化为分布偏移问题，提出了一种无需原始训练数据的流式遗忘算法。

Result: 理论分析表明算法具有$O(\sqrt{T} + V_T)$的误差界限，实验验证了其性能。

Conclusion: 提出的流式遗忘算法在理论和实验上均表现出色，适用于实际场景。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [377] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出了一种利用不完整专家演示的强化学习框架，通过映射函数将状态与专家数据的相似性转化为内在奖励，实现灵活探索。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，强化学习代理需从无奖励交互和不完整演示中学习，但现有内在动机方法在高维空间或密集奖励环境中效果有限。

Method: 采用映射函数将状态与专家数据的相似性转化为内在奖励，并使用混合自编码专家模型处理演示中的缺失信息。

Result: 实验表明，该方法在稀疏和密集奖励环境中均能实现稳健探索和优异性能，即使演示不完整。

Conclusion: 为现实场景中缺乏最优数据和精确奖励控制的强化学习提供了实用框架。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [378] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 论文扩展了PSID方法，引入了滤波和平滑技术，以优化多变量时间序列的建模和预测。


<details>
  <summary>Details</summary>
Motivation: 现有PSID方法仅利用过去数据预测，而滤波和平滑技术可以提升离线应用的估计精度。

Method: 通过引入最优卡尔曼更新步骤和正向-反向PSID平滑算法，扩展PSID以实现滤波和平滑。

Result: 在模拟数据中验证了方法能恢复真实模型参数，并达到理想滤波和平滑性能。

Conclusion: 为多变量时间序列分析提供了更强大的工具，显著提升了动态交互建模的能力。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [379] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: FG-TS通过乐观奖励解决TS在高维问题中探索不足的问题，但在近似后验下的表现未被系统研究。本文首次全面评估FG-TS及其平滑变体SFG-TS，发现其在精确后验下表现优异，但在神经网络中较弱。


<details>
  <summary>Details</summary>
Motivation: 研究FG-TS在近似后验下的性能，填补现有理论空白，并评估其在实际应用中的鲁棒性。

Method: 在11个真实和合成基准上测试FG-TS和SFG-TS，比较精确后验（线性和逻辑bandits）与近似后验（随机梯度采样器）的性能。

Result: FG-TS在线性和逻辑bandits中优于TS，但在神经网络中表现较弱。研究发现奖励规模和采样噪声之间存在权衡。

Conclusion: FG-TS及其变体易于使用且性能良好，建议作为现代上下文bandit基准的基线。所有实验代码已开源。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [380] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT框架通过融合SE3不变和SO3等变的图表示，显著提升了晶体结构预测的准确性，并在多任务自监督预训练中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉晶体结构的复杂几何和拓扑特征，限制了机器学习在大规模晶体材料模拟中的应用。

Method: 提出MGT框架，结合SE3不变和SO3等变的图表示，并采用轻量级专家混合路由器动态调整权重。

Result: MGT在晶体属性预测任务中平均绝对误差降低21%，在迁移学习场景中性能提升高达58%。

Conclusion: MGT为晶体材料属性预测提供了高效工具，有望推动新材料发现。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [381] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN是一个用于神经网络模型优化的知识库管道，通过动态查询和知识编织技术改进模型选择与优化。


<details>
  <summary>Details</summary>
Motivation: 传统的静态模型选择方法忽略了任务查询与模型架构之间的动态依赖关系，导致匹配不理想。M-DESIGN旨在填补这一研究空白。

Method: 提出知识编织引擎，将模型优化转化为任务元数据的自适应查询问题，并利用图关系知识模式支持细粒度分析。

Result: 在33个数据-任务对中，M-DESIGN在26个案例中成功找到最优模型。

Conclusion: M-DESIGN通过动态知识库和自适应查询显著提升了模型优化效果。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [382] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，用于安全高效的协作式大型语言模型（LLM）微调，解决了传统联邦学习中的中心化漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（FL）需要中心服务器，存在单点攻击和中毒攻击的风险，而FLock通过区块链和激励机制实现了去中心化协作。

Method: FLock整合了基于区块链的信任层和经济激励机制，取代了中心聚合器，提供了安全、可审计的合作协议。

Result: 实验表明，FLock能抵御后门中毒攻击，减少对抗攻击成功率68%以上，并提升跨领域泛化能力。

Conclusion: FLock为去中心化环境下的大规模LLM微调提供了安全高效的解决方案，显著优于传统方法。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [383] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM是一种统一且可解释的数学模型，用于分析主动学习（AL）的动态过程，通过四个关键参数预测性能，并在多个数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习评估方法仅关注最终准确性，无法全面捕捉学习动态，PALM旨在填补这一空白。

Method: 提出PALM模型，通过四个参数（可实现准确性、覆盖效率、早期性能和可扩展性）描述AL轨迹，支持从部分观测预测未来性能。

Result: PALM在CIFAR和ImageNet数据集上广泛验证，能准确预测学习曲线并揭示AL方法的关键特性。

Conclusion: PALM为系统化、可重复的AL评估提供基础，支持在预算有限时选择高效策略。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [384] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSG的新框架，通过联合优化信道估计和网格化，仅使用RSRP数据实现高效网格化，并开发了CSG-AE模型和PIDA训练方案，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法（如GSG或BSG）依赖不可获取的位置数据或错误假设，限制了大规模网络优化的效率。

Method: 提出CSG框架，联合优化信道估计和网格化，开发CSG-AE模型（含RSRP-to-CAPS编码器、稀疏码本量化器和物理解码器）和PIDA训练方案。

Result: 在合成数据上，CSG-AE在CAPS估计和聚类质量上表现优异；在真实数据上，RSRP预测误差显著降低（Active MAE减少30%，Overall MAE减少65%）。

Conclusion: CSG框架和CSG-AE模型显著提升了网格化效率和性能，为大规模网络优化提供了新思路。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [385] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 论文提出了一种简单算法，证明其在先验分布满足对数凹性假设时能收敛到近端算子，为实践中常用的启发式方法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，预训练的降噪模型被用作近端算子的替代，但缺乏理论支持。本文旨在填补这一理论空白。

Method: 提出一种与实践中常用方法相关的简单算法，通过梯度下降平滑近端目标实现收敛。

Result: 证明算法在对数凹先验假设下收敛到近端算子，为启发式方法提供了理论依据。

Conclusion: 研究为实践中成功的启发式方法提供了理论支持，填补了现有方法的理论空白。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [386] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过拉格朗日优化在令牌空间上为Transformer提供了理论数学背景，将其视为高维单位球上的流映射，并推导了其变分问题。


<details>
  <summary>Details</summary>
Motivation: 为Transformer提供一个数学框架，通过变分法理解其动态行为，填补该领域的研究空白。

Method: 利用拉格朗日优化和变分法，推导Transformer的欧拉-拉格朗日方程，并证明其作为变分问题自然求解器的特性。

Result: 提出了Transformer的变分框架，证明了其流映射满足特定泛函，并推导了新的欧拉-拉格朗日方程。

Conclusion: 论文为Transformer的变分分析奠定了基础，为未来研究提供了新的理论工具和方向。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [387] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出了一种基于自适应随机傅里叶特征（ARFF）的训练算法，用于从快照数据中学习随机微分方程的漂移和扩散分量，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决从快照数据中学习随机微分方程漂移和扩散分量的挑战，提升建模效率和准确性。

Method: 采用自适应随机傅里叶特征（ARFF）结合Metropolis采样和重采样，基于Euler-Maruyama积分的似然损失函数。

Result: 在多项基准测试中，ARFF方法在损失最小化和收敛速度上均优于传统的Adam优化方法。

Conclusion: ARFF方法为数据驱动的随机动力学建模提供了高效且性能优越的替代方案。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [388] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo是一个隐私保护的多模态情感识别框架，通过联邦学习融合视觉和生理信号，实现高精度且保护隐私的车内情感识别。


<details>
  <summary>Details</summary>
Motivation: 解决车内情感识别中的模态脆弱性、生理信号个体差异和隐私风险问题。

Method: 结合CNN和随机森林，采用联邦学习框架，实现多模态决策级融合。

Result: 联邦CNN准确率77%，随机森林74%，融合后87%，与集中式基线相当，且保护数据隐私。

Conclusion: FedMultiEmo为车内实时隐私保护情感识别提供了实用解决方案。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [389] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 论文提出了一种名为Off-Policy Corrected Reward Modeling (OCRM)的方法，通过重要性加权迭代修正奖励模型，解决了RLHF中的过优化问题，提升了最终策略的性能。


<details>
  <summary>Details</summary>
Motivation: 在基于人类反馈的强化学习（RLHF）中，奖励模型（RM）在训练过程中因分布偏移而变得不准确，导致策略与人类偏好不匹配。

Method: 提出OCRM方法，通过重要性加权迭代修正RM，无需新标签或样本。

Result: 在摘要和聊天机器人数据集上的实验表明，OCRM显著优于标准RLHF方法和基线。

Conclusion: OCRM通过修正RM的分布偏移问题，有效提升了策略性能，为RLHF提供了一种改进方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [390] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 论文提出了一种名为“数据感知可微分神经架构搜索”的新方法，通过同时优化模型架构和输入数据特性，降低TinyML系统设计的复杂性。


<details>
  <summary>Details</summary>
Motivation: 机器学习的高资源消耗限制了其广泛应用，尤其是TinyML系统设计复杂，阻碍了其普及。

Method: 扩展了可微分神经架构搜索的搜索空间，包括数据配置参数和架构选择，实现模型与输入数据的协同优化。

Result: 在关键词识别任务上的初步结果显示，该方法能生成轻量且高精度的系统。

Conclusion: 该方法为TinyML系统设计提供了一种高效且资源友好的解决方案。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [391] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 该研究评估了传统放射组学（CR）和深度学习（DL）MRI放射组学在胶质母细胞瘤预后中的附加价值，发现其相对于临床和分子预测因子的优势有限。


<details>
  <summary>Details</summary>
Motivation: 探讨放射组学在胶质母细胞瘤预后中的实际价值，特别是在多中心数据集上的表现。

Method: 研究使用了1152名胶质母细胞瘤患者的数据，开发了CR和DL模型，并在内部和外部队列中评估其性能。

Result: 在外部验证中，结合特征的CR模型表现最佳（AUC=0.75），但仅略优于临床模型（0.74）。DL模型趋势相似但无统计学意义。

Conclusion: 研究证实了MRI序列的预测价值，但CR和DL放射组学在胶质母细胞瘤预后中的附加价值有限。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [392] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym是一个新的基准套件和模拟平台，用于评估基于大型语言模型（LLM）的科学推理能力，特别是在交互式物理环境中。它通过控制先验知识水平，帮助研究人员分析代理在不同问题复杂度和知识水平下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估LLM在科学发现能力方面的基准，特别是在处理环境复杂性和利用先验知识方面。

Method: PhysGym提供了一套交互式模拟环境，代理需要主动探索、收集数据并形成关于物理规律的假设。平台还提供了标准化的评估协议和指标。

Result: 通过基线LLM的结果展示了PhysGym能够区分不同先验知识和任务复杂度下的能力差异。

Conclusion: PhysGym填补了现有基准的空白，为评估LLM的科学推理能力提供了有效工具。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [393] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文探讨了住院时长（LOS）预测准确性与床位调度灵活性之间的关系，旨在优化资源利用并防止床位溢出。


<details>
  <summary>Details</summary>
Motivation: 下游资源（如床位）的可用性对计划选择性手术患者入院至关重要，但LOS预测误差可能导致调度不可行。

Method: 利用模拟机器学习评估不同纠正策略下LOS预测误差与调度灵活性的关系。

Result: 研究发现，更准确的LOS预测可减少调度调整的影响，但训练高精度模型成本较高。

Conclusion: 有效的患者调度策略需平衡预测准确性和操作灵活性，以优化资源利用并避免床位溢出。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [394] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: 本文探讨了AI在优化卫星巨型星座运营中的作用，通过强化学习在数据路由和资源分配中超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星星座的快速扩张对网络管理提出挑战，需要高效、可扩展且稳健的解决方案。

Method: 采用强化学习（RL）优化数据路由（降低延迟）和资源分配（高效利用电池和内存）。

Result: RL在多种卫星配置和场景中表现优于传统方法，提供更高的灵活性、可扩展性和通用性。

Conclusion: AI可改变卫星星座管理，提供更自适应、稳健且经济高效的解决方案。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [395] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 论文指出当前异常检测算法的评估方法存在局限性，导致进展停滞，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测算法的评估方法未能充分反映实际应用中的多样性，限制了算法的发展。

Method: 提出重新思考异常检测的基准测试方法，包括基于通用分类法的场景识别、端到端分析以及目标导向的评估。

Result: 指出当前评估方法的不足，并提出三个改进方向。

Conclusion: 需要重新设计异常检测的评估方法，以更好地反映实际应用需求。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [396] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 提出了一种红队多智能体强化学习框架，通过干扰和探索生成安全关键场景中的极端案例。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成安全关键场景时效率低且难以捕捉极端案例，需改进。

Method: 使用红队智能体干扰自动驾驶车辆，结合约束图表示马尔可夫决策过程和安全规则。

Result: 实验表明，该方法显著影响自动驾驶车辆决策安全并生成多种极端案例。

Conclusion: 为安全关键场景研究提供了新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [397] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 论文提出了一种深度学习替代模型，结合GRU和Geo-FNO，用于加速HEC-RAS洪水模拟，保持高精度并显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统物理求解器（如HEC-RAS）计算耗时，难以在洪水事件中实时决策，需在不牺牲精度的情况下加速模拟。

Method: 采用混合自回归架构，结合GRU捕捉短期时间动态和Geo-FNO建模长程空间依赖，从HEC-RAS文件中提取特征进行训练。

Result: 模型在密西西比河流域67个河段上测试，中位绝对水位误差为0.31英尺，计算时间从139分钟降至40分钟，提速3.5倍。

Conclusion: 数据驱动方法通过特征工程可替代传统水力模型，提升大规模洪水预测的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [398] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于模型的端到端框架Data Mixing Agent，通过强化学习自动调整源领域和目标领域的数据权重，以平衡模型性能，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决在任务特定数据上持续预训练大型语言模型时可能导致的灾难性遗忘问题，传统方法依赖人工启发式调整数据权重，缺乏通用性。

Method: 提出Data Mixing Agent框架，通过强化学习从大量数据混合轨迹中学习通用启发式规则，自动调整数据权重。

Result: 在数学推理任务上，Data Mixing Agent在源领域和目标领域性能平衡方面优于基线方法，且能泛化到未见过的领域和模型。

Conclusion: Data Mixing Agent展示了自动调整数据权重的有效性，能够平衡模型性能并减少对源领域数据的依赖。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [399] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的异常检测框架，用于共享单车系统，结合多源数据和无监督学习方法，识别异常并优化运营。


<details>
  <summary>Details</summary>
Motivation: 共享出行系统（如共享单车）对城市交通至关重要，但异常检测对优化运营、提高服务可靠性和用户体验至关重要。

Method: 采用Isolation Forest算法进行无监督异常检测，并结合DIFFI算法提供可解释性，整合了共享单车行程记录、天气条件和公共交通可用性等多源数据。

Result: 结果显示，站点级分析能有效识别异常，并揭示外部因素（如恶劣天气和公共交通限制）的影响。

Conclusion: 该框架有助于提升共享出行系统的决策能力。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [400] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN是一种神经网络框架，通过显式编码物理定律的几何先验，显著提升了长期稳定性、准确性和能量守恒。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法常忽略物理系统的几何基础，导致预测不稳定，尤其是在高维和混沌系统中。

Method: GeoHNN结合黎曼几何和辛几何，通过对称正定矩阵参数化惯性矩阵，并使用约束自编码器保持相空间体积。

Result: 实验表明，GeoHNN在耦合振子和高维可变形物体等系统中表现优于现有模型。

Conclusion: 嵌入物理几何不仅是理论需求，更是构建稳健、可泛化物理模型的实践必需。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [401] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 该论文研究了电动汽车充电站中的异常检测，结合可解释人工智能技术，通过无监督方法识别异常行为及其根本原因。


<details>
  <summary>Details</summary>
Motivation: 支持可再生能源交通转型需要可靠的充电基础设施，而异常检测是确保其效率和可靠性的关键。

Method: 采用Isolation Forest进行异常检测，并使用DIFFI方法识别异常的关键特征。

Result: 方法在真实工业案例中验证了有效性。

Conclusion: 结合可解释AI的无监督异常检测方法能有效识别充电行为异常及其原因。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [402] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 提出了一种基于多模态感知的毫米波信号阻塞预测框架，结合摄像头、GPS、LiDAR和雷达数据，通过深度学习模型和软加权融合策略实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段的车载通信系统易受动态障碍物（如车辆、行人、基础设施）的信号阻塞影响，需解决这一问题以提高通信可靠性。

Method: 利用多模态传感器数据（摄像头、GPS、LiDAR、雷达），通过独立的深度学习模型处理各传感器数据，并采用基于验证性能的软加权融合策略。

Result: 摄像头单独模型在1.5秒预测范围内表现最佳（F1分数97.1%，推理时间89.8ms），摄像头+雷达组合进一步提升至97.2% F1分数（95.7ms）。

Conclusion: 多模态感知在毫米波阻塞预测中表现出高效性和准确性，为动态环境中的主动无线通信提供了可行方案。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [403] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA是一种基于变分自编码器的全自动工作流程，用于通过拉曼光谱检测植物应激，无需手动预处理。


<details>
  <summary>Details</summary>
Motivation: 植物应激检测对农业至关重要，传统拉曼分析方法存在偏见和不一致问题。

Method: DIVA利用变分自编码器处理原始拉曼光谱（包括荧光背景），自动识别和量化光谱特征。

Result: DIVA成功检测了多种植物应激（如非生物和生物应激）。

Conclusion: DIVA结合深度学习和振动光谱，为AI驱动的植物健康评估铺平了道路。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [404] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 论文提出了一种名为PRO-DYN的命名法，用于分析时间序列预测模型中动态学习的重要性，并验证了动态学习块在模型末端的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在时间序列预测任务中表现不佳，作者假设这是因为模型未能充分学习数据的底层动态。

Method: 通过系统和实证研究，提出PRO-DYN命名法，分析现有模型的动态学习能力，并进行广泛实验验证。

Result: 实验表明，表现不佳的模型仅部分学习动态，且动态学习块位于模型末端时效果最佳。

Conclusion: 研究支持在模型中引入可学习的动态块，并将其作为最终预测器的重要性。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [405] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein-Rubinstein距离的专家融合模型（WR-EFM），用于解决图节点分类中类别间性能差异问题，显著提升了分类平衡性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在PubMed引文网络数据集中，传统GCN模型对不同类别的分类性能存在显著差异，尤其是Category 2的准确率较低。

Method: 提出WR-EFM模型，结合层归一化和残差连接的GNN模型（用于Category 0/1）和多跳图注意力网络（GAT，用于Category 2），并通过WR距离优化模型表示相似性。

Result: WR-EFM在三个类别上的准确率分别为77.8%、78.0%和79.9%，优于单一模型和标准融合方法，且类别间性能差异显著降低。

Conclusion: WR-EFM为处理类别不平衡的图分类任务提供了新范式，并通过WR距离实现了更优的特征融合。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [406] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 研究探讨小规模大语言模型（LLM）是否能通过带可验证奖励的强化学习（RLVR）获得稳健且可泛化的心智理论（ToM）能力，发现模型在训练数据上表现提升但无法泛化到新任务，表明其仅是窄域过拟合而非真正抽象ToM能力。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习技术是否能让小规模LLM具备类似人类的心智理论能力，以提升其社交智能。

Method: 使用RLVR方法训练小规模LLM，并在多个ToM数据集（HiToM、ExploreToM、FANToM）上进行系统评估，测试其在未见过数据集（如OpenToM）上的泛化能力。

Result: 小规模LLM无法发展出通用ToM能力，训练数据上的性能提升无法迁移到新任务，且长时间训练导致模型仅学习数据统计模式而非抽象能力。

Conclusion: RLVR方法无法让小规模LLM获得真正的ToM能力，其学习行为表现为窄域过拟合，而非抽象能力的掌握。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [407] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 本文提出了一种名为CSE-FSL的新型通信和存储高效联邦分裂学习方法，通过辅助网络减少客户端与服务器之间的通信和存储需求。


<details>
  <summary>Details</summary>
Motivation: 联邦分裂学习（FSL）虽然降低了边缘设备的计算负担，但仍存在高通信开销和服务器存储需求大的问题。

Method: CSE-FSL利用辅助网络在客户端本地更新权重，服务器仅维护单一模型，并通过选择性地传输数据减少通信量。

Result: 理论分析和实验结果表明，CSE-FSL在非凸损失函数下收敛，并显著降低了通信开销。

Conclusion: CSE-FSL是一种高效的FSL解决方案，适用于实际联邦学习任务。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [408] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 提出了一种结合CNN-LSTM-注意力-Adaboost和蛇群优化算法的混合模型，用于改进4D轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 解决中长期4D轨迹预测模型的局限性。

Method: 使用Adaboost算法划分多个弱学习器，结合CNN提取空间特征、LSTM捕获时间特征、注意力机制捕获全局特征，并通过蛇群优化算法优化超参数。

Result: 在真实ADS-B数据上测试，SO-CLA-adaboost优于传统优化器，预测精度提升39.89%。

Conclusion: 混合模型显著提高了4D轨迹预测的准确性和鲁棒性。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [409] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文提出了一种优化差分隐私学习算法隐私审计的方法，通过改进审计员的“金丝雀”数据集，显著提高了隐私参数的下界估计。


<details>
  <summary>Details</summary>
Motivation: 研究黑盒隐私审计，目标是仅通过算法输出（如训练好的模型）来估计差分隐私学习算法的隐私参数下界。

Method: 利用元梯度优化技术优化审计员使用的金丝雀数据集，以提高隐私审计的效果。

Result: 实验表明，优化后的金丝雀数据集可以将差分隐私图像分类模型的隐私参数下界提高超过2倍。

Conclusion: 该方法具有可迁移性和高效性，适用于不同规模的模型和训练方法。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [410] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出了一种利用大语言模型（LLM）快速、低成本生成合成表格数据的方法，通过将字段分布编码为可重用的采样脚本，显著提高了生成效率和数据真实性。


<details>
  <summary>Details</summary>
Motivation: 解决直接使用LLM逐条生成数据时的高时间和成本问题，特别是在需要大规模合成数据时。

Method: 利用LLM推断并编码每个字段的分布为可重用的采样脚本，自动分类字段类型（数值、分类或自由文本），从而高效生成多样且真实的数据。

Result: 实验结果表明，该方法在多样性和数据真实性上优于传统直接生成方法，并大幅降低了大规模数据生成的负担。

Conclusion: 该方法可加速生产管道的测试，缩短开发周期，提升系统效率，为研究者和从业者提供了可扩展且经济高效的合成数据生成方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>
